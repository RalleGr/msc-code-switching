<doc id="6631" url="https://en.wikipedia.org/wiki?curid=6631" title="Bus (computing)">
Bus (computing)

In computer architecture, a bus (a contraction of the Latin "omnibus", and historically also called "data highway") is a communication system that transfers data between components inside a computer, or between computers. This expression covers all related hardware components (wire, optical fiber, etc.) and software, including communication protocols.

Early computer buses were parallel electrical wires with multiple hardware connections, but the term is now used for any physical arrangement that provides the same logical function as a parallel electrical bus. Modern computer buses can use both parallel and bit serial connections, and can be wired in either a multidrop (electrical parallel) or daisy chain topology, or connected by switched hubs, as in the case of USB.

Computer systems generally consist of three main parts: 
An early computer might contain a hand-wired CPU of vacuum tubes, a magnetic drum for main memory, and a punch tape and printer for reading and writing data respectively. A modern system might have a multi-core CPU, DDR4 SDRAM for memory, a solid-state drive for secondary storage, a graphics card and LCD as a display system, a mouse and keyboard for interaction, and a Wi-Fi connection for networking. In both examples, computer buses of one form or another move data between all of these devices.

In most traditional computer architectures, the CPU and main memory tend to be tightly coupled. A microprocessor conventionally is a single chip which has a number of electrical connections on its pins that can be used to select an "address" in the main memory and another set of pins to read and write the data stored at that location. In most cases, the CPU and memory share signalling characteristics and operate in synchrony. The bus connecting the CPU and memory is one of the defining characteristics of the system, and often referred to simply as the system bus.

It is possible to allow peripherals to communicate with memory in the same fashion, attaching adaptors in the form of expansion cards directly to the system bus. This is commonly accomplished through some sort of standardized electrical connector, several of these forming the expansion bus or local bus. However, as the performance differences between the CPU and peripherals varies widely, some solution is generally needed to ensure that peripherals do not slow overall system performance. Many CPUs feature a second set of pins similar to those for communicating with memory, but able to operate at very different speeds and using different protocols. Others use smart controllers to place the data directly in memory, a concept known as direct memory access. Most modern systems combine both solutions, where appropriate.

As the number of potential peripherals grew, using an expansion card for every peripheral became increasingly untenable. This has led to the introduction of bus systems designed specifically to support multiple peripherals. Common examples are the SATA ports in modern computers, which allow a number of hard drives to be connected without the need for a card. However, these high-performance systems are generally too expensive to implement in low-end devices, like a mouse. This has led to the parallel development of a number of low-performance bus systems for these solutions, the most common example being the standardized Universal Serial Bus (USB). All such examples may be referred to as peripheral buses, although this terminology is not universal.

In modern systems the performance difference between the CPU and main memory has grown so great that increasing amounts of high-speed memory is built directly into the CPU, known as a cache. In such systems, CPUs communicate using high-performance buses that operate at speeds much greater than memory, and communicate with memory using protocols similar to those used solely for peripherals in the past. These system buses are also used to communicate with most (or all) other peripherals, through adaptors, which in turn talk to other peripherals and controllers. Such systems are architecturally more similar to multicomputers, communicating over a bus rather than a network. In these cases, expansion buses are entirely separate and no longer share any architecture with their host CPU (and may in fact support many different CPUs, as is the case with PCI). What would have formerly been a system bus is now often known as a front-side bus.

Given these changes, the classical terms "system", "expansion" and "peripheral" no longer have the same connotations. Other common categorization systems are based on the bus's primary role, connecting devices internally or externally, PCI vs. SCSI for instance. However, many common modern bus systems can be used for both; SATA and the associated eSATA are one example of a system that would formerly be described as internal, while certain automotive applications use the primarily external IEEE 1394 in a fashion more similar to a system bus. Other examples, like InfiniBand and IÂ²C were designed from the start to be used both internally and externally.

The internal bus, also known as internal data bus, memory bus, system bus or front-side bus, connects all the internal components of a computer, such as CPU and memory, to the motherboard. Internal data buses are also referred to as local buses, because they are intended to connect to local devices. This bus is typically rather quick and is independent of the rest of the computer operations.

The external bus, or expansion bus, is made up of the electronic pathways that connect the different external devices, such as printer etc., to the computer.

An address bus is a bus that is used to specify a physical address. When a processor or DMA-enabled device needs to read or write to a memory location, it specifies that memory location on the address bus (the value to be read or written is sent on the data bus). The width of the address bus determines the amount of memory a system can address. For example, a system with a 32-bit address bus can 
address 2 (4,294,967,296) memory locations. If each memory location holds one byte, the addressable memory space is 4 GiB.

Early processors used a wire for each bit of the address width. For example, a 16-bit address bus had 16 physical wires making up the bus. As the buses became wider and lengthier, this approach became expensive in terms of the number of chip pins and board traces. Beginning with the Mostek 4096 DRAM, address multiplexing implemented with multiplexers became common. In a multiplexed address scheme, the address is sent in two equal parts on alternate bus cycles. This halves the number of address bus signals required to connect to the memory. For example, a 32-bit address bus can be implemented by using 16 lines and sending the first half of the memory address, immediately followed by the second half memory address

Typically 2 additional pins in the control bus -- a row-address strobe (RAS) and the column-address strobe (CAS) -- are used to tell the DRAM whether the address bus is currently sending the first half of the memory address or the second half.

Accessing an individual byte frequently requires reading or writing the full bus width (a word) at once. In these instances the least significant bits of the address bus may not even be implemented - it is instead the responsibility of the controlling device to isolate the individual byte required from the complete word transmitted. This is the case, for instance, with the VESA Local Bus which lacks the two least significant bits, limiting this bus to aligned 32-bit transfers.

Historically, there were also some examples of computers which were only able to address words -- word machines.

Buses can be parallel buses, which carry data words in parallel on multiple wires, or serial buses, which carry data in bit-serial form. The addition of extra power and control connections, differential drivers, and data connections in each direction usually means that most serial buses have more conductors than the minimum of one used in 1-Wire and UNI/O. As data rates increase, the problems of timing skew, power consumption, electromagnetic interference and crosstalk across parallel buses become more and more difficult to circumvent. One partial solution to this problem has been to double pump the bus. Often, a serial bus can be operated at higher overall data rates than a parallel bus, despite having fewer electrical connections, because a serial bus inherently has no timing skew or crosstalk. USB, FireWire, and Serial ATA are examples of this. Multidrop connections do not work well for fast serial buses, so most modern serial buses use daisy-chain or hub designs.

Network connections such as Ethernet are not generally regarded as buses, although the difference is largely conceptual rather than practical. An attribute generally used to characterize a bus is that power is provided by the bus for the connected hardware. This emphasizes the busbar origins of bus architecture as supplying switched or distributed power. This excludes, as buses, schemes such as serial RS-232, parallel Centronics, IEEE 1284 interfaces and Ethernet, since these devices also needed separate power supplies. Universal Serial Bus devices may use the bus supplied power, but often use a separate power source. This distinction is exemplified by a telephone system with a connected modem, where the RJ11 connection and associated modulated signalling scheme is not considered a bus, and is analogous to an Ethernet connection. A phone line connection scheme is not considered to be a bus with respect to signals, but the Central Office uses buses with cross-bar switches for connections between phones.

However, this distinctionthat power is provided by the busis not the case in many avionic systems, where data connections such as ARINC 429, ARINC 629, MIL-STD-1553B (STANAG 3838), and EFABus (STANAG 3910) are commonly referred to as âdata busesâ or, sometimes, "databuses". Such avionic data buses are usually characterized by having several equipments or Line Replaceable Items/Units (LRI/LRUs) connected to a common, shared media. They may, as with ARINC 429, be simplex, i.e. have a single source LRI/LRU or, as with ARINC 629, MIL-STD-1553B, and STANAG 3910, be duplex, allow all the connected LRI/LRUs to act, at different times (half duplex), as transmitters and receivers of data.

Some processors use a dedicated wire for each bit of the address bus, data bus, and the control bus.
For example, the 64-pin STEbus is composed of 8 physical wires dedicated to the 8-bit data bus, 20 physical wires dedicated to the 20-bit address bus, 21 physical wires dedicated to the control bus, and 15 physical wires dedicated to various power buses.

Bus multiplexing requires fewer wires, which reduces costs in many early microprocessors and DRAM chips.
One common multiplexing scheme, address multiplexing, has already been mentioned.
Another multiplexing scheme re-uses the address bus pins as the data bus pins, an approach used by conventional PCI.
The various "serial buses" can be seen as the ultimate limit of multiplexing, sending each of the address bits and each of the data bits, one at a time, through a single pin (or a single differential pair).

Over time, several groups of people worked on various computer bus standards, including the IEEE Bus Architecture Standards Committee (BASC), the IEEE "Superbus" study group, the open microprocessor initiative (OMI), the open microsystems initiative (OMI), the "Gang of Nine" that developed EISA, etc.

Early computer buses were bundles of wire that attached computer memory and peripherals. Anecdotally termed the ""digit trunk"", they were named after electrical power buses, or busbars. Almost always, there was one bus for memory, and one or more separate buses for peripherals. These were accessed by separate instructions, with completely different timings and protocols.

One of the first complications was the use of interrupts. Early computer programs performed I/O by waiting in a loop for the peripheral to become ready. This was a waste of time for programs that had other tasks to do. Also, if the program attempted to perform those other tasks, it might take too long for the program to check again, resulting in loss of data. Engineers thus arranged for the peripherals to interrupt the CPU. The interrupts had to be prioritized, because the CPU can only execute code for one peripheral at a time, and some devices are more time-critical than others.

High-end systems introduced the idea of channel controllers, which were essentially small computers dedicated to handling the input and output of a given bus. IBM introduced these on the IBM 709 in 1958, and they became a common feature of their platforms. Other high-performance vendors like Control Data Corporation implemented similar designs. Generally, the channel controllers would do their best to run all of the bus operations internally, moving data when the CPU was known to be busy elsewhere if possible, and only using interrupts when necessary. This greatly reduced CPU load, and provided better overall system performance.
To provide modularity, memory and I/O buses can be combined into a unified system bus. In this case, a single mechanical and electrical system can be used to connect together many of the system components, or in some cases, all of them.

Later computer programs began to share memory common to several CPUs. Access to this memory bus had to be prioritized, as well. The simple way to prioritize interrupts or bus access was with a daisy chain. In this case signals will naturally flow through the bus in physical or logical order, eliminating the need for complex scheduling.

Digital Equipment Corporation (DEC) further reduced cost for mass-produced minicomputers, and mapped peripherals into the memory bus, so that the input and output devices appeared to be memory locations. This was implemented in the Unibus of the PDP-11 around 1969.

Early microcomputer bus systems were essentially a passive backplane connected directly or through buffer amplifiers to the pins of the CPU. Memory and other devices would be added to the bus using the same address and data pins as the CPU itself used, connected in parallel. Communication was controlled by the CPU, which read and wrote data from the devices as if they are blocks of memory, using the same instructions, all timed by a central clock controlling the speed of the CPU. Still, devices interrupted the CPU by signaling on separate CPU pins.

For instance, a disk drive controller would signal the CPU that new data was ready to be read, at which point the CPU would move the data by reading the "memory location" that corresponded to the disk drive. Almost all early microcomputers were built in this fashion, starting with the S-100 bus in the Altair 8800 computer system.

In some instances, most notably in the IBM PC, although similar physical architecture can be employed, instructions to access peripherals (codice_1 and codice_2) and memory (codice_3 and others) have not been made uniform at all, and still generate distinct CPU signals, that could be used to implement a separate I/O bus.

These simple bus systems had a serious drawback when used for general-purpose computers. All the equipment on the bus had to talk at the same speed, as it shared a single clock.

Increasing the speed of the CPU becomes harder, because the speed of all the devices must increase as well. When it is not practical or economical to have all devices as fast as the CPU, the CPU must either enter a wait state, or work at a slower clock frequency temporarily, to talk to other devices in the computer. While acceptable in embedded systems, this problem was not tolerated for long in general-purpose, user-expandable computers.

Such bus systems are also difficult to configure when constructed from common off-the-shelf equipment. Typically each added expansion card requires many jumpers in order to set memory addresses, I/O addresses, interrupt priorities, and interrupt numbers.

"Second generation" bus systems like NuBus addressed some of these problems. They typically separated the computer into two "worlds", the CPU and memory on one side, and the various devices on the other. A "bus controller" accepted data from the CPU side to be moved to the peripherals side, thus shifting the communications protocol burden from the CPU itself. This allowed the CPU and memory side to evolve separately from the device bus, or just "bus". Devices on the bus could talk to each other with no CPU intervention. This led to much better "real world" performance, but also required the cards to be much more complex. These buses also often addressed speed issues by being "bigger" in terms of the size of the data path, moving from 8-bit parallel buses in the first generation, to 16 or 32-bit in the second, as well as adding software setup (now standardised as Plug-n-play) to supplant or replace the jumpers.

However, these newer systems shared one quality with their earlier cousins, in that everyone on the bus had to talk at the same speed. While the CPU was now isolated and could increase speed, CPUs and memory continued to increase in speed much faster than the buses they talked to. The result was that the bus speeds were now very much slower than what a modern system needed, and the machines were left starved for data. A particularly common example of this problem was that video cards quickly outran even the newer bus systems like PCI, and computers began to include AGP just to drive the video card. By 2004 AGP was outgrown again by high-end video cards and other peripherals and has been replaced by the new PCI Express bus.

An increasing number of external devices started employing their own bus systems as well. When disk drives were first introduced, they would be added to the machine with a card plugged into the bus, which is why computers have so many slots on the bus. But through the 1980s and 1990s, new systems like SCSI and IDE were introduced to serve this need, leaving most slots in modern systems empty. Today there are likely to be about five different buses in the typical machine, supporting various devices.

"Third generation" buses have been emerging into the market since about 2001, including HyperTransport and InfiniBand. They also tend to be very flexible in terms of their physical connections, allowing them to be used both as internal buses, as well as connecting different machines together. This can lead to complex problems when trying to service different requests, so much of the work on these systems concerns software design, as opposed to the hardware itself. In general, these third generation buses tend to look more like a network than the original concept of a bus, with a higher protocol overhead needed than early systems, while also allowing multiple devices to use the bus at once.

Buses such as Wishbone have been developed by the open source hardware movement in an attempt to further remove legal and patent constraints from computer design.

The Compute Express Link (CXL) is an open standard interconnect for high-speed CPU-to-device and CPU-to-memory, designed to accelerate next-generation data center performance.





</doc>
<doc id="6634" url="https://en.wikipedia.org/wiki?curid=6634" title="Cadillac (disambiguation)">
Cadillac (disambiguation)

Cadillac is a General Motors luxury car brand.

Cadillac may also refer to:














</doc>
<doc id="6635" url="https://en.wikipedia.org/wiki?curid=6635" title="Chinese checkers">
Chinese checkers

Sternhalma, commonly known as Chinese Checkers (US and Canadian spelling) or Chinese Chequers (UK spelling), is a strategy board game of German origin which can be played by two, three, four, or six people, playing individually or with partners. The game is a modern and simplified variation of the game Halma.

The objective is to be first to race all of one's pieces across the hexagram-shaped board into "home"âthe corner of the star opposite one's starting cornerâusing single-step moves or moves that over other pieces. The remaining players continue the game to establish second-, third-, fourth-, fifth-, and last-place finishers. The rules are simple, so even young children can play.

Despite its name, the game is not a variation of checkers, nor did it originate in China or any part of Asia. The game was invented in Germany in 1892 under the name "Stern-Halma" as a variation of the older American game Halma. The "Stern" (German for "star") refers to the board's star shape (in contrast to the square board used in Halma). 

The name "Chinese Checkers" originated in the United States as a marketing scheme by Bill and Jack Pressman in 1928. The Pressman company's game was originally called "Hop Ching Checkers".

In Japan, the game is known as "Diamond Game" (ãã¤ã¤ã¢ã³ãã²ã¼ã ). The game was introduced to Chinese-speaking regions mostly by the Japanese, where it is known as "Tiaoqi" (, "jump chess").

The aim is to race all one's pieces into the star corner on the opposite side of the board before the opponents do the same. The destination corner is called "home". Each player has 10 pieces, except in games between two players when 15 pieces are used. (On bigger star boards, 15 or 21 pieces are used.)

In "hop across", the most popular variation, each player starts with their colored pieces on one of the six points or corners of the star and attempts to race them all home into the opposite corner. Players take turns moving a single piece, either by moving one step in any direction to an adjacent empty space, or by jumping in one or any number of available consecutive hops over other single pieces. A player may not combine hopping with a single-step moveÂ â a move consists of one or the other. There is no capturing in Sternhalma, so pieces that are hopped over remain active and in play. Turns proceed clockwise around the board.

In the diagram, Green might move the topmost piece one space diagonally forward as shown. A "hop" consists of jumping over a single adjacent piece, either one's own or an opponent's, to the empty space directly beyond it in the same line of direction. Red might advance the indicated piece by a chain of three hops in a single move. It is not mandatory to make the most number of hops possible. (In some instances a player may choose to stop the jumping sequence part way in order to impede the opponent's progress, or to align pieces for planned future moves.)

Can be played "all versus all", or three teams of two. When playing teams, teammates usually sit at opposite corners of the star, with each team member controlling their own colored set of pieces. The first team to advance both sets to their home destination corners is the winner. The remaining players usually continue play to determine second- and third-place finishers, etc.

The four-player game is the same as the game for six players, except that two opposite corners will be unused.

In a three-player game, all players control either one or two sets of pieces each. If one set is used, pieces race across the board into empty, opposite corners. If two sets are used, each player controls two differently colored sets of pieces at opposite corners of the star.

In a two-player game, each player plays one, two, or three sets of pieces. If one set is played, the pieces usually go into the opponent's starting corner, and the number of pieces per side is increased to 15 (instead of the usual 10). If two sets are played, the pieces can either go into the opponent's starting corners, or one of the players' two sets can go into an opposite empty corner. If three sets are played, the pieces usually go into the opponent's starting corners.

A basic strategy is to create or find the longest hopping path that leads closest to home, or immediately into it. (Multiple-jump moves are obviously faster to advance pieces than step-by-step moves.) Since either player can make use of any hopping 'ladder' or 'chain' created, a more advanced strategy involves hindering an opposing player in addition to helping oneself make jumps across the board. Of equal importance are the players' strategies for emptying and filling their starting and home corners. Games between top players are rarely decided by more than a couple of moves.

Differing numbers of players result in different starting layouts, in turn imposing different best-game strategies. For example, if a player's home destination corner starts empty (i.e. is not an opponent's starting corner), the player can freely build a 'ladder' or 'bridge' with their pieces between the two opposite ends. But if a player's opponent occupies the home corner, the player may need to wait for opponent pieces to clear before filling the home vacancies.

While the standard rules allow hopping over only a single adjacent occupied position at a time (as in checkers), this version of the game allows pieces to catapult over multiple adjacent occupied positions in a line when hopping.

In the "fast-paced" or "Super Chinese Checkers" variant popular in France, a piece may hop over a "non-adjacent" piece. A hop consists of jumping over a distant piece (friendly or enemy) to a symmetrical position on the opposite side, in the same line of direction. (For example, if there are two empty positions between the jumping piece and the piece being jumped, the jumping piece lands leaving exactly two empty positions immediately beyond the jumped piece.) As in the standard rules, a jumping move may consist of any number of a chain of hops. (When making a chain of hops, a piece is usually allowed to enter an empty corner, as long as it hops out again before the move is completed.)

Jumping over two or more pieces in a hop is not allowed. Therefore, in this variant even more than in the standard version, it is sometimes strategically important to keep one's pieces bunched in order to prevent a long opposing hop.

An alternative variant allows hops over "any" symmetrical arrangement, including pairs of pieces, pieces separated by empty positions, and so on.

In the "capture" variant, all sixty game pieces start out in the hexagonal field in the center of the gameboard. The center position is left unoccupied, so pieces form a symmetric hexagonal pattern. Color is irrelevant in this variant, so players take turns hopping any game piece over any other eligible game piece(s) on the board. The hopped-over pieces are captured (retired from the game, as in English draughts) and collected in the capturing player's bin. Only jumping moves are allowed; the game ends when no further jumps are possible. The player with the most captured pieces is the winner.

The board is tightly packed at the start of the game; as more pieces are captured, the board frees up, often allowing multiple captures to take place in a single move.

Two or more players can compete in this variant, but if there are more than six players, not everyone will get a fair turn.

This variant resembles the game Leap Frog. The main difference being that in Leap Frog the board is a square board.

Diamond game is a variant of Sternhalma played in South Korea and Japan. It uses the same jump rule as in Sternhalma. The aim of the game is to enter all one's pieces into the star corner on the opposite side of the board, before opponents do the same. Each player has ten or fifteen pieces. Ten-piece diamond uses a smaller gameboard than Sternhalma, with 73 spaces. Fifteen-piece diamond uses the same board as in Sternhalma, with 121 spaces. To play diamond each player selects one color and places their 10 or 15 pieces on a triangle. Two or three players can compete.

Bibliography



</doc>
<doc id="6639" url="https://en.wikipedia.org/wiki?curid=6639" title="Cantor Fitzgerald">
Cantor Fitzgerald

Cantor Fitzgerald is an American financial services firm that was founded in 1945. It specializes in institutional equity, fixed income sales and trading, and serving the middle market with investment banking services, prime brokerage, and commercial real estate financing. It is also active in new businesses, including advisory and asset management services, gaming technology, and e-commerce. It has more than 5,000 institutional clients.

Cantor Fitzgerald is one of 22 primary dealers that are authorized to trade US government securities with the Federal Reserve Bank of New York.

Cantor Fitzgerald's 1,600 employees work in more than 30 locations, including financial centers in the Americas, Europe, Asia-Pacific, and the Middle East. Together with its affiliates, Cantor Fitzgerald operates in more than 60 offices in 20 countries and has more than 8,500 employees.

In 2011, Cantor's affiliate, BGC Partners, expanded into commercial real estate services by its purchase of Newmark Knight Frank and the assets of Grubb & Ellis, to form Newmark Grubb Knight Frank.

Cantor Fitzgerald was formed in 1945 by Bernard Gerald Cantor and John Fitzgerald as an investment bank and brokerage business. It later became known for its computer-based bond brokerage, for the quality of its institutional distribution business model, and for being the market's premier dealer of government securities.

In 1965, Cantor Fitzgerald began "large block" sales/trading of equities for institutional customers. It became the world's first electronic marketplace for US government securities in 1972 and in 1983, it was the first to offer worldwide screen brokerage services in US government securities.

In 1991, Howard Lutnick was named president and CEO of Cantor Fitzgerald; he became chairman of Cantor Fitzgerald, L.P., in 1996.

Cantor Fitzgerald's corporate headquarters and New York City office, on the 101st to the 105th floors of One World Trade Center in Lower Manhattan (2 to 6 floors above the impact zone of a hijacked airliner), were destroyed during the September 11, 2001 attacks. At 8:46:46Â a.m., six seconds after the tower was struck by the plane, a Goldman Sachs server issued an alert saying that its trading system had gone offline because it was unable to connect with the server. Every employee that reported for work that morning was killed in the attacks; 658 of its 960 New York employees, 68.5% of its workforce, which was considerably more than any of the other World Trade Center tenants or the New York City Police Department, the Port Authority of New York and New Jersey Police Department, the New York City Fire Department, or the United States Department of Defense since all stairwells leading past the impact zone were destroyed by the initial crash or blocked with smoke, fire, or debris. Lutnick himself was not present because he was taking his son to his first day of kindergarten, but his younger brother, Gary, was among those killed. Lutnick vowed to keep the company alive, and the company was able to bring its trading markets back online within a week.

On September 19, Cantor Fitzgerald made a pledge to distribute 25% of the firm's profits for the next five years, and it committed to paying for ten years of health care for the benefit of the families of its 658 former Cantor Fitzgerald, eSpeed, and TradeSpark employees (profits that would otherwise have been distributed to the Cantor Fitzgerald partners). In 2006, the company had completed its promise, having paid a total of $180Â million (and an additional $17Â million from a relief fund run by Lutnick's sister, Edie).

Until the attacks, Cantor had handled about a quarter of the daily transactions in the multitrillion-dollar treasury security market. Cantor Fitzgerald has since rebuilt its infrastructure, partly by the efforts of its London office, and it now has its headquarters in Midtown Manhattan. The company's effort to regain its footing was the subject of Tom Barbash's 2003 book "On Top of the World: Cantor Fitzgerald, Howard Lutnick, and 9/11: A Story of Loss and Renewal" as well as a 2012 documentary, "Out of the Clear Blue Sky".

On September 2, 2004, Cantor and other organizations filed a civil lawsuit against Saudi Arabia for allegedly providing money to the hijackers and Al Qaeda. It was later joined in the suit by the Port Authority of New York. Most of the claims against Saudi Arabia were dismissed on January 18, 2005.

In December 2013, Cantor Fitzgerald settled its lawsuit against American Airlines for $135 million. Cantor Fitzgerald had been suing for loss of property and interruption of business by alleging the airline to have been negligent by allowing hijackers to board Flight 11.

In 2003, the firm launched its fixed income sales and trading group.

In 2006, the Federal Reserve added Cantor Fitzgerald & Co. to its list of primary dealers.

In 2009, the firm launched Cantor Prime Services, a provider of multi-asset, perimeter brokerage prime brokerage platforms to exploit its clearing, financing, and execution capabilities.

Cantor Fitzgerald began building its real estate business with the launch of CCRE in 2010.

On December 5, 2014, two Cantor Fitzgerald analysts were said to be in the top 25 analysts on TipRanks.

Cantor Fitzgerald has a prolific Special-purpose acquisition company underwriting practice, having led all banks in SPAC underwriting activity in both 2018 and 2019.

Edie wrote "An Unbroken Bond: The Untold Story of How the 658 Cantor Fitzgerald Families Faced the Tragedy of 9/11 and Beyond". All proceeds from the sale of the book benefit the Cantor Fitzgerald Relief Fund and the charities that it assists.

The Cantor Fitzgerald Relief Fund provided $10 million to families affected by Hurricane Sandy. Howard Lutnick and the Relief Fund "adopted" 19 elementary schools in impacted areas by distributing $1,000 prepaid debit cards to each family from the schools. A total of $10 million in funds was given to families affected by the storm.

Two days after the 2013 Moore tornado struck Moore, Oklahoma, killing 24 people and injuring hundreds, Lutnick pledged to donate $2 million to families affected by the tornado. The donation was given out in the form of $1,000 debit cards given out to families.

Each year, on September 11, Cantor Fitzgerald and its affiliate, BGC Partners, donate 100% of their revenue to charitable causes on their annual Charity Day, which was originally established to raise money to assist the families of the Cantor employees who died in the World Trade Center attacks. Since its inception, Charity Day has raised $110 million for charities globally.

The firm has many subsidiaries and affiliates such as the following:



An employee, Venetia Thompson, published, in February 2008, an article in "The Spectator" about her and her colleagues' behavior, with an emphasis on the drinking culture. She was subsequently fired for gross misconduct. She responded in 2010 by publishing a book, "Gross Misconduct: My Year of Excess in the City", about her experiences.



</doc>
<doc id="6641" url="https://en.wikipedia.org/wiki?curid=6641" title="Cane toad">
Cane toad

The cane toad ("Rhinella marina"), also known as the giant neotropical toad or marine toad, is a large, terrestrial true toad native to South and mainland Central America, but which has been introduced to various islands throughout Oceania and the Caribbean, as well as Northern Australia. It is the world's largest toad. It is a member of the genus "Rhinella", which includes many true toad species found throughout Central and South America, but it was formerly assigned to the genus "Bufo".

The cane toad is an old species. A fossil toad (specimen UCMP 41159) from the La Venta fauna of the late Miocene of Colombia is indistinguishable from modern cane toads from northern South America. It was discovered in a floodplain deposit, which suggests the "R. marina" habitat preferences have long been for open areas. The cane toad is a prolific breeder; females lay single-clump spawns with thousands of eggs. Its reproductive success is partly because of opportunistic feeding: it has a diet, unusual among anurans, of both dead and living matter. Adults average in length; the largest recorded specimen had a snout-vent length of .

The cane toad has poison glands, and the tadpoles are highly toxic to most animals if ingested. Its toxic skin can kill many animals, both wild and domesticated, and cane toads are particularly dangerous to dogs. Because of its voracious appetite, the cane toad has been introduced to many regions of the Pacific and the Caribbean islands as a method of agricultural pest control. The common name of the species is derived from its use against the cane beetle ("Dermolepida albohirtum"), which damages sugar cane. The cane toad is now considered a pest and an invasive species in many of its introduced regions. The 1988 film "" documented the trials and tribulations of the introduction of cane toads in Australia.

Historically, the cane toads were used to eradicate pests from sugarcane, giving rise to their common name. The cane toad has many other common names, including "giant toad" and "marine toad"; the former refers to its size, and the latter to the binomial name, "R. marina". It was one of many species described by Carl Linnaeus in his 18th-century work "Systema Naturae" (1758). Linnaeus based the specific epithet "marina" on an illustration by Dutch zoologist Albertus Seba, who mistakenly believed the cane toad to inhabit both terrestrial and marine environments. Other common names include "giant neotropical toad", "Dominican toad", "giant marine toad", and "South American cane toad". In Trinidadian English, they are commonly called "crapaud", the French word for toad.

The genus "Rhinella" is considered to constitute a distinct genus of its own, thus changing the scientific name of the cane toad. In this case, the specific name "marinus" (masculine) changes to "marina" (feminine) to conform with the rules of gender agreement as set out by the International Code of Zoological Nomenclature, changing the binomial name from "Bufo marinus" to "Rhinella marina"; the binomial "Rhinella marinus" was subsequently introduced as a synonym through misspelling by Pramuk, Robertson, Sites, and Noonan (2008). Though controversial (with many traditional herpetologists still using "Bufo marinus") the binomial "Rhinella marina" is gaining in acceptance with such bodies as the IUCN, Encyclopaedia of Life, Amphibian Species of the World and increasing numbers of scientific publications adopting its usage.

Since 2016, cane toad populations native to Mesoamerica and northwestern South America are sometimes considered to be a separate species, "Rhinella horribilis".
In Australia, the adults may be confused with large native frogs from the genera "Limnodynastes", "Cyclorana", and "Mixophyes". These species can be distinguished from the cane toad by the absence of large parotoid glands behind their eyes and the lack of a ridge between the nostril and the eye. Cane toads have been confused with the giant burrowing frog ("Heleioporus australiacus"), because both are large and warty in appearance; however, the latter can be readily distinguished from the former by its vertical pupils and its silver-grey (as opposed to gold) irises. Juvenile cane toads may be confused with species of the genus "Uperoleia", but their adult colleagues can be distinguished by the lack of bright colouring on the groin and thighs.

In the United States, the cane toad closely resembles many bufonid species. In particular, it could be confused with the southern toad ("Bufo terrestris"), which can be distinguished by the presence of two bulbs in front of the parotoid glands.

The cane toad genome has been sequenced and certain Australian academics believe this will help in understanding how the toad can quickly evolve to adapt to new environments, the workings of its infamous toxin, and hopefully provide new options for halting this species' march across Australia and other places it has spread as an invasive pest.

The cane toad is very large; the females are significantly longer than males, reaching a typical length of , with a maximum of . Larger toads tend to be found in areas of lower population density. They have a life expectancy of 10 to 15Â years in the wild, and can live considerably longer in captivity, with one specimen reportedly surviving for 35Â years.

The skin of the cane toad is dry and warty. It has distinct ridges above the eyes, which run down the snout. Individual cane toads can be grey, yellowish, red-brown, or olive-brown, with varying patterns. A large parotoid gland lies behind each eye. The ventral surface is cream-coloured and may have blotches in shades of black or brown. The pupils are horizontal and the irises golden. The toes have a fleshy webbing at their base, and the fingers are free of webbing.

Typically, juvenile cane toads have smooth, dark skin, although some specimens have a red wash. Juveniles lack the adults' large parotoid glands, so they are usually less poisonous. The tadpoles are small and uniformly black, and are bottom-dwellers, tending to form schools. Tadpoles range from in length.

The common name "marine toad" and the scientific name "Rhinella marina" suggest a link to marine life, but cane toads do not live in the sea. However, laboratory experiments suggest that tadpoles can tolerate salt concentrations equivalent to 15% of seawater (~5.4â°), and recent field observations found living tadpoles and toadlets at salinities of 27.5â° on Coiba Island, Panama. The cane toad inhabits open grassland and woodland, and has displayed a "distinct preference" for areas modified by humans, such as gardens and drainage ditches. In their native habitats, the toads can be found in subtropical forests, although dense foliage tends to limit their dispersal.

The cane toad begins life as an egg, which is laid as part of long strings of jelly in water. A female lays 8,000â25,000 eggs at once and the strings can stretch up to in length. The black eggs are covered by a membrane and their diameter is about . The rate at which an egg grows into a tadpole increases with temperature. Tadpoles typically hatch within 48Â hours, but the period can vary from 14Â hours to almost a week. This process usually involves thousands of tadpolesâwhich are small, black, and have short tailsâforming into groups. Between 12 and 60Â days are needed for the tadpoles to develop into juveniles, with four weeks being typical. Similarly to their adult counterparts, eggs and tadpoles are toxic to many animals.

When they emerge, toadlets typically are about in length, and grow rapidly. While the rate of growth varies by region, time of year, and gender, an average initial growth rate of per day is seen, followed by an average rate of per day. Growth typically slows once the toads reach sexual maturity. This rapid growth is important for their survival; in the period between metamorphosis and subadulthood, the young toads lose the toxicity that protected them as eggs and tadpoles, but have yet to fully develop the parotoid glands that produce bufotoxin. Because they lack this key defence, only an estimated 0.5% of cane toads reach adulthood.

As with rates of growth, the point at which the toads become sexually mature varies across different regions. In New Guinea, sexual maturity is reached by female toads with a snoutâvent length between , while toads in Panama achieve maturity when they are between in length. In tropical regions, such as their native habitats, breeding occurs throughout the year, but in subtropical areas, breeding occurs only during warmer periods that coincide with the onset of the wet season.

The cane toad is estimated to have a critical thermal maximum of and a minimum of around . The ranges can change due to adaptation to the local environment. The cane toad has a high tolerance to water loss; some can withstand a 52.6% loss of body water, allowing them to survive outside tropical environments.

Most frogs identify prey by movement, and vision appears to be the primary method by which the cane toad detects prey; however, it can also locate food using its sense of smell. They eat a wide range of material; in addition to the normal prey of small rodents, reptiles, other amphibians, birds, and even bats and a range of invertebrates, they also eat plants, dog food, and household refuse.

The skin of the adult cane toad is toxic, as well as the enlarged parotoid glands behind the eyes, and other glands across its back. When the toad is threatened, its glands secrete a milky-white fluid known as bufotoxin. Components of bufotoxin are toxic to many animals; even human deaths have been recorded due to the consumption of cane toads. Dogs are especially prone to be poisoned by licking or biting toads. Pets showing excessive drooling, extremely red gums, head-shaking, crying, loss of coordination, and/or convulsions require immediate veterinary attention.

Bufotenin, one of the chemicals excreted by the cane toad, is classified as a schedule 9 drug under Australian law, alongside heroin and LSD. The effects of bufotenin are thought to be similar to those of mild poisoning; the stimulation, which includes mild hallucinations, lasts less than an hour. As the cane toad excretes bufotenin in small amounts, and other toxins in relatively large quantities, toad licking could result in serious illness or death.

In addition to releasing toxin, the cane toad is capable of inflating its lungs, puffing up, and lifting its body off the ground to appear taller and larger to a potential predator.

Since 2011, experimenters in the Kimberley region of Western Australia have used poisonous sausages containing toad meat to try to protect native animals from cane toads' deadly impact. The Western Australian Department of Environment and Conservation, along with the University of Sydney, developed baits to train native animals not to eat the toads. By blending bits of toad with a nausea-inducing chemical, the baits train the animals to stay away from the amphibians.

Many species prey on the cane toad and its tadpoles in its native habitat, including the broad-snouted caiman ("Caiman latirostris"), the banded cat-eyed snake ("Leptodeira annulata"), eels (family Anguillidae), various species of killifish, the rock flagtail ("Kuhlia rupestris"), some species of catfish (order Siluriformes), some species of ibis (subfamily Threskiornithinae), and "Paraponera clavata" (bullet ants).

Predators outside the cane toad's native range include the whistling kite ("Haliastur sphenurus"), the rakali ("Hydromys chrysogaster"), the black rat ("Rattus rattus") and the water monitor ("Varanus salvator"). The tawny frogmouth ("Podargus strigoides") and the Papuan frogmouth ("Podargus papuensis") have been reported as feeding on cane toads; some Australian crows ("Corvus" spp.) have also learned strategies allowing them to feed on cane toads, such as using their beak to flip toads onto their backs. Rakalis have been observed eating the hearts and livers of the toads, where the toads have moved into their territory.

Opossums of the genus "Didelphis" likely can eat cane toads with impunity. Meat ants are unaffected by the cane toads' toxins, so are able to kill them. The cane toad's normal response to attack is to stand still and let its toxin kill the attacker, which allows the ants to attack and eat the toad.

The cane toad is native to the Americas, and its range stretches from the Rio Grande Valley in South Texas to the central Amazon and southeastern Peru, and some of the continental islands near Venezuela (such as Trinidad and Tobago). This area encompasses both tropical and semiarid environments. The density of the cane toad is significantly lower within its native distribution than in places where it has been introduced. In South America, the density was recorded to be 20 adults per 100Â m (109Â yd) of shoreline, 1 to 2% of the density in Australia.

The cane toad has been introduced to many regions of the worldâparticularly the Pacificâfor the biological control of agricultural pests. These introductions have generally been well documented, and the cane toad may be one of the most studied of any introduced species.

Before the early 1840s, the cane toad had been introduced into Martinique and Barbados, from French Guiana and Guyana. An introduction to Jamaica was made in 1844 in an attempt to reduce the rat population. Despite its failure to control the rodents, the cane toad was introduced to Puerto Rico in the early 20th century in the hope that it would counter a beetle infestation ravaging the sugarcane plantations. The Puerto Rican scheme was successful and halted the economic damage caused by the beetles, prompting scientists in the 1930s to promote it as an ideal solution to agricultural pests.

As a result, many countries in the Pacific region emulated the lead of Puerto Rico and introduced the toad in the 1930s. Introduced populations are in Australia, Florida, Papua New Guinea, the Philippines, the Ogasawara, Ishigaki Island and the DaitÅ Islands of Japan, most Caribbean islands, Fiji and many other Pacific islands, including Hawaii. Since then, the cane toad has become a pest in many host countries, and poses a serious threat to native animals.

Following the apparent success of the cane toad in eating the beetles threatening the sugarcane plantations of Puerto Rico, and the fruitful introductions into Hawaii and the Philippines, a strong push was made for the cane toad to be released in Australia to negate the pests ravaging the Queensland cane fields. As a result, 102 toads were collected from Hawaii and brought to Australia. Queensland's sugar scientists released the toad into cane fields in August 1935. After this initial release, the Commonwealth Department of Health decided to ban future introductions until a study was conducted into the feeding habits of the toad. The study was completed in 1936 and the ban lifted, when large-scale releases were undertaken; by March 1937, 62,000 toadlets had been released into the wild. The toads became firmly established in Queensland, increasing exponentially in number and extending their range into the Northern Territory and New South Wales. In 2010, one was found on the far western coast in Broome, Western Australia.

However, the toad was generally unsuccessful in reducing the targeted grey-backed cane beetles ("Dermolepida albohirtum"), in part because the cane fields provided insufficient shelter for the predators during the day, and in part because the beetles live at the tops of sugar caneâand cane toads are not good climbers. Since its original introduction, the cane toad has had a particularly marked effect on Australian biodiversity. The population of a number of native predatory reptiles has declined, such as the varanid lizards "Varanus mertensi", "V. mitchelli", and "V. panoptes", the land snakes "Pseudechis australis" and "Acanthophis antarcticus", and the crocodile species "Crocodylus johnstoni"; in contrast, the population of the agamid lizard "Amphibolurus gilberti"âknown to be a prey item of "V. panoptes"âhas increased.

The cane toad was introduced to various Caribbean islands to counter a number of pests infesting local crops. While it was able to establish itself on some islands, such as Barbados, Jamaica, and Puerto Rico, other introductions, such as in Cuba before 1900 and in 1946, and on the islands of Dominica and Grand Cayman, were unsuccessful.

The earliest recorded introductions were to Barbados and Martinique. The Barbados introductions were focused on the biological control of pests damaging the sugarcane crops, and while the toads became abundant, they have done even less to control the pests than in Australia. The toad was introduced to Martinique from French Guiana before 1944 and became established. Today, they reduce the mosquito and mole cricket populations. A third introduction to the region occurred in 1884, when toads appeared in Jamaica, reportedly imported from Barbados to help control the rodent population. While they had no significant effect on the rats, they nevertheless became well established. Other introductions include the release on Antiguaâpossibly before 1916, although this initial population may have died out by 1934 and been reintroduced at a later dateâ and Montserrat, which had an introduction before 1879 that led to the establishment of a solid population, which was apparently sufficient to survive the SoufriÃ¨re Hills volcano eruption in 1995.

In 1920, the cane toad was introduced into Puerto Rico to control the populations of white grub ("Phyllophaga" spp.), a sugarcane pest. Before this, the pests were manually collected by humans, so the introduction of the toad eliminated labor costs. A second group of toads was imported in 1923, and by 1932, the cane toad was well established. The population of white grubs dramatically decreased, and this was attributed to the cane toad at the annual meeting of the International Sugar Cane Technologists in Puerto Rico. However, there may have been other factors. The six-year period after 1931âwhen the cane toad was most prolific, and the white grub had a dramatic declineâhad the highest-ever rainfall for Puerto Rico. Nevertheless, the cane toad was assumed to have controlled the white grub; this view was reinforced by a "Nature" article titled "Toads save sugar crop", and this led to large-scale introductions throughout many parts of the Pacific.

The cane toad has been spotted in Carriacou and Dominica, the latter appearance occurring in spite of the failure of the earlier introductions. On September 8, 2013, the cane toad was also discovered on the island of New Providence in the Bahamas.

The cane toad was first introduced deliberately into the Philippines in 1930 as a biological control agent of pests in sugarcane plantations, after the success of the experimental introductions into Puerto Rico. It subsequently became the most ubiquitous amphibian in the islands. It still retains the common name of "bakÃ®" or "kamprag" in the Visayan languages, a corruption of 'American frog', referring to its origins. It is also commonly known as "bullfrog" in Philippine English.
The cane toad was introduced into Fiji to combat insects that infested sugarcane plantations. The introduction of the cane toad to the region was first suggested in 1933, following the successes in Puerto Rico and Hawaii. After considering the possible side effects, the national government of Fiji decided to release the toad in 1953, and 67 specimens were subsequently imported from Hawaii. Once the toads were established, a 1963 study concluded, as the toad's diet included both harmful and beneficial invertebrates, it was considered "economically neutral". Today, the cane toad can be found on all major islands in Fiji, although they tend to be smaller than their counterparts in other regions.

The cane toad was introduced into New Guinea to control the hawk moth larvae eating sweet potato crops. The first release occurred in 1937 using toads imported from Hawaii, with a second release the same year using specimens from the Australian mainland. Evidence suggests a third release in 1938, consisting of toads being used for human pregnancy testsâmany species of toad were found to be effective for this task, and were employed for about 20Â years after the discovery was announced in 1948. Initial reports argued the toads were effective in reducing the levels of cutworms and sweet potato yields were thought to be improving. As a result, these first releases were followed by further distributions across much of the region, although their effectiveness on other crops, such as cabbages, has been questioned; when the toads were released at Wau, the cabbages provided insufficient shelter and the toads rapidly left the immediate area for the superior shelter offered by the forest. A similar situation had previously arisen in the Australian cane fields, but this experience was either unknown or ignored in New Guinea. The cane toad has since become abundant in rural and urban areas.

The cane toad naturally exists in South Texas, but attempts (both deliberate and accidental) have been made to introduce the species to other parts of the country. These include introductions to Florida and to the islands of Hawaii, as well as largely unsuccessful introductions to Louisiana.

Initial releases into Florida failed. Attempted introductions before 1936 and 1944, intended to control sugarcane pests, were unsuccessful as the toads failed to proliferate. Later attempts failed in the same way. However, the toad gained a foothold in the state after an accidental release by an importer at Miami International Airport in 1957, and deliberate releases by animal dealers in 1963 and 1964 established the toad in other parts of Florida. Today, the cane toad is well established in the state, from the Keys to north of Tampa, and they are gradually extending further northward. In Florida, the toad is a regarded as a threat to native species and pets; so much so, the Florida Fish and Wildlife Conservation Commission recommends residents to kill them.

Around 150 cane toads were introduced to Oahu in Hawaii in 1932, and the population swelled to 105,517 after 17Â months. The toads were sent to the other islands, and more than 100,000 toads were distributed by July 1934; eventually over 600,000 were transported.

Other than the use as a biological control for pests, the cane toad has been employed in a number of commercial and noncommercial applications. Traditionally, within the toad's natural range in South America, the Embera-Wounaan would "milk" the toads for their toxin, which was then employed as an arrow poison. The toxins may have been used as an entheogen by the Olmec people. The toad has been hunted as a food source in parts of Peru, and eaten after the careful removal of the skin and parotoid glands. When properly prepared, the meat of the toad is considered healthy and as a source of omega-3 fatty acids. More recently, the toad's toxins have been used in a number of new ways: bufotenin has been used in Japan as an aphrodisiac and a hair restorer, and in cardiac surgery in China to lower the heart rates of patients. New research has suggested that the cane toad's poison may have some applications in treating prostate cancer.

Other modern applications of the cane toad include pregnancy testing, as pets, laboratory research, and the production of leather goods. Pregnancy testing was conducted in the mid-20th century by injecting urine from a woman into a male toad's lymph sacs, and if spermatozoa appeared in the toad's urine, the patient was deemed to be pregnant. The tests using toads were faster than those employing mammals; the toads were easier to raise, and, although the initial 1948 discovery employed "Bufo arenarum" for the tests, it soon became clear that a variety of anuran species were suitable, including the cane toad. As a result, toads were employed in this task for around 20Â years. As a laboratory animal, the cane toad is regarded as ideal; they are plentiful, and easy and inexpensive to maintain and handle. The use of the cane toad in experiments started in the 1950s, and by the end of the 1960s, large numbers were being collected and exported to high schools and universities. Since then, a number of Australian states have introduced or tightened importation regulations. Even dead toads have value. Cane toad skin has been made into leather and novelty items; stuffed cane toads, posed and accessorised, have found a home in the tourist market, and attempts have been made to produce fertiliser from their bodies.

Cane toads pose a serious threat to native species when introduced to a new ecosystem. Classified as an invasive species in over 20 countries, multiple reports exist of the cane toad moving into a new area to be followed by a decline in the biodiversity in that region. The most documented region of the cane toad's invasion and subsequent effect on native species is Australia, where multiple surveys and observations of the toad's conquest have been completed. The best way to illustrate this effect is through the plight of the northern quoll, as well as Mertens' water monitor, a large lizard native to South and Southeast Asia.

Two sites were chosen to study the effects of cane toads on the northern quoll, one of which was at Mary River ranger station, which is located in the southern region of Kakadu National Park. The other site was located at the north end of the park. In addition to these two sites, a third site was located at the East Alligator ranger station, and this site was used as a control site, where the cane toads would not interact with the northern quoll population. Monitoring of the quoll population began at the Mary River ranger station using radio tracking in 2002, months before the first cane toads arrived at the site. After the arrival of the cane toads, the population of northern quolls in the Mary River site plummeted between October and December 2002, and by March 2003, the northern quoll appeared to be extinct in this section of the park, as no northern quolls were caught in the trapping trips in the following two months. In contrast, the population of northern quolls in the control site at the East Alligator ranger station remained relatively constant, not showing any symptoms of declining. The evidence from the Kakadu National Park is compelling not only because of the timing of the population of northern quolls plummeting just months after the arrival of the cane toad, but also because in the Mary River region 31% of mortalities within the quoll population were attributed to lethal toxic ingestion, as no signs of disease, parasite infestation, or any other obvious changes at the site were found that could have caused such a rapid decline. The most obvious evidence that supports the hypothesis that the invasion of the cane toads caused the local extinction of the northern quoll is that the closely monitored population of the control group, in the absence of cane toads, showed no signs of decline.

In the case of Mertens' water monitor, only one region was monitored, but over the course of 18 months. This region is located 70Â km south of Darwin, at the Manton Dam Recreation Area. Within the Manton Dam Recreation Area, 14 sites were set up to survey the population of water monitors, measuring abundance and site occupancy at each one. Seven surveys were conducted, each of which ran for 4 weeks and included 16 site visits, where each site was sampled twice per day for 2 consecutive days throughout the 4 weeks. Each site visit occurred between 7:30 and 10:30 am, and 4:00â7:00 pm, when "Varanus mertensi" can be viewed sunbathing on the shore or wrapped around a tree branch close to shore. The whole project lasted from December 2004 to May 2006, and had a total of 194 sightings of "Varanus mertensi" in 1568 site visits. Of the seven surveys, abundance was highest during the second survey, which took place in February 2005, 2 months into the project. Following this measurement, the abundance declined in the next four surveys, before declining sharply after the second to last survey in February 2006. In the final survey taken in May 2006, only two "V. mertensi" lizards were observed. Cane toads were first recorded in the region of study during the second survey during February 2005, also when the water monitor abundance was at its highest over the course of the study. Numbers of the cane toad population stayed low for the next year after introduction, and then skyrocketed to its peak in the last survey during May 2006. When compared, the two populations side by side clearly show that the onset of the cane toads had an immediate negative impact on the monitors, as their population began to drop in February 2005, which was when the first cane toads entered the Manton Dam Recreation Area. At the end of the study, some scattered populations of water monitors remained in the upper sites of the Manton Dam, which suggests that local extinctions occurred at certain shoreline sites within Manton Dam, but a complete extinction of the population did not occur.

Notes
Bibliography



</doc>
<doc id="6643" url="https://en.wikipedia.org/wiki?curid=6643" title="Croquet">
Croquet

Croquet (; (UK) or (US)) is a sport that involves hitting wooden or plastic balls with a mallet through hoops (often called "wickets" in the United States) embedded in a grass playing court.

The oldest document to bear the word "croquet" with a description of the modern game is the set of rules registered by Isaac Spratt in November 1856 with the Stationers' Company in London. This record is now in the Public Record Office. In 1868, the first croquet all-comers meet was held at Moreton-in-Marsh, Gloucestershire and in the same year the All England Croquet Club was formed at Wimbledon, London.

Regardless when and by what route it reached England and the British colonies in its recognizable form, croquet is, like golf, pall-mall, trucco, and kolven, among the later forms of ground billiards, which as a class have been popular in Western Europe back to at least the Late Middle Ages, with roots in classical antiquity, including sometimes the use of arches and pegs along with balls and mallets or other striking sticks (some more akin to modern field hockey sticks). By the 12th century, a team ball game called ' or ', akin to a chaotic version of hockey or football (depending on whether sticks were used), was regularly played in France and southern Britain between villages or parishes; it was attested in Cornwall as early as 1283.

In the book "Queen of Games: The History of Croquet", Nicky Smith presents two theories of the origin of the modern game of croquet, which took England by storm in the 1860s and then spread overseas.

The first explanation is that the ancestral game was introduced to Britain from France during the 1660â1685 reign of Charles II of England, Scotland and Ireland, and was played under the name of ' (among other spellings, today usually "pall-mall"), derived ultimately from Latin words for 'ball and mallet' (the latter also found in the name of the earlier French game, '). This was the explanation given in the ninth edition of "EncyclopÃ¦dia Britannica", dated 1877.

In his 1810 book "The Sports and Pastimes of the People of England", Joseph Strutt described the way pall-mall was played in England at the time:"Pale-maille is a game wherein a round box[wood] ball is struck with a mallet through a high arch of iron, which he that can do at the fewest blows, or at the number agreed upon, wins. It is to be observed, that there are two of these arches, that is one at either end of the alley. The game of mall was a fashionable amusement in the reign of Charles the Second, and the walk in Saint James's Park, now called the Mall, received its name from having been appropriated to the purpose of playing at mall, where Charles himself and his courtiers frequently exercised themselves in the practice of this pastime."

While the name "pall-mall" and various games bearing this name also appeared elsewhere (France and Italy), the description above suggests that the croquet-like games in particular were popular in England by the early 17th century. Some other early modern sources refer to pall-mall being played over a large distance (as in golf); however, an image in Strutt's 1801 book shows a croquet-like ground billiards game (balls on ground, hoop, bats, and peg) being played over a , garden-sized distance. The image's caption describes the game as "a curious ancient pastime", confirming that croquet games were not new in early-19th-century England.
In Samuel Johnson's 1755 dictionary, his definition of "pall-mall" clearly describes a game with similarities to modern croquet: "A play in which the ball is struck with a mallet through an iron ring". However, there is no evidence that pall-mall involved the croquet stroke which is the distinguishing characteristic of the modern game.

The second theory is that the rules of the modern game of croquet arrived from Ireland during the 1850s, perhaps after being brought there from Brittany, where a similar game was played on the beaches. Regular contact between Ireland and France had continued since the Norman invasion of Ireland in 1169. By no later than the early 15th century, the game " (itself ancestral to pall-mall and perhaps to indoor billiards) was popular in France, including in the courts of Henry II in the 16th century and Louis XIV of the 17th.

At least one version of it, " ('wheel') was a multi-ball lawn game. Records show a game called "crookey", similar to croquet, being played at Castlebellingham in County Louth, Ireland, in 1834, which was introduced to Galway in 1835 and played on the bishop's palace garden, and in the same year to the genteel Dublin suburb of Kingstown (today DÃºn Laoghaire) where it was first spelt as "croquet". There is, however, no pre-1858 Irish document that describes the way game was played, in particular there is no reference to the distinctive croquet stroke, which is described below under "Variations: Association". The noted croquet historian Dr Prior, in his book of 1872, makes the categoric statement "One thing only is certain: it is from Ireland that croquet came to England and it was on the lawn of the late Lord Lonsdale that it was first played in this country." This was about 1851.

John Jaques apparently claimed in a letter to Arthur Lillie in 1873 that he had himself seen the game played in Ireland, writing "I made the implements and published directions (such as they were) before Mr. Spratt [mentioned above] introduced the subject to me." Whatever the truth of the matter, Jaques certainly played an important role in popularising the game, producing editions of the rules in 1857, 1860, and 1864.

Croquet became highly popular as a social pastime in England during the 1860s. It was enthusiastically adopted and promoted by the Earl of Essex who held lavish croquet parties at Cassiobury House, his stately home in Watford, Hertfordshire, and the Earl even launched his own "Cassiobury" brand croquet set. By 1867, Jaques had printed 65,000 copies of his "Laws and Regulations" of the game. It quickly spread to other Anglophone countries, including Australia, Canada, New Zealand, South Africa, and the United States. No doubt one of the attractions was that the game could be played by both sexes; this also ensured a certain amount of adverse comment.
By the late 1870s, however, croquet had been eclipsed by another fashionable game, lawn tennis, and many of the newly created croquet clubs, including the All England Club at Wimbledon, converted some or all of their lawns into tennis courts.

There was a revival in the 1890s, but from then onwards, croquet was always a minority sport, with national individual participation amounting to a few thousand players. The All England Lawn Tennis and Croquet Club still has a croquet lawn, but has not hosted any significant tournaments. The English headquarters for the game is now in Cheltenham.
The earliest known reference to croquet in Scotland is the booklet "The Game of Croquet, its Laws and Regulations" which was published in the mid-1860s for the proprietor of Eglinton Castle, the Earl of Eglinton. On the page facing the title page is a picture of Eglinton Castle with a game of "croquet" in full swing.

The croquet lawn existed on the northern terrace, between Eglinton Castle and the Lugton Water. The 13th Earl developed a variation on croquet named Captain Moreton's Eglinton Castle croquet, which had small bells on the eight hoops "to ring the changes", two pegs, a double hoop with a bell and two tunnels for the ball to pass through. In 1865 the 'Rules of the Eglinton Castle and Cassiobury Croquet' was published by Edmund Routledge. Several incomplete sets of this form of croquet are known to exist, and one complete set is still used for demonstration games in the West of Scotland.

There are several variations of croquet currently played, differing in the scoring systems, order of shots, and layout (particularly in social games where play must be adapted to smaller-than-standard playing courts). Two forms of the game, association croquet and golf croquet, have rules that are agreed internationally and are played in many countries around the world. The United States has its own set of rules for domestic games. Gateball, a sport originated in Japan under the influence of croquet, is played mainly in East and Southeast Asia and the Americas, and can also be regarded as a croquet variant.

As well as club-level games, there are regular world championships and international matches between croquet-playing countries. The sport has particularly strong followings in the UK, US, New Zealand and Australia; every four years, these countries play the MacRobertson Shield tournament. Many other countries also play. The current world rankings show England in top place for association croquet, followed by Australia and New Zealand sharing second place, with the United States in fourth position; the same four countries appear in the top six of the golf croquet league table, below Egypt in top position, and with South Africa at number five.

Croquet is popularly believed to be viciously competitive. This may derive from the fact that (unlike in golf) players will often attempt to move their opponents' balls to unfavourable positions. However, purely negative play is rarely a winning strategy: successful players (in all versions other than golf croquet) will use all four balls to set up a break for themselves, rather than simply making the game as difficult as possible for their opponents. At championship-standard association croquet, players can often make all 26 points (13 for each ball) in two turns.

Croquet was an event at the 1900 Summer Olympics. Roque, an American variation on croquet, was an event at the 1904 Summer Olympics.

Association croquet is the name of an advanced game of croquet, played at all levels up to international level. It involves four balls teamed in pairs, with both balls going through every hoop for one pair to win. The game's distinguishing feature is the "croquet" shot: when certain balls hit other balls, extra shots are allowed. The six hoops are arranged three at each end of the court, with a centre peg.

One side takes the black and blue balls, the other takes red and yellow. At each turn, players can choose to play with either of their balls for that turn. At the start of a turn, the player plays a stroke. If the player either hits the ball through the correct hoop ("runs" the hoop), or hits another ball (a "roquet"), the turn continues.

Following a roquet, the player picks up his or her own ball and puts it down next to the ball that it hit. The next shot is played with the two balls touching: this is the "croquet stroke" from which the game takes its name. By varying the speed and angle at which the mallet hits the striker's ball, a good player can control the final position of both balls: the horizontal angle determines how far the balls diverge in direction, while the vertical angle and the amount of follow-through determine the relative distance that the two balls travel.

After the croquet stroke, the player plays a "continuation" stroke, during which the player may again attempt to make a roquet or run a hoop. Each of the other three balls may be roqueted once in a turn before a hoop is run, after which they become available to be roqueted again.

The winner of the game is the team who completes the set circuit of six hoops (and then back again the other way), with both balls, and then strikes the centre peg (making a total of 13 points per ball = 26).

Good players may make "s" or "s" of several hoops in a single turn. The best players may take a ball round a full circuit in one turn. "Advanced play" (a variant of association play for expert players) gives penalties to a player who runs certain hoops in a turn, to allow the opponent a chance of getting back into the game; feats of skill such as triple peels or better, in which the partner ball (or occasionally an opponent ball) is caused to run a number of hoops in a turn by the striker's ball, help avoid these penalties.

A handicap system ("bisques") provides less experienced players a chance of winning against more formidable opponents. Players of all ages and both sexes compete on level terms.

The World Championships are organised by the World Croquet Federation (WCF) and usually take place every two or three years. The 2018 championships took place in Wellington, New Zealand; the winner was Paddy Chapman of New Zealand. The current Women's Association Croquet World Champion (2015) is Miranda Chapman of England. Paddy and Miranda are married.

The Australian team won the last MacRobertson International Croquet Shield tournament, which is the major international test tour trophy in association croquet. It is contested every three to four years between Australia, Britain, the United States and New Zealand. Historically the British have been the dominant force, winning 14 out of the 22 times that the event has been held. In individual competition, the UK is often divided by subnational country (England, Scotland and Wales), while Northern Ireland joins with the republic in an All Ireland association (as it does in several other sports).

The world's top 10 association croquet players as of February 2018 were Robert Fletcher (Australia), Reg Bamford (South Africa), Robert Fulford (England), Paddy Chapman (New Zealand), Ben Rothman (USA), Malcolm Fletcher (Australia), Jamie Burch (England), Jose Riva (Spain), Stephen Mulliner (England), Greg Bryant (New Zealand).

Unlike most sports, men and women compete and are ranked together. Three women have won the British Open Championship: Lily Gower in 1905, Dorothy Steel in 1925, 1933, 1935 and 1936, and Hope Rotherham in 1960. While male players are in the majority at club level in the UK, the opposite is the case in Australia and New Zealand.

The governing body in England is The Croquet Association, which has been the driving force of the development of the game. The rules and tournament regulations are now maintained by the International Laws Committee, established by the croquet associations of England and Wales (CA), Australia (ACA), New Zealand (CNZ) and the United States (USCA).

In golf croquet, a hoop is won by the first ball to go through each hoop. Unlike association croquet, there are no additional turns for hitting other balls.

Each player takes a stroke in turn, each trying to hit a ball through the same hoop. The sequence of play is blue, red, black, yellow. Blue and black balls play against red and yellow. When a hoop is won, the sequence of play continues as before. The winner of the game is the player/team who wins the most hoops.

Golf croquet is the fastest-growing version of the game, owing largely to its simplicity and competitiveness. There is an especially large interest with competitive success by players in Egypt. Golf croquet is easier to learn and play, but requires strategic skills and accurate play. In comparison with association croquet, play is faster and balls are more likely to be lifted off the ground.

In April 2013, Reg Bamford of South Africa beat Ahmed Nasr of Egypt in the final of the Golf Croquet World Championship in Cairo, becoming the first person to simultaneously hold the title in both association croquet and golf croquet. As of 2017, the Golf Croquet World Champion was Reg Bamford (South Africa) and the Women's Golf Croquet World Champion was Judith Hanekom (South Africa).

In 2018, two international championships open to both sexes were won by women: in May, Rachel Gee of England beat Pierre Beaudry to win the European Golf Croquet championship, and in October, Hanan Rashad of Egypt beat Yasser Fathy (also from Egypt) to win the World over-50s Golf Croquet championship.

Garden croquet is widely played in the UK. The rules are easy to learn and the game can be played on lawns of almost any size but usually around by . The rules are similar to those described above for Association Croquet with three major differences:


This version of the game is easy for beginners to learn. The main Garden Croquet Club in the UK is the Bygrave Croquet Club which is a private club with five lawns. Other clubs also use garden croquet as an introduction to the game, notably the Hampstead Heath Croquet Club and the Watford Croquet Club.

The American-rules version of croquet, another six-hoop game, is the dominant version of the game in the United States and is also widely played in Canada. It is governed by the United States Croquet Association. Its genesis is mostly in association croquet, but it differs in a number of important ways that reflect the home-grown traditions of American "backyard" croquet.

Two of the most notable differences are that the balls are always played in the same sequence (blue, red, black, yellow) throughout the game, and that a ball's "deadness" on other balls is carried over from turn to turn until the ball has been "cleared" by scoring its next hoop. A Deadness Board is used to keep track of deadness on all four balls. Tactics are simplified on the one hand by the strict sequence of play, and complicated on the other hand by the continuation of deadness. A further difference is the more restrictive boundary-line rules of American croquet.

In the American game, roqueting a ball out of bounds or running a hoop out of bounds causes the turn to end, and balls that go out of bounds are replaced only from the boundary rather than as in association croquet. "Attacking" balls on the boundary line to bring them into play is thus far more challenging.

Nine-wicket croquet, sometimes called "backyard croquet", is played mainly in Canada and the United States, and is the game most recreational players in those countries call simply "croquet". In this version of croquet there are nine wickets, two stakes, and up to six balls. The course is arranged in a double-diamond pattern, with one stake at each end of the course. Players start at one stake, navigate one side of the double diamond, hit the turning stake, then navigate the opposite side of the double diamond and hit the starting stake to end. If playing individually ("Cutthroat"), the first player to stake out is the winner. In partnership play, all members of a team must stake out, and a player might choose to avoid staking out (becoming a "Rover") in order to help a lagging teammate.

Each time a ball is roqueted, the striker gets two bonus shots.
For the first bonus shot, the player has four options:
The second bonus shot ("continuation shot") is an ordinary shot played from where the striker ball came to rest.

An alternate endgame is "poison": in this variant, a player who has scored the last wicket but not hit the starting stake becomes a "poison ball", which may eliminate other balls from the game by roqueting them. A non-poison ball that roquets a poison ball has the normal options. A poison ball that hits a stake or passes through any wicket (possibly by the action of a non-poison player) is eliminated. The last person remaining is the winner.

This version of the game was invented by John Riches of Adelaide, Australia with help from Tom Armstrong in the 1980s. The game can be played by up to six people and is very easy to learn. For this reason it is often used as a stepping stone to association croquet.

Ricochet has similar rules to association and garden croquet, except that when a ball is roqueted, the striker's ball remains live and two free shots are earned. This enables strikers to play their ball near to another opponent ball and ricochet that too thus earning two more free shots. Running a hoop earns one free shot.

One-ball croquet has become popular in recent years as a way of bringing AC (association) and GC (golf) players together. The rules are essentially
those of association croquet, except that each player/team has only one ball rather than two. This makes it very hard to create a break, which leads to more interactive play.


The way croquet is depicted in paintings and books says much about popular perceptions of the game, though little about the reality of modern play.

About 200 croquet clubs across the United States are members of the United States Croquet Association.

Many colleges have croquet clubs as well, such as The University of Virginia, The University of Chicago, Pennsylvania State University, Bates College, SUNY New Paltz, Harvard University, and Dartmouth College. Notably, St. John's College and the US Naval Academy engage in a yearly match in Annapolis, Maryland. Both schools also compete at the collegiate level and the rivalry continues to be an Annapolis tradition, attracting thousands of spectators each April.

In England and Wales, there are around 170 clubs affiliated with the Croquet Association. The All England Lawn Tennis and Croquet Club at Wimbledon is famous for its lawn tennis tournament, but retains an active croquet section. There are also clubs in many universities and colleges, with an annual Varsity match being played between Oxford and Cambridge. With over 1800 participants, the 2011 Oxford University "Cuppers" (inter-college) tournament claimed to be not only the largest croquet tournament ever, but the largest sporting event in the university's history.



</doc>
<doc id="6644" url="https://en.wikipedia.org/wiki?curid=6644" title="Curling">
Curling

Curling is a sport in which players slide stones on a sheet of ice toward a target area which is segmented into four concentric circles. It is related to bowls, boules and shuffleboard. Two teams, each with four players, take turns sliding heavy, polished granite stones, also called "rocks", across the ice "curling sheet" toward the "house", a circular target marked on the ice. Each team has eight stones, with each player throwing two. The purpose is to accumulate the highest score for a "game"; points are scored for the stones resting closest to the centre of the house at the conclusion of each "end", which is completed when both teams have thrown all of their stones. A game usually consists of eight or ten ends.

The player can induce a curved path, described as "curl", by causing the stone to slowly turn as it slides. The path of the rock may be further influenced by two sweepers with brooms or brushes, who accompany it as it slides down the sheet and sweep the ice in front of the stone. "Sweeping a rock" decreases the friction, which makes the stone travel a straighter path (with less "curl") and a longer distance. A great deal of strategy and teamwork go into choosing the ideal path and placement of a stone for each situation, and the skills of the curlers determine the degree to which the stone will achieve the desired result. This gives curling its nickname of "chess on ice".

Evidence that curling existed in Scotland in the early 16th century includes a curling stone inscribed with the date 1511 found (along with another bearing the date 1551) when an old pond was drained at Dunblane, Scotland. The world's oldest curling stone and the world's oldest football are now kept in the same museum (the Stirling Smith Art Gallery and Museum) in Stirling. The first written reference to a contest using stones on ice coming from the records of Paisley Abbey, Renfrewshire, in February 1541. Two paintings, "" and "The Hunters in the Snow" (both dated 1565) by Pieter Bruegel the Elder depict Flemish peasants curling, albeit without brooms; Scotland and the Low Countries had strong trading and cultural links during this period, which is also evident in the history of golf.

The word "curling" first appears in print in 1620 in Perth, Scotland, in the preface and the verses of a poem by Henry Adamson. The sport was (and still is, in Scotland and Scottish-settled regions like southern New Zealand) also known as "the roaring game" because of the sound the stones make while traveling over the "pebble" (droplets of water applied to the playing surface). The verbal noun "curling" is formed from the Scots (and English) verb "curl", which describes the motion of the stone.

Kilsyth Curling Club claims to be the first club in the world, having been formally constituted in 1716; it is still in existence today. Kilsyth also claims the oldest purpose-built curling pond in the world at Colzium, in the form of a low dam creating a shallow pool some in size. The International Olympic Committee recognises the Royal Caledonian Curling Club (founded as the Grand Caledonian Curling Club in 1838) as developing the first official rules for the sport.

In the early history of curling, the playing stones were simply flat-bottomed stones from rivers or fields, which lacked a handle and were of inconsistent size, shape and smoothness. Some early stones had holes for a finger and the thumb, akin to ten-pin bowling balls. Unlike today, the thrower had little control over the 'curl' or velocity and relied more on luck than on precision, skill and strategy. The sport was often played on frozen rivers although purpose-built ponds were later created in many Scottish towns. For example, the Scottish poet David Gray describes whisky-drinking curlers on the Luggie Water at Kirkintilloch.

In Darvel, East Ayrshire, the weavers relaxed by playing curling matches using the heavy stone weights from the looms' "warp beams", fitted with a detachable handle for the purpose. Many a wife would keep her husband's brass curling stone handle on the mantelpiece, brightly polished until the next time it was needed. Central Canadian curlers often used 'irons' rather than stones until the early 1900s; Canada is the only country known to have done so, while others experimented with wood or ice-filled tins.

Outdoor curling was very popular in Scotland between the 16th and 19th centuries because the climate provided good ice conditions every winter. Scotland is home to the international governing body for curling, the World Curling Federation in Perth, which originated as a committee of the Royal Caledonian Curling Club, the mother club of curling.

Today, the sport is most firmly established in Canada, having been taken there by Scottish emigrants. The Royal Montreal Curling Club, the oldest established sports club still active in North America, was established in 1807. The first curling club in the United States was established in 1830, and the sport was introduced to Switzerland and Sweden before the end of the 19th century, also by Scots. Today, curling is played all over Europe and has spread to Brazil, Japan, Australia, New Zealand, China, and Korea.

The first world championship for curling was limited to men and was known as the "Scotch Cup", held in Falkirk and Edinburgh, Scotland, in 1959. The first world title was won by the Canadian team from Regina, Saskatchewan, skipped by Ernie Richardson. (The "skip" is the team member who calls the shots; see below.)

Curling was one of the first sports that was popular with women and girls.

Curling has been a medal sport in the Winter Olympic Games since the 1998 Winter Olympics. It currently includes men's, women's and mixed doubles tournaments (the mixed doubles event was held for the first time in 2018).

In February 2002, the International Olympic Committee retroactively decided that the curling competition from the 1924 Winter Olympics (originally called "Semaine des Sports d'Hiver", or International Winter Sports Week) would be considered official Olympic events and no longer be considered demonstration events. Thus, the first Olympic medals in curling, which at the time was played outdoors, were awarded for the 1924 Winter Games, with the gold medal won by Great Britain, two silver medals by Sweden, and the bronze by France. A demonstration tournament was also held during the 1932 Winter Olympic Games between four teams from Canada and four teams from the United States, with Canada winning 12 games to 4.

Since the sport's official addition in the 1998 Olympics, Canada has dominated the sport with their men's teams winning gold in 2006, 2010, and 2014, and silver in 1998 and 2002. The women's team won gold in 1998 and 2014, a silver in 2010, and a bronze in 2002 and 2006. The mixed doubles team won gold in 2018.

The playing surface or "curling sheet" is defined by the World Curling Federation Rules of Curling. It is a rectangular area of ice, carefully prepared to be as flat and level as possible, in length by in width. The shorter borders of the sheet are called the backboards. Because of the elongated shape, several sheets may be laid out side by side in the same arena, allowing multiple games to be played simultaneously.

A target, the "house", is centred on the intersection of the "centre line", drawn lengthwise down the centre of the sheet and the "tee line", drawn from, and parallel to, the backboard. These lines divide the house into quarters. The house consists of a centre circle (the "button") and three concentric rings, of diameters 4, 8 and 12 feet, formed by painting or laying coloured vinyl sheet under the ice and are usually distinguished by colour. A stone must at least touch the outer ring in order to score (see Scoring below); otherwise the rings are merely a visual aid for aiming and judging which stone is closer to the button.
Two "hog lines" are drawn from, and parallel to, the backboard.

The "hacks", which give the thrower something to push against when making the throw, are fixed behind each button. On indoor rinks, there are usually two fixed hacks, rubber-lined holes, one on each side of the centre line, with the inside edge no more than from the centre line and the front edge on the hack line. A single moveable hack may also be used.

The ice may be natural but is usually frozen by a refrigeration plant pumping a brine solution through numerous pipes fixed lengthwise at the bottom of a shallow pan of water. Most curling clubs have an ice maker whose main job is to care for the ice. At the major curling championships, ice maintenance is extremely important. Large events, such as national/international championships, are typically held in an arena that presents a challenge to the ice maker, who must constantly monitor and adjust the ice and air temperatures as well as air humidity levels to ensure a consistent playing surface. It is common for each sheet of ice to have multiple sensors embedded in order to monitor surface temperature, as well as probes set up in the seating area (to monitor humidity) and in the compressor room (to monitor brine supply and return temperatures). The surface of the ice is maintained at a temperature of around .

A key part of the preparation of the playing surface is the spraying of water droplets onto the ice, which form "pebble" on freezing. The pebbled ice surface resembles an orange peel, and the stone moves on top of the pebbled ice. The pebble, along with the concave bottom of the stone, decreases the friction between the stone and the ice, allowing the stone to travel farther. As the stone moves over the pebble, any rotation of the stone causes it to "curl", or travel along a curved path. The amount of curl (commonly referred to as the "feet of curl") can change during a game as the pebble wears; the ice maker must monitor this and be prepared to scrape and re-pebble the surface prior to each game.

The curling stone (also sometimes called a "rock" in North America) is made of granite and is specified by the World Curling Federation, which requires a weight between , a maximum circumference of and a minimum height of . The only part of the stone in contact with the ice is the "running surface", a narrow, flat annulus or ring, wide and about in diameter; the sides of the stone bulge convex down to the ring and the inside of the ring is hollowed concave to clear the ice. This concave bottom was first proposed by J. S. Russell of Toronto, Ontario, Canada sometime after 1870, and was subsequently adopted by Scottish stone manufacturer Andrew Kay.

The granite for the stones comes from two sources: Ailsa Craig, an island off the Ayrshire coast of Scotland, and the Trefor Granite Quarry in Wales.

Ailsa Craig is the traditional source and produces two types of granite, "Blue Hone" and "Ailsa Craig Common Green". "Blue Hone" has very low water absorption, which prevents the action of repeatedly freezing water from eroding the stone. "Ailsa Craig Common Green" is a lesser quality granite than "Blue Hone". In the past, most curling stones were made from "Blue Hone" but the island is now a wildlife reserve and the quarry is restricted by environmental conditions that exclude blasting.

Kays of Scotland has been making curling stones in Mauchline, Ayrshire, since 1851 and has the exclusive rights to the Ailsa Craig granite, granted by the Marquess of Ailsa, whose family has owned the island since 1560. According to the 1881 Census, Andrew Kay employed 30 people in his curling stone factory in Mauchline. The last harvest of Ailsa Craig granite by Kays took place in 2013, after a hiatus of 11 years; 2,000 tons were harvested, sufficient to fill anticipated orders through at least 2020. Kays have been involved in providing curling stones for the Winter Olympics since Chamonix in 1924 and has been the exclusive manufacturer of curling stones for the Olympics since the 2006 Winter Olympics.

"Trefor" granite comes from the Yr Eifl or Trefor Granite Quarry in the village of Trefor on the north coast of the LlÅ·n Peninsula in Gwynedd, Wales and has produced granite since 1850. "Trefor" granite comes in shades of pink, blue and grey. The quarry supplies curling stone granite exclusively to the Canada Curling Stone Company, which has been producing stones since 1992 and supplied the stones for the 2002 Winter Olympics.

A handle is attached by a bolt running vertically through a hole in the centre of the stone. The handle allows the stone to be gripped and rotated upon release; on properly prepared ice the rotation will bend ("curl") the path of the stone in the direction in which the front edge of the stone is turning, especially as the stone slows. Handles are coloured to identify each team, two popular colours in major tournaments being red and yellow.
In competition, an electronic handle known as the "eye on the hog" may be fitted to detect hog line violations. This electronically detects whether the thrower's hand is in contact with the handle as it passes the hog line and indicates a violation by lights at the base of the handle (see "delivery" below). The "eye on the hog" eliminates human error and the need for hog line officials. It is mandatory in high-level national and international competition, but its cost, around US$650 each, currently puts it beyond the reach of most curling clubs.

The "curling broom", or "brush", is used to sweep the ice surface in the path of the stone (see "sweeping") and is also often used as a balancing aid during delivery of the stone.

Prior to the 1950s, most curling brooms were made of corn strands and were similar to household brooms of the day. In 1958, Fern Marchessault of Montreal inverted the corn straw in the centre of the broom. This style of corn broom was referred to as "the Blackjack".

Artificial brooms made from man-made fabrics rather than corn, such as the "Rink Rat", also became common later during this time period. Prior to the late sixties, "Scottish" curling brushes were used primarily by some of the Scots, as well as by recreational and elderly curlers, as a substitute for corn brooms, since the technique was easier to learn. In the late sixties, competitive curlers from Calgary, Alberta, such as John Mayer, Bruce Stewart, and, later, the world junior championship teams skipped by Paul Gowsell, proved that the curling brush could be just as (or more) effective without all the blisters common to corn broom use. During that time period, there was much debate in competitive curling circles as to which sweeping device was more effective: brush or broom. Eventually, the brush won out with the majority of curlers making the switch to the less costly and more efficient brush. Today, brushes have replaced traditional corn brooms at every level of curling; it is rare now to see a curler using a corn broom on a regular basis.

Curling brushes may have fabric, hog hair, or horsehair heads. Modern curling brush handles are usually hollow tubes made of fibreglass or carbon fibre instead of a solid length of wooden dowel. These hollow tube handles are lighter and stronger than wooden handles, allowing faster sweeping and also enabling more downward force to be applied to the broom head with reduced shaft flex. New, "directional fabric" brooms, which players are worried will alter the fundamentals of the sport by reducing the level of skill required, have been accused of giving players an unfair advantage. The new brooms were temporarily banned by the World Curling Federation and Curling Canada for the 2015â2016 season. The new brooms give sweepers unprecedented control over the direction the stone goes.

Curling shoes are similar to ordinary athletic shoes except for special soles; the "slider shoe" (usually known as a "slider") is designed for the sliding foot and the "gripper shoe" (usually known as a "gripper") for the foot that kicks off from the hack.

The "slider" is designed to slide and typically has a Teflon sole. It is worn by the thrower during delivery from the hack and by sweepers or the skip to glide down the ice when sweeping or otherwise traveling down the sheet quickly. Stainless steel and "red brick" sliders with lateral blocks of PVC on the sole are also available as alternatives to Teflon. Most shoes have a full-sole sliding surface, but some shoes have a sliding surface covering only the outline of the shoe and other enhancements with the full-sole slider. Some shoes have small disc sliders covering the front and heel portions or only the front portion of the foot, which allow more flexibility in the sliding foot for curlers playing with tuck deliveries. When a player is not throwing, the player's slider shoe can be temporarily rendered non-slippery by using a slip-on gripper. Ordinary athletic shoes may be converted to sliders by using a step-on or slip-on Teflon slider or by applying electrical or gaffer tape directly to the sole or over a piece of cardboard. This arrangement often suits casual or beginning players.

The "gripper" is worn by the thrower on the foot that kicks off from the hack during delivery and is designed to grip the ice. It may have a normal athletic shoe sole or a special layer of rubbery material applied to the sole of a thickness to match the sliding shoe. The toe of the hack foot shoe may also have a rubberised coating on the top surface or a flap that hangs over the toe to reduce wear on the top of the shoe as it drags on the ice behind the thrower.

Other types of equipment include:


The purpose of a game is to score points by getting stones closer to the house centre, or the "button", than the other team's stones. Players from either team alternate in taking shots from the far side of the sheet. An end is complete when all eight rocks from each team have been delivered, a total of sixteen stones. If the teams are tied at the end of regulation, often extra ends are played to break the tie. The winner is the team with the highest score after all ends have been completed (see Scoring below). A game may be conceded if winning the game is infeasible.

International competitive games are generally ten ends, so most of the national championships that send a representative to the World Championships or Olympics also play ten ends. However, there is a movement on the World Curling Tour to make the games only eight ends. Most tournaments on that tour are eight ends, as are the vast majority of recreational games.

In international competition, each side is given 73 minutes to complete all of its throws. Each team is also allowed two minute-long timeouts per 10-end game. If extra ends are required, each team is allowed 10 minutes of playing time to complete its throws and one added 60-second timeout for each extra end. However, the "thinking time" system, in which the delivering team's game timer stops as soon as the shooter's rock crosses the t-line during the delivery, is becoming more popular, especially in Canada. This system allows each team 38 minutes per 10 ends, or 30 minutes per 8 ends, to make strategic and tactical decisions, with 4 minutes and 30 seconds an end for extra ends. The "thinking time" system was implemented after it was recognized that using shots which take more time for the stones to come to rest was being penalized in terms of the time the teams had available compared to teams which primarily use hits which require far less time per shot.

The process of sliding a stone down the sheet is known as the "delivery" or "throw". The players, with the exception of the skip, take turns throwing and sweeping; when one player (e.g., the lead) throws, the players not delivering (the second and third) sweep (see Sweeping, below). When the skip throws the vice-skip takes his or her role.

The "skip", or the captain of the team, determines the desired stone placement and the required "weight", "turn", and "line" that will allow the stone to stop there. The placement will be influenced by the tactics at this point in the game, which may involve taking out, blocking or tapping another stone.

The skip may communicate the "weight", "turn", "line," and other tactics by calling or tapping a broom on the ice. In the case of a takeout, guard, or a tap, the skip will indicate the stones involved.

Before delivery, the running surface of the stone is wiped clean and the path across the ice swept with the broom if necessary, because any dirt on the bottom of a stone or in its path can alter the trajectory and ruin the shot. Intrusion by a foreign object is called a "pick-up" or "pick".

The thrower starts from the "hack". The thrower's "gripper" shoe (with the non-slippery sole) is positioned against one of the hacks; for a right-handed curler the right foot is placed against the left hack and vice versa for a left-hander. The thrower, now "in the hack", lines the body up with shoulders square to the skip's broom at the far end for "line".

The stone is placed in front of the foot now in the hack. Rising slightly from the hack, the thrower pulls the stone back (some older curlers may actually raise the stone in this backward movement) then lunges smoothly out from the hack pushing the stone ahead while the slider foot is moved in front of the gripper foot, which trails behind. The thrust from this lunge determines the "weight" and hence the distance the stone will travel. Balance may be assisted by a broom held in the free hand with the back of the broom down so that it slides. One older writer suggests the player keep "a basilisk glance" at the mark.

There are two common types of delivery currently, the typical flat-foot delivery and the Manitoba tuck delivery where the curler slides on the front ball of his foot.

When the player releases the stone a rotation (called the "turn)" is imparted by a slight clockwise or counter-clockwise twist of the handle from around the two or ten o'clock position to the twelve o'clock on release. A typical rate of turn is about rotations before coming to a rest.

The stone must be released before its front edge crosses the near hog line, and it must clear the far hog line or else be removed from play ("hogged"); an exception is made if a stone fails to come to rest beyond the far hog line after rebounding from a stone in play just past the hog line. In major tournaments the "eye on the hog" sensor is commonly used to enforce this rule. The sensor is in the handle of the stone and will indicate whether the stone was released before the near hog line. The lights on the stone handle will either light up green, indicating that the stone has been legally thrown, or red, in which case the illegally thrown stone will be immediately pulled from play instead of waiting for the stone to come to rest.

After the stone is delivered, its trajectory is influenced by the two sweepers under instruction from the skip. Sweeping is done for several reasons: to make the stone travel farther, to decrease the amount of curl, and to clean debris from the stone's path. Sweeping is able to make the stone travel farther and straighter by slightly melting the ice under the brooms, thus decreasing the friction as the stone travels across that part of the ice. The stones curl more as they slow down, so sweeping early in travel tends to increase distance as well as straighten the path, and sweeping after sideways motion is established can increase the sideways distance.

One of the basic technical aspects of curling is knowing when to sweep. When the ice in front of the stone is swept a stone will usually travel both farther and straighter and in some situations one of those is not desirable. For example, a stone may be traveling too fast (said to have too much weight) but require sweeping to prevent curling into another stone. The team must decide which is better: getting by the other stone but traveling too far or hitting the stone.

Much of the yelling that goes on during a curling game are the skip and sweepers exchanging information about the stone's "line" and "weight" and deciding whether to sweep. The skip evaluates the path of the stone and calls to the sweepers to sweep as necessary to maintain the intended track. The sweepers themselves are responsible for judging the weight of the stone, ensuring the length of travel is correct and communicating the weight of the stone back to the skip. Many teams use a "number system" to communicate in which of 10 zones the sweepers estimate the stone will stop. Some sweepers use stopwatches to time the stone from the back line or tee line to the nearest hog line to aid in estimating how far the stone will travel.

Usually, the two sweepers will be on opposite sides of the stone's path, although depending on which side the sweepers' strengths lie this may not always be the case. Speed and pressure are vital to sweeping. In gripping the broom, one hand should be one third of the way from the top (non-brush end) of the handle while the other hand should be one third of the way from the head of the broom. The angle of the broom to the ice should be so that the most force possible can be exerted on the ice. The precise amount of pressure may vary from relatively light brushing ("just cleaning" - to ensure debris will not alter the stone's path) to maximum-pressure scrubbing.

Sweeping is allowed anywhere on the ice up to the "tee line", once the leading edge of a stone crosses the tee line only one player may sweep it. Additionally, if a stone is behind the tee line one player from the opposing team is allowed to sweep it. This is the only case that a stone may be swept by an opposing team member. In international rules, this player must be the skip; or if the skip is throwing, then the sweeping player must be the third.

Occasionally, players may accidentally touch a stone with their broom or a body part. This is often referred to as "burning" a stone. Players touching a stone in such a manner are expected to call their own infraction as a matter of good sportsmanship. Touching a stationary stone when no stones are in motion (there is no delivery in progress) is not an infraction as long as the stone is struck in such a manner that its position is not altered, and is a common way for the skip to indicate a stone that is to be taken out.

When a stone is touched when stones are in play, the remedies vary between leaving the stones as they end up after the touch, replacing the stones as they would have been if no stone were touched, or removal of the touched stone from play. In non-officiated league play, the skip of the non-offending team has the final say on where the stones are placed after the infraction.

Many different types of shots are used to carefully place stones for strategic or tactical reasons; they fall into three fundamental categories as follows:

Guards are thrown in front of the house in the "free guard zone", usually to protect a stone or to make the opposing team's shot difficult. Guard shots include the "centre-guard", on the centreline and the "corner-guards" to the left or right sides of the centre line. See "Free Guard Zone" below.

Draws are thrown only to reach the house. Draw shots include "raise", "come-around", and "freeze" shots.

Takeouts are intended to remove stones from play and include the "peel", "hit-and-roll" and "double" shots.

For a more complete listing, see Glossary of curling terms.

The "free guard zone" is the area of the curling sheet between the hog line and tee line, excluding the house. Until five stones have been played (three from the side without hammer, and two from the side with hammer), stones in the free guard zone may not be removed by an opponent's stone, although they can be moved within the playing area. If a stone in the free guard zone is knocked out of play, it is placed back in the position it was in before the shot was thrown and the opponent's stone is removed from play. This rule is known as the "five-rock rule" or the "free guard zone rule" (previous versions of the free guard zone rule only limited removing guards from play in the first three or four rocks).

This rule, a relatively recent addition to curling, was added in response to a strategy by teams of gaining a lead in the game and then "peeling" all of the opponents' stones (knocking them out of play at an angle that caused the shooter's stone to also roll out of play, leaving no stones on the ice). By knocking all stones out the opponents could at best score one point, if they had the last stone of the end (called the hammer). If the team peeling the rocks had the hammer they could peel rock after rock which would "blank the end" (leave the end scoreless), keeping the last rock advantage for another end. This strategy had developed (mostly in Canada) as ice-makers had become skilled at creating a predictable ice surface and newer brushes allowed greater control over the rock. While a sound strategy, this made for an unexciting game. Observers at the time noted that if two teams equally skilled in the peel game faced each other on good ice, the outcome of the game would be predictable from who won the coin flip to have last rock (or had earned it in the schedule) at the beginning of the game. The 1990 Brier (Canadian men's championship) was considered by many curling fans as boring to watch because of the amount of peeling and the quick adoption of the free guard zone rule the following year reflected how disliked this aspect of the game had become.

The free guard zone rule was originally called the Modified Moncton Rule and was developed from a suggestion made by Russ Howard for the Moncton 100 cashspiel in Moncton, New Brunswick, in January 1990. "Howard's Rule" (later known as the Moncton Rule), used for the tournament and based on a practice drill his team used, had the first four rocks in play unable to be removed no matter where they were at any time during the end. This method of play was altered by restricting the area in which a stone was protected to the free guard zone only for the first four rocks thrown and adopted as a four-rock free guard zone rule for international competition shortly after. Canada kept to the traditional rules until a three-rock free guard zone rule was adopted for the 1993â94 season. After several years of having the three-rock rule used for the Canadian championships and the winners then having to adjust to the four-rock rule in the World Championships, the Canadian Curling Association adopted the four-rock free guard zone in the 2002â2003 season.

One strategy that has been developed by curlers in response to the free guard zone (Kevin Martin from Alberta is one of the best examples) is the "tick" game, where a shot is made attempting to knock (tick) the guard to the side, far enough that it is difficult or impossible to use but still remaining in play while the shot itself goes out of play. The effect is functionally identical to peeling the guard but significantly harder, as a shot that hits the guard too hard (knocking it out of play) results in its being replaced, while not hitting it hard enough can result in it still being tactically useful for the opposition. There is also a greater chance that the shot will miss the guard entirely because of the greater accuracy required to make the shot. Because of the difficulty of making this type of shot, only the best teams will normally attempt it, and it does not dominate the game the way the peel formerly did. Steve Gould from Manitoba popularized ticks played across the face of the guard stone. These are easier to make because they impart less speed on the object stone, therefore increasing the chance that it remains in play even if a bigger chunk of it is hit.

With the tick shot reducing the effectiveness of the four-rock rule, the Grand Slam of Curling series of bonspiels adopted a five-rock rule in 2014. In 2017, the five-rock rule was adopted by the World Curling Federation and member organizations for official play, beginning in the 2018â19 season.

The last rock in an end is called the "hammer" and throwing the hammer gives a team a tactical advantage. Before the game, teams typically decide who gets the hammer in the first end either by chance (such as a coin toss), by a "draw-to-the-button" contest, where a representative of each team shoots to see who gets closer to the centre of the rings, or, particularly in tournament settings like the Winter Olympics, by a comparison of each team's win-loss record. In all subsequent ends the team that did not score in the preceding end gets to throw second, thus having the hammer. In the event that neither team scores, called a "blanked end", the hammer remains with the same team. Naturally, it is easier to score points with the hammer than without; the team with the hammer generally tries to score two or more points. If only one point is possible, the skip may try to avoid scoring at all in order to retain the hammer the next end, giving the team another chance to use the hammer advantage to try to score two points. Scoring without the hammer is commonly referred to as "stealing", or "a steal", and is much more difficult.

Curling is a game of strategy, tactics and skill. The strategy depends on the team's skill, the opponent's skill, the conditions of the ice, the score of the game, how many ends remain and whether the team has last-stone advantage (the "hammer"). A team may play an end aggressively or defensively. Aggressive playing will put a lot of stones in play by throwing mostly draws; this makes for an exciting game and is very risky but the reward can be very great. Defensive playing will throw a lot of hits preventing a lot of stones in play; this tends to be less exciting and less risky. A good drawing team will usually opt to play aggressively, while a good hitting team will opt to play defensively.

If a team does not have the hammer in an end, it will opt to try to clog up the four-foot zone in the house to deny the opposing team access to the button. This can be done by throwing "centre line" guards in front of the house on the centre line, which can be tapped into the house later or drawn around. If a team has the hammer, they will try to keep this four-foot zone free so that they have access to the button area at all times. A team with the hammer may throw a "corner guard" as their first stone of an end placed in front of the house but outside the four-foot zone to utilize the free guard zone. Corner guards are key for a team to score two points in an end, because they can either draw around it later or hit and roll behind it, making the opposing team's shot to remove it more difficult.

Ideally, the strategy in an end for a team with the hammer is to score two points or more. Scoring one point is often a wasted opportunity, as they will then lose last-rock advantage for the next end. If a team cannot score two points, they will often attempt to "blank an end" by removing any leftover opposition rocks and rolling out; or, if there are no opposition rocks, just throwing the rock through the house so that no team scores any points, and the team with the hammer can try again the next end to score two or more with it. Generally, a team without the hammer would want to either force the team with the hammer to only one point (so that they can get the hammer back) or "steal" the end by scoring one or more points of their own.

Generally, the larger the lead a team will have in a game, the more defensively they should play. By hitting all of the opponent's stones, it removes opportunities for their getting multiple points, therefore defending the lead. If the leading team is quite comfortable, leaving their own stones in play can also be dangerous. Guards can be drawn around by the other team, and stones in the house can be tapped back (if they are in front of the tee line) or frozen onto (if they are behind the tee line). A frozen stone is difficult to remove, because it is "frozen" (in front of and touching) to the opponents stone. At this point, a team will opt for "peels", meaning that the stones they throw will be to not only hit their opposition stones, but to roll out of play as well. Peels are hits that are thrown with the most amount of power.

It is not uncommon at any level for a losing team to terminate the match before all ends are completed if it believes it no longer has a realistic chance of winning. Competitive games end once the losing team has "run out of rocks"âthat is, once it has fewer stones in play and available for play than the number of points needed to tie the game.

Most decisions about rules are left to the skips, although in official tournaments, decisions may be left to the officials. However, all scoring disputes are handled by the vice skip. No players other than the vice skip from each team should be in the house while score is being determined. In tournament play, the most frequent circumstance in which a decision has to be made by someone other than the vice skip is the failure of the vice skips to agree on which stone is closest to the button. An independent official (supervisor at Canadian and World championships) then measures the distances using a specially designed device that pivots at the centre of the button. When no independent officials are available, the vice skips measure the distances.

The winner is the team having the highest number of accumulated points at the completion of ten ends. Points are scored at the conclusion of each of these ends as follows: when each team has thrown its eight stones, the team with the stone closest to the button wins that end; the winning team is then awarded one point for each of its own stones lying closer to the button than the opponent's closest stone.

Only stones that are "in the house" are considered in the scoring. A stone is in the house if it lies within the zone or any portion of its edge lies over the edge of the ring. Since the bottom of the stone is rounded, a stone just barely in the house will not have any actual contact with the ring, which will pass under the rounded edge of the stone, but it still counts. This type of stone is known as a "biter".

It may not be obvious to the eye which of two rocks is closer to the button (centre) or if a rock is actually biting or not. There are specialized devices to make these determinations, but these cannot be brought out until after an end is completed. Therefore, a team may make strategic decisions during an end based on assumptions of rock position that turn out to be incorrect.

The score is marked on a scoreboard, of which there are two types; the baseball type and the club scoreboard.

The baseball-style scoreboard was created for televised games for audiences not familiar with the club scoreboard. The "ends" are marked by columns 1 through 10 (or 11 for the possibility of an extra end to break ties) plus an additional column for the total. Below this are two rows, one for each team, containing the team's score for that end and their total score in the right hand column.

The club scoreboard is traditional and used in most curling clubs. Scoring on this board only requires the use of (up to) 11 digit cards, whereas with baseball-type scoring an unknown number of multiples of the digits (especially low digits like "1") may be needed. The numbered centre row represents various possible scores, and the numbers placed in the team rows represent the end in which that team achieved that cumulative score. If the red team scores three points in the first end (called a "three-ender"), then a 1 (indicating the first end) is placed beside the number 3 in the red row. If they score two more in the second end, then a 2 will be placed beside the 5 in the red row, indicating that the red team has five points in total (3+2). This scoreboard works because only one team can get points in an end. However, some confusion may arise if neither team scores points in an end, this is called a "blank end". The blank end numbers are usually listed in the farthest column on the right in the row of the team that has the "hammer" (last rock advantage), or on a special spot for blank ends.

The following example illustrates the difference between the two types. The example illustrates the men's final at the 2006 Winter Olympics.

Eight points â all the rocks thrown by one team counting â is the highest score possible in an end, and is known as an "eight-ender" or "snowman". Scoring an eight-ender against a relatively competent team is very difficult; in curling, it is considered the equivalent of pitching a perfect game in baseball. Probably the best-known snowman came at the 2006 Players' Championships. Future (2007) World Champion Kelly Scott scored eight points in one of her games against 1998 World bronze medalist Cathy King.

Competition teams are normally named after the skip, for example, Team Martin after skip Kevin Martin. Amateur league players can (and do) creatively name their teams, but when in competition (a bonspiel) the official team will have a standard name.

Top curling championships are typically played by all-male or all-female teams. It is known as mixed curling when a team consists of two men and two women. For many years, in the absence of world championship or Olympic mixed curling events, national championships (of which the Canadian Mixed Curling Championship was the most prominent) were the highest-level mixed curling competitions. However, a European Mixed Curling Championship was inaugurated in 2005, a World Mixed Doubles Curling Championship was established in 2008, and the European Mixed Championship was replaced with the World Mixed Curling Championship in 2015. A mixed tournament was held at the Olympic level for the first time in 2018, although it was a doubles tournament, not a four-person.

Curling tournaments may use the Schenkel system for determining the participants in matches.

Curling is played in many countries, including Canada, the United Kingdom (especially Scotland), the United States, Norway, Sweden, Switzerland, Denmark, Finland and Japan, all of which compete in the world championships.

Curling has been depicted by many artists including: George Harvey, John Levack, The Dutch School, Charles Martin Hardie, John Elliot Maguire, John McGhie, and John George Brown.

Curling is particularly popular in Canada. Improvements in ice making and changes in the rules to increase scoring and promote complex strategy have increased the already high popularity of the sport in Canada, and large television audiences watch annual curling telecasts, especially the Scotties Tournament of Hearts (the national championship for women), the Tim Hortons Brier (the national championship for men), and the women's and men's world championships.

Despite the Canadian province of Manitoba's small population (ranked 5th of 10 Canadian provinces), Manitoban teams have won the Brier more times than teams from any other province. The Tournament of Hearts and the Brier are contested by provincial and territorial champions, and the world championships by national champions.

Curling is the provincial sport of Saskatchewan. From there Ernie Richardson and his family team dominated Canadian and international curling during the late 1950s and early 1960s and have been considered to be the best male curlers of all time. Sandra Schmirler led her team to the first ever gold medal in women's curling in the 1998 Winter Olympics. When she died two years later from cancer, over 15,000 people attended her funeral, and it was broadcast on national television.

More so than in many other team sports, good sportsmanship, often referred to as the "Spirit of Curling", is an integral part of curling. The Spirit of Curling also leads teams to congratulate their opponents for making a good shot, strong sweeping or spectacular form. Perhaps most importantly, the Spirit of Curling dictates that one never cheers mistakes, misses or gaffes by one's opponent (unlike most team sports) and one should not celebrate one's own good shots during the game beyond modest acknowledgement of the shot such as a head nod, fist bump or thumbs-up gesture. Modest congratulation, however, may be exchanged between winning team members after the match. On-the-ice celebration is usually reserved for the winners of a major tournament after winning the final game of the championship. It is completely unacceptable to attempt to throw opposing players off their game by way of negative comment, distraction or heckling.

A match traditionally begins with players shaking hands with and saying "good curling" or "have a pleasant game" to each member of the opposing team. It is also traditional in some areas for the winning team to buy the losing team a drink after the game. Even at the highest levels of play, players are expected to call their own fouls.

It is not uncommon for a team to concede a curling match after it believes it no longer has any hope of winning. Concession is an honourable act and does not carry the stigma associated with quitting, and also allows for more socializing. To concede a match, members of the losing team offer congratulatory handshakes to the winning team. Thanks, wishes of future good luck and hugs are usually exchanged between the teams. To continue playing when a team has no realistic chance of winning can be seen as a breach of etiquette.

Curling has been adapted for wheelchair users and people otherwise unable to throw the stone from the hack. These curlers may use a device known as a "delivery stick". The cue holds on to the handle of the stone and is then pushed along by the curler. At the end of delivery, the curler pulls back on the cue, which releases it from the stone. The Canadian Curling Association "Rules of Curling" allows the use of a delivery stick in club play but does not permit it in championships.

The delivery stick was specifically invented for elderly curlers in Canada in 1999. In early 2016 an international initiative started to allow use of the delivery sticks by players over 60 years of age in World Curling Federation Senior Championships, as well as in any projected Masters (60+) Championship that develops in the future.

Terms used to describe the game include:

The ice in the game may be "fast (keen)" or "slow". If the ice is keen, a rock will travel farther with a given amount of weight (throwing force) on it. The speed of the ice is measured in seconds. One such measure, known as "hog-to-hog" time, is the speed of the stone and is the time in seconds the rock takes from the moment it crosses the near hog line until it crosses the far hog line. If this number is lower, the rock is moving faster, so again low numbers mean more speed. The ice in a match will be somewhat consistent and thus this measure of speed can also be used to measure how far down the ice the rock will travel. Once it is determined that a rock taking (for example) 13 seconds to go from hog line to hog line will stop on the tee line, the curler can know that if the hog-to-hog time is matched by a future stone, that stone will likely stop at approximately the same location. As an example, on keen ice, common times might be 16 seconds for guards, 14 seconds for draws, and 8 seconds for peel weight.

"The back line to hog line speed" is used principally by sweepers to get an initial sense of the weight of a stone. As an example, on keen ice, common times might be 4.0 seconds for guards, 3.8 seconds for draws, 3.2 for normal hit weight, and 2.9 seconds for peel weight. Especially at the club level, this metric can be misleading, due to amateurs sometimes pushing stones on release, causing the stone to travel faster than the back-to-hog speed.


In the 19th century several private railway stations in the United Kingdom were built to serve curlers attending bonspiels, such as those at Aboyne, Carsbreck and Drummuir.

The Beatles participate in a game of curling during one scene of their 1965 film "Help!". The villains booby-trap one of the curling stones with a bomb; George sees the "fiendish thingy" and tells everyone to run. The bomb eventually goes off after a delay, creating a big hole in the ice.

Curling is featured prominently in "Boy Meets Curl", the twelfth episode of the comedy series "The Simpsons"<nowiki>'</nowiki> twenty-first season. The episode aired on the Fox network in the United States on 14 February 2010.

"Men with Brooms" is a 2002 Canadian film that takes a satirical look at curling. A TV adaptation, also titled "Men with Brooms", debuted in 2010 on CBC Television.

The "Corner Gas" episode "Hurry Hard" involves the townspeople of Dog River competing in a local curling bonspiel for the fictitious "Clavet Cup". The episode also features cameos by Canadian curlers Randy Ferbey and Dave Nedohin.



</doc>
<doc id="6645" url="https://en.wikipedia.org/wiki?curid=6645" title="Craven Cottage">
Craven Cottage

Craven Cottage is a football stadium located in Fulham, London. It has been the home ground of Fulham F.C. since 1896.
The ground's capacity was 25,700, all-seated, until the closure of the Riverside Stand for redevelopment in the close season of 2019, though the record attendance is 49,335, for a game against Millwall, 8 October 1938. Located next to Bishop's Park on the banks of the River Thames, 'Craven Cottage' was originally a royal hunting lodge and has history dating back over 300 years.

As well as by Fulham, the stadium has been also been used by the United States men's national football team, Australia national football team, the Republic of Ireland national football team (for a friendly match in 2012), and Canada men's national football team, and was formerly the home ground for rugby league team .

The original 'Cottage' was built in 1780, by William Craven, the sixth Baron Craven and was located close to where the Johnny Haynes Stand is now. At the time, the surrounding areas were woods which made up part of Anne Boleyn's hunting grounds.

The Cottage was lived in by Edward Bulwer-Lytton (who wrote "The Last Days of Pompeii") and other somewhat notable (and moneyed) persons until it was destroyed by fire in May 1888. Many rumours persist among Fulham fans of past tenants of Craven Cottage. Sir Arthur Conan Doyle, Jeremy Bentham, Florence Nightingale and even Queen Victoria are reputed to have stayed there, although there is no real evidence for this. Following the fire, the site was abandoned. Fulham had had 8 previous grounds before settling in at Craven Cottage for good. Therefore, The Cottagers have had 12 grounds overall (including a temporary stay at Loftus Road), meaning that only their former 'landlords' and rivals QPR have had more home grounds (14) in British football. Of particular note, was Ranelagh House, Fulham's palatial home from 1886â1888.

When representatives of Fulham first came across the land, in 1894, it was so overgrown that it took two years to be made suitable for football to be played on it. A deal was struck for the owners of the ground to carry out the work, in return for which they would receive a proportion of the gate receipts.

The first football match at which there were any gate receipts was when Fulham played against Minerva in the Middlesex Senior Cup, on 10 October 1896. The ground's first stand was built shortly after. Described as looking like an "orange box", it consisted of four wooden structures each holding some 250 seats, and later was affectionately nicknamed the "rabbit hutch".

In 1904 London County Council became concerned with the level of safety at the ground, and tried to get it closed. A court case followed in January 1905, as a result of which Archibald Leitch, a Scottish architect who had risen to prominence after his building of the Ibrox Stadium, a few years earlier, was hired to work on the stadium. In a scheme costing Â£15,000 (a record for the time), he built a pavilion (the present-day 'Cottage' itself) and the Stevenage Road Stand, in his characteristic red brick style.

The stand on Stevenage Road celebrated its centenary in the 2005â2006 season and, following the death of Fulham FC's favourite son, former England captain Johnny Haynes, in a car accident in October 2005 the Stevenage Road Stand was renamed the Johnny Haynes Stand after the club sought the opinions of Fulham supporters.
Both the Johnny Haynes Stand and Cottage remain among the finest examples of Archibald Leitch football architecture to remain in existence and both have been designated as Grade II listed buildings.

An England v Wales match was played at the ground in 1907, followed by a rugby league international between England and Australia in 1911.

One of the club's directors Henry Norris, and his friend William Hall, took over Arsenal in the early 1910s, the plan being to merge Fulham with Arsenal, to form a "London superclub" at Craven Cottage. This move was largely motivated by Fulham's failure thus far to gain promotion to the top division of English football. There were also plans for Henry Norris to build a larger stadium on the other side of Stevenage Road but there was little need after the merger idea failed. During this era, the Cottage was used for choir singing and marching bands along with other performances, and Mass.

In 1933 there were plans to demolish the ground and start again from scratch with a new 80,000 capacity stadium. These plans never materialised mainly due to the Great Depression.

On 8 October 1938, 49,335 spectators watched Fulham play Millwall. It was the largest attendance ever at Craven Cottage and the record remains today, unlikely to be bettered as it is now an all-seater stadium with currently no room for more than 25,700. The ground hosted several football games for the 1948 Summer Olympics, and is one of the last extant that did.

It was not until after Fulham first reached the top division, in 1949, that further improvements were made to the stadium. In 1962 Fulham became the final side in the first division to erect floodlights. The floodlights were said to be the most expensive in Europe at the time as they were so modern. The lights were like large pylons towering 50 metres over the ground and were similar in appearance to those at the WACA. An electronic scoreboard was installed on the Riverside Terrace at the same time as the floodlights were installed and flagpoles flying the flags of all of the other first division teams were flown from them. Following the sale of Alan Mullery to Tottenham Hotspur in 1964 (for Â£72,500) the Hammersmith End had a roof put over it at a cost of approximately Â£42,500.

Although Fulham was relegated, the development of Craven Cottage continued. The Riverside terracing, infamous for the fact that fans occupying it would turn their heads annually to watch The Boat Race pass, was replaced by what was officially named the 'Eric Miller Stand', Eric Miller being a director of the club at the time. The stand, which cost Â£334,000 and held 4,200 seats, was opened with a friendly game against Benfica in February 1972, (which included EusÃ©bio). PelÃ© was also to appear on the ground, with a friendly played against his team Santos F.C. The Miller stand brought the seated capacity up to 11,000 out of a total 40,000. Eric Miller committed suicide five years later after a political and financial scandal, and had shady dealings with trying to move Fulham away from the Cottage. The stand is now better known as the Riverside Stand.

On Boxing Day 1963, Craven Cottage was the venue of the fastest hat-trick in the history of the English football league, which was completed in less than three minutes, by Graham Leggat. This helped his Fulham team to beat Ipswich 10â1 (a club record). The international record is held by Jimmy O'Connor, an Irish player who notched up his hat trick in 2 minutes 14 seconds in 1967.

Between 1980 and 1984, Fulham rugby league played their home games at the Cottage. They have since evolved into the London Crusaders, the London Broncos and Harlequins Rugby League before reverting to London Broncos ahead of the 2012 season. Craven Cottage held the team's largest ever crowd at any ground with 15,013, at a game against Wakefield Trinity on 15 February 1981.

When the Hillsborough disaster occurred in 1989, Fulham were in the second bottom rung of The Football League, but following the Taylor report Fulham's ambitious chairman Jimmy Hill tabled plans in 1996 for an all-seater stadium. These plans never came to fruition, partly due to local residents' pressure groups, and by the time Fulham reached the Premier League, they still had standing areas in the ground, something virtually unheard of at the time. A year remained to do something about this (teams reaching the second tier for the first time are allowed a three-year period to reach the required standards for the top two divisions), but by the time the last league game was played there, against Leicester City on 27 April 2002, no building plans had been made. Two more Intertoto Cup games were played there later that year (against FC Haka of Finland and Egaleo FC of Greece), and the eventual solution was to decamp to Loftus Road, home of local rivals QPR. During this time, many Fulham fans only went to away games in protest of moving from Craven Cottage. 'Back to the Cottage', later to become the 'Fulham Supporters Trust', was set up as a fans pressure group to encourage the chairman and his advisers that Craven Cottage was the only viable option for Fulham Football Club.

After one and a half seasons at Loftus Road, no work had been done on the Cottage. In December 2003, plans were unveiled for Â£8 million worth of major refurbishment work to bring it in line with Premier League requirements. With planning permission granted, work began in January 2004 in order to meet the deadline of the new season. The work proceeded as scheduled and the club were able to return to their home for the start of the 2004â05 season. Their first game in the new-look 22,000 all-seater stadium was a pre-season friendly against Watford on 10 July 2004. Fenway Sports Group originally partnered with Fulham in 2009, due to the perceived heritage and quirks shared between the Cottage and Fenway Park, saying no English club identifies with its stadium as much as Fulham.

The current stadium was one of the Premier League's smallest grounds at the time of Fulham's relegation at the end of the 2013â14 season (it was third-smallest, after the KC Stadium and the Liberty Stadium). Much admired for its fine architecture, the stadium has recently hosted a few international games, mostly including Australia. This venue is suitable for Australia because most of the country's top players are based in Europe, and West London has a significant community of expatriate Australians. Also, Greece vs. South Korea was hosted on 6 February 2007. In 2011 Brazil played Ghana, in an international friendly, and the Women's Champions League Final was hosted.

Craven Cottage often hosts many other events such as 5-a-side football tournaments and weddings. Also, many have Sunday Lunch at the Riverside restaurant or the 'Cottage Cafe' on non-match days. Craven Cottage hosted the Oxbridge Varsity Football match annually between 1991 and 2000 and again in 2003, 2006 (the same day as the famous 'Boat Race'), 2008, 2009, and 2014 as well as having a Soccer Aid warm-up match in 2006. The half-time entertainment often includes the SW6ers (previously called The Cravenettes) which are a group of female cheerleaders. Other events have included brass bands, Michael Jackson (all though just walking on the pitch, as opposed to performing), Travis playing, Arabic dancing, keepie uppie professionals and presentational awards. Most games also feature the 'Fulham flutter', a half-time draw; and a shoot-out competition of some kind, usually involving scoring through a 'hoop' or 'beat the goalie'. On the first home game of the season, there is a carnival where every Fulham fan is expected to turn up in black-and-white colours. There is usually live rock bands, player signings, clowns, stilt walkers, a steel (calypso) band, food stalls and a free training session for children in Bishops Park.

The Fulham Ladies (before their demise) and Reserve teams occasionally play home matches at the Cottage. Other than this, they generally play at the club's training ground at Motspur Park or at Kingstonian and AFC Wimbledon's stadium, Kingsmeadow. Craven Cottage is known by several affectionate nicknames from fans, including: The (River) Cottage, The Fortress (or Fortress Fulham), Thameside, The Friendly Confines, SW6, Lord of the Banks, The House of Hope, The Pavilion of Perfection, The 'True' Fulham Palace and The Palatial Home. The Thames at the banks of the Cottage is often referred to as 'Old Father' or The River of Dreams.

The most accessible route to the ground is to walk through Bishops Park from Putney Bridge (the nearest Underground station), often known as 'The Green Mile' by Fulham fans (as it is roughly a mile walk through pleasant greenery). The Telegraph ranked the Cottage 9th out of 54 grounds to hold Premier League football.

On 27 July 2012, Fulham FC were granted permission to redevelop the Riverside Stand, increasing the capacity of Craven Cottage to 30,000 seats. Beforehand various rumours arose including plans to return to ground-sharing with QPR in a new 40,000 seater White City stadium, although these now appear firmly on hold with the construction of the Westfield shopping centre on the proposed site. The board seem to have moved away from their ambition to make Fulham the "Manchester United of the south" as it became clear how expensive such a plan would be. With large spaces of land at a premium in south-west London, Fulham appear to be committed to a gradual increase of the ground's capacity often during the summer between seasons. The capacity of Craven Cottage has been increased during summers for instance in 2008 with a small increase in the capacity of the Hammersmith End. Fulham previously announced in 2007 that they are planning to increase the capacity of Craven Cottage by 4,000 seats, but this is yet to be implemented. There was also proposals for a bridge to span the Thames, for a redeveloped Riverside stand and a museum.

More substantial plans arose in October 2011 with the 'Fulham Forever' campaign. With Al-Fayed selling Harrods department store for Â£1.5 billion in May 2010 a detailed plan emerged in the Riverside Stand as the only viable area for expansion. The scheme involved the demolition of the back of the Riverside Stand with a new tier of seating added on top of the current one and a row of corporate boxes; bringing Craven Cottage up to 30,000 capacity. Taking into account local residents, the proposal would: reopen the riverside walk; light pollution would be reduced with the removal of floodlight masts; new access points would make match-day crowds more manageable; and the new stand would be respectful in design to its position on the River Thames. Buckingham Group Contracting were chosen in March 2013 as the construction company for the project. In May 2019, the club confirmed that work on the new Riverside Stand would commence in the summer of 2019. During construction, set to occur through the 2020-21 season, the ground's capacity will be temporarily reduced to 19,000.

The Hammersmith End (or Hammy) is the northernmost stand in the ground, the closest to Hammersmith. The roofing was financed through the sale of Alan Mullery to Tottenham Hotspur F.C. It is traditionally the "home" end where the more vocal Fulham fans sit, and many stand during games at the back rows of the stand. If Fulham win the toss, they usually choose to play towards the Hammersmith End in the second half. The hardcore fans tend to sit (or rather stand) in the back half of H6 and H7 zones (known as âH Blockâ to the faithful). The stand had terracing until the reopening of the ground in 2004, when it was replaced with seating in order to comply with league rules following the Taylor Report. 
The Putney End is the southernmost stand in the ground, nearest to Putney and backing onto Bishops Park. This stand currently a mixture of home and away fans, separated by a "wall" of stewards, with away fans usually allocated blocks P5 and P6. When the ground became redeveloped, with the standing terraces replaced in 2003â04 the club applied for a licence to have a designated neutral area, in the rows closest to the Cottage, but this currently doesnât exist due to the reduced capacity. Due to Fulham's history of having no segregation in the Putney End and no history of crowd trouble, the FA gave the club special dispensation to allow for this, making Fulham the only club currently in the UK to have such an area. Flags of every nationality in the Fulham squad were hung from the roofing, although they were removed after the 2006â07 season commenced and there is now an electronic scoreboard in place. There is a plane tree in the corner by the river.

The Riverside was originally terracing that backed onto the Thames. It also featured large advertising hoardings above the fans. In 1971-72, an all-seater stand was built, originally known as the Riverside Stand (the name was confirmed in the Fulham v Carlisle United programme on 4 December 1971). Its hard lines and metallic and concrete finish are in stark contrast to the Johnny Haynes stand opposite. The stand was opened for a prestigious friendly against S.L. Benfica, who included EusÃ©bio in the team. In the Fulham v Burnley programme on 4 October 1977, it was revealed that the stand would be re-named the Eric Miller Stand, following the recent death of the former vice-chairman. It is sometimes incorrectly stated that, contrary to the above, the name of the stand was changed from the Eric Miller Stand to The Riverside Stand after the discovery of Miller's suicide. He had been under investigation for fraud and embezzlement. The name of the stand actually reverted to "Riverside Stand" in the 1990s.

The Riverside Stand backs onto the River Thames and is elevated above pitch level, unlike the other three stands. It contains the corporate hospitality seating alongside Fulham fans. Jimmy Hill once referred to the Riverside being "a bit like the London Palladium" as Blocks V & W (the middle section) are often filled with the rich and famous (including often Al-Fayed). There were then several Harrods advertising hoardings. Above the advertising hoardings is the gantry, for the press and cameras. Tickets in this area are often the easiest to buy, not surprisingly they are also some of the more expensive. It has the Hammersmith End to its left, the Putney End to its right and is opposite the Johnny Haynes Stand.

During the 1970s, Craven Cottage flooded, with water gushing in from the Riverside. The stand houses the George Cohen restaurant, while on non-match days there is the Cottage Cafe, located near to the Cottage itself. (The River CafÃ© is also located nearby). Under Tommy Trinder's chairmanship in the 60s, flags of all other teams in the Division 1 were proudly flown along the Thames. However, when Fulham were relegated in 1968, Trinder decided not to change the flags as "Fulham won't be in this division next season". True to Trinder's prophecy, Fulham were relegated again. The roof of the stand has been used by sponsors, with VisitFlorida currently advertising in this way, and Pipex.com, FxPro, Lee Cooper Jeans and LG having previously done so. The end of the Riverside Stand towards the 'Hammy End' indicates the end of the 'Fulham Wall', a landmark in The Boat Race.

From the 2019â20 season, this stand is being demolished and rebuilt, which will expand the capacity of the ground to around 29,600. It is due for completion for the beginning of the 2021-22 season. On 26 November 2019, the Chairman Shahid Khan provided an update in which it was announced that the new development will be known as Fulham Pier, a destination venue outside of match-day use.

Originally called the Stevenage Road Stand, after the road it backs onto, the Johnny Haynes stand is the oldest remaining football stand in the Football League and professional football, originally constructed in 1905, and is a Grade II listed building. Designed by Archibald Leitch, the stand contains the ticket office and club shop and features original 'Bennet' wooden seating. Following his death in 2005, the stand was renamed after former player Johnny Haynes.

The exterior facing Stevenage Road has a brick faÃ§ade and features the club's old emblem in the artwork. Decorative pillars show the club's foundation date as 1880 though this is thought to be incorrect. Also, a special stone to commemorate the supporters' fund-raising group Fulham 2000, and The Cottagers' return to Craven Cottage, was engraved on the faÃ§ade. The family enclosures are located in the two corners of the stand, one nearest to the Hammersmith End and one nearest to the Putney End. The front of the stand now contains plastic seating, but originally was a standing area. Children were often placed at the front of this enclosure and the area had a distinctive white picket fence to keep fans off the pitch up until the 1970s.

The Cottage Pavilion dates back to 1905 along with the Johnny Haynes Stand, built by renowned football architect Archibald Leitch. Besides being the changing rooms, the Cottage (also called The Clubhouse) is traditionally used by the players' families and friends who sit on the balcony to watch the game. In the past, board meetings used to be held in The Cottage itself as well. There is a large tapestry draped from the Cottage which says "Still Believe". It encapsulates the now-famous moment, when fans facing defeat against Hamburg SV in the Europa League semi-final roused the players with the chant of "Stand up if you still believe". In the three other corners of the ground there are what have been described as large 'filing cabinets', which are corporate boxes on three levels, but currently the box on the other side of the Putney End has been removed due to the redevelopment of the Riverside



Craven Cottage hosted the Northern Ireland versus Cyprus 1974 World Cup Qualifier on 8 May 1973, a match moved from Belfast due to The Troubles. Northern Ireland won 3-0, Sammy Morgan and a Trevor Anderson brace concluded the scoring in the first half.

On 22 February 2000, it hosted Englandâs under 21s international under 21 friendly against Argentinaâs under 21s. The hosts won 1â0 with Lee Hendrieâs sixty seventh-minute goal with 15,747 in attendance.

In recent years, Craven Cottage has hosted several International Friendly matches, including the Ireland national team who played Colombia and Nigeria there in May 2008 and May 2009 respectively and Oman in 2012.
The South Korea national football team have also used the ground thrice in recent years for international friendlies, first against Greece in February 2007 second against Serbia in November 2009, and then against Croatia in February 2013.
On 17 November 2007 Australia beat Nigeria 1â0 in an international friendly at Craven Cottage.

On 26 May 2011, Craven Cottage hosted the game of 2011 UEFA Women's Champions League Final between Lyon and Potsdam. In September 2011, a friendly between Ghana and Brazil was also held at Craven Cottage. 
On 15 October 2013, Australia beat Canada 3â0 at Craven Cottage. On 28 May 2014 Scotland played out a 2â2 draw with a Nigerian team who had qualified for the 2014 World Cup Finals.

On 27 March 2018, Australia played host to Colombia in the international friendlies, the match ended 0-0, both teams having qualified for the 2018 World Cup Finals in Russia.





</doc>
<doc id="6649" url="https://en.wikipedia.org/wiki?curid=6649" title="Constantine (disambiguation)">
Constantine (disambiguation)

Constantine most commonly refers to one of the following:

It may also refer to:











</doc>
<doc id="6650" url="https://en.wikipedia.org/wiki?curid=6650" title="Lists of composers">
Lists of composers

This is a list of lists of composers grouped by various criteria.











</doc>
<doc id="6651" url="https://en.wikipedia.org/wiki?curid=6651" title="Cedar Falls, Iowa">
Cedar Falls, Iowa

Cedar Falls is a city in Black Hawk County, Iowa, United States. As of the 2010 census, the city population was 39,260. It is home to the University of Northern Iowa, a public university.

Cedar Falls was founded in 1845 by William Sturgis. It was originally named Sturgis Falls, for the first family who settled the site and who continued to live in the city for years. The city was called Sturgis Falls until it was merged with Cedar City (another city on the other side of the Cedar River), creating Cedar Falls. The city's founders are honored each year with a week long community-wide celebration named in their honor â the Sturgis Falls Celebration.

Because of the availability of water power, Cedar Falls developed as a milling and industrial center prior to the Civil War. The establishment of the Civil War Soldiers' Orphans Home in Cedar Falls changed the direction in which the city developed when, following the war, it became the first building on the campus of the Iowa State Normal School (now the University of Northern Iowa). 

Cedar Falls is located at (42.523520, â92.446402). According to the United States Census Bureau, the city has a total area of , of which is land and is water.

Natural forest, prairie and wetland areas are found within the city limits at the Hartman Reserve Nature Center.

Cedar Falls is part of the Waterloo-Cedar Falls metropolitan area.

As of the census of 2010, there were 39,260 people, 14,608 households, and 8,091 families living in the city. The population density was . There were 15,477 housing units at an average density of . The racial makeup of the city was 93.4% White, 2.1% African American, 0.2% Native American, 2.3% Asian, 0.5% from other races, and 1.7% from two or more races. Hispanic or Latino of any race were 2.0% of the population.

There were 14,608 households, of which 24.8% had children under the age of 18 living with them, 45.5% were married couples living together, 7.2% had a female householder with no husband present, 2.7% had a male householder with no wife present, and 44.6% were non-families. 28.0% of all households were made up of individuals, and 10.4% had someone living alone who was 65 years of age or older. The average household size was 2.37 and the average family size was 2.88.

The median age in the city was 26.8 years. 17.3% of residents were under the age of 18; 29.7% were between the ages of 18 and 24; 20.5% were from 25 to 44; 20.1% were from 45 to 64; and 12.4% were 65 years of age or older. The gender makeup of the city was 48.1% male and 51.9% female.

As of the census of 2000, there were 36,145 people, 12,833 households, and 7,558 families living in the city. The population density was 1,277.2 people per square mile (493.1 per km). There were 13,271 housing units at an average density of 468.9 per square mile (181.1 per km). The racial makeup of the city was 95.14% White, 1.57% Black or African American, 0.15% Native American, 1.61% Asian, 0.02% Pacific Islander, 0.41% from other races, and 1.09% from two or more races. 1.08% of the population were Hispanic or Latino of any race.

There were 12,833 households, out of which 26.9% had children under the age of 18 living with them, 48.9% were married couples living together, 7.5% had a female householder with no husband present, and 41.1% were non-families. 25.5% of all households were made up of individuals, and 9.4% had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 2.91.

Age spread: 18.0% under the age of 18, 30.6% from 18 to 24, 20.5% from 25 to 44, 19.0% from 45 to 64, and 11.9% who were 65 years of age or older. The median age was 26 years. For every 100 females, there were 88.5 males. For every 100 females age 18 and over, there were 85.7 males.

The median income for a household in the city was $70,226, and the median income for a family was $85,158. Males had a median income of $60,235 versus $50,312 for females. The per capita income for the city was $27,140. About 5.6% of families and 4.7% of the population were below the poverty line, including 8.5% of those under age 18, and 6.1% of those age 65 or over.

In 1986, the City of Cedar Falls established the Cedar Falls Art and Culture Board, which oversees the operation of the City's Cultural Division and the James & Meryl Hearst Center for the Arts.

The Cedar Falls Public Library is housed in the Adele Whitenach Davis building located at 524 Main Street. The 47,000Â square footÂ (4,400Â m) structure, designed by Struxture Architects, replaced the Carniege-Dayton building in early 2004. As of the 2016 fiscal year, the library's holdings included approximately 8,000 audio materials, 12,000 video materials, and 104,000 books and periodicals for a grand total of approximately 124,000 items. Patrons made 245,000 visits which took advantage of circulation services, adult, teen, and youth programming. Circulation of library materials for fiscal year 2016 was 543,134. The library also provides public access to more than 30 public computers which provide internet access, office software suites, high resolution color printing, wi-fi, and various games.

The mission of the Cedar Falls Public Library is to promote literacy and provide open access to resources which facilitate lifelong learning. The library is a member of the Cedar Valley Library Consortium. Cedar Falls Public Library shares an Integrated Library System (SirsiDynix Symphony) with the Waterloo Public Library. Library management is provided by Kelly Stern, Director of the Cedar Falls Public Library.

The Cedar Falls Historical Society has its offices in the Victorian Home and Carriage House Museum. It preserves Cedar Falls' history through its five museums, collection, archives, and public programs. Besides the Victorian House, the Society operates the Cedar Falls Ice House, Little Red Schoolhouse, and Behrens-Rapp Station.

It hosts one of three public universities in Iowa, University of Northern Iowa (UNI).

Cedar Falls Community Schools, which covers most of the city limits, includes Cedar Falls High School, two junior high schools, seven elementary schools. Waterloo Community School District covers a small section of Cedar Falls. There is a private Christian school, Valley Lutheran High School. Additionally there is a private Catholic elementary school at St. Patrick Catholic Church, under the Roman Catholic Archdiocese of Dubuque. A significant renovation occurred beginning in May 2014.

The Malcolm Price Lab School/Northern University High School, was a state-funded K-12 school run by the university. It closed in 2012 following cuts at UNI.

The city owns its power, gas and water, and cable TV service. Because of this, Cedar Falls Utilities provides gigabit speeds to residents, this became available on January 14, 2015. Cedar Falls has the power to do so because, unlike 19 other states, Iowa does not prohibit municipal broadband from competing with the private cable TV monopoly. In 2020, Cedar Falls Utilities was recognized by PC Magazine as having the nation's fastest internet, by a factor of three.





The underground music scene in the Cedar Falls area from 1977 to present-day is well documented. The Wartburg College Art Gallery in Waverly, Iowa hosted a collaborative history of the bands, record labels, and music venues involved in the Cedar Falls music scene which ran from March 17 to April 14, 2007. This effort has been continued as a wiki-style website called "The Secret History of the Cedar Valley".














</doc>
<doc id="6652" url="https://en.wikipedia.org/wiki?curid=6652" title="Cleveland Indians">
Cleveland Indians

The Cleveland Indians are an American professional baseball team based in Cleveland, Ohio. The Indians compete in Major League Baseball (MLB) as a member club of the American League (AL) Central division. Since , they have played at Progressive Field. The team's spring training facility is at Goodyear Ballpark in Goodyear, Arizona. Since their establishment as a major league franchise in 1901, the team has won 10 Central Division titles, six American League pennants, and two World Series championships, (in 1920 and 1948). The team's World Series championship drought since 1948 is the longest active among all 30 current Major League teams.

The name "Indians" originated from a request by club owner Charles Somers to baseball writers to choose a new name to replace "Cleveland Naps" following the departure of Nap Lajoie after the 1914 season. It was a revival of the nickname that fans gave to the Cleveland Spiders while Louis Sockalexis, a Native American, was playing for the team. Common nicknames for the Indians include the "Tribe" and the "Wahoos", the latter referencing their former logo, Chief Wahoo. The team's mascot is named "Slider."

The franchise originated in 1894 as the Grand Rapids Rustlers, a minor league team in the Western League. The team relocated to Cleveland in 1900 and was renamed the Cleveland Lake Shores. The Western League itself was renamed the American League while continuing its minor league status. One of the American League's eight charter franchises, the major league incarnation of the club was founded in Cleveland in 1901. Originally called the Cleveland Bluebirds, the team played in League Park until moving permanently to Cleveland Stadium in 1946. From August 24 to September 14, 2017, the Indians won 22 consecutive games, the longest winning streak in American League history.

As of the end of the 2019 season, the Indians overall record is ().

""In 1857 baseball games were a daily spectacle in Cleveland's Public Squares. City authorities tried to find an ordinance forbidding it, to the joy of the crowd, they were unsuccessful.Â â Harold Seymour"" 

From 1865 to 1868 Forest Citys was an amateur ball club. During the 1869 season, Cleveland was among several cities that established professional baseball teams following the success of the 1869 Cincinnati Red Stockings, the first fully professional team. In the newspapers before and after 1870, the team was often called the Forest Citys, in the same generic way that the team from Chicago was sometimes called The Chicagos.

In 1871 the Forest Citys joined the new National Association of Professional Base Ball Players (NA), the first professional league. Ultimately, two of the league's western clubs went out of business during the first season and the Chicago Fire left that city's White Stockings impoverished, unable to field a team again until 1874. Cleveland was thus the NA's westernmost outpost in 1872, the year the club folded. Cleveland played its full schedule to July 19 followed by two games versus Boston in mid-August and disbanded at the end of the season.


In 1876, the National League (NL) supplanted the NA as the major professional league. Cleveland was not among its charter members, but by 1879 the league was looking for new entries and the city gained an NL team. The Cleveland Forest Citys were recreated, but rebranded in 1882 as the Cleveland Blues, because the National League required distinct colors for that season. The Blues had mediocre records for six seasons and were ruined by a trade war with the Union Association (UA) in 1884, when its three best players (Fred Dunlap, Jack Glasscock, and Jim McCormick) jumped to the UA after being offered higher salaries. The Cleveland Blues merged with the St. Louis Maroons UA team in 1885.


Cleveland went without major league baseball for two seasons until gaining a team in the American Association (AA) in 1887. After the AA's Allegheny club jumped to the NL, Cleveland followed suit in 1889, as the AA began to crumble. The Cleveland ball club, named the Spiders (supposedly inspired by their "skinny and spindly" players) slowly became a power in the league. The next year the Spiders moved into League Park, which would serve as the home of Cleveland professional baseball for the next 55 years. Led by native Ohioan Cy Young, the Spiders became a contender in the mid-1890s, playing in the Temple Cup Series (that era's World Series) twice and winning it in 1895. The team began to fade after this success, and was dealt a severe blow under the ownership of the Robison brothers.

Prior to the season, Frank Robison, the Spiders' owner, bought the St. Louis Browns, thus owning two clubs at the same time. The Browns were renamed the "Perfectos", and restocked with Cleveland talent. Just weeks before the season opener, most of the better Spiders were transferred to St. Louis, including three future Hall of Famers: Cy Young, Jesse Burkett and Bobby Wallace.

The roster maneuvers failed to create a powerhouse Perfectos team, as St. Louis finished fifth in both 1899 and . The Spiders were left with essentially a minor league lineup, and began to lose games at a record pace. Drawing almost no fans at home, they ended up playing most of their season on the road, and became known as "The Wanderers." The team ended the season in 12th place, 84 games out of first place, with an all-time worst record of 20-134 (.130 winning percentage). Following the 1899 season, the National League disbanded four teams, including the Spiders franchise. The disastrous 1899 season would actually be a step toward a new future for Cleveland fans the next year.


The Cleveland Infants competed in the Players' League, which was well-attended in some cities, but club owners lacked the confidence to continue beyond the one season. The Cleveland Infants finished with 55 wins and 75 losses, playing their home games at Brotherhood Park.

The Grand Rapids Rustlers were founded in Michigan in 1894 and were part of the Western League. In 1900 the team moved to Cleveland and was named the Cleveland Lake Shores. Around the same time Ban Johnson changed the name of his minor league (Western League) to the American League. In 1900 the American League was still considered a minor league. In 1901 team was renamed the Cleveland Bluebirds when the American League broke with the National Agreement and declared itself a competing Major League. The Cleveland franchise was among its eight charter members, and is one of four teams that remain in its original city, along with Boston, Chicago, and Detroit.
The new team was owned by coal magnate Charles Somers and tailor Jack Kilfoyl. Somers, a wealthy industrialist and also co-owner of the Boston Americans, lent money to other team owners, including Connie Mack's Philadelphia Athletics, to keep them and the new league afloat. Players didn't think the name "Bluebirds" was suitable for a baseball team. Writers frequently shortened it to Cleveland Blues due to the players' all-blue uniforms, but the players didn't like this unofficial name either. The players themselves tried to change the name to Cleveland Broncos in , but this unofficial name never really caught on.

The Bluebirds suffered from financial problems in their first two seasons. This led Somers to seriously consider moving to either Pittsburgh or Cincinnati. Relief came in 1902 as a result of the conflict between the National and American Leagues. In 1901, Napoleon "Nap" Lajoie, the Philadelphia Phillies' star second baseman, jumped to the A's after his contract was capped at $2,400 per yearâone of the highest-profile players to jump to the upstart AL. The Phillies subsequently filed an injunction to force Lajoie's return, which was granted by the Pennsylvania Supreme Court. The injunction appeared to doom any hopes of an early settlement between the warring leagues. However, a lawyer discovered that the injunction was only enforceable in the state of Pennsylvania. Mack, partly to thank Somers for his past financial support, agreed to trade Lajoie to the then-moribund Blues, who offered $25,000 salary over three years. Due to the injunction, however, Lajoie had to sit out any games played against the A's in Philadelphia. Lajoie arrived in Cleveland on June 4 and was an immediate hit, drawing 10,000 fans to League Park. Soon afterward, he was named team captain, and in 1903 the team was renamed the Cleveland Napoleons (soon shortened to Naps) after a newspaper conducted a write-in contest.

Lajoie was named manager in , and the team's fortunes improved somewhat. They finished half a game short of the pennant in 1908. However, the success did not last and Lajoie resigned during the 1909 season as manager but remained on as a player.
After that, the team began to unravel, leading Kilfoyl to sell his share of the team to Somers. Cy Young, who returned to Cleveland in 1909, was ineffective for most of his three remaining years and Addie Joss died from tubercular meningitis prior to the 1911 season.

Despite a strong lineup anchored by the potent Lajoie and Shoeless Joe Jackson, poor pitching kept the team below third place for most of the next decade. One reporter referred to the team as the Napkins, "because they fold up so easily". The team hit bottom in 1914 and 1915, finishing in the cellar both years.

1915 brought significant changes to the team. Lajoie, nearly 40 years old, was no longer a top hitter in the league, batting only .258 in 1914. With Lajoie engaged in a feud with manager Joe Birmingham, the team sold Lajoie back to the A's.

With Lajoie gone, the club needed a new name. Somers asked the local baseball writers to come up with a new name, and based on their input, the team was renamed the Cleveland Indians. The name referenced the nickname "Indians" that was applied to the Cleveland Spiders baseball club during the time when Louis Sockalexis, a Native American, played in Cleveland (1897â99).

At the same time, Somers' business ventures began to fail, leaving him deeply in debt. With the Indians playing poorly, attendance and revenue suffered. Somers decided to trade Jackson midway through the 1915 season for two players and $31,500, one of the largest sums paid for a player at the time.

By 1916, Somers was at the end of his tether, and sold the team to a syndicate headed by Chicago railroad contractor James C. "Jack" Dunn. Manager Lee Fohl, who had taken over in early 1915, acquired two minor league pitchers, Stan Coveleski and Jim Bagby and traded for center fielder Tris Speaker, who was engaged in a salary dispute with the Red Sox. All three would ultimately become key players in bringing a championship to Cleveland.
Speaker took over the reins as player-manager in , and led the team to a championship in 1920. On August 16, 1920, the Indians were playing the Yankees at the Polo Grounds in New York. Shortstop Ray Chapman, who often crowded the plate, was batting against Carl Mays, who had an unusual underhand delivery. It was also late in the afternoon and the infield was completely shaded with the center field area (the batters' background) bathed in sunlight. As well, at the time, "part of every pitcher's job was to dirty up a new ball the moment it was thrown onto the field. By turns, they smeared it with dirt, licorice, tobacco juice; it was deliberately scuffed, sandpapered, scarred, cut, even spiked. The result was a misshapen, earth-colored ball that traveled through the air erratically, tended to soften in the later innings, and as it came over the plate, was very hard to see."

In any case, Chapman did not move reflexively when Mays' pitch came his way. The pitch hit Chapman in the head, fracturing his skull. Chapman died the next day, becoming the only player to sustain a fatal injury from a pitched ball. The Indians, who at the time were locked in a tight three-way pennant race with the Yankees and White Sox, were not slowed down by the death of their teammate. Rookie Joe Sewell hit .329 after replacing Chapman in the lineup.

In September 1920, the Black Sox Scandal came to a boil. With just a few games left in the season, and Cleveland and Chicago neck-and-neck for first place at 94â54 and 95â56 respectively, the Chicago owner suspended eight players. The White Sox lost two of three in their final series, while Cleveland won four and lost two in their final two series. Cleveland finished two games ahead of Chicago and three games ahead of the Yankees to win its first pennant, led by Speaker's .388 hitting, Jim Bagby's 30 victories and solid performances from Steve O'Neill and Stan Coveleski. Cleveland went on to defeat the Brooklyn Robins 5â2 in the World Series for their first title, winning four games in a row after the Robins took a 2â1 Series lead. The Series included three memorable "firsts", all of them in Game 5 at Cleveland, and all by the home team. In the first inning, right fielder Elmer Smith hit the first Series grand slam. In the fourth inning, Jim Bagby hit the first Series home run by a pitcher. And in the top of the fifth inning, second baseman Bill Wambsganss executed the first (and only, so far) unassisted triple play in World Series history, in fact the only Series triple play of any kind.

The team would not reach the heights of 1920 again for 28 years. Speaker and Coveleski were aging and the Yankees were rising with a new weapon: Babe Ruth and the home run. They managed two second-place finishes but spent much of the decade in the cellar. In 1927 Dunn's widow, Mrs. George Pross (Dunn had died in 1922), sold the team to a syndicate headed by Alva Bradley.

The Indians were a middling team by the 1930s, finishing third or fourth most years. brought Cleveland a new superstar in 17-year-old pitcher Bob Feller, who came from Iowa with a dominating fastball. That season, Feller set a record with 17 strikeouts in a single game and went on to lead the league in strikeouts from 1938â1941.
On August 20, 1938, Indians catchers Hank Helf and Frank Pytlak set the "all-time altitude mark" by catching baseballs dropped from the Terminal Tower.

By , Feller, along with Ken Keltner, Mel Harder and Lou Boudreau, led the Indians to within one game of the pennant. However, the team was wracked with dissension, with some players (including Feller and Mel Harder) going so far as to request that Bradley fire manager Ossie Vitt. Reporters lampooned them as the Cleveland Crybabies. Feller, who had pitched a no-hitter to open the season and won 27 games, lost the final game of the season to unknown pitcher Floyd Giebell of the Detroit Tigers. The Tigers won the pennant and Giebell never won another major league game.

Cleveland entered 1941 with a young team and a new manager; Roger Peckinpaugh had replaced the despised Vitt; but the team regressed, finishing in fourth. Cleveland would soon be depleted of two stars. Hal Trosky retired in 1941 due to migraine headaches and Bob Feller enlisted in the Navy two days after the Attack on Pearl Harbor. Starting third baseman Ken Keltner and outfielder Ray Mack were both drafted in 1945 taking two more starters out of the lineup.

In , Bill Veeck formed an investment group that purchased the Cleveland Indians from Bradley's group for a reported $1.6Â million. Among the investors was Bob Hope, who had grown up in Cleveland, and former Tigers slugger, Hank Greenberg.
A former owner of a minor league franchise in Milwaukee, Veeck brought to Cleveland a gift for promotion. At one point, Veeck hired rubber-faced Max Patkin, the "Clown Prince of Baseball" as a coach. Patkin's appearance in the coaching box was the sort of promotional stunt that delighted fans but infuriated the American League front office.

Recognizing that he had acquired a solid team, Veeck soon abandoned the aging, small and lightless League Park to take up full-time residence in massive Cleveland Municipal Stadium. The Indians had briefly moved from League Park to Municipal Stadium in mid-1932, but moved back to League Park due to complaints about the cavernous environment. From 1937 onward, however, the Indians began playing an increasing number of games at Municipal, until by 1940 they played most of their home slate there. League Park was mostly demolished in 1951, but has since been rebuilt as a recreational park.

Making the most of the cavernous stadium, Veeck had a portable center field fence installed, which he could move in or out depending on how the distance favored the Indians against their opponents in a given series. The fence moved as much as between series opponents. Following the 1947 season, the American League countered with a rule change that fixed the distance of an outfield wall for the duration of a season. The massive stadium did, however, permit the Indians to set the then record for the largest crowd to see a Major League baseball game. On October 10, 1948, Game 5 of the World Series against the Boston Braves drew over 84,000. The record stood until the Los Angeles Dodgers drew a crowd in excess of 92,500 to watch Game 5 of the 1959 World Series at the Los Angeles Memorial Coliseum against the Chicago White Sox.

Under Veeck's leadership, one of Cleveland's most significant achievements was breaking the color barrier in the American League by signing Larry Doby, formerly a player for the Negro League's Newark Eagles in , 11 weeks after Jackie Robinson signed with the Dodgers. Similar to Robinson, Doby battled racism on and off the field but posted a .301 batting average in 1948, his first full season. A power-hitting center fielder, Doby led the American League twice in homers.

In 1948, needing pitching for the stretch run of the pennant race, Veeck turned to the Negro League again and signed pitching great Satchel Paige amid much controversy. Barred from Major League Baseball during his prime, Veeck's signing of the aging star in 1948 was viewed by many as another publicity stunt. At an official age of 42, Paige became the oldest rookie in Major League baseball history, and the first black pitcher. Paige ended the year with a 6â1 record with a 2.48 ERA, 45 strikeouts and two shutouts.
In , veterans Boudreau, Keltner, and Joe Gordon had career offensive seasons, while newcomers Doby and Gene Bearden also had standout seasons. The team went down to the wire with the Boston Red Sox, winning a one-game playoff, the first in American League history, to go to the World Series. In the series, the Indians defeated the Boston Braves four games to two for their first championship in 28 years. Boudreau won the American League MVP Award.

The Indians appeared in a film the following year titled "The Kid From Cleveland", in which Veeck had an interest. The film portrayed the team helping out a "troubled teenaged fan" and featured many members of the Indians organization. However, filming during the season cost the players valuable rest days leading to fatigue towards the end of the season. That season, Cleveland again contended before falling to third place. On September 23, 1949, Bill Veeck and the Indians buried their 1948 pennant in center field the day after they were mathematically eliminated from the pennant race.

Later in 1949, Veeck's first wife (who had a half-stake in Veeck's share of the team) divorced him. With most of his money tied up in the Indians, Veeck was forced to sell the team to a syndicate headed by insurance magnate Ellis Ryan.

In , Al Rosen was an All Star for the second year in a row, was named "The Sporting News" Major League Player of the Year, and won the American League Most Valuable Player Award in a unanimous vote playing for the Indians after leading the AL in runs, home runs, RBIs (for the second year in a row), and slugging percentage, and coming in second by one point in batting average. Ryan was forced out in 1953 in favor of Myron Wilson, who in turn gave way to William Daley in . Despite this turnover in the ownership, a powerhouse team composed of Feller, Doby, Minnie MiÃ±oso, Luke Easter, Bobby Ãvila, Al Rosen, Early Wynn, Bob Lemon, and Mike Garcia continued to contend through the early 1950s. However, Cleveland only won a single pennant in the decade, in 1954, finishing second to the New York Yankees five times.
The winningest season in franchise history came in 1954, when the Indians finished the season with a record of 111â43 (.721). That mark set an American League record for wins that stood for 44 years until the Yankees won 114 games in 1998 (a 162-game regular season). The Indians 1954 winning percentage of .721 is still an American League record. The Indians returned to the World Series to face the New York Giants. The team could not bring home the title, however, ultimately being upset by the Giants in a sweep. The series was notable for Willie Mays' over-the-shoulder catch off the bat of Vic Wertz in Game 1. Cleveland remained a talented team throughout the remainder of the decade, finishing in second place in 1959, George Strickland's last full year in the majors.

From 1960 to 1993, the Indians managed one third-place finish (in 1968) and six fourth-place finishes (in 1960, 1974 to 1976, 1990, and 1992) but spent the rest of the time at or near the bottom of the standings.

The Indians hired general manager Frank Lane, known as "Trader" Lane, away from the St. Louis Cardinals in 1957. Lane over the years had gained a reputation as a GM who loved to make deals. With the White Sox, Lane had made over 100 trades involving over 400 players in seven years. In a short stint in St. Louis, he traded away Red Schoendienst and Harvey Haddix. Lane summed up his philosophy when he said that the only deals he regretted were the ones that he didn't make.

One of Lane's early trades in Cleveland was to send Roger Maris to the Kansas City Athletics in the middle of 1958. Indians executive Hank Greenberg was not happy about the trade and neither was Maris, who said that he could not stand Lane. After Maris broke Babe Ruth's home run record, Lane defended himself by saying he still would have done the deal because Maris was unknown and he received good ballplayers in exchange.

After the Maris trade, Lane acquired 25-year-old Norm Cash from the White Sox for Minnie MiÃ±oso and then traded him to Detroit before he ever played a game for the Indians; Cash went on to hit over 350 home runs for the Tigers. The Indians received Steve Demeter in the deal, who had only five at-bats for Cleveland.

In 1960, Lane made the trade that would define his tenure in Cleveland when he dealt slugging right fielder and fan favorite Rocky Colavito to the Detroit Tigers for Harvey Kuenn just before Opening Day in .

It was a blockbuster trade that swapped the AL home run co-champion (Colavito) for the AL batting champion (Kuenn). After the trade, however, Colavito hit over 30 home runs four times and made three All-Star teams for Detroit and Kansas City before returning to Cleveland in . Kuenn, on the other hand, played only one season for the Indians before departing for San Francisco in a trade for an aging Johnny Antonelli and Willie Kirkland. "Akron Beacon Journal" columnist Terry Pluto documented the decades of woe that followed the trade in his book "The Curse of Rocky Colavito". Despite being attached to the curse, Colavito said that he never placed a curse on the Indians but that the trade was prompted by a salary dispute with Lane.

Lane also engineered a unique trade of managers in mid-season 1960, sending Joe Gordon to the Tigers in exchange for Jimmy Dykes. Lane left the team in 1961, but ill-advised trades continued. In 1965, the Indians traded pitcher Tommy John, who would go on to win 288 games in his career, and 1966 Rookie of the Year Tommy Agee to the White Sox to get Colavito back.

Indians' pitchers also set numerous strikeout records. They led the league in K's every year from 1963 to 1968, and narrowly missed in 1969. The 1964 staff was the first to amass 1,100 strikeouts, and in 1968, they were the first to collect more strikeouts than hits allowed.

The 1970s were not much better, with the Indians trading away several future stars, including Graig Nettles, Dennis Eckersley, Buddy Bell and 1971 Rookie of the Year Chris Chambliss, for a number of players who made no impact.

Constant ownership changes did not help the Indians. In 1963, Daley's syndicate sold the team to a group headed by general manager Gabe Paul. Three years later, Paul sold the Indians to Vernon Stouffer, of the Stouffer's frozen-food empire. Prior to Stouffer's purchase, the team was rumored to be relocated due to poor attendance. Despite the potential for a financially strong owner, Stouffer had some non-baseball related financial setbacks and, consequently, the team was cash-poor. In order to solve some financial problems, Stouffer had made an agreement to play a minimum of 30 home games in New Orleans with a view to a possible move there. After rejecting an offer from George Steinbrenner and former Indian Al Rosen, Stouffer sold the team in 1972 to a group led by Cleveland Cavaliers and Cleveland Barons owner Nick Mileti. Steinbrenner went on to buy the New York Yankees in 1973.

Only five years later, Mileti's group sold the team for $11Â million to a syndicate headed by trucking magnate Steve O'Neill and including former general manager and owner Gabe Paul. O'Neill's death in 1983 led to the team going on the market once more. O'Neill's nephew Patrick O'Neill did not find a buyer until real estate magnates Richard and David Jacobs purchased the team in 1986.

The team was unable to move out of the cellar, with losing seasons between 1969 and 1975. One highlight was the acquisition of Gaylord Perry in . The Indians traded fireballer "Sudden Sam" McDowell for Perry, who became the first Indian pitcher to win the Cy Young Award. In , Cleveland broke another color barrier with the hiring of Frank Robinson as Major League Baseball's first African American manager. Robinson served as player-manager and provided a franchise highlight when he hit a pinch hit home run on Opening Day. But the high-profile signing of Wayne Garland, a 20-game winner in Baltimore, proved to be a disaster after Garland suffered from shoulder problems and went 28â48 over five years. The team failed to improve with Robinson as manager and he was fired in . In 1977, pitcher Dennis Eckersley threw a no-hitter against the California Angels. The next season, he was be dealt to the Boston Red Sox where he won 20 games in 1978 and another 17 in 1979.

The 1970s also featured the infamous Ten Cent Beer Night at Cleveland Municipal Stadium. The ill-conceived promotion at a 1974 game against the Texas Rangers ended in a riot by fans and a forfeit by the Indians.

There were more bright spots in the 1980s. In May 1981, Len Barker threw a perfect game against the Toronto Blue Jays, joining Addie Joss as the only other Indian pitcher to do so. "Super Joe" Charbonneau won the American League Rookie of the Year award. Unfortunately, Charboneau was out of baseball by 1983 after falling victim to back injuries and Barker, who was also hampered by injuries, never became a consistently dominant starting pitcher.

Eventually, the Indians traded Barker to the Atlanta Braves for Brett Butler and Brook Jacoby, who became mainstays of the team for the remainder of the decade. Butler and Jacoby were joined by Joe Carter, Mel Hall, Julio Franco and Cory Snyder, bringing new hope to fans in the late 1980s.

Cleveland's struggles over the 30-year span were highlighted in the 1989 film "Major League", which comically depicted a hapless Cleveland ball club going from worst to first by the end of the film.

Throughout the 1980s, the Indians' owners had pushed for a new stadium. Cleveland Stadium had been a symbol of the Indians' glory years in the 1940s and 1950s. However, during the lean years even crowds of 40,000 were swallowed up by the cavernous environment. The old stadium was not aging gracefully; chunks of concrete were falling off in sections and the old wooden pilings now petrified. In 1984, a proposal for a $150Â million domed stadium was defeated in a referendum 2â1.

Finally, in May 1990, Cuyahoga County voters passed an excise tax on sales of alcohol and cigarettes in the county. The tax proceeds were to be used for financing the construction of the Gateway Sports and Entertainment Complex, which would include Jacobs Field for the Indians and Gund Arena for the Cleveland Cavaliers basketball team.

The team's fortunes started to turn in , ironically with a very unpopular trade. The team sent power-hitting outfielder Joe Carter to the San Diego Padres for two unproven players, Sandy Alomar, Jr. and Carlos Baerga. Alomar made an immediate impact, not only being elected to the All-Star team but also winning Cleveland's fourth Rookie of the Year award and a Gold Glove. Baerga became a three-time All-Star with consistent offensive production.

Indians general manager John Hart made a number of moves that finally brought success to the team. In , he hired former Indian Mike Hargrove to manage and traded catcher Eddie Taubensee to the Houston Astros who, with a surplus of outfielders, were willing to part with Kenny Lofton. Lofton finished second in AL Rookie of the Year balloting with a .285 average and 66 stolen bases.

The Indians were named "Organization of the Year" by "Baseball America" in 1992, in response to the appearance of offensive bright spots and an improving farm system.

The team suffered a tragedy during spring training of , when a boat carrying pitchers Steve Olin, Tim Crews, and Bob Ojeda crashed into a pier. Olin and Crews were killed, and Ojeda was seriously injured. (Ojeda missed most of the season, and retired the following year).

By the end of the 1993 season, the team was in transition, leaving Cleveland Stadium and fielding a talented nucleus of young players. Many of those players came from the Indians' new AAA farm team, the Charlotte Knights, who won the International League title that year.

Indians General Manager John Hart and team owner Richard Jacobs managed to turn the team's fortunes around. The Indians opened Jacobs Field in 1994 with the aim of improving on the prior season's sixth-place finish. The Indians were only one game behind the division-leading Chicago White Sox on August 12 when a players strike wiped out the rest of the season.

Having contended for the division in the aborted 1994 season, Cleveland sprinted to a 100â44 record (the season was shortened by 18 games due to player/owner negotiations) in 1995, winning its first ever divisional title. Veterans Dennis MartÃ­nez, Orel Hershiser and Eddie Murray combined with a young core of players including Omar Vizquel, Albert Belle, Jim Thome, Manny RamÃ­rez, Kenny Lofton and Charles Nagy to lead the league in team batting average as well as team ERA.

After defeating the Boston Red Sox in the Division Series and the Seattle Mariners in the ALCS, Cleveland clinched the American League pennant and a World Series berth, for the first time since 1954. The World Series ended in disappointment with the Indians falling in six games to the Atlanta Braves.

Tickets for every Indians home game sold out several months before opening day in 1996. The Indians repeated as AL Central champions but lost to the Baltimore Orioles in the Division Series.

In 1997 Cleveland started slow but finished with an 86â75 record. Taking their third consecutive AL Central title, the Indians defeated the New York Yankees in the Division Series, 3â2. After defeating the Baltimore Orioles in the ALCS, Cleveland went on to face the Florida Marlins in the World Series that featured the coldest game in World Series history. With the series tied after Game 6, the Indians went into the ninth inning of Game Seven with a 2â1 lead, but closer JosÃ© Mesa allowed the Marlins to tie the game. In the eleventh inning, Ãdgar RenterÃ­a drove in the winning run giving the Marlins their first championship. Cleveland became the first team to lose the World Series after carrying the lead into the ninth inning of the seventh game.

In 1998, the Indians made the postseason for the fourth straight year. After defeating the wild-card Boston Red Sox 3â1 in the Division Series, Cleveland lost the 1998 ALCS in six games to the New York Yankees, who had come into the postseason with a then-AL record 114 wins in the regular season.

For the 1999 season, Cleveland added relief pitcher Ricardo RincÃ³n and second baseman Roberto Alomar, brother of catcher Sandy Alomar, Jr, and won the Central Division title for the fifth consecutive year. The team scored 1,009 runs, becoming the first (and to date only) team since the 1950 Boston Red Sox to score more than 1,000 runs in a season. This time, Cleveland did not make it past the first round, losing the Division Series to the Red Sox, despite taking a 2â0 lead in the series. In game three, Indians starter Dave Burba went down with an injury in the 4th inning. Four pitchers, including presumed game four starter Jaret Wright, surrendered nine runs in relief. Without a long reliever or emergency starter on the playoff roster, Hargrove started both Bartolo ColÃ³n and Charles Nagy in games four and five on only three days rest. The Indians lost game four 23â7 and game five 12â8. Four days later, Hargrove was dismissed as manager.

In 2000, the Indians had a 44â42 start, but caught fire after the All Star break and went 46â30 the rest of the way to finish 90â72. The team had one of the league's best offenses that year and a defense that yielded three gold gloves. However, they ended up five games behind the Chicago White Sox in the Central division and missed the wild card by one game to the Seattle Mariners. Mid-season trades brought Bob Wickman and Jake Westbrook to Cleveland. After the season, free agent outfielder Manny RamÃ­rez departed for the Boston Red Sox.

In 2000, Larry Dolan bought the Indians for $320Â million from Richard Jacobs, who, along with his late brother David, had paid $45Â million for the club in 1986. The sale set a record at the time for the sale of a baseball franchise.

2001 saw a return to the postseason. After the departures of RamÃ­rez and Sandy Alomar, Jr., the Indians signed Ellis Burks and former MVP Juan GonzÃ¡lez, who helped the team win the Central division with a 91â71 record. One of the highlights came on August 5, when the Indians completed the biggest comeback in MLB History. Cleveland rallied to close a 14â2 deficit in the seventh inning to defeat the Seattle Mariners 15â14 in 11 innings. The Mariners, who won an MLB record-tying 116 games that season, had a strong bullpen, and Indians manager Charlie Manuel had already pulled many of his starters with the game seemingly out of reach.

Seattle and Cleveland met in the first round of the postseason, however the Mariners won the series 3â2. In the 2001â02 offseason, GM John Hart resigned and his assistant, Mark Shapiro, took the reins.

Shapiro moved to rebuild by dealing aging veterans for younger talent. He traded Roberto Alomar to the New York Mets for a package that included outfielder Matt Lawton and prospects Alex Escobar and Billy Traber. When the team fell out of contention in mid-, Shapiro fired manager Charlie Manuel and traded pitching ace Bartolo ColÃ³n for prospects Brandon Phillips, Cliff Lee, and Grady Sizemore; acquired Travis Hafner from the Rangers for Ryan Drese and Einar DÃ­az; and picked up Coco Crisp from the St. Louis Cardinals for aging starter Chuck Finley. Jim Thome left after the season, going to the Phillies for a larger contract.

Young Indians teams finished far out of contention in 2002 and under new manager Eric Wedge. They posted strong offensive numbers in , but continued to struggle with a bullpen that blew more than 20 saves. A highlight of the season was a 22â0 victory over the New York Yankees on August 31, one of the worst defeats suffered by the Yankees in team history.

In early , the offense got off to a poor start. After a brief July slump, the Indians caught fire in August, and cut a 15.5 game deficit in the Central Division down to 1.5 games. However, the season came to an end as the Indians went on to lose six of their last seven games, five of them by one run, missing the playoffs by only two games. Shapiro was named Executive of the Year in 2005. The next season, the club made several roster changes, while retaining its nucleus of young players. The off-season was highlighted by the acquisition of top prospect Andy Marte from the Boston Red Sox. The Indians had a solid offensive season, led by career years from Travis Hafner and Grady Sizemore. Hafner, despite missing the last month of the season, tied the single season grand slam record of six, which was set in by Don Mattingly. Despite the solid offensive performance, the bullpen struggled with 23 blown saves (a Major League worst), and the Indians finished a disappointing fourth.

In , Shapiro signed veteran help for the bullpen and outfield in the offseason. Veterans Aaron Fultz and Joe Borowski joined Rafael Betancourt in the Indians bullpen. The Indians improved significantly over the prior year and went into the All-Star break in second place. The team brought back Kenny Lofton for his third stint with the team in late July. The Indians finished with a 96â66 record tied with the Red Sox for best in baseball, their seventh Central Division title in 13 years and their first postseason trip since 2001.

The Indians began their playoff run by defeating the Yankees in the ALDS three games to one. This series will be most remembered for the swarm of bugs that overtook the field in the later innings of Game Two. They also jumped out to a three-games-to-one lead over the Red Sox in the ALCS. The season ended in disappointment when Boston swept the final three games to advance to the 2007 World Series.

Despite the loss, Cleveland players took home a number of awards. Grady Sizemore, who had a .995 fielding percentage and only two errors in 405 chances, won the Gold Glove award, Cleveland's first since 2001. Indians Pitcher CC Sabathia won the second Cy Young Award in team history with a 19â7 record, a 3.21 ERA and an MLB-leading 241 innings pitched. Eric Wedge was awarded the first Manager of the Year Award in team history. Shapiro was named to his second Executive of the Year in 2007.

The Indians struggled during the 2008 season. Injuries to sluggers Travis Hafner and Victor Martinez, as well as starting pitchers Jake Westbrook and Fausto Carmona led to a poor start. The Indians, falling to last place for a short time in June and July, traded CC Sabathia to the Milwaukee Brewers for prospects Matt LaPorta, Rob Bryson, and Michael Brantley. and traded starting third baseman Casey Blake for catching prospect Carlos Santana. Pitcher Cliff Lee went 22â3 with an ERA of 2.54 and earned the AL Cy Young Award. Grady Sizemore had a career year, winning a Gold Glove Award and a Silver Slugger Award, and the Indians finished with a record of 81â81.

Prospects for the 2009 season dimmed early when the Indians ended May with a record of 22â30. Shapiro made multiple trades: Cliff Lee and Ben Francisco to the Philadelphia Phillies for prospects Jason Knapp, Carlos Carrasco, Jason Donald and Lou Marson; Victor Martinez to the Boston Red Sox for prospects Bryan Price, Nick Hagadone and Justin Masterson; Ryan Garko to the Texas Rangers for Scott Barnes; and Kelly Shoppach to the Tampa Bay Rays for Mitch Talbot. The Indians finished the season tied for fourth in their division, with a record of 65â97. The team announced on September 30, 2009, that Eric Wedge and all of the team's coaching staff were released at the end of the 2009 season. Manny Acta was hired as the team's 40th manager on October 25, 2009.

On February 18, 2010, it was announced that Shapiro (following the end of the 2010 season) would be promoted to team President, with current President Paul Dolan becoming the new Chairman/CEO, and longtime Shapiro assistant Chris Antonetti filling the GM role.

On January 18, 2011, longtime popular former first baseman and manager Mike Hargrove was brought in as a special adviser. The Indians started the 2011 season strongÂ â going 30â15 in their first 45 games and seven games ahead of the Detroit Tigers for first place. Injuries led to a slump where the Indians fell out of first place. Many minor leaguers such as Jason Kipnis and Lonnie Chisenhall got opportunities to fill in for the injuries. The biggest news of the season came on July 30 when the Indians traded four prospects for Colorado Rockies star pitcher, Ubaldo JimÃ©nez. The Indians sent their top two pitchers in the minors, Alex White and Drew Pomeranz along with Joe Gardner and Matt McBride. On August 25, the Indians signed the team leader in home runs, Jim Thome off of waivers. He made his first appearance in an Indians uniform since he left Cleveland after the 2002 season. To honor Thome, the Indians placed him at his original position, third base, for one pitch against the Minnesota Twins on September 25. It was his first appearance at third base since 1996, and his last for Cleveland. The Indians finished the season in 2nd place, 15 games behind the division champion Tigers.
The Indians broke Progressive Field's Opening Day attendance record with 43,190 against the Toronto Blue Jays on April 5, 2012. The game went 16 innings, setting the MLB Opening Day record, and lasted 5 hours and 14 minutes.

On September 27, 2012, with six games left in the Indians' 2012 season, Manny Acta was fired; Sandy Alomar, Jr. was named interim manager for the remainder of the season. On October 6, the Indians announced that Terry Francona, who managed the Boston Red Sox to five playoff appearances and two World Series between 2004 and 2011, would take over as manager for 2013.

The Indians entered the 2013 season following an active offseason of dramatic roster turnover. Key acquisitions included free agent 1B/OF Nick Swisher and CF Michael Bourn. The team added prized right handed pitching prospect Trevor Bauer, OF Drew Stubbs, and relief pitchers Bryan Shaw and Matt Albers in a three-way trade with the Arizona Diamondbacks and Cincinnati Reds that sent RF Shin-Soo Choo to the Reds, and Tony Sipp to the Arizona Diamondbacks Other notable additions included utility man Mike AvilÃ©s, catcher Yan Gomes, designated hitter Jason Giambi, and starting pitcher Scott Kazmir. The 2013 Indians increased their win total by 24 over 2012 (from 68 to 92), finishing in second place, one game behind Detroit in the Central Division, but securing the number one seed in the American League Wild Card Standings. In their first postseason appearance since 2007, Cleveland lost the 2013 American League Wild Card Game 4â0 at home to Tampa Bay. Francona was recognized for the turnaround with the 2013 American League Manager of the Year Award.

With an 85â77 record, the 2014 Indians had consecutive winning seasons for the first time since 1999â2001, but they were eliminated from playoff contention during the last week of the season and finished third in the AL Central.

In 2015, after struggling through the first half of the season, the Indians finished 81â80 for their third consecutive winning season, which the team had not done since 1999â2001. For the second straight year, the Tribe finished third in the Central and was eliminated from the Wild Card race during the last week of the season. Following the departure of longtime team executive Mark Shapiro on October 6, the Indians promoted GM Chris Antonetti to President of Baseball Operations, assistant general manager Mike Chernoff to GM, and named Derek Falvey as assistant GM. Falvey was later hired by the Minnesota Twins in 2016, becoming their President of Baseball Operations.

The Indians set what was then a franchise record for longest winning streak when they won their 14th consecutive game, a 2â1 win over the Toronto Blue Jays in 19 innings on July 1, 2016 at Rogers Centre. The team clinched the Central Division pennant on September 26, their eighth division title overall and first since 2007, as well as returning to the playoffs for the first time since 2013. They finished the regular season at 94â67, marking their fourth straight winning season, a feat not accomplished since the 1990s and early 2000s.

The Indians began the 2016 postseason by sweeping the Boston Red Sox in the best-of-five American League Division Series, then defeated the Blue Jays in five games in the 2016 American League Championship Series to claim their sixth American League pennant and advance to the World Series against the Chicago Cubs. It marked the first appearance for the Indians in the World Series since 1997 and first for the Cubs since 1945. The Indians took a 3â1 series lead following a victory in Game 4 at Wrigley Field, but the Cubs rallied to take the final three games and won the series 4 games to 3. The Indians' 2016 success led to Francona winning his second AL Manager of the Year Award with the club.

From August 24 through September 15 during the 2017 season, the Indians set a new American League record by winning 22 games in a row. On September 28, the Indians won their 100th game of the season, marking only the third time in history the team has reached that milestone. They finished the regular season with 102 wins, second most in team history (behind 1954's 111 win team). The Indians earned the AL Central title for the second consecutive year, along with home-field advantage throughout the American League playoffs, but they lost the 2017 ALDS to the Yankees 3â2 after being up 2â0.

In 2018, the Indians won their third consecutive AL Central crown with a 91â71 record, but were swept in the 2018 American League Division Series by the Houston Astros, who outscored Cleveland 21â6. In 2019, despite a two-game improvement, the Indians missed the playoffs as they trailed three games behind the Tampa Bay Rays for the second AL Wild Card berth.

The rivalry with fellow Ohio team the Cincinnati Reds is known as the Battle of Ohio or Buckeye Series and features the Ohio Cup trophy for the winner. Prior to 1997, the winner of the cup was determined by an annual pre-season baseball game, played each year at minor-league Cooper Stadium in the state capital of Columbus, and staged just days before the start of each new Major League Baseball season. A total of eight Ohio Cup games were played, with the Indians winning six of them. It ended with the start of interleague play in 1997. The winner of the game each year was awarded the Ohio Cup in postgame ceremonies. The Ohio Cup was a favorite among baseball fans in Columbus, with attendances regularly topping 15,000.

Since 1997, the two teams have played each other as part of the regular season, with the exception of 2002. The Ohio Cup was reintroduced in 2008 and is presented to the team who wins the most games in the series that season. Initially, the teams played one three-game series per season, meeting in Cleveland in 1997 and Cincinnati the following year. The teams have played two series per season against each other since 1999, with the exception of 2002, one at each ballpark. A format change in 2013 made each series two games, except in years when the AL and NL Central divisions meet interleague play, where it is usually extended to three games per series. Through the 2020 meetings, the Indians lead the series 66â51.

An on-and-off rivalry with the Pittsburgh Pirates stems from the close proximity of the two cities, and features some carryover elements from the longstanding rivalry in the National Football League between the Cleveland Browns and Pittsburgh Steelers. Because the Indians' designated interleague rival is the Reds and the Pirates' designated rival is the Tigers, the teams have played periodically, with one three-game series per season from 1997â2001, 2003, 2006, 2009â12, 2015, and 2018. Since 2012, the Indians and Pirates play three or four games every three seasons when the AL Central plays the NL Central as part of the interleague play rotation. The Pirates lead the series 21â18. The teams will play six games in 2020 as MLB instituted an abbreviated schedule focusing on regional match-ups

As the Indians play 19 games every year with each of their AL Central competitors, several rivalries have developed.

The Indians have a geographic rivalry with the Detroit Tigers, highlighted in recent years by intense battles for the AL Central title. The match up has some carryover elements from the Ohio State-Michigan rivalry, as well as the general historic rivalry between Michigan and Ohio dating back to the Toledo War.

The Chicago White Sox are another rival, dating back to the 1959 season, when the Sox slipped past the Tribe to win the AL pennant. The rivalry intensified when both clubs were moved to the new AL Central in 1994. During that season, the two teams challenged for the division title, with the Indians one game back of Chicago when the strike began in August. During a game in Chicago, the White Sox confiscated Albert Belle's corked bat, followed by an attempt by Indians pitcher Jason Grimsley to crawl through the Comiskey Park clubhouse ceiling to retrieve it. Belle later signed with the White Sox in 1997, adding additional intensity to the rivalry.

For the first time in over 70 years, the Indians featured uniforms without the Chief Wahoo logo in 2019, with the official team colors being red, navy blue, and white.

The primary home uniform is white with navy piping around each sleeve. Across the front of the jersey in script font is the word "Indians" in red with a navy blue outline, with navy blue undershirts, belts, and socks.

The alternate home jersey is red with a navy blue script "Indians" trimmed in white on the front, and navy blue piping on the sleeves, with navy blue undershirts, belts, and socks.

The home cap is navy blue with a red bill and features a red block "C" on the front.

The primary road uniform is gray, with "Cleveland" in navy blue block letters trimmed in red across the front of the jersey, navy blue piping around the sleeves, and navy blue undershirts, belts, and socks.

The alternate road jersey is navy blue with "Cleveland" in red block letters trimmed in white on the front of the jersey, and navy blue undershirts, belts, and socks.

The road cap is similar to the home cap, with the only difference being the bill is navy blue.

For all games, the team uses a navy blue batting helmet with a red block "C" on the front.

John Adams, known by baseball fans as "The Drummer", has played a bass drum at nearly every home game since 1973. He is the only fan for whom the team has dedicated a bobble head day. Adams originally paid for his tickets (one for himself, and one for his drum), but recently the Indians have paid for his seats in honor of the contributions he has made to the ballpark atmosphere. He has been featured and interviewed on national TV shows and newspaper articles.
Between June 12, 1995 and April 4, 2001, the Indians sold out 455 consecutive home games, drawing a total of 19,324,248 fans to Jacobs Field. The demand for tickets was so great that all 81 home games were sold out before Opening Day on at least three separate occasions. The sellout streak set a Major League Baseball record; this was broken by the Boston Red Sox on September 8, 2008.

The Indians play "Hang On Sloopy" by The McCoys during the middle of the 8th inning, bringing with it the Ohio State Buckeyes' tradition of the fans chanting "O-H-I-O" during the chorus. They also play "Cleveland Rocks" at the end of the game for Indians wins.

The club nickname and its cartoon logo have been criticized for perpetuating Native American stereotypes. In 1997 and 1998, protesters were arrested after effigies were burned. Charges were dismissed in the 1997 case, and were not filed in the 1998 case. Protesters arrested in the 1998 incident subsequently fought and lost a lawsuit alleging that their First Amendment rights had been violated.

Bud Selig (then-Commissioner of Baseball) said in 2014 that he had never received a complaint about the logo. He has heard that there are some protesting mascots, but individual teams such as the Indians and Atlanta Braves, whose nickname was also criticized for similar reasons, should make their own decisions. An organized group consisting of Native Americans protested Chief Wahoo on Opening Day 2015, as they have for many years, making note that this is the 100th anniversary of the team becoming the Indians. Owner Paul Dolan, while stating he is respectful of critics, says he mainly hears from fans who want to keep Chief Wahoo, and has no plans to change.

On January 29, 2018, Major League Baseball announced that Chief Wahoo would be removed from the Indians' uniforms as of the 2019 season, stating that the logo was no longer appropriate for on-field use. The team will consider a new logo in the future, but the block "C" will be promoted to the primary logo; additionally, there are no plans to change the team's name.

On July 3, 2020, on the heels of the Washington Redskins announcing that they would "undergo a thorough review" of that team's name, the Cleveland Indians announced that they would "determine the best path forward" regarding the team's name and emphasized the need to "keep improving as an organization on issues of social justice".

Cleveland radio stations WTAM (1100Â AM) and WMMS (100.7Â FM) serve as flagship stations for the Cleveland Indians Radio Network. Tom Hamilton and Jim Rosenhaus serve as play-by-play announcers.

The television rights are held by SportsTime Ohio (STO), Matt Underwood and former Indians Gold Glove winning CF Rick Manning form the announcing team, with veteran Cleveland sportscaster Andre Knott as field reporter, and Al Pawlowski and former Indians pitcher Jensen Lewis as pregame/postgame hosts. Select games are shown on free TV, airing on NBC affiliate WKYC channel 3 via simulcast.

Notable former Indians broadcasters include Tom Manning, Jack Graney (the first ex-baseball player to become a play-by-play announcer), Ken Coleman, Joe Castiglione, Van Patrick, Nev Chandler, Bruce Drennan, Jim "Mudcat" Grant, Rocky Colavito, Dan Coughlin, and Jim Donovan.

Previous broadcasters who have had lengthy tenures with the team include Joe Tait (15 seasons between TV and radio), Jack Corrigan (18 seasons on TV), Ford C. Frick Award winner Jimmy Dudley (19 seasons on radio), Mike Hegan (23 seasons between TV and radio), and Herb Score (34 seasons between TV and radio).

Over the years, the Indians have been featured in several movies and television shows. Examples include:


Numerous Indians players have had statues made in their honor:



(*) â Inducted into the Baseball Hall of Fame as an Indian/Nap.

The Cleveland Indians farm system consists of nine minor league affiliates.




</doc>
<doc id="6653" url="https://en.wikipedia.org/wiki?curid=6653" title="Cape Town">
Cape Town

Cape Town (Afrikaans: Kaapstad ; Xhosa: "iKapa;") is the second most populous city in South Africa after Johannesburg and also the legislative capital of South Africa. Colloquially named the Mother City, it is the largest city of the Western Cape province and forms part of the City of Cape Town metropolitan municipality. The Parliament of South Africa is situated in Cape Town. The other two capitals are located in Gauteng (Pretoria the executive capital where the Presidency is based) and in the Free State (Bloemfontein the judicial capital where the Supreme Court of Appeal is located). The city is known for its harbour, for its natural setting in the Cape Floristic Region, and for landmarks such as Table Mountain and Cape Point. Cape Town is home to 64% of the Western Cape's population. The city was named the World Design Capital for 2014 by the International Council of Societies of Industrial Design. 

In 2014, Cape Town was named the best place in the world to visit by both "The New York Times" and "The Daily Telegraph". Cape Town was one of the host cities of the tournaments of 1995 Rugby World Cup and 2010 FIFA World Cup.

Located on the shore of Table Bay, Cape Town, as the oldest urban area in South Africa, was developed by the United East India Company (VOC) as a supply station for Dutch ships sailing to East Africa, India, and the Far East. Jan van Riebeeck's arrival on 6 April 1652 established the VOC Cape Colony, the first permanent European settlement in South Africa. Cape Town outgrew its original purpose as the first European outpost at the Castle of Good Hope, becoming the economic and cultural hub of the Cape Colony. Until the Witwatersrand Gold Rush and the development of Johannesburg, Cape Town was the largest city in South Africa.

The earliest known remnants in the region were found at Peers Cave in Fish Hoek and date to between 15,000 and 12,000 years ago. Little is known of the history of the region's first residents, since there is no written history from the area before it was first mentioned by Portuguese explorer Bartolomeu Dias in 1488 who was the first European to reach the area and named it "Cape of Storms" (Cabo das Tormentas). It was later renamed by John II of Portugal as "Cape of Good Hope" (Cabo da Boa EsperanÃ§a) because of the great optimism engendered by the opening of a sea route to India and the East. Vasco da Gama recorded a sighting of the Cape of Good Hope in 1497. In 1510, at the Battle of Salt River, Francisco de Almeida and fifty of his men were killed and his party were defeated by ox-mounted !UriÇâaekua ("Goringhaiqua" in Dutch approximate spelling), which was one of the so-called Khoekhoe clans of the area that also included the !UriÇâaeÇâona ("Goringhaicona" in Dutch approximate spelling, also known as "Strandlopers"), said to be the ancestors of the !Ora nation of today. In the late 16th century, Portuguese, French, Danish, Dutch and English but mainly Portuguese ships regularly continued to stop over in Table Bay en route to the Indies. They traded tobacco, copper and iron with the Khoekhoe-speaking clans of the region, in exchange for fresh meat.

In 1652, Jan van Riebeeck and other employees of the United East India Company (, VOC) were sent to the Cape to establish a way-station for ships travelling to the Dutch East Indies, and the Fort de Goede Hoop (later replaced by the Castle of Good Hope). The settlement grew slowly during this period, as it was hard to find adequate labour. This labour shortage prompted the authorities to import slaves from Indonesia and Madagascar. Many of these became ancestors of the first Cape Coloured communities. Under Van Riebeeck and his successors as VOC commanders and later governors at the Cape, an impressive range of useful plants were introduced to the Cape â in the process changing the natural environment forever. Some of these, including grapes, cereals, ground nuts, potatoes, apples and citrus, had an important and lasting influence on the societies and economies of the region.

The Dutch Republic being transformed in Revolutionary France's vassal Batavian Republic, Great Britain moved to take control of its colonies. Britain captured Cape Town in 1795, but the Cape was returned to the Dutch by treaty in 1803. British forces occupied the Cape again in 1806 following the Battle of Blaauwberg. In the Anglo-Dutch Treaty of 1814, Cape Town was permanently ceded to the United Kingdom. It became the capital of the newly formed Cape Colony, whose territory expanded very substantially through the 1800s. With expansion came calls for greater independence from the UK, with the Cape attaining its own parliament (1854) and a locally accountable Prime Minister (1872). Suffrage was established according to the non-racial, but sexist Cape Qualified Franchise. 

During the 1850s and 1860s additional plant species were introduced from Australia by the British authorities. Notably rooikrans to stabilise the sand of the Cape Flats to allow for a road connecting the peninsula with the rest of the African continent and eucalyptus to drain marshes. In 1859 the first railway line was built and system of railways radically expanded in the 1870s. The discovery of diamonds in Griqualand West in 1867, and the Witwatersrand Gold Rush in 1886, prompted a flood of immigrants to South Africa. In 1895 the city's first public power station, the Graaff Electric Lighting Works, was opened. Conflicts between the Boer republics in the interior and the British colonial government resulted in the Second Boer War of 1899â1902, which Britain won. From 1891 to 1901 the city's population more than doubled from 67,000 to 171,000.

In 1910, Britain established the Union of South Africa, which unified the Cape Colony with the two defeated Boer Republics and the British colony of Natal. Cape Town became the legislative capital of the Union, and later of the Republic of South Africa.

Prior to the mid-twentieth century the Cape Town was arguably the most racially integrated city in the South Africa. In the 1948 national elections, the National Party won on a platform of "apartheid" (racial segregation) under the slogan of "swart gevaar". This led to the erosion and eventual abolition of the Cape's multiracial franchise, as well as to the Group Areas Act, which classified all areas according to race. Formerly multi-racial suburbs of Cape Town were either purged of residents deemed unlawful by apartheid legislation or demolished. The most infamous example of this in Cape Town was District Six. After it was declared a whites-only region in 1965, all housing there was demolished and over 60,000 residents were forcibly removed. Many of these residents were relocated to the Cape Flats. Under apartheid, the Cape was considered a "Coloured labour preference area", to the exclusion of "Bantus", i.e. Africans. The implementation of this policy was widely opposed by trade unions, civil society and opposition parties. It is notable that this policy was not advocated for by any coloured person, and its implementation was a unilateral decision by the apartheid government.

School students from Langa, Gugulethu and Nyanga in Cape Town reacted to the news of protests against Bantu Education in Soweto in June 1976 and organised gatherings and marches, which were met with resistance from the police. A number of school buildings were burnt down.

Cape Town was home to many leaders of the anti-apartheid movement. On Robben Island, a former penitentiary island from the city, many famous political prisoners were held for years. In one of the most famous moments marking the end of apartheid, Nelson Mandela made his first public speech since his imprisonment, from the balcony of Cape Town City Hall hours after being released on 11 February 1990. His speech heralded the beginning of a new era for the country, and the first democratic election, was held four years later, on 27 April 1994. Nobel Square in the Victoria & Alfred Waterfront features statues of South Africa's four Nobel Peace Prize winners: Albert Luthuli, Desmond Tutu, F. W. de Klerk and Nelson Mandela.

Cape Town is located at latitude 33.55Â° S (approx. the same as Sydney and Buenos Aires and equivalent to Casablanca and Los Angeles in the northern hemisphere) and longitude 18.25Â° E. Table Mountain, with its near vertical cliffs and flat-topped summit over high, and with Devil's Peak and Lion's Head on either side, together form a dramatic mountainous backdrop enclosing the central area of Cape Town, the so-called City Bowl. A thin strip of cloud, known colloquially as the "tablecloth", sometimes forms on top of the mountain. To the immediate south, the Cape Peninsula is a scenic mountainous spine jutting southwards into the Atlantic Ocean and terminating at Cape Point. There are over 70 peaks above within Cape Town's official city limits. Many of the city's suburbs lie on the large plain called the Cape Flats, which extends over to the east and joins the peninsula to the mainland. The Cape Town region is characterised by an extensive coastline, rugged mountain ranges, coastal plains, inland valleys and semi-desert fringes.
UNESCO declared Robben Island in the Western Cape a World Heritage Site in 1999. Robben Island is located in Table Bay, some west of Bloubergstrand in Cape Town, and stands some 30m above sea level. Robben Island has been used as a prison where people were isolated, banished and exiled to for nearly 400 years. It was also used as a leper colony, a post office, a grazing ground, a mental hospital, and an outpost.

Visitors can only access the island via the Robben Island Museum boat service, which runs three times daily until the beginning of the peak season (1 September). The ferries depart from the Nelson Mandela Gateway at the V&A Waterfront.

Cape Town has a warm Mediterranean climate (KÃ¶ppen "Csb"),
with mild, moderately wet winters and dry, warm summers. Winter, which lasts from the beginning of June to the end of August, may see large cold fronts entering for limited periods from the Atlantic Ocean with significant precipitation and strong north-westerly winds. Winter months in the city average a maximum of and minimum of 
Total annual rainfall in the city averages although in the southern suburbs, close to the mountains, rainfall is significantly higher and averages closer to 1000 millimetres (40 in). Summer, which lasts from December to March, is warm and dry with an average maximum of and minimum of . The region can get uncomfortably hot when the Berg Wind, meaning "mountain wind", blows from the Karoo interior for a couple of weeks in February or March. Spring and summer generally feature a strong wind from the south-east, known locally as the south- or the Cape Doctor, so called because it blows air pollution away. This wind is caused by a high-pressure system which sits in the South Atlantic to the west of Cape Town, known as the South Atlantic High. Cape Town receives 3,100 hours of sunshine per year.

Water temperatures range greatly, between on the Atlantic Seaboard, to over in False Bay. Average annual Ocean temperatures are between on the Atlantic Seaboard (similar to Californian waters, such as San Francisco or Big Sur), and in False Bay (similar to Northern Mediterranean temperatures, such as Nice or Monte Carlo).

Located in a CI Biodiversity hotspot as well as the unique Cape Floristic Region, the city of Cape Town has one of the highest levels of biodiversity of any equivalent area in the world. These protected areas are a World Heritage Site, and an estimated 2,200 species of plants are confined to Table Mountain â more than exist in the whole of the United Kingdom which has 1200 plant species and 67 endemic plant species. Many of these species, including a great many types of proteas, are endemic to the mountain and can be found nowhere else.

It is home to a total of 19 different vegetation types, of which several are endemic to the city and occur nowhere else in the world.
It is also the only habitat of hundreds of endemic species, and hundreds of others which are severely restricted or threatened. This enormous species diversity is mainly because the city is uniquely located at the convergence point of several different soil types and micro-climates.

Table Mountain has an unusually rich biodiversity. Its vegetation consists predominantly of several different types of the unique and rich Cape Fynbos. The main vegetation type is endangered Peninsula Sandstone Fynbos, but critically endangered Peninsula Granite Fynbos, Peninsula Shale Renosterveld and Afromontane forest occur in smaller portions on the mountain.

Unfortunately, rapid population growth and urban sprawl has covered much of these ecosystems with development. Consequently, Cape Town now has over 300 threatened plant species and 13 which are now extinct. The Cape Peninsula, which lies entirely within the city of Cape Town, has the highest concentration of threatened species of any continental area of equivalent size in the world.
Tiny remnant populations of critically endangered or near extinct plants sometimes survive on road sides, pavements and sports fields. The remaining ecosystems are partially protected through a system of over 30 nature reserves â including the massive Table Mountain National Park.

Cape Town reached first place in the 2019 iNaturalist City Nature Challenge in two out of the three categories: Most Observations, and Most Species. This was the first entry by Capetonians in this annual competition to observe and record the local biodiversity over a four-day long weekend during what is considered the worst time of the year for local observations. However, a worldwide survey showed that the extinction rate of endemic plants from the City of Cape Town is one of the highest in the world, at roughly three per year since 1900 - partly a consequence of the very small and localised habitats and high endemicity.

Cape Town's urban geography is influenced by the contours of Table Mountain, its surrounding peaks, the Durbanville Hills, and the expansive lowland region known as the Cape Flats. These geographic features in part divide the city into several commonly known groupings of suburbs (equivalent to districts outside South Africa), many of which developed historically together and share common attributes of language and culture.

The City Bowl is a natural amphitheatre-shaped area bordered by Table Bay and defined by the mountains of Signal Hill, Lion's Head, Table Mountain and Devil's Peak.

The area includes the central business district of Cape Town, the harbour, the Company's Garden, and the residential suburbs of De Waterkant, Devil's Peak, District Six, Zonnebloem, Gardens, Bo-Kaap, Higgovale, Oranjezicht, Schotsche Kloof, Tamboerskloof, University Estate, Vredehoek, Walmer Estate and Woodstock.

The Foreshore Freeway Bridge has stood in its unfinished state since construction officially ended in 1977. It was intended to be the Eastern Boulevard Highway in the city bowl, but is unfinished due to budget constraints.

The Atlantic Seaboard lies west of the City Bowl and Table Mountain, and is characterised by its beaches, cliffs, promenade and hillside communities. The area includes, from north to south, the neighbourhoods of Green Point, Mouille Point, Three Anchor Bay, Sea Point, Fresnaye, Bantry Bay, Clifton, Camps Bay, Llandudno, and Hout Bay. The Atlantic Seaboard has some of the most expensive real estate in South Africa particularly on Nettleton and Clifton Roads in Clifton, Ocean View Drive and St Leon Avenue in Bantry Bay, Theresa Avenue in Bakoven and Fishermans Bend in Llandudno. Camps Bay is home to the highest concentration of multimillionaires in Cape Town and has the highest number of high-priced mansions in South Africa with more than 155 residential units exceeding R20 million (or $US1.8 million).

The Blaauwberg suburbs is a coastal region of the Cape Town Metropolitan area and lies along the coast to the north of Cape Town, and include Bloubergstrand, Milnerton, Tableview, West Beach, Big Bay, Sunset Beach, Sunningdale, Parklands and Parklands North, as well as the exurbs of Atlantis, Mamre and Melkbosstrand. The Koeberg Nuclear Power Station is located within this area and maximum housing density regulations are enforced in much of the area surrounding the nuclear plant.

The Northern Suburbs is a predominantly Afrikaans-speaking region of the Cape Town Metropolitan area and includes Belhar, Bellville, Blue Downs, Bothasig, Burgundy Estate, Durbanville, Edgemead, Brackenfell, Elsie's River, Eerste River, Kraaifontein, Goodwood, Kensington, Maitland, Monte Vista, Panorama, Parow, Richwood, Kraaifontein and Kuils River..

The Northern Suburbs are home to Tygerberg Hospital, the largest hospital in the Western Cape and second largest in South Africa

The Southern Suburbs lie along the eastern slopes of Table Mountain, southeast of the city centre. This area is predominantly English-speaking, and includes, from north to south, Observatory, Mowbray, Pinelands, Rosebank, Rondebosch, Rondebosch East, Newlands, Claremont, Lansdowne, Kenilworth, Bishopscourt, Constantia, Wynberg, Plumstead, Ottery, Bergvliet and Diep River. West of Wynberg lies Constantia which, in addition to being a wealthy neighbourhood, is a notable wine-growing region within the City of Cape Town, and attracts tourists for its well-known wine farms and Cape Dutch architecture. The Southern Suburbs is also well known as having some of the oldest, and most sought after residential areas within the City of Cape Town.

The South Peninsula is a predominantly English-speaking area in the Cape Town Metropolitan area and is generally regarded as the area South of Muizenberg on False Bay and Noordhoek on the Atlantic Ocean, all the way to Cape Point. Until recently, this region was quite rural, however the population of the area is growing quickly as new coastal developments proliferate and larger plots are subdivided to provide more compact housing. It includes Capri Village, Clovelly, Fish Hoek, Glencairn, Kalk Bay, Kommetjie, Masiphumelele, Muizenberg, Noordhoek, Ocean View, Scarborough, Simon's Town, St James, Sunnydale and Sun Valley. South Africa's largest naval base is located at Simon's Town harbour, and close by is Boulders Beach, the site of a large colony of African penguins.

The Cape Flats is an expansive, low-lying, flat area situated to the southeast of the city centre. 

Due to the region having a Mediterranean climate, the wettest months on the Cape Flats are from April to September, with 82% most of its rainfall occurring between these months. The rainfall patterns on the Cape Flats vary with longitude, such that the eastern parts get a minimum of 214mm per year and the central and western parts get 800mm per year. A significant portion of this water ends up in the Cape Flats Aquifer, which lie beneath the central and southern parts of the Cape Flats. Most of the land of the Cape Flats is used for residential areas, the majority of which are formal, but with several informal settlements present. Light industrial areas are also found in the area. A part of the land in the south-east is used for cultivation and contains many smallholdings.

The Helderberg is a small region in the Cape Town Metropolitan area and is located on the north-eastern corner of False Bay. It consists of Somerset West, Strand, Gordons Bay and a few other suburbs which were previously towns in the Helderberg district. The district takes its name from the imposing Helderberg Mountain, which reaches a height of 

Cape Town is governed by a 231-member city council elected in a system of mixed-member proportional representation. The city is divided into 116 wards, each of which elects a councillor by first-past-the-post voting. The remaining 115 councillors are elected from party lists so that the total number of councillors for each party is proportional to the number of votes received by that party.

In the local government elections of 3 August 2016, the Democratic Alliance (DA) won an outright majority, taking 154 of the 231 council seats. The African National Congress, the national ruling party, received 57 seats. As a result of this victory, Patricia de Lille of the Democratic Alliance was re-elected to a second term as Executive Mayor. However, De Lille resigned as Mayor on 31 October 2018. The Democratic Alliance designated Dan Plato as their candidate to replace her.

According to the South African National Census of 2011, the population of the City of Cape Town metropolitan municipalityan area that includes suburbs and exurbs not always considered as part of Cape Townis 3,740,026 people. This represents an annual growth rate of 2.6% compared to the results of the previous census in 2001 which found a population of 2,892,243 people.

In 1944, 47% of the city's population was White, 46% was Coloured, less than 6% was Black African and 1% was Asian.

Of those residents who were asked about their first language, 35.7% spoke Afrikaans, 29.8% spoke Xhosa and 28.4% spoke English. 24.8% of the population is under the age of 15, while 5.5% is 65 or older.

Of those residents aged 20 or older, 1.8% have no schooling, 8.1% have some schooling but did not finish primary school, 4.6% finished primary school but have no secondary schooling, 38.9% have some secondary schooling but did not finish Grade 12, 29.9% finished Grade 12 but have no higher education, and 16.7% have higher education. Overall, 46.6% have at least a Grade 12 education. Of those aged between 5 and 25, 67.8% are attending an educational institution. Amongst those aged between 15 and 65 the unemployment rate is 23.7%. The average annual household income is R161,762.

There are 1,068,573 households in the municipality, giving an average household size of 3.3 people. Of those households, 78.4% are in formal structures (houses or flats), while 20.5% are in informal structures (shacks). 94.0% of households use electricity for lighting. 87.3% of households have piped water to the dwelling, while 12.0% have piped water through a communal tap. 94.9% of households have regular refuse collection service. 91.4% of households have a flush toilet or chemical toilet, while 4.5% still use a bucket toilet. 82.1% of households have a refrigerator, 87.3% have a television and 70.1% have a radio. Only 34.0% have a landline telephone, but 91.3% have a cellphone. 37.9% have a computer, and 49.3% have access to the Internet (either through a computer or a cellphone).
Cape Town is the economic hub of the Western Cape province, South Africa's second main economic centre and Africa's third main economic hub city. It serves as the regional manufacturing centre in the Western Cape. In 2011 the city's GDP was US$56.8Â billion with a GDP per capita of US$15,721. In the five years preceding 2014 Cape Town GDP grew at an average of 3.7% a year. As a proportion of GDP, the agriculture and manufacturing sectors have declined whilst finance, business services, transport and logistics have grown reflecting the growth in specialised services sectors of the local economy. Fishing, clothing and textiles, wood product manufacturing, electronics, furniture, hospitality, finance and business services are industries in which Cape Town's economy has the largest comparative advantage.

Between 2001 and 2010 the city's Gini coefficient, a measure of inequality, improved by dropping from 0.59 in 2007 to 0.57 in 2010 only to increase to 0.58 by 2017. The city has the lowest rate of inequality in South Africa.

Cape Town has recently enjoyed a booming real estate and construction market, because of the 2010 FIFA World Cup as well as many people buying summer homes in the city or relocating there permanently. Cape Town hosted nine World Cup matches: Six first-round matches, one second-round match, one quarter final and one semifinal. The central business district is under an extensive urban renewal programme, with numerous new buildings and renovations taking place under the guidance of the Cape Town Partnership.

Cape Town has four major commercial nodes, with Cape Town Central Business District containing the majority of job opportunities and office space. Century City, the Bellville/Tygervalley strip and Claremont commercial nodes are well established and contain many offices and corporate headquarters as well. Most companies headquartered in the city are insurance companies, retail groups, publishers, design houses, fashion designers, shipping companies, petrochemical companies, architects and advertising agencies. The most notable companies headquartered in the city are food and fashion retailer Woolworths, supermarket chain Pick n Pay Stores and Shoprite, New Clicks Holdings Limited, fashion retailer Foschini Group, internet service provider MWEB, Mediclinic International, eTV, multinational mass media giant Naspers, and financial services giant Sanlam. Other notable companies include Belron (vehicle glass repair and replacement group operating worldwide), CapeRay (develops, manufactures and supplies medical imaging equipment for the diagnosis of breast cancer), Ceres Fruit Juices (produces fruit juice and other fruit based products), Coronation Fund Managers (third-party fund management company), ICS (was one of the largest meat processing and distribution companies in the world), Vida e CaffÃ¨ (chain of coffee retailers), Capitec Bank (commercial bank in the Republic of South Africa). The city is a manufacturing base for several multinational companies including, Johnson & Johnson, GlaxoSmithKline, Levi Strauss & Co., Adidas, Bokomo Foods, Yoco and Nampak.

Most goods are handled through the Port of Cape Town or Cape Town International Airport. Most major shipbuilding companies have offices and manufacturing locations in Cape Town. The province is also a centre of energy development for the country, with the existing Koeberg nuclear power station providing energy for the Western Cape's needs.

The Western Cape is an important tourist region in South Africa; the tourism industry accounts for 9.8% of the GDP of the province and employs 9.6% of the province's workforce. In 2010, over 1.5Â million international tourists visited the area.

With the highest number of successful Technology companies in Africa, Cape Town is an important centre for the industry on the continent. This includes an increasing number of companies in the Space industry. Growing at an annual rate of 8.5% and an estimated worth of R77Â billion in 2010, nationwide the high tech industry in Cape Town is becoming increasingly important to the city's economy.

The city was recently named as the most entrepreneurial city in South Africa, with the percentage of Capetonians pursuing business opportunities almost three times higher than the national average. Those aged between 18 and 64 were 190% more likely to pursue new business, whilst in Johannesburg, the same demographic group was only 60% more likely than the national average to pursue a new business. Numerous startups in the Space industry have been founded in Cape Town. With a number of entrepreneurship initiatives and world class universities providing skills, Cape Town has become the Silicon Valley of South Africa, hosting innovative technology startups such as Jumo, Yoco, Aerobotics, Luno and The Sun Exchange.

Cape Town is not only a popular international tourist destination in South Africa, but Africa as a whole. This is due to its mild climate, natural setting, and well-developed infrastructure. The city has several well-known natural features that attract tourists, most notably Table Mountain, which forms a large part of the Table Mountain National Park and is the back end of the City Bowl. Reaching the top of the mountain can be achieved either by hiking up, or by taking the Table Mountain Cableway. Cape Point is recognised as the dramatic headland at the end of the Cape Peninsula. Many tourists also drive along Chapman's Peak Drive, a narrow road that links Noordhoek with Hout Bay, for the views of the Atlantic Ocean and nearby mountains. It is possible to either drive or hike up Signal Hill for closer views of the City Bowl and Table Mountain.
Many tourists also visit Cape Town's beaches, which are popular with local residents. Due to the city's unique geography, it is possible to visit several different beaches in the same day, each with a different setting and atmosphere. Though the Cape's water ranges from cold to mild, the difference between the two sides of the city is dramatic. While the Atlantic Seaboard averages annual water temperatures barely above that of coastal California around , the False Bay coast is much warmer, averaging between annually. This is similar to water temperatures in much of the Northern Mediterranean (for example Nice). In summer, False Bay water averages slightly over , with a common high. Beaches located on the Atlantic Coast tend to have very cold water due to the Benguela current which originates from the Southern Ocean, whilst the water at False Bay beaches may be warmer by up to at the same moment due to the influence of the warm Agulhas current. It is a common misconception that False Bay is part of the Indian Ocean, with Cape Point being both the meeting point of the Indian and Atlantic Oceans, and the southernmost tip of Africa. The oceans in fact meet at the actual southernmost tip, Cape Agulhas, which lies approximately to the south east. The misconception is fuelled by the relative warmth of the False Bay water to the Atlantic Seaboard water, and the many confusing instances of "Two Oceans" in names synonymous with Cape Town, such as the Two Oceans Marathon, the Two Oceans Aquarium, and places such as Two Oceans wine farm.
Both coasts are equally popular, although the beaches in affluent Clifton and elsewhere on the Atlantic Coast are better developed with restaurants and cafÃ©s, with a strip of restaurants and bars accessible to the beach at Camps Bay. The Atlantic seaboard, known as Cape Town's Riviera, is regarded as one of the most scenic routes in South Africa, along the slopes of the Twelve Apostles to the boulders and white sand beaches of Llandudno, which the route ending in Hout Bay, a diverse bustling suburb with a harbour and a seal island. This fishing village is flanked by the Constantia valley and the picturesque Chapman's Peak drive. Boulders Beach near Simon's Town is known for its colony of African penguins. Surfing is popular and the city hosts the Red Bull Big Wave Africa surfing competition every year.

The city has several notable cultural attractions. The Victoria & Alfred Waterfront, built on top of part of the docks of the Port of Cape Town, is the city's most visited tourist attraction. It is also one of the city's most popular shopping venues, with several hundred shops and the Two Oceans Aquarium. The V&A also hosts the Nelson Mandela Gateway, through which ferries depart for Robben Island. It is possible to take a ferry from the V&A to Hout Bay, Simon's Town and the Cape fur seal colonies on Seal and Duiker Islands. Several companies offer tours of the Cape Flats, a mostly Coloured township, and Khayelitsha, a mostly black township.

The most popular areas for visitors to stay include Camps Bay, Sea Point, the V&A Waterfront, the City Bowl, Hout Bay, Constantia, Rondebosch, Newlands, and Somerset West.

In November 2013, Cape Town was voted the best global city in "The Daily Telegraph"'s annual Travel Awards.

Cape Town offers tourists a range of air, land and sea based adventure activities, including paragliding and skydiving.

The City of Cape Town works closely with Cape Town Tourism to promote the city both locally and internationally. The primary focus of Cape Town Tourism is to represent Cape Town as a tourist destination. Cape Town Tourism receives a portion of its funding from the City of Cape Town while the remainder is made up of membership fees and own-generated funds.

The Tristan da Cunha government owns and operates a lodging facility in Cape Town which charges discounted rates to Tristan da Cunha residents and non-resident natives.

Cape Town is noted for its architectural heritage, with the highest density of Cape Dutch style buildings in the world. Cape Dutch style, which combines the architectural traditions of the Netherlands, Germany, France and Indonesia, is most visible in Constantia, the old government buildings in the Central Business District, and along Long Street. The annual Cape Town Minstrel Carnival, also known by its Afrikaans name of "Kaapse Klopse", is a large minstrel festival held annually on 2 January or ""Tweede Nuwe Jaar"" (Afrikaans: Second New Year). Competing teams of minstrels parade in brightly coloured costumes, performing Cape Jazz, either carrying colourful umbrellas or playing an array of musical instruments. The Artscape Theatre Centre is the largest performing arts venue in Cape Town.
The city also encloses the 36 hectare Kirstenbosch National Botanical Garden that contains protected natural forest and fynbos along with a variety of animals and birds. There are over 7,000 species in cultivation at Kirstenbosch, including many rare and threatened species of the Cape Floristic Region. In 2004 this Region, including Kirstenbosch, was declared a UNESCO World Heritage Site.

Cape Town's transport system links it to the rest of South Africa; it serves as the gateway to other destinations within the province. The Cape Winelands and in particular the towns of Stellenbosch, Paarl and Franschhoek are popular day trips from the city for sightseeing and wine tasting. Whale watching is popular amongst tourists: southern right whales and humpback whales are seen off the coast during the breeding season (August to November) and Bryde's whales and killer whale can be seen any time of the year. The nearby town of Hermanus is known for its Whale Festival, but whales can also be seen in False Bay. Heaviside's dolphins are endemic to the area and can be seen from the coast north of Cape Town; dusky dolphins live along the same coast and can occasionally be seen from the ferry to Robben Island.

The only complete windmill in South Africa is Mostert's Mill, Mowbray. It was built in 1796 and restored in 1935 and again in 1995.

In recent years, the city has struggled with problems such as drugs, a surge in violent drug-related crime and more recently gang violence. In the Cape Flats alone, there are approximately 100,000 people in over 130 different gangs in 2018. While there are some alliances, this multitude and division is also cause for conflict between groups. At the same time, the economy has grown due to the boom in the tourism and the real estate industries. With a Gini coefficient of 0.58, Cape Town had the lowest inequality rate in South Africa in 2012. Between 2015 and 2018 Cape Town experienced a severe water shortage that resulted in temporary but widespread water rationing. Since July 2019 widespread violent crime in poorer gang dominated areas of greater Cape Town has resulted in an ongoing military presence in these neighbourhoods. Cape Town had the highest murder rate among large South African cities at 77 murders per 100,000 people in the period April 2018 to March 2019, with 3157 murders mostly occurring in poor townships created under the apartheid regime.

Most places of worship in the city are Christian churches and cathedrals: Zion Christian Church, Apostolic Faith Mission of South Africa, Assemblies of God, Baptist Union of Southern Africa (Baptist World Alliance), Methodist Church of Southern Africa (World Methodist Council), Anglican Church of Southern Africa (Anglican Communion), Presbyterian Church of Africa (World Communion of Reformed Churches), Roman Catholic Archdiocese of Cape Town (Catholic Church). Islam is the city's second largest religion with a long history in Cape Town resulting in a number of mosques and other Muslim religious sites spread across the city such as the Auwal Mosque South Africa's first mosque. Cape Town's significant Jewish population supports a number of synagogues most notably the historic Gardens Shul. The Cape Town Progressive Jewish Congregation (CTPJC) also has three temples in the city. Other religious sites in the city include Hindu, Buddhist and Baha'i temples.

Several newspapers, magazines and printing facilities have their offices in the city. Independent News and Media publishes the major English language papers in the city, the "Cape Argus" and the "Cape Times". Naspers, the largest media conglomerate in South Africa, publishes "Die Burger", the major Afrikaans language paper.

Cape Town has many local community newspapers. Some of the largest community newspapers in English are the "Athlone News" from Athlone, the "Atlantic Sun", the "Constantiaberg Bulletin" from Constantiaberg, the "City Vision" from Bellville, the "False Bay Echo" from False Bay, the "Helderberg Sun" from Helderberg, the "Plainsman" from Michell's Plain, the "Sentinel News" from Hout Bay, the "Southern Mail" from the Southern Peninsula, the "Southern Suburbs Tatler" from the Southern Suburbs, "Table Talk" from Table View and "Tygertalk" from Tygervalley/Durbanville. Afrikaans language community newspapers include the "Landbou-Burger" and the "Tygerburger".
"Vukani", based in the Cape Flats, is published in Xhosa.

Cape Town is a centre for major broadcast media with several radio stations that only broadcast within the city. 94.5 Kfm (94.5Â MHz FM) and Good Hope FM (94â97 MHz FM) mostly play pop music. Heart FM (104.9Â MHz FM), the former P4 Radio, plays jazz and R&B, while Fine Music Radio (101.3 FM) plays classical music and jazz. Bush Radio is a community radio station (89.5Â MHz FM). The Voice of the Cape (95.8Â MHz FM) and Cape Talk (567 kHz MW) are the major talk radio stations in the city. Bokradio (98.9Â MHz FM) is an Afrikaans music station. The University of Cape Town also runs its own radio station, UCT Radio (104.5Â MHz FM).

The SABC has a small presence in the city, with satellite studios located at Sea Point. e.tv has a greater presence, with a large complex located at Longkloof Studios in Gardens. M-Net is not well represented with infrastructure within the city. Cape Town TV is a local TV station, supported by numerous organisation and focusing mostly on documentaries. Numerous productions companies and their support industries are located in the city, mostly supporting the production of overseas commercials, model shoots, TV-series and movies. The local media infrastructure remains primarily in Johannesburg.

Cape Town's most popular sports by participation are cricket, association football, swimming, and rugby union. In rugby union, Cape Town is the home of the Western Province side, who play at Newlands Stadium and compete in the Currie Cup. In addition, Western Province players (along with some from Wellington's Boland Cavaliers) comprise the Stormers in the Southern Hemisphere's Super Rugby competition. Cape Town also regularly hosts the national team, the Springboks, and hosted matches during the 1995 Rugby World Cup, including the opening ceremony and game, as well as the semi-final between New Zealand and England that saw Jonah Lomu run in four tries.

Association football, which is also known as "soccer" in South Africa, is also popular. Two clubs from Cape Town play in the Premier Soccer League (PSL), South Africa's premier league. These teams are Ajax Cape Town, which formed as a result of the 1999 amalgamation of the Seven Stars and the Cape Town Spurs and resurrected Cape Town City F.C. Cape Town was also the location of several of the matches of the FIFA 2010 World Cup including a semi-final, held in South Africa. The Mother City built a new 70,000 seat stadium (Cape Town Stadium) in the Green Point area.

In cricket, the Cape Cobras represent Cape Town at the Newlands Cricket Ground. The team is the result of an amalgamation of the Western Province Cricket and Boland Cricket teams. They take part in the Supersport and Standard Bank Cup Series. The Newlands Cricket Ground regularly hosts international matches.

Cape Town has had Olympic aspirations. For example, in 1996, Cape Town was one of the five candidate cities shortlisted by the IOC to launch official candidatures to host the 2004 Summer Olympics. Although the Games ultimately went to Athens, Cape Town came in third place. There has been some speculation that Cape Town was seeking the South African Olympic Committee's nomination to be South Africa's bid city for the 2020 Summer Olympic Games. That however was quashed when the International Olympic Committee awarded the 2020 Games to Tokyo.

The city of Cape Town has vast experience in hosting major national and international sports events.

The Cape Town Cycle Tour is the world's largest individually timed cycle raceand the first event outside Europe to be included in the International Cycling Union's Golden Bike Series. It sees over 35,000 cyclists tackling a route around Cape Town. The Absa Cape Epic is the largest full-service mountain bike stage race in the world.

Some notable events hosted by Cape Town have included the 1995 Rugby World Cup, 2003 ICC Cricket World Cup, and World Championships in various sports such as athletics, fencing, weightlifting, hockey, cycling, canoeing, gymnastics and others.

Cape Town was also a host city to the 2010 FIFA World Cup from 11 June to 11 July 2010, further enhancing its profile as a major events city. It was also one of the host cities of the 2009 Indian Premier League cricket tournament.

Public primary and secondary schools in Cape Town are run by the Western Cape Education Department. This provincial department is divided into seven districts; four of these are "Metropole" districtsMetropole Central, North, South, and Eastwhich cover various areas of the city. There are also many private schools, both religious and secular, in Cape Town.

Cape Town has a well-developed higher system of public universities. Cape Town is served by three public universities: the University of Cape Town (UCT), the University of the Western Cape (UWC) and the Cape Peninsula University of Technology (CPUT). Stellenbosch University, while not in the city itself, is 50Â kilometres from the City Bowl and has additional campuses, such as the Tygerberg Faculty of Medicine and Health Sciences and the Bellville Business Park closer to the City.

Both the University of Cape Town and Stellenbosch University are leading universities in South Africa. This is due in large part to substantial financial contributions made to these institutions by both the public and private sector. UCT is an English-speaking institution. It has over 21,000 students and has an MBA programme that was ranked 51st by the Financial Times in 2006. It is also the top-ranked university in Africa, being the only African university to make the world's Top 200 university list at number 146. Since the African National Congress has become the country's ruling party, some restructuring of Western Cape universities has taken place and as such, traditionally non-white universities have seen increased financing, which has evidently benefitted the University of the Western Cape.

The Cape Peninsula University of Technology was formed on 1 January 2005, when two separate institutions â Cape Technikon and Peninsula Technikon â were merged. The new university offers education primarily in English, although one may take courses in any of South Africa's official languages. The institution generally awards the National Diploma.

Students from the universities and high schools are involved in the South African SEDS, Students for the Exploration and Development of Space. This is the South African SEDS, and there are many SEDS branches in other countries, preparing enthusiastic students and young professionals for the growing Space industry.

Cape Town has also become a popular study abroad destination for many international college students. Many study abroad providers offer semester, summer, short-term, and internship programs in partnership with Cape Town universities as a chance for international students to gain intercultural understanding.

Cape Town International Airport serves both domestic and international flights. It is the second-largest airport in South Africa and serves as a major gateway for travelers to the Cape region. Cape Town has regularly scheduled services to Southern Africa, East Africa, Mauritius, Middle East, Far East, Europe and the United States as well as eleven domestic destinations.

Cape Town International Airport recently opened a brand new central terminal building that was developed to handle an expected increase in air traffic as tourism numbers increased in the lead-up to the 2010 FIFA World Cup. Other renovations include several large new parking garages, a revamped domestic departure terminal, a new Bus Rapid Transit system station and a new double-decker road system. The airport's cargo facilities are also being expanded and several large empty lots are being developed into office space and hotels.

The Cape Town International Airport was among the winners of the World Travel Awards for being Africa's leading airport.

Cape Town International Airport is located 18Â km from the Central Business District

Cape Town has a long tradition as a port city. The Port of Cape Town, the city's main port, is in Table Bay directly to the north of the CBD. The port is a hub for ships in the southern Atlantic: it is located along one of the busiest shipping corridors in the world. It is also a busy container port, second in South Africa only to Durban. In 2004, it handled 3,161 ships and 9.2Â million tonnes of cargo.

Simon's Town Harbour on the False Bay coast of the Cape Peninsula is the main operational base of the South African Navy.

Until the 1970s the city was served by the Union Castle Line with service to the United Kingdom and St Helena. The RMS "St Helena" provided passenger and cargo service between Cape Town and St Helena until the opening of St Helena Airport. 

The cargo vessel M/V "Helena", under AW Shipping Management, takes a limited number of passengers, between Cape Town and St Helena and Ascension Island on its voyages. Multiple vessels also take passengers to and from Tristan da Cunha, inaccessible by aircraft, to and from Cape Town. In addition takes passengers on its cargo service to the Canary Islands and Hamburg, Germany.

The Shosholoza Meyl is the passenger rail operations of Spoornet and operates two long-distance passenger rail services from Cape Town: a daily service to and from Johannesburg via Kimberley and a weekly service to and from Durban via Kimberley, Bloemfontein and Pietermaritzburg. These trains terminate at Cape Town railway station and make a brief stop at Bellville. Cape Town is also one terminus of the luxury tourist-oriented Blue Train as well as the five-star Rovos Rail.

Metrorail operates a commuter rail service in Cape Town and the surrounding area. The Metrorail network consists of 96 stations throughout the suburbs and outskirts of Cape Town.

Cape Town is the origin of three national roads. The N1 and N2 begin in the foreshore area near the City Center and the N7, which runs North toward Namibia.

The N1 runs East-North-East through Edgemead, Parow, Bellville, and Brackenfell. It connects Cape Town to major cities further inland, namely Bloemfontein, Johannesburg, and Pretoria An older at-grade road, the R101, runs parallel to the N1 from Bellville.

The N2 runs East-South-East through Rondebosch, Guguletu, Khayelitsha, Macassar to Somerset West. It becomes a multiple-carriageway, at-grade road from the intersection with the R44 onwards. The N2 continues east along the coast, linking Cape Town to the coastal cities of Port Elizabeth, East London and Durban. An older at-grade road, the R101, runs parallel to the N1 initially, before veering south at Bellville, to join the N2 at Somerset West via the suburbs of Kuils River and Eerste River.

The N7 originates from the N1 at Wingfield Interchange near Edgemead. It begins, initially as a highway, but becoming an at-grade road from the intersection with the M5 onwards.

There are also a number of regional routes linking Cape Town with surrounding areas. The R27 originates from the N1 near the Foreshore and runs north parallel to the N7, but nearer to the coast. It passes through the suburbs of Milnerton, Table View and Bloubergstrand and links the City to the West Coast, ending at the town of Velddrif. The R44 enters the east of the metro from the north, from Stellenbosch. It connects Stellenbosch to Somerset West, then crosses the N2 to Strand and Gordon's Bay. It exits the metro heading south hugging the coast, leading to the towns of Betty's Bay and Kleinmond.

Of the three-digit routes, the R300, is an expressway linking the N1 at Brackenfell to the N2 near Mitchells Plain and the Cape Town International Airport. The R302 runs from the R102 in Bellville, heading north across the N1 through Durbanville leaving the metro to Malmesbury. The R304 enters the northern limits of the metro from Stellenbosch, running NNW before veering west to cross the N7 at Philadelphia to end at Atlantis at a junction with the R307. This R307 starts north of Koeberg from the R27 and, after meeting the R304, continues north to Darling. The R310 originates from Muizenberg and runs along the coast, to the south of Mitchell's Plain and Khayelitsha, before veering north-east, crossing the N2 west of Macassar, and exiting the metro heading to Stellenbosch.

Cape Town, like most South African cities, uses Metropolitan or "M" routes for important intra-city routes, a layer below National (N) roads and Regional (R) routes. Each city's M roads are independently numbered. Most are at-grade roads. However, the M3 splits from the N2 and runs to the south along the eastern slopes of Table Mountain, connecting the City Bowl with Muizenberg. Except for a section between Rondebosch and Newlands that has at-grade intersections, this route is a highway. The M5 splits from the N1 further east than the M3, and links the Cape Flats to the CBD. It is a highway as far as the interchange with the M68 at Ottery, before continuing as an at-grade road.

Cape Town suffers from the worst traffic congestion in South Africa.

Golden Arrow Bus Services operates scheduled bus services in the Cape Town metropolitan area. Several companies run long-distance bus services from Cape Town to the other cities in South Africa.

Cape Town has a public transport system in about 10% of the city, running north to south along the west coastline of the city, comprising Phase 1 of the IRT system. This is known as the MyCiTi service.

MyCiTi Phase 1 includes services linking the Airport to the Cape Town inner city, as well as the following areas: Blouberg / Table View, Dunoon, Atlantis and Melkbosstrand, Milnerton, Paarden Eiland, Century City, Salt River and Walmer Estate, and all suburbs of the City Bowl and Atlantic Seaboard all the way to Llandudno and Hout Bay.

The MyCiTi N2 Express service consists of two routes each linking the Cape Town inner city and Khayelitsha and Mitchells Plain on the Cape Flats.

The service use high floor articulated and standard size buses in dedicated busways, low floor articulated and standard size buses on the N2 Express service, and smaller Optare buses in suburban and inner city areas. It offers universal access through level boarding and numerous other measures, and requires cashless fare payment using the EMV compliant smart card system, called myconnect. Headway of services (i.e. the time between buses on the same route) range from 3 mins to 20 mins in peak times to 60 minutes during quiet off-peak periods.

Cape Town has two kinds of taxis: metered taxis and minibus taxis. Unlike many cities, metered taxis are not allowed to drive around the city to solicit fares and instead must be called to a specific location.

Cape Town metered taxi cabs mostly operate in the city bowl, suburbs and Cape Town International Airport areas. Large companies that operate fleets of cabs can be reached by phone and are cheaper than the single operators that apply for hire from taxi ranks and Victoria and Alfred Waterfront. There are about one thousand meter taxis in Cape Town. Their rates vary from R8 per kilometre to about R15 per kilometre. The larger taxi companies in Cape Town are Excite Taxis, Cabnet and Intercab and single operators are reachable by cellular phone. The seven seated Toyota Avanza are the most popular with larger Taxi companies. Meter cabs are mostly used by tourists and are safer to use than minibus taxis.

Minibus taxis are the standard form of transport for the majority of the population who cannot afford private vehicles. Although essential, these taxis are often poorly maintained and are frequently not road-worthy. These taxis make frequent unscheduled stops to pick up passengers, which can cause accidents. With the high demand for transport by the working class of South Africa, minibus taxis are often filled over their legal passenger allowance. Minibuses are generally owned and operated in fleets.

Cape Town has seventeen active sister city agreements 



</doc>
<doc id="6654" url="https://en.wikipedia.org/wiki?curid=6654" title="Chicago Cubs">
Chicago Cubs

The Chicago Cubs are an American professional baseball team based in Chicago, Illinois. The Cubs compete in Major League Baseball (MLB) as a member club of the National League (NL) Central division. The team plays its home games at Wrigley Field, located on the city's North Side. The Cubs are one of two major league teams in Chicago; the other, the Chicago White Sox, is a member of the American League (AL) Central division. The Cubs, first known as the White Stockings, were a founding member of the NL in 1876, becoming the Chicago Cubs in 1903.

The Cubs have appeared in a total of eleven World Series. The 1906 Cubs won 116 games, finishing 116â36 and posting a modern-era record winning percentage of , before losing the World Series to the Chicago White Sox ("The Hitless Wonders") by four games to two. The Cubs won back-to-back World Series championships in 1907 and 1908, becoming the first major league team to play in three consecutive World Series, and the first to win it twice. Most recently, the Cubs won the 2016 National League Championship Series and 2016 World Series, which ended a 71-year National League pennant drought and a 108-year World Series championship drought, both of which are record droughts in Major League Baseball. The 108-year drought was also the longest such occurrence in all major North American sports. Since the start of divisional play in 1969, the Cubs have appeared in the postseason ten times through the 2019 season.

The Cubs are known as "the North Siders", a reference to the location of Wrigley Field within the city of Chicago, and in contrast to the White Sox, whose home field (Guaranteed Rate Field) is located on the South Side.

The Cubs began playing in 1870 as the Chicago White Stockings, joining the National League (NL) in 1876 as a charter member. Owner William Hulbert signed multiple star players, such as pitcher Albert Spalding and infielders Ross Barnes, Deacon White, and Adrian "Cap" Anson, to join the team prior to the N.L.'s first season. The White Stockings played their home games at West Side Grounds and quickly established themselves as one of the new league's top teams. Spalding won forty-seven games and Barnes led the league in hitting at .429 as Chicago won the first ever National League pennant, which at the time was the game's top prize.

After back-to-back pennants in 1880 and 1881, Hulbert died, and Spalding, who had retired to start Spalding sporting goods, assumed ownership of the club. The White Stockings, with Anson acting as player-manager, captured their third consecutive pennant in 1882, and Anson established himself as the game's first true superstar. In 1885 and 1886, after winning N.L. pennants, the White Stockings met the champions of the short-lived American Association in that era's version of a World Series. Both seasons resulted in matchups with the St. Louis Brown Stockings, with the clubs tying in 1885 and with St. Louis winning in 1886. This was the genesis of what would eventually become one of the greatest rivalries in sports. In all, the Anson-led Chicago Base Ball Club won six National League pennants between 1876 and 1886. As a result, Chicago's club nickname transitioned, and by 1890 they had become known as the Chicago Colts, or sometimes "Anson's Colts", referring to Cap's influence within the club. Anson was the first player in history credited with collecting 3,000 career hits. After a disappointing record of 59â73 and a ninth-place finish in 1897, Anson was released by the Cubs as both a player and manager. Due to Anson's absence from the club after 22 years, local newspaper reporters started to refer to the Colts as the "Orphans".

After the 1900 season, the American Base-Ball League formed as a rival professional league, and incidentally the club's old White Stockings nickname (eventually shortened to White Sox) would be adopted by a new American League neighbor to the south.

 In 1902, Spalding, who by this time had revamped the roster to boast what would soon be one of the best teams of the early century, sold the club to Jim Hart. The franchise was nicknamed the Cubs by the "Chicago Daily News" in 1902, although not officially becoming the Chicago Cubs until the 1907 season. During this period, which has become known as baseball's dead-ball era, Cub infielders Joe Tinker, Johnny Evers, and Frank Chance were made famous as a double-play combination by Franklin P. Adams' poem "Baseball's Sad Lexicon". The poem first appeared in the July 18, 1910 edition of the "New York Evening Mail". Mordecai "Three-Finger" Brown, Jack Taylor, Ed Reulbach, Jack Pfiester, and Orval Overall were several key pitchers for the Cubs during this time period. With Chance acting as player-manager from 1905 to 1912, the Cubs won four pennants and two World Series titles over a five-year span. Although they fell to the "Hitless Wonders" White Sox in the 1906 World Series, the Cubs recorded a record 116 victories and the best winning percentage (.763) in Major League history. With mostly the same roster, Chicago won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play three times in the Fall Classic and the first to win it twice. However, the Cubs would not win another World Series until 2016; this remains the longest championship drought in North American professional sports.
The next season, veteran catcher Johnny Kling left the team to become a professional pocket billiards player. Some historians think Kling's absence was significant enough to prevent the Cubs from also winning a third straight title in 1909, as they finished 6 games out of first place. When Kling returned the next year, the Cubs won the pennant again, but lost to the Philadelphia Athletics in the 1910 World Series.

In 1914, advertising executive Albert Lasker obtained a large block of the club's shares and before the 1916 season assumed majority ownership of the franchise. Lasker brought in a wealthy partner, Charles Weeghman, the proprietor of a popular chain of lunch counters who had previously owned the Chicago Whales of the short-lived Federal League. As principal owners, the pair moved the club from the West Side Grounds to the much newer Weeghman Park, which had been constructed for the Whales only two years earlier, where they remain to this day. The Cubs responded by winning a pennant in the war-shortened season of 1918, where they played a part in another team's curse: the Boston Red Sox defeated Grover Cleveland Alexander's Cubs four games to two in the 1918 World Series, Boston's last Series championship until 2004.

Beginning in 1916, Bill Wrigley of chewing-gum fame acquired an increasing quantity of stock in the Cubs. By 1921 he was the majority owner, maintaining that status into the 1930s.

Meanwhile, the year 1919 saw the start of the tenure of Bill Veeck, Sr. as team president. Veeck would hold that post throughout the 1920s and into the 30s. The management team of Wrigley and Veeck came to be known as the "double-Bills."

Near the end of the first decade of the double-Bills' guidance, the Cubs won the NL Pennant in 1929 and then achieved the unusual feat of winning a pennant every three years, following up the 1929 flag with league titles in 1932, 1935, and 1938. Unfortunately, their success did not extend to the Fall Classic, as they fell to their AL rivals each time. The '32 series against the Yankees featured Babe Ruth's "called shot" at Wrigley Field in game three. There were some historic moments for the Cubs as well; In 1930, Hack Wilson, one of the top home run hitters in the game, had one of the most impressive seasons in MLB history, hitting 56 home runs and establishing the current runs-batted-in record of 191. That 1930 club, which boasted six eventual hall of fame members (Wilson, Gabby Hartnett, Rogers Hornsby, George "High Pockets" Kelly, Kiki Cuyler and manager Joe McCarthy) established the current team batting average record of .309. In 1935 the Cubs claimed the pennant in thrilling fashion, winning a record 21 games in a row in September. The '38 club saw Dizzy Dean lead the team's pitching staff and provided a historic moment when they won a crucial late-season game at Wrigley Field over the Pittsburgh Pirates with a walk-off home run by Gabby Hartnett, which became known in baseball lore as "The Homer in the Gloamin'".

After the "Double-Bills" (Wrigley and Veeck) died in 1932 and 1933 respectively, P.K. Wrigley, son of Bill Wrigley, took over as majority owner. He was unable to extend his father's baseball success beyond 1938, and the Cubs slipped into years of mediocrity, although the Wrigley family would retain control of the team until 1981.

The Cubs enjoyed one more pennant at the close of World War II, finishing 98â56. Due to the wartime travel restrictions, the first three games of the 1945 World Series were played in Detroit, where the Cubs won two games, including a one-hitter by Claude Passeau, and the final four were played at Wrigley. The Cubs lost the series, and did not return until the 2016 World Series. After losing the 1945 World Series to the Detroit Tigers, the Cubs finished with a respectable 82â71 record in the following year, but this was only good enough for third place.

In the following two decades, the Cubs played mostly forgettable baseball, finishing among the worst teams in the National League on an almost annual basis. From 1947 to 1966, they only notched one winning season. Longtime infielder-manager Phil Cavarretta, who had been a key player during the 1945 season, was fired during spring training in 1954 after admitting the team was unlikely to finish above fifth place. Although shortstop Ernie Banks would become one of the star players in the league during the next decade, finding help for him proved a difficult task, as quality players such as Hank Sauer were few and far between. This, combined with poor ownership decisions such as the College of Coaches, and the ill-fated trade of future Hall of Fame member Lou Brock to the Cardinals for pitcher Ernie Broglio (who won only seven games over the next three seasons), hampered on-field performance.

The late-1960s brought hope of a renaissance, with third baseman Ron Santo, pitcher Ferguson Jenkins, and outfielder Billy Williams joining Banks. After losing a dismal 103 games in 1966, the Cubs brought home consecutive winning records in '67 and '68, marking the first time a Cub team had accomplished that feat in over two decades.

In the Cubs, managed by Leo Durocher, built a substantial lead in the newly created National League Eastern Division by mid-August. Ken Holtzman pitched a no-hitter on August 19, and the division lead grew to 8 games over the St. Louis Cardinals and by 9 games over the New York Mets. After the game of September 2, the Cubs record was 84â52 with the Mets in second place at 77â55. But then a losing streak began just as a Mets winning streak was beginning. The Cubs lost the final game of a series at Cincinnati, then came home to play the resurgent Pittsburgh Pirates (who would finish in third place). After losing the first two games by scores of 9â2 and 13â4, the Cubs led going into the ninth inning. A win would be a positive springboard since the Cubs were to play a crucial series with the Mets the next day. But Willie Stargell drilled a two-out, two-strike pitch from the Cubs' ace reliever, Phil Regan, onto Sheffield Avenue to tie the score in the top of the ninth. The Cubs would lose 7â5 in extra innings.[6]
Burdened by a four-game losing streak, the Cubs traveled to Shea Stadium for a short two-game set. The Mets won both games, and the Cubs left New York with a record of 84â58 just 1â2 game in front. More of the same followed in Philadelphia, as a 99 loss Phillies team nonetheless defeated the Cubs twice, to extend Chicago's losing streak to eight games. In a key play in the second game, on September 11, Cubs starter Dick Selma threw a surprise pickoff attempt to third baseman Ron Santo, who was nowhere near the bag or the ball. Selma's throwing error opened the gates to a Phillies rally.
After that second Philly loss, the Cubs were 84â60 and the Mets had pulled ahead at 85â57. The Mets would not look back. The Cubs' eight-game losing streak finally ended the next day in St. Louis, but the Mets were in the midst of a ten-game winning streak, and the Cubs, wilting from team fatigue, generally deteriorated in all phases of the game.[1] The Mets (who had lost a record 120 games 7 years earlier), would go on to win the World Series. The Cubs, despite a respectable 92â70 record, would be remembered for having lost a remarkable 17Â½ games in the standings to the Mets in the last quarter of the season.

Following the 1969 season, the club posted winning records for the next few seasons, but no playoff action. After the core players of those teams started to move on, the 70s got worse for the team, and they became known as "the Loveable Losers." In , the team found some life, but ultimately experienced one of its biggest collapses. The Cubs hit a high-water mark on June 28 at 47â22, boasting an game NL East lead, as they were led by Bobby Murcer (27 HR/89 RBI), and Rick Reuschel (20â10). However, the Philadelphia Phillies cut the lead to two by the All-star break, as the Cubs sat 19 games over .500, but they swooned late in the season, going 20â40 after July 31. The Cubs finished in fourth place at 81â81, while Philadelphia surged, finishing with 101 wins. The following two seasons also saw the Cubs get off to a fast start, as the team rallied to over 10 games above .500 well into both seasons, only to again wear down and play poorly later on, and ultimately settling back to mediocrity. This trait became known as the "June Swoon". Again, the Cubs' unusually high number of day games is often pointed to as one reason for the team's inconsistent late season play.

Wrigley died in 1977. The Wrigley family sold the team to the Chicago Tribune in 1981, ending a 65-year family relationship with the Cubs.

After over a dozen more subpar seasons, in 1981 the Cubs hired GM Dallas Green from Philadelphia to turn around the franchise. Green had managed the 1980 Phillies to the World Series title. One of his early GM moves brought in a young Phillies minor-league 3rd baseman named Ryne Sandberg, along with Larry Bowa for IvÃ¡n DeJesÃºs. The 1983 Cubs had finished 71â91 under Lee Elia, who was fired before the season ended by Green. Green continued the culture of change and overhauled the Cubs roster, front-office and coaching staff prior to 1984. Jim Frey was hired to manage the 1984 Cubs, with Don Zimmer coaching 3rd base and Billy Connors serving as pitching coach.

Green shored up the 1984 roster with a series of transactions. In December, 1983 Scott Sanderson was acquired from Montreal in a three-team deal with San Diego for Carmelo MartÃ­nez. Pinch hitter Richie Hebner (.333 BA in 1984) was signed as a free-agent. In spring training, moves continued: LF Gary Matthews and CF Bobby Dernier came from Philadelphia on March 26, for Bill Campbell and a minor leaguer. Reliever Tim Stoddard (10â6 3.82, 7 saves) was acquired the same day for a minor leaguer; veteran pitcher Ferguson Jenkins was released.

The team's commitment to contend was complete when Green made a midseason deal on June 15 to shore up the starting rotation due to injuries to Rick Reuschel (5â5) and Sanderson. The deal brought 1979 NL Rookie of the Year pitcher Rick Sutcliffe from the Cleveland Indians. Joe Carter (who was with the Triple-A Iowa Cubs at the time) and right fielder Mel Hall were sent to Cleveland for Sutcliffe and back-up catcher Ron Hassey (.333 with Cubs in 1984). Sutcliffe (5â5 with the Indians) immediately joined Sanderson (8â5 3.14), Eckersley (10â8 3.03), Steve Trout (13â7 3.41) and Dick Ruthven (6â10 5.04) in the starting rotation. Sutcliffe proceeded to go 16â1 for Cubs and capture the Cy Young Award.

The Cubs 1984 starting lineup was very strong. It consisted of LF Matthews (.291 14â82 101 runs 17 SB), C Jody Davis (.256 19â94), RF Keith Moreland (.279 16â80), SS Larry Bowa (.223 10 SB), 1B Leon "Bull" Durham (.279 23â96 16SB), CF Dernier (.278 45 SB), 3B Ron Cey (.240 25â97), Closer Lee Smith(9â7 3.65 33 saves) and 1984 NL MVP Ryne Sandberg (.314 19â84 114 runs, 19 triples,32 SB).

Reserve players Hebner, Thad Bosley, Henry Cotto, Hassey and Dave Owen produced exciting moments. The bullpen depth of Rich Bordi, George Frazier, Warren Brusstar and Dickie Noles did their job in getting the game to Smith or Stoddard.

At the top of the order, Dernier and Sandberg were exciting, aptly coined "the Daily Double" by Harry Caray. With strong defense â Dernier CF and Sandberg 2B, won the NL Gold Glove- solid pitching and clutch hitting, the Cubs were a well balanced team. Following the "Daily Double", Matthews, Durham, Cey, Moreland and Davis gave the Cubs an order with no gaps to pitch around. Sutcliffe anchored a strong top-to-bottom rotation, and Smith was one of the top closers in the game.

The shift in the Cubs' fortunes was characterized June 23 on the "NBC Saturday Game of the Week" contest against the St. Louis Cardinals; it has since been dubbed simply "The Sandberg Game." With the nation watching and Wrigley Field packed, Sandberg emerged as a superstar with not one, but two game-tying home runs against Cardinals closer Bruce Sutter. With his shots in the 9th and 10th innings Wrigley Field erupted and Sandberg set the stage for a comeback win that cemented the Cubs as the team to beat in the East. No one would catch them.

In early August the Cubs swept the Mets in a 4-game home series that further distanced them from the pack. An infamous Keith Moreland-Ed Lynch fight erupted after Lynch hit Moreland with a pitch, perhaps forgetting Moreland was once a linebacker at the University of Texas. It was the second game of a double header and the Cubs had won the first game in part due to a three run home run by Moreland. After the bench-clearing fight the Cubs won the second game, and the sweep put the Cubs at 68â45.

In 1984, each league had two divisions, East and West. The divisional winners met in a best-of-5 series to advance to the World Series, in a "2â3" format, first two games were played at the home of the team who did not have home field advantage. Then the last three games were played at the home of the team, with home field advantage. Thus the first two games were played at Wrigley Field and the next three at the home of their opponents, San Diego. A common and unfounded myth is that since Wrigley Field did not have lights at that time the National League decided to give the home field advantage to the winner of the NL West. In fact, home field advantage had rotated between the winners of the East and West since 1969 when the league expanded. In even numbered years, the NL West had home field advantage. In odd numbered years, the NL East had home field advantage. Since the NL East winners had had home field advantage in 1983, the NL West winners were entitled to it.

The confusion may stem from the fact that Major League Baseball did decide that, should the Cubs make it to the World Series, the American League winner would have home field advantage unless the Cubs hosted home games at an alternate site since the Cubs home field of Wrigley Field did not yet have lights. Rumor was the Cubs could hold home games across town at Comiskey Park, home of the American League's Chicago White Sox. Rather than hold any games in the cross town rival Sox Park, the Cubs made arrangements with the August A. Busch, owner of the St. Louis Cardinals, to use Busch Stadium in St. Louis as the Cubs "home field" for the World Series. This was approved by Major League Baseball and would have enabled the Cubs to host games 1 and 2, along with games 6 and 7 if necessary. At the time home field advantage was rotated between each league. Odd numbered years the AL had home field advantage. Even numbered years the NL had home field advantage. In the 1982 World Series the St. Louis Cardinals of the NL had home field advantage. In the 1983 World Series the Baltimore Orioles of the AL had home field advantage.

In the NLCS, the Cubs easily won the first two games at Wrigley Field against the San Diego Padres. The Padres were the winners of the Western Division with Steve Garvey, Tony Gwynn, Eric Show, Goose Gossage and Alan Wiggins. With wins of 13â0 and 4â2, the Cubs needed to win only one game of the next three in San Diego to make it to the World Series. After being beaten in Game 3 7â1, the Cubs lost Game 4 when Smith, with the game tied 5â5, allowed a game-winning home run to Garvey in the bottom of the ninth inning. In Game 5 the Cubs took a 3â0 lead into the 6th inning, and a 3â2 lead into the seventh with Sutcliffe (who won the Cy Young Award that year) still on the mound. Then, Leon Durham had a sharp grounder go under his glove. This critical error helped the Padres win the game 6â3, with a 4-run 7th inning and keep Chicago out of the 1984 World Series against the Detroit Tigers. The loss ended a spectacular season for the Cubs, one that brought alive a slumbering franchise and made the Cubs relevant for a whole new generation of Cubs fans.

The Padres would be defeated in 5 games by Sparky Anderson's Tigers in the World Series.

The 1985 season brought high hopes. The club started out well, going 35â19 through mid-June, but injuries to Sutcliffe and others in the pitching staff contributed to a 13-game losing streak that pushed the Cubs out of contention.

In 1989, the first full season with night baseball at Wrigley Field, Don Zimmer's Cubs were led by a core group of veterans in Ryne Sandberg, Rick Sutcliffe and Andre Dawson, who were boosted by a crop of youngsters such as Mark Grace, Shawon Dunston, Greg Maddux, Rookie of the Year Jerome Walton, and Rookie of the Year Runner-Up Dwight Smith. The Cubs won the NL East once again that season winning 93 games. This time the Cubs met the San Francisco Giants in the NLCS. After splitting the first two games at home, the Cubs headed to the Bay Area, where despite holding a lead at some point in each of the next three games, bullpen meltdowns and managerial blunders ultimately led to three straight losses. The Cubs couldn't overcome the efforts of Will Clark, whose home run off Maddux, just after a managerial visit to the mound, led Maddux to think Clark knew what pitch was coming. Afterward, Maddux would speak into his glove during any mound conversation, beginning what is a norm today. Mark Grace was 11â17 in the series with 8 RBI. Eventually, the Giants lost to the "Bash Brothers" and the Oakland A's in the famous ""Earthquake Series"."

The 1998 season would begin on a somber note with the death of legendary broadcaster Harry Caray. After the retirement of Sandberg and the trade of Dunston, the Cubs had holes to fill, and the signing of Henry RodrÃ­guez to bat cleanup provided protection for Sammy Sosa in the lineup, as Rodriguez slugged 31 round-trippers in his first season in Chicago. Kevin Tapani led the club with a career high 19 wins while Rod Beck anchored a strong bullpen and Mark Grace turned in one of his best seasons. The Cubs were swamped by media attention in 1998, and the team's two biggest headliners were Sosa and rookie flamethrower Kerry Wood. Wood's signature performance was one-hitting the Houston Astros, a game in which he tied the major league record of 20 strikeouts in nine innings. His torrid strikeout numbers earned Wood the nickname ""Kid K,"" and ultimately earned him the 1998 NL Rookie of the Year award. Sosa caught fire in June, hitting a major league record 20 home runs in the month, and his home run race with Cardinals slugger Mark McGwire transformed the pair into international superstars in a matter of weeks. McGwire finished the season with a new major league record of 70 home runs, but Sosa's .308 average and 66 homers earned him the National League MVP Award. After a down-to-the-wire Wild Card chase with the San Francisco Giants, Chicago and San Francisco ended the regular season tied, and thus squared off in a one-game playoff at Wrigley Field. Third baseman Gary Gaetti hit the eventual game winning homer in the playoff game. The win propelled the Cubs into the postseason for the first time since 1989 with a 90â73 regular season record. Unfortunately, the bats went cold in October, as manager Jim Riggleman's club batted .183 and scored only four runs en route to being swept by Atlanta in the National League Division Series. The home run chase between Sosa, McGwire and Ken Griffey, Jr. helped professional baseball to bring in a new crop of fans as well as bringing back some fans who had been disillusioned by the 1994 strike. The Cubs retained many players who experienced career years in 1998, but, after a fast start in 1999, they collapsed again (starting with being swept at the hands of the cross-town White Sox in mid-June) and finished in the bottom of the division for the next two seasons.

Despite losing fan favorite Grace to free agency and the lack of production from newcomer Todd Hundley, skipper Don Baylor's Cubs put together a good season in 2001. The season started with Mack Newton being brought in to preach "positive thinking." One of the biggest stories of the season transpired as the club made a midseason deal for Fred McGriff, which was drawn out for nearly a month as McGriff debated waiving his no-trade clause. The Cubs led the wild card race by 2.5 games in early September, but crumbled when Preston Wilson hit a three run walk off homer off of closer Tom "Flash" Gordon, which halted the team's momentum. The team was unable to make another serious charge, and finished at 88â74, five games behind both Houston and St. Louis, who tied for first. Sosa had perhaps his finest season and Jon Lieber led the staff with a 20-win season.

The Cubs had high expectations in 2002, but the squad played poorly. On July 5, 2002, the Cubs promoted assistant general manager and player personnel director Jim Hendry to the General Manager position. The club responded by hiring Dusty Baker and by making some major moves in 2003. Most notably, they traded with the Pittsburgh Pirates for outfielder Kenny Lofton and third baseman Aramis RamÃ­rez, and rode dominant pitching, led by Kerry Wood and Mark Prior, as the Cubs led the division down the stretch.

Chicago halted St. Louis' run to the playoffs by taking four of five games from the Cardinals at Wrigley Field in early September, after which they won their first division title in 14 years. They then went on to defeat the Atlanta Braves in a dramatic five-game Division Series, the franchise's first postseason series win since beating the Detroit Tigers in the 1908 World Series.

After losing an extra-inning game in Game 1, the Cubs rallied and took a three-games-to-one lead over the Wild Card Florida Marlins in the National League Championship Series. Florida shut the Cubs out in Game 5, but the Cubs returned home to Wrigley Field with young pitcher Mark Prior to lead the Cubs in Game 6 as they took a 3â0 lead into the 8th inning. It was at this point when a now-infamous incident took place. Several spectators attempted to catch a foul ball off the bat of Luis Castillo. A Chicago Cubs fan by the name of Steve Bartman, of Northbrook, Illinois, reached for the ball and deflected it away from the glove of MoisÃ©s Alou for the second out of the eighth inning. Alou reacted angrily toward the stands and after the game stated that he would have caught the ball. Alou at one point recanted, saying he would not have been able to make the play, but later said this was just an attempt to make Bartman feel better and believing the whole incident should be forgotten. Interference was not called on the play, as the ball was ruled to be on the spectator side of the wall. Castillo was eventually walked by Prior. Two batters later, and to the chagrin of the packed stadium, Cubs shortstop Alex Gonzalez misplayed an inning-ending double play, loading the bases. The error would lead to eight Florida runs and a Marlin victory. Despite sending Kerry Wood to the mound and holding a lead twice, the Cubs ultimately dropped Game 7, and failed to reach the World Series.

The "Steve Bartman incident" was seen as the "first domino" in the turning point of the era, and the Cubs did not win a playoff game for the next eleven seasons.

In 2004, the Cubs were a consensus pick by most media outlets to win the World Series. The offseason acquisition of Derek Lee (who was acquired in a trade with Florida for Hee-seop Choi) and the return of Greg Maddux only bolstered these expectations. Despite a mid-season deal for Nomar Garciaparra, misfortune struck the Cubs again. They led the Wild Card by 1.5 games over San Francisco and Houston on September 25. On that day, both teams lost, giving the Cubs a chance at increasing the lead to 2.5 games with only eight games remaining in the season, but reliever LaTroy Hawkins blew a save to the Mets, and the Cubs lost the game in extra innings. The defeat seemingly deflated the team, as they proceeded to drop six of their last eight games as the Astros won the Wild Card.

Despite the fact that the Cubs had won 89 games, this fallout was decidedly unlovable, as the Cubs traded superstar Sammy Sosa after he had left the season's final game early and then lied about it publicly. Already a controversial figure in the clubhouse after his corked-bat incident, Sammy's actions alienated much of his once strong fan base as well as the few teammates still on good terms with him, (many teammates grew tired of Sosa playing loud salsa music in the locker room) and possibly tarnished his place in Cubs' lore for years to come. The disappointing season also saw fans start to become frustrated with the constant injuries to ace pitchers Mark Prior and Kerry Wood. Additionally, the 2004 season led to the departure of popular commentator Steve Stone, who had become increasingly critical of management during broadcasts and was verbally attacked by reliever Kent Mercker. Things were no better in 2005, despite a career year from first baseman Derrek Lee and the emergence of closer Ryan Dempster. The club struggled and suffered more key injuries, only managing to win 79 games after being picked by many to be a serious contender for the N.L. pennant. In 2006, bottom fell out as the Cubs finished 66â96, last in the NL Central.

After finishing last in the NL Central with 66 wins in 2006, the Cubs re-tooled and went from "worst to first" in 2007. In the offseason they signed Alfonso Soriano to a contract at eight years for $136 million, and replaced manager Dusty Baker with fiery veteran manager Lou Piniella. After a rough start, which included a brawl between Michael Barrett and Carlos Zambrano, the Cubs overcame the Milwaukee Brewers, who had led the division for most of the season. The Cubs traded Barrett to the Padres, and later acquired catcher Jason Kendall from Oakland. Kendall was highly successful with his management of the pitching rotation and helped at the plate as well. By September, Geovany Soto became the full-time starter behind the plate, replacing the veteran Kendall. Winning streaks in June and July, coupled with a pair of dramatic, late-inning wins against the Reds, led to the Cubs ultimately clinching the NL Central with a record of 85â77. They met Arizona in the NLDS, but controversy followed as Piniella, in a move that has since come under scrutiny, pulled Carlos Zambrano after the sixth inning of a pitcher's duel with D-Backs ace Brandon Webb, to "...save Zambrano for (a potential) Game 4." The Cubs, however, were unable to come through, losing the first game and eventually stranding over 30 baserunners in a three-game Arizona sweep.

The Tribune company, in financial distress, was acquired by real-estate mogul Sam Zell in December 2007. This acquisition included the Cubs. However, Zell did not take an active part in running the baseball franchise, instead concentrating on putting together a deal to sell it.

The Cubs successfully defended their National League Central title in 2008, going to the postseason in consecutive years for the first time since 1906â08. The offseason was dominated by three months of unsuccessful trade talks with the Orioles involving 2B Brian Roberts, as well as the signing of Chunichi Dragons star Kosuke Fukudome. The team recorded their 10,000th win in April, while establishing an early division lead. Reed Johnson and Jim Edmonds were added early on and Rich Harden was acquired from the Oakland Athletics in early July. The Cubs headed into the All-Star break with the N.L.'s best record, and tied the league record with eight representatives to the All-Star game, including catcher Geovany Soto, who was named Rookie of the Year. The Cubs took control of the division by sweeping a four-game series in Milwaukee. On September 14, in a game moved to Miller Park due to Hurricane Ike, Zambrano pitched a no-hitter against the Astros, and six days later the team clinched by beating St. Louis at Wrigley. The club ended the season with a 97â64 record and met Los Angeles in the NLDS. The heavily favored Cubs took an early lead in Game 1, but James Loney's grand slam off Ryan Dempster changed the series' momentum. Chicago committed numerous critical errors and were outscored 20â6 in a Dodger sweep, which provided yet another sudden ending.

The Ricketts family acquired a majority interest in the Cubs in 2009, ending the Tribune years. Apparently handcuffed by the Tribune's bankruptcy and the sale of the club to the Ricketts siblings, led by chairman Thomas S. Ricketts, the Cubs' quest for a NL Central three-peat started with notice that there would be less invested into contracts than in previous years. Chicago engaged St. Louis in a see-saw battle for first place into August 2009, but the Cardinals played to a torrid 20â6 pace that month, designating their rivals to battle in the Wild Card race, from which they were eliminated in the season's final week. The Cubs were plagued by injuries in 2009, and were only able to field their Opening Day starting lineup three times the entire season. Third baseman Aramis RamÃ­rez injured his throwing shoulder in an early May game against the Milwaukee Brewers, sidelining him until early July and forcing journeyman players like Mike Fontenot and Aaron Miles into more prominent roles. Additionally, key players like Derrek Lee (who still managed to hit .306 with 35 HR and 111 RBI that season), Alfonso Soriano, and Geovany Soto also nursed nagging injuries. The Cubs posted a winning record (83â78) for the third consecutive season, the first time the club had done so since 1972, and a new era of ownership under the Ricketts family was approved by MLB owners in early October.

Rookie Starlin Castro debuted in early May (2010) as the starting shortstop. However, the club played poorly in the early season, finding themselves 10 games under .500 at the end of June. In addition, long-time ace Carlos Zambrano was pulled from a game against the White Sox on June 25 after a tirade and shoving match with Derrek Lee, and was suspended indefinitely by Jim Hendry, who called the conduct "unacceptable." On August 22, Lou Piniella, who had already announced his retirement at the end of the season, announced that he would leave the Cubs prematurely to take care of his sick mother. Mike Quade took over as the interim manager for the final 37 games of the year. Despite being well out of playoff contention the Cubs went 24â13 under Quade, the best record in baseball during that 37 game stretch, earning Quade the manager position going forward on October 19.

On December 3, 2010, Cubs broadcaster and former third baseman, Ron Santo, died due to complications from bladder cancer and diabetes. He spent 13 seasons as a player with the Cubs, and at the time of his death was regarded as one of the greatest players not in the Hall of Fame. He was posthumously elected to the Major League Baseball Hall of Fame in 2012.

Despite trading for pitcher Matt Garza and signing free-agent slugger Carlos PeÃ±a, the Cubs finished the 2011 season 20 games under .500 with a record of 71â91. Weeks after the season came to an end, the club was rejuvenated in the form of a new philosophy, as new owner Tom Ricketts signed Theo Epstein away from the Boston Red Sox, naming him club President and giving him a five-year contract worth over $18 million, and subsequently discharged manager Mike Quade. Epstein, a proponent of sabremetrics and one of the architects of the 2004 and 2007 World Series championships in Boston, brought along Jed Hoyer from the Padres to fill the role of GM and hired Dale Sveum as manager. Although the team had a dismal 2012 season, losing 101 games (the worst record since 1966), it was largely expected. The youth movement ushered in by Epstein and Hoyer began as longtime fan favorite Kerry Wood retired in May, followed by Ryan Dempster and Geovany Soto being traded to Texas at the All-Star break for a group of minor league prospects headlined by Christian Villanueva, but also included little thought of Kyle Hendricks. The development of Castro, Anthony Rizzo, Darwin Barney, Brett Jackson and pitcher Jeff Samardzija, as well as the replenishing of the minor-league system with prospects such as Javier Baez, Albert Almora, and Jorge Soler became the primary focus of the season, a philosophy which the new management said would carry over at least through the 2013 season.

The 2013 season resulted in much as the same the year before. Shortly before the trade deadline, the Cubs traded Matt Garza to the Texas Rangers for Mike Olt, Carl Edwards Jr, Neil Ramirez, and Justin Grimm. Three days later, the Cubs sent Alfonso Soriano to the New York Yankees for minor leaguer Corey Black. The mid season fire sale led to another last place finish in the NL Central, finishing with a record of 66â96. Although there was a five-game improvement in the record from the year before, Anthony Rizzo and Starlin Castro seemed to take steps backward in their development. On September 30, 2013, Theo Epstein made the decision to fire manager Dale Sveum after just two seasons at the helm of the Cubs. The regression of several young players was thought to be the main focus point, as the front office said Sveum would not be judged based on wins and losses. In two seasons as skipper, Sveum finished with a record of 127â197.

The 2013 season was also notable as the Cubs drafted future Rookie of the Year and MVP Kris Bryant with the second overall selection.

On November 7, 2013, the Cubs hired San Diego Padres bench coach Rick Renteria to be the 53rd manager in team history. The Cubs finished the 2014 season in last place with a 73â89 record in RenterÃ­a's first and only season as manager. Despite the poor record, the Cubs improved in many areas during 2014, including rebound years by Anthony Rizzo and Starlin Castro, ending the season with a winning record at home for the first time since 2009, and compiling a 33â34 record after the All-Star Break. However, following unexpected availability of Joe Maddon when he exercised a clause that triggered on October 14 with the departure of General Manager Andrew Friedman to the Los Angeles Dodgers, the Cubs relieved RenterÃ­a of his managerial duties on October 31, 2014. During the season, the Cubs drafted Kyle Schwarber with the fourth overall selection.

Hall of Famer Ernie Banks died of a heart attack on January 23, 2015, shortly before his 84th birthday. The 2015 uniform carried a commemorative #14 patch on both its home and away jerseys in his honor.

On November 2, 2014, the Cubs announced that Joe Maddon had signed a five-year contract to be the 54th manager in team history. On December 10, 2014, Maddon announced that the team had signed free agent Jon Lester to a six-year, $155 million contract. Many other trades and acquisitions occurred during the off season. The opening day lineup for the Cubs contained five new players including center fielder Dexter Fowler. Rookies Kris Bryant and Addison Russell were in the starting lineup by mid-April, and rookie Kyle Schwarber was added in mid-June. On August 30, Jake Arrieta threw a no hitter against the Los Angeles Dodgers. The Cubs finished the 2015 season in third place in the NL Central, with a record of 97â65, the third best record in the majors and earned a wild card berth. On October 7, in the 2015 National League Wild Card Game, Arrieta pitched a complete game shutout and the Cubs defeated the Pittsburgh Pirates 4â0.

The Cubs defeated the Cardinals in the NLDS three-games-to-one, qualifying for a return to the NLCS for the first time in 12 years, where they faced the New York Mets. This was the first time in franchise history that the Cubs had clinched a playoff series at Wrigley Field. However, they were swept in four games by the Mets and were unable to make it to their first World Series since 1945.

Before the season, in an effort to shore up their lineup, free agents Ben Zobrist, Jason Heyward and John Lackey were signed. To make room for the Zobrist signing, Starlin Castro was traded to the Yankees for Adam Warren and Brendan Ryan, the latter of whom was released a week later. Also during the middle of the season, the Cubs traded their top prospect Gleyber Torres for Aroldis Chapman.
In a season that included a no-hitter on April 21 by Jake Arrieta, the Cubs finished with the best record in Major League Baseball and won their first National League Central title since the 2008 season, winning by 17.5 games. The team also reached the 100-win mark for the first time since 1935 and won 103 total games, the most wins for the franchise since 1910. The Cubs defeated the San Francisco Giants in the National League Division Series and returned to the National League Championship Series for the second year in a row, where they defeated the Los Angeles Dodgers in six games. This was their first NLCS win since the series was created in 1969. The win earned the Cubs their first World Series appearance since 1945 and a chance for their first World Series win since 1908. Coming back from a three-games-to-one deficit, the Cubs defeated the Cleveland Indians in seven games in the 2016 World Series, They were the first team to come back from a three-games-to-one deficit since the Kansas City Royals in 1985. On November 4, the city of Chicago held a victory parade and rally for the Cubs that began at Wrigley Field, headed down Lake Shore Drive, and ended in Grant Park. The city estimated that over five million people attended the parade and rally, which made it one of the largest recorded gatherings in history.
In an attempt to be the first team to repeat as World Series champions since the Yankees in 1998, 1999, and 2000, the Cubs struggled for most of the first half of the 2017 season, never moving more than four games over .500 and finishing the first half two games under .500. On July 15, the Cubs fell to a season-high 5.5 games out of first in the NL Central. The Cubs struggled mainly due to their pitching as Jake Arrieta and Jon Lester struggled and no starting pitcher managed to win more than 14 games (four pitchers won 15 games or more for the Cubs in 2016). The Cub offense also struggled as Kyle Schwarber batted near .200 for most of the first half and was even sent to the minors. However, the Cubs recovered in the second half of the season to finish 22 games over .500 and win the NL Central by six games over the Milwaukee Brewers. The Cubs pulled out a five-game NLDS series win over the Washington Nationals to advance to the NLCS for the third consecutive year. For the second consecutive year, they faced the Dodgers. This time, however, the Dodgers defeated the Cubs in five games. In May 2017, the Cubs and the Rickets family formed Marquee Sports & Entertainment as a central sales and marketing company for the various Rickets family sports and entertainment assets: the Cubs, Wrigley Rooftops and Hickory Street Capital.
Prior to the 2018 season, the Cubs made several key free agent signings to bolster their pitching staff. The team signed starting pitcher Yu Darvish to a six-year, $126 million contract and veteran closer Brandon Morrow to two-year, $21-million contract, in addition to Tyler Chatwood and Steve Cishek. However, the Cubs struggled to stay healthy throughout the season. Anthony Rizzo missed much of April due to a back injury, and Bryant missed almost a month due to shoulder injury. However, Darvish, who only started eight games in 2018, was lost for the season due to elbow and triceps injuries. Morrow also faced two injuries before the team ruled him out for the season in September. The team maintained first place in their division for much of the season. The injury-depleted team only went 16â11 during September, which allowed the Milwaukee Brewers, to finish with the same record. The Brewers defeated the Cubs in a tie-breaker game to win the Central Division and secure the top-seed in the National League. The Cubs subsequently lost to the Colorado Rockies in the 2018 National League Wild Card Game for their earliest playoff exit in three seasons.
The Cubs' roster remained largely intact going into the 2019 season. The team led the Central Division by a half-game over the Brewers at the All-Star Break. However, the teamâs control over the division once again dissipated going into final months of the season. The Cubs lost several key players to injuries, including Javier BÃ¡ez, Anthony Rizzo, and Kris Bryant during this stretch. The team's postseason chances were compromised after suffering a nine-game losing streak in late September. The Cubs were eliminated from playoff contention on September 25, marking the first time the team had failed to qualify for the playoffs since 2014. The Cubs announced they would not renew manager Joe Maddon's contract at the end of the season. On October 24, 2019, the Cubs hired David Ross as their new manager.

The Cubs have played their home games at Wrigley Field, also known as ""The Friendly Confines"" since 1916. It was built in 1914 as Weeghman Park for the Chicago Whales, a Federal League baseball team. The Cubs also shared the park with the Chicago Bears of the NFL for 50 years. The ballpark includes a manual scoreboard, ivy-covered brick walls, and relatively small dimensions.

Located in Chicago's Lake View neighborhood, Wrigley Field sits on an irregular block bounded by Clark and Addison Streets and Waveland and Sheffield Avenues. The area surrounding the ballpark is typically referred to as Wrigleyville. There is a dense collection of sports bars and restaurants in the area, most with baseball inspired themes, including Sluggers, Murphy's Bleachers and The Cubby Bear. Many of the apartment buildings surrounding Wrigley Field on Waveland and Sheffield Avenues have built bleachers on their rooftops for fans to view games and other sell space for advertisement. One building on Sheffield Avenue has a sign atop its roof which says "Eamus Catuli!" which is Latin for "Let's Go Cubs!" and another chronicles the time since the last Division title, pennant, and World Series championship. The 00 denotes the 2016 NL Central title, NL pennant, and the World Series championship. On game days, many residents rent out their yards and driveways to people looking for parking spots. The uniqueness of the neighborhood itself has ingrained itself into the culture of the Chicago Cubs as well as the Wrigleyville neighborhood, and has led to being used for concerts and other sporting events, such as the 2010 NHL Winter Classic between the Chicago Blackhawks and Detroit Red Wings, as well as a 2010 NCAA men's football game between the Northwestern Wildcats and Illinois Fighting Illini.

In 2013, Tom Ricketts and team president Crane Kenney unveiled plans for a five-year, $575 million privately funded renovation of Wrigley Field. Called the 1060 Project, the proposed plans included vast improvements to the stadium's facade, infrastructure, restrooms, concourses, suites, press box, bullpens, and clubhouses, as well as a jumbotron to be added in the left field bleachers, batting tunnels, a video board in right field, and, eventually, an adjacent hotel, plaza, and office-retail complex. In previous years mostly all efforts to conduct any large-scale renovations to the field had been opposed by the city, former mayor Richard M. Daley (a staunch White Sox fan), and especially the rooftop owners.

Months of negotiations between the team, a group of rooftop properties investors, local Alderman Tom Tunney, and Chicago Mayor Rahm Emanuel followed with the eventual endorsements of the city's Landmarks Commission, the Plan Commission and final approval by the Chicago City Council in July 2013. The project began at the conclusion of the 2014 season.

The "Bleacher Bums" is a name given to fans, many of whom spend much of the day heckling, who sit in the bleacher section at Wrigley Field. Initially, the group was called "bums" because they attended most of the games, and as Wrigley did not yet have lights, these were all day games, so it was jokingly presumed these fans were jobless. A Broadway play, starring Joe Mantegna, Dennis Farina, Dennis Franz, and James Belushi ran for years and was based on a group of Cub fans who frequented the club's games. The group was started in 1967 by dedicated fans Ron Grousl, Tom Nall and "mad bugler" Mike Murphy, who was a sports radio host during mid days on Chicago-based WSCR AM 670 "The Score". Murphy alleges that Grousl started the Wrigley tradition of throwing back opposing teams' home run balls.

Beginning in the days of P.K. Wrigley and the 1937 bleacher/scoreboard reconstruction, and prior to modern media saturation, a flag with either a "W" or an "L" has flown from atop the scoreboard masthead, indicating the day's result(s) when baseball was played at Wrigley. In case of a split doubleheader, both the "W" and "L" flags are flown.

Past Cubs media guides show that originally the flags were blue with a white "W" and white with a blue "L". In 1978, consistent with the dominant colors of the flags, blue and white lights were mounted atop the scoreboard, denoting "win" and "loss" respectively for the benefit of nighttime passers-by.

The flags were replaced by 1990, the first year in which the Cubs media guide reports the switch to the now familiar colors of the flags: White with blue "W" and blue with white "L". In addition to needing to replace the worn-out flags, by then the retired numbers of Banks and Williams were flying on the foul poles, as white with blue numbers; so the "good" flag was switched to match that scheme.

This long-established tradition has evolved to fans carrying the white-with-blue-W flags to both home and away games, and displaying them after a Cub win. The flags are known as the Cubs Win Flag. The flags have become more and more popular each season since 1998, and are now even sold as T-shirts with the same layout. In 2009, the tradition spilled over to the NHL as Chicago Blackhawks fans adopted a red and black "W" flag of their own.

During the early and mid-2000s, Chip Caray usually declared that a Cubs win at home meant it was ""White flag time at Wrigley!"" More recently, the Cubs have promoted the phrase "Fly the W!" among fans and on social media.
The official Cubs team mascot is a young bear cub, named Clark, described by the team's press release as a young and friendly Cub. Clark made his debut at Advocate Health Care on January 13, 2014, the same day as the press release announcing his installation as the club's first ever official physical mascot. The bear cub itself was used in the clubs since the early 1900s and was the inspiration of the Chicago Staleys changing their team's name to the Chicago Bears, because the Cubs allowed the bigger football playersâlike bears to cubsâto play at Wrigley Field in the 1930s.

The Cubs had no official physical mascot prior to Clark, though a man in a 'polar bear' looking outfit, called "The Bear-man" (or Beeman), which was mildly popular with the fans, paraded the stands briefly in the early 1990s. There is no record of whether or not he was just a fan in a costume or employed by the club. Through the 2013 season, there were "Cubbie-bear" mascots outside of Wrigley on game day, but none were employed by the team. They pose for pictures with fans for tips. The most notable of these was "Billy Cub" who worked outside of the stadium for over six years until July 2013, when the club asked him to stop. Billy Cub, who is played by fan John Paul Weier, had unsuccessfully petitioned the team to become the official mascot.

Another unofficial but much more well-known mascot is Ronnie "Woo Woo" Wickers who is a longtime fan and local celebrity in the Chicago area. He is known to Wrigley Field visitors for his idiosyncratic cheers at baseball games, generally punctuated with an exclamatory "Woo!" (e.g., "Cubs, woo! Cubs, woo! Big-Z, woo! Zambrano, woo! Cubs, woo!") Longtime Cubs announcer Harry Caray dubbed Wickers "Leather Lungs" for his ability to shout for hours at a time. He is not employed by the team, although the club has on two separate occasions allowed him into the broadcast booth and allow him some degree of freedom once he purchases or is given a ticket by fans to get into the games. He is largely allowed to roam the park and interact with fans by Wrigley Field security.

During the summer of 1969, a Chicago studio group produced a single record called "Hey Hey! Holy Mackerel! (The Cubs Song)" whose title and lyrics incorporated the catch-phrases of the respective TV and radio announcers for the Cubs, Jack Brickhouse and Vince Lloyd. Several members of the Cubs recorded an album called "Cub Power" which contained a cover of the song. The song received a good deal of local airplay that summer, associating it very strongly with that bittersweet season. It was played much less frequently thereafter, although it remained an unofficial Cubs theme song for some years after.

For many years, Cubs radio broadcasts started with "It's a Beautiful Day for a Ball Game" by the Harry Simeone Chorale. In 1979, Roger Bain released a 45 rpm record of his song "Thanks Mr. Banks", to honor "Mr. Cub" Ernie Banks.

The song "Go, Cubs, Go!" by Steve Goodman was recorded early in the 1984 season, and was heard frequently during that season. Goodman died in September of that year, four days before the Cubs clinched the National League Eastern Division title, their first title in 39 years. Since 1984, the song started being played from time to time at Wrigley Field; since 2007, the song has been played over the loudspeakers following each Cubs home victory.

The Mountain Goats recorded a song entitled "Cubs in Five" on its 1995 EP Nine Black Poppies which refers to the seeming impossibility of the Cubs winning a World Series in both its title and Chorus.

In 2007, Pearl Jam frontman Eddie Vedder composed a song dedicated to the team called "All the Way". Vedder, a Chicago native, and lifelong Cubs fan, composed the song at the request of Ernie Banks.
Pearl Jam has played this song live multiple times several of which occurring at Wrigley Field. Eddie Vedder has played this song live twice, at his solo shows at the Chicago Auditorium on August 21 and 22, 2008.

An album entitled "Take Me Out to a Cubs Game" was released in 2008. It is a collection of 17 songs and other recordings related to the team, including Harry Caray's final performance of "Take Me Out to the Ball Game" on September 21, 1997, the Steve Goodman song mentioned above, and a newly recorded rendition of "Talkin' Baseball" (subtitled "Baseball and the Cubs") by Terry Cashman. The album was produced in celebration of the 100th anniversary of the Cubs' 1908 World Series victory and contains sounds and songs of the Cubs and Wrigley Field.

Season 1 Episode 3 of the American television show "" ("They Have Been, They Are, They Will Be...") is supposed to take place during a 1974 World Series matchup between the Chicago Cubs and the Boston Red Sox.

The 1986 film "Ferris Bueller's Day Off" showed a game played by the Cubs when Ferris' principal goes to a bar looking for him.

The 1989 film "Back to the Future Part II" depicts the Chicago Cubs defeating a baseball team from Miami in the 2015 World Series, ending the longest championship drought in all four of the major North American professional sports leagues. In 2015, the Miami Marlins failed to make the playoffs but the Cubs were able to make it to the 2015 National League Wild Card round and move on to the 2015 National League Championship Series by October 21, 2015, the date where protagonist Marty McFly traveled to the future in the film. However, it was on October 21 that the Cubs were swept by the New York Mets in the NLCS.

The 1993 film "Rookie of the Year", directed by Daniel Stern, centers on the Cubs as a team going nowhere into August when the team chances upon 12-year-old Cubs fan Henry Rowengartner (Thomas Ian Nicholas), whose right (throwing) arm tendons have healed tightly after a broken arm and granted him the ability to regularly pitch at speeds in excess of . Following the Cubs' win over the Cleveland Indians in Game 7 of the 2016 World Series, Nicholas, in celebration, tweeted the final shot from the movie: Henry holding his fist up to the camera to show a Cubs World Series ring.

""Baseball's Sad Lexicon,"" also known as ""Tinker to Evers to Chance"" after its refrain, is a 1910 baseball poem by Franklin Pierce Adams. The poem is presented as a single, rueful stanza from the point of view of a New York Giants fan seeing the talented Chicago Cubs infield of shortstop Joe Tinker, second baseman Johnny Evers, and first baseman Frank Chance complete a double play. The trio began playing together with the Cubs in 1902, and formed a double play combination that lasted through April 1912. The Cubs won the pennant four times between 1906 and 1910, often defeating the Giants en route to the World Series.

The poem was first published in the "New York Evening Mail" on July 12, 1912. Popular among sportswriters, numerous additional verses were written. The poem gave Tinker, Evers, and Chance increased popularity and has been credited with their elections to the National Baseball Hall of Fame in 1946.


Throughout the history of the Chicago Cubs' franchise, 15 different Cubs pitchers have pitched no-hitters; however, no Cubs pitcher has thrown a perfect game.

As of 2018, the Chicago Cubs are ranked as the 16th most valuable sports team in the world, 12th in the United States, third in MLB behind the New York Yankees and the Los Angeles Dodgers, and first in the city of Chicago.

The Chicago Cubs retired numbers are commemorated on pinstriped flags flying from the foul poles at Wrigley Field, with the exception of Jackie Robinson, the Brooklyn Dodgers player whose number 42 was retired for all clubs. The first retired number flag, Ernie Banks' number 14, was raised on the left field pole, and they have alternated since then. 14, 10 and 31 (Jenkins) fly on the left field pole; and 26, 23 and 31 (Maddux) fly on the right field pole.

<nowiki>*</nowiki> Robinson's number was retired by all MLB clubs.

The Chicago Cubs farm system consists of nine minor league affiliates.

Before signing a developmental agreement with the Kane County Cougars in 2012, the Cubs had a Class A minor league affiliation on two occasions with the Peoria Chiefs (1985â1995 and 2004â2012). Ryne Sandberg managed the Chiefs from 2006 to 2010. In the period between those associations with the Chiefs, the club had affiliations with the Dayton Dragons and Lansing Lugnuts. The Lugnuts were often affectionately referred to by Chip Caray as "Steve Stone's favorite team." The 2007 developmental contract with the Tennessee Smokies was preceded by Double-A affiliations with the Orlando Cubs and West Tenn Diamond Jaxx. On September 16, 2014, the Cubs announced a move of their top Class A affiliate from Daytona in the Florida State League to Myrtle Beach in the Carolina League for the 2015 season. Two days later, the Cubs signed a four-year player development contract with the South Bend Silver Hawks of the Midwest League, ending their brief relationship with the Kane County Cougars and shortly thereafter renaming the Silver Hawks the South Bend Cubs.

The Chicago White Stockings, (today's Chicago Cubs), began spring training in Hot Springs, Arkansas in 1886. President Albert Spalding (founder of Spalding Sporting Goods) and player/manager Cap Anson brought their players to Hot Springs and played at the Hot Springs Baseball Grounds. The concept was for the players to have training and fitness before the start of the regular season, utilizing the bath houses of Hot Springs after practices. After the White Stockings had a successful season in 1886, winning the National League Pennant, other teams began bringing their players to Hot Springs for "spring training". The Chicago Cubs, St. Louis Browns, New York Yankees, St. Louis Cardinals, Cleveland Spiders, Detroit Tigers, Pittsburgh Pirates, Cincinnati Reds, New York Highlanders, Brooklyn Dodgers and Boston Red Sox were among the early squads to arrive. Whittington Park (1894) and later Majestic Park (1909) and Fogel Field (1912) were all built in Hot Springs specifically to host Major League teams.

The Cubs' current spring training facility is located in Sloan Park in Mesa, Arizona, where they play in the Cactus League. The park seats 15,000, making it Major League baseball's largest spring training facility by capacity. The Cubs annually sell out most of their games both at home and on the road. Before Sloan Park opened in 2014, the team played games at HoHoKam Park â Dwight Patterson Field from 1979. "HoHoKam" is literally translated from Native American as "those who vanished." The North Siders have called Mesa their spring home for most seasons since 1952.

In addition to Mesa, the club has held spring training in Hot Springs, Arkansas (1886, 1896â1900), (1909â1910) New Orleans (1870, 1907, 1911â1912); Champaign, Illinois (1901â02, 1906); Los Angeles (1903â04, 1948â1949), Santa Monica, California (1905); French Lick, Indiana (1908, 1943â1945); Tampa, Florida (1913â1916); Pasadena, California (1917â1921); Santa Catalina Island, California (1922â1942, 1946â1947, 1950â1951); Rendezvous Park in Mesa (1952â1965); Blair Field in Long Beach, California (1966); and Scottsdale, Arizona (1967â1978).

The curious location on Catalina Island stemmed from Cubs owner William Wrigley Jr.'s then-majority interest in the island in 1919. Wrigley constructed a ballpark on the island to house the Cubs in spring training: it was built to the same dimensions as Wrigley Field. The ballpark was called Wrigley Field of Avalon. (The ballpark is long gone, but a clubhouse built by Wrigley to house the Cubs exists as the Catalina County Club.) However, by 1951 the team chose to leave Catalina Island and spring training was shifted to Mesa, Arizona. The Cubs' 30-year association with Catalina is chronicled in the book, "The Cubs on Catalina", by Jim Vitti, which was named International 'Book of the Year' by "The Sporting News". The Cubs left Catalina after some bad weather in 1951, choosing to move to Mesa, a city where the Wrigleys also had interests. Today, there is an exhibit at the Catalina Museum dedicated to the Cubs' spring training on the island.

The former location in Mesa is actually the second HoHoKam Park; the first was built in 1976 as the spring-training home of the Oakland Athletics who left the park in 1979. Apart from HoHoKam Park and Sloan Park the Cubs also have another Mesa training facility called Fitch Park, this complex provides of team facilities, including major league clubhouse, four practice fields, one practice infield, enclosed batting tunnels, batting cages, a maintenance facility, and administrative offices for the Cubs.

Cubs radio rights are held by Entercom; its acquisition of the radio rights effective 2015 (under CBS Radio) ended the team's 90-year association with 720 WGN. During the first season of the contract, Cubs games aired on WBBM, taking over as flagship of the Chicago Cubs Radio Network. On November 11, 2015, CBS announced that the Cubs would move to WBBM's all-sports sister station, WSCR, beginning in the 2016 season. The move was enabled by WSCR's end of their rights agreement for the White Sox, who moved to WLS.

The play-by-play voice of the Cubs is Pat Hughes, who has held the position since 1996, joined by Ron Coomer. Former Cubs third baseman and fan favorite Ron Santo had been Hughes' long-time partner until his death in 2010. Keith Moreland replaced Hall of Fame inductee Santo for three seasons, followed by Coomer for the 2014 season.

The club also produces its own print media; the Cubs' official magazine "Vineline", which has 12 annual issues, is in its third decade, and spotlights players and events involving the club. The club also publishes a traditional media guide.

As of the 2020 season, all Cubs games not aired on broadcast television will air on Marquee Sports Network, a joint venture between the team and Sinclair Broadcast Group. The venture was officially announced in February 2019.

WGN-TV had a long-term association with the team, having aired Cubs games via its WGN Sports department from its establishment in 1948, through the 2019 season. For a period, WGN's Cubs games aired nationally on WGN America (formerly Superstation WGN); however, prior to the 2015 season, the Cubs, as well as all other Chicago sports programming, was dropped from the channel as part of its re-positioning as a general entertainment cable channel. To compensate, all games carried by over-the-air channels were syndicated to a network of other television stations within the Cubs' market, which includes Illinois and parts of Indiana and Iowa. Due to limits on program pre-emptions imposed by WGN's former affiliations with The WB and its successor The CW, WGN occasionally sub-licensed some of its sports broadcasts to another station in the market, particularly independent station WCIU-TV (and later MyNetworkTV station WPWR-TV).

In November 2013, the Cubs exercised an option to terminate its existing broadcast rights with WGN-TV after the 2014 season, requesting a higher-valued contract lasting through the 2019 season (which would be aligned with the end of its contract with CSN Chicago). The team would split its over-the-air package with a second partner, ABC owned-and-operated station WLS-TV, who would acquire rights to 25 games per season from 2015 through 2019. On January 7, 2015, WGN announced that it would air 45 games per-season through 2019. 

From 1999, regional sports network FSN Chicago served as a cable rightsholder for games not on WGN or MLB's national television outlets. In 2003, the owners of the Cubs, White Sox, Blackhawks, and Bulls all broke away from FSN Chicago, and partnered with Comcast to form Comcast SportsNet Chicago (CSN Chicago, now NBC Sports Chicago) in 2004, assuming cable rights to all four teams.

Len Kasper has been the Cubs' television play-by-play announcer since 2005 and was joined by Jim Deshaies in 2013. Bob Brenly (analyst, 2005â12), Chip Caray (play-by-play, 1998â2004), Steve Stone (analyst, 1983â2000, 2003â04), Joe Carter (analyst for WGN-TV games, 2001â02) and Dave Otto (analyst for FSN Chicago games, 2001â02) also have spent time broadcasting from the Cubs booth since the death of Harry Caray in 1998.




</doc>
<doc id="6655" url="https://en.wikipedia.org/wiki?curid=6655" title="Coldcut">
Coldcut

Coldcut are an English electronic music duo composed of Matt Black and Jonathan More. Credited as pioneers for pop sampling in the 1980s, Coldcut are also considered the first stars of UK electronic dance music due to their innovative style, which featured cut-up samples of hip-hop, soul, funk, spoken word and various other types of music, as well as video and multimedia. According to "Spin", "in '87 Coldcut pioneered the British fad for 'DJ records'".

Coldcut's records first introduced the public to pop artists Yazz and Lisa Stansfield, through which these artists achieved pop chart success. In addition, Coldcut has remixed and created productions on tracks by the likes of Eric B & Rakim, James Brown, Queen Latifah, Eurythmics, INXS, Steve Reich, Blondie, The Fall, Pierre Henry, Nina Simone, Fog, Red Snapper, and BBC Radiophonic Workshop.

Beyond their work as a production duo, Coldcut are the founders of Ninja Tune, an independent record label in London, England (with a satellite office in Montreal) with an overall emphasis on encouraging interactive technology and finding innovative uses of software. The label's first releases (the first four volumes of DJ Food - "Jazz Brakes") were produced by Coldcut in the early 90s, and composed of instrumental hip-hop cuts that led the duo to help pioneer the trip hop genre, with artists such as Funki Porcini, The Herbaliser and DJ Vadim.

In 1986, computer programmer Matt Black and ex-art teacher Jonathan More were part-time DJs on the rare groove scene. More also DJed on pirate radio, hosting the "Meltdown Show" on Kiss FM and worked at the Reckless Records store on Berwick Street, London where Black visited as a customer. The first collaboration between the two artists was "Say Kids What Time Is It?" on a white label in January 1987, which mixed Jungle Book's "King of the Swingers" with the break from James Brown's "Funky Drummer". The innovation of "Say Kids..." caused More and Black to be heralded by SPIN as "the first Brit artists to really get hip-hop's class-cutup aesthetic". It is regarded as the UK's first breaks record, the first UK record to be built entirely of samples and "the final link in the chain connecting European collage-experiment with the dance-remix-scratch edit". This was later sampled in "Pump Up the Volume" by MARRS, a single that reached #1 in the UK in October 1987.

Though Black had joined Kiss FM with his own mix-based show, the pair eventually joined forces on its own show later in 1987 called "Solid Steel". The eclectic show became a unifying force in underground experimental electronic music and is still running, celebrating 25 years in 2013.

The duo adopted the name "Coldcut" and set up a record label called Ahead Of Our Time to release the single "Beats + Pieces" (one of the formats also included "That Greedy Beat") in 1987. All of these tracks were assembled using cassette pause button edits and later spliced tape edits that would sometimes run "all over the room". The duo used sampling from Led Zeppelin to James Brown. Electronic act The Chemical Brothers have described "Beats + Pieces" as the "first bigbeat record", a style which appeared in the mid-1990s.

Coldcut's first mainstream success came when Julian Palmer from Island Records asked them to remix Eric B. & Rakim's "Paid in Full". Released in October 1987, the landmark remix is said to have "laid the groundwork for hip hop's entry into the UK mainstream", becoming a breakthrough hit for Eric B & Rakim outside the U.S., reaching No. 15 in the UK, and the top 20 in a number of European countries. It featured a prominent Ofra Haza sample and many other vocal cut ups as well as a looped rhythm which later, when speeded up, proved popular in the Breakbeat genre. Off the back of its success in clubs, the Coldcut "Seven Minutes of Madness" remix ended up being promoted as the single in the UK.

In 1988, More and Black formed Hex, a self-titled "multimedia pop group", with Mile Visman and Rob Pepperell. While working on videos for artists such as Kevin Saunderson, Queen Latifah and Spiritualized, Hex's collaborative work went on to incorporate 3D modelling, punk video art, and algorithmic visuals on desktop machines. The video for Coldcut's 'Christmas Break' in 1989 is arguably one of the first pop promos produced entirely on microcomputers.

In 1988, Coldcut released "Out To Lunch With Ahead Of Our Time", a double LP of Coldcut productions and re-cuts, and the various aliases under which the duo had recorded. This continued the duo's tradition of releasing limited available vinyl.

The next Coldcut single, released in February 1988, moved towards a more house-influenced style. "Doctorin' the House", which debuted singer Yazz, became a top ten hit, and peaked at No. 6. In the same year, under the guise Yazz and the Plastic Population, they produced "The Only Way Is Up", a cover of a Northern soul song. The record reached No. 1 in the UK in August, and remained there for five weeks, becoming 1988's second biggest selling single. Producer Youth of Killing Joke also helped Coldcut with this record. The duo had another top hit in September with "Stop This Crazy Thing", which featured reggae vocalist Junior Reid and reached number 21 in the UK.

The single "People Hold On" became another UK Top 20 hit. Released in March 1989, it helped launch the career of the then relatively unknown singer Lisa Stansfield. Coldcut and Mark Saunders produced her debut solo single "This Is the Right Time", which became another UK Top 20 hit in August as well as reaching No. 21 on the U.S. "Billboard" Hot 100 the following year.

As the duo started to enjoy critical and commercial success, their debut album "What's That Noise?" was released in April 1989 on Ahead of Our Time and distributed by Big Life Records. The album gave "breaks the full length treatment", and showcased "their heady blend of hip-hop production aesthetics and proto-acid house grooves". It also rounded up a heap of unconventional guest features, quoted by SPIN as having "somehow found room at the same table for Queen Latifah and Mark E. Smith". The album's track "I'm in Deep" (featuring Smith) prefigured the indie-dance guitar-breaks crossover of such bands as the Stone Roses and Happy Mondays, utilizing Smith's freestyle raucous vocals over an acid house backing, and also including psych guitar samples from British rock band Deep Purple. "What's That Noise?" reached the Top 20 in the UK and was certified Silver.

Coldcut's second album, "Some Like It Cold", released in 1990 on Ahead Of Our Time, featured a collaboration with Queen Latifah on the single "Find a Way". Though "Find a Way" was a minor hit in the UK, no more singles were released from the album. The duo was given the BPI "Producer of the Year Award" in 1990. Hex - alongside some other London visual experimenters such as iE - produced a series of videos for a longform VHS version of the album. This continued Coldcut and Hex's pioneering of the use of microcomputers to synthesize electronic music visuals.

After their success with Lisa Stansfield, Coldcut signed with her label, Arista. Conflicts arose with the major label, as Coldcut's "vision extended beyond the formulae of house and techno" and mainstream pop culture (CITATION: The Virgin Encyclopedia Of Nineties Music, 2000). Eventually, the duo's album Philosophy emerged in 1993. Singles "Dreamer" and "Autumn Leaves" (1994) sung by vocalist Janis Alexander were both minor hits but the album did not chart.

"Autumn Leaves" had strings recorded at Abbey Road, with a 30-piece string section and an arrangement by film composer Ed Shearmur. The leader of the string section was Simon Jeffes of Penguin Cafe Orchestra. Coldcut's insistence on their friend Mixmaster Morris to remix "Autumn Leaves" led to one of Morris' most celebrated remixes, which became a minor legend in ambient music. It has appeared on numerous compilations.

In 1990, whilst on their first tour in Japan (which also featured Norman Cook, who later became Fatboy Slim), Matt and Jon formed their second record label, Ninja Tune, as a self-titled "technocoloured escape pod", and a way to escape the creative control of major labels. The label enabled them to release music under different aliases (e.g. Bogus Order, DJ Food), which also helped them to avoid pigeonholing as producers. Ninja Tune's first release was Bogus Order's "Zen Brakes". The name Coldcut stayed with Arista so there were no official Coldcut releases for the next three years.

During this time, Coldcut still produced for artists on their new label, releasing a flood of material under different names and continuing to work with young groups. They additionally kept on with "Solid Steel" on Kiss FM and running the night club Stealth (Club of the Year in the NME, "The Face", and "Mixmag" in 1996).

In 1991, Hex released their first video game, "Top Banana", which was included on a Hex release for the Commodore CDTV machine in 1992, arguably the first complete purpose-designed multimedia system. "Top Banana" was innovative in that it used sampled graphics, contained an ecological theme and a female lead character (dubbed "KT"), and its music changed through random processes. Coldcut and Hex presented this multimedia project as an example of the forthcoming convergence of pop music and computer-game characters.

In 1992, Hex's first single - "Global Chaos" / "Digital Love Opus 1" - combined rave visuals with techno and ambient interactive visuals.
In November of that year, Hex released Global Chaos CDTV, which took advantage of the possibilities of the new CD-ROM medium. The Global Chaos CDTV disk (which contained the "Top Banana" game, interactive visuals and audio), was a forerunner of the "CD+" concept, uniting music, graphics, and video games into one. This multi-dimensional entertainment product received wide coverage in the national media, including features on Dance Energy, Kaleidoscope on BBC Radio 4, "What's Up Doc?" on ITV and "Reportage" on BBC Two. "i-D Magazine" was quoted as saying, "It's like your TV tripping".

Coldcut videos were made for most songs, often by Hexstatic, and used a lot of stock and sampled footage. Their "Timber" video, which created an AV collage piece using analogous techniques to audio sample collage, was put on heavy rotation on MTV. Stuart Warren Hill of Hexstatic referred to this technique as: "What you see is what you hear". "Timber" (which appears on both "Let Us Play", Coldcut's fourth album, and "Let Us Replay", their fifth) won awards for its innovative use of repetitive video clips synced to the music, including being shortlisted at the Edinburgh Television and Film Festival in their top five music videos of the year in 1998.

Coldcut began integrating video sampling into their live DJ gigs at the time, and incorporated multimedia content that caused press to credit the act as segueing "into the computer age". Throughout the 90s, Hex created visuals for Coldcut's live performances, and developed the CD-ROM portion of Coldcut's "Let Us Play" and "Let Us Replay", in addition to software developed specifically for the album's world tour. Hex's inclusion of music videos and "playtools" (playful art/music software programs) on Coldcut's CD-Roms was completely ahead of the curve at that time, offering viewers/listeners a high level of interactivity. Playtools such as My Little Funkit and Playtime were the prototypes for Ninja Jamm, the app Coldcut designed and launched 16 years later. Playtime followed on from Coldcut and Hex's Synopticon installation, developing the auto-cutup algorhythm, and using other random processes to generate surprising combinations. Coldcut and Hex performed live using Playtime at the 1st Sonar Festival in 1994. Playtime was also used to generate the backing track for Coldcut's collaboration with Jello Biafra, "Every Home a Prison".

In 1994 Coldcut and Hex contributed an installation to the Glasgow Gallery of Modern Art. The piece, called "Generator" was installed in the Fire Gallery. "Generator" was an interactive installation which allowed users to mix sound, video, text and graphics and make their own audio-visual mix, modelled on the techniques and technology used by Coldcut in clubs and live performance events. It consisted of two consoles: the left controlling how the sounds are played, the right controlling how the images are played.

As part of the JAM exhibition of "Style, Music and Media" at the Barbican Art Gallery in 1996, Coldcut and Hex were commissioned to produce an interactive audiovisual piece called Synopticon. Conceived and designed by Robert Pepperell and Matt Black, the digital culture synthesiser allows users to "remix" sounds, images, text and music in a partially random, partially controlled way.

The year 1996 also brought the Coldcut name back to More and Black, and the pair celebrated with "70 Minutes of Madness", a mix CD that became part of the Journeys by DJ series. The release was credited with "bringing to wider attention the sort of freestyle mixing the pair were always known for through their radio show on KISS FM, Solid Steel, and their steady club dates". It was voted "Best Compilation of All Time" by Jockey Slut in 1998.

In February 1997, they released a double pack single "Atomic Moog 2000" / "Boot the System", the first Coldcut release on Ninja Tune. This was not eligible for the UK chart because time and format restrictions prevented the inclusion of the "Natural Rhythm" video on the CD. In August 1997, a reworking of the early track "More Beats + Pieces" gave them their first UK Top 40 hit since 1989.

The album "Let Us Play!" followed in September and also made the Top 40. The fourth album by Coldcut, "Let Us Play!" paid homage to the greats that inspired them. Their first album to be released on Ninja Tune, it featured guest appearances by Grandmaster Flash, Steinski, Jello Biafra, Jimpster, The Herbaliser, Talvin Singh, Daniel Pemberton and Selena Saliva. Coldcut's cut 'n' paste method on the album was compared to that of Dadaism and William Burroughs. Hex collaborated with Coldcut to produce the multimedia CD-Rom for the album. Hex later evolved the software into the engine that was used on the Let Us Play! world tour.

In 1997, Matt Black - alongside Cambridge based developers Camart - created real-time video manipulation software VJAMM. It allowed users to be a "digital video jockey", remixing and collaging sound and images and trigger audio and visual samples simultaneously, subsequently bringing futuristic technology to the audio-visual field. VJAMM rivalled some of the features of high-end and high cost tech at the time. The VJAMM technology, praised as being proof of how far computers changed the face of live music, became seminal in both Coldcut's live sets (which were called a "revelaton" by Melody Maker and DJ sets. Their CCTV live show was featured at major festivals including Glastonbury, Roskilde, SÃ³nar, the Montreux Jazz Festival, and John Peel's Meltdown. The "beautifully simple and devastatingly effective" software was deemed revolutionary, and became recognized as a major factor in the evolution of clubs. It eventually earned a place in the American Museum of the Moving Image's permanent collection. As quoted by The Independent, Coldcut's rallying cry was "Don't hate the media, be the media'". NME was quoted as saying: "Veteran duo Coldcut are so cool they invented the remix - now they are doing the same for television."

Also working with Camart, Black designed DJamm software in 1998, which Coldcut used on laptops for their live shows, providing the audio bed alongside VJAMM's audiovisual samples. Matt Black explained they designed DJamm so they "could perform electronic music in a different way â i.e., not just taking a session band out to reproduce what you put together in the studio using samples. It had a relationship to DJing, but was more interactive and more effective." Excitingly at that time, DJamm was pioneering in its ability to shuffle sliced loops into intricate sequences, enabling users to split loops into any number of parts.

In 1999, "Let Us Replay!" was released, a double-disc remix album where Coldcut's classic tunes were remixed by the likes of Cornelius (which was heralded as a highlight of the album, Irresistible Force, Shut Up And Dance, Carl Craig and J Swinscoe. Let Us Replay! pieces together "short sharp shocks that put the mental in 'experimental' and still bring the breaks till the breakadawn". It also includes a few live tracks from the duo's innovative world tour. The CD-Rom of the album, which also contained a free demo disc of the VJamm software, was one of the earliest audiovisual CD- ROMs on the market, and Muzik claimed deserved to "have them canonized...it's like buying an entire mini studio for under $15".

In 2000, the "Solid Steel" show moved to BBC London.

Coldcut continued to forge interesting collaborations, including 2001's "Re:volution" as an EP in which Coldcut created their own political party (The Guilty Party). Featuring scratches and samples of Tony Blair and William Hague speeches, the 3-track EP included Nautilus' "Space Journey", which won an Intermusic contest in 2000. The video was widely played on MTV. With "Space Journey", Coldcut were arguably the first group to give fans access to the multitrack parts, or "stems" of their songs, building on the idea of interactivity and sharing from "Let Us Play".

In 2001, Coldcut produced tracks for the Sega music video game "Rez". "Rez" replaced typical video-game sound effects with electronic music; the player created sounds and melodies, intended to simulate a form of synesthesia. The soundtrack also featured Adam Freeland and Oval.

In 2002, while utilizing VJamm and Detraktor, Coldcut and Juxta remixed Herbie Hancock's classic "Rockit", creating both an audio and video remix.

Working with Marcus Clements in 2002, Coldcut released the sample manipulation algorhythm from their DJamm software as a standalone VST plugin that could be used in other software, naming it the "Coldcutter".

Also in 2002, Coldcut with UK VJs Headspace (now mainly performing as the VJamm Allstars developed Gridio, an interactive, immersive audio-visual installation for the Pompidou Centre as part of the â'Sonic Process" exhibition. The "Sonic Process" exhibition was launched at the MACBA in Barcelona in conjunction with SÃ³nar, featuring Gridio as its centerpiece. In 2003, a commission for Graz led to a specially built version of Gridio, in a cave inside the castle mountain in Austria. Gridio was later commissioned by O2 for two simultaneous customised installations at the O2 Wireless Festivals in Leeds and London in 2007. That same year, Gridio was featured as part of Optronica at the opening week of the new BFI Southbank development in London.

In 2003, Black worked with Penny Rimbaud (ex Crass) on Crass Agenda's Savage Utopia project. Black performed the piece with Rimbaud, Eve Libertine and other players at London's Vortex Jazz Club.

In 2004, Coldcut collaborated with American video mashup artist TV Sheriff to produce their cut-up entitled "Revolution USA". The tactical-media project (coordinated with Canadian art duo NomIg) followed on from the UK version and extended the premise "into an open access participatory project". Through the multimedia political art project, over 12 gigabytes of footage from the last 40 years of US politics were made accessible to download, allowing participants to create a cut-up over a Coldcut beat. Coldcut also collaborated with TV Sheriff and NomIg to produce two audiovisual pieces "World of Evil" (2004) and "Revolution '08" (2008), both composed of footage from the United States presidential elections of respective years. The music used was composed by Coldcut, with "Revolution '08" featuring a remix by the Qemists.

Later that year, a collaboration with the British Antarctic Survey (BAS) led to the psychedelic art documentary "Wavejammer". Coldcut was given access to the BAS archive in order to create sounds and visuals for the short film.

2004 also saw Coldcut produce a radio play in conjunction with renowned young author Hari Kunzru for BBC Radio 3 (incidentally called "Sound Mirrors").

Coldcut returned with the single "Everything Is Under Control" at the end of 2005, featuring Jon Spencer (of Jon Spencer Blues Explosion) and Mike Ladd. It was followed in 2006 by their fifth studio album "Sound Mirrors", which was quoted as being "one of the most vital and imaginative records Jon Moore and Matt Black have ever made", and saw the duo "continue, impressively, to find new ways to present political statements through a gamut of pristine electronics and breakbeats" (CITATION: Future Music, 2007). The fascinating array of guest vocalists included Soweto Kinch, Annette Peacock, Ameri Baraka, and Saul Williams. The latter followed on from Coldcut's remix of Williams' "The Pledge" for a project with DJ Spooky.

A 100-date audiovisual world tour commenced for "Sound Mirrors", which was considered "no small feat in terms of technology or human effort". Coldcut was accompanied by scratch DJ Raj and AV artist Juxta, in addition to guest vocalists from the album, including UK rapper Juice Aleem, Roots Manuva, Mpho Skeef, Jon Spencer and house legend Robert Owens.

Three further singles were released from the album including the Top 75 hit "True Skool" with Roots Manuva. The same track appeared on the soundtrack of the video game FIFA Street 2.

Sponsored by the British Council, in 2005 Coldcut introduced AV mixing to India with the Union project, alongside collaborators Howie B and Aki Nawaz of Fun-Da-Mental. Coldcut created an A/V remix of the Bollywood hit movie "Kal Ho Naa Ho".

In 2006, Coldcut performed an A/V set based on "Music for 18 Musicians" as part of Steve Reich's 70th birthday gig at the Barbican Centre in London. This was originally written for the 1999 album "Reich Remixed".

Coldcut remixed another classic song in 2007: Nina Simone's "Save Me". This was part of a remix album called "Nina Simone: Remixed & Re-imagined", featuring remixes from Tony Humphries, Francois K and Chris Coco.

In February 2007, Coldcut and Mixmaster Morris created a psychedelic AV obituary/tribute Coldcut, Mixmaster Morris, Ken Campbell, Bill Drummond and Alan Moore (18 March 2007). Robert Anton Wilson tribute show. Queen Elizabeth Hall, London: Mixmaster Morris. (28 August 2009) to Robert Anton Wilson, the 60s author of Illuminatus! Trilogy. The tribute featured graphic novel writer Alan Moore and artist Bill Drummond and a performance by experimental theatre legend Ken Campbell. Coldcut and Morris' hour and a half performance resembled a documentary being remixed on the fly, cutting up nearly 15 hours' worth of Wilson's lectures.

In 2008, an international group of party organisers, activists and artists including Coldcut received a grant from the Intelligent Energy Department of the European Union, to create a project that promoted intelligent energy and environmental awareness to the youth of Europe. The result was Energy Union, a piece of VJ cinema, political campaign, music tour, party, art exhibition and social media hub. Energy Union toured 12 EU countries throughout 2009 and 2010, completing 24 events in total. Coldcut created the Energy Union show for the tour, a one-hour Audio/Visual montage on the theme of Intelligent Energy. In presenting new ideas for climate, environmental and energy communication strategies, the Energy Union tour was well received, and reached a widespread audience in cities across the UK, Germany, Belgium, The Netherlands, Croatia, Slovenia, Austria, Hungary, Bulgaria, Spain and the Czech Republic.

Also in 2008, Coldcut was asked to remix the theme song for British cult TV show "Doctor Who" for the program's 40th anniversary. In October 2008, Coldcut celebrated the legacy of the BBC Radiophonic Workshop (the place where the Doctor Who theme was created) with a live DJ mix at London's legendary Roundhouse. The live mix incorporated classic Radiophonic Workshop compositions with extended sampling of the original gear.

Additionally in 2008, Coldcut remixed "Ourselves", a Japanese No. 1 hit from the single "&" by Ayumi Hamasaki. This mix was included on the album "".

Starting in 2009, Matt Black, with musician/artist/coder Paul Miller (creator of the TX Modular Open Source synth), developed Granul8, a new type of visual fx/source Black termed a "granular video synthesiser". Granul8 allows the use of realtime VJ techniques including video feedback combined with VDMX VJ software.

From 2009 onwards, Black has been collaborating with coder and psychedelic mathematician William Rood to create a forthcoming project called Liveloom, a social media AV mixer.

In 2010, Coldcut celebrated 20 years of releasing music with its label, Ninja Tune. A book entitled "Ninja Tune: 20 Years of Beats and Pieces" was released on 12 August 2010, and an exhibition was held at Black Dog Publishing's Black Dog Space in London, showcasing artwork, design and photography from the label's 20-year history. A compilation album was released on 20 September in two formats: a regular version consisting of two 2-disc volumes, and a limited edition which contained six CDs, six 7" vinyl singles, a hardback copy of the book, a poster and additional items. Ninja Tune also incorporated a series of international parties. This repositioned Ninja as a continually compelling and influential label, being one of the "longest-running (and successful) UK indie labels to come out of the late-1980s/early-90s explosion in dance music and hip-hop" (Pitchfork, 28 September 2010). Pitchfork claimed it had a "right to show off a little".

In July 2013, Coldcut produced a piece entitled "D'autre" based on the writings of French poet Arthur Rimbaud, for Forum Des Images in Paris. The following month, in August, Coldcut produced a new soundtrack for a section of AndrÃ© Sauvage's classic film "Ãtudes sur Paris", which was shown as part of Noise of Art at the BFI in London, which celebrated 100 years of electronic music and silent cinema. Coldcut put new music to films from the Russolo era, incorporating original recordings of Russolo's proto-synths.

In 2014 Coldcut did 3 soundtracks as part of the project New City, a series of animated skylines of the near future developed by Tomorrow's Thought Today's Liam Young, with accompanying writing from sci-fi authors Jeff Noon, Pat Cadigan and Tim Maughan.

Most recently, Coldcut released Ninja Jamm, a music making app, for Android and iOS, in collaboration with London-based arts and technology firm Seeper. Geared toward both casual listeners and more experienced DJs and music producers, the freemium app allows users to download, remix and make music with samplepacks and tunepacks that feature pro quality sample libraries and also original tracks and mixes by Coldcut, as well as other Ninja artists, creating something new altogether. With the "intuitive yet deep" app, users can turn instruments on and off, swap between clips, add glitches and effects, trigger and pitch-bend stabs and one-off samples, and change the tempo of the track instantly. Users can additionally record as they mix and instantly upload to SoundCloud or save the mixes locally. Tunepack releases for Ninja Jamm are increasingly synchronised with Ninja Tune releases on conventional formats. To date over 30 tunepacks have been released, including Amon Tobin, Bonobo, Coldcut, DJ Food, Martyn, Lapalux, Machinedrum, Raffertie, Irresistible Force, FaltyDL, Shuttle, Starkey. Ninja Jamm was featured by Apple in the New and Noteworthy section of the App Store in the week of release and it received over 100,000 downloads in the first week. Coldcut are developing Ninja Jamm further after the Android release garnered acclaim from the Guardian, Independent, Gizmodo and many more reviewers.

In 2015, Coldcut are working on a new album, collaborating with producer Dave Taylor (a.k.a. Solid Groove a.k.a. Switch). This is planned for release in 2016.

On 6 December 2017, BBC Radio 4 broadcast a play, "Billie Homeless Dies at the End" by Tom Kelly with electronic music by Coldcut.





</doc>
<doc id="6656" url="https://en.wikipedia.org/wiki?curid=6656" title="Cuisine">
Cuisine

A cuisine is a style of cooking characterized by distinctive ingredients, techniques and dishes, and usually associated with a specific culture or geographic region. Regional food preparation traditions, customs and ingredients often combine to create dishes unique to a particular region.

A cuisine is primarily influenced by the ingredients that are available locally or through trade, they can even be made into distinct ingredients themselves when they become popular within a region, take for example Japanese rice in Japanese cuisine and New Mexico chile in New Mexican cuisine.

Religious food laws can also exercise a strong influence on cuisine, such as Hinduism in Indian cuisine, Sikhism in Punjabi cuisine, Buddhism in East Asian cuisine, Islam in Middle Eastern cuisine, and Judaism in Israeli cuisine.

Some factors that have an influence on a region's cuisine include the area's climate, the trade among different countries, religiousness or sumptuary laws and culinary culture exchange. For example, a tropical diet may be based more on fruits and vegetables, while a polar diet might rely more on meat and fish.

The area's climate, in large measure, determines the native foods that are available. In addition, climate influences food preservation. For example, foods preserved for winter consumption by smoking, curing, and pickling have remained significant in world cuisines for their altered gustatory properties.

The trade among different countries also largely affects a region's cuisine. Dating back to the ancient spice trade, seasonings such as cinnamon, cassia, cardamom, ginger, and turmeric were important items of commerce in the earliest evolution of trade. Cinnamon and cassia found their way to the Middle East at least 4,000 years ago.

Certain foods and food preparations are required or proscribed by the religiousness or sumptuary laws, such as Islamic dietary laws and Jewish dietary laws.

Culinary culture exchange is also an important factor for cuisine in many regions: Japan's first substantial and direct exposure to the West came with the arrival of European missionaries in the second half of the 16th century. At that time, the combination of Spanish and Portuguese game frying techniques with an East Asian method for cooking vegetables in oil led to the development of "tempura", the "popular Japanese dish in which seafood and many different types of vegetables are coated with batter and deep fried".

Cuisine dates back to the Antiquity. As food began to require more planning, there was an emergence of meals that situated around culture.

Cuisines evolve continually, and new cuisines are created by innovation and cultural interaction. One recent example is fusion cuisine, which combines elements of various culinary traditions while not being categorized per any one cuisine style, and generally refers to the innovations in many contemporary restaurant cuisines since the 1970s. "Nouvelle cuisine" (New cuisine) is an approach to cooking and food presentation in French cuisine that was popularized in the 1960s by the food critics Henri Gault, who invented the phrase, and his colleagues AndrÃ© Gayot and Christian Millau in a new restaurant guide, the Gault-Millau, or "Le Nouveau Guide". Molecular cuisine, is a modern style of cooking which takes advantage of many technical innovations from the scientific disciplines (molecular cooking). The term was coined in 1999 by the French INRA chemist HervÃ© This because he wanted to distinguish it from the name Molecular cuisine that was previously introduced by him and the late Oxford physicist Nicholas Kurti. It is also named as multi sensory cooking, modernist cuisine, culinary physics, and experimental cuisine by some chefs. Besides, international trade brings new foodstuffs including ingredients to existing cuisines and leads to changes. The introduction of hot pepper to China from South America around the end of the 17th century, greatly influencing Sichuan cuisine, which combines the original taste (with use of Sichuan pepper) with the taste of newly introduced hot pepper and creates a unique mala () flavor that's mouth-numbingly spicy and pungent.

A global cuisine is a cuisine that is practiced around the world, and can be categorized according to the common use of major foodstuffs, including grains, produce and cooking fats.

Regional cuisines can vary based on availability and usage of specific ingredients, local cooking traditions and practices, as well as overall cultural differences. Such factors can be more-or-less uniform across wide swaths of territory, or vary intensely within individual regions. For example, in Central and North South America, corn (maize), both fresh and dried, is a staple food, and is used in many different ways. In northern Europe, wheat, rye, and fats of animal origin predominate, while in southern Europe olive oil is ubiquitous and rice is more prevalent. In Italy, the cuisine of the north, featuring butter and rice, stands in contrast to that of the south, with its wheat pasta and olive oil. In some parts of China, rice is the staple, while in others this role is filled by noodles and bread. Throughout the Middle East and Mediterranean, common ingredients include lamb, olive oil, lemons, peppers, and rice. The vegetarianism practiced in much of India has made pulses (crops harvested solely for the dry seed) such as chickpeas and lentils as important as wheat or rice. From India to Indonesia, the extensive use of spices is characteristic; coconuts and seafood are also used throughout the region both as foodstuffs and as seasonings.

African cuisines use a combination of locally available fruits, cereals and vegetables, as well as milk and meat products. In some parts of the continent, the traditional diet features a preponderance of milk, curd and whey products. In much of tropical Africa, however, cow's milk is rare and cannot be produced locally (owing to various diseases that affect livestock). The continent's diverse demographic makeup is reflected in the many different eating and drinking habits, dishes, and preparation techniques of its manifold populations.

Asian cuisines are many and varied, and include East Asian cuisine, South Asian cuisine, Southeast Asian cuisine, Central Asian cuisine and West Asian cuisine. Ingredients common to East Asia and Southeast Asia (due to overseas Chinese influence) include rice, ginger, garlic, sesame seeds, chilies, dried onions, soy, and tofu, with stir frying, steaming, and deep frying being common cooking methods. While rice is common to most regional cuisines in Asia, different varieties are popular in the different regions: Basmati rice is popular in South Asia, Jasmine rice in Southeast Asia, and long-grain rice in China and short-grain rice in Japan and Korea. Curry is also a common ingredient found in South Asia, Southeast Asia, and East Asia (notably Japanese curry); however, they are not popular in West Asian and Central Asian cuisines. Those curry dishes with origins in South Asia usually have a yogurt base, with origins in Southeast Asia a coconut milk base, and in East Asia a stewed meat and vegetable base. South Asian cuisine and Southeast Asian cuisine are often characterized by their extensive use of spices and herbs native to the tropical regions of Asia.

European cuisine (alternatively, "Western cuisine") include the cuisines of Europe and other Western countries. European cuisine includes that of Europe and to some extent Russia, as well as non-indigenous cuisines of North America, Australasia, Oceania, and Latin America. The term is used by East Asians to contrast with East Asian styles of cooking. When used in English, the term may refer more specifically to cuisine "in" (Continental) Europe; in this context, a synonym is Continental cuisine, especially in British English.

Oceanian cuisines include Australian cuisine, New Zealand cuisine, and the cuisines from many other islands or island groups throughout Oceania. Australian cuisine consists of immigrant Anglo-Celtic derived cuisine, and Bushfood prepared and eaten by native Aboriginal Australian peoples, and various newer Asian influences. New Zealand cuisine also consists of European inspired dishes, such as Pavlova, and native Maori cuisine. Across Oceania, staples include the Kumura (Sweet potato) and Taro, which was/is a staple from Papua New Guinea to the South Pacific. On most islands in the south pacific, fish are widely consumed because of the proximity to the ocean.

The cuisines of the Americas are found across North and South America, and are based on the cuisines of the countries from which the immigrant people came, primarily Europe. However, the traditional European cuisine has been adapted by the addition of many local and native ingredients, and many techniques have been added to traditional foods as well. Native American cuisine is prepared by indigenous populations across the continent, and its influences can be seen on multi-ethnic Latin American cuisine. Many staple foods eaten across the continent, such as corn (maize), beans, and potatoes have native origins. The regional cuisines are North American cuisine, Mexican cuisine, Central American cuisine, South American cuisine, and Caribbean cuisine.




</doc>
<doc id="6660" url="https://en.wikipedia.org/wiki?curid=6660" title="Codec">
Codec

A codec is a device or computer program which encodes or decodes a digital data stream or signal. "Codec" is a portmanteau of "coder-decoder".

A coder encodes a data stream or a signal for transmission or storage, possibly in encrypted form, and the decoder function reverses the encoding for playback or editing. Codecs are used in videoconferencing, streaming media, and video editing applications.

In the mid-20th century, a codec was a device that coded analog signals into digital form using pulse-code modulation (PCM). Later, the name was also applied to software for converting between digital signal formats, including compander functions.

An audio codec converts analog audio signals into digital signals for transmission or encodes them for storage. A receiving device converts the digital signals back to analog form using an audio decoder for playback. An example of this are the codecs used in the sound cards of personal computers. A video codec accomplishes the same task for video signals.

In addition to encoding a signal, a codec may also compress the data to reduce transmission bandwidth or storage space. Compression codecs are classified primarily into lossy codecs and lossless codecs.

Lossless codecs are often used for archiving data in a compressed form while retaining all information present in the original stream. If preserving the original quality of the stream is more important than eliminating the correspondingly larger data sizes, lossless codecs are preferred. This is especially true if the data is to undergo further processing (for example editing) in which case the repeated application of processing (encoding and decoding) on lossy codecs will degrade the quality of the resulting data such that it is no longer identifiable (visually, audibly or both). Using more than one codec or encoding scheme successively can also degrade quality significantly. The decreasing cost of storage capacity and network bandwidth has a tendency to reduce the need for lossy codecs for some media.

Many popular codecs are lossy. They reduce quality in order to maximize compression. Often, this type of compression is virtually indistinguishable from the original uncompressed sound or images, depending on the codec and the settings used. The most widely used lossy data compression technique in digital media is based on the discrete cosine transform (DCT), used in compression standards such as JPEG images, H.26x and MPEG video, and MP3 and AAC audio. Smaller data sets ease the strain on relatively expensive storage sub-systems such as non-volatile memory and hard disk, as well as write-once-read-many formats such as CD-ROM, DVD and Blu-ray Disc. Lower data rates also reduce cost and improve performance when the data is transmitted.

Two principal techniques are used in codecs, pulse-code modulation and delta modulation. Codecs are often designed to emphasize certain aspects of the media to be encoded. For example, a digital video (using a DV codec) of a sports event needs to encode motion well but not necessarily exact colors, while a video of an art exhibit needs to encode color and surface texture well.

Audio codecs for cell phones need to have very low latency between source encoding and playback. In contrast, audio codecs for recording or broadcast can use high-latency audio compression techniques to achieve higher fidelity at a lower bit-rate.

There are thousands of audio and video codecs, ranging in cost from free to hundreds of dollars or more. This variety of codecs can create compatibility and obsolescence issues. The impact is lessened for older formats, for which free or nearly-free codecs have existed for a long time. The older formats are often ill-suited to modern applications, however, such as playback in small portable devices. For example, raw uncompressed PCM audio (44.1Â kHz, 16 bit stereo, as represented on an audio CD or in a .wav or .aiff file) has long been a standard across multiple platforms, but its transmission over networks is slow and expensive compared with more modern compressed formats, such as Opus and MP3.

Many multimedia data streams contain both audio and video, and often some metadata that permits synchronization of audio and video. Each of these three streams may be handled by different programs, processes, or hardware; but for the multimedia data streams to be useful in stored or transmitted form, they must be encapsulated together in a container format.

Lower bitrate codecs allow more users, but they also have more distortion. Beyond the initial increase in distortion, lower bit rate codecs also achieve their lower bit rates by using more complex algorithms that make certain assumptions, such as those about the media and the packet loss rate. Other codecs may not make those same assumptions. When a user with a low bitrate codec talks to a user with another codec, additional distortion is introduced by each transcoding.

Audio Video Interleave (AVI) is sometimes erroneously described as a codec, but AVI is actually a container format, while a codec is a software or hardware tool that encodes or decodes audio or video into or from some audio or video format. Audio and video encoded with many codecs might be put into an AVI container, although AVI is not an ISO standard. There are also other well-known container formats, such as Ogg, ASF, QuickTime, RealMedia, Matroska, and DivX Media Format. MPEG transport stream, MPEG program stream, MP4, and ISO base media file format are examples of container formats that are ISO standardized.


</doc>
<doc id="6663" url="https://en.wikipedia.org/wiki?curid=6663" title="Clyde Tombaugh">
Clyde Tombaugh

Clyde William Tombaugh (February 4, 1906 January 17, 1997) was an American astronomer. He discovered Pluto in 1930, the first object to be discovered in what would later be identified as the Kuiper belt. At the time of discovery, Pluto was considered a planet but was later reclassified as a dwarf planet in 2006. Tombaugh also discovered many asteroids. He called for the serious scientific research of unidentified flying objects, or UFOs.

Tombaugh was born in Streator, Illinois, son of Muron Dealvo Tombaugh, a farmer, and his wife Adella Pearl Chritton. After his family moved to Burdett, Kansas, in 1922, Tombaugh's plans for attending college were frustrated when a hailstorm ruined his family's farm crops. Starting in 1926, he built several telescopes with lenses and mirrors by himself. To better test his telescope mirrors, Tombaugh, with just a pick and shovel, dug a pit 24 feet long, 8 feet deep, and 7 feet wide. This provided a constant air temperature, free of air currents, and was also used by the family as a root cellar and emergency shelter. He sent drawings of Jupiter and Mars to the Lowell Observatory, at Flagstaff, Arizona, which offered him a job. Tombaugh worked there from 1929 to 1945.

Following his discovery of Pluto, Tombaugh earned bachelor's and master's degrees in astronomy from the University of Kansas in 1936 and 1938. During World War II he taught naval personnel navigation at Northern Arizona University. He worked at White Sands Missile Range in the early 1950s, and taught astronomy at New Mexico State University from 1955 until his retirement in 1973. In 1980 he was inducted into the International Space Hall of Fame.

The asteroid 1604 Tombaugh, discovered in 1931, is named after him. He discovered hundreds of asteroids, beginning with 2839 Annette in 1929, mostly as a by-product of his search for Pluto and his searches for other celestial objects. Tombaugh named some of them after his wife, children and grandchildren. The Royal Astronomical Society awarded him the Jackson-Gwilt Medal in 1931.

Direct visual observation became rare in astronomy. By 1965 Robert S. Richardson called Tombaugh one of two great living experienced visual observers as talented as Percival Lowell or Giovanni Schiaparelli. In 1980, Tombaugh and Patrick Moore wrote a book "Out of the Darkness: The Planet Pluto". In August 1992, JPL scientist Robert Staehle called Tombaugh, requesting permission to visit his planet. "I told him he was welcome to it", Tombaugh later remembered, "though he's got to go one long, cold trip." The call eventually led to the launch of the "New Horizons" space probe to Pluto in 2006. Following the passage on July 14, 2015, of Pluto by the "New Horizons" spacecraft the "Cold Heart of Pluto" was named Tombaugh Regio.

Tombaugh died on January 17, 1997, when he was in Las Cruces, New Mexico, at the age of 90. He was cremated. A small portion of his ashes was placed aboard the "New Horizons" spacecraft. The container includes the inscription: "Interred herein are remains of American Clyde W. Tombaugh, discoverer of Pluto and the Solar System's 'third zone'. Adelle and Muron's boy, Patricia's husband, Annette and Alden's father, astronomer, teacher, punster, and friend: Clyde W. Tombaugh (1906â1997)". Tombaugh was survived by his wife, Patricia (1912â2012), and their children, Annette and Alden.

Tombaugh was an active Unitarian Universalist, and he and his wife helped found the Unitarian Universalist Church of Las Cruces, New Mexico.

Clyde Tombaugh had five siblings. Through the daughter of his youngest brother Robert, he is the great-uncle of Los Angeles Dodgers pitcher Clayton Kershaw.

While a young researcher working for the Lowell Observatory in Flagstaff, Arizona, Tombaugh was given the job to perform a systematic search for a trans-Neptunian planet (also called Planet X), which had been predicted by Percival Lowell based on calculations performed by his student mathematician Elizabeth Williams and William Pickering.

Starting 6 April, 1929, Tombaugh used the observatory's astrograph to take photographs of the same section of sky several nights apart. He then used a blink comparator to compare the different images. When he shifted between the two images, a moving object, such as a planet, would appear to jump from one position to another, while the more distant objects such as stars would appear stationary. Tombaugh noticed such a moving object in his search, near the place predicted by Lowell, and subsequent observations showed it to have an orbit beyond that of Neptune. This ruled out classification as an asteroid, and they decided this was the ninth planet that Lowell had predicted. The discovery was made on Tuesday, February 18, 1930, using images taken the previous month. 

Three classical mythological names were about equally popular among proposals for the new planet: Minerva, Cronus and Pluto. However, Minerva was already in use and the primary supporter of Cronus was widely disliked, leaving Pluto as the front-runner. Outside of Lowell staff, it was first proposed by an 11-year-old English schoolgirl, Venetia Burney. In its favor was that the Pluto of Roman mythology was able to render himself invisible, and that its first two letters formed Percival Lowell's initials. In order to avoid the name changes suffered by Neptune, the name was proposed to both the American Astronomical Society and the Royal Astronomical Society, both of which approved it unanimously. The name was officially adopted on May 1, 1930.
Following the discovery, it was recognized that Pluto wasn't massive enough to be the expected ninth planet, and some astronomers began to consider it the first of a new class of object â and indeed Tombaugh searched for additional Trans-Neptunian objects for years, though due to the lack of any further discoveries he concluded that Pluto was indeed a planet. The idea that Pluto was not a true planet remained a minority position until the discovery of other Kuiper belt objects in the late 1990s, which showed that it did not orbit alone but was at best the largest of a number of icy bodies in its region of space. After it was shown that at least one such body was more massive than Pluto, the International Astronomical Union (IAU) reclassified Pluto on August 24, 2006, as a dwarf planet, leaving eight planets in the Solar System.

Tombaugh's widow Patricia stated after the IAU's decision that while he might have been disappointed with the change since he had resisted attempts to remove Pluto's planetary status in his lifetime, he would have accepted the decision now if he were alive. She noted that he "was a scientist. He would understand they had a real problem when they start finding several of these things flying around the place." Hal Levison offered this perspective on Tombaugh's place in history: "Clyde Tombaugh discovered the Kuiper Belt. That's a helluva lot more interesting than the ninth planet."

Tombaugh continued searching for over a decade after the discovery of Pluto, and the lack of further discoveries left him satisfied that no other object of a comparable apparent magnitude existed near the ecliptic. No more trans-Neptunian objects were discovered until 15760 Albion in 1992.

However, more recently the relatively bright object has been discovered. It has a relatively high orbital inclination, but at the time of Tombaugh's discovery of Pluto, Makemake was only a few degrees from the ecliptic near the border of Taurus and Auriga at an apparent magnitude of 16. This position was also very near the galactic equator, making it almost impossible to find such an object within the dense concentration of background stars of the Milky Way. In the fourteen years of looking for planets, until he was drafted in July 1943, Tombaugh looked for motion in 90 million star images (two each of 45 million stars).

Tombaugh is officially credited by the Minor Planet Center with discovering 15 asteroids, and he observed nearly 800 asteroids during his search for Pluto and years of follow-up searches looking for another candidate for the postulated Planet X. Tombaugh is also credited with the discovery of periodic comet 274P/TombaughâTenagra. He also discovered hundreds of variable stars, as well as star clusters, galaxy clusters, and a galaxy supercluster.
Tombaugh was probably the most eminent astronomer to have reported seeing unidentified flying objects. On August 20, 1949, Tombaugh saw several unidentified objects near Las Cruces, New Mexico. He described them as six to eight rectangular lights, stating: "I doubt that the phenomenon was any terrestrial reflection, because... nothing of the kind has ever appeared before or since... I was so unprepared for such a strange sight that I was really petrified with astonishment.".

Tombaugh observed these rectangles of light for about 3 seconds and his wife saw them for about seconds. He never supported the interpretation as a spaceship that has often been attributed to him. He considered other possibilities, with a temperature inversion as the most likely cause.From my own studies of the solar system I cannot entertain any serious possibility for intelligent life on other planets, not even for Mars... The logistics of visitations from planets revolving around the nearer stars is staggering. In consideration of the hundreds of millions of years in the geologic time scale when such visits may have possibly occurred, the odds of a single visit in a given century or millennium are overwhelmingly against such an event.
A much more likely source of explanation is some natural optical phenomenon in our own atmosphere. In my 1949 sightings the faintness of the object, together with the manner of fading in intensity as it traveled away from the zenith towards the southeastern horizon, is quite suggestive of a reflection from an optical boundary or surface of slight contrast in refractive index, as in an inversion layer.
I have never seen anything like it before or since, and I have spent a lot of time where the night sky could be seen well. This suggests that the phenomenon involves a comparatively rare set of conditions or circumstances to produce it, but nothing like the odds of an interstellar visitation.

Another sighting by Tombaugh a year or two later while at a White Sands observatory was of an object of â6 magnitude, four times brighter than Venus at its brightest, going from the zenith to the southern horizon in about 3 seconds. The object executed the same maneuvers as in Tombaugh's first sighting.

Tombaugh later reported having seen three of the mysterious green fireballs, which suddenly appeared over New Mexico in late 1948 and continued at least through the early 1950s. A researcher on Project Twinkle reported that Tombaugh "... never observed an unexplainable aerial object despite his continuous and extensive observations of the sky."

According to an entry in "UFO updates", Tombaugh said: "I have seen three objects in the last seven years which defied any explanation of known phenomenon, such as Venus, atmospheric optic, meteors or planes. I am a professional, highly skilled, professional astronomer. In addition I have seen three green fireballs which were unusual in behavior from normal green fireballs... I think that several reputable scientists are being unscientific in refusing to entertain the possibility of extraterrestrial origin and nature."

Shortly after this, in January 1957, in an Associated Press article in the "Alamogordo Daily News" titled "Celestial Visitors May Be Invading Earth's Atmosphere", Tombaugh was again quoted on his sightings and opinion about them. "Although our own solar system is believed to support no other life than on Earth, other stars in the galaxy may have hundreds of thousands of habitable worlds. Races on these worlds may have been able to utilize the tremendous amounts of power required to bridge the space between the starsÂ ...". Tombaugh stated that he had observed celestial phenomena which he could not explain, but had seen none personally since 1951 or 1952. "These things, which do appear to be directed, are unlike any other phenomena I ever observed. Their apparent lack of obedience to the ordinary laws of celestial motion gives credence."

In 1949, Tombaugh had also told the Naval missile director at White Sands Missile Range, Commander Robert McLaughlin, that he had seen a bright flash on Mars on August 27, 1941, which he now attributed to an atomic blast. Tombaugh also noted that the first atomic bomb tested in New Mexico would have lit up the dark side of the Earth like a neon sign and that Mars was coincidentally quite close at the time, the implication apparently being that the atomic test would have been visible from Mars.

In June 1952, Dr. J. Allen Hynek, an astronomer acting as a scientific consultant to the Air Force's Project Blue Book UFO study, secretly conducted a survey of fellow astronomers on UFO sightings and attitudes while attending an astronomy convention. Tombaugh and four other astronomers, including Dr. Lincoln LaPaz of the University of New Mexico, told Hynek about their sightings. Tombaugh also told Hynek that his telescopes were at the Air Force's disposal for taking photos of UFOs, if he was properly alerted.

Tombaugh's offer may have led to his involvement in a search for near-Earth satellites, first announced in late 1953 and sponsored by the Army Office of Ordnance Research. Another public statement was made on the search in March 1954, emphasizing the rationale that such an orbiting object would serve as a natural space station. However, according to Donald Keyhoe, later director of the National Investigations Committee on Aerial Phenomena (NICAP), the real reason for the sudden search was because two near-Earth orbiting objects had been picked up on new long-range radar in the summer of 1953, according to his Pentagon source.

By May 1954, Keyhoe was making public statements that his sources told him the search had indeed been successful, and either one or two objects had been found. However, the story did not break until August 23, 1954, when "Aviation Week" magazine stated that two satellites had been found only 400 and 600 miles out. They were termed "natural satellites" and implied that they had been recently captured, despite this being a virtual impossibility. The next day, the story was in many major newspapers. Dr. LaPaz was implicated in the discovery in addition to Tombaugh. LaPaz had earlier conducted secret investigations on behalf of the Air Force on the green fireballs and other unidentified aerial phenomena over New Mexico. "The New York Times" reported on August 29 that "a source close to the O. O. R. unit here described as 'quite accurate' the report in the magazine Aviation Week that two previously unobserved satellites had been spotted and identified by Dr. Lincoln LaPaz of the University of New Mexico as natural and not artificial objects. This source also said there was absolutely no connection between the reported satellites and flying saucer reports." However, in the October 10th issue, LaPaz said the magazine article was "false in every particular, in so far as reference to me is concerned."

Both LaPaz and Tombaugh were to issue public denials that anything had been found. The October 1955 issue of "Popular Mechanics" magazine reported: "Professor Tombaugh is closemouthed about his results. He won't say whether or not any small natural satellites have been discovered. He does say, however, that newspaper reports of 18 months ago announcing the discovery of natural satellites at 400 and 600 miles out are not correct. He adds that there is no connection between the search program and the reports of so-called flying saucers."

At a meteor conference in Los Angeles in 1957, Tombaugh reiterated that his four-year search for "natural satellites" had been unsuccessful. In 1959, Tombaugh was to issue a final report stating that nothing had been found in his search. His personal 16-inch telescope was reassembled and dedicated on September 17, 2009, at Rancho Hidalgo, New Mexico (near Animas, New Mexico), adjacent to "Astronomy" 's new observatory.






</doc>
<doc id="6666" url="https://en.wikipedia.org/wiki?curid=6666" title="Christopher BÃ¡thory">
Christopher BÃ¡thory

Christopher BÃ¡thory (; 1530 â 27 May 1581) was voivode of Transylvania from 1576 to 1581. He was a younger son of Stephen BÃ¡thory of SomlyÃ³. Christopher's career began during the reign of Queen Isabella Jagiellon, who administered the eastern territories of the Kingdom of Hungary on behalf of her son, John Sigismund ZÃ¡polya, from 1556 to 1559. He was one of the commanders of John Sigismund's army in the early 1560s.

Christopher's brother, Stephen BÃ¡thory, who succeeded John Sigismund in 1571, made Christopher captain of VÃ¡rad (now Oradea in Romania). After being elected King of Poland, Stephen BÃ¡thory adopted the title of Prince of Transylvania and made Christopher voivode in 1576. Christopher cooperated with MÃ¡rton Berzeviczy, whom his brother appointed to supervise the administration of the Principality of Transylvania as the head of the Transylvanian chancellery at KrakÃ³w. Christopher ordered the imprisonment of Ferenc DÃ¡vid, a leading theologian of the Unitarian Church of Transylvania, who started to condemn the adoration of Jesus. He supported his brother's efforts to settle the Jesuits in Transylvania.

Christopher was the third of the four sons of Stephen BÃ¡thory of SomlyÃ³ and Catherine Telegdi. His father was a supporter of John ZÃ¡polya, King of Hungary, who made him voivode of Transylvania in February 1530. Christopher was born in BÃ¡thorys' castle at SzilÃ¡gysomlyÃ³ (now Èimleu Silvaniei in Romania) in the same year. His father died in 1534.

His brother, Andrew, and their kinsman, TamÃ¡s NÃ¡dasdy, took charge of Christopher's education. Christopher visited England, France, Italy, Spain, and the Holy Roman Empire in his youth. He also served as a page in Emperor Charles V's court.

Christopher entered the service of John ZÃ¡polya's widow, Isabella Jagiellon, in the late 1550s. At the time, Isabella administered the eastern territories of the Kingdom of Hungary on behalf of her son, John Sigismund ZÃ¡polya. She wanted to persuade Henry II of France to withdraw his troops from three fortresses that the Ottomans had captured in Banat, so she sent Christopher to France to start negotiations in 1557.

John Sigismund took charge of the administration of his realm after his mother died on 15 November 1559. He retained his mother's advisors, including Christopher who became one of his most influential officials. After the rebellion of Melchior Balassa, Christopher persuaded John Sigismund to fight for his realm instead of fleeing to Poland in 1562. Christopher was one of the commanders of John Sigismund's troops during the ensuing war against the Habsburg rulers of the western territories of the Kingdom of Hungary, Ferdinand and Maximilian, who tried to reunite the kingdom under their rule. Christopher defeated Maximilian's commander, Lazarus von Schwendi, forcing him to lift the siege of Huszt (now Khust in Ukraine) in 1565.

After the death of John Sigismund, the Diet of Transylvania elected Christopher's younger brother, Stephen BÃ¡thory, voivode (or ruler) on 25 May 1571. Stephen made Christopher captain of VÃ¡rad (now Oradea in Romania). The following year, the Ottoman Sultan, Selim II (who was the overlord of Transylvania), acknowledged the hereditary right of the BÃ¡thory family to rule the province.

Stephen BÃ¡thory was elected King of Poland on 15 December 1575. He adopted the title of Prince of Transylvania and made Christopher voivode on 14 January 1576. An Ottoman delegation confirmed Christopher's appointment at the Diet in GyulafehÃ©rvÃ¡r (now Alba Iulia in Romania) in July. The sultan's charter (or "ahidnÃ¢me") sent to Christopher emphasized that he should keep the peace along the frontiers. Stephen set up a separate chancellery in KrakÃ³w to keep an eye on the administration of Transylvania. The head of the new chancellery, MÃ¡rton Berzeviczy, and Christopher cooperated closely.

Anti-Trinitarian preachers began to condemn the worshiping of Jesus in Partium and SzÃ©kely Land in 1576, although the Diet had already forbade all doctrinal innovations. Ferenc DÃ¡vid, the most influential leader of the Unitarian Church of Transylvania, openly joined the dissenters in the autumn of 1578. Christopher invited Fausto Sozzini, a leading Anti-Trinitarian theologian, to Transylvania to convince DÃ¡vid that the new teaching was erroneous. Since DÃ¡vid refused to obey, Christopher held a Diet and the "Three Nations" (including the Unitarian delegates) ordered DÃ¡vid's imprisonment. Christopher also supported his brother's attempts to strengthen the position of the Roman Catholic Church in Transylvania. He granted estates to the Jesuits to promote the establishment of a college in KolozsvÃ¡r (now Cluj-Napoca in Romania) on 5 May 1579.

Christopher fell seriously ill after his second wife, Elisabeth Bocskai, died in early 1581. After a false rumor about Christopher's death reached Istanbul, Koca Sinan Pasha proposed Transylvania to PÃ¡l MÃ¡rkhÃ¡zy whom Christopher had been forced into exile. Although Christopher's only surviving son Sigismund was still a minor, the Diet elected him as voivode before Christopher's death, because they wanted to prevent the appointment of MÃ¡rkhÃ¡zy. Christopher died in GyulafehÃ©rvÃ¡r on 27 May 1581. He was buried in the Jesuits' church in GyulafehÃ©rvÃ¡r, almost two years later, on 14 March 1583.

Christopher's first wife, Catherina Danicska, was a Polish noblewoman, but only the Hungarian form of her name is known. Their eldest son, Balthasar BÃ¡thory, moved to KrakÃ³w shortly after Stephen BÃ¡thory was crowned King of Poland; he drowned in the Vistula River in May 1577 at the age of 22. Christopher's and Catherina's second son, Nicholas, was born in 1567 and died in 1576.

Christopher's second wife, Elisabeth Bocskai, was a Calvinist noblewoman. Their first child, Cristina (or Griselda), was born in 1569. She was given in marriage to Jan Zamoyski, Chancellor of Poland, in 1583. Christopher's youngest son, Sigismund, was born in 1573.



</doc>
<doc id="6667" url="https://en.wikipedia.org/wiki?curid=6667" title="CPAN">
CPAN

The Comprehensive Perl Archive Network (CPAN) is a repository of over 250,000 software modules and accompanying documentation for 39,000 distributions, written in the Perl programming language by over 12,000 contributors. "CPAN" can denote either the archive network or the Perl program that acts as an interface to the network and as an automated software installer (somewhat like a package manager). Most software on CPAN is free and open source software.

CPAN was conceived in 1993 and has been active online since October 1995. It is based on the CTAN model and began as a place to unify the structure of scattered Perl archives.

Like many programming languages, Perl has mechanisms to use external libraries of code, making one file contain common routines used by several programs. Perl calls these "modules". Perl modules are typically installed in one of several directories whose paths are placed in the Perl interpreter when it is first compiled; on Unix-like operating systems, common paths include "/usr/lib/perl5", "/usr/local/lib/perl5", and several of their subdirectories.

Perl comes with a small set of "core modules". Some of these perform bootstrapping tasks, such as codice_1, which is used for building and installing other extension modules; others, like codice_2, are merely commonly used.

CPAN's main purpose is to help programmers locate modules and programs not included in the Perl standard distribution. Its structure is decentralized. Authors maintain and improve their own modules. Forking, and creating competing modules for the same task or purpose, is common. There is a third-party bug tracking system that is automatically set up for any uploaded distribution, but authors may opt to use a different bug tracking system such as GitHub. Similarly, though GitHub is a popular location to store the source for distributions, it may be stored anywhere the author prefers, or may not be publicly accessible at all. Maintainers may grant permissions to others to maintain or take over their modules, and permissions may be granted by admins for those wishing to take over abandoned modules. Previous versions of updated distributions are retained on CPAN until deleted by the uploader, and a secondary mirror network called BackPAN retains distributions even if they are deleted from CPAN. Also, the complete history of the CPAN and all its modules is available as the GitPAN project, allowing to easily see the complete history for all the modules and for easy maintenance of forks. CPAN is also used to distribute new versions of Perl, as well as related projects, such as Parrot and Raku.

Files on the CPAN are referred to as "distributions". A distribution may consist of one or more modules, documentation files, or programs packaged in a common archiving format, such as a gzipped tar archive or a ZIP file. Distributions will often contain installation scripts (usually called "Makefile.PL" or "Build.PL") and test scripts which can be run to verify the contents of the distribution are functioning properly. New distributions are uploaded to the Perl Authors Upload Server, or PAUSE (see the section Uploading distributions with PAUSE).

In 2003, distributions started to include metadata files, called "META.yml", indicating the distribution's name, version, dependencies, and other useful information; however, not all distributions contain metadata. When metadata is not present in a distribution, the PAUSE's software will try to analyze the code in the distribution to look for the same information; this is not necessarily very reliable. In 2010, version 2 of this specification was created to be used via a new file called "META.json", with the YAML format file often also included for backward compatibility.

With thousands of distributions, CPAN needs to be structured to be useful. Authors often place their modules in the natural hierarchy of Perl module names (such as codice_3 or codice_4) according to purpose or domain, though this is not enforced.

CPAN module distributions usually have names in the form of "CGI-Application-3.1" (where the :: used in the module's name has been replaced with a dash, and the version number has been appended to the name), but this is only a convention; many prominent distributions break the convention, especially those that contain multiple modules. Security restrictions prevent a distribution from ever being replaced with an identical filename, so virtually all distribution names do include a version number.

The distribution infrastructure of CPAN consists of its worldwide network of more than 250 mirrors in more than 60 countries. Each full mirror hosts around 31 gigabytes of data.

Most mirrors update themselves hourly, daily or bidaily from the CPAN master site. Some sites are major FTP servers which mirror lots of other software, but others are simply servers owned by companies that use Perl heavily. There are at least two mirrors on every continent except Antarctica.

Several search engines have been written to help Perl programmers sort through the CPAN. The official includes textual search, a browsable index of modules, and extracted copies of all distributions currently on the CPAN. On 16 May 2018, the Perl Foundation announced that search.cpan.org would be shut down on 29 June 2018 (after 19 years of operation), due to its aging codebase and maintenance burden. Users will be transitioned and redirected to the third-party alternative MetaCPAN.

CPAN Testers are a group of volunteers, who will download and test distributions as they are uploaded to CPAN. This enables the authors to have their modules tested on many platforms and environments that they would otherwise not have access to, thus helping to promote portability, as well as a degree of quality. Smoke testers send reports, which are then collated and used for a variety of presentation websites, including the main reports site, statistics and dependencies.

Authors can upload new distributions to the CPAN through the "Perl Authors Upload Server" (PAUSE). To do so, they must request a PAUSE account.

Once registered, they may use a web interface at pause.perl.org, or an FTP interface to upload files to their directory and delete them. Modules in the upload will only be indexed as canonical if the module name has not been used before (granting "first-come" permission to the uploader), or if the uploader has permission for that name, and if the module is a higher version than any existing entry. This can be specified through PAUSE's web interface.

There is also a Perl core module named CPAN; it is usually differentiated from the repository itself by using the name CPAN.pm. CPAN.pm is mainly an interactive shell which can be used to search for, download, and install distributions. An interactive shell called cpan is also provided in the Perl core, and is the usual way of running CPAN.pm. After a short configuration process and mirror selection, it uses tools available on the user's computer to automatically download, unpack, compile, test, and install modules. It is also capable of updating itself.

An effort to replace CPAN.pm with something cleaner and more modern resulted in the CPANPLUS (or CPAN++) set of modules. CPANPLUS separates the back-end work of downloading, compiling, and installing modules from the interactive shell used to issue commands. It also supports several advanced features, such as cryptographic signature checking and test result reporting. Finally, CPANPLUS can uninstall a distribution. CPANPLUS was added to the Perl core in version 5.10.0, and removed from it in version 5.20.0.

A smaller, leaner modern alternative to these CPAN installers was developed called cpanminus. cpanminus was designed to have a much smaller memory footprint as often required in limited memory environments, and to be usable as a standalone script such that it can even install itself, requiring only the expected set of core Perl modules to be available. It is also available from CPAN as the module App::cpanminus, which installs the cpanm script. It does not maintain or rely on a persistent configuration, but is configured only by the environment and command-line options. cpanminus does not have an interactive shell component. It recognizes the cpanfile format for specifying prerequisites, useful in ad-hoc Perl projects that may not be designed for CPAN installation. cpanminus also has the ability to uninstall distributions.

Each of these modules can check a distribution's dependencies and recursively install any prerequisites, either automatically or with individual user approval. Each support FTP and HTTP and can work through firewalls and proxies.

Experienced Perl programmers often comment that half of Perl's power is in the CPAN. It has been called Perl's killer app. It is roughly equivalent to the PECL and PEAR for PHP; the PyPI (Python Package Index) repository for Python; RubyGems for Ruby; CRAN for R; npm for Node.js; LuaRocks for Lua; Maven for Java; and Hackage for Haskell. CPAN's use of arbitrated name spaces, a testing regime and a well defined documentation style makes it unique.

Given its importance to the Perl developer community, the CPAN both shapes and is shaped by Perl's culture. Its "self-appointed master librarian", Jarkko Hietaniemi, often takes part in the April Fools' Day jokes; on 1 April 2002 the site was temporarily named to "CJAN", where the "J" stood for "Java". In 2003, the codice_5 domain name was redirected to Matt's Script Archive, a site infamous in the Perl community for having badly written code.

Some of the distributions on the CPAN are distributed as jokes. The codice_6 hierarchy is reserved for joke modules; for instance, codice_7 adds a codice_8 function that doesn't run the code given to it (to complement the codice_9 built-in, which does). Even outside the codice_6 hierarchy, some modules are still written largely for amusement; one example is codice_11, which can be used to write Perl programs in a subset of Latin.

In 2005, a group of Perl developers who also had an interest in JavaScript got together to create JSAN, the JavaScript Archive Network. The JSAN is a near-direct port of the CPAN infrastructure for use with the JavaScript language, which for most of its lifespan did not have a cohesive "community".

In 2008, after a chance meeting with CPAN admin Adam Kennedy at the Open Source Developers Conference, Linux kernel developer Rusty Russell created the CCAN, the Comprehensive C Archive Network. The CCAN is a direct port of the CPAN architecture for use with the C language.

CRAN, the Comprehensive R Archive Network, is a set of mirrors hosting the R programming language distribution(s), documentation, and contributed extensions.

Adam Kennedy is an Australian Perl programmer, and one of several CPAN administrators. Under his CPAN author id of ADAMK, he is the maintainer of over 200 module distributions on CPAN, which places him at the top of the CPAN contribution leaderboard. Kennedy is the first maintainer of more than 200 CPAN modules, many of which he has adopted from other authors and included in his Open Repository, which is available for use by any registered CPAN author. He is a frequent presenter at open source conferences such as OSDC, OSCON, and YAPC as well as the Perl QA hackathons.

Kennedy has developed some significant modules for the Perl programming language, particularly in the area of tools to improve the development and build toolchain such as PPI (a Perl parser), CPAN::Metrics (generate metrics on the 16 m+ lines of code in CPAN), Portable Perl (a.k.a. "Perl on a Stick") and Padre (a Perl IDE). He has also been a strong advocate of platform equality for Perl on Windows, and started both the Win32 Perl Wiki and the Strawberry Perl distribution for Windows.


The Perl PPI Parser has provided an essential building block for Perl 5 code analysis, documentation, and refactoring tools. Perl::Critic uses PPI to critique Perl source code against the criteria in Perl Best Practices; the Padre IDE uses PPI for code analysis and refactoring. The PPI documentation makes reference to the truism that "Only perl (the interpreter) can parse Perl (source code)" because it is a dynamic language; a post on PerlMonks posits a formal proof.


Strawberry Perl is a binary distribution of Perl for the Windows operating system. Unlike most Windows-based Perl distributions, Strawberry Perl also bundles a C compiler, make tool, and some pre-configured modules to improve compatibility with the Windows environment. The aim of this distribution is to provide a practical environment to test and use the latest modules available from CPAN. Larry Wall, creator of Perl, has endorsed this as good port for Windows.


Portable Perl is the first Perl distribution suitable for installation to portable flash memory devices, iPods, mobile phones, cameras, etc. The first release was distributed on USB thumbdrives at OSCON 2008. Initial work for this project was funded by a grant from The Perl Foundation.


Adam Kennedy has spoken at a number of technical conferences worldwide, including at OSCON, OSDC, Linux.conf.au, and numerous YAPC events. These talks include:



Adam moved to San Francisco, CA in the United States in 2012. He worked for Kaggle from 2012 to 2015 and began working as an engineer with Apple in 2016.. He currently holds the position of Data Engineering Manager, Siri Search at Apple



</doc>
<doc id="6669" url="https://en.wikipedia.org/wiki?curid=6669" title="Colorado Rockies">
Colorado Rockies

The Colorado Rockies are an American professional baseball team based in Denver, Colorado. The Rockies compete in Major League Baseball (MLB) as a member club of the National League (NL) West division. The team plays its home baseball games at Coors Field, which is located in the Lower Downtown area of Denver. It is owned by the Monfort brothers and managed by Bud Black.

The Rockies began play as an expansion team for the 1993 season, and played their home games for their first two seasons at Mile High Stadium. Since 1995, they have played at Coors Field, which has earned a reputation as a hitter's park. The Rockies have qualified for the postseason five times, each time as a Wild Card winner. In 2007, the team earned its first (and only) NL pennant after winning 14 of their final 15 games in the regular season to secure a Wild Card position, capping the streak off with a 13 inning 9-8 victory against the San Diego Padres in the tiebreaker game affectionately known as "Game 163" by Rockies fans. The Rockies then proceeded to sweep the Philadelphia Phillies and Arizona Diamondbacks in the NLDS and NLCS respectively and entered the 2007 World Series as winners of 21 of their last 22 games. However, they were swept by the American League (AL) champions Boston Red Sox in four games.

From 1993 to 2019, the Rockies have an overall record of 2,033â2,280 (a winning percentage).

Denver had long been a hotbed of Denver Bears/Zephyrs minor league baseball and residents and businesses in the area desired a Major League team. Denver's Mile High Stadium was built originally as Denver Bears Stadium, a minor league baseball stadium that could be upgraded to major league standards. Following the Pittsburgh drug trials in 1985, an unsuccessful attempt was made to purchase the Pittsburgh Pirates and relocate them. However, in January 1990, Colorado's chances for a new team improved significantly when Coors Brewing Company became a limited partner with the AAA Denver Zephyrs. 

In 1991, as part of Major League Baseball's two-team expansion (along with the Florida (now Miami) Marlins), an ownership group representing Denver led by John Antonucci and Michael I. Monus was granted a franchise. They took the name "Rockies" due to Denver's proximity to the Rocky Mountains, which is reflected in their logo; the name was previously used by the city's first NHL team (who are now the New Jersey Devils). Monus and Antonucci were forced to drop out in 1992 after Monus' reputation was ruined by an accounting scandal. Trucking magnate Jerry McMorris stepped in at the 11th hour to save the franchise, allowing the team to begin play in 1993. The Rockies shared Mile High Stadium with the National Football League (NFL)'s Denver Broncos for their first two seasons while Coors Field was constructed. It was completed for the 1995 Major League Baseball season.

In 1993, they started play in the West division of the National League. The Rockies were MLB's first team based in the Mountain Time Zone. They have reached the Major League Baseball postseason five times, each time as the National League wild card team. Twice (1995 and 2009) they were eliminated in the first round of the playoffs. In 2007, the Rockies advanced to the World Series, only to be swept by the Boston Red Sox. Like their expansion brethren, the Miami Marlins, they have never won a division title since their establishment; they are also one of two current MLB teams that have never won their division.

The Rockies have played their home games at Coors Field since 1995. Their newest spring training home, Salt River Fields at Talking Stick in Scottsdale, Arizona, opened in March 2011 and is shared with the Arizona Diamondbacks.

At the start of the 2012 season, the Rockies introduced "Purple Mondays" in which the team wears its purple uniform every Monday game day.

In 2020, Larry Walker was the first Colorado Rockies player to be inducted to the Baseball Hall of Fame.











Todd Helton is the first Colorado player to have his number (17) retired, which was done on Sunday, August 17, 2014.

Newly-elected Hall of Fame member, Larry Walker, will have his number (33) retired on April 19, 2020 at Coors Field.

Jackie Robinson's number, 42, was retired throughout all of baseball in 1997.

Keli McGregor had worked with the Rockies since their inception in 1993, rising from senior director of operations to team president in 2002, until his death on April 20, 2010. He is honored at Coors Field alongside Helton and Robinson with his initials.

The Colorado Rockies farm system consists of eight minor league affiliates.

As of 2010, Rockies' flagship radio station is KOA 850AM, with some late-season games broadcast on KHOW 630 AM due to conflicts with Denver Broncos games. The Rockies Radio Network is composed of 38 affiliate stations in eight states.

As of 2019, Jack Corrigifan is the radio announcer, serving as a backup TV announcer whenever Drew Goodman is not available.

In January 2020, long-time KOA radio announcer Jerry Schemmel was let go from his role for budgetary reasons from KOAâs parent company. 

As of 2013, Spanish language radio broadcasts of the Rockies are heard on KNRV 1150 AM.

As of 2013, all games are produced and televised by AT&T SportsNet Rocky Mountain. All 150 games produced by AT&T SportsNet Rocky Mountain are broadcast in HD. Jeff Huson and Drew Goodman are the usual the TV broadcast team, with Ryan Spilborghs and Taylor McGregor handling on-field coverage and clubhouse interviews. Jenny Cavnar, Jason Hirsh, and Cory Sullivan handle the pre-game and post-game shows. Schemmel, Corrigan, Spilborghs, Cavnar, and Sullivan also fill in as play-by-play or color commentator during absences of Huson or Goodman.

The Rockies led MLB attendance records for the first seven years of their existence. The inaugural season is currently the MLB all-time record for home attendance.

+ = 57 home games in strike shortened season. ++ = 72 home games in strike shortened season.


</doc>
<doc id="6670" url="https://en.wikipedia.org/wiki?curid=6670" title="Cement">
Cement

A cement is a binder, a substance used for construction that sets, hardens, and adheres to other materials to bind them together. Cement is seldom used on its own, but rather to bind sand and gravel (aggregate) together. Cement mixed with fine aggregate produces mortar for masonry, or with sand and gravel, produces concrete. Concrete is the most widely used material in existence and is only behind water as the planet's most-consumed resource.

Cements used in construction are usually inorganic, often lime or calcium silicate based, which can be characterized as non-hydraulic or hydraulic respectively, depending on the ability of the cement to set in the presence of water (see hydraulic and non-hydraulic lime plaster).

Non-hydraulic cement does not set in wet conditions or under water. Rather, it sets as it dries and reacts with carbon dioxide in the air. It is resistant to attack by chemicals after setting.

Hydraulic cements (e.g., Portland cement) set and become adhesive due to a chemical reaction between the dry ingredients and water. The chemical reaction results in mineral hydrates that are not very water-soluble and so are quite durable in water and safe from chemical attack. This allows setting in wet conditions or under water and further protects the hardened material from chemical attack. The chemical process for hydraulic cement was found by ancient Romans who used volcanic ash (pozzolana) with added lime (calcium oxide).

The word "cement" can be traced back to the Roman term "opus caementicium", used to describe masonry resembling modern concrete that was made from crushed rock with burnt lime as binder. The volcanic ash and pulverized brick supplements that were added to the burnt lime, to obtain a hydraulic binder, were later referred to as "cementum", "cimentum", "cÃ¤ment", and "cement". In modern times, organic polymers are sometimes used as cements in concrete.

If the cement industry were a country, it would be the third largest carbon dioxide emitter in the world with up to 2.8bn tonnes, surpassed only by China and the US.

Cement materials can be classified into two distinct categories: non-hydraulic cements and hydraulic cements according to their respective setting and hardening mechanisms. Hydraulic cements setting and hardening involve hydration reactions and therefore require water, while non-hydraulic cements only react with a gas and can directly set under air.

Non-hydraulic cement, such as slaked lime (calcium oxide mixed with water), hardens by carbonation in contact with carbon dioxide, which is present in the air (~Â 412 vol. ppm â 0.04 vol.Â %). First calcium oxide (lime) is produced from calcium carbonate (limestone or chalk) by calcination at temperatures above 825Â Â°C (1,517Â Â°F) for about 10 hours at atmospheric pressure:
The calcium oxide is then "spent" (slaked) mixing it with water to make slaked lime (calcium hydroxide):
Once the excess water is completely evaporated (this process is technically called "setting"), the carbonation starts:
This reaction is slow, because the partial pressure of carbon dioxide in the air is low (~Â 0.4Â millibar). The carbonation reaction requires that the dry cement be exposed to air, so the slaked lime is a non-hydraulic cement and cannot be used under water. This process is called the "lime cycle".

Conversely, hydraulic cement hardens by hydration of the clinker minerals when water is added. Hydraulic cements (such as Portland cement) are made of a mixture of silicates and oxides, the four main mineral phases of the clinker, abbreviated in the cement chemist notation, being:

The silicates are responsible for the cement's mechanical properties â the tricalcium aluminate and brownmillerite are essential for the formation of the liquid phase during the sintering (firing) process of clinker at high temperature in the kiln. The chemistry of these reactions is not completely clear and is still the object of research.

Perhaps the earliest known occurrence of cement is from twelve million years ago. A deposit of cement was formed after an occurrence of oil shale located adjacent to a bed of limestone burned due to natural causes. These ancient deposits were investigated in the 1960s and 1970s.

Cement, chemically speaking, is a product that includes lime as the primary binding ingredient, but is far from the first material used for cementation. The Babylonians and Assyrians used bitumen to bind together burnt brick or alabaster slabs. In Egypt stone blocks were cemented together with a mortar made of sand and roughly burnt gypsum (CaSO Â· 2HO), which often contained calcium carbonate (CaCO).

Lime (calcium oxide) was used on Crete and by the ancient Greeks. There is evidence that the Minoans of Crete used crushed potshards as an artificial pozzolan for hydraulic cement. Nobody knows who first discovered that a combination of hydrated non-hydraulic lime and a pozzolan produces a hydraulic mixture (see also: Pozzolanic reaction), but such concrete was used by the Ancient Macedonians, and three centuries later on a large scale by Roman engineers.

The Greeks used volcanic tuff from the island of Thera as their pozzolan and the Romans used crushed volcanic ash (activated aluminium silicates) with lime. This mixture could set under water, increasing its resistance. The material was called "pozzolana" from the town of Pozzuoli, west of Naples where volcanic ash was extracted. In the absence of pozzolanic ash, the Romans used powdered brick or pottery as a substitute and they may have used crushed tiles for this purpose before discovering natural sources near Rome. The huge dome of the Pantheon in Rome and the massive Baths of Caracalla are examples of ancient structures made from these concretes, many of which still stand. The vast system of Roman aqueducts also made extensive use of hydraulic cement. Roman concrete was rarely used on the outside of buildings. The normal technique was to use brick facing material as the formwork for an infill of mortar mixed with an aggregate of broken pieces of stone, brick, potsherds, recycled chunks of concrete, or other building rubble.

Any preservation of this knowledge in literature from the Middle Ages is unknown, but medieval masons and some military engineers actively used hydraulic cement in structures such as canals, fortresses, harbors, and shipbuilding facilities. A mixture of lime mortar and aggregate with brick or stone facing material was used in the Eastern Roman Empire as well as in the West into the Gothic period. The German Rhineland continued to use hydraulic mortar throughout the Middle Ages, having local pozzolana deposits called trass.

Tabby is a building material made from oyster-shell lime, sand, and whole oyster shells to form a concrete. The Spanish introduced it to the Americas in the sixteenth century.

The technical knowledge for making hydraulic cement was formalized by French and British engineers in the 18th century.

John Smeaton made an important contribution to the development of cements while planning the construction of the third Eddystone Lighthouse (1755â59) in the English Channel now known as Smeaton's Tower. He needed a hydraulic mortar that would set and develop some strength in the twelve-hour period between successive high tides. He performed experiments with combinations of different limestones and additives including trass and pozzolanas and did exhaustive market research on the available hydraulic limes, visiting their production sites, and noted that the "hydraulicity" of the lime was directly related to the clay content of the limestone used to make it. Smeaton was a civil engineer by profession, and took the idea no further.

In the South Atlantic seaboard of the United States, tabby relying on the oyster-shell middens of earlier Native American populations was used in house construction from the 1730s to the 1860s.

In Britain particularly, good quality building stone became ever more expensive during a period of rapid growth, and it became a common practice to construct prestige buildings from the new industrial bricks, and to finish them with a stucco to imitate stone. Hydraulic limes were favored for this, but the need for a fast set time encouraged the development of new cements. Most famous was Parker's "Roman cement". This was developed by James Parker in the 1780s, and finally patented in 1796. It was, in fact, nothing like material used by the Romans, but was a "natural cement" made by burning septaria â nodules that are found in certain clay deposits, and that contain both clay minerals and calcium carbonate. The burnt nodules were ground to a fine powder. This product, made into a mortar with sand, set in 5â15 minutes. The success of "Roman cement" led other manufacturers to develop rival products by burning artificial hydraulic lime cements of clay and chalk.
Roman cement quickly became popular but was largely replaced by Portland cement in the 1850s.

Apparently unaware of Smeaton's work, the same principle was identified by Frenchman Louis Vicat in the first decade of the nineteenth century. Vicat went on to devise a method of combining chalk and clay into an intimate mixture, and, burning this, produced an "artificial cement" in 1817 considered the "principal forerunner" of Portland cement and "...Edgar Dobbs of Southwark patented a cement of this kind in 1811."

In Russia, Egor Cheliev created a new binder by mixing lime and clay. His results were published in 1822 in his book "A Treatise on the Art to Prepare a Good Mortar" published in St. Petersburg. A few years later in 1825, he published another book, which described various methods of making cement and concrete, and the benefits of cement in the construction of buildings and embankments.
Portland cement, the most common type of cement in general use around the world as a basic ingredient of concrete, mortar, stucco, and non-speciality grout, was developed in England in the mid 19th century, and usually originates from limestone. James Frost produced what he called "British cement" in a similar manner around the same time, but did not obtain a patent until 1822. In 1824, Joseph Aspdin patented a similar material, which he called "Portland cement", because the render made from it was in color similar to the prestigious Portland stone quarried on the Isle of Portland, Dorset, England. However, Aspdins' cement was nothing like modern Portland cement but was a first step in its development, called a "proto-Portland cement". Joseph Aspdins' son William Aspdin had left his father's company and in his cement manufacturing apparently accidentally produced calcium silicates in the 1840s, a middle step in the development of Portland cement. William Aspdin's innovation was counterintuitive for manufacturers of "artificial cements", because they required more lime in the mix (a problem for his father), a much higher kiln temperature (and therefore more fuel), and the resulting clinker was very hard and rapidly wore down the millstones, which were the only available grinding technology of the time. Manufacturing costs were therefore considerably higher, but the product set reasonably slowly and developed strength quickly, thus opening up a market for use in concrete. The use of concrete in construction grew rapidly from 1850 onward, and was soon the dominant use for cements. Thus Portland cement began its predominant role. Isaac Charles Johnson further refined the production of "meso-Portland cement" (middle stage of development) and claimed he was the real father of Portland cement.

Setting time and "early strength" are important characteristics of cements. Hydraulic limes, "natural" cements, and "artificial" cements all rely on their belite (2 CaO Â· SiO, abbreviated as CS) content for strength development. Belite develops strength slowly. Because they were burned at temperatures below , they contained no alite (3 CaO Â· SiO, abbreviated as CS) , which is responsible for early strength in modern cements. The first cement to consistently contain alite was made by William Aspdin in the early 1840s: This was what we call today "modern" Portland cement. Because of the air of mystery with which William Aspdin surrounded his product, others ("e.g.," Vicat and Johnson) have claimed precedence in this invention, but recent analysis of both his concrete and raw cement have shown that William Aspdin's product made at Northfleet, Kent was a true alite-based cement. However, Aspdin's methods were "rule-of-thumb": Vicat is responsible for establishing the chemical basis of these cements, and Johnson established the importance of sintering the mix in the kiln.

In the US the first large-scale use of cement was Rosendale cement, a natural cement mined from a massive deposit of a large dolomite deposit discovered in the early 19th century near Rosendale, New York. Rosendale cement was extremely popular for the foundation of buildings ("e.g.", Statue of Liberty, Capitol Building, Brooklyn Bridge) and lining water pipes.

Sorel cement, or magnesia-based cement, was patented in 1867 by the Frenchman Stanislas Sorel. It was stronger than Portland cement but its poor water resistance (leaching) and corrosive properties (pitting corrosion due to the presence of leachable chloride anions and the low pH (8.5â9.5) of its pore water) limited its use as reinforced concrete for building construction. 

The next development in the manufacture of Portland cement was the introduction of the rotary kiln, which produced a stronger (more alite, CS, formed at higher temperature, 1450 Â°C), more homogeneous clinker mixture and facilitated a continuous manufacturing process.

Calcium aluminate cements were patented in 1908 in France by Jules Bied for better resistance to sulfates. Also in 1908, Thomas Edison experimented with pre-cast concrete in houses in Union, N.J.

In the US, after World War One, the long curing time of at least a month for Rosendale cement made it unpopular for constructing highways and bridges, and many states and construction firms turned to Portland cement. Because of the switch to Portland cement, by the end of the 1920s only one of the 15 Rosendale cement companies had survived. But in the early 1930s, builders discovered that, while Portland cement set faster, it was not as durable, especially for highwaysâto the point that some states stopped building highways and roads with cement. Bertrain H. Wait, an engineer whose company had helped construct the New York City's Catskill Aqueduct, was impressed with the durability of Rosendale cement, and came up with a blend of both Rosendale and Portland cements that had the good attributes of both. It was highly durable and had a much faster setting time. Wait convinced the New York Commissioner of Highways to construct an experimental section of highway near New Paltz, New York, using one sack of Rosendale to six sacks of Portland cement. It was a success, and for decades the Rosendale-Portland cement blend was used in highway and bridge construction.

Cementitious materials have been used as a nuclear waste immobilizing matrix for more than a half-century. Technologies of waste cementation have been developed and deployed at industrial scale in many countries. Cementitious wasteforms require a careful selection and design process adapted to each specific type of waste to satisfy the strict waste acceptance criteria for long-term storage and disposal.

Modern hydraulic development began with the start of the Industrial Revolution (around 1800), driven by three main needs:
Modern cements are often Portland cement or Portland cement blends, but industry also uses other cements.

Portland cement is by far the most common type of cement in general use around the world. This cement is made by heating limestone (calcium carbonate) with other materials (such as clay) to in a kiln, in a process known as calcination that liberates a molecule of carbon dioxide from the calcium carbonate to form calcium oxide, or quicklime, which then chemically combines with the other materials in the mix to form calcium silicates and other cementitious compounds. The resulting hard substance, called 'clinker', is then ground with a small amount of gypsum into a powder to make "ordinary Portland cement", the most commonly used type of cement (often referred to as OPC).
Portland cement is a basic ingredient of concrete, mortar, and most non-specialty grout. The most common use for Portland cement is to make concrete. Concrete is a composite material made of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape, and once it hardens, can be a structural (load bearing) element. Portland cement may be grey or white.

Portland cement blends are often available as inter-ground mixtures from cement producers, but similar formulations are often also mixed from the ground components at the concrete mixing plant.

Portland blast-furnace slag cement, or Blast furnace cement (ASTM C595 and EN 197-1 nomenclature respectively), contains up to 95% ground granulated blast furnace slag, with the rest Portland clinker and a little gypsum. All compositions produce high ultimate strength, but as slag content is increased, early strength is reduced, while sulfate resistance increases and heat evolution diminishes. Used as an economic alternative to Portland sulfate-resisting and low-heat cements.

Portland-fly ash cement contains up to 40% fly ash under ASTM standards (ASTM C595), or 35% under EN standards (EN 197-1). The fly ash is pozzolanic, so that ultimate strength is maintained. Because fly ash addition allows a lower concrete water content, early strength can also be maintained. Where good quality cheap fly ash is available, this can be an economic alternative to ordinary Portland cement.

Portland pozzolan cement includes fly ash cement, since fly ash is a pozzolan, but also includes cements made from other natural or artificial pozzolans. In countries where volcanic ashes are available (e.g., Italy, Chile, Mexico, the Philippines), these cements are often the most common form in use. The maximum replacement ratios are generally defined as for Portland-fly ash cement.

Portland silica fume cement. Addition of silica fume can yield exceptionally high strengths, and cements containing 5â20% silica fume are occasionally produced, with 10% being the maximum allowed addition under EN 197-1. However, silica fume is more usually added to Portland cement at the concrete mixer.

Masonry cements are used for preparing bricklaying mortars and stuccos, and must not be used in concrete. They are usually complex proprietary formulations containing Portland clinker and a number of other ingredients that may include limestone, hydrated lime, air entrainers, retarders, waterproofers and coloring agents. They are formulated to yield workable mortars that allow rapid and consistent masonry work. Subtle variations of Masonry cement in the US are Plastic Cements and Stucco Cements. These are designed to produce a controlled bond with masonry blocks.

Expansive cements contain, in addition to Portland clinker, expansive clinkers (usually sulfoaluminate clinkers), and are designed to offset the effects of drying shrinkage normally encountered in hydraulic cements. This cement can make concrete for floor slabs (up to 60Â m square) without contraction joints.

White blended cements may be made using white clinker (containing little or no iron) and white supplementary materials such as high-purity metakaolin. Colored cements serve decorative purposes. Some standards allow the addition of pigments to produce "colored Portland cement". Other standards (e.g., ASTM) do not allow pigments in Portland cement, and colored cements are sold as "blended hydraulic cements".

Very finely ground cements are cement mixed with sand or with slag or other pozzolan type minerals that are extremely finely ground together. Such cements can have the same physical characteristics as normal cement but with 50% less cement, particularly due to their increased surface area for the chemical reaction. Even with intensive grinding they can use up to 50% less energy (and thus less carbon emissions) to fabricate than ordinary Portland cements.

"Pozzolan-lime cements" are mixtures of ground pozzolan and lime. These are the cements the Romans used, and are present in surviving Roman structures like the Pantheon in Rome. They develop strength slowly, but their ultimate strength can be very high. The hydration products that produce strength are essentially the same as those in Portland cement.

"Slag-lime cements"âground granulated blast-furnace slag is not hydraulic on its own, but is "activated" by addition of alkalis, most economically using lime. They are similar to pozzolan lime cements in their properties. Only granulated slag (i.e., water-quenched, glassy slag) is effective as a cement component.

"Supersulfated cements" contain about 80% ground granulated blast furnace slag, 15% gypsum or anhydrite and a little Portland clinker or lime as an activator. They produce strength by formation of ettringite, with strength growth similar to a slow Portland cement. They exhibit good resistance to aggressive agents, including sulfate.
Calcium aluminate cements are hydraulic cements made primarily from limestone and bauxite. The active ingredients are monocalcium aluminate CaAlO (CaO Â· AlO or CA in Cement chemist notation, CCN) and mayenite CaAlO (12 CaO Â· 7 AlO, or CA in CCN). Strength forms by hydration to calcium aluminate hydrates. They are well-adapted for use in refractory (high-temperature resistant) concretes, e.g., for furnace linings.

"Calcium sulfoaluminate cements" are made from clinkers that include ye'elimite (Ca(AlO)SO or CA in Cement chemist's notation) as a primary phase. They are used in expansive cements, in ultra-high early strength cements, and in "low-energy" cements. Hydration produces ettringite, and specialized physical properties (such as expansion or rapid reaction) are obtained by adjustment of the availability of calcium and sulfate ions. Their use as a low-energy alternative to Portland cement has been pioneered in China, where several million tonnes per year are produced. Energy requirements are lower because of the lower kiln temperatures required for reaction, and the lower amount of limestone (which must be endothermically decarbonated) in the mix. In addition, the lower limestone content and lower fuel consumption leads to a CO emission around half that associated with Portland clinker. However, SO emissions are usually significantly higher.

""Natural" cements" corresponding to certain cements of the pre-Portland era, are produced by burning argillaceous limestones at moderate temperatures. The level of clay components in the limestone (around 30â35%) is such that large amounts of belite (the low-early strength, high-late strength mineral in Portland cement) are formed without the formation of excessive amounts of free lime. As with any natural material, such cements have highly variable properties.

"Geopolymer cements" are made from mixtures of water-soluble alkali metal silicates, and aluminosilicate mineral powders such as fly ash and metakaolin.

Polymer cements are made from organic chemicals that polymerise. Producers often use thermoset materials. While they are often significantly more expensive, they can give a water proof material that has useful tensile strength.

"Sorel Cement" is a hard, durable cement made by combining magnesium oxide and a magnesium chloride solution

Cement starts to set when mixed with water, which causes a series of hydration chemical reactions. The constituents slowly hydrate and the mineral hydrates solidify and harden. The interlocking of the hydrates gives cement its strength. Contrary to popular belief, hydraulic cement does not set by drying out â proper curing requires maintaining the appropriate moisture content necessary for the hydration reactions during the setting and the hardening processes. If hydraulic cements dry out during the curing phase, the resulting product can be insufficiently hydrated and significantly weakened. A minimum temperature of 5Â Â°C is recommended, and no more than 30Â Â°C. The concrete at young age must be protected against water evaporation due to direct insolation, elevated temperature, low relative humidity and wind.

Bags of cement routinely have health and safety warnings printed on them because not only is cement highly alkaline, but the setting process is exothermic. As a result, wet cement is strongly caustic (pH = 13.5) and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. Some trace elements, such as chromium, from impurities naturally present in the raw materials used to produce cement may cause allergic dermatitis. Reducing agents such as ferrous sulfate (FeSO) are often added to cement to convert the carcinogenic hexavalent chromate (CrO) into trivalent chromium (Cr), a less toxic chemical species. Cement users need also to wear appropriate gloves and protective clothing.

In 2010, the world production of hydraulic cement was . The top three producers were China with 1,800, India with 220, and USA with 63.5 million tonnes for a total of over half the world total by the world's three most populated states.

For the world capacity to produce cement in 2010, the situation was similar with the top three states (China, India, and USA) accounting for just under half the world total capacity.

Over 2011 and 2012, global consumption continued to climb, rising to 3585 Mt in 2011 and 3736 Mt in 2012, while annual growth rates eased to 8.3% and 4.2%, respectively.

China, representing an increasing share of world cement consumption, remains the main engine of global growth. By 2012, Chinese demand was recorded at 2160 Mt, representing 58% of world consumption. Annual growth rates, which reached 16% in 2010, appear to have softened, slowing to 5â6% over 2011 and 2012, as China's economy targets a more sustainable growth rate.

Outside of China, worldwide consumption climbed by 4.4% to 1462 Mt in 2010, 5% to 1535 Mt in 2011, and finally 2.7% to 1576 Mt in 2012.

Iran is now the 3rd largest cement producer in the world and has increased its output by over 10% from 2008 to 2011. Due to climbing energy costs in Pakistan and other major cement-producing countries, Iran is in a unique position as a trading partner, utilizing its own surplus petroleum to power clinker plants. Now a top producer in the Middle-East, Iran is further increasing its dominant position in local markets and abroad.

The performance in North America and Europe over the 2010â12 period contrasted strikingly with that of China, as the global financial crisis evolved into a sovereign debt crisis for many economies in this region and recession. Cement consumption levels for this region fell by 1.9% in 2010 to 445 Mt, recovered by 4.9% in 2011, then dipped again by 1.1% in 2012.

The performance in the rest of the world, which includes many emerging economies in Asia, Africa and Latin America and representing some 1020 Mt cement demand in 2010, was positive and more than offset the declines in North America and Europe. Annual consumption growth was recorded at 7.4% in 2010, moderating to 5.1% and 4.3% in 2011 and 2012, respectively.

As at year-end 2012, the global cement industry consisted of 5673 cement production facilities, including both integrated and grinding, of which 3900 were located in China and 1773 in the rest of the world.

Total cement capacity worldwide was recorded at 5245 Mt in 2012, with 2950 Mt located in China and 2295 Mt in the rest of the world.

"For the past 18 years, China consistently has produced more cement than any other country in the world. [...] (However,) China's cement export peaked in 1994 with 11 million tonnes shipped out and has been in steady decline ever since. Only 5.18 million tonnes were exported out of China in 2002. Offered at $34 a ton, Chinese cement is pricing itself out of the market as Thailand is asking as little as $20 for the same quality."

In 2006, it was estimated that China manufactured 1.235 billion tonnes of cement, which was 44% of the world total cement production. "Demand for cement in China is expected to advance 5.4% annually and exceed 1 billion tonnes in 2008, driven by slowing but healthy growth in construction expenditures. Cement consumed in China will amount to 44% of global demand, and China will remain the world's largest national consumer of cement by a large margin."

In 2010, 3.3 billion tonnes of cement was consumed globally. Of this, China accounted for 1.8 billion tonnes.

Cement manufacture causes environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust, gases, noise and vibration when operating machinery and during blasting in quarries, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and manufacture of cement is widely used, and equipment to trap and separate exhaust gases are coming into increased use. Environmental protection also includes the re-integration of quarries into the countryside after they have been closed down by returning them to nature or re-cultivating them.

Carbon concentration in cement spans from â5% in cement structures to â8% in the case of roads in cement. Cement manufacturing releases in the atmosphere both directly when calcium carbonate is heated, producing lime and carbon dioxide, and also indirectly through the use of energy if its production involves the emission of CO. The cement industry produces about 10% of global man-made CO emissions, of which 60% is from the chemical process, and 40% from burning fuel. A Chatham House study from 2018 estimates that the 4 billion tonnes of cement produced annually account for 8% of worldwide CO emissions.

Nearly 900Â kg of CO are emitted for every 1000Â kg of Portland cement produced. In the European Union, the specific energy consumption for the production of cement clinker has been reduced by approximately 30% since the 1970s. This reduction in primary energy requirements is equivalent to approximately 11 million tonnes of coal per year with corresponding benefits in reduction of CO emissions. This accounts for approximately 5% of anthropogenic CO.

The majority of carbon dioxide emissions in the manufacture of Portland cement (approximately 60%) are produced from the chemical decomposition of limestone to lime, an ingredient in Portland cement clinker. These emissions may be reduced by lowering the clinker content of cement. They can also be reduced by alternative fabrication methods such as the intergrinding cement with sand or with slag or other pozzolan type minerals to a very fine powder.

To reduce the transport of heavier raw materials and to minimize the associated costs, it is more economical to build cement plants closer to the limestone quarries rather than to the consumer centers.

In certain applications, lime mortar reabsorbs some of the CO as was released in its manufacture, and has a lower energy requirement in production than mainstream cement. Newly developed cement types from Novacem and Eco-cement can absorb carbon dioxide from ambient air during hardening.

In some circumstances, mainly depending on the origin and the composition of the raw materials used, the high-temperature calcination process of limestone and clay minerals can release in the atmosphere gases and dust rich in volatile heavy metals, e.g. thallium, cadmium and mercury are the most toxic. Heavy metals (Tl, Cd, Hg, ...) and also selenium are often found as trace elements in common metal sulfides (pyrite (FeS), zinc blende (ZnS), galena (PbS), ...) present as secondary minerals in most of the raw materials. Environmental regulations exist in many countries to limit these emissions. As of 2011 in the United States, cement kilns are "legally allowed to pump more toxins into the air than are hazardous-waste incinerators."

The presence of heavy metals in the clinker arises both from the natural raw materials and from the use of recycled by-products or alternative fuels. The high pH prevailing in the cement porewater (12.5 < pH < 13.5) limits the mobility of many heavy metals by decreasing their solubility and increasing their sorption onto the cement mineral phases. Nickel, zinc and lead are commonly found in cement in non-negligible concentrations. Chromium may also directly arise as natural impurity from the raw materials or as secondary contamination from the abrasion of hard chromium steel alloys used in the ball mills when the clinker is ground. As chromate (CrO) is toxic and may cause severe skin allergies at trace concentration, it is sometimes reduced into trivalent Cr(III) by addition of ferrous sulfate (FeSO).

A cement plant consumes 3 to 6 GJ of fuel per tonne of clinker produced, depending on the raw materials and the process used. Most cement kilns today use coal and petroleum coke as primary fuels, and to a lesser extent natural gas and fuel oil. Selected waste and by-products with recoverable calorific value can be used as fuels in a cement kiln (referred to as co-processing), replacing a portion of conventional fossil fuels, like coal, if they meet strict specifications. Selected waste and by-products containing useful minerals such as calcium, silica, alumina, and iron can be used as raw materials in the kiln, replacing raw materials such as clay, shale, and limestone. Because some materials have both useful mineral content and recoverable calorific value, the distinction between alternative fuels and raw materials is not always clear. For example, sewage sludge has a low but significant calorific value, and burns to give ash containing minerals useful in the clinker matrix. Scrap automobile and truck tires are useful in cement manufacturing as they have high calorific value and the iron embedded in tires is useful as a feed stock.

Clinker is manufactured by heating raw materials inside the main burner of a kiln to a temperature of 1450Â Â°C. The flame reaches temperatures of 1800Â Â°C. The material remains at 1200Â Â°C for 12â15 seconds at 1800Â Â°C for 5â8 seconds (also referred to as residence time). These characteristics of a clinker kiln offer numerous benefits and they ensure a complete destruction of organic compounds, a total neutralization of acid gases, sulphur oxides and hydrogen chloride. Furthermore, heavy metal traces are embedded in the clinker structure and no by-products, such as ash of residues, are produced.

The EU cement industry already uses more than 40% fuels derived from waste and biomass in supplying the thermal energy to the grey clinker making process. Although the choice for this so-called alternative fuels (AF) is typically cost driven, other factors are becoming more important. Use of alternative fuels provides benefits for both society and the company: CO-emissions are lower than with fossil fuels, waste can be co-processed in an efficient and sustainable manner and the demand for certain virgin materials can be reduced. Yet there are large differences in the share of alternative fuels used between the European Union (EU) member states. The societal benefits could be improved if more member states increase their alternative fuels share. The Ecofys study assessed the barriers and opportunities for further uptake of alternative fuels in 14 EU member states. The Ecofys study found that local factors constrain the market potential to a much larger extent than the technical and economic feasibility of the cement industry itself.

Ecological cement is a cementitious material that meets or exceeds the functional performance capabilities of ordinary Portland cement by incorporating and optimizing recycled materials, thereby reducing consumption of natural raw materials, water, and energy, resulting in a more sustainable construction material. One is Geopolymer cement.

New manufacturing processes for producing ecological cement are being researched with the goal to reduce, or even eliminate, the production and release of damaging pollutants and greenhouse gasses, particularly CO.

Growing environmental concerns and the increasing cost of fuels of fossil origin have resulted in many countries in a sharp reduction of the resources needed to produce cement and effluents (dust and exhaust gases).

A team at the University of Edinburgh has developed the 'DUPE' process based on the microbial activity of "Sporosarcina pasteurii", a bacterium precipitating calcium carbonate, which, when mixed with sand and urine, can produce mortar blocks with a compressive strength 70% of that of conventional construction materials.




</doc>
<doc id="6671" url="https://en.wikipedia.org/wiki?curid=6671" title="Cincinnati Reds">
Cincinnati Reds

The Cincinnati Reds are an American professional baseball team based in Cincinnati, Ohio. They compete in Major League Baseball (MLB) as a member club of the National League (NL) Central division. They were a charter member of the American Association in 1882 and joined the NL in 1890.

The Reds played in the NL West division from 1969 to 1993, before joining the Central division in 1994. They have won five World Series championships, nine NL pennants, one AA pennant, and 10 division titles. The team plays its home games at Great American Ball Park, which opened in 2003 replacing Riverfront Stadium. Bob Castellini has been chief executive officer since 2006.

From 1882 to 2019, the Reds' overall win-loss record is 10,599â10,393 (a winning percentage).

The origins of the modern Cincinnati Reds can be traced to the expulsion of an earlier team bearing that name. In 1876, Cincinnati became one of the charter members of the new National League, but the club ran afoul of league organizer and long-time president William Hulbert for selling beer during games and renting out their ballpark on Sundays. Both were important activities to entice the city's large German population. While Hulbert made clear his distaste for both beer and Sunday baseball at the founding of the league, neither practice was actually against league rules in those early years. On October 6, 1880, however, seven of the eight team owners pledged at a special league meeting to formally ban both beer and Sunday baseball at the regular league meeting that December. Only Cincinnati president W. H. Kennett refused to sign the pledge, so the other owners formally expelled Cincinnati for violating a rule that would not actually go into effect for two more months.

Cincinnati's expulsion from the National League incensed "Cincinnati Enquirer" sports editor O. P. Caylor, who made two attempts to form a new league on behalf of the receivers for the now bankrupt Reds franchise. When these attempts failed, he formed a new independent ballclub known as the Red Stockings in the Spring of 1881, and brought the team to St. Louis for a weekend exhibition. The Reds' first game was a 12â3 victory over the St. Louis club. After the 1881 series proved a success, Caylor and a former president of the old Reds named Justus Thorner received an invitation from Philadelphia businessman Horace Phillips to attend a meeting of several clubs in Pittsburgh with the intent of establishing a rival to the National League. Upon arriving in the city, however, Caylor and Thorner discovered that no other owners had decided to accept the invitation, with even Phillips not bothering to attend his own meeting. By chance, the duo met a former pitcher named Al Pratt, who hooked them up with former Pittsburgh Alleghenys president H. Denny McKnight. Together, the three men hatched a scheme to form a new league by sending a telegram to each of the other owners who were supposed to attend the meeting stating that he was the only person who did not attend and that everyone else was enthusiastic about the new venture and eager to attend a second meeting in Cincinnati. The ploy worked, and the American Association was officially formed at the Hotel Gibson in Cincinnati with the new Reds a charter member with Thorner as president.

Led by the hitting of third baseman Hick Carpenter, the defense of future Hall of Fame second baseman Bid McPhee, and the pitching of 40-game-winner Will White, the Reds won the inaugural AA pennant in 1882. With the establishment of the Union Association Justus Thorner left the club to finance the Cincinnati Outlaw Reds and managed to acquire the lease on the Reds Bank Street Grounds playing field, forcing new president Aaron Stern to relocate three blocks away at the hastily built League Park. The club never placed higher than second or lower than fifth for the rest of its tenure in the American Association.

The Cincinnati Red Stockings left the American Association on November 14, 1889 and joined the National League along with the Brooklyn Bridegrooms after a dispute with St. Louis Browns owner Chris Von Der Ahe over the selection of a new league president. The National League was happy to accept the teams in part due to the emergence of the new Player's League. This new league, an early failed attempt to break the reserve clause in baseball, threatened both existing leagues. Because the National League decided to expand while the American Association was weakening, the team accepted an invitation to join the National League. It was also at this time that the team first shortened their name from "Red Stockings" to "Reds". The Reds wandered through the 1890s signing local stars and aging veterans. During this time, the team never finished above third place (1897) and never closer than 10 games (1890).

At the start of the 20th century, the Reds had hitting stars Sam Crawford and Cy Seymour. Seymour's .377 average in 1905 was the first individual batting crown won by a Red. In 1911, Bob Bescher stole 81 bases, which is still a team record. Like the previous decade, the 1900s (decade) were not kind to the Reds, as much of the decade was spent in the league's second division.

In 1912, the club opened a new steel-and-concrete ballpark, Redland Field (later to be known as Crosley Field). The Reds had been playing baseball on that same site, the corner of Findlay and Western Avenues on the city's west side, for 28 years, in wooden structures that had been occasionally damaged by fires. By the late 1910s the Reds began to come out of the second division. The 1918 team finished fourth, and new manager Pat Moran led the Reds to an NL pennant in 1919, in what the club advertised as its "Golden Anniversary". The 1919 team had hitting stars Edd Roush and Heinie Groh while the pitching staff was led by Hod Eller and left-hander Harry "Slim" Sallee. The Reds finished ahead of John McGraw's New York Giants, and then won the world championship in eight games over the Chicago White Sox.

By 1920, the "Black Sox" scandal had brought a taint to the Reds' first championship. After 1926, and well into the 1930s, the Reds were second division dwellers. Eppa Rixey, Dolf Luque and Pete Donohue were pitching stars, but the offense never lived up to the pitching. By 1931, the team was bankrupt, the Great Depression was in full swing and Redland Field was in a state of disrepair.

Powel Crosley, Jr., an electronics magnate who, with his brother Lewis M. Crosley, produced radios, refrigerators, and other household items, bought the Reds out of bankruptcy in 1933, and hired Larry MacPhail to be the General Manager. Crosley had started WLW radio, the Reds flagship radio broadcaster, and the Crosley Broadcasting Corporation in Cincinnati, where he was also a prominent civic leader. MacPhail began to develop the Reds' minor league system and expanded the Reds' fan base. The Reds, throughout the 1930s, became a team of "firsts". The now-renamed Crosley Field became the host of the first night game in 1935, which was also the first baseball fireworks night, the fireworks at the game were shot by Joe Rozzi of Rozzi's Famous Fireworks. Johnny Vander Meer became the only pitcher in major league history to throw back-to-back no-hitters in 1938. Thanks to Vander Meer, Paul Derringer and second baseman/third baseman-turned-pitcher Bucky Walters, the Reds had a solid pitching staff. The offense came around in the late 1930s. By 1938 the Reds, now led by manager Bill McKechnie, were out of the second division finishing fourth. Ernie Lombardi was named the National League's Most Valuable Player in 1938. By 1939, they were National League champions, but in the World Series, they were swept by the New York Yankees. In 1940, they repeated as NL Champions, and for the first time in 21 years, the Reds captured a World championship, beating the Detroit Tigers 4 games to 3. Frank McCormick was the 1940 NL MVP. Other position players included Harry Craft, Lonny Frey, Ival Goodman, Lew Riggs and Bill Werber.

World War II and age finally caught up with the Reds. Throughout the 1940s and early 1950s, Cincinnati finished mostly in the second division. In 1944, Joe Nuxhall (who was later to become part of the radio broadcasting team), at age 15, pitched for the Reds on loan from Wilson Junior High school in Hamilton, Ohio. He became the youngest player ever to appear in a major league gameâa record that still stands today. Ewell "The Whip" Blackwell was the main pitching stalwart before arm problems cut short his career. Ted Kluszewski was the NL home run leader in 1954. The rest of the offense was a collection of over-the-hill players and not-ready-for-prime-time youngsters.

In April 1953, the Reds announced a preference to be called the "Redlegs", saying that the name of the club had been "Red Stockings" and then "Redlegs". A newspaper speculated that it was due to the developing political connotation of the word 'red' to mean Communism. From 1956 to 1960, the club's logo was altered to remove the term "REDS" from the inside of the "wishbone "C" symbol. The "REDS" reappeared on the 1961 uniforms, but the point of the "C" was removed, leaving a smooth, non-wishbone curve. The traditional home-uniform logo was restored in 1967.

In 1956, led by National League Rookie of the Year Frank Robinson, the Redlegs hit 221 HR to tie the NL record. By 1961, Robinson was joined by Vada Pinson, Wally Post, Gordy Coleman, and Gene Freese. Pitchers Joey Jay, Jim O'Toole, and Bob Purkey led the staff.

The Reds captured the 1961 National League pennant, holding off the Los Angeles Dodgers and the San Francisco Giants, only to be defeated by the perennially powerful New York Yankees in the World Series.

The Reds had winning teams during the rest of the 1960s, but did not produce any championships. They won 98 games in 1962, paced by Purkey's 23, but finished third. In 1964, they lost the pennant by one game to the Cardinals after having taken first place when the Phillies collapsed in September. Their beloved manager Fred Hutchinson died of cancer just weeks after the end of the 1964 season. The failure of the Reds to win the 1964 pennant led to owner Bill DeWitt's selling off key components of the team, in anticipation of relocating the franchise. In response to DeWitt's threatened move, the women of Cincinnati banded together to form the Rosie Reds to urge DeWitt to keep the franchise in Cincinnati. The Rosie Reds are still in existence, and are currently the oldest fan club in Major League Baseball. After the 1965 season he executed what may be the most lopsided trade in baseball history, sending former Most Valuable Player Frank Robinson to the Baltimore Orioles for pitchers Milt Pappas and Jack Baldschun, and outfielder Dick Simpson. Robinson went on to win the MVP and triple crown in the American league for 1966, and lead Baltimore to its first ever World Series title in a sweep of the Los Angeles Dodgers. The Reds did not recover from this trade until the rise of the "Big Red Machine" of the 1970s.

Starting in the early 1960s, the Reds' farm system began producing a series of stars, including Jim Maloney (the Reds' pitching ace of the 1960s), Pete Rose, Tony PÃ©rez, Johnny Bench, Lee May, Tommy Helms, Bernie Carbo, Hal McRae, Dave ConcepciÃ³n, and Gary Nolan. The tipping point came in 1967 with the appointment of Bob Howsam as general manager. That same year the Reds avoided a move to San Diego when the city of Cincinnati and Hamilton County agreed to build a state of the art, downtown stadium on the edge of the Ohio River. The Reds entered into a 30-year lease in exchange for the stadium commitment keeping the franchise in its original home city. In a series of strategic moves, Howsam brought in key personnel to complement the homegrown talent. The Reds' final game at Crosley Field, home to more than 4,500 baseball games, was played on June 24, 1970, a 5â4 victory over the San Francisco Giants.

Under Howsam's administration starting in the late 1960s, the Reds instituted a strict rule barring the team's players from wearing facial hair and long hair. The clean cut look was meant to present the team as wholesome in an era of turmoil. All players coming to the Reds were required to shave and cut their hair for the next three decades. Over the years, the rule was controversial, but persisted well into the ownership of Marge Schott. On at least one occasion, in the early 1980s, enforcement of this rule lost them the services of star reliever and Ohio native Rollie Fingers, who would not shave his trademark handlebar mustache in order to join the team. The rule was not officially rescinded until 1999 when the Reds traded for slugger Greg Vaughn, who had a goatee. The New York Yankees continue to have a similar rule today, though unlike the Reds during this period, Yankees players are permitted to have mustaches. Much like when players leave the Yankees today, players who left the Reds took advantage with their new teams; Pete Rose, for instance, grew his hair out much longer than would be allowed by the Reds once he signed with the Philadelphia Phillies in 1979.

The Reds' rules also included conservative uniforms. In Major League Baseball, a club generally provides most of the equipment and clothing needed for play. However, players are required to supply their gloves and shoes themselves. Many players enter into sponsorship arrangements with shoe manufacturers, but through the mid-1980s, the Reds had a strict rule that players were to wear only plain black shoes with no prominent logo. Reds players decried what they considered to be the boring color choice as well as the denial of the opportunity to earn more money through shoe contracts. A compromise was struck in 1985 in which players could paint red marks on their black shoes, then the following year, they were allowed to wear all-red shoes.

In , little known George "Sparky" Anderson was hired as manager, and the Reds embarked upon a decade of excellence, with a team that came to be known as "The Big Red Machine". Playing at Crosley Field until June 30, 1970, when the Reds moved into brand-new Riverfront Stadium, a 52,000 seat multi-purpose venue on the shores of the Ohio River, the Reds began the 1970s with a bang by winning 70 of their first 100 games. Johnny Bench, Tony PÃ©rez, Pete Rose, Lee May and Bobby Tolan were the early Red Machine offensive leaders; Gary Nolan, Jim Merritt, Wayne Simpson and Jim McGlothlin led a pitching staff which also contained veterans Tony Cloninger and Clay Carroll and youngsters Pedro BorbÃ³n and Don Gullett. The Reds breezed through the 1970 season, winning the NL West and captured the NL pennant by sweeping the Pittsburgh Pirates in three games. By the time the club got to the World Series, however, the Reds pitching staff had run out of gas and the veteran Baltimore Orioles, led by Hall of Fame third baseman and World Series MVP Brooks Robinson, beat the Reds in five games.

After the disastrous season (the only season of the 1970s during which the Reds finished with a losing record) the Reds reloaded by trading veterans Jimmy Stewart, May, and Tommy Helms for Joe Morgan, CÃ©sar GerÃ³nimo, Jack Billingham, Ed Armbrister, and Denis Menke. Meanwhile, Dave ConcepciÃ³n blossomed at shortstop. 1971 was also the year a key component of the future world championships was acquired in George Foster from the San Francisco Giants in a trade for shortstop Frank Duffy.

The Reds won the NL West in baseball's first ever strike-shortened season and defeated the Pittsburgh Pirates in an exciting five-game playoff series. They then faced the Oakland Athletics in the World Series. Six of the seven games were won by one run. With powerful slugger Reggie Jackson sidelined by an injury incurred during Oakland's playoff series, Ohio native Gene Tenace got a chance to play in the series, delivering four home runs that tied the World Series record for homers, propelling Oakland to a dramatic seven-game series win. This was one of the few World Series in which no starting pitcher for either side pitched a complete game.

The Reds won a third NL West crown in after a dramatic second half comeback, that saw them make up games on the Los Angeles Dodgers after the All-Star break. However they lost the NL pennant to the New York Mets in five games in the NLCS. In game one, Tom Seaver faced Jack Billingham in a classic pitching duel, with all three runs of the 2â1 margin being scored on home runs. John Milner provided New York's run off Billingham, while Pete Rose tied the game in the seventh inning off Seaver, setting the stage for a dramatic game ending home run by Johnny Bench in the bottom of the ninth. The New York series provided plenty of controversy with the riotous behavior of Shea Stadium fans towards Pete Rose when he and Bud Harrelson scuffled after a hard slide by Rose into Harrelson at second base during the fifth inning of Game 3. A full bench-clearing fight resulted after Harrelson responded to Rose's aggressive move to prevent him from completing a double play by calling him a name. This also led to two more incidents in which play was stopped. The Reds trailed 9â3 and New York's manager, Yogi Berra, and legendary outfielder Willie Mays, at the request of National League president Warren Giles, appealed to fans in left field to restrain themselves. The next day the series was extended to a fifth game when Rose homered in the 12th inning to tie the series at two games each.

The Reds won 98 games in but they finished second to the 102-win Los Angeles Dodgers. The 1974 season started off with much excitement, as the Atlanta Braves were in town to open the season with the Reds. Hank Aaron entered opening day with 713 home runs, one shy of tying Babe Ruth's record of 714. The first pitch Aaron swung at in the 1974 season was the record tying home run off Jack Billingham. The next day the Braves benched Aaron, hoping to save him for his record-breaking home run on their season-opening homestand. The commissioner of baseball, Bowie Kuhn, ordered Braves management to play Aaron the next day, where he narrowly missed the historic home run in the fifth inning. Aaron went on to set the record in Atlanta two nights later. The 1974 season was also the debut of Hall of Fame radio announcer Marty Brennaman, who replaced Al Michaels, after Michaels left the Reds to broadcast for the San Francisco Giants.

With 1975, the Big Red Machine lineup solidified with the "Great Eight" starting team of Johnny Bench (catcher), Tony PÃ©rez (first base), Joe Morgan (second base), Dave ConcepciÃ³n (shortstop), Pete Rose (third base), Ken Griffey (right field), CÃ©sar GerÃ³nimo (center field), and George Foster (left field). The starting pitchers included Don Gullett, Fred Norman, Gary Nolan, Jack Billingham, Pat Darcy, and Clay Kirby. The bullpen featured Rawly Eastwick and Will McEnaney combining for 37 saves, and veterans Pedro BorbÃ³n and Clay Carroll. On Opening Day, Rose still played in left field, Foster was not a starter, while John Vukovich, an off-season acquisition, was the starting third baseman. While Vuckovich was a superb fielder, he was a weak hitter. In May, with the team off to a slow start and trailing the Dodgers, Sparky Anderson made a bold move by moving Rose to third base, a position where he had very little experience, and inserting Foster in left field. This was the jolt that the Reds needed to propel them into first place, with Rose proving to be reliable on defense, while adding Foster to the outfield gave the offense some added punch. During the season, the Reds compiled two notable streaks: (1) by winning 41 out of 50 games in one stretch, and (2) by going a month without committing any errors on defense.
In the 1975 season, Cincinnati clinched the NL West with 108 victories, then swept the Pittsburgh Pirates in three games to win the NL pennant. In the World Series, the Boston Red Sox were the opponents. After splitting the first four games, the Reds took Game 5. After a three-day rain delay, the two teams met in Game 6, one of the most memorable baseball games ever played and considered by many to be the best World Series game ever. The Reds were ahead 6â3 with 5 outs left, when the Red Sox tied the game on former Red Bernie Carbo's three-run home run. It was Carbo's second pinch-hit three-run homer in the series. After a few close-calls either way, Carlton Fisk hit a dramatic 12th inning home run off the foul pole in left field to give the Red Sox a 7â6 win and force a deciding Game 7. Cincinnati prevailed the next day when Morgan's RBI single won Game 7 and gave the Reds their first championship in 35 years. The Reds have not lost a World Series game since Carlton Fisk's home run, a span of 9 straight wins.

1976 saw a return of the same starting eight in the field. The starting rotation was again led by Nolan, Gullett, Billingham, and Norman, while the addition of rookies Pat Zachry and Santo AlcalÃ¡ comprised an underrated staff in which four of the six had ERAs below 3.10. Eastwick, Borbon, and McEnaney shared closer duties, recording 26, 8, and 7 saves respectively. The Reds won the NL West by ten games. They went undefeated in the postseason, sweeping the Philadelphia Phillies (winning Game 3 in their final at-bat) to return to the World Series. They continued to dominate by sweeping the Yankees in the newly renovated Yankee Stadium, the first World Series games played in Yankee Stadium since 1964. This was only the second ever sweep of the Yankees in the World Series. In winning the Series, the Reds became the first NL team since the 1921â22 New York Giants to win consecutive World Series championships, and the Big Red Machine of 1975â76 is considered one of the best teams ever. So far in MLB history, the 1975 and '76 Reds were the last NL team to repeat as champions.

Beginning with the 1970 National League pennant, the Reds beat either of the two Pennsylvania-based clubs, the Philadelphia Phillies or the Pittsburgh Pirates to win their pennants (Pirates in 1970, 1972, 1975, and 1990, Phillies in 1976), making The Big Red Machine part of the rivalry between the two Pennsylvania teams. In 1979, Pete Rose added further fuel in The Big Red Machine being part of the rivalry when he signed with the Phillies and helped them win their first World Series championship in .

The later years of the 1970s brought turmoil and change. Popular Tony PÃ©rez was sent to Montreal after the 1976 season, breaking up the Big Red Machine's starting lineup. Manager Sparky Anderson and General Manager Bob Howsam later considered this trade the biggest mistake of their careers. Starting pitcher Don Gullett left via free agency and signed with the New York Yankees. In an effort to fill that gap, a trade with the Oakland A's for starting ace Vida Blue was arranged during the 1976â77 off-season. However, Bowie Kuhn, the Commissioner of Baseball, vetoed the trade for the stated reason of maintaining competitive balance in baseball. Some have suggested that the actual reason had more to due with Kuhn's continued feud with Oakland A's owner Charlie Finley. On June 15, 1977, the Reds acquired Mets' franchise pitcher Tom Seaver for Pat Zachry, Doug Flynn, Steve Henderson, and Dan Norman. In other deals that proved to be less successful, the Reds traded Gary Nolan to the Angels for Craig Hendrickson, Rawly Eastwick to St. Louis for Doug Capilla and Mike Caldwell to Milwaukee for Rick O'Keeffe and Garry Pyka, and got Rick Auerbach from Texas. The end of the Big Red Machine era was heralded by the replacement of General Manager Bob Howsam with Dick Wagner.

In Rose's last season as a Red, he gave baseball a thrill as he challenged Joe DiMaggio's 56-game hitting streak, tying for the second-longest streak ever at 44 games. The streak came to an end in Atlanta after striking out in his fifth at bat in the game against Gene Garber. Rose also earned his 3,000th hit that season, on his way to becoming baseball's all-time hits leader when he rejoined the Reds in the mid-1980s. The year also witnessed the only no-hitter of Hall of Fame pitcher Tom Seaver's career, coming against the St. Louis Cardinals on June 16, 1978.

After the 1978 season and two straight second-place finishes, Wagner fired manager Andersonâan unpopular move. Pete Rose, who since 1963 had played almost every position for the team except pitcher, shortstop, and catcher, signed with Philadelphia as a free agent. By 1979, the starters were Bench (c), Dan Driessen (1b), Morgan (2b), ConcepciÃ³n (ss), Ray Knight (3b), with Griffey, Foster, and Geronimo again in the outfield. The pitching staff had experienced a complete turnover since 1976 except for Fred Norman. In addition to ace starter Tom Seaver; the remaining starters were Mike LaCoss, Bill Bonham, and Paul Moskau. In the bullpen, only Borbon had remained. Dave Tomlin and Mario Soto worked middle relief with Tom Hume and Doug Bair closing. The Reds won the 1979 NL West behind the pitching of Tom Seaver but were dispatched in the NL playoffs by Pittsburgh. Game 2 featured a controversial play in which a ball hit by Pittsburgh's Phil Garner was caught by Cincinnati outfielder Dave Collins but was ruled a trap, setting the Pirates up to take a 2â1 lead. The Pirates swept the series 3 games to 0 and went on to win the World Series against the Baltimore Orioles.

The 1981 team fielded a strong lineup, but with only ConcepciÃ³n, Foster, and Griffey retaining their spots from the 1975â76 heyday. After Johnny Bench was able to play only a few games at catcher each year after 1980 due to ongoing injuries, Joe Nolan took over as starting catcher. Driessen and Bench shared 1st base, and Knight starred at third. Morgan and Geronimo had been replaced at second base and center field by Ron Oester and Dave Collins. Mario Soto posted a banner year starting on the mound, only surpassed by the outstanding performance of Seaver's Cy Young runner-up season. La Coss, Bruce Berenyi, and Frank Pastore rounded out the starting rotation. Hume again led the bullpen as closer, joined by Bair and Joe Price. In , Cincinnati had the best overall record in baseball, but they finished second in the division in both of the half-seasons that were created after a mid-season players' strike, and missed the playoffs. To commemorate this, a team photo was taken, accompanied by a banner that read "Baseball's Best Record 1981".

By , the Reds were a shell of the original Red Machine; they lost 101 games that year. Johnny Bench, after an unsuccessful transition to 3rd base, retired a year later.

After the heartbreak of 1981, General Manager Dick Wagner pursued the strategy of ridding the team of veterans including third-baseman Knight and the entire starting outfield of Griffey, Foster, and Collins. Bench, after being able to catch only seven games in 1981, was moved from platooning at first base to be the starting third baseman; Alex TreviÃ±o became the regular starting catcher. The outfield was staffed with Paul Householder, CÃ©sar CedeÃ±o, and future Colorado Rockies & Pittsburgh Pirates manager Clint Hurdle on opening day. Hurdle was an immediate bust, and rookie Eddie Milner took his place in the starting outfield early in the year. The highly touted Householder struggled throughout the year despite extensive playing time. Cedeno, while providing steady veteran play, was a disappointment, and was unable to recapture his glory days with the Houston Astros. The starting rotation featured the emergence of a dominant Mario Soto, and featured strong years by Pastore and Bruce Berenyi, but Seaver was injured all year, and their efforts were wasted without a strong offensive lineup. Tom Hume still led the bullpen, along with Joe Price. But the colorful Brad "The Animal" Lesley was unable to consistently excel, and former all-star Jim Kern was a big disappointment. Kern was also publicly upset over having to shave off his prominent beard to join the Reds, and helped force the issue of getting traded during mid-season by growing it back. The season also saw the midseason firing of Manager John McNamara, who was replaced as skipper by Russ Nixon.

The Reds fell to the bottom of the Western Division for the next few years. After the 1982 season, Seaver was traded back to the Mets. The year 1983 found Dann Bilardello behind the plate, Bench returning to part-time duty at first base, rookies Nick Esasky taking over at third base and Gary Redus taking over from Cedeno. Tom Hume's effectiveness as a closer had diminished, and no other consistent relievers emerged. Dave ConcepciÃ³n was the sole remaining starter from the Big Red Machine era.

Wagner's tenure ended in 1983, when Howsam, the architect of the Big Red Machine, was brought back. The popular Howsam began his second term as Reds' General Manager by signing Cincinnati native Dave Parker as a free agent from Pittsburgh. In the Reds began to move up, depending on trades and some minor leaguers. In that season Dave Parker, Dave ConcepciÃ³n and Tony PÃ©rez were in Cincinnati uniforms. In August 1984, Pete Rose was reacquired and hired to be the Reds player-manager. After raising the franchise from the grave, Howsam gave way to the administration of Bill Bergesch, who attempted to build the team around a core of highly regarded young players in addition to veterans like Parker. However, he was unable to capitalize on an excess of young and highly touted position players including Kurt Stillwell, Tracy Jones, and Kal Daniels by trading them for pitching. Despite the emergence of Tom Browning as rookie of the year in 1985 when he won 20 games, the rotation was devastated by the early demise of Mario Soto's career to arm injury.

Under Bergesch, from â89 the Reds finished second four times. Among the highlights, Rose became the all-time hits leader, Tom Browning threw a perfect game, Eric Davis became the first player in baseball history to hit at least 35 home runs and steal 50 bases, and Chris Sabo was the 1988 National League Rookie of the Year. The Reds also had a bullpen star in John Franco, who was with the team from 1984 to 1989. Rose once had ConcepciÃ³n pitch late in a game at Dodger Stadium. Following the release of the Dowd Report which accused Rose for betting on baseball games, in Rose was banned from baseball by Commissioner Bart Giamatti, who declared Rose guilty of "conduct detrimental to baseball". Controversy also swirled around Reds owner Marge Schott, who was accused several times of ethnic and racial slurs.

In , General Manager Bergesch was replaced by Murray Cook, who initiated a series of deals that would finally bring the Reds back to the championship, starting with acquisitions of Danny Jackson and JosÃ© Rijo. An aging Dave Parker was let go after a revival of his career in Cincinnati following the Pittsburgh drug trials. Barry Larkin emerged as the starting shortstop over Kurt Stillwell, who along with reliever Power, was traded for Jackson. In , Cook was succeeded by Bob Quinn, who put the final pieces of the championship puzzle together, with the acquisitions of Hal Morris, Billy Hatcher and Randy Myers.
In , the Reds under new manager Lou Piniella shocked baseball by leading the NL West from wire-to-wire. Winning their first nine games, they started off 33â12 and maintained their lead throughout the year. Led by Chris Sabo, Barry Larkin, Eric Davis, Paul O'Neill and Billy Hatcher in the field, and by JosÃ© Rijo, Tom Browning and the "Nasty Boys" of Rob Dibble, Norm Charlton and Randy Myers on the mound, the Reds took out the Pirates in the NLCS. The Reds swept the heavily favored Oakland Athletics in four straight, and extended a Reds winning streak in the World Series to nine consecutive games. The World Series, however, saw Eric Davis severely bruise a kidney diving for a fly ball in Game 4, and his play was greatly limited the next year. In winning the World Series the Reds became the only National League team to go wire to wire.

In , Quinn was replaced in the front office by Jim Bowden. On the field, manager Lou Piniella wanted outfielder Paul O'Neill to be a power-hitter to fill the void Eric Davis left when he was traded to the Los Angeles Dodgers in exchange for Tim Belcher. However, O'Neill only hit .246 and 14 homers. The Reds returned to winning after a losing season in 1991, but 90 wins was only enough for second place behind the division-winning Atlanta Braves. Before the season ended, Piniella got into an altercation with reliever Rob Dibble. In the off season, Paul O'Neill was traded to the New York Yankees for outfielder Roberto Kelly. Kelly was a disappointment for the Reds over the next couple of years, while O'Neill blossomed, leading a down-trodden Yankees franchise to a return to glory. Also, the Reds would replace their "Big Red Machine" era uniforms in favor of a pinstriped uniform with no sleeves.

For the 1993 season Piniella was replaced by fan favorite Tony PÃ©rez, but he lasted only 44 games at the helm, replaced by Davey Johnson. With Johnson steering the team, the Reds made steady progress. In 1994, the Reds were in the newly created National League Central Division with the Chicago Cubs, St. Louis Cardinals, as well as fellow rivals Pittsburgh Pirates and Houston Astros. By the time the strike hit, the Reds finished a half-game ahead of the Astros for first-place in the NL Central. By , the Reds won the division thanks to Most Valuable Player Barry Larkin. After defeating the NL West champion Dodgers in the first NLDS since 1981, they lost to the Atlanta Braves.

Team owner Marge Schott announced mid-season that Johnson would be gone by the end of the year, regardless of outcome, to be replaced by former Reds third baseman Ray Knight. Johnson and Schott had never gotten along and she did not approve of Johnson living with his fiancÃ©e before they were married, In contrast, Knight, along with his wife, professional golfer Nancy Lopez, were friends of Schott. The team took a dive under Knight and he was unable to complete two full seasons as manager, subject to complaints in the press about his strict managerial style.

In the Reds won 96 games, led by manager Jack McKeon, but lost to the New York Mets in a one game playoff. Earlier that year, Schott sold controlling interest in the Reds to Cincinnati businessman Carl Lindner. Despite an 85â77 finish in 2000, and being named 1999 NL manager of the year, McKeon was fired after the 2000 season. The Reds did not have another winning season until 2010.

Riverfront Stadium, by then known as Cinergy Field, was demolished in . Great American Ball Park opened in with high expectations for a team led by local favorites, including outfielder Ken Griffey, Jr., shortstop Barry Larkin, and first baseman Sean Casey. Although attendance improved considerably with the new ballpark, the team continued to lose. Schott had not invested much in the farm system since the early 1990s, leaving the team relatively thin on talent. After years of promises that the club was rebuilding toward the opening of the new ballpark, General Manager Jim Bowden and manager Bob Boone were fired on July 28. This broke up the father-son combo of manager Bob Boone and third baseman Aaron Boone, and Aaron was soon traded to the New York Yankees. Tragedy struck in November when Dernell Stenson, a promising young outfielder for the Reds, was shot and killed during a carjack. Following the season Dan O'Brien was hired as the Reds' 16th General Manager.

The and seasons continued the trend of big hitting, poor pitching, and poor records. Griffey, Jr. joined the 500 home run club in 2004, but was again hampered by injuries. Adam Dunn emerged as consistent home run hitter, including a home run against JosÃ© Lima. He also broke the major league record for strikeouts in 2004. Although a number of free agents were signed before 2005, the Reds were quickly in last place and manager Dave Miley was forced out in the 2005 mid season and replaced by Jerry Narron. Like many other small market clubs, the Reds dispatched some of their veteran players and began entrusting their future to a young nucleus that included Adam Dunn and Austin Kearns.

Late summer 2004 saw the opening of the Cincinnati Reds Hall of Fame (HOF). The Reds HOF had been in existence in name only since the 1950s, with player plaques, photos and other memorabilia scattered throughout their front offices. Ownership and management desired a stand-alone facility, where the public could walk through inter-active displays, see locker room recreations, watch videos of classic Reds moments and peruse historical items. The first floor houses a movie theater which resembles an older, ivy-covered brick wall ball yard. The hallways contain many vintage photographs. The rear of the building features a three-story wall containing a baseball for every hit Pete Rose had during his career. The third floor contains interactive exhibits including a pitcher's mound, radio booth, and children's area where the fundamentals of baseball are taught through videos featuring former Reds players.
Robert Castellini took over as controlling owner from Lindner in 2006. Castellini promptly fired general manager Dan O'Brien and hired Wayne Krivsky. The Reds made a run at the playoffs but ultimately fell short. The 2007 season was again mired in mediocrity. Midway through the season Jerry Narron was fired as manager and replaced by Pete Mackanin. The Reds ended up posting a winning record under Mackanin, but finished the season in 5th place in the Central Division. Mackanin was manager in an interim capacity only, and the Reds, seeking a big name to fill the spot, ultimately brought in Dusty Baker. Early in the 2008 season, Krivsky was fired and replaced by Walt Jocketty. Though the Reds did not win under Krivsky, he is credited with revamping the farm system and signing young talent that could potentially lead the Reds to success in the future.

The Reds failed to post winning records in both 2008 and 2009. In 2010, with NL MVP Joey Votto and Gold Glovers Brandon Phillips and Scott Rolen the Reds posted a 91-71 record and were NL Central champions. The following week, the Reds became only the second team in MLB history to be no-hit in a postseason game when Philadelphia's Roy Halladay shut down the National League's number one offense in game one of the NLDS. The Reds lost in a 3-game sweep of the NLDS to Philadelphia.

After coming off their surprising 2010 NL Central Division Title, the Reds fell short of many expectations for the 2011 season. Multiple injuries and inconsistent starting pitching played a big role in their mid-season collapse, along with a less productive offense as compared to the previous year. The Reds ended the season at 79-83. The Reds won the 2012 NL Central Division Title. On September 28, Homer Bailey threw a 1-0 no-hitter against the Pittsburgh Pirates at PNC Park, this was the first Reds no-hitter since Tom Browning's perfect game in September of the 1988 season. Finishing with a 97â65 record, they earned the second seed in the Division Series and a match-up with the eventual World Series champion San Francisco Giants. After taking a 2â0 lead with road victories at AT&T Park, they headed home looking to win the series. However, they lost three straight at their home ballpark to become the first National League team since the Cubs in 1984 to lose a division series after leading 2â0.

In the off-season, the team traded outfielder Drew Stubbs, as part of a three team deal with the Arizona Diamondbacks and Cleveland Indians, to the Indians, and in turn received right fielder Shin-Soo Choo. On July 2, 2013, Homer Bailey pitched a no-hitter against the San Francisco Giants for a 4-0 Reds victory, making Bailey the third pitcher in Reds history with two complete game no-hitters in their career.

Following six consecutive losses to close out the 2013 season, including a loss to the Pittsburgh Pirates, at PNC Park, in the National League wild-card playoff game, the Reds decided to fire Dusty Baker. During his six years as manager, Baker led the Reds to the playoff three times; however, they never advanced beyond the first round.

On October 22, 2013, the Reds hired pitching coach Bryan Price to replace Baker as manager.

Under Bryan Price, the Reds were led by pitchers Johnny Cueto and the hard-throwing Cuban Aroldis Chapman. While the offense was led by all-star third baseman Todd Frazier, Joey Votto, and Brandon Phillips. Although with plenty of star power, the Reds never got off to a good start and ending the season in lowly fourth place in the division to go along with a 76-86 record. During the offseason, the Reds traded pitchers Alfredo SimÃ³n to the Tigers and Mat Latos to the Marlins. In return, they acquired young talents such as Eugenio SuÃ¡rez and Anthony DeSclafani. They also acquired veteran slugger Marlon Byrd from the Phillies to play left field.

The Reds' 2015 season wasn't much better, as they finished with the second worst record in the league with a record of 64-98, their worst finish since 1982. The Reds were forced to trade star pitchers Johnny Cueto (to the Kansas City Royals) and Mike Leake (to the San Francisco Giants), receiving minor league pitching prospects for both. Shortly after the season's end, the Reds traded home run derby champion Todd Frazier to the Chicago White Sox, and closing pitcher Aroldis Chapman to the New York Yankees.

In 2016, the Reds broke the then record for home runs allowed during a single season, The Reds held this record until the 2019 season when it was broken by the Baltimore Orioles .The previous record holder was the 1996 Detroit Tigers with 241 longballs yielded to opposing teams. The Reds went 68-94, and again were one of the worst teams in the MLB. The Reds traded outfielder Jay Bruce to the Mets just before the July 31st non-waiver trade deadline in exchange for two prospects, infielder Dilson Herrera and pitcher Max Wotell. During the offseason, the Reds traded Brandon Phillips to the Atlanta Braves in exchange for two minor league pitchers.

The Cincinnati Reds play their home games at Great American Ball Park, located at 100 Joe Nuxhall Way, in downtown Cincinnati. Great American Ball Park opened in 2003 at the cost of $290 million and has a capacity of 42,271. Along with serving as the home field for the Reds, the stadium also holds the Cincinnati Reds Hall of Fame. The Hall of Fame was added as a part of Reds tradition allowing fans to walk through the history of the franchise as well as participating in many interactive baseball features.

Great American Ball Park is the seventh home of the Cincinnati Reds, built immediately to the north of the site on which Riverfront Stadium, later named Cinergy Field, once stood. The first ballpark the Reds occupied was Bank Street Grounds from 1882 to 1883 until they moved to League Park I in 1884, where they would remain until 1893. Through the late 1890s and early 1900s (decade), the Reds moved to two different parks where they stayed for less than ten years. League Park II was the third home field for the Reds from 1894 to 1901, and then moved to the Palace of the Fans which served as the home of the Reds in the 1910s. It was in 1912 that the Reds moved to Crosley Field which they called home for 58 years. Crosley served as the home field for the Reds for two World Series titles and five National League pennants. Beginning June 30, 1970, and during the dynasty of the Big Red Machine, the Reds played in Riverfront Stadium, appropriately named due to its location right by the Ohio River. Riverfront saw three World Series titles and five National League pennants. It was in the late 1990s that the city agreed to build two separate stadiums on the riverfront for the Reds and the Cincinnati Bengals. Thus, in 2003, the Reds began a new era with the opening of the current stadium.

The Reds hold their spring training in Goodyear, Arizona at Goodyear Ballpark. The Reds moved into this stadium and the Cactus League in 2010 after staying in the Grapefruit League for most of their history. The Reds share Goodyear Park with their rivals in Ohio, the Cleveland Indians.

Throughout the history of the Cincinnati Reds, many different variations of the classic wishbone "C" logo have been introduced. For most of the history of the Reds, especially during the early history, the Reds logo has been simply the wishbone "C" with the word "REDS" inside, the only colors used being red and white. However, during the 1950s, during the renaming and re-branding of the team as the Cincinnati Redlegs because of the connections to communism of the word 'Reds', the color blue was introduced as part of the Reds color combination. During the 1960s and 1970s the Reds saw a move towards the more traditional colors, abandoning the navy blue. A new logo also appeared with the new era of baseball in 1972, when the team went away from the script "REDS" inside of the "C", instead, putting their mascot Mr. Redlegs in its place as well as putting the name of the team inside of the wishbone "C". In the 1990s the more traditional, early logos of Reds came back with the current logo reflecting more of what the team's logo was when they were first founded.

Along with the logo, the Reds' uniforms have been changed many different times throughout their history. Following their departure from being called the "Redlegs" in 1956 the Reds made a groundbreaking change to their uniforms with the use of sleeveless jerseys, seen only once before in the Major Leagues by the Chicago Cubs. At home and away, the cap was all-red with a white wishbone C insignia. The long-sleeved undershirts were red. The uniform was plain white with a red wishbone C logo on the left and the uniform number on the right. On the road the wishbone C was replaced by the mustachioed "Mr. Red" logo, the pillbox-hat-wearing man with a baseball for a head. The home stockings were red with six white stripes. The away stockings had only three white stripes.

The Reds changed uniforms again in 1961, when they replaced the traditional wishbone C insignia with an oval C logo, but continued to use the sleeveless jerseys. At home, the Reds wore white caps with the red bill with the oval C in red, white sleeveless jerseys with red pinstripes, with the oval C-REDS logo in black with red lettering on the left breast and the number in red on the right. The gray away uniform included a gray cap with the red oval C and a red bill. Their gray away uniforms, which also included a sleeveless jersey, bore CINCINNATI in an arched block style across with the number below on the left. In 1964, players' last names were placed on the back of each set of uniforms, below the numbers. Those uniforms were scrapped after the 1966 season.

However, the Cincinnati uniform design most familiar to baseball enthusiasts is the one whose basic form, with minor variations, held sway for the 26 seasons from 1967 to 1992. Most significantly, the point was restored to the C insignia, making it a wishbone again. During this era, the Reds wore all-red caps both at home and on the road. The caps bore the simple wishbone C insignia in white. The uniforms were standard short-sleeved jerseys and standard trousersâwhite at home and grey on the road. The home uniform featured the Wishbone C-REDS logo in red with white type on the left breast and the uniform number in red on the right. The away uniform bore CINCINNATI in an arched block style across the front with the uniform number below on the left. Red, long-sleeved undershirts and plain red stirrups over white sanitary stockings completed the basic design. The Reds wore pinstriped home uniforms in 1967 only, and the uniforms were flannel through 1971, changing to double-knits with pullover jerseys and beltless pants in 1972. Those uniforms lasted 21 seasons, and the 1992 Reds were the last MLB team to date whose primary uniforms featured pullover jerseys and beltless pants. 

The 1993 uniforms (which did away with the pullovers and brought back button-down jerseys) kept white and gray as the base colors for the home and away uniforms, but added red pinstripes. The home jerseys were sleeveless, showing more of the red undershirts. The color scheme of the C-REDS logo on the home uniform was reversed, now red lettering on a white background. A new home cap was created that had a red bill and a white crown with red pinstripes and a red wishbone C insignia. The away uniform kept the all-red cap, but moved the uniform number to the left, to more closely match the home uniform. The only additional change to these uniforms was the introduction of black as a primary color of the Reds in 1999, especially on their road uniforms.

The Reds latest uniform change came in December 2006 which differed significantly from the uniforms worn during the previous eight seasons. The home caps returned to an all-red design with a white wishbone C, lightly outlined in black. Caps with red crowns and a black bill became the new road caps. Additionally, the sleeveless jersey was abandoned for a more traditional design. The numbers and lettering for the names on the backs of the jerseys were changed to an early-1900s style typeface, and a handlebar mustached "Mr. Redlegs" â reminiscent of the logo used by the Reds in the 1950s and 1960s â was placed on the left sleeve.


The Cincinnati Reds have retired ten numbers in franchise history, as well as honoring Jackie Robinson, whose number is retired league-wide around Major League Baseball.

All of the retired numbers are located at Great American Ball Park behind home-plate on the outside of the press box. Along with the retired player and manager number, the following broadcasters are honored with microphones by the broadcast booth: Marty Brennaman, Waite Hoyt, and Joe Nuxhall.

On April 15, 1997, #42 was retired throughout Major League Baseball in honor of Jackie Robinson.

The Reds have hosted the Major League Baseball All-Star Game five times: twice at Crosley Field (1938, 1953), twice at Riverfront Stadium (1970, 1988), and once at Great American Ball Park (2015). (Until 2019, the Reds shared this record with the Cleveland Indians and the Pittsburgh Pirates; Cleveland broke this tie in 2019, hosting the All-Star Game for the sixth time.)

The Ohio Cup was an annual pre-season baseball game, which pitted the Ohio rivals Cleveland Indians and Cincinnati Reds. In its first series it was a single-game cup, played each year at minor-league Cooper Stadium in Columbus, was staged just days before the start of each new Major League Baseball season.

A total of eight Ohio Cup games were played, in 1989 to 1996, with the Indians winning six of them. The winner of the game each year was awarded the Ohio Cup in postgame ceremonies. The Ohio Cup was a favorite among baseball fans in Columbus, with attendances regularly topping 15,000.

The Ohio Cup games ended with the introduction of regular-season interleague play in 1997. Thereafter, the two teams competed annually in the regular-season Battle of Ohio or Buckeye Series. The Ohio Cup was revived in 2008 as a reward for the team with the better overall record in the Reds-Indians series each year.

The Reds' flagship radio station has been WLW, 700AM since 1969. Prior to that, the Reds were heard over: WKRC, WCPO, WSAI and WCKY. WLW, a 50,000-watt station, is "clear channel" in more than one way, as iHeartMedia owns the "blowtorch" outlet which is also known as "The Nation's Station".

Marty Brennaman has been the Reds' play-by-play voice since 1974 and has won the Ford C. Frick Award for his work, which includes his famous call of "... and this one belongs to the Reds!" after a win. Joining him for years on color was former Reds pitcher Joe Nuxhall, who worked in the radio booth from 1967 (the year after his retirement as an active player) until 2004, plus three more seasons doing select home games until his death, in 2007.

In 2007, Thom Brennaman, a veteran announcer seen nationwide on Fox Sports, joined his father Marty in the radio booth. Retired relief pitcher Jeff Brantley, formerly of ESPN, also joined the network in 2007. , Brantley and Thom Brennaman's increased TV schedule (see below) has led to more appearances for Jim Kelch, who had filled in on the network since 2008. In 2019, the Reds hired then-Pensacola Blue Wahoos radio play-by-play announcer Tommy Thrall to provide in-game and post-game coverage as well as a fill in play-by-play announcer.

Televised games are seen exclusively on Fox Sports Ohio and Fox Sports Indiana. In addition, Fox Sports South televises Fox Sports Ohio broadcasts of Reds games to Tennessee and western North Carolina. George Grande, who hosted the first "SportsCenter" on ESPN in 1979, was the play-by-play announcer, usually alongside Chris Welsh, from 1993 until his retirement during the final game of the 2009 season. Since 2009, Grande has worked part-time for the Reds as play-by-play announcer in September when Thom Brennaman is covering the NFL for Fox Sports. He has also made guest appearances throughout the season. Brennaman has been the head play-by-play commentator since 2010, with Welsh and Brantley sharing time as the color commentators. Paul Keels, who left in 2011 to become the play-by-play announcer for the Ohio State Buckeyes Radio Network, was the Reds' backup play-by-play television announcer during the 2010 season. Jim Kelch served as Keels' replacement. The Reds also added former Cincinnati First Baseman Sean Casey â known as "The Mayor" by Reds fans â to do color commentary for approximately 15 games in 2011.

NBC affiliate WLWT carried Reds games from 1948 to 1995. Among those that have called games for WLWT include Waite Hoyt, Ray Lane, Steve Physioc, Johnny Bench, Joe Morgan, and Ken Wilson. Al Michaels, who established a long career with ABC and NBC, spent three years in Cincinnati early in his career. The last regularly-scheduled, over-the-air broadcasts of Reds games were on WSTR-TV from 1996 to 1998. Since 2010, WKRC-TV has simulcast Opening Day games with Fox Sports Ohio.

The Cincinnati Reds farm system consists of eight minor league affiliates.

 

 


</doc>
<doc id="6672" url="https://en.wikipedia.org/wiki?curid=6672" title="Caribbean cuisine">
Caribbean cuisine

Caribbean cuisine is a fusion of African, Creole, Cajun, Amerindian, European, Latin American, Indian/South Asian, Middle Eastern, and Chinese. These traditions were brought from many different countries when they came to the Caribbean. In addition, the population has created styles that are unique to the region.

Ingredients that are common in most islands' dishes are rice, plantains, beans, cassava, culantro, bell peppers, chickpeas, tomatoes, sweet potatoes, coconut, and any of various meats that are locally available like beef, poultry, pork or fish. A characteristic seasoning for the region is a green herb-and-oil-based marinade which imparts a flavor profile which is quintessentially Caribbean in character. Ingredients may include garlic, onions, scotch bonnet peppers, celery, green onions, and herbs like culantro, Mexican mint, chives, marjoram, rosemary, tarragon and thyme. This green seasoning is used for a variety of dishes like curries, stews and roasted meats.

Traditional dishes are so important to regional culture that, for example, the local version of Caribbean goat stew has been chosen as the official national dish of Montserrat and is also one of the signature dishes of St. Kitts and Nevis. Another popular dish in the Anglophone Caribbean is called "cook-up", or pelau. Ackee and saltfish is another popular dish that is unique to Jamaica. Callaloo is a dish containing leafy vegetables such as spinach and sometimes okra amongst others, widely distributed in the Caribbean, with a distinctively mixed African and indigenous character.

The variety of dessert dishes in the area also reflects the mixed origins of the recipes. In some areas, black cake, a derivative of English Christmas pudding, may be served, especially on special occasions.

Over time, food from the Caribbean has evolved into a narrative technique through which their culture has been accentuated and promoted. However, by studying Caribbean culture through a literary lens there then runs the risk of generalizing exoticist ideas about food practices from the tropical. Some food theorists argue that this depiction of Caribbean food in various forms of media contributes to the inaccurate conceptions revolving around their culinary practices, which are much more grounded in unpleasant historical events. Therefore, it can be argued that the connection between the idea of the Caribbean being the ultimate paradise and Caribbean food being exotic is based on inaccurate information.



</doc>
<doc id="6673" url="https://en.wikipedia.org/wiki?curid=6673" title="Central Powers">
Central Powers

The Central Powers, also Central Empires (; ; / ; ), consisting of Germany, , the Ottoman Empire and Bulgaria â hence also known as the Quadruple Alliance (, , , )âwas one of the two main coalitions that fought World WarÂ I (1914â18).

It faced and was defeated by the Allied Powers that had formed around the Triple Entente. The Powers' origin was the alliance of Germany and Austria-Hungary in 1879. Despite having nominally joined the Triple Alliance before, Italy did not take part in World War I on the side of the Central Powers; the Ottoman Empire and Bulgaria did not join until after World War I had begun, even though the Ottoman Empire had retained close relations with both Germany and Austria-Hungary since the beginning of the 20th century.

The Central Powers consisted of the German Empire and the Austro-Hungarian Empire at the beginning of the war. The Ottoman Empire joined the Central Powers later in 1914. In 1915, the Kingdom of Bulgaria joined the alliance. The name "Central Powers" is derived from the location of these countries; all four (including the other groups that supported them except for Finland and Lithuania) were located between the Russian Empire in the east and France and the United Kingdom in the west. Finland, Azerbaijan, and Lithuania joined them in 1918 before the war ended and after the Russian Empire collapsed

The Central Powers were composed of the following nations:

In early July 1914, in the aftermath of the assassination of Austro-Hungarian Archduke Franz Ferdinand and the immediate likelihood of war between Austria-Hungary and Serbia, Kaiser Wilhelm II and the German government informed the Austro-Hungarian government that Germany would uphold its alliance with Austria-Hungary and defend it from possible Russian intervention if a war between Austria-Hungary and Serbia took place. When Russia enacted a general mobilization, Germany viewed the act as provocative. The Russian government promised Germany that its general mobilization did not mean preparation for war with Germany but was a reaction to the events between Austria-Hungary and Serbia. The German government regarded the Russian promise of no war with Germany to be nonsense in light of its general mobilization, and Germany, in turn, mobilized for war. On 1 August, Germany sent an ultimatum to Russia stating that since both Germany and Russia were in a state of military mobilization, an effective state of war existed between the two countries. Later that day, France, an ally of Russia, declared a state of general mobilization.

In August 1914, Germany waged war on Russia, the German government justified military action against Russia as necessary because of Russian aggression as demonstrated by the mobilization of the Russian army that had resulted in Germany mobilizing in response.

After Germany declared war on Russia, France with its alliance with Russia prepared a general mobilization in expectation of war. On 3 August 1914, Germany responded to this action by declaring war on France. Germany, facing a two-front war, enacted what was known as the Schlieffen Plan, that involved German armed forces needing to move through Belgium and swing south into France and towards the French capital of Paris. This plan was hoped to quickly gain victory against the French and allow German forces to concentrate on the Eastern Front. Belgium was a neutral country and would not accept German forces crossing its territory. Germany disregarded Belgian neutrality and invaded the country to launch an offensive towards Paris. This caused Great Britain to declare war against the German Empire, as the action violated the Treaty of London that both nations signed in 1839 guaranteeing Belgian neutrality and defense of the kingdom if a nation reneged.

Subsequently, several states declared war on Germany in late August 1914, with Italy declaring war on Austria-Hungary in 1915 and Germany on 27 August 1916, the United States declaring war on Germany on 6 April 1917 and Greece declaring war on Germany in July 1917.

Upon its founding in 1871, the German Empire controlled Alsace-Lorraine as an "imperial territory" incorporated from France after the Franco-Prussian War. It was held as part of Germany's sovereign territory.

Germany held multiple African colonies at the time of World War I. All of Germany's African colonies were invaded and occupied by Allied forces during the war.

Kamerun, German East Africa, and German Southwest Africa were German colonies in Africa. Togoland was a German protectorate in Africa.


The Kiautschou Bay concession was a German dependency in East Asia leased from China in 1898. It was occupied by Japanese forces following the Siege of Tsingtao.


German New Guinea was a German protectorate in the Pacific. It was occupied by Australian forces in 1914.

German Samoa was a German protectorate following the Tripartite Convention. It was occupied by the New Zealand Expeditionary Force in 1914.

Austria-Hungary regarded the assassination of Archduke Franz Ferdinand as being orchestrated with the assistance of Serbia. The country viewed the assassination as setting a dangerous precedent of encouraging the country's South Slav population to rebel and threaten to tear apart the multinational country. Austria-Hungary formally sent an ultimatum to Serbia demanding a full-scale investigation of Serbian government complicity in the assassination, and complete compliance by Serbia in agreeing to the terms demanded by Austria-Hungary. Serbia submitted to accept most of the demands. However, Austria-Hungary viewed this as insufficient and used this lack of full compliance to justify military intervention. These demands have been viewed as a diplomatic cover for what was going to be an inevitable Austro-Hungarian declaration of war on Serbia.

Austria-Hungary had been warned by Russia that the Russian government would not tolerate Austria-Hungary invading Serbia. However, with Germany supporting Austria-Hungary's actions, the Austro-Hungarian government hoped that Russia would not intervene and that the conflict with Serbia would be a regional conflict.

Austria-Hungary's invasion of Serbia resulted in Russia declaring war on the country and Germany in turn declared war on Russia, setting off the beginning of the clash of alliances that resulted in the World War.

Austria-Hungary was internally divided into two states with their own governments, joined in communion through the Habsburg throne. Austrian Cisleithania contained various duchies and principalities but also the Kingdom of Bohemia, the Kingdom of Dalmatia, the Kingdom of Galicia and Lodomeria. Hungarian Transleithania comprised the Kingdom of Hungary and the Kingdom of Croatia-Slavonia. In Bosnia and Herzegovina sovereign authority was shared by both Austria and Hungary.

The Ottoman Empire joined the war on the side of the Central Powers in November 1914. The Ottoman Empire had gained strong economic connections with Germany through the Berlin-to-Baghdad railway project that was still incomplete at the time. The Ottoman Empire made a formal alliance with Germany signed on 2 August 1914. The alliance treaty expected that the Ottoman Empire would become involved in the conflict in a short amount of time. However, for the first several months of the war the Ottoman Empire maintained neutrality though it allowed a German naval squadron to enter and stay near the strait of Bosphorus. Ottoman officials informed the German government that the country needed time to prepare for conflict. Germany provided financial aid and weapons shipments to the Ottoman Empire.

After pressure escalated from the German government demanding that the Ottoman Empire fulfill its treaty obligations, or else Germany would expel the country from the alliance and terminate economic and military assistance, the Ottoman government entered the war with the recently acquired cruisers from Germany, the "Yavuz Sultan Selim" (formerly "SMS Goeben") and the "Midilli" (formerly "SMS Breslau") launching a naval raid on the Russian port of Odessa, thus engaging in a military action in accordance with its alliance obligations with Germany. Russia and the Triple Entente declared war on the Ottoman Empire.

Bulgaria was still resentful after its defeat in July 1913 at the hands of Serbia, Greece and Romania. It signed a treaty of defensive alliance with the Ottoman Empire on 19 August 1914. It was the last country to join the Central Powers, which Bulgaria did in October 1915 by declaring war on Serbia. It invaded Serbia in conjunction with German and Austro-Hungarian forces. Bulgaria held claims on the region of Vardar Macedonia then held by Serbia following the Balkan Wars of 1912â1913 and (from the Bulgarian point of view), the costly Treaty of Bucharest (1913). As a condition of entering WW1 on the side of the Central Powers, Bulgaria was granted the right to reclaim that territory.

In opposition to offensive operations by Union of South Africa, which had joined the war, Boer army officers of what is now known as the Maritz Rebellion "refounded" the South African Republic in September 1914. Germany assisted the rebels, some rebels operating in and out of the German colony of German South-West Africa. The rebels were all defeated or captured by South African government forces by 4 February 1915.

The Dervish movement was a rebel Somali movement which had existed since before World War I. It was led by Mohammed Abdullah Hassan, who was seeking the independence of Somali territories. The Dervish movement was supported by the Ottoman Empire and Germany, and also briefly by the Ethiopian Empire from 1915â1916. Dervish forces fought against Italian and British forces in Italian Somaliland and British Somaliland during the Somaliland Campaign.

The Senussi Order was a Muslim political-religious tariqa (Sufi order) and clan in Libya, previously under Ottoman control, which had been lost to Italy in 1912. In 1915, they were courted by the Ottoman Empire and Germany and Grand Senussi Ahmed Sharif as-Senussi declared jihad and attacked the Italians in Libya and British controlled Egypt in the Senussi Campaign.

In 1915 the Sultanate of Darfur renounced allegiance to the Sudan government and aligned with the Ottomans. The Anglo-Egyptian Darfur Expedition pre-emptively in March 1916 to prevent an attack on Sudan and took control of the Sultanate by November 1916.

On 11 June 1916, while pursuing the Austro-Hungarian Army in Bukovina during the Brusilov Offensive, Russian forces inadvertently crossed into Romanian territory, where they overwhelmed the border guard at MamorniÈa and had a cavalry patrol disarmed and interned at HerÈa. Having no intention to force the hand of the Romanian Government, the Russians quickly left Romanian territory. The Romanian reaction to the Russian incursion left the Austro-Hungarian Minister to Romania, Count Ottokar Czernin, "fully satisfied".

During 1917 and 1918, the Finns under Carl Gustaf Emil Mannerheim and Lithuanian nationalists fought Russia for a common cause. With the Bolshevik attack of late 1917, the General Secretariat of Ukraine sought military protection first from the Central Powers and later from the armed forces of the Entente.

The Ottoman Empire also had its own allies in Azerbaijan and the Northern Caucasus. The three nations fought alongside each other under the Army of Islam in the Battle of Baku.

The Belarusian People's Republic was a client state of Germany created in 1918.

The Duchy of Courland and Semigallia was a client state of Germany created in 1918.

The Crimean Regional Government was a client state of Germany created in 1918.

The Don Republic proclaimed by Don Cossacks on May 18, 1918 fought against the Bolsheviks; it was closely associated with the German Empire

Finland existed as an autonomous Grand Duchy of Russia since 1809 and collapse of Russia in 1917 gave it independence. Following the end of the Finnish Civil War, in which Germany supported the "White" against the Soviet-backed labour movement, in May 1918 there were moves to create a Kingdom of Finland. A German prince was elected but the Armistice intervened.

The Kuban People's Republic was a client state of Germany created in 1918.

The Kingdom of Lithuania was a client state of Germany created in 1918.

The Mountainous Republic of the Northern Caucasus was associated with the Central Powers.

The Democratic Republic of Georgia declared independence in 1918 which then led to border conflicts between newly formed republic and Ottoman Empire. Soon after Ottoman Empire invaded the republic and quickly reached Borjomi. This forced Georgia to ask for help from Germany which they were granted. Germany forced the Ottomans to withdraw from Georgian territories and recognize Georgian sovereignty. Germany, Georgia and the Ottomans signed a peace treaty, the Treaty of Batum which ended the conflict with the last two. In return Georgia become a German "ally". This time period of Georgian-German friendship was known as German Caucasus expedition.

The Kingdom of Poland was a client state of Germany created in 1916. This government was recognized by the emperors of Germany and Austria-Hungary in November 1916, and it adopted a constitution in 1917. The decision to create a Polish State was taken by Germany in order to attempt to legitimize its military occupation amongst the Polish inhabitants, following upon German propaganda sent to Polish inhabitants in 1915 that German soldiers were arriving as liberators to free Poland from subjugation by Russia.
The state was utilized by the German government alongside punitive threats to induce Polish landowners living in the German-occupied Baltic territories to move to the state and sell their Baltic property to Germans in exchange for moving to Poland, and efforts were made to induce similar emigration of Poles from Prussia to the state.

The Ukrainian State was a client state of Germany led by Hetman Pavlo Skoropadskyi, who overthrew the government of the Ukrainian People's Republic.

The United Baltic Duchy was a German protectorate proposed by Germans in the former Courland, Livonian, and Estonian governorates. It briefly existed in 1918.

In 1918, the Azerbaijan Democratic Republic, facing Bolshevik revolution and opposition from the Muslim Musavat Party, was then occupied by the Ottoman Empire, which expelled the Bolsheviks while supporting the Musavat Party. The Ottoman Empire maintained a presence in Azerbaijan until the end of the war in November 1918.

Jabal Shammar was an Arab state in the Middle East that was closely associated with the Ottoman Empire.

States listed in this section were not officially members of the Central Powers, but at some point during the war engaged in cooperation with one or more Central Powers members on a level that makes their neutrality disputable.

The Ethiopian Empire was officially neutral throughout World War I but widely suspected of sympathy for the Central Powers between 1915 and 1916. At the time, Ethiopia was one of the few independent states in Africa and a major power in the Horn of Africa. Its ruler, Lij Iyasu, was widely suspected of harbouring pro-Islamic sentiments and being sympathetic to the Ottoman Empire. The German Empire also attempted to reach out to Iyasu, dispatching several unsuccessful expeditions to the region to attempt to encourage it to collaborate in an Arab Revolt-style uprising in East Africa. One of the unsuccessful expeditions was led by Leo Frobenius, a celebrated ethnographer and personal friend of Kaiser Wilhelm II. Under Iyasu's directions, Ethiopia probably supplied weapons to the Muslim Dervish rebels during the Somaliland Campaign of 1915 to 1916, indirectly helping the Central Powers' cause. 

Fearing the rising influence of Iyasu and the Ottoman Empire, the Christian nobles of Ethiopia conspired against Iyasu over 1915. Iyasu was first excommunicated by the Ethiopian Orthodox Patriarch and eventually deposed in a coup d'Ã©tat on 27 September 1916. A less pro-Ottoman regent, "Ras" Tafari Makonnen, was installed on the throne.

Other movements supported the efforts of the Central Powers for their own reasons, such as the radical Irish Nationalists who launched the Easter Rising in Dublin in April 1916; they referred to their "gallant allies in Europe". However, the majority of Irish Nationalists supported the British and allied war effort up until 1916 when the Irish political landscape was changing. In 1914, JÃ³zef PiÅsudski was permitted by Germany and Austria-Hungary to form independent Polish legions. PiÅsudski wanted his legions to help the Central Powers defeat Russia and then side with France and the UK and win the war with them.

Bulgaria signed an armistice with the Allies on 29 September 1918, following a successful Allied advance in Macedonia. The Ottoman Empire followed suit on 30 October 1918 in the face of British and Arab gains in Palestine and Syria. Austria and Hungary concluded ceasefires separately during the first week of November following the disintegration of the Habsburg Empire and the Italian offensive at Vittorio Veneto; Germany signed the armistice ending the war on the morning of 11 November 1918 after the Hundred Days Offensive, and a succession of advances by New Zealand, Australian, Canadian, Belgian, British, French and US forces in north-eastern France and Belgium. There was no unified treaty ending the war; the Central Powers were dealt with in separate treaties.




</doc>
<doc id="6675" url="https://en.wikipedia.org/wiki?curid=6675" title="Conservatism">
Conservatism

Conservatism is a political and social philosophy promoting traditional social institutions in the context of culture and civilization. The central tenets of conservatism include tradition, organic society, hierarchy, authority, and property rights. Conservatives seek to preserve a range of institutions such as religion, parliamentary government, and property rights, with the aim of emphasizing social stability and continuity. The more traditional elementsâreactionariesâoppose modernism and seek a return to "the way things were". 

The first established use of the term in a political context originated in 1818 with FranÃ§ois-RenÃ© de Chateaubriand during the period of Bourbon Restoration that sought to roll back the policies of the French Revolution. Historically associated with right-wing politics, the term has since been used to describe a wide range of views. There is no single set of policies regarded as conservative because the meaning of conservatism depends on what is considered traditional in a given place and time. Thus conservatives from different parts of the worldâeach upholding their respective traditionsâmay disagree on a wide range of issues. Edmund Burke, an 18th-century politician who opposed the French Revolution, but supported the American Revolution, is credited as one of the main theorists of conservatism in Great Britain in the 1790s.

According to Quintin Hogg, the chairman of the British Conservative Party in 1959: "Conservatism is not so much a philosophy as an attitude, a constant force, performing a timeless function in the development of a free society, and corresponding to a deep and permanent requirement of human nature itself".

Liberal conservatism incorporates the classical liberal view of minimal government intervention in the economy. Individuals should be free to participate in the market and generate wealth without government interference. However, individuals cannot be thoroughly depended on to act responsibly in other spheres of life, therefore liberal conservatives believe that a strong state is necessary to ensure law and order and social institutions are needed to nurture a sense of duty and responsibility to the nation. Liberal conservatism is a variant of conservatism that is strongly influenced by liberal stances.

As these latter two terms have had different meanings over time and across countries, liberal conservatism also has a wide variety of meanings. Historically, the term often referred to the combination of economic liberalism, which champions "laissez-faire" markets, with the classical conservatism concern for established tradition, respect for authority and religious values. It contrasted itself with classical liberalism, which supported freedom for the individual in both the economic and social spheres.

Over time, the general conservative ideology in many countries adopted economic liberal arguments and the term liberal conservatism was replaced with conservatism. This is also the case in countries where liberal economic ideas have been the tradition such as the United States and are thus considered conservative. In other countries where liberal conservative movements have entered the political mainstream, such as Italy and Spain, the terms liberal and conservative may be synonymous. The liberal conservative tradition in the United States combines the economic individualism of the classical liberals with a Burkean form of conservatism (which has also become part of the American conservative tradition, such as in the writings of Russell Kirk).

A secondary meaning for the term liberal conservatism that has developed in Europe is a combination of more modern conservative (less traditionalist) views with those of social liberalism. This has developed as an opposition to the more collectivist views of socialism. Often this involves stressing what are now conservative views of free market economics and belief in individual responsibility, with social liberal views on defence of civil rights, environmentalism and support for a limited welfare state. In continental Europe, this is sometimes also translated into English as social conservatism.

Conservative liberalism is a variant of liberalism that combines liberal values and policies with conservative stances. The roots of conservative liberalism are found at the beginning of the history of liberalism. Until the two World Wars, in most European countries the political class was formed by conservative liberals, from Germany to Italy. Events after World War I brought the more radical version of classical liberalism to a more conservative (i.e. more moderate) type of liberalism.

Libertarian conservatism describes certain political ideologies most prominently within the United States which combine libertarian economic issues with aspects of conservatism. Its four main branches are constitutionalism, paleolibertarianism, small government conservatism and Christian libertarianism. They generally differ from paleoconservatives, in that they favor more personal and economic freedom.

Agorists such as Samuel Edward Konkin III labeled libertarian conservatism right-libertarianism.

In contrast to paleoconservatives, libertarian conservatives support strict "laissez-faire" policies such as free trade, opposition to any national bank and opposition to business regulations. They are vehemently opposed to environmental regulations, corporate welfare, subsidies and other areas of economic intervention.

Many conservatives, especially in the United States, believe that the government should not play a major role in regulating business and managing the economy. They typically oppose efforts to charge high tax rates and to redistribute income to assist the poor. Such efforts, they argue, do not properly reward people who have earned their money through hard work.

Fiscal conservatism is the economic philosophy of prudence in government spending and debt. In his "Reflections on the Revolution in France", Edmund Burke argued that a government does not have the right to run up large debts and then throw the burden on the taxpayer: [I]t is to the property of the citizen, and not to the demands of the creditor of the state, that the first and original faith of civil society is pledged. The claim of the citizen is prior in time, paramount in title, superior in equity. The fortunes of individuals, whether possessed by acquisition or by descent or in virtue of a participation in the goods of some community, were no part of the creditor's security, expressed or implied...[T]he public, whether represented by a monarch or by a senate, can pledge nothing but the public estate; and it can have no public estate except in what it derives from a just and proportioned imposition upon the citizens at large.

National conservatism is a political term used primarily in Europe to describe a variant of conservatism which concentrates more on national interests than standard conservatism as well as upholding cultural and ethnic identity, while not being outspokenly nationalist or supporting a far-right approach. In Europe, national conservatives are usually eurosceptics.

National conservatism is heavily oriented towards the traditional family and social stability as well as in favour of limiting immigration. As such, national conservatives can be distinguished from economic conservatives, for whom free market economic policies, deregulation and fiscal conservatism are the main priorities. Some commentators have identified a growing gap between national and economic conservatism: "[M]ost parties of the Right [today] are run by economic conservatives who, in varying degrees, have marginalized social, cultural, and national conservatives". National conservatism is also related to traditionalist conservatism.

Traditionalist conservatism is a political philosophy emphasizing the need for the principles of natural law and transcendent moral order, tradition, hierarchy and organic unity, agrarianism, classicism and high culture as well as the intersecting spheres of loyalty. Some traditionalists have embraced the labels "reactionary" and "counterrevolutionary", defying the stigma that has attached to these terms since the Enlightenment. Having a hierarchical view of society, many traditionalist conservatives, including a few Americans, defend the monarchical political structure as the most natural and beneficial social arrangement.

Cultural conservatives support the preservation of the heritage of one nation, or of a shared culture that is not defined by national boundaries. The shared culture may be as divergent as Western culture or Chinese culture. In the United States, the term "cultural conservative" may imply a conservative position in the culture war. Cultural conservatives hold fast to traditional ways of thinking even in the face of monumental change. They believe strongly in traditional values and traditional politics and often have an urgent sense of nationalism.

Social conservatism is distinct from cultural conservatism, although there are some overlaps. Social conservatives may believe that society is built upon a fragile network of relationships which need to be upheld through duty, traditional values and established institutions; and that the government has a role in encouraging or enforcing traditional values or behaviours. A social conservative wants to preserve traditional morality and social mores, often by opposing what they consider radical policies or social engineering. Social change is generally regarded as suspect.

A second meaning of the term social conservatism developed in the Nordic countries and continental Europe, where it refers to liberal conservatives supporting modern European welfare states.

Social conservatives (in the first meaning of the phrase) in many countries generally favour the anti-abortion position in the abortion controversy and oppose human embryonic stem cell research (particularly if publicly funded); oppose both eugenics and human enhancement (transhumanism) while supporting bioconservatism; support a traditional definition of marriage as being one man and one woman; view the nuclear family model as society's foundational unit; oppose expansion of civil marriage and child adoption to couples in same-sex relationships; promote public morality and traditional family values; oppose atheism, especially militant atheism, secularism and the separation of church and state; support the prohibition of drugs, prostitution and euthanasia; and support the censorship of pornography and what they consider to be obscenity or indecency. Most conservatives in the United States support the death penalty.

In contrast to the tradition-based definition of conservatism, some political theorists such as Corey Robin define conservatism primarily in terms of a general defense of social and economic inequality. From this perspective, conservatism is less an attempt to uphold traditional institutions and more "a meditation onâand theoretical rendition ofâthe felt experience of having power, seeing it threatened, and trying to win it back". Conversely, some conservatives may argue that they are seeking less to protect their own power than they are seeking to protect "inalienable rights" and promote norms and rules that they believe should stand timeless and eternal, applying to each citizen.

Religious conservatism principally apply the teachings of particular religions to politics, sometimes by merely proclaiming the value of those teachings, at other times by having those teachings influence laws.

In most democracies, political conservatism seeks to uphold traditional family structures and social values. Religious conservatives typically oppose abortion, LGBT behavior (or, in certain cases, identity), drug use, and sexual activity outside of marriage. In some cases, conservative values are grounded in religious beliefs, and conservatives seek to increase the role of religion in public life.

Paternalistic conservatism is a strand in conservatism which reflects the belief that societies exist and develop organically and that members within them have obligations towards each other. There is particular emphasis on the paternalistic obligation of those who are privileged and wealthy to the poorer parts of society. Since it is consistent with principles such as organicism, hierarchy and duty, it can be seen as an outgrowth of traditional conservatism. Paternal conservatives support neither the individual nor the state in principle, but are instead prepared to support either or recommend a balance between the two depending on what is most practical.

It stresses the importance of a social safety net to deal with poverty, support of limited redistribution of wealth along with government regulation of markets in the interests of both consumers and producers. Paternalistic conservatism first arose as a distinct ideology in the United Kingdom under Prime Minister Benjamin Disraeli's "One Nation" Toryism. There have been a variety of one nation conservative governments. In the United Kingdom, the Prime Ministers Disraeli, Stanley Baldwin, Neville Chamberlain, Winston Churchill, Harold Macmillan and Boris Johnson were or are one nation conservatives.

In Germany, during the 19th-century German Chancellor Otto von Bismarck adopted policies of state-organized compulsory insurance for workers against sickness, accident, incapacity and old age. Chancellor Leo von Caprivi promoted a conservative agenda called the "New Course".

In the United States, the administration of President William Howard Taft was a progressive conservative and he described himself as "a believer in progressive conservatism" and President Dwight D. Eisenhower declared himself an advocate of "progressive conservatism".

In Canada, a variety of conservative governments have been part of the Red tory tradition, with Canada's former major conservative party being named the Progressive Conservative Party of Canada from 1942 to 2003. In Canada, the Prime Ministers Arthur Meighen, R. B. Bennett, John Diefenbaker, Joe Clark, Brian Mulroney, and Kim Campbell led Red tory federal governments.

Authoritarian conservatism or reactionary conservatism refers to autocratic regimes that center their ideology around conservative nationalism, rather than ethnic nationalism, though certain racial components such as antisemitism may exist. Authoritarian conservative movements show strong devotion towards religion, tradition and culture while also expressing fervent nationalism akin to other far-right nationalist movements. Examples of authoritarian conservative leaders include AntÃ³nio de Oliveira Salazar and Engelbert Dollfuss. Authoritarian conservative movements were prominent in the same era as fascism, with which it sometimes clashed. Although both ideologies shared core values such as nationalism and had common enemies such as communism and materialism, there was nonetheless a contrast between the traditionalist nature of authoritarian conservatism and the revolutionary, palingenetic and populist nature of fascismâthus it was common for authoritarian conservative regimes to suppress rising fascist and National Socialist movements. The hostility between the two ideologies is highlighted by the struggle for power for the National Socialists in Austria, which was marked by the assassination of Engelbert Dollfuss.

Sociologist Seymour Martin Lipset has examined the class basis of right-wing extremist politics in the 1920â1960 era. He reports:
Conservative or rightist extremist movements have arisen at different periods in modern history, ranging from the Horthyites in Hungary, the Christian Social Party of Dollfuss in Austria, "Der Stahlhelm" and other nationalists in pre-Hitler Germany, and Salazar in Portugal, to the pre-1966 Gaullist movements and the monarchists in contemporary France and Italy. The right extremists are conservative, not revolutionary. They seek to change political institutions in order to preserve or restore cultural and economic ones, while extremists of the centre and left seek to use political means for cultural and social revolution. The ideal of the right extremist is not a totalitarian ruler, but a monarch, or a traditionalist who acts like one. Many such movements in Spain, Austria, Hungary, Germany, and Italy-have been explicitly monarchist... The supporters of these movements differ from those of the centrists, tending to be wealthier, and more religious, which is more important in terms of a potential for mass support.

In Great Britain, conservative ideas (though not yet called that) emerged in the Tory movement during the Restoration period (1660â1688). Toryism supported a hierarchical society with a monarch who ruled by divine right. Tories opposed the idea that sovereignty derived from the people and rejected the authority of parliament and freedom of religion. Robert Filmer's "Patriarcha: or the Natural Power of Kings" (published posthumously in 1680, but written before the English Civil War of 1642â1651) became accepted as the statement of their doctrine. However, the Glorious Revolution of 1688 destroyed this principle to some degree by establishing a constitutional government in England, leading to the hegemony of the Tory-opposed Whig ideology. Faced with defeat, the Tories reformed their movement, now holding that sovereignty was vested in the three estates of Crown, Lords and Commons rather than solely in the Crown. Toryism became marginalized during the long period of Whig ascendancy in the 18th century.

Conservatives typically see Richard Hooker (1554â1600) as the founding father of conservatism, along with the Marquess of Halifax (1633â1695), David Hume (1711â1776) and Edmund Burke (1729â1797). Halifax promoted pragmatism in government whilst Hume argued against political rationalism and utopianism. Burke served as the private secretary to the Marquis of Rockingham and as official pamphleteer to the Rockingham branch of the Whig party. Together with the Tories, they were the conservatives in the late 18th century United Kingdom. Burke's views were a mixture of liberal and conservative. He supported the American Revolution of 1765â1783, but abhorred the violence of the French Revolution (1789â1799). He accepted the liberal ideals of private property and the economics of Adam Smith (1723â1790), but thought that economics should remain subordinate to the conservative social ethic, that capitalism should be subordinate to the medieval social tradition and that the business class should be subordinate to aristocracy. He insisted on standards of honor derived from the medieval aristocratic tradition and saw the aristocracy as the nation's natural leaders. That meant limits on the powers of the Crown, since he found the institutions of Parliament to be better informed than commissions appointed by the executive. He favored an established church, but allowed for a degree of religious toleration. Burke justified the social order on the basis of tradition: tradition represented the wisdom of the species and he valued community and social harmony over social reforms. Burke was a leading theorist in his day, finding extreme idealism (either Tory or Whig) an endangerment to broader liberties and (like Hume) rejecting abstract reason as an unsound guide for political theory. Despite their influence on future conservative thought, none of these early contributors were explicitly involved in Tory politics. Hooker lived in the 16th century, long before the advent of toryism, whilst Hume was an apolitical philosopher and Halifax similarly politically independent. Burke described himself as a Whig.

Shortly after Burke's death in 1797, conservatism revived as a mainstream political force as the Whigs suffered a series of internal divisions. This new generation of conservatives derived their politics not from Burke, but from his predecessor, the Viscount Bolingbroke (1678â1751), who was a Jacobite and traditional Tory, lacking Burke's sympathies for Whiggish policies such as Catholic emancipation and American independence (famously attacked by Samuel Johnson in "Taxation No Tyranny"). In the first half of the 19th century, many newspapers, magazines, and journals promoted loyalist or right-wing attitudes in religion, politics and international affairs. Burke was seldom mentioned, but William Pitt the Younger (1759â1806) became a conspicuous hero. The most prominent journals included "The Quarterly Review", founded in 1809 as a counterweight to the Whigs' "Edinburgh Review" and the even more conservative "Blackwood's Edinburgh Magazine". Sack finds that the "Quarterly Review" promoted a balanced Canningite toryism as it was neutral on Catholic emancipation and only mildly critical of Nonconformist Dissent; it opposed slavery and supported the current poor laws; and it was "aggressively imperialist". The high-church clergy of the Church of England read the "Orthodox Churchman's Magazine" which was equally hostile to Jewish, Catholic, Jacobin, Methodist and Unitarian spokesmen. Anchoring the ultra Tories, "Blackwood's Edinburgh Magazine" stood firmly against Catholic emancipation and favoured slavery, cheap money, mercantilism, the Navigation Acts and the Holy Alliance.

Conservatism evolved after 1820, embracing free trade in 1846 and a commitment to democracy, especially under Disraeli. The effect was to significantly strengthen conservatism as a grassroots political force. Conservatism no longer was the philosophical defense of the landed aristocracy, but had been refreshed into redefining its commitment to the ideals of order, both secular and religious, expanding imperialism, strengthened monarchy and a more generous vision of the welfare state as opposed to the punitive vision of the Whigs and liberals. As early as 1835, Disraeli attacked the Whigs and utilitarians as slavishly devoted to an industrial oligarchy, while he described his fellow Tories as the only "really democratic party of England" and devoted to the interests of the whole people. Nevertheless, inside the party there was a tension between the growing numbers of wealthy businessmen on the one side and the aristocracy and rural gentry on the other. The aristocracy gained strength as businessmen discovered they could use their wealth to buy a peerage and a country estate.

Although conservatives opposed attempts to allow greater representation of the middle class in parliament, they conceded that electoral reform could not be reversed and promised to support further reforms so long as they did not erode the institutions of church and state. These new principles were presented in the Tamworth Manifesto of 1834, which historians regard as the basic statement of the beliefs of the new Conservative Party.
Some conservatives lamented the passing of a pastoral world where the ethos of "noblesse oblige" had promoted respect from the lower classes. They saw the Anglican Church and the aristocracy as balances against commercial wealth. They worked toward legislation for improved working conditions and urban housing. This viewpoint would later be called Tory democracy. However, since Burke, there has always been tension between traditional aristocratic conservatism and the wealthy business class.

In 1834, Tory Prime Minister Robert Peel issued the Tamworth Manifesto in which he pledged to endorse moderate political reform. This marked the beginning of the transformation of British conservatism from High Tory reactionism towards a more modern form based on "conservation". The party became known as the Conservative Party as a result, a name it has retained to this day. However, Peel would also be the root of a split in the party between the traditional Tories (led by the Earl of Derby and Benjamin Disraeli) and the "Peelites" (led first by Peel himself, then by the Earl of Aberdeen). The split occurred in 1846 over the issue of free trade, which Peel supported, versus protectionism, supported by Derby. The majority of the party sided with Derby whilst about a third split away, eventually merging with the Whigs and the radicals to form the Liberal Party. Despite the split, the mainstream Conservative Party accepted the doctrine of free trade in 1852.

In the second half of the 19th century, the Liberal Party faced political schisms, especially over Irish Home Rule. Leader William Gladstone (himself a former Peelite) sought to give Ireland a degree of autonomy, a move that elements in both the left and right-wings of his party opposed. These split off to become the Liberal Unionists (led by Joseph Chamberlain), forming a coalition with the Conservatives before merging with them in 1912. The Liberal Unionist influence dragged the Conservative Party towards the left as Conservative governments passing a number of progressive reforms at the turn of the 20th century. By the late 19th century, the traditional business supporters of the Liberal Party had joined the Conservatives, making them the party of business and commerce.

After a period of Liberal dominance before the First World War, the Conservatives gradually became more influential in government, regaining full control of the cabinet in 1922. In the inter-war period, conservatism was the major ideology in Britain as the Liberal Party vied with the Labour Party for control of the left. After the Second World War, the first Labour government (1945â1951) under Clement Attlee embarked on a program of nationalization of industry and the promotion of social welfare. The Conservatives generally accepted those policies until the 1980s.
In the 1980s, the Conservative government of Margaret Thatcher, guided by neoliberal economics, reversed many of Labour's programmes. The Conservative Party also adopt soft eurosceptic politics, and oppose Federal Europe. Other conservative political parties, such as the United Kingdom Independence Party (UKIP, founded in 1993), Northern Ireland's Ulster Unionist Party (UUP) and the Democratic Unionist Party (DUP, founded in 1971), began to appear, although they have yet to make any significant impact at Westminster (, the DUP comprises the largest political party in the ruling coalition in the Northern Ireland Assembly), and from 2017 to 2019 the DUP provided support for the Conservative minority government.

Another form of conservatism developed in France in parallel to conservatism in Britain. It was influenced by Counter-Enlightenment works by men such as Joseph de Maistre and Louis de Bonald. Many continental or traditionalist conservatives do not support separation of church and state, with most supporting state recognition of and cooperation with the Catholic Church, such as had existed in France before the Revolution.

Eventually, conservatives added Gaullism, patriotism, and nationalism to the list of traditional values they support. Conservatives were the first to embrace nationalism, which was previously associated with liberalism and the Revolution in France.

Conservatism developed alongside nationalism in Germany, culminating in Germany's victory over France in the Franco-Prussian War, the creation of the unified German Empire in 1871 and the simultaneous rise of Otto von Bismarck on the European political stage. Bismarck's "balance of power" model maintained peace in Europe for decades at the end of the 19th century. His "revolutionary conservatism" was a conservative state-building strategy designed to make ordinary Germansânot just the Junker eliteâmore loyal to state and emperor, he created the modern welfare state in Germany in the 1880s. According to Kees van Kersbergen and Barbara Vis, his strategy was: 

Bismarck also enacted universal male suffrage in the new German Empire in 1871. He became a great hero to German conservatives, who erected many monuments to his memory after he left office in 1890.

With the rise of Nazism in 1933, agrarian movements faded and was supplanted by a more command-based economy and forced social integration. Though Adolf Hitler succeeded in garnering the support of many German industrialists, prominent traditionalists openly and secretly opposed his policies of euthanasia, genocide and attacks on organized religion, including Claus von Stauffenberg, Dietrich Bonhoeffer, Henning von Tresckow, Bishop Clemens August Graf von Galen and the monarchist Carl Friedrich Goerdeler.

More recently, the work of conservative Christian Democratic Union leader and Chancellor Helmut Kohl helped bring about German reunification, along with the closer European integration in the form of the Maastricht Treaty.

Today, German conservatism is often associated with politicians such as Chancellor Angela Merkel, whose tenure has been marked by attempts to save the common European currency (Euro) from demise. The German conservatives are divided under Merkel due to the refugee crisis in Germany and many conservatives in the CDU/CSU oppose the refugee and migrant policies developed under Merkel.

American conservatism is a broad system of political beliefs in the United States that is characterized by respect for American traditions, support for Judeo-Christian values, economic liberalism, anti-communism and a defense of Western culture. Liberty within the bounds of conformity to conservatism is a core value, with a particular emphasis on strengthening the free market, limiting the size and scope of government and opposition to high taxes and government or labor union encroachment on the entrepreneur.

Interestingly, in early American politics, it was the Democratic party practicing 'conservatism' in its attempts to maintain the social and economic institution of slavery. Democratic president Andrew Johnson, as one commonly known example, was considered a Conservative. âThe Democrats were often called conservative and embraced that label. Many of them were conservative in the sense that they wanted things to be like they were in the past, especially as far as race was concerned.â 

The major conservative party in the United States today is the Republican Party, also known as the GOP (Grand Old Party). American conservatives consider individual liberty, as long as it conforms to conservative values, small government, deregulation of the government, economic liberalism, and free trade, as the fundamental traits of democracy, which contrasts with modern American liberals, who generally place a greater value on social equality and social justice.

Conservative political parties vary widely from country to country in the goals they wish to achieve. Both conservative and liberal parties tend to favor private ownership of property, in opposition to communist, socialist and green parties, which favor communal ownership or laws requiring social responsibility on the part of property owners. Where conservatives and liberals differ is primarily on social issues. Conservatives tend to reject behavior that does not conform to some social norm. Modern conservative parties often define themselves by their opposition to liberal or labor parties. The United States usage of the term "conservative" is unique to that country.

According to Alan Ware, Belgium, Denmark, Finland, France, Greece, Iceland, Luxembourg, Netherlands, Norway, Sweden, Switzerland and the United Kingdom retained viable conservative parties into the 1980s. Ware said that Australia, Germany, Israel, Italy, Japan, Malta, New Zealand, Spain, and the United States had no conservative parties, although they had either Christian democrats or liberals as major right-wing parties. Canada, Ireland and Portugal had right-wing political parties that defied categorization: the Progressive Conservative Party of Canada; Fianna FÃ¡il, Fine Gael and Progressive Democrats in Ireland; and the Social Democratic Party of Portugal. Since then, the Swiss People's Party has moved to the extreme right and is no longer considered to be conservative.

Klaus von Beyme, who developed the method of party categorization, found that no modern Eastern European parties could be considered conservative, although the communist and communist-successor parties had strong similarities.

In Italy, which was united by liberals and radicals ("Risorgimento"), liberals, not conservatives, emerged as the party of the right. In the Netherlands, conservatives merged into a new Christian democratic party in 1980. In Austria, Germany, Portugal and Spain, conservatism was transformed into and incorporated into fascism or the far-right. In 1940, all Japanese parties were merged into a single fascist party. Following the war, Japanese conservatives briefly returned to politics, but were largely purged from public office.

Louis Hartz explained the absence of conservatism in Australia or the United States as a result of their settlement as radical or liberal fragments of Great Britain. Although he said English Canada had a negligible conservative influence, subsequent writers claimed that loyalists opposed to the American Revolution brought a Tory ideology into Canada. Hartz explained conservatism in Quebec and Latin America as a result of their settlement as feudal societies. The American conservative writer Russell Kirk provided the opinion that conservatism had been brought to the United States and interpreted the American Revolution as a "conservative revolution".

Conservative elites have long dominated Latin American nations. Mostly, this has been achieved through control of and support for civil institutions, the church and the armed forces, rather than through party politics. Typically, the church was exempt from taxes and its employees immune from civil prosecution. Where national conservative parties were weak or non-existent, conservatives were more likely to rely on military dictatorship as a preferred form of government. However, in some nations where the elites were able to mobilize popular support for conservative parties, longer periods of political stability were achieved. Chile, Colombia and Venezuela are examples of nations that developed strong conservative parties. Argentina, Brazil, El Salvador and Peru are examples of nations where this did not occur. The Conservative Party of Venezuela disappeared following the Federal Wars of 1858â1863. Chile's conservative party, the National Party, disbanded in 1973 following a military coup and did not re-emerge as a political force following the subsequent return to democracy.

Having its roots in the conservative Catholic Party, the Christian People's Party retained a conservative edge through the twentieth century, supporting the king in the Royal Question, supporting nuclear family as the cornerstone of society, defending Christian education, and opposing euthanasia. The Christian People's Party dominated politics in post-war Belgium. In 1999, the party's support collapsed, and it became the country's fifth-largest party. Currently, the N-VA (nieuw-vlaamse alliantie/New Flemish Alliance) is the largest party in Belgium.

Canada's conservatives had their roots in the Tory loyalists who left America after the American Revolution. They developed in the socio-economic and political cleavages that existed during the first three decades of the 19th century and had the support of the business, professional and established Church (Anglican) elites in Ontario and to a lesser extent in Quebec. Holding a monopoly over administrative and judicial offices, they were called the "Family Compact" in Ontario and the "Chateau Clique" in Quebec. John A. Macdonald's successful leadership of the movement to confederate the provinces and his subsequent tenure as prime minister for most of the late 19th century rested on his ability to bring together the English-speaking Protestant oligarchy and the ultramontane Catholic hierarchy of Quebec and to keep them united in a conservative coalition.

The conservatives combined pro-market liberalism and Toryism. They generally supported an activist government and state intervention in the marketplace and their policies were marked by "noblesse oblige", a paternalistic responsibility of the elites for the less well-off. From 1942, the party was known as the Progressive Conservatives until 2003, when the national party merged with the Canadian Alliance to form the Conservative Party of Canada.

The conservative and autonomist Union Nationale, led by Maurice Duplessis, governed the province of Quebec in periods from 1936 to 1960 and in a close alliance with the Catholic Church, small rural elites, farmers and business elites. This period, known by liberals as the Great Darkness, ended with the Quiet Revolution and the party went into terminal decline. By the end of the 1960s, the political debate in Quebec centered around the question of independence, opposing the social democratic and sovereignist Parti QuÃ©bÃ©cois and the centrist and federalist Quebec Liberal Party, therefore marginalizing the conservative movement. Most French Canadian conservatives rallied either the Quebec Liberal Party or the Parti QuÃ©bÃ©cois, while some of them still tried to offer an autonomist third-way with what was left of the Union Nationale or the more populists Ralliement crÃ©ditiste du QuÃ©bec and Parti national populaire, but by the 1981 provincial election politically organized conservatism had been obliterated in Quebec. It slowly started to revive at the 1994 provincial election with the Action dÃ©mocratique du QuÃ©bec, who served as Official opposition in the National Assembly from 2007 to 2008, before its merger with FranÃ§ois Legault's Coalition Avenir QuÃ©bec in 2012, that took power in 2018.

The modern Conservative Party of Canada has rebranded conservatism and under the leadership of Stephen Harper, the Conservative Party added more conservative policies.

The Colombian Conservative Party, founded in 1849, traces its origins to opponents of General Francisco de Paula Santander's 1833â1837 administration. While the term "liberal" had been used to describe all political forces in Colombia, the conservatives began describing themselves as "conservative liberals" and their opponents as "red liberals". From the 1860s until the present, the party has supported strong central government; supported the Catholic Church, especially its role as protector of the sanctity of the family; and opposed separation of church and state. Its policies include the legal equality of all men, the citizen's right to own property and opposition to dictatorship. It has usually been Colombia's second largest party, with the Colombian Liberal Party being the largest.

Founded in 1915, the Conservative People's Party of Denmark. was the successor of "HÃ¸jre" (literally "Right"). The conservative party led the government coalition from 1982 to 1993. The party was a junior partner in coalition with the Liberals from 2001 to 2011. The party is preceded by 11 years by the Young Conservatives (KU), today the youth movement of the party. The party suffered a major defeat in the parliamentary elections of September 2011 in which the party lost more than half of its seat and also lost governmental power. A liberal cultural policy dominated during the post-war period. However, by the 1990s, disagreements regarding immigrants from entirely different cultures ignited a conservative backlash.

The conservative party in Finland is the National Coalition Party (in Finnish "Kansallinen Kokoomus", "Kok"). The party was founded in 1918 when several monarchist parties united. Although in the past the party was right-wing, today it is a moderate liberal conservative party. While the party advocates economic liberalism, it is committed to the social market economy.

Conservatism in France focused on the rejection of the secularism of the French Revolution, support for the role of the Catholic Church and the restoration of the monarchy. The monarchist cause was on the verge of victory in the 1870s, but then collapsed because the proposed king refused to fly the tri-colored flag. Religious tensions heightened in the 1890â1910 era, but moderated after the spirit of unity in fighting the First World War. An extreme form of conservatism characterized the Vichy regime of 1940â1944 with heightened antisemitism, opposition to individualism, emphasis on family life and national direction of the economy.
Following the Second World War, conservatives in France supported Gaullist groups and have been nationalistic and emphasized tradition, order and the regeneration of France. Gaullists held divergent views on social issues. The number of conservative groups, their lack of stability and their tendency to be identified with local issues defy simple categorization. Conservatism has been the major political force in France since the Second World War. Unusually, post-war French conservatism was formed around the personality of a leader, Charles de Gaulle; and did not draw on traditional French conservatism, but on the Bonapartism tradition. Gaullism in France continues under The Republicans (formerly Union for a Popular Movement), which was previously led by Nicolas Sarkozy, a conservative figure in France. The word "conservative" itself is a term of abuse in France.

The main inter-war conservative party was called the People's Party (PP), which supported constitutional monarchy and opposed the republican Liberal Party. Both it and the Liberal party were suppressed by the authoritarian, arch-conservative and royalist 4th of August Regime of Ioannis Metaxas in 1936â1941. The PP was able to re-group after the Second World War as part of a United Nationalist Front which achieved power campaigning on a simple anticommunist, ultranationalist platform during the Greek Civil War (1946â1949). However, the vote received by the PP declined during the so-called "Centrist Interlude" in 1950â1952. In 1952, Marshal Alexandros Papagos created the Greek Rally as an umbrella for the right-wing forces. The Greek Rally came to power in 1952 and remained the leading party in Greece until 1963âafter Papagos' death in 1955 reformed as the National Radical Union under Konstantinos Karamanlis. Right-wing governments backed by the palace and the army overthrew the Centre Union government in 1965 and governed the country until the establishment of the far-right Greek junta (1967â1974). After the regime's collapse in August 1974, Karamanlis returned from exile to lead the government and founded the New Democracy party. The new conservative party had four objectives: to confront Turkish expansionism in Cyprus, to reestablish and solidify democratic rule, to give the country a strong government and to make a powerful moderate party a force in Greek politics.

The Independent Greeks, a newly formed political party in Greece, has also supported conservatism, particularly national and religious conservatism. The Founding Declaration of the Independent Greeks strongly emphasises in the preservation of the Greek state and its sovereignty, the Greek people and the Greek Orthodox Church.

Founded in 1924 as the Conservative Party, Iceland's Independence Party adopted its current name in 1929 after the merger with the Liberal Party. From the beginning, they have been the largest vote-winning party, averaging around 40%. They combined liberalism and conservatism, supported nationalization of infrastructure and opposed class conflict. While mostly in opposition during the 1930s, they embraced economic liberalism, but accepted the welfare state after the war and participated in governments supportive of state intervention and protectionism. Unlike other Scandanivian conservative (and liberal) parties, it has always had a large working-class following. After the financial crisis in 2008, the party has sunk to a lower support level around 20â25%.

Luxembourg's major Christian democratic conservative party, the Christian Social People's Party (CSV or PCS), was formed as the Party of the Right in 1914 and adopted its present name in 1945. It was consistently the largest political party in Luxembourg, and dominated politics throughout the 20th century.

The Conservative Party of Norway (Norwegian: HÃ¸yre, literally "right") was formed by the old upper class of state officials and wealthy merchants to fight the populist democracy of the Liberal Party, but lost power in 1884, when parliamentarian government was first practised. It formed its first government under parliamentarism in 1889 and continued to alternate in power with the Liberals until the 1930s, when Labour became the dominant political party. It has elements both of paternalism, stressing the responsibilities of the state, and of economic liberalism. It first returned to power in the 1960s. During KÃ¥re Willoch's premiership in the 1980s, much emphasis was laid on liberalizing the credit and housing market, and abolishing the NRK TV and radio monopoly, while supporting law and order in criminal justice and traditional norms in education

Sweden's conservative party, the Moderate Party, was formed in 1904, two years after the founding of the Liberal Party. The party emphasizes tax reductions, deregulation of private enterprise and privatization of schools, hospitals, and kindergartens.

There are a number of conservative parties in Switzerland's parliament, the Federal Assembly. These include the largest, the Swiss People's Party (SVP), the Christian Democratic People's Party (CVP) and the Conservative Democratic Party of Switzerland (BDP), which is a splinter of the SVP created in the aftermath to the election of Eveline Widmer-Schlumpf as Federal Council. The right-wing parties have a majority in the Federal Assembly.

The Swiss People's Party (SVP or UDC) was formed from the 1971 merger of the Party of Farmers, Traders and Citizens, formed in 1917 and the smaller Swiss Democratic Party, formed in 1942. The SVP emphasized agricultural policy and was strong among farmers in German-speaking Protestant areas. As Switzerland considered closer relations with the European Union in the 1990s, the SVP adopted a more militant protectionist and isolationist stance. This stance has allowed it to expand into German-speaking Catholic mountainous areas. The Anti-Defamation League, a non-Swiss lobby group based in the United States has accused them of manipulating issues such as immigration, Swiss neutrality and welfare benefits, awakening antisemitism and racism. The Council of Europe has called the SVP "extreme right", although some scholars dispute this classification. For instance, Hans-Georg Betz describes it as "populist radical right". The SVP is the largest party since 2003.

According to historian James Sack, English conservatives celebrate Edmund Burke as their intellectual father. Burke was affiliated with the Whig Party which eventually became the Liberal Party, but the modern Conservative Party is generally thought to derive from the Tory party and the MPs of the modern conservative party are still frequently referred to as Tories.

While conservatism has been seen as an appeal to traditional, hierarchical society, some writers such as Samuel P. Huntington see it as situational. Under this definition, conservatives are seen as defending the established institutions of their time.

The Liberal Party of Australia adheres to the principles of social conservatism and liberal conservatism. It is liberal in the sense of economics. Other conservative parties are the National Party of Australia, a sister party of the Liberals, Family First Party, Democratic Labor Party, Shooters, Fishers and Farmers Party, Australian Conservatives, and the Katter's Australian Party.

The second largest party in the country is the Australian Labor Party and its dominant faction is Labor Right, a socially conservative element. Australia undertook significant economic reform under the Labor Party in the mid-1980s. Consequently, issues like protectionism, welfare reform, privatization and deregulation are no longer debated in the political space as they are in Europe or North America. Moser and Catley explain: "In America, 'liberal' means left-of-center, and it is a pejorative term when used by conservatives in adversarial political debate. In Australia, of course, the conservatives are in the Liberal Party". Jupp points out that, "[the] decline in English influences on Australian reformism and radicalism, and appropriation of the symbols of Empire by conservatives continued under the Liberal Party leadership of Sir Robert Menzies, which lasted until 1966".

Conservatism in Brazil originates from the cultural and historical tradition of Brazil, whose cultural roots are Luso-Iberian and Roman Catholic. Brazilian conservatism from the 20th century on includes names such as Gerardo Melo MourÃ£o and Otto Maria Carpeaux in literature; Oliveira Lima and Oliveira Torres in historiography; Sobral Pinto and Miguel Reale in law; Plinio CorrÃªa de Oliveira and Father Paulo Ricardo in the Catholic Church; Roberto Campos and Mario Henrique Simonsen in economics; Carlos Lacerda in the political arena; and Olavo de Carvalho in philosophy. Brazilian Labour Renewal Party, Patriota, Progressistas, Social Christian Party and Social Liberal Party are the conservative parties in Brazil.

In India, the Bharatiya Janata Party (BJP), led by Narendra Modi, represent conservative politics. The BJP is the largest right-wing conservative party in the world. It promotes cultural nationalism, Hindu Nationalism, an aggressive foreign policy against Pakistan and a conservative social and fiscal policy.

After World War II, in Italy the conservative parties were mainly represented by the Christian Democracy (DC) party, which government form the foundation of the Republic until the party's dissolution in 1994. Officially, DC refused the ideology of conservatism, but in many aspects, for example family values, it was a typical social conservative party.

In 1994, the media tycoon and entrepreneur Silvio Berlusconi founded the liberal conservative party Forza Italia (FI). Berlusconi won three elections in 1994, 2001 and 2008, governing the country for almost ten years as Prime Minister. Forza Italia formed a coalition with right-wing regional party Lega Nord while in government.

Besides FI, now the conservative ideas are mainly expressed by the New Centre-Right party led by Angelino Alfano, Berlusconi formed a new party, which is a rebirth of Forza Italia, thuds founding a new conservative movement. Alfano served as Minister of Foreign Affairs. After the 2018 election, Lega Nord and the Five Star Movement formed the current right-wing populist government.

Under Vladimir Putin, the dominant leader since 1999, Russia has promoted explicitly conservative policies in social, cultural and political matters, both at home and abroad. Putin has attacked globalism and economic liberalism. Russian conservatism is unique in some respects as it supports Economic intervention with a mixed economy, with a strong nationalist sentiment and social conservatism with its views being largely populist. Russian conservatism as a result opposes libertarian ideals such as the aforementioned concept of economic liberalism found in other conservative movements around the world. Putin has as a result promoted new think tanks that bring together like-minded intellectuals and writers. For example, the Izborsky Club, founded in 2012 by Aleksandr Prokhanov, stresses Russian nationalism, the restoration of Russia's historical greatness and systematic opposition to liberal ideas and policies. Vladislav Surkov, a senior government official, has been one of the key ideologists during Putin's presidency.

In cultural and social affairs, Putin has collaborated closely with the Russian Orthodox Church. Mark Woods provides specific examples of how the Church under Patriarch Kirill of Moscow has backed the expansion of Russian power into Crimea and eastern Ukraine. More broadly, "The New York Times" reports in September 2016 how that Church's policy prescriptions support the Kremlin's appeal to social conservatives:

South Korea's major conservative party, the Liberty Korea Party, has changed its form throughout its history. First it was the Democratic-Republican Party (1963â1980) and its head was Park Chung-hee, who seized power in a 1961 military coup d'Ã©tat and ruled as an unelected military strongman until his formal election as president in 1963. He was president for 16 years until his assassination on 26 October 1979. The Democratic Justice Party inherited the same ideology as the Democratic-Republican Party. Its head, Chun Doo-hwan, also gained power through a coup and his followers called themselves the Hanahae. The Democratic Justice Party changed its form and acted to suppress the opposition party and to follow the people's demand for direct elections. The party's Roh Tae-woo became the first President who was elected through direct election. The next form of the major conservative party was the Democratic-Liberal Party and again through election its second leader, Kim Young-sam, became the fourteenth President of Korea. When the conservative party was beaten by the opposition party in the general election, it changed its form again to follow the party members' demand for reforms. It became the New Korean Party, but it changed again one year later since the President Kim Young-sam was blamed by the citizen for the International Monetary Fund. It changed its name to Grand National Party (GNP). Since the late Kim Dae-jung assumed the presidency in 1998, GNP had been the opposition party until Lee Myung-bak won the presidential election of 2007.

The meaning of "conservatism" in the United States has little in common with the way the word is used elsewhere. As Ribuffo (2011) notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism". Interestingly, in early American politics, it was the Democratic party practicing 'conservatism' in its attempts to maintain the social and economic institution of slavery. Democratic president Andrew Johnson, as one commonly known example, was considered a Conservative. âThe Democrats were often called conservative and embraced that label. Many of them were conservative in the sense that they wanted things to be like they were in the past, especially as far as race was concerned.â In 1892, Democrat Grover Cleveland won the election on a conservative platform, that argued for maintaining the gold standard, reducing tariffs, and supporting a laisse faire approach to government intervention. Since the 1950s, conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation, many Southern Democrats were conservatives and they played a key role in the conservative coalition that largely controlled domestic policy in Congress from 1937 to 1963. The conservative Democrats continued to have influence in the US politics until 1994's Republican Revolution, when the American South shifted from solid Democrat to solid Republican, while maintaining its conservative values.

Major priorities within American conservatism include support for the traditional family, law and order, the right to bear arms, Christian values, anti-communism and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments". Economic conservatives and libertarians favor small government, low taxes, limited regulation and free enterprise. Some social conservatives see traditional social values threatened by secularism, so they support school prayer and oppose abortion and homosexuality. Neoconservatives want to expand American ideals throughout the world and show a strong support for Israel. Paleoconservatives, in opposition to multiculturalism, press for restrictions on immigration. Most US conservatives prefer Republicans over Democrats and most factions favor a strong foreign policy and a strong military. The conservative movement of the 1950s attempted to bring together these divergent strands, stressing the need for unity to prevent the spread of "godless communism", which Reagan later labeled an "evil empire". During the Reagan administration, conservatives also supported the so-called "Reagan Doctrine" under which the US as part of a Cold War strategy provided military and other support to guerrilla insurgencies that were fighting governments identified as socialist or communist. The Reagan administration also adopted neoliberalism and trickle-down economics, as well as Reaganomics, which made for economic growth in the 1980s, fueled by trillion-dollar deficits.

Other modern conservative positions include opposition to big government and opposition to environmentalism. On average, American conservatives desire tougher foreign policies than liberals do. Economic liberalism, deregulation and social conservatism are major principles of the Republican Party.

The Tea Party movement, founded in 2009, has proven a large outlet for populist American conservative ideas. Their stated goals include rigorous adherence to the US constitution, lower taxes, and opposition to a growing role for the federal government in health care. Electorally, it was considered a key force in Republicans reclaiming control of the US House of Representatives in 2010.

This is a broad checklist of modern conservatism in seven countries based on the linked articles.
Following the Second World War, psychologists conducted research into the different motives and tendencies that account for ideological differences between left and right. The early studies focused on conservatives, beginning with Theodor W. Adorno's "The Authoritarian Personality" (1950) based on the F-scale personality test. This book has been heavily criticized on theoretical and methodological grounds, but some of its findings have been confirmed by further empirical research.

In 1973, British psychologist Glenn Wilson published an influential book providing evidence that a general factor underlying conservative beliefs is "fear of uncertainty." A meta-analysis of research literature by Jost, Glaser, Kruglanski, and Sulloway in 2003 found that many factors, such as intolerance of ambiguity and need for cognitive closure, contribute to the degree of one's political conservatism and its manifestations in decision-making. A study by Kathleen Maclay stated these traits "might be associated with such generally valued characteristics as personal commitment and unwavering loyalty". The research also suggested that while most people are resistant to change, liberals are more tolerant of it.

According to psychologist Bob Altemeyer, individuals who are politically conservative tend to rank high in right-wing authoritarianism (RWA) on his RWA scale. This finding was echoed by Adorno. A study done on Israeli and Palestinian students in Israel found that RWA scores of right-wing party supporters were significantly higher than those of left-wing party supporters. However, a 2005 study by H. Michael Crowson and colleagues suggested a moderate gap between RWA and other conservative positions, stating that their "results indicated that conservatism is not synonymous with RWA".

Psychologist Felicia Pratto and her colleagues have found evidence to support the idea that a high social dominance orientation (SDO) is strongly correlated with conservative political views and opposition to social engineering to promote equality, though Pratto's findings have been highly controversial as Pratto and her colleagues found that high SDO scores were highly correlated with measures of prejudice. However, David J. Schneider argued for a more complex relationships between the three factors, writing that "correlations between prejudice and political conservative are reduced virtually to zero when controls for SDO are instituted, suggesting that the conservatismâprejudice link is caused by SDO". Conservative political theorist Kenneth Minogue criticized Pratto's work, saying: "It is characteristic of the conservative temperament to value established identities, to praise habit and to respect prejudice, not because it is irrational, but because such things anchor the darting impulses of human beings in solidities of custom which we do not often begin to value until we are already losing them. Radicalism often generates youth movements, while conservatism is a condition found among the mature, who have discovered what it is in life they most value".

A 1996 study on the relationship between racism and conservatism found that the correlation was stronger among more educated individuals, though "anti-Black affect had essentially no relationship with political conservatism at any level of educational or intellectual sophistication". They also found that the correlation between racism and conservatism could be entirely accounted for by their mutual relationship with social dominance orientation.

In his 2008 book, "Gross National Happiness", Arthur C. Brooks presents the finding that conservatives are roughly twice as happy as liberals. A 2008 study demonstrates that conservatives tend to be happier than liberals because of their tendency to justify the current state of affairs and because they're less bothered by inequalities in society. In fact, as income inequality increases, this difference in relative happiness increases because conservatives, more so than liberals, possess an ideological buffer against the negative hedonic effects of economic inequality.

A 2009 study found that conservatism and cognitive ability are negatively correlated. It found that conservatism has a negative correlation with SAT, Vocabulary, and Analogy test scores, measures of education (such as gross enrollment in primary, secondary, and tertiary levels), and performance on math and reading assignments from the PISA. It also found that conservatism correlates with components of the Failed States Index and "several other measures of economic and political development of nations."





</doc>
<doc id="6677" url="https://en.wikipedia.org/wiki?curid=6677" title="Classical liberalism">
Classical liberalism

Classical liberalism is a political ideology and a branch of liberalism which advocates civil liberties under the rule of law with an emphasis on economic freedom. Closely related to economic liberalism, it developed in the early 19th century, building on ideas from the previous century as a response to urbanisation and to the Industrial Revolution in Europe and North America.

Notable liberal individuals whose ideas contributed to classical liberalism include John Locke, Jean-Baptiste Say, Thomas Robert Malthus and David Ricardo. It drew on classical economics, especially the economic ideas as espoused by Adam Smith in Book One of "The Wealth of Nations" and on a belief in natural law, progress and utilitarianism.

As a term, "classical liberalism" has often been applied in retrospect to distinguish earlier 19th-century liberalism from social liberalism.

Core beliefs of classical liberals included new ideasâwhich departed from both the older conservative idea of society as a family and from the later sociological concept of society as a complex set of social networks. Classical liberals believed that individuals are "egoistic, coldly calculating, essentially inert and atomistic" and that society is no more than the sum of its individual members.

Classical liberals agreed with Thomas Hobbes that government had been created by individuals to protect themselves from each other and that the purpose of government should be to minimize conflict between individuals that would otherwise arise in a state of nature. These beliefs were complemented by a belief that laborers could be best motivated by financial incentive. This belief led to the passage of the Poor Law Amendment Act 1834, which limited the provision of social assistance, based on the idea that markets are the mechanism that most efficiently leads to wealth. Adopting Thomas Robert Malthus's population theory, they saw poor urban conditions as inevitable, believed population growth would outstrip food production and thus regarded that consequence desirable because starvation would help limit population growth. They opposed any income or wealth redistribution, believing it would be dissipated by the lowest orders.

Drawing on ideas of Adam Smith, classical liberals believed that it is in the common interest that all individuals be able to secure their own economic self-interest. They were critical of what would come to be the idea of the welfare state as interfering in a free market. Despite Smith's resolute recognition of the importance and value of labor and of laborers, classical liberals selectively criticized labour's group rights being pursued at the expense of individual rights while accepting corporations' rights, which led to inequality of bargaining power. Classical liberals argued that individuals should be free to obtain work from the highest-paying employers while the profit motive would ensure that products that people desired were produced at prices they would pay. In a free market, both labor and capital would receive the greatest possible reward while production would be organized efficiently to meet consumer demand. Classical liberals argued for what they called a minimal state, limited to the following functions:

Classical liberals asserted that rights are of a negative nature and therefore stipulate that other individuals and governments are to refrain from interfering with the free market, opposing social liberals who assert that individuals have positive rights, such as the right to vote, the right to an education, the right to health care and the right to a living wage. For society to guarantee positive rights, it requires taxation over and above the minimum needed to enforce negative rights.

Core beliefs of classical liberals did not necessarily include democracy nor government by a majority vote by citizens because "there is nothing in the bare idea of majority rule to show that majorities will always respect the rights of property or maintain rule of law". For example, James Madison argued for a constitutional republic with protections for individual liberty over a pure democracy, reasoning that in a pure democracy a "common passion or interest will, in almost every case, be felt by a majority of the whole [...] and there is nothing to check the inducements to sacrifice the weaker party".

In the late 19th century, classical liberalism developed into neo-classical liberalism, which argued for government to be as small as possible to allow the exercise of individual freedom. In its most extreme form, neo-classical liberalism advocated social Darwinism. Right-libertarianism is a modern form of neo-classical liberalism.

Friedrich Hayek identified two different traditions within classical liberalism, namely the British tradition and the French tradition. Hayek saw the British philosophers Bernard Mandeville, David Hume, Adam Smith, Adam Ferguson, Josiah Tucker and William Paley as representative of a tradition that articulated beliefs in empiricism, the common law and in traditions and institutions which had spontaneously evolved but were imperfectly understood. The French tradition included Jean-Jacques Rousseau, Marquis de Condorcet, the Encyclopedists and the Physiocrats. This tradition believed in rationalism and sometimes showed hostility to tradition and religion. Hayek conceded that the national labels did not exactly correspond to those belonging to each tradition since he saw the Frenchmen Montesquieu, Benjamin Constant and Alexis de Tocqueville as belonging to the British tradition and the British Thomas Hobbes, Joseph Priestley, Richard Price and Thomas Paine as belonging to the French tradition. Hayek also rejected the label "laissez-faire" as originating from the French tradition and alien to the beliefs of Hume and Smith.

Guido De Ruggiero also identified differences between "Montesquieu and Rousseau, the English and the democratic types of liberalism" and argued that there was a "profound contrast between the two Liberal systems". He claimed that the spirit of "authentic English Liberalism" had "built up its work piece by piece without ever destroying what had once been built, but basing upon it every new departure". This liberalism had "insensibly adapted ancient institutions to modern needs" and "instinctively recoiled from all abstract proclamations of principles and rights". Ruggiero claimed that this liberalism was challenged by what he called the "new Liberalism of France" that was characterised by egalitarianism and a "rationalistic consciousness".

In 1848, Francis Lieber distinguished between what he called "Anglican and Gallican Liberty". Lieber asserted that "independence in the highest degree, compatible with safety and broad national guarantees of liberty, is the great aim of Anglican liberty, and self-reliance is the chief source from which it draws its strength". On the other hand, Gallican liberty "is sought in government [...]. [T]he French look for the highest degree of political civilization in organization, that is, in the highest degree of interference by public power".

Classical liberalism in Britain traces its roots to the Whigs and radicals, and was heavily influenced by French physiocracy. Whiggery had become a dominant ideology following the Glorious Revolution of 1688 and was associated with supporting the British Parliament, upholding the rule of law, and defending landed property. The origins of rights were seen as being in an ancient constitution, which had existed from time immemorial. These rights, which some Whigs considered to include freedom of the press and freedom of speech, were justified by custom rather than as natural rights. These Whigs believed that the power of the executive had to be constrained. While they supported limited suffrage, they saw voting as a privilege rather than as a right. However, there was no consistency in Whig ideology and diverse writers including John Locke, David Hume, Adam Smith and Edmund Burke were all influential among Whigs, although none of them were universally accepted.

From the 1790s to the 1820s, British radicals concentrated on parliamentary and electoral reform, emphasising natural rights and popular sovereignty. Richard Price and Joseph Priestley adapted the language of Locke to the ideology of radicalism. The radicals saw parliamentary reform as a first step toward dealing with their many grievances, including the treatment of Protestant Dissenters, the slave trade, high prices, and high taxes.

There was greater unity among classical liberals than there had been among Whigs. Classical liberals were committed to individualism, liberty, and equal rights. They believed these goals required a free economy with minimal government interference. Some elements of Whiggery were uncomfortable with the commercial nature of classical liberalism. These elements became associated with conservatism.
Classical liberalism was the dominant political theory in Britain from the early 19th century until the First World War. Its notable victories were the Catholic Emancipation Act of 1829, the Reform Act of 1832 and the repeal of the Corn Laws in 1846. The Anti-Corn Law League brought together a coalition of liberal and radical groups in support of free trade under the leadership of Richard Cobden and John Bright, who opposed aristocratic privilege, militarism, and public expenditure and believed that the backbone of Great Britain was the yeoman farmer. Their policies of low public expenditure and low taxation were adopted by William Ewart Gladstone when he became Chancellor of the Exchequer and later Prime Minister. Classical liberalism was often associated with religious dissent and nonconformism.

Although classical liberals aspired to a minimum of state activity, they accepted the principle of government intervention in the economy from the early 19th century on, with passage of the Factory Acts. From around 1840 to 1860, "laissez-faire" advocates of the Manchester School and writers in "The Economist" were confident that their early victories would lead to a period of expanding economic and personal liberty and world peace, but would face reversals as government intervention and activity continued to expand from the 1850s. Jeremy Bentham and James Mill, although advocates of "laissez-faire", non-intervention in foreign affairs, and individual liberty, believed that social institutions could be rationally redesigned through the principles of utilitarianism. The Conservative Prime Minister Benjamin Disraeli rejected classical liberalism altogether and advocated Tory democracy. By the 1870s, Herbert Spencer and other classical liberals concluded that historical development was turning against them. By the First World War, the Liberal Party had largely abandoned classical liberal principles.

The changing economic and social conditions of the 19th century led to a division between neo-classical and social (or welfare) liberals, who while agreeing on the importance of individual liberty differed on the role of the state. Neo-classical liberals, who called themselves "true liberals", saw Locke's "Second Treatise" as the best guide and emphasised "limited government" while social liberals supported government regulation and the welfare state. Herbert Spencer in Britain and William Graham Sumner were the leading neo-classical liberal theorists of the 19th century. Neo-classical liberalism has continued into the contemporary era, with writers such as John Rawls. The evolution from classical to social/welfare liberalism is for example reflected in Britain in the evolution of the thought of John Maynard Keynes.

In the United States, liberalism took a strong root because it had little opposition to its ideals, whereas in Europe liberalism was opposed by many reactionary or feudal interests such as the nobility, the aristocracy, the landed gentry, the established church and the aristocratic army officers.

Thomas Jefferson adopted many of the ideals of liberalism, but in the Declaration of Independence changed Locke's "life, liberty and property" to the more socially liberal "Life, Liberty and the pursuit of Happiness". As the United States grew, industry became a larger and larger part of American life; and during the term of its first populist President, Andrew Jackson, economic questions came to the forefront. The economic ideas of the Jacksonian era were almost universally the ideas of classical liberalism. Freedom, according to classical liberals, was maximised when the government took a "hands off" attitude toward the economy.

Historian Kathleen G. Donohue argues:

[A]t the center of classical liberal theory [in Europe] was the idea of "laissez-faire". To the vast majority of American classical liberals, however, "laissez-faire" did not mean no government intervention at all. On the contrary, they were more than willing to see government provide tariffs, railroad subsidies, and internal improvements, all of which benefited producers. What they condemned was intervention on behalf of consumers.

Leading magazine "The Nation" espoused liberalism every week starting in 1865 under the influential editor Edwin Lawrence Godkin (1831â1902).

The ideas of classical liberalism remained essentially unchallenged until a series of depressions, thought to be impossible according to the tenets of classical economics, led to economic hardship from which the voters demanded relief. In the words of William Jennings Bryan, "You shall not crucify the American farmer on a cross of gold". Classical liberalism remained the orthodox belief among American businessmen until the Great Depression.

The Great Depression of the 1930s saw a sea change in liberalism, with priority shifting from the producers to consumers. Franklin D. Roosevelt's New Deal represented the dominance of modern liberalism in politics for decades. In the words of Arthur Schlesinger Jr.: 

Alan Wolfe summarizes the viewpoint that there is a continuous liberal understanding that includes both Adam Smith and John Maynard Keynes: 

The view that modern liberalism is a continuation of classical liberalism is not universally shared. James Kurth, Robert E. Lerner, John Micklethwait, Adrian Wooldridge and several other political scholars have argued that classical liberalism still exists today, but in the form of American conservatism. According to Deepak Lal, only in the United States does classical liberalism continue to be a significant political forceâthrough American conservatism.

Central to classical liberal ideology was their interpretation of John Locke's "Second Treatise of Government" and "A Letter Concerning Toleration", which had been written as a defence of the Glorious Revolution of 1688. Although these writings were considered too radical at the time for Britain's new rulers, they later came to be cited by Whigs, radicals and supporters of the American Revolution. However, much of later liberal thought was absent in Locke's writings or scarcely mentioned and his writings have been subject to various interpretations. For example, there is little mention of constitutionalism, the separation of powers and limited government.

James L. Richardson identified five central themes in Locke's writing: individualism, consent, the concepts of the rule of law and government as trustee, the significance of property and religious toleration. Although Locke did not develop a theory of natural rights, he envisioned individuals in the state of nature as being free and equal. The individual, rather than the community or institutions, was the point of reference. Locke believed that individuals had given consent to government and therefore authority derived from the people rather than from above. This belief would influence later revolutionary movements.

As a trustee, government was expected to serve the interests of the people, not the rulers; and rulers were expected to follow the laws enacted by legislatures. Locke also held that the main purpose of men uniting into commonwealths and governments was for the preservation of their property. Despite the ambiguity of Locke's definition of property, which limited property to "as much land as a man tills, plants, improves, cultivates, and can use the product of", this principle held great appeal to individuals possessed of great wealth.

Locke held that the individual had the right to follow his own religious beliefs and that the state should not impose a religion against Dissenters, but there were limitations. No tolerance should be shown for atheists, who were seen as amoral, or to Catholics, who were seen as owing allegiance to the Pope over their own national government.

Adam Smith's "The Wealth of Nations", published in 1776, was to provide most of the ideas of economics, at least until the publication of John Stuart Mill's "Principles of Political Economy" in 1848. Smith addressed the motivation for economic activity, the causes of prices and the distribution of wealth and the policies the state should follow to maximise wealth.

Smith wrote that as long as supply, demand, prices and competition were left free of government regulation, the pursuit of material self-interest, rather than altruism, would maximise the wealth of a society through profit-driven production of goods and services. An "invisible hand" directed individuals and firms to work toward the public good as an unintended consequence of efforts to maximise their own gain. This provided a moral justification for the accumulation of wealth, which had previously been viewed by some as sinful.

He assumed that workers could be paid wages as low as was necessary for their survival, which was later transformed by David Ricardo and Thomas Robert Malthus into the "iron law of wages". His main emphasis was on the benefit of free internal and international trade, which he thought could increase wealth through specialisation in production. He also opposed restrictive trade preferences, state grants of monopolies and employers' organisations and trade unions. Government should be limited to defence, public works and the administration of justice, financed by taxes based on income.

Smith's economics was carried into practice in the nineteenth century with the lowering of tariffs in the 1820s, the repeal of the Poor Relief Act that had restricted the mobility of labour in 1834 and the end of the rule of the East India Company over India in 1858.

In addition to Smith's legacy, Say's law, Thomas Robert Malthus' theories of population and David Ricardo's iron law of wages became central doctrines of classical economics. The pessimistic nature of these theories provided a basis for criticism of capitalism by its opponents and helped perpetuate the tradition of calling economics the "dismal science".

Jean-Baptiste Say was a French economist who introduced Smith's economic theories into France and whose commentaries on Smith were read in both France and Britain. Say challenged Smith's labour theory of value, believing that prices were determined by utility and also emphasised the critical role of the entrepreneur in the economy. However, neither of those observations became accepted by British economists at the time. His most important contribution to economic thinking was Say's law, which was interpreted by classical economists that there could be no overproduction in a market and that there would always be a balance between supply and demand. This general belief influenced government policies until the 1930s. Following this law, since the economic cycle was seen as self-correcting, government did not intervene during periods of economic hardship because it was seen as futile.

Malthus wrote two books, "An Essay on the Principle of Population" (published in 1798) and "Principles of Political Economy" (published in 1820). The second book which was a rebuttal of Say's law had little influence on contemporary economists. However, his first book became a major influence on classical liberalism. In that book, Malthus claimed that population growth would outstrip food production because population grew geometrically while food production grew arithmetically. As people were provided with food, they would reproduce until their growth outstripped the food supply. Nature would then provide a check to growth in the forms of vice and misery. No gains in income could prevent this and any welfare for the poor would be self-defeating. The poor were in fact responsible for their own problems which could have been avoided through self-restraint.

Ricardo, who was an admirer of Smith, covered many of the same topics, but while Smith drew conclusions from broadly empirical observations he used deduction, drawing conclusions by reasoning from basic assumptions While Ricardo accepted Smith's labour theory of value, he acknowledged that utility could influence the price of some rare items. Rents on agricultural land were seen as the production that was surplus to the subsistence required by the tenants. Wages were seen as the amount required for workers' subsistence and to maintain current population levels. According to his iron law of wages, wages could never rise beyond subsistence levels. Ricardo explained profits as a return on capital, which itself was the product of labour, but a conclusion many drew from his theory was that profit was a surplus appropriated by capitalists to which they were not entitled.

Utilitarianism provided the political justification for implementation of economic liberalism by British governments, which was to dominate economic policy from the 1830s. Although utilitarianism prompted legislative and administrative reform and John Stuart Mill's later writings on the subject foreshadowed the welfare state, it was mainly used as a justification for "laissez-faire".

The central concept of utilitarianism, which was developed by Jeremy Bentham, was that public policy should seek to provide "the greatest happiness of the greatest number". While this could be interpreted as a justification for state action to reduce poverty, it was used by classical liberals to justify inaction with the argument that the net benefit to all individuals would be higher.

Classical liberals saw utility as the foundation for public policies. This broke both with conservative "tradition" and Lockean "natural rights", which were seen as irrational. Utility, which emphasises the happiness of individuals, became the central ethical value of all liberalism. Although utilitarianism inspired wide-ranging reforms, it became primarily a justification for "laissez-faire" economics. However, classical liberals rejected Smith's belief that the "invisible hand" would lead to general benefits and embraced Malthus' view that population expansion would prevent any general benefit and Ricardo's view of the inevitability of class conflict. "Laissez-faire" was seen as the only possible economic approach and any government intervention was seen as useless and harmful. The Poor Law Amendment Act 1834 was defended on "scientific or economic principles" while the authors of the Elizabethan Poor Law of 1601 were seen as not having had the benefit of reading Malthus.

However, commitment to "laissez-faire" was not uniform and some economists advocated state support of public works and education. Classical liberals were also divided on free trade as Ricardo expressed doubt that the removal of grain tariffs advocated by Richard Cobden and the Anti-Corn Law League would have any general benefits. Most classical liberals also supported legislation to regulate the number of hours that children were allowed to work and usually did not oppose factory reform legislation.

Despite the pragmatism of classical economists, their views were expressed in dogmatic terms by such popular writers as Jane Marcet and Harriet Martineau. The strongest defender of "laissez-faire" was "The Economist" founded by James Wilson in 1843. "The Economist" criticised Ricardo for his lack of support for free trade and expressed hostility to welfare, believing that the lower orders were responsible for their economic circumstances. "The Economist" took the position that regulation of factory hours was harmful to workers and also strongly opposed state support for education, health, the provision of water and granting of patents and copyrights.

"The Economist" also campaigned against the Corn Laws that protected landlords in the United Kingdom of Great Britain and Ireland against competition from less expensive foreign imports of cereal products. A rigid belief in "laissez-faire" guided the government response in 1846â1849 to the Great Famine in Ireland, during which an estimated 1.5 million people died. The minister responsible for economic and financial affairs, Charles Wood, expected that private enterprise and free trade, rather than government intervention, would alleviate the famine. The Corn Laws were finally repealed in 1846 by the removal of tariffs on grain which kept the price of bread artificially high, but it came too late to stop the Irish famine, partly because it was done in stages over three years.

Several liberals, including Smith and Cobden, argued that the free exchange of goods between nations could lead to world peace. Erik Gartzke states: "Scholars like Montesquieu, Adam Smith, Richard Cobden, Norman Angell, and Richard Rosecrance have long speculated that free markets have the potential to free states from the looming prospect of recurrent warfare". American political scientists John R. Oneal and Bruce M. Russett, well known for their work on the democratic peace theory, state: 

In "The Wealth of Nations", Smith argued that as societies progressed from hunter gatherers to industrial societies the spoils of war would rise, but that the costs of war would rise further and thus making war difficult and costly for industrialised nations: 
Cobden believed that military expenditures worsened the welfare of the state and benefited a small, but concentrated elite minority, summing up British imperialism, which he believed was the result of the economic restrictions of mercantilist policies. To Cobden and many classical liberals, those who advocated peace must also advocate free markets. The belief that free trade would promote peace was widely shared by English liberals of the 19th and early 20th century, leading the economist John Maynard Keynes (1883â1946), who was a classical liberal in his early life, to say that this was a doctrine on which he was "brought up" and which he held unquestioned only until the 1920s. In his review of a book on Keynes, Michael S. Lawlor argues that it may be in large part due to Keynes' contributions in economics and politics, as in the implementation of the Marshall Plan and the way economies have been managed since his work, "that we have the luxury of not facing his unpalatable choice between free trade and full employment". A related manifestation of this idea was the argument of Norman Angell (1872â1967), most famously before World War I in "The Great Illusion" (1909), that the interdependence of the economies of the major powers was now so great that war between them was futile and irrational; and therefore unlikely.



</doc>
<doc id="6678" url="https://en.wikipedia.org/wiki?curid=6678" title="Cat">
Cat

The cat ("Felis catus") is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae and is often referred to as the domestic cat to distinguish it from the wild members of the family. A cat can either be a house cat, a farm cat or a feral cat; the latter ranges freely and avoids human contact. Domestic cats are valued by humans for companionship and their ability to hunt rodents. About 60 cat breeds are recognized by various cat registries.

The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.

Female domestic cats can have kittens from spring to late autumn, with litter sizes often ranging from two to five kittens. Domestic cats are bred and shown at events as registered pedigreed cats, a hobby known as cat fancy. Failure to control breeding of pet cats by spaying and neutering, as well as abandonment of pets, resulted in large numbers of feral cats worldwide, contributing to the extinction of entire bird, mammal, and reptile species, and evoking population control.

Cats were first domesticated in the Near East around 7500 BCE. It was long thought that cat domestication was initiated in ancient Egypt, as since around 3100 BCE veneration was given to cats in ancient Egypt. , the domestic cat was the second-most popular pet in the United States, with 95 million cats owned. In the United Kingdom, around 7.3 million cats lived in more than 4.8 million households .

The origin of the English word 'cat', Old English , is thought to be the Late Latin word , which was first used at the beginning of the 6th century. It was suggested that the word 'cattus' is derived from an Egyptian precursor of Coptic , "tomcat", or its feminine form suffixed with .
The Late Latin word is also thought to be derived from Afro-Asiatic languages. The Nubian word "wildcat" and Nobiin are possible sources or cognates. The Nubian word may be a loan from Arabic ~ . It is "equally likely that the forms might derive from an ancient Germanic word, imported into Latin and thence to Greek and to Syriac and Arabic". The word may be derived from Germanic and Northern European languages, and ultimately be borrowed from Uralic, cf. Northern Sami , "female stoat", and Hungarian , "stoat"; from Proto-Uralic "*kÃ¤ÄwÃ¤", "female (of a furred animal)".

The English "puss", extended as "pussy" and "pussycat", is attested from the 16th century and may have been introduced from Dutch or from Low German , related to Swedish , or Norwegian , . Similar forms exist in Lithuanian and Irish or . The etymology of this word is unknown, but it may have simply arisen from a sound used to attract a cat.

A male cat is called a "tom" or "tomcat" (or a "gib", if neutered) An unspayed female is called a "queen", especially in a cat-breeding context. A juvenile cat is referred to as a "kitten". In Early Modern English, the word "kitten" was interchangeable with the now-obsolete word "catling". A group of cats can be referred to as a "clowder" or a "glaring".

The scientific name "Felis catus" was proposed by Carl Linnaeus in 1758 for a domestic cat.
"Felis catus domesticus" was proposed by Johann Christian Polycarp Erxleben in 1777.
"Felis daemon" proposed by Konstantin Alekseevich Satunin in 1904 was a black cat from the Transcaucasus, later identified as a domestic cat.

In 2003, the International Commission on Zoological Nomenclature ruled that the domestic cat is a distinct species, namely "Felis catus".
In 2007, it was considered a subspecies of the European wildcat, "F. silvestris catus", following results of phylogenetic research. In 2017, the IUCN Cat Classification Taskforce followed the recommendation of the ICZN in regarding the domestic cat as a distinct species, "Felis catus".

The domestic cat is a member of the Felidae, a family that had a common ancestor about 10â15Â million years ago.
The genus "Felis" diverged from the Felidae around 6â7Â million years ago.
Results of phylogenetic research confirm that the wild "Felis" species evolved through sympatric or parapatric speciation, whereas the domestic cat evolved through artificial selection. The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38Â chromosomes and roughly 20,000Â genes.
The leopard cat ("Prionailurus bengalensis") was tamed independently in China around 5500Â BCE. This line of partially domesticated cats leaves no trace in the domestic cat populations of today.

The earliest known indication for the taming of an African wildcat ("F.Â lybica") was excavated close by a human Neolithic grave in Shillourokambos, southern Cyprus, dating to about 9,200 to 9,500Â years before present. Since there is no evidence of native mammalian fauna on Cyprus, the inhabitants of this Neolithic village most likely brought the cat and other wild mammals to the island from the Middle Eastern mainland. Scientists therefore assume that African wildcats were attracted to early human settlements in the Fertile Crescent by rodents, in particular the house mouse ("Mus musculus"), and were tamed by Neolithic farmers. This commensal relationship between early farmers and tamed cats lasted thousands of years. As agricultural practices spread, so did tame and domesticated cats. Wildcats of Egypt contributed to the maternal gene pool of the domestic cat at a later time.
The earliest known evidence for the occurrence of the domestic cat in Greece dates to around 1200Â BCE. Greek, Phoenician, Carthaginian and Etruscan traders introduced domestic cats to southern Europe.
During the Roman Empire they were introduced to Corsica and Sardinia before the beginning of the 1st millennium.
By the 5thÂ centuryÂ BCE, they were familiar animals around settlements in Magna Graecia and Etruria.
By the end of the Roman Empire in the 5thÂ century, the Egyptian domestic cat lineage had arrived in a Baltic Sea port in northern Germany.

During domestication, cats have undergone only minor changes in anatomy and behavior, and they are still capable of surviving in the wild. Several natural behaviors and characteristics of wildcats may have pre-adapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play and relatively high intelligence. Captive "Leopardus" cats may also display affectionate behavior toward humans, but were not domesticated. House cats often mate with feral cats, producing hybrids such as the Kellas cat in Scotland. Hybridisation between domestic and other Felinae species is also possible.

Development of cat breeds started in the mid 19thÂ century.
An analysis of the domestic cat genome revealed that the ancestral wildcat genome was significantly altered in the process of domestication, as specific mutations were selected to develop cat breeds. Most breeds are founded on random-bred domestic cats. Genetic diversity of these breeds varies between regions, and is lowest in purebred populations, which show more than 20Â deleterious genetic disorders.

The domestic cat has a smaller skull and shorter bones than the European wildcat.
It averages about in head-to-body length and in height, with about long tails. Males are larger than females.
Adult domestic cats typically weigh between .

Cats have seven cervical vertebrae (as do most mammals); 13Â thoracic vertebrae (humans haveÂ 12); seven lumbar vertebrae (humans have five); three sacral vertebrae (as do most mammals, but humans have five); and a variable number of caudal vertebrae in the tail (humans have only vestigial caudal vertebrae, fused into an internal coccyx). The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13Â ribs, the shoulder, and the pelvis. Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their head.

The cat skull is unusual among mammals in having very large eye sockets and a powerful specialized jaw. Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death. Compared to other felines, domestic cats have narrowly spaced canine teeth relative to the size of their jaw, which is an adaptation to their preferred prey of small rodents, which have small vertebrae. The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively, and cats are largely incapable of mastication. Although cats tend to have better teeth than most humans, with decay generally less likely because of a thicker protective layer of enamel, a less damaging saliva, less retention of food particles between teeth, and a diet mostly devoid of sugar, they are nonetheless subject to occasional tooth loss and infection.

The cat is digitigrade. It walks on the toes, with the bones of the feet making up the lower part of the visible leg. Unlike most mammals, it uses a "pacing" gait and moves both legs on one side of the body before the legs on the other side. It registers directly by placing each hind paw close to the track of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for hind paws when navigating rough terrain. As it speeds up walking to trotting, its gait changes to a "diagonal" gait: The diagonally opposite hind and fore legs move simultaneously.

Cats have protractable and retractable claws. In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows the silent stalking of prey. The claws on the fore feet are typically sharper than those on the hind feet. Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Cats shed the outside layer of their claw sheaths when scratching rough surfaces.

Most cats have five claws on their front paws, and four on their rear paws. The dewclaw is proximal to the other claws. More proximally is a protrusion which appears to be a sixth "finger". This special feature of the front paws, on the inside of the wrists has no function in normal walking, but is thought to be an antiskidding device used while jumping. Some cat breeds are prone to having extra digits (âpolydactylyâ). Polydactylous cats occur along North America's northeast coast and in Great Britain.

Cats have excellent night vision and can see at only one-sixth the light level required for human vision. This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light. Large pupils are an adaptation to dim light. The domestic cat has slit pupils, which allow it to focus bright light without chromatic aberration. At low light, a cat's pupils expand to cover most of the exposed surface of its eyes. However, the domestic cat has rather poor color vision and only two types of cone cells, optimized for sensitivity to blue and yellowish green; its ability to distinguish between red and green is limited. A response to middle wavelengths from a system other than the rod cells might be due to a third type of cone. However, this appears to be an adaptation to low light levels rather than representing true trichromatic vision.

The domestic cat's hearing is most acute in the range of 500Â Hz to 32Â kHz. It can detect an extremely broad range of frequencies ranging from 55Â Hz to 79,000Â Hz. It can hear a range of 10.5Â octaves, while humans and dogs can hear ranges of about 9Â octaves.
Its hearing sensitivity is enhanced by its large movable outer ears, the pinnae, which amplify sounds and help detect the location of a noise. It can detect ultrasound, which enables it to detect ultrasonic calls made by rodent prey.

Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about in area, which is about twice that of humans. Cats and many other animals have a Jacobson's organ in their mouths that is used in the behavioral process of flehmening. It allows them to sense certain aromas in a way that humans cannot. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol, which they use to communicate through urine spraying and marking with scent glands. Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion. About 70â80% of cats are affected by nepetalactone. This response is also produced by other plants, such as silver vine ("Actinidia polygama") and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.

Cats have relatively few taste buds compared to humans (470 or so versus more than 9,000 on the human tongue). Domestic and wild cats share a taste receptor gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness. Their taste buds instead respond to acids, amino acids like protein, and bitter tastes. Cats also have a distinct temperature preference for their food, preferring food with a temperature around which is similar to that of a fresh kill and routinely rejecting food presented cold or refrigerated (which would signal to the cat that the "prey" item is long dead and therefore possibly toxic or decomposing).

To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their body, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.

Most breeds of cat have a noted fondness for sitting in high places, or "perching". A higher place may serve as a concealed site from which to hunt; domestic cats strike prey by pouncing from a perch such as a tree branch. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. A cat falling from heights of up to 3Â meters can right itself and land on its paws.
During a fall from a high place, a cat reflexively twists its body and rights itself to land on its feet using its acute sense of balance and flexibility. This reflex is known as the cat righting reflex.
A cat always rights itself in the same way during a fall, if it has enough time to do so, which is the case in falls of or more. How cats are able to right themselves when falling has been investigated as the "falling cat problem".

Outdoor cats are active both day and night, although they tend to be slightly more active at night. Domestic cats spend the majority of their time in the vicinity of their homes, but can range many hundreds of meters from this central point. They establish territories that vary considerably in size, in one study ranging from . The timing of cats' activity is quite flexible and varied, which means house cats may be more active in the morning and evening, as a response to greater human activity at these times.

Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually between 12 and 16 hours, with 13 and 14 being the average. Some cats can sleep as much as 20 hours. The term "cat nap" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.

The social behavior of the domestic cat ranges from widely dispersed individuals to feral cat colonies that gather around a food source, based on groups of co-operating females. Within such groups, one cat is usually dominant over the others. Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation. Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling and, if that does not work, by short but noisy and violent attacks. Despite some cats cohabiting in colonies, they do not have a social survival strategy, or a pack mentality and always hunt alone.

However, some pet cats are poorly socialized. In particular, older cats show aggressiveness towards newly arrived kittens, which include biting and scratching; this type of behavior is known as feline asocial aggression.

Life in proximity to humans and other domestic animals has led to a symbiotic social adaptation in cats, and cats may express great affection toward humans or other animals. Ethologically, the human keeper of a cat functions as a sort of surrogate for the cat's mother. Adult cats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. Their high-pitched sounds may mimic the cries of a hungry human infant, making them particularly difficult for humans to ignore.

Domestic cats' scent rubbing behavior towards humans or other cats is thought to be a feline means for social bonding.

Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing. Their body language, including position of ears and tail, relaxation of the whole body, and kneading of the paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats. A raised tail indicates a friendly greeting, and flattened ears indicates hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate ones. Feral cats are generally silent. Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.

Purring may have developed as an evolutionary advantage as a signalling mechanism of reassurance between mother cats and nursing kittens. Post-nursing cats often purr as a sign of contentment: when being petted, becoming relaxed, or eating. The mechanism by which cats purr is elusive. The cat has no unique anatomical feature that is clearly responsible for the sound.

Cats are known for spending considerable amounts of time licking their coats to keep them clean. The cat's tongue has backwards-facing spines about 500Â Î¼m long, which are called papillae. These contain keratin which makes them rigid so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush.

Among domestic cats, males are more likely to fight than females. Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male. Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home. Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.

When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting. Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. They may also vocalize loudly and bare their teeth in an effort to further intimidate their opponent. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.

Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. However, fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus. Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and nose.

The shape and structure of cats' cheeks is insufficient to suck. They lap with the tongue to draw liquid upwards into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it like a corkscrew, drawing water upwards.

Feral cats and free-fed house cats consume several small meals in a day. The frequency and size of meals varies between individuals. They select food based on its temperature, smell and texture; they dislike chilled foods and respond most strongly to moist foods rich in amino acids, which are similar to meat. Cats reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past. They also avoid sweet food and milk. Most adult cats are lactose intolerant; the sugar in milk is not easily digested and may cause soft stools or diarrhea. Some also develop odd eating habits and like to eat or chew on things like wool, plastic, cables, paper, string, aluminum foil, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten.

Cats hunt small prey, primarily birds and rodents, and are often used as a form of pest control. Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured. The strategy used depends on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds. Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.4 to 3.7Â billion birds and 6.9 to 20.7Â billion mammals annually.
Certain species appear more susceptible than others; for example, 30% of house sparrow mortality is linked to the domestic cat. In the recovery of ringed robins ("Erithacus rubecula") and dunnocks ("Prunella modularis"), 31% of deaths were a result of cat predation. In parts of North America, the presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety.
Perhaps the best-known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to "play" with prey by releasing it after capture. This cat and mouse behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat.
Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. One explanation is that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at, or near, the top. Another explanation is that they attempt to teach their guardians to hunt or to help their human as if feeding "an elderly cat, or an inept kitten". This hypothesis is inconsistent with the fact that male cats also bring home prey, despite males having negligible involvement in raising kittens.

On islands, birds can contribute as much as 60% of a cat's diet. In nearly all cases, however, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a "mesopredator release" effect; where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are, however, known to be a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham rail, and the New Zealand merganser are a few from a long list, with the most extreme case being the flightless Lyall's wren, which was driven to extinction only a few years after its discovery.
One feral cat in New Zealand killed 102 New Zealand lesser short-tailed bats in seven days. In the US, feral and free-ranging domestic cats kill an estimated 6.3 â 22.3 billion mammals annually.

In Australia, the impact of cats on mammal populations is even greater than the impact of habitat loss. More than one million reptiles are killed by feral cats each day, representing 258 species. Cats have contributed to the extinction of the Navassa curly-tailed lizard and "Chioninia coctei."

Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey. Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.

Cats also tend to play with toys more when they are hungry. Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest. They become habituated to a toy they have played with before. String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death. Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase.

Female cats called queens are polyestrous with several estrus cycles during a year, lasting usually 21 days. They are usually ready to mate between early February and August.

Several males, called tomcats, are attracted to a female in heat. They fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually, the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120â150 backward-pointing penile spines, which are about long; upon withdrawal of the penis, the spines rake the walls of the female's vagina, which acts to induce ovulation. This act also occurs to clear the vagina of other sperm in the context of a second (or more) mating, thus giving the later males a larger chance of conception.

After mating, the female cleans her vulva thoroughly. If a male attempts to mate with her at this point, the female attacks him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.
Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate. Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.

The morula forms 124 hours after conception. At 148 hours, early blastocysts form. At 10â12 days, implantation occurs.
The gestation of queens lasts between 64 and 67 days, with an average of 65 days.
Data on the reproductive capacity of more than 2,300 free-ranging queens were collected during a study between May 1998 and October 2000. They had one to six kittens per litter, with an average of three kittens. They produced a mean of 1.4 litters per year, but a maximum of three litters in a year. Of 169 kittens, 127 died before they were six months old due to a trauma caused in most cases by dog attacks and road accidents.
The first litter is usually smaller than subsequent litters. Kittens are weaned between six and seven weeks of age. Queens normally reach sexual maturity at 5â10 months, and males at 5â7 months. This varies depending on breed. Kittens reach puberty at the age of 9â10 months.

Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother. They can be surgically sterilized (spayed or castrated) as early as seven weeks to limit unwanted reproduction. This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed before puberty, at about three to six months. In the United States, about 80% of household cats are neutered.

The average lifespan of pet cats has risen in recent decades. In the early 1980s, it was about seven years, rising to 9.4 years in 1995 and 15.1 years in 2018. Some cats have been reported as surviving into their 30s, with the oldest known cat, Creme Puff, dying at a verified age of 38.

Spaying or neutering increases life expectancy: one study found neutered male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females. Having a cat neutered confers health benefits, because castrated males cannot develop testicular cancer, spayed females cannot develop uterine or ovarian cancer, and both have a reduced risk of mammary cancer.

Despite widespread concern about the welfare of free-roaming cats, the lifespans of neutered feral cats in managed colonies compare favorably with those of pet cats.

About two hundred fifty heritable genetic disorders have been identified in cats, many similar to human inborn errors of metabolism. The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.
Diseases affecting domestic cats include acute infections, parasitic infestations, injuries, and chronic diseases such as kidney disease, thyroid disease, and arthritis. Vaccinations are available for many infectious diseases, as are treatments to eliminate parasites such as worms and fleas.

The domestic cat is a cosmopolitan species and occurs across much of the world. It is adaptable and now present on all continents except Antarctica, and on 118 of the 131 main groups of islandsâeven on isolated islands such as the Kerguelen Islands.
Due to its ability to thrive in almost any terrestrial habitat, it is among the world's most invasive species.
As it is little altered from the wildcat, it can readily interbreed with the wildcat. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula. It lives on small islands with no human inhabitants.
Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands.

Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas. The numbers of feral cats is not known, but estimates of the United States feral population range from twenty-five to sixty million. Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food. Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.

Public attitudes towards feral cats vary widely, ranging from seeing them as free-ranging pets, to regarding them as vermin. One common approach to reducing the feral cat population is termed "trap-neuter-return", where the cats are trapped, neutered, immunized against diseases such as rabies and the feline panleukopenia and leukemia viruses, and then released. Before releasing them back into their feral colonies, the attending veterinarian often nips the tip off one ear to mark it as neutered and inoculated, since these cats may be trapped again. Volunteers continue to feed and give care to these cats throughout their lives. Given this support, their lifespans are increased, and behavior and nuisance problems caused by competition for food are reduced.

Some feral cats can be successfully socialised and 're-tamed' for adoption; young cats, especially kittens and cats that have had prior experience and contact with humans are the most receptive to these efforts.

Cats are common pets throughout the world, and their worldwide population exceeds 500Â million as of 2007. Cats have been used for millennia to control rodents, notably around grain stores and aboard ships, and both uses extend to the present day.

As well as being kept as pets, cats are also used in the international fur and leather industries for making coats, hats, blankets, and stuffed toys; and shoes, gloves, and musical instruments respectively (about 24 cats are needed to make a cat-fur coat). This use has been outlawed in the United States, Australia, and the European Union in 2007. Cat pelts have been used for superstitious purposes as part of the practise of witchcraft, and are still made into blankets in Switzerland as folk remedies believed to help rheumatism. In the Western intellectual tradition, the idea of cats as everyday objects have served to illustrate problems of quantum mechanics in the SchrÃ¶dinger's cat thought experiment.

A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as the Canadian Federation of Humane Societies's one) and over the Internet, but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600Â million.
Walter Chandoha made his career photographing cats after his 1949 images of "Loco", an especially charming stray taken in, were published around the world. He is reported to have photographed 90,000 cats during his career and maintained an archive of 225,000 images that he drew from for publications during his lifetime.

A cat show is a judged event in which the owners of cats compete to win titles in various cat-registering organizations by entering their cats to be judged after a breed standard. Both pedigreed and non-purebred companion ("moggy") cats are admissible, although the rules differ from organization to organization. Competing cats are compared to the applicable breed standard, and assessed for temperament and apparent health; the owners of those judged to be most ideal awarded a prize. Moggies are judged based on their temperament and healthy appearance. Some events also include activity judging, such as trained navigation of obstacle course. Often, at the end of the year, all of the points accrued at various shows are added up and more national and regional titles are awarded to champion cats.

Cats can be infected or infested with viruses, bacteria, fungus, protozoans, arthropods or worms that can transmit diseases to humans. In some cases, the cat exhibits no symptoms of the disease, However, the same disease can then become evident in a human. The likelihood that a person will become diseased depends on the age and immune status of the person. Humans who have cats living in their home or in close association are more likely to become infected, however, those who do not keep cats as pets might also acquire infections from cat feces and parasites exiting the cat's body. Some of the infections of most concern include salmonella, cat-scratch disease and toxoplasmosis.

In ancient Egypt, cats were worshipped, and the goddess Bastet often depicted in cat form, sometimes taking on the war-like aspect of a lioness. The Greek historian Herodotus reported that killing a cat was forbidden, and when a household cat died, the entire family mourned and shaved their eyebrows. Families took their dead cats to the sacred city of Bubastis, where they were embalmed and buried in sacred repositories. Herodotus expressed astonishment at the domestic cats in Egypt, because he had only ever seen wildcats.
Ancient Greeks and Romans kept weasels as pets, which were seen as the ideal rodent-killers. The earliest unmistakable evidence of the Greeks having domestic cats comes from two coins from Magna Graecia dating to the mid-fifth century BC showing Iokastos and Phalanthos, the legendary founders of Rhegion and Taras respectively, playing with their pet cats. The usual ancient Greek word for 'cat' was , meaning 'thing with the waving tail'. Cats are rarely mentioned in ancient Greek literature. Aristotle remarked in his "History of Animals" that "female cats are naturally lecherous." The Greeks later syncretized their own goddess Artemis with the Egyptian goddess Bastet, adopting Bastet's associations with cats and ascribing them to Artemis. In Ovid's "Metamorphoses", when the deities flee to Egypt and take animal forms, the goddess Diana turns into a cat. Cats eventually displaced ferrets as the pest control of choice because they were more pleasant to have around the house and were more enthusiastic hunters of mice. During the Middle Ages, many of Artemis's associations with cats were grafted onto the Virgin Mary. Cats are often shown in icons of Annunciation and of the Holy Family and, according to Italian folklore, on the same night that Mary gave birth to Jesus, a cat in Bethlehem gave birth to a kitten. Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as ships' cats were carried on sailing ships to control shipboard rodents and as good-luck charms.

Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the cat is a symbol of good fortune. In Norse mythology, Freyja, the goddess of love, beauty, and fertility, is depicted as riding a chariot drawn by cats. In Jewish legend, the first cat was living in the house of the first man Adam as a pet that got rid of mice. The cat was once partnering with the first dog before the latter broke an oath they had made which resulted in enmity between the descendants of these two animals. It is also written that neither cats nor foxes are represented in the water, while every other animal has an incarnation species in the water. Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, "Muezza". He is reported to have loved cats so much, "he would do without his cloak rather than disturb one that was sleeping on it". The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i, centuries after Muhammad. One of the companions of Muhammad was known as Abu Hurayrah ("father of the kitten"), in reference to his documented affection to cats.

Many cultures have negative superstitions about cats. An example would be the belief that a black cat "crossing one's path" leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade). In medieval France, cats would be burnt alive as a form of entertainment. According to Norman Davies, the assembled people "shrieked with laughter as the animals, howling with pain, were singed, roasted, and finally carbonized".

James Frazer wrote that "It was the custom to burn a basket, barrel, or sack full of live cats, which was hung from a tall mast in the midst of the bonfire; sometimes a fox was burned. The people collected the embers and ashes of the fire and took them home, believing that they brought good luck. The French kings often witnessed these spectacles and even lit the bonfire with their own hands. In 1648 Louis XIV, crowned with a wreath of roses and carrying a bunch of roses in his hand, kindled the fire, danced at it and partook of the banquet afterwards in the town hall. But this was the last occasion when a monarch presided at the midsummer bonfire in Paris. At Metz midsummer fires were lighted with great pomp on the esplanade, and a dozen cats, enclosed in wicker cages, were burned alive in them, to the amusement of the people. Similarly at Gap, in the department of the Hautes-Alpes, cats used to be roasted over the midsummer bonfire."

According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, Brazil and some Spanish-speaking regions, they are said to have seven lives, while in Turkish and Arabic traditions, the number of lives is six. The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations. Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.



</doc>
<doc id="6681" url="https://en.wikipedia.org/wiki?curid=6681" title="Crank">
Crank

Crank may refer to:








</doc>
<doc id="6682" url="https://en.wikipedia.org/wiki?curid=6682" title="Clade">
Clade

A clade (; from , "klados", "branch"), also known as a monophyletic group or natural group, is a group of organisms that are monophyleticâthat is, composed of a common ancestor and all its lineal descendants. Rather than the English term, the equivalent Latin term "cladus" (plural "cladi") is often used in taxonomical literature.

The common ancestor may be an individual, a population, a species (extinct or extant), and so on right up to a kingdom and further. Clades are nested, one in another, as each branch in turn splits into smaller branches. These splits reflect evolutionary history as populations diverged and evolved independently. Clades are termed monophyletic (Greek: "one clan") groups.

Over the last few decades, the cladistic approach has revolutionized biological classification and revealed surprising evolutionary relationships among organisms. Increasingly, taxonomists try to avoid naming taxa that are not clades; that is, taxa that are not monophyletic. Some of the relationships between organisms that the molecular biology arm of cladistics has revealed are that fungi are closer relatives to animals than they are to plants, archaea are now considered different from bacteria, and multicellular organisms may have evolved from archaea.

The term "clade" is also used with a similar meaning in other fields besides biology, such as historical linguistics; see Cladistics Â§ In disciplines other than biology.

The term "clade" was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, the evolutionary splitting of a parent species into two distinct species, a concept Huxley borrowed from Bernhard Rensch.

Many commonly named groups, rodents and insects for example, are clades because, in each case, the group consists of a common ancestor with all its descendant branches. Rodents, for example, are a branch of mammals that split off after the end of the period when the clade Dinosauria stopped being the dominant terrestrial vertebrates 66 million years ago. The original population and all its descendants are a clade. The rodent clade corresponds to the order Rodentia, and insects to the class Insecta. These clades include smaller clades, such as chipmunk or ant, each of which consists of even smaller clades. The clade "rodent" is in turn included in the mammal, vertebrate and animal clades.

The idea of a clade did not exist in pre-Darwinian Linnaean taxonomy, which was based by necessity only on internal or external morphological similarities between organisms â although as it happens, many of the better known animal groups in Linnaeus' original Systema Naturae (notably among the vertebrate groups) do represent clades. The phenomenon of convergent evolution is, however, responsible for many cases where there are misleading similarities in the morphology of groups that evolved from different lineages.

With the increasing realization in the first half of the 19th century that species had changed and split through the ages, classification increasingly came to be seen as branches on the evolutionary tree of life. The publication of Darwin's theory of evolution in 1859 gave this view increasing weight. Thomas Henry Huxley, an early advocate of evolutionary theory, proposed a revised taxonomy based on a concept strongly resembling clades,<ref name="original text w/ figures">Huxley, T.H. (1876): Lectures on Evolution. "New York Tribune". Extra. no 36. In Collected Essays IV: pp 46-138 original text w/ figures</ref> although the term "clade" itself would not be coined until 1957 by his grandson, Julian Huxley. For example, the elder Huxley grouped birds with reptiles, based on fossil evidence.

German biologist Emil Hans Willi Hennig (1913 â 1976) is considered to be the founder of cladistics.
He proposed a classification system that represented repeated branchings of the family tree, as opposed to the previous systems, which put organisms on a "ladder", with supposedly more "advanced" organisms at the top.

Taxonomists have increasingly worked to make the taxonomic system reflect evolution. When it comes to naming, however, this principle is not always compatible with the traditional rank-based nomenclature (in which only taxa associated with a rank can be named) because there are not enough ranks to name a long series of nested clades. For these and other reasons, phylogenetic nomenclature has been developed; it is still controversial.

A clade is by definition monophyletic, meaning that it contains one ancestor (which can be an organism, a population, or a species) and all its descendants. The ancestor can be known or unknown; any and all members of a clade can be extant or extinct.

The science that tries to reconstruct phylogenetic trees and thus discover clades is called phylogenetics or cladistics, the latter term coined by Ernst Mayr (1965), derived from "clade". The results of phylogenetic/cladistic analyses are tree-shaped diagrams called "cladograms"; they, and all their branches, are phylogenetic hypotheses.

Three methods of defining clades are featured in phylogenetic nomenclature: node-, stem-, and apomorphy-based (see Phylogenetic nomenclatureÂ§Phylogenetic definitions of clade names for detailed definitions).

The relationship between clades can be described in several ways:


"Clade" is the title of a novel by James Bradley, who chose it both because of its biological meaning and also because of the larger implications of the word.

An episode of "Elementary" is titled "Dead Clade Walking" and deals with a case involving a rare fossil.



</doc>
<doc id="6684" url="https://en.wikipedia.org/wiki?curid=6684" title="Communications in Afghanistan">
Communications in Afghanistan

Communications in Afghanistan is under the control of the Ministry of Communications and Information Technology (MCIT). It has rapidly expanded after the Karzai administration took over in late 2001, and has embarked on wireless companies, internet, radio stations and television channels.

The Afghan government signed a $64.5 million agreement in 2006 with China's ZTE on the establishment of a countrywide optical fiber cable network. The project began to improve telephone, internet, television and radio broadcast services throughout Afghanistan. About 90% of the country's population had access to communication services in 2014.

Afghanistan uses its own space satellite called Afghansat 1. There are about 18 million mobile phone users in the country. Telecom companies include Afghan Telecom, Afghan Wireless, Etisalat, MTN, Roshan, Salaam and a few others. Over 60% of the population have access to the internet.

There are about 32 million GSM mobile phone subscribers in Afghanistan as of 2016, with over 114,192 fixed-telephone-lines and over 264,000 CDMA subscribers. Mobile communications have improved because of the introduction of wireless carriers into this developing country. The first was Afghan Wireless, which is US based that was founded by Ehsan Bayat. The second was Roshan, which began providing services to all major cities within Afghanistan. There are also a number of VSAT stations in major cities such as Kabul, Kandahar, Herat, Mazari Sharif, and Jalalabad, providing international and domestic voice/data connectivity. The international calling code for Afghanistan is +93. The following is a partial list of mobile phone companies in the country:


All the companies providing communication services are obligated to deliver 2.5% of their income to the communication development fund annually. According to the Ministry of Communication and Information Technology there are 4760 active towers throughout the country which covers 85% of the population. The Ministry of Communication and Information Technology plans to expand its services in remote parts of the country where the remaining 15% of the population will be covered with the installation of 700 new towers.

Phone calls in Afghanistan have been monitored by the National Security Agency according to WikiLeaks.

MTN 21 According to a three-year duopoly agreement between the MCIT and mobile operators AWCC and Roshan, no mobile operator could enter the Afghan telecom market until July 2006. The third GSM license was awarded to Areeba in September 2005 for a period of 15 years, and a total license fee of $40.1 million. Areeba was a subsidiary of the Lebanon-based firm Investcom in consortium with Alokozai-FZE. After commencing services in July 2006, Areeba had an estimated subscribership of 200,000 by the end of that year. Areeba was later acquired by the South African-based Mobile Telephone Network (MTN) in mid-2007 as part of a $5.53 billion global merger between the two companies. MTN-Afghanistan is a subsidiary of the South African-based MTN Group, a multinational telecommunications company operating across the Middle East and Africa. MTN is the majority (90%) shareholder, while International Finance Corporation (IFC) at 9% is also a debt and equity shareholder of MTN-Afghanistan. MTN operates at 900-1800Â MHz GSM band, and as of 2012 has 4.5 million subscribers and service coverage in most major cities, 464 districts, and all 34 provincial capitals. With over $400 million in total investment, MTN offers mobile voice, SMS, MMS, SRS, GPRS, fax, voicemail and PCO services through prepaid, postpaid and corporate tariffs.

MTN has interconnection agreements with all national telecom operators and provides international voice and SMS roaming in 121 countries and across 227 operators through prepaid and postpaid roaming tariffs. MTN also has a national ISP license which the company received in November 2008. MTN was the first company to introduce the popular per-second billing system in the country (also known as "pay as you talk") allowing its subscribers to transparently track their talk-time and receive billing summaries via SMS. The scheme was so popular that other GSM companies quickly adopted this method.

Afghanistan was given legal control of the ".af" domain in 2003, and the Afghanistan Network Information Center (AFGNIC) was established to administer domain names. As of 2016, there are at least 55 internet service providers (ISPs) in the country. Internet in Afghanistan is also at the peak with over 5 million users as of 2016.

According to the Ministry of Communications, the following are some of the different ISPs operating in Afghanistan:

There are over 106 television operators in Afghanistan and 320 television transmitters, many of which are based Kabul, while others are broadcast from other provinces. Selected foreign channels are also shown to the public in Afghanistan, but with the use of the internet, over 3,500 international TV channels may be accessed in Afghanistan.

There are an estimated 150 FM radio operators throughout the country. Broadcasts are in Dari, Pashto, English, Uzbeki and a number of other languages.

Radio listeners are generally decreasing and are being slowly outnumbered by television. Of Afghanistan's 6 main cities, Kandahar and Khost have the maximum number of radio listeners. Kabul and Jalalabad have moderate number of listeners. However, Mazar-e-Sharif and especially Herat have very few radio listeners.

In 1870, a central post office was established at Bala Hissar in Kabul and a post office in the capital of each province. The service was slowly being expanded over the years as more postal offices were established in each large city by 1918. Afghanistan became a member of the Universal Postal Union in 1928, and the postal administration elevated to the Ministry of Communication in 1934. Civil war caused a disruption in issuing official stamps during the 1980s-90s war but in 1999 postal service was operating again. Postal services to/from Kabul worked remarkably well all throughout the war years. Postal services to/from Herat resumed in 1997. The Afghan government has reported to the UPU several times about illegal stamps being issued and sold in 2003 and 2007.

Afghanistan Post has been reorganizing the postal service in 2000s with assistance from Pakistan Post. The Afghanistan Postal commission was formed to prepare a written policy for the development of the postal sector, which will form the basis of a new postal services law governing licensing of postal services providers. The project was expected to finish by 2008.

In January 2014 the Afghan Ministry of Communications and Information Technology signed an agreement with Eutelsat for the use of satellite resources to enhance deployment of Afghanistan's national broadcasting and telecommunications infrastructure as well as its international connectivity. Afghansat 1 was officially launched in May 2014, with expected service for at least seven years in Afghanistan. The Afghan government plans to launch Afghansat 2 after the lease of Afghansat 1 ends.



</doc>
<doc id="6689" url="https://en.wikipedia.org/wiki?curid=6689" title="Christian of Oliva">
Christian of Oliva

Christian of Oliva (), also Christian of Prussia () (died 4 December(?) 1245) was the first missionary bishop of Prussia. 

Christian was born about 1180 in the Duchy of Pomerania, possibly in the area of Chociwel (according to Johannes Voigt). Probably as a juvenile he joined the Cistercian Order at newly established KoÅbacz ("Kolbatz") Abbey and in 1209 entered Oliwa Abbey near GdaÅsk, founded in 1178 by the Samboride dukes of Pomerelia. At this time the Piast duke Konrad I of Masovia with the consent of Pope Innocent III had started the first of several unsuccessful Prussian Crusades into the adjacent CheÅmno Land and Christian acted as a missionary among the Prussians east of the Vistula River.
In 1209, Christian was commissioned by the Pope to be responsible for the Prussian missions between the Vistula and Neman Rivers and in 1212 he was appointed bishop. In 1215 he went to Rome in order to report to the Curia on the condition and prospects of his mission, and was consecrated first "Bishop of Prussia" at the Fourth Council of the Lateran. His seat as a bishop remained at Oliwa Abbey on the western side of the Vistula, whereas the pagan Prussian (later East Prussian) territory was on the eastern side of it.

The attempts by Konrad of Masovia to subdue the Prussian lands had picked long-term and intense border quarrels, whereby the Polish lands of Masovia, Cuyavia and even Greater Poland became subject to continuous Prussian raids. Bishop Christian asked the new Pope Honorius III for the consent to start another Crusade, however a first campaign in 1217 proved a failure and even the joint efforts by Duke Konrad with the Polish High Duke Leszek I the White and Duke Henry I the Bearded of Silesia in 1222/23 only led to the reconquest of CheÅmno Land but did not stop the Prussian invasions. At least Christian was able to establish the Diocese of CheÅmno east of the Vistula, adopting the episcopal rights from the Masovian Bishop of PÅock, confirmed by both Duke Konrad and the Pope.

Duke Konrad of Masovia still was not capable to end the Prussian attacks on his territory and in 1226 began to conduct negotiations with the Teutonic Knights under Grand Master Hermann von Salza in order to strengthen his forces. As von Salza initially hesitated to offer his services, Christian created the military Order of DobrzyÅ ("Fratres Milites Christi") in 1228, however to little avail.

Meanwhile, von Salza had to abandon his hope to establish an Order's State in the Burzenland region of Transylvania, which had led to an Ã©clat with King Andrew II of Hungary. He obtained a charter by Emperor Frederick II issued in the 1226 Golden Bull of Rimini, whereby CheÅmno Land would be the unshared possession of the Teutonic Knights, which was confirmed by Duke Konrad of Masovia in the 1230 Treaty of Kruszwica. Christian ceded his possessions to the new State of the Teutonic Order and in turn was appointed Bishop of CheÅmno the next year.

Bishop Christian continued his mission in Sambia ("Samland"), where from 1233 to 1239 he was held captive by pagan Prussians, and freed in trade for five other hostages who then in turn were released for a ransom of 800 Marks, granted to him by Pope Gregory IX. He had to deal with the constant cut-back of his autonomy by the Knights and asked the Roman Curia for mediation. In 1243, the Papal legate William of Modena divided the Prussian lands of the Order's State into four dioceses, whereby the bishops retained the secular rule over about on third of the diocesan territory:
all suffragan dioceses under the Archbishopric of Riga. Christian was supposed to choose one of them, but did not agree to the division. He possibly retired to the Cistercians Abbey in SulejÃ³w, where he died before the conflict was solved.



</doc>
<doc id="6690" url="https://en.wikipedia.org/wiki?curid=6690" title="Coca-Cola">
Coca-Cola

Coca-Cola, or Coke, is a carbonated soft drink manufactured by The Coca-Cola Company. Originally marketed as a temperance drink and intended as a patent medicine, it was invented in the late 19th century by John Stith Pemberton and was bought out by businessman Asa Griggs Candler, whose marketing tactics led Coca-Cola to its dominance of the world soft-drink market throughout the 20th century. The drink's name refers to two of its original ingredients: coca leaves, and kola nuts (a source of caffeine). The current formula of Coca-Cola remains a trade secret; however, a variety of reported recipes and experimental recreations have been published.

The Coca-Cola Company produces concentrate, which is then sold to licensed Coca-Cola bottlers throughout the world. The bottlers, who hold exclusive territory contracts with the company, produce the finished product in cans and bottles from the concentrate, in combination with filtered water and sweeteners. A typical can contains of sugar (usually in the form of high-fructose corn syrup). The bottlers then sell, distribute, and merchandise Coca-Cola to retail stores, restaurants, and vending machines throughout the world. The Coca-Cola Company also sells concentrate for soda fountains of major restaurants and foodservice distributors.

The Coca-Cola Company has on occasion introduced other cola drinks under the Coke name. The most common of these is Diet Coke, along with others including Caffeine-Free Coca-Cola, Diet Coke Caffeine-Free, Coca-Cola Zero Sugar, Coca-Cola Cherry, Coca-Cola Vanilla, and special versions with lemon, lime, and coffee. Coca-Cola was called Coca-Cola Classic from July 1985 to 2009, to distinguish it from "New Coke". Based on Interbrand's "best global brand" study of 2015, Coca-Cola was the world's third most valuable brand, after Apple and Google. In 2013, Coke products were sold in over 200 countries worldwide, with consumers drinking more than 1.8 billion company beverage servings each day. Coca-Cola ranked No. 87 in the 2018 "Fortune" 500 list of the largest United States corporations by total revenue.

Confederate Colonel John Pemberton, wounded in the American Civil War and addicted to morphine, also had a medical degree and began a quest to find a substitute for the problematic drug. In 1885 at Pemberton's Eagle Drug and Chemical House, his drugstore in Columbus, Georgia, he registered Pemberton's French Wine Coca nerve tonic. Pemberton's tonic may have been inspired by the formidable success of Vin Mariani, a French-Corsican coca wine, but his recipe additionally included the African kola nut, the beverage's source of caffeine.

It is also worth noting that a Spanish drink called "Kola Coca" was presented at a contest in Philadelphia in 1885, a year before the official birth of Coca-Cola. The rights for this Spanish drink were bought by Coca-Cola in 1953.

In 1886, when Atlanta and Fulton County passed prohibition legislation, Pemberton responded by developing Coca-Cola, a nonalcoholic version of Pemberton's French Wine Coca. It was marketed as "Coca-Cola: The temperance drink", which appealed to many people as the temperance movement enjoyed wide support during this time. The first sales were at Jacob's Pharmacy in Atlanta, Georgia, on May 8, 1886, where it initially sold for five cents a glass. Drugstore soda fountains were popular in the United States at the time due to the belief that carbonated water was good for the health, and Pemberton's new drink was marketed and sold as a patent medicine, Pemberton claiming it a cure for many diseases, including morphine addiction, indigestion, nerve disorders, headaches, and impotence. Pemberton ran the first advertisement for the beverage on May 29 of the same year in the "Atlanta Journal".

By 1888, three versions of Coca-Cola â sold by three separate businesses â were on the market. A co-partnership had been formed on January 14, 1888, between Pemberton and four Atlanta businessmen: J.C. Mayfield, A.O. Murphey, C.O. Mullahy, and E.H. Bloodworth. Not codified by any signed document, a verbal statement given by Asa Candler years later asserted under testimony that he had acquired a stake in Pemberton's company as early as 1887. John Pemberton declared that the "name" "Coca-Cola" belonged to his son, Charley, but the other two manufacturers could continue to use the "formula".

Charley Pemberton's record of control over the "Coca-Cola" name was the underlying factor that allowed for him to participate as a major shareholder in the March 1888 Coca-Cola Company incorporation filing made in his father's place. Charley's exclusive control over the "Coca-Cola" name became a continual thorn in Asa Candler's side. Candler's oldest son, Charles Howard Candler, authored a book in 1950 published by Emory University. In this definitive biography about his father, Candler specifically states: " on April 14, 1888, the young druggist Asa Griggs Candler purchased a one-third interest in the formula of an almost completely unknown proprietary elixir known as Coca-Cola." The deal was actually between John Pemberton's son Charley and Walker, Candler & Co. â with John Pemberton acting as cosigner for his son. For $50 down and $500 in 30 days, Walker, Candler & Co. obtained all of the one-third interest in the Coca-Cola Company that Charley held, all while Charley still held on to the name. After the April 14 deal, on April 17, 1888, one-half of the Walker/Dozier interest shares were acquired by Candler for an additional $750.

In 1892, Candler set out to incorporate a second company; "The Coca-Cola Company" (the current corporation). When Candler had the earliest records of the "Coca-Cola Company" destroyed in 1910, the action was claimed to have been made during a move to new corporation offices around this time.

After Candler had gained a better foothold on Coca-Cola in April 1888, he nevertheless was forced to sell the beverage he produced with the recipe he had under the names "Yum Yum" and "Koke". This was while Charley Pemberton was selling the elixir, although a cruder mixture, under the name "Coca-Cola", all with his father's blessing. After both names failed to catch on for Candler, by the middle of 1888, the Atlanta pharmacist was quite anxious to establish a firmer legal claim to Coca-Cola, and hoped he could force his two competitors, Walker and Dozier, completely out of the business, as well.

John Pemberton died suddenly on August 16, 1888. Asa Candler then decided to move swiftly forward to attain full control of the entire Coca-Cola operation.

Charley Pemberton, an alcoholic and opium addict unnerved Asa Candler more than anyone else. Candler is said to have quickly maneuvered to purchase the exclusive rights to the name "Coca-Cola" from Pemberton's son Charley immediately after he learned of Dr. Pemberton's death. One of several stories states that Candler approached Charley's mother at John Pemberton's funeral and offered her $300 in cash for the title to the name. Charley Pemberton was found on June 23, 1894, unconscious, with a stick of opium by his side. Ten days later, Charley died at Atlanta's Grady Hospital at the age of 40.

In Charles Howard Candler's 1950 book about his father, he stated: "On August 30 [1888], he Asa Candler became the sole proprietor of Coca-Cola, a fact which was stated on letterheads, invoice blanks and advertising copy."

With this action on August 30, 1888, Candler's sole control became technically all true. Candler had negotiated with Margaret Dozier and her brother Woolfolk Walker a full payment amounting to $1,000, which all agreed Candler could pay off with a series of notes over a specified time span. By May 1, 1889, Candler was now claiming full ownership of the Coca-Cola beverage, with a total investment outlay by Candler for the drink enterprise over the years amounting to $2,300.

In 1914, Margaret Dozier, as co-owner of the original Coca-Cola Company in 1888, came forward to claim that her signature on the 1888 Coca-Cola Company bill of sale had been forged. Subsequent analysis of other similar transfer documents had also indicated John Pemberton's signature had most likely been forged as well, which some accounts claim was precipitated by his son Charley.

On September 12, 1919, Coca-Cola Co. was purchased by a group of investors for $25 million and reincorporated in Delaware. The company publicly offered 500,000 shares of the company for $40 a share.

In 1986, The Coca-Cola Company merged with two of their bottling operators (owned by JTL Corporation and BCI Holding Corporation) to form Coca-Cola Enterprises Inc. (CCE).

In December 1991, Coca-Cola Enterprises merged with the Johnston Coca-Cola Bottling Group, Inc.

The first bottling of Coca-Cola occurred in Vicksburg, Mississippi, at the Biedenharn Candy Company on March 12, 1894. The proprietor of the bottling works was Joseph A. Biedenharn. The original bottles were Hutchinson bottles, very different from the much later hobble-skirt design of 1915 now so familiar.

A few years later two entrepreneurs from Chattanooga, Tennessee, namely Benjamin F. Thomas and Joseph B. Whitehead, proposed the idea of bottling and were so persuasive that Candler signed a contract giving them control of the procedure for only one dollar. Candler never collected his dollar, but in 1899, Chattanooga became the site of the first Coca-Cola bottling company. Candler remained very content just selling his company's syrup. The loosely termed contract proved to be problematic for The Coca-Cola Company for decades to come. Legal matters were not helped by the decision of the bottlers to subcontract to other companies, effectively becoming parent bottlers. This contract specified that bottles would be sold at 5Â¢ each and had no fixed duration, leading to the fixed price of Coca-Cola from 1886 to 1959.

The first outdoor wall advertisement that promoted the Coca-Cola drink was painted in 1894 in Cartersville, Georgia. Cola syrup was sold as an over-the-counter dietary supplement for upset stomach. By the time of its 50th anniversary, the soft drink had reached the status of a national icon in the US. In 1935, it was certified kosher by Atlanta Rabbi Tobias Geffen with the help of Harold Hirsch, Geffen was the first person to see the top-secret ingredients list after facing scrutiny from the American Jewish population regarding the drink's kosher status, consequently the company made minor changes in the sourcing of some ingredients so it could continue to be consumed by Americas Jewish population and during Passover.

The longest running commercial Coca-Cola soda fountain anywhere was Atlanta's Fleeman's Pharmacy, which first opened its doors in 1914. Jack Fleeman took over the pharmacy from his father and ran it until 1995; closing it after 81 years. On July 12, 1944, the one-billionth gallon of Coca-Cola syrup was manufactured by The Coca-Cola Company. Cans of Coke first appeared in 1955.

On April 23, 1985, Coca-Cola, amid much publicity, attempted to change the formula of the drink with "New Coke". Follow-up taste tests revealed most consumers preferred the taste of New Coke to both Coke and Pepsi but Coca-Cola management was unprepared for the public's nostalgia for the old drink, leading to a backlash. The company gave in to protests and returned to the old formula under the name Coca-Cola Classic, on July 10, 1985. "New Coke" remained available and was renamed Coke II in 1992; it was discontinued in 2002.

On July 5, 2005, it was revealed that Coca-Cola would resume operations in Iraq for the first time since the Arab League boycotted the company in 1968.

In April 2007, in Canada, the name "Coca-Cola Classic" was changed back to "Coca-Cola". The word "Classic" was removed because "New Coke" was no longer in production, eliminating the need to differentiate between the two. The formula remained unchanged. In January 2009, Coca-Cola stopped printing the word "Classic" on the labels of bottles sold in parts of the southeastern United States. The change is part of a larger strategy to rejuvenate the product's image. The word "Classic" was removed from all Coca-Cola products by 2011.

In November 2009, due to a dispute over wholesale prices of Coca-Cola products, Costco stopped restocking its shelves with Coke and Diet Coke for two months; a separate pouring rights deal in 2013 saw Coke products removed from Costco food courts in favor of Pepsi. Some Costco locations (such as the ones in Tucson, Arizona) additionally sell imported Coca-Cola from Mexico with cane sugar instead of corn syrup from separate distributors. Coca-Cola introduced the 7.5-ounce mini-can in 2009, and on September 22, 2011, the company announced price reductions, asking retailers to sell eight-packs for $2.99. That same day, Coca-Cola announced the 12.5-ounce bottle, to sell for 89 cents. A 16-ounce bottle has sold well at 99 cents since being re-introduced, but the price was going up to $1.19.

In 2012, Coca-Cola resumed business in Myanmar after 60 years of absence due to U.S.-imposed investment sanctions against the country. Coca-Cola's bottling plant will be located in Yangon and is part of the company's five-year plan and $200 million investment in Myanmar. Coca-Cola with its partners is to invest US$5 billion in its operations in India by 2020. In 2013, it was announced that Coca-Cola Life would be introduced in Argentina and other parts of the world that would contain stevia and sugar. However, the drink was discontinued in Britain on June 2017.

On August 28, 2020, the company announced the cut of "thousands" of jobs as a result the COVID-19 pandemic effects; closing of bars, restaurants, and other venues resulted in a lower demand for Coca-Cola. 4,000 "voluntary separations" will be provided to employees based in the United States, Canada, and Puerto Rico. Subsequently, the same model is set to be used for other Coca-Cola companies worldwide.

A typical can of Coca-Cola (12 fl ounces/355Â ml) contains 38 grams of sugar (usually in the form of HFCS), 50Â mg of sodium, 0Â grams fat, 0Â grams potassium, and 140 calories. On May 5, 2014, Coca-Cola said it is working to remove a controversial ingredient, brominated vegetable oil, from all of its drinks.

The exact formula of Coca-Cola's natural flavorings (but not its other ingredients, which are listed on the side of the bottle or can) is a trade secret. The original copy of the formula was held in SunTrust Bank's main vault in Atlanta for 86 years. Its predecessor, the Trust Company, was the underwriter for the Coca-Cola Company's initial public offering in 1919. On December 8, 2011, the original secret formula was moved from the vault at SunTrust Banks to a new vault containing the formula which will be on display for visitors to its World of Coca-Cola museum in downtown Atlanta.

According to Snopes, a popular myth states that only two executives have access to the formula, with each executive having only half the formula. However, several sources state that while Coca-Cola does have a rule restricting access to only two executives, each knows the entire formula and others, in addition to the prescribed duo, have known the formulation process.

On February 11, 2011, Ira Glass said on his PRI radio show, "This American Life", that "TAL" staffers had found a recipe in "Everett Beal's Recipe Book", reproduced in the February 28, 1979, issue of "The Atlanta Journal-Constitution", that they believed was either Pemberton's original formula for Coca-Cola, or a version that he made either before or after the product hit the market in 1886. The formula basically matched the one found in Pemberton's diary. Coca-Cola archivist Phil Mooney acknowledged that the recipe "could. be a precursor" to the formula used in the original 1886 product, but emphasized that Pemberton's original formula is not the same as the one used in the current product.

When launched, Coca-Cola's two key ingredients were cocaine and caffeine. The cocaine was derived from the coca leaf and the caffeine from kola nut (also spelled "cola nut" at the time), leading to the name Coca-Cola.

Pemberton called for five ounces of coca leaf per gallon of syrup (approximately 37Â g/L), a significant dose; in 1891, Candler claimed his formula (altered extensively from Pemberton's original) contained only a tenth of this amount. Coca-Cola once contained an estimated nine milligrams of cocaine per glass. (For comparison, a typical dose or "line" of cocaine is 50â75Â mg.) In 1903, it was removed.

After 1904, instead of using fresh leaves, Coca-Cola started using "spent" leavesÂ â the leftovers of the cocaine-extraction process with trace levels of cocaine. Since then, Coca-Cola has used a cocaine-free coca leaf extract. Today, that extract is prepared at a Stepan Company plant in Maywood, New Jersey, the only manufacturing plant authorized by the federal government to import and process coca leaves, which it obtains from Peru and Bolivia. Stepan Company extracts cocaine from the coca leaves, which it then sells to Mallinckrodt, the only company in the United States licensed to purify cocaine for medicinal use.

Long after the syrup had ceased to contain any significant amount of cocaine, in the southeastern U.S., "dope" remained a common colloquialism for Coca-Cola, and "dope-wagons" were trucks that transported it.

Kola nuts act as a flavoring and the original source of caffeine in Coca-Cola. Kola nuts contain about 2.0 to 3.5% caffeine, and has a bitter flavor.

In 1911, the U.S. government sued in "United States v. Forty Barrels and Twenty Kegs of Coca-Cola", hoping to force the Coca-Cola Company to remove caffeine from its formula. The court found that the syrup, when diluted as directed, would result in a beverage containing 1.21 grains (or 78.4Â mg) of caffeine per serving. The case was decided in favor of the Coca-Cola Company at the district court, but subsequently in 1912, the U.S. Pure Food and Drug Act was amended, adding caffeine to the list of "habit-forming" and "deleterious" substances which must be listed on a product's label. In 1913 the case was appealed to the Sixth Circuit in Cincinnati, where the ruling was affirmed, but then appealed again in 1916 to the Supreme Court, where the government effectively won as a new trial was ordered. The company then voluntarily reduced the amount of caffeine in its product, and offered to pay the government's legal costs to settle and avoid further litigation.

Coca-Cola contains 34Â mg of caffeine per 12 fluid ounces (9.8Â mg per 100Â ml).

The actual production and distribution of Coca-Cola follows a franchising model. The Coca-Cola Company only produces a syrup concentrate, which it sells to bottlers throughout the world, who hold Coca-Cola franchises for one or more geographical areas. The bottlers produce the final drink by mixing the syrup with filtered water and sweeteners, putting the mixture into cans and bottles, and carbonating it, which the bottlers then sell and distribute to retail stores, vending machines, restaurants, and foodservice distributors.

The Coca-Cola Company owns minority shares in some of its largest franchises, such as Coca-Cola Enterprises, Coca-Cola Amatil, Coca-Cola Hellenic Bottling Company, and Coca-Cola FEMSA, but fully independent bottlers produce almost half of the volume sold in the world.
Independent bottlers are allowed to sweeten the drink according to local tastes.

The bottling plant in Skopje, Macedonia, received the 2009 award for "Best Bottling Company".

Since it announced its intention to begin distribution in Myanmar in June 2012, Coca-Cola has been officially available in every country in the world except Cuba and North Korea. However, it is reported to be available in both countries as a grey import.

Coca-Cola has been a point of legal discussion in the Middle East. In the early 20th century, a fatwa was created in Egypt to discuss the question of "whether Muslims were permitted to drink Coca-Cola and Pepsi cola." The fatwa states: "According to the Muslim Hanefite, Shafi'ite, etc., the rule in Islamic law of forbidding or allowing foods and beverages is based on the presumption that such things are permitted unless it can be shown that they are forbidden on the basis of the Qur'an." The Muslim jurists stated that, unless the Qu'ran specifically prohibits the consumption of a particular product, it is permissible to consume. Another clause was discussed, whereby the same rules apply if a person is unaware of the condition or ingredients of the item in question.

This is a list of variants of Coca-Cola introduced around the world. In addition to the caffeine-free version of the original, additional fruit flavors have been included over the years. Not included here are versions of Diet Coke and Coca-Cola Zero Sugar; variant versions of those no-calorie colas can be found at their respective articles.

The Coca-Cola logo was created by John Pemberton's bookkeeper, Frank Mason Robinson, in 1885. Robinson came up with the name and chose the logo's distinctive cursive script. The writing style used, known as Spencerian script, was developed in the mid-19th century and was the dominant form of formal handwriting in the United States during that period.

Robinson also played a significant role in early Coca-Cola advertising. His promotional suggestions to Pemberton included giving away thousands of free drink coupons and plastering the city of Atlanta with publicity banners and streetcar signs.

Coca-Cola came under scrutiny in Egypt in 1951 because of a conspiracy theory that the Coca-Cola logo, when reflected in a mirror, spells out "No Mohammed no Mecca" in Arabic.

The Coca-Cola bottle, called the "contour bottle" within the company, was created by bottle designer Earl R. Dean and Coca-Cola's general counsel, Harold Hirsch. In 1915, The Coca-Cola Company was represented by their general counsel to launch a competition among its bottle suppliers as well as any competition entrants to create a new bottle for their beverage that would distinguish it from other beverage bottles, "a bottle which a person could recognize even if they felt it in the dark, and so shaped that, even if broken, a person could tell at a glance what it was."

Chapman J. Root, president of the Root Glass Company of Terre Haute, Indiana, turned the project over to members of his supervisory staff, including company auditor T. Clyde Edwards, plant superintendent Alexander Samuelsson, and Earl R. Dean, bottle designer and supervisor of the bottle molding room. Root and his subordinates decided to base the bottle's design on one of the soda's two ingredients, the coca leaf or the kola nut, but were unaware of what either ingredient looked like. Dean and Edwards went to the Emeline Fairbanks Memorial Library and were unable to find any information about coca or kola. Instead, Dean was inspired by a picture of the gourd-shaped cocoa pod in the EncyclopÃ¦dia Britannica. Dean made a rough sketch of the pod and returned to the plant to show Root. He explained to Root how he could transform the shape of the pod into a bottle. Root gave Dean his approval.

Faced with the upcoming scheduled maintenance of the mold-making machinery, over the next 24 hours Dean sketched out a concept drawing which was approved by Root the next morning. Chapman Root approved the prototype bottle and a design patent was issued on the bottle in November 1915. The prototype never made it to production since its middle diameter was larger than its base, making it unstable on conveyor belts. Dean resolved this issue by decreasing the bottle's middle diameter. During the 1916 bottler's convention, Dean's contour bottle was chosen over other entries and was on the market the same year. By 1920, the contour bottle became the standard for The Coca-Cola Company. A revised version was also patented in 1923. Because the Patent Office releases the "Patent Gazette" on Tuesday, the bottle was patented on December 25, 1923, and was nicknamed the "Christmas bottle." Today, the contour Coca-Cola bottle is one of the most recognized packages on the planet..."even in the dark!".

As a reward for his efforts, Dean was offered a choice between a $500 bonus or a lifetime job at the Root Glass Company. He chose the lifetime job and kept it until the Owens-Illinois Glass Company bought out the Root Glass Company in the mid-1930s. Dean went on to work in other Midwestern glass factories.

Raymond Loewy updated the design in 1955 to accommodate larger formats.

Others have attributed inspiration for the design not to the cocoa pod, but to a Victorian hooped dress.

In 1944, Associate Justice Roger J. Traynor of the Supreme Court of California took advantage of a case involving a waitress injured by an exploding Coca-Cola bottle to articulate the doctrine of strict liability for defective products. Traynor's concurring opinion in "Escola v. Coca-Cola Bottling Co." is widely recognized as a landmark case in U.S. law today.

Karl Lagerfeld is the latest designer to have created a collection of aluminum bottles for Coca-Cola. Lagerfeld is not the first fashion designer to create a special version of the famous Coca-Cola Contour bottle. A number of other limited edition bottles by fashion designers for Coca-Cola Light soda have been created in the last few years, including Jean-Paul Gaultier.

In 2009, in Italy, Coca-Cola Light had a Tribute to Fashion to celebrate 100 years of the recognizable contour bottle. Well known Italian designers Alberta Ferretti, Blumarine, Etro, Fendi, Marni, Missoni, Moschino, and Versace each designed limited edition bottles.

In 2019, Coca-Cola shared the first beverage bottle made with ocean plastic.

Pepsi, the flagship product of PepsiCo, The Coca-Cola Company's main rival in the soft drink industry, is usually second to Coke in sales, and outsells Coca-Cola in some markets. RC Cola, now owned by the Dr Pepper Snapple Group, the third-largest soft drink manufacturer, is also widely available.

Around the world, many local brands compete with Coke. In South and Central America Kola Real, known as Big Cola in Mexico, is a growing competitor to Coca-Cola. On the French island of Corsica, Corsica Cola, made by brewers of the local Pietra beer, is a growing competitor to Coca-Cola. In the French region of Brittany, Breizh Cola is available. In Peru, Inca Kola outsells Coca-Cola, which led The Coca-Cola Company to purchase the brand in 1999. In Sweden, Julmust outsells Coca-Cola during the Christmas season. In Scotland, the locally produced Irn-Bru was more popular than Coca-Cola until 2005, when Coca-Cola and Diet Coke began to outpace its sales. In the former East Germany, Vita Cola, invented during Communist rule, is gaining popularity.

In India, Coca-Cola ranked third behind the leader, Pepsi-Cola, and local drink Thums Up. The Coca-Cola Company purchased Thums Up in 1993. , Coca-Cola held a 60.9% market-share in India. Tropicola, a domestic drink, is served in Cuba instead of Coca-Cola, due to a United States embargo. French brand Mecca Cola and British brand Qibla Cola are competitors to Coca-Cola in the Middle East.

In Turkey, Cola Turka, in Iran and the Middle East, Zamzam Cola and Parsi Cola, in some parts of China, China Cola, in the Czech Republic and Slovakia, Kofola, in Slovenia, Cockta, and the inexpensive Mercator Cola, sold only in the country's biggest supermarket chain, Mercator, are some of the brand's competitors. Classiko Cola, made by Tiko Group, the largest manufacturing company in Madagascar, is a competitor to Coca-Cola in many regions.

Coca-Cola's advertising has significantly affected American culture, and it is frequently credited with inventing the modern image of Santa Claus as an old man in a red-and-white suit. Although the company did start using the red-and-white Santa image in the 1930s, with its winter advertising campaigns illustrated by Haddon Sundblom, the motif was already common. Coca-Cola was not even the first soft drink company to use the modern image of Santa Claus in its advertising: White Rock Beverages used Santa in advertisements for its ginger ale in 1923, after first using him to sell mineral water in 1915. Before Santa Claus, Coca-Cola relied on images of smartly dressed young women to sell its beverages. Coca-Cola's first such advertisement appeared in 1895, featuring the young Bostonian actress Hilda Clark as its spokeswoman.

1941 saw the first use of the nickname "Coke" as an official trademark for the product, with a series of advertisements informing consumers that "Coke means Coca-Cola". In 1971, a song from a Coca-Cola commercial called "I'd Like to Teach the World to Sing", produced by Billy Davis, became a hit single.

Coke's advertising is pervasive, as one of Woodruff's stated goals was to ensure that everyone on Earth drank Coca-Cola as their preferred beverage. This is especially true in southern areas of the United States, such as Atlanta, where Coke was born.

Some Coca-Cola television commercials between 1960 through 1986 were written and produced by former Atlanta radio veteran Don Naylor (WGST 1936â1950, WAGA 1951â1959) during his career as a producer for the McCann Erickson advertising agency. Many of these early television commercials for Coca-Cola featured movie stars, sports heroes, and popular singers.

During the 1980s, Pepsi-Cola ran a series of television advertisements showing people participating in taste tests demonstrating that, according to the commercials, "fifty percent of the participants who said they preferred Coke "actually" chose the Pepsi." Statisticians pointed out the problematic nature of a 50/50 result: most likely, the taste tests showed that in blind tests, most people cannot tell the difference between Pepsi and Coke. Coca-Cola ran ads to combat Pepsi's ads in an incident sometimes referred to as the "cola wars"; one of Coke's ads compared the so-called Pepsi challenge to two chimpanzees deciding which tennis ball was furrier. Thereafter, Coca-Cola regained its leadership in the market.

Selena was a spokesperson for Coca-Cola from 1989 until the time of her death. She filmed three commercials for the company. During 1994, to commemorate her five years with the company, Coca-Cola issued special Selena coke bottles.

The Coca-Cola Company purchased Columbia Pictures in 1982, and began inserting Coke-product images into many of its films. After a few early successes during Coca-Cola's ownership, Columbia began to underperform, and the studio was sold to Sony in 1989.

Coca-Cola has gone through a number of different advertising slogans in its long history, including "The pause that refreshes", "I had like to buy the world a Coke", and "Coke is it".

In 2006, Coca-Cola introduced My Coke Rewards, a customer loyalty campaign where consumers earn points by entering codes from specially marked packages of Coca-Cola products into a website. These points can be redeemed for various prizes or sweepstakes entries.

In Australia in 2011, Coca-Cola began the "share a Coke" campaign, where the Coca-Cola logo was replaced on the bottles and replaced with first names. Coca-Cola used the 150 most popular names in Australia to print on the bottles. The campaign was paired with a website page, Facebook page, and an online "share a virtual Coke". The same campaign was introduced to Coca-Cola, Diet Coke & Coke Zero bottles and cans in the UK in 2013.

Coca-Cola has also advertised its product to be consumed as a breakfast beverage, instead of coffee or tea for the morning caffeine.

From 1886 to 1959, the price of Coca-Cola was fixed at five cents, in part due to an advertising campaign.

Throughout the years, Coca-Cola has released limited-time collector bottles for Christmas.

The "Holidays are coming!" advertisement features a train of red delivery trucks, emblazoned with the Coca-Cola name and decorated with Christmas lights, driving through a snowy landscape and causing everything that they pass to light up and people to watch as they pass through.

The advertisement fell into disuse in 2001, as the Coca-Cola company restructured its advertising campaigns so that advertising around the world was produced locally in each country, rather than centrally in the company's headquarters in Atlanta, Georgia. In 2007, the company brought back the campaign after, according to the company, many consumers telephoned its information center saying that they considered it to mark the beginning of Christmas. The advertisement was created by U.S. advertising agency Doner, and has been part of the company's global advertising campaign for many years.

Keith Law, a producer and writer of commercials for Belfast CityBeat, was not convinced by Coca-Cola's reintroduction of the advertisement in 2007, saying that "I do not think there's anything Christmassy about HGVs and the commercial is too generic."

In 2001, singer Melanie Thornton recorded the campaign's advertising jingle as a single, "Wonderful Dream (Holidays are Coming)", which entered the pop-music charts in Germany at no. 9. In 2005, Coca-Cola expanded the advertising campaign to radio, employing several variations of the jingle.

In 2011, Coca-Cola launched a campaign for the Indian holiday Diwali. The campaign included commercials, a song, and an integration with Shah Rukh Khan's film "Ra.One".

Coca-Cola was the first commercial sponsor of the Olympic Games, at the 1928 games in Amsterdam, and has been an Olympics sponsor ever since. This corporate sponsorship included the 1996 Summer Olympics hosted in Atlanta, which allowed Coca-Cola to spotlight its hometown. Most recently, Coca-Cola has released localized commercials for the 2010 Winter Olympics in Vancouver; one Canadian commercial referred to Canada's hockey heritage and was modified after Canada won the gold medal game on February 28, 2010 by changing the ending line of the commercial to say "Now they know whose game they're playing".

Since 1978, Coca-Cola has sponsored the FIFA World Cup, and other competitions organized by FIFA. One FIFA tournament trophy, the FIFA World Youth Championship from Tunisia in 1977 to Malaysia in 1997, was called "FIFAÂ â Coca-Cola Cup". In addition, Coca-Cola sponsors NASCAR's annual Coca-Cola 600 and Coke Zero Sugar 400 at Charlotte Motor Speedway in Concord, North Carolina and Daytona International Speedway in Daytona, Florida; since 2020, Coca-Cola has served as a premier partner of the NASCAR Cup Series, which includes holding the naming rights to the series' regular season championship trophy.

Coca-Cola has a long history of sports marketing relationships, which over the years have included Major League Baseball, the National Football League, the National Basketball Association, and the National Hockey League, as well as with many teams within those leagues. Coca-Cola has had a longtime relationship with the NFL's Pittsburgh Steelers, due in part to the now-famous 1979 television commercial featuring "Mean Joe" Greene, leading to the two opening the Coca-Cola Great Hall at Heinz Field in 2001 and a more recent Coca-Cola Zero commercial featuring Troy Polamalu.

Coca-Cola is the official soft drink of many collegiate football teams throughout the nation, partly due to Coca-Cola providing those schools with upgraded athletic facilities in exchange for Coca-Cola's sponsorship. This is especially prevalent at the high school level, which is more dependent on such contracts due to tighter budgets.

Coca-Cola was one of the official sponsors of the 1996 Cricket World Cup held on the Indian subcontinent. Coca-Cola is also one of the associate sponsors of Delhi Daredevils in the Indian Premier League.

In England, Coca-Cola was the main sponsor of The Football League between 2004 and 2010, a name given to the three professional divisions below the Premier League in soccer (football). In 2005, Coca-Cola launched a competition for the 72 clubs of The Football LeagueÂ â it was called "Win a Player". This allowed fans to place one vote per day for their favorite club, with one entry being chosen at random earning Â£250,000 for the club; this was repeated in 2006. The "Win A Player" competition was very controversial, as at the end of the 2 competitions, Leeds United A.F.C. had the most votes by more than double, yet they did not win any money to spend on a new player for the club. In 2007, the competition changed to "Buy a Player". This competition allowed fans to buy a bottle of Coca-Cola or Coca-Cola Zero and submit the code on the wrapper on the Coca-Cola website. This code could then earn anything from 50p to Â£100,000 for a club of their choice. This competition was favored over the old "Win a Player" competition, as it allowed all clubs to win some money. Between 1992 and 1998, Coca-Cola was the title sponsor of the Football League Cup (Coca-Cola Cup), the secondary cup tournament of England.

Between 1994 and 1997, Coca-Cola was also the title sponsor of the Scottish League Cup, renaming it to the Coca-Cola Cup like its English counterpart. From 1998 to 2001, the company was the title sponsor of the Irish League Cup in Northern Ireland, where it was named the Coca-Cola League Cup.

Coca-Cola is the presenting sponsor of the Tour Championship, the final event of the PGA Tour held each year at East Lake Golf Club in Atlanta, GA.

Introduced March 1, 2010, in Canada, to celebrate the 2010 Winter Olympics, Coca-Cola sold gold colored cans in packs of 12 each, in select stores.

Coca-Cola has been prominently featured in many films and television programs. It was a major plot element in films such as One, Two, Three, The Coca-Cola Kid, and The Gods Must Be Crazy, among many others. In music, in the Beatles' song, "Come Together", the lyrics say, "He shoot Coca-Cola", he say... The Beach Boys also referenced Coca-Cola in their 1964 song "All Summer Long" (i.e. Member when you spilled Coke all over your blouse?)

The best selling artist of all time Elvis Presley, promoted Coca-Cola during his last tour of 1977. The Coca-Cola Company used Elvis' image to promote the product. For example, the company used a song performed by Presley, A Little Less Conversation, in a Japanese Coca-Cola commercial.

Other artists that promoted Coca-Cola include David Bowie, George Michael, Elton John, and Whitney Houston, who appeared in the Diet Coke commercial, among many others.

Not all musical references to Coca-Cola went well. A line in "Lola" by the Kinks was originally recorded as "You drink champagne and it tastes just like Coca-Cola." When the British Broadcasting Corporation refused to play the song because of the commercial reference, lead singer Ray Davies re-recorded the lyric as "it tastes just like cherry cola" to get airplay for the song.

Political cartoonist Michel Kichka satirized a famous Coca-Cola billboard in his 1982 poster "And I Love New York." On the billboard, the Coca-Cola wave is accompanied by the words "Enjoy Coke." In Kichka's poster, the lettering and script above the Coca-Cola wave instead read "Enjoy Cocaine."

Coca-Cola has a high degree of identification with the United States, being considered by some an "American Brand" or as an item representing America. During World War II, this gave rise to the brief production of White Coke by the request of and for Soviet Marshall Georgy Zhukov, who did not want to be seen drinking an American imperial symbol. The drink is also often a metonym for the Coca-Cola Company.

Coca-Cola was introduced to China in 1927, and was very popular until 1949. After the Chinese Civil War ended in 1949, the beverage was no longer imported into China, as it was perceived to be a symbol of decadent Western culture and the capitalist lifestyle. Importation and sales of the beverage resumed in 1979, after diplomatic relations between the United States and China were restored.

There are some consumer boycotts of Coca-Cola in Arab countries due to Coke's early investment in Israel during the Arab League boycott of Israel (its competitor Pepsi stayed out of Israel). Mecca-Cola and Pepsi are popular alternatives in the Middle East.

A Coca-Cola fountain dispenser (officially a Fluids Generic Bioprocessing Apparatus or FGBA) was developed for use on the Space Shuttle as a test bed to determine if carbonated beverages can be produced from separately stored carbon dioxide, water, and flavored syrups and determine if the resulting fluids can be made available for consumption without bubble nucleation and resulting foam formation. FGBA-1 flew on STS-63 in 1995 and dispensed pre-mixed beverages, followed by FGBA-2 on STS-77 the next year. The latter mixed COâ, water, and syrup to make beverages. It supplied 1.65Â liters each of Coca-Cola and Diet Coke.

Coca-Cola is sometimes used for the treatment of gastric phytobezoars. In about 50% of cases studied, Coca-Cola alone was found to be effective in gastric phytobezoar dissolution. Unfortunately, this treatment can result in the potential of developing small bowel obstruction in a minority of cases, necessitating surgical intervention.

Criticism of Coca-Cola has arisen from various groups around the world, concerning a variety of issues, including health effects, environmental issues, and business practices. The drink's coca flavoring, and the nickname "Coke", remain a common theme of criticism due to the relationship with the illegal drug cocaine. In 1911, the US government seized 40 barrels and 20 kegs of Coca-Cola syrup in Chattanooga, Tennessee, alleging the caffeine in its drink was "injurious to health", leading to amended food safety legislation.

Beginning in the 1940s, Pepsi started marketing their drinks to African Americans, a niche market that was largely ignored by white-owned manufacturers in the US, and was able to use its anti-racism stance as a selling point, attacking Coke's reluctance to hire blacks and support by the chairman of The Coca-Cola Company for segregationist Governor of Georgia Herman Talmadge. As a result of this campaign, Pepsi's market share as compared to Coca-Cola's shot up dramatically in the 1950s with African American soft-drink consumers three times more likely to purchase Pepsi over Coke.

The Coca-Cola Company, its subsidiaries and products have been subject to sustained criticism by consumer groups, environmentalists, and watchdogs, particularly since the early 2000s. In 2019, BreakFreeFromPlastic named Coca-Cola the single biggest plastic polluter in the world. After 72,541 volunteers collected 476,423 pieces of plastic waste from around where they lived, a total of 11,732 pieces were found to be labeled with a Coca-Cola brand (including the Dasani, Sprite, and Fanta brands) in 37 countries across four continents. At the 2020 World Economic Forum in Davos, Coca-Cola's Head of Sustainability, Bea Perez, said customers like them because they reseal and are lightweight, and "business won't be in business if we don't accommodate consumers."

Coca-Cola Classic is rich in sugar (or sweetners in some countries) especially sucrose, which causes dental caries when consumed regularly. Besides this, the high caloric value of the sugars themselves can contribute to obesity. Both are major health issues in the developed world.

In July 2001, the Coca-Cola company was sued over its alleged use of political far-right wing death squads (the United Self-Defense Forces of Colombia) to kidnap, torture, and kill Colombian bottler workers that were linked with trade union activity. Coca-Cola was sued in a US federal court in Miami by the Colombian food and drink union Sinaltrainal. The suit alleged that Coca-Cola was indirectly responsible for having "contracted with or otherwise directed paramilitary security forces that utilized extreme violence and murdered, tortured, unlawfully detained or otherwise silenced trade union leaders". This sparked campaigns to boycott Coca-Cola in the UK, US, Germany, Italy, and Australia. Javier Correa, the president of Sinaltrainal, said the campaign aimed to put pressure on Coca-Cola "to mitigate the pain and suffering" that union members had suffered.

Speaking from the Coca-Cola company's headquarters in Atlanta, company spokesperson Rafael Fernandez Quiros said "Coca-Cola denies any connection to any human-rights violation of this type" and added "We do not own or operate the plants".





</doc>
<doc id="6693" url="https://en.wikipedia.org/wiki?curid=6693" title="Cofinality">
Cofinality

In mathematics, especially in order theory, the cofinality cf("A") of a partially ordered set "A" is the least of the cardinalities of the cofinal subsets of "A".

This definition of cofinality relies on the axiom of choice, as it uses the fact that every non-empty set of cardinal numbers has a least member. The cofinality of a partially ordered set "A" can alternatively be defined as the least ordinal "x" such that there is a function from "x" to "A" with cofinal image. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.

Cofinality can be similarly defined for a directed set and is used to generalize the notion of a subsequence in a net.


If "A" admits a totally ordered cofinal subset, then we can find a subset "B" which is well-ordered and cofinal in "A". Any subset of "B" is also well-ordered. Two cofinal subsets of "B" with minimal cardinality (i.e. their cardinality is the cofinality of "B") need not be order isomorphic (for example if formula_1, then both formula_2 and formula_3 viewed as subsets of "B" have the countable cardinality of the cofinality of "B" but are not order isomorphic.) But cofinal subsets of "B" with minimal order type will be order isomorphic.

The cofinality of an ordinal Î± is the smallest ordinal Î´ which is the order type of a cofinal subset of Î±. The cofinality of a set of ordinals or any other well-ordered set is the cofinality of the order type of that set.

Thus for a limit ordinal Î±, there exists a Î´-indexed strictly increasing sequence with limit Î±. For example, the cofinality of ÏÂ² is Ï, because the sequence ÏÂ·"m" (where "m" ranges over the natural numbers) tends to ÏÂ²; but, more generally, any countable limit ordinal has cofinality Ï. An uncountable limit ordinal may have either cofinality Ï as does Ï or an uncountable cofinality.

The cofinality of 0 is 0. The cofinality of any successor ordinal is 1. The cofinality of any nonzero limit ordinal is an infinite regular cardinal.

A regular ordinal is an ordinal which is equal to its cofinality. A singular ordinal is any ordinal which is not regular.

Every regular ordinal is the initial ordinal of a cardinal. Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the axiom of choice, formula_4 is regular for each Î±. In this case, the ordinals 0, 1, formula_5, formula_6, and formula_7 are regular, whereas 2, 3, formula_8, and Ï are initial ordinals which are not regular.

The cofinality of any ordinal "Î±" is a regular ordinal, i.e. the cofinality of the cofinality of "Î±" is the same as the cofinality of "Î±". So the cofinality operation is idempotent.

If Îº is an infinite cardinal number, then cf(Îº) is the least cardinal such that there is an unbounded function from cf(Îº) to Îº; cf(Îº) is also the cardinality of the smallest set of strictly smaller cardinals whose sum is Îº; more precisely

That the set above is nonempty comes from the fact that

i.e. the disjoint union of Îº singleton sets. This implies immediately that cf(Îº) â¤ Îº.
The cofinality of any totally ordered set is regular, so one has cf(Îº) = cf(cf(Îº)).

Using KÃ¶nig's theorem, one can prove Îº < Îº and Îº < cf(2) for any infinite cardinal Îº.

The last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,

the ordinal number Ï being the first infinite ordinal, so that the cofinality of formula_12 is card(Ï) = formula_13. (In particular, formula_12 is singular.) Therefore,

Generalizing this argument, one can prove that for a limit ordinal Î´

On the other hand, if the axiom of choice holds, then for a successor or zero ordinal Î´




</doc>
<doc id="6695" url="https://en.wikipedia.org/wiki?curid=6695" title="Citadel">
Citadel

A citadel is the core fortified area of a town or city. It may be a castle, fortress, or fortified center. The term is a diminutive of "city" and thus means "little city", so called because it is a smaller part of the city of which it is the defensive core. Ancient Sparta had a citadel, as did many other Greek cities and towns.

In a fortification with bastions, the citadel is the strongest part of the system, sometimes well inside the outer walls and bastions, but often forming part of the outer wall for the sake of economy. It is positioned to be the last line of defense, should the enemy breach the other components of the fortification system. The functions of the police and the army, as well as the army barracks were developed in the citadel.

Some of the oldest known structures which have served as citadels were built by the Indus Valley Civilisation, where citadels represented a centralised authority. Citadels in Indus Valley were almost 12 meters tall. The purpose of these structures, however, remains debated. Though the structures found in the ruins of Mohenjo-daro were walled, it is far from clear that these structures were defensive against enemy attacks. Rather, they may have been built to divert flood waters.

Several settlements in Anatolia, including the Assyrian city of KaneÅ¡ in modern-day KÃ¼ltepe, featured citadels. KaneÅ¡' citadel contained the city's palace, temples, and official buildings. The citadel of the Greek city of Mycenae was built atop a highly-defensible rectangular hill and was later surrounded by walls in order to increase its defensive capabilities.

In Ancient Greece, the Acropolis (literally: "high city"), placed on a commanding eminence, was important in the life of the people, serving as a refuge and stronghold in peril and containing military and food supplies, the shrine of the god and a royal palace. The most well known is the Acropolis of Athens, but nearly every Greek city-state had one â the Acrocorinth famed as a particularly strong fortress. In a much later period, when Greece was ruled by the Latin Empire, the same strong points were used by the new feudal rulers for much the same purpose.

In the first millennium BCE, the Castro culture emerged in northwestern Portugal and Spain in the region extending from the Douro river up to the Minho, but soon expanding north along the coast, and east following the river valleys. It was an autochthonous evolution of Atlantic Bronze Age communities. In 2008, the origins of the Celts were attributed to this period by John T. Koch and supported by Barry Cunliffe. The Ave River Valley in Portugal was the core region of this culture, with a large number of small settlements (the "castros"), but also settlements known as citadels or oppida by the Roman conquerors. These had several rings of walls and the Roman conquest of the citadels of Abobriga, Lambriaca and Cinania around 138 B.C. was possible only by prolonged siege. Ruins of notable citadels still exist, and are known by archaeologists as CitÃ¢nia de Briteiros, CitÃ¢nia de Sanfins, Cividade de Terroso and Cividade de Bagunte.

Rebels who took power in the city but with the citadel still held by the former rulers could by no means regard their tenure of power as secure. One such incident played an important part in the history of the Maccabean Revolt against the Seleucid Empire. The Hellenistic garrison of Jerusalem and local supporters of the Seleucids held out for many years in the Acra citadel, making Maccabean rule in the rest of Jerusalem precarious. When finally gaining possession of the place, the Maccabeans pointedly destroyed and razed the Acra, though they constructed another citadel for their own use in a different part of Jerusalem.

At various periods, and particularly during the Middle Ages and the Renaissance, the citadel â having its own fortifications, independent of the city walls â was the last defence of a besieged army, often held after the town had been conquered. Locals and defending armies have often held out citadels long after the city had fallen. For example, in the 1543 Siege of Nice the Ottoman forces led by Barbarossa conquered and pillaged the town and took many captives, but the citadel held out.

In the Philippines, the Ivatan people of the northern islands of Batanes often built fortifications to protect themselves during times of war. They built their so-called "idjangs" on hills and elevated areas. These fortifications were likened to European castles because of their purpose. Usually, the only entrance to the castles would be via a rope ladder that would only be lowered for the villagers and could be kept away when invaders arrived.

In time of war the citadel in many cases afforded retreat to the people living in the areas around the town. However, citadels were often used also to protect a garrison or political power from the inhabitants of the town where it was located, being designed to ensure loyalty from the town that they defended.
For example, during the Dutch Wars of 1664â1667, King Charles II of England constructed a Royal Citadel at Plymouth, an important channel port which needed to be defended from a possible naval attack. However, due to Plymouth's support for the Parliamentarians in the then-recent English Civil War, the Plymouth Citadel was so designed that its guns could fire on the town as well as on the sea approaches.

Barcelona had a great citadel built in 1714 to intimidate the Catalans against repeating their mid-17th- and early-18th-century rebellions against the Spanish central government. In the 19th century, when the political climate had liberalized enough to permit it, the people of Barcelona had the citadel torn down, and replaced it with the city's main central park, the Parc de la Ciutadella. A similar example is the Citadella in Budapest, Hungary.

The attack on the Bastille in the French Revolution â though afterwards remembered mainly for the release of the handful of prisoners incarcerated there â was to considerable degree motivated by the structure's being a Royal citadel in the midst of revolutionary Paris.

Similarly, after Garibaldi's overthrow of Bourbon rule in Palermo, during the 1860 Unification of Italy, Palermo's Castellamare Citadel â symbol of the hated and oppressive former rule â was ceremoniously demolished.

Following Belgium declaring independence in 1830, a Dutch garrison under General David Hendrik ChassÃ© held out in Antwerp Citadel between 1830 and 1832, while the city had already become part of the independent Belgium.

The Siege of the AlcÃ¡zar in the Spanish Civil War, in which the Nationalists held out against a much larger Republican force for two months until relieved, shows that in some cases a citadel can be effective even in modern warfare; a similar case is the Battle of Huáº¿ during the Vietnam war, where a North Vietnamese Army division held the citadel of Huáº¿ for 26 days against roughly their own numbers of much better-equipped US and South Vietnamese troops.

The Citadelle of QuÃ©bec (construction started 1673, completed 1820) still survives as the largest citadel still in official military operation in North America. It is home to the Royal 22nd Regiment of the Canadian Army and forms part of the Ramparts of Quebec City dating back to 1620s.

Since the mid 20th century, citadels commonly enclose military command and control centres, rather than cities or strategic points of defense on the boundaries of a country. These modern citadels are built to protect the command center from heavy attacks, such as aerial or nuclear bombardment. The military citadels under London in the UK, including the massive underground complex Pindar beneath the Ministry of Defence, are examples, as is the Cheyenne Mountain nuclear bunker in the US.

On armored warships, the heavily armored section of the ship that protects the ammunition and machinery spaces is called the armored citadel.

A modern naval interpretation refers to the heaviest protected part of the hull as "the vitals", and the citadel is the semi-armoured freeboard above the vitals. Generally Anglo-American and German language follow this while Russian sources/language refer to "the vitals" as ÑÐ¸ÑÐ°Ð´ÐµÐ» "tsitadel". Likewise Russian literature often refers to the turret of a tank as the 'tower'.

The safe room on a ship is also called a citadel.



</doc>
<doc id="6696" url="https://en.wikipedia.org/wiki?curid=6696" title="Chain mail">
Chain mail

Chain mail (often just mail or sometimes chainmail) is a type of armour consisting of small metal rings linked together in a pattern to form a mesh. It was generally in common military use between the 3rd century BC and the 14th century AD. A coat of this armour is often referred to as a hauberk, and sometimes a byrnie.

The earliest examples of surviving mail were found in the Carpathian Basin at a burial in HornÃ½ Jatov, Slovakia dated at 3rd century BC, and in a chieftain's burial located in CiumeÈti, Romania. Its invention is commonly credited to the Celts, but there are examples of Etruscan pattern mail dating from at least the 4th century BC. Mail may have been inspired by the much earlier scale armour. Mail spread to North Africa, West Africa, the Middle East, Central Asia, India, Tibet, South East Asia, and Japan.

Herodotus wrote that the ancient Persians wore scale armour, but mail is also distinctly mentioned in the Avesta, the ancient holy scripture of the Persian religion of Zoroastrianism that was founded by the prophet Zoroaster in the 5th century BC.

Mail continues to be used in the 21st century as a component of stab-resistant body armour, cut-resistant gloves for butchers and woodworkers, shark-resistant wetsuits for defense against shark bites, and a number of other applications.

The origins of the word "mail" are not fully known. One theory is that it originally derives from the Latin word "macula", meaning "spot" or "opacity" (as in macula of retina). Another theory relates the word to the old French "maillier", meaning "to hammer" (related to the modern English word "malleable"). In modern French, "maille" refers to a loop or stitch. The Arabic words "burnus", , a burnoose; a hooded cloak, also a chasuble (worn by Coptic priests) and "barnaza", , to bronze, suggest an Arabic influence for the Carolingian armour known as "byrnie" (see below).

The first attestations of the word "mail" are in Old French and Anglo-Norman: "maille", "maile", or "male" or other variants, which became "mailye", "maille", "maile", "male", or "meile" in Middle English.
The modern usage of terms for mail armour is highly contested in popular and, to a lesser degree, academic culture. Medieval sources referred to armour of this type simply as "mail"; however, "chain-mail" has become a commonly used, if incorrect, neologism coined no later than 1786, appearing in Francis Grose's "A Treatise on Ancient Armour and Weapons", and brought to popular attention no later than 1822 in Sir Walter Scott's novel "The Fortunes of Nigel". Since then the word "mail" has been commonly, if incorrectly, applied to other types of armour, such as in "plate-mail" (first attested in Grose's Treatise in 1786). The more correct term is "plate armour".

Civilizations that used mail invented specific terms for each garment made from it. The standard terms for European mail armour derive from French: leggings are called chausses, a hood is a mail coif, and mittens, mitons. A mail collar hanging from a helmet is a camail or aventail. A shirt made from mail is a hauberk if knee-length and a haubergeon if mid-thigh length. A layer (or layers) of mail sandwiched between layers of fabric is called a jazerant.

A waist-length coat in medieval Europe was called a byrnie, although the exact construction of a byrnie is unclear, including whether it was constructed of mail or other armour types. Noting that the byrnie was the "most highly valued piece of armour" to the Carolingian soldier, Bennet, Bradbury, DeVries, Dickie, and Jestice indicate that:

There is some dispute among historians as to what exactly constituted the Carolingian byrnie. Relying... only on artistic and some literary sources because of the lack of archaeological examples, some believe that it was a heavy leather jacket with metal scales sewn onto it. It was also quite long, reaching below the hips and covering most of the arms. Other historians claim instead that the Carolingian byrnie was nothing more than a coat of mail, but longer and perhaps heavier than traditional early medieval mail. Without more certain evidence, this dispute will continue.

The use of mail as battlefield armour was common during the Iron Age and the Middle Ages, becoming less common over the course of the 16th and 17th centuries when plate armour and more advanced firearms were developed. It is believed that the Roman Republic first came into contact with mail fighting the Gauls in Cisalpine Gaul, now Northern Italy. The Roman army adopted the technology for their troops in the form of the lorica hamata which was used as a primary form of armour through the Imperial period.
After the fall of the Western Empire, much of the infrastructure needed to create plate armour diminished. Eventually the word "mail" came to be synonymous with armour. It was typically an extremely prized commodity, as it was expensive and time-consuming to produce and could mean the difference between life and death in a battle. Mail from dead combatants was frequently looted and was used by the new owner or sold for a lucrative price. As time went on and infrastructure improved, it came to be used by more soldiers. The oldest intact mail hauberk still in existence is thought to have been worn by Leopold III, Duke of Austria, who died in 1386 during the Battle of Sempach. Eventually with the rise of the lanced cavalry charge, impact warfare, and high-powered crossbows, mail came to be used as a secondary armour to plate for the mounted nobility.

By the 14th century, articulated plate armour was commonly used to supplement mail. Eventually mail was supplanted by plate for the most part, as it provided greater protection against windlass crossbows, bludgeoning weapons, and lance charges while maintaining most of the mobility of mail. However, it was still widely used by many soldiers as well as brigandines and padded jacks. These three types of armour made up the bulk of the equipment used by soldiers, with mail being the most expensive. It was sometimes more expensive than plate armour. Mail typically persisted longer in less technologically advanced areas such as Eastern Europe but was in use everywhere into the 16th century.

During the late 19th and early 20th century, mail was used as a material for bulletproof vests, most notably by the Wilkinson Sword Company. Results were unsatisfactory; Wilkinson mail worn by the Khedive of Egypt's regiment of "Iron Men" was manufactured from split rings which proved to be too brittle, and the rings would fragment when struck by bullets and aggravate the injury. The riveted mail armour worn by the opposing Sudanese Madhists did not have the same problem but also proved to be relatively useless against the firearms of British forces at the battle of Omdurman. During World War I, Wilkinson Sword transitioned from mail to a lamellar design which was the precursor to the flak jacket.

Also during World War I, a mail fringe, designed by Captain Cruise of the British Infantry, was added to helmets to protect the face. This proved unpopular with soldiers, in spite of being proven to defend against a three-ounce (100 g) shrapnel round fired at a distance of . A protective face mask or splatter mask had a mail veil and was used by early tank crews as a measure against flying steel fragments (spalling) inside the vehicle.

Mail armour was introduced to the Middle East and Asia through the Romans and was adopted by the Sassanid Persians starting in the 3rd century AD, where it was supplemental to the scale and lamellar armour already used. 

Mail was commonly also used as horse armour for cataphracts and heavy cavalry as well as armour for the soldiers themselves. Asian mail could be just as heavy as the European variety and sometimes had prayer symbols stamped on the rings as a sign of their craftsmanship as well as for divine protection. Indeed, mail armour is mentioned in the Quran as being a gift revealed by Allah to David:

21:80 It was We Who taught him the making of coats of mail for your benefit, to guard you from each other's violence: will ye then be grateful? (Yusuf Ali's translation)
From the Abbasid Caliphate, mail was quickly adopted in Central Asia by Timur (Tamerlane) and the Sogdians and by India's Delhi Sultanate. Mail armour was introduced by the Turks in late 12th century and commonly used by Turk and the Mughal and Suri armies where it eventually became the armour of choice in India. Indian mail was constructed with alternating rows of solid links and round riveted links and it was often integrated with plate protection (mail and plate armour). Mail and plate armour was commonly used in India until the Battle of Plassey by the Nawabs of Bengal and the subsequent British conquest of the sub-continent.

The Ottoman Empire and the other Islamic Gunpowders used mail armour as well as mail and plate armour, and it was used in their armies until the 18th century by heavy cavalry and elite units such as the Janissaries. They spread its use into North Africa where it was adopted by Mamluk Egyptians and the Sudanese who produced it until the early 20th century. Ottoman mail was constructed with alternating rows of solid links and round riveted links. The Persians used mail armour as well as mail and plate armour. Persian mail and Ottoman mail were often quite similar in appearance.

Mail was introduced to China when its allies in Central Asia paid tribute to the Tang Emperor in 718 by giving him a coat of "link armour" assumed to be mail. China first encountered the armour in 384 when its allies in the nation of Kuchi arrived wearing "armour similar to chains". Once in China, mail was imported but was not produced widely. Due to its flexibility, comfort, and rarity, it was typically the armour of high-ranking guards and those who could afford the exotic import (to show off their social status) rather than the armour of the rank and file, who used more common brigandine, scale, and lamellar types. However, it was one of the few military products that China imported from foreigners. Mail spread to Korea slightly later where it was imported as the armour of imperial guards and generals.

In Japan mail is called "" which means chain. When the word "kusari" is used in conjunction with an armoured item it usually means that mail makes up the majority of the armour composition. An example of this would be "kusari gusoku" which means chain armour. "Kusari" "", "", "", "", "", shoulder, "", and other armoured clothing were produced, even "" socks.

"" was used in samurai armour at least from the time of the Mongol invasion (1270s) but particularly from the Nambokucho Period (1336â1392). The Japanese used many different weave methods including a square 4-in-1 pattern ("so gusari"), a hexagonal 6-in-1 pattern ("hana gusari") and a European 4-in-1 ("nanban gusari"). The rings of Japanese mail were much smaller than their European counterparts; they would be used in patches to link together plates and to drape over vulnerable areas such as the armpits.

"Riveted kusari" was known and used in Japan. On page 58 of the book "Japanese Arms & Armor: Introduction" by H. Russell Robinson, there is a picture of Japanese riveted kusari, and
this quote from the translated reference of 1800 book, "The Manufacture of Armour and Helmets in Sixteenth-Century Japan", shows that the Japanese not only knew of and used riveted kusari but that they manufactured it as well.

...Â karakuri-namban (riveted namban), with stout links each closed by a rivet. Its invention is credited to Fukushima Dembei Kunitaka, pupil, of Hojo Awa no Kami Ujifusa, but it is also said to be derived directly from foreign models. It is heavy because the links are tinned (biakuro-nagashi) and these are also sharp-edged because they are punched out of iron plate

Butted or split (twisted) links made up the majority of "kusari" links used by the Japanese. Links were either "butted" together meaning that the ends touched each other and were not riveted, or the "kusari" was constructed with links where the wire was turned or twisted two or more times; these split links are similar to the modern split ring commonly used on keychains. The rings were lacquered black to prevent rusting, and were always stitched onto a backing of cloth or leather. The kusari was sometimes concealed entirely between layers of cloth.

"Kusari gusoku" or chain armour was commonly used during the Edo period 1603 to 1868 as a stand-alone defense. According to George Cameron Stone

Entire suits of mail "kusari gusoku" were worn on occasions, sometimes under the ordinary clothing

Ian Bottomley in his book "Arms and Armor of the Samurai: The History of Weaponry in Ancient Japan" shows a picture of a kusari armour and mentions "" (chain jackets) with detachable arms being worn by samurai police officials during the Edo period. The end of the samurai era in the 1860s, along with the 1876 ban on wearing swords in public, marked the end of any practical use for mail and other armour in Japan. Japan turned to a conscription army and uniforms replaced armour.

Mail armour provided an effective defense against slashing blows by edged weapons and some forms of penetration by many thrusting and piercing weapons; in fact, a study conducted at the Royal Armouries at Leeds concluded that "it is almost impossible to penetrate using any conventional medieval weapon". Generally speaking, mail's resistance to weapons is determined by four factors: linkage type (riveted, butted, or welded), material used (iron versus bronze or steel), weave density (a tighter weave needs a thinner weapon to surpass), and ring thickness (generally ranging from 18 to 14 gauge (1.02â1.63Â mm diameter) wire in most examples). Mail, if a warrior could afford it, provided a significant advantage when combined with competent fighting techniques.

When the mail was not riveted, a thrust from most sharp weapons could penetrate it. However, when mail was riveted, only a strong well-placed thrust from certain spears, or thin or dedicated mail-piercing swords like the estoc could penetrate, and a pollaxe or halberd blow could break through the armour. Strong projectile weapons such as stronger self bows, recurve bows, and crossbows could also penetrate riveted mail. Some evidence indicates that during armoured combat, the intention was to actually get around the armour rather than through itâaccording to a study of skeletons found in Visby, Sweden, a majority of the skeletons showed wounds on less well protected legs. Although mail was a formidable protection, due to longswords getting more tapered as time progressed, mail worn under plate armour (and stand-alone mail as well) could be penetrated by the conventional weaponry of another knight.

The flexibility of mail meant that a blow would often injure the wearer, potentially causing serious bruising or fractures, and it was a poor defence against head trauma. Mail-clad warriors typically wore separate rigid helms over their mail coifs for head protection. Likewise, blunt weapons such as maces and warhammers could harm the wearer by their impact without penetrating the armour; usually a soft armour, such as gambeson, was worn under the hauberk. Medieval surgeons were very well capable of setting and caring for bone fractures resulting from blunt weapons. With the poor understanding of hygiene, however, cuts that could get infected were much more of a problem. Thus mail armour proved to be sufficient protection in most situations.

Several patterns of linking the rings together have been known since ancient times, with the most common being the 4-to-1 pattern (where each ring is linked with four others). In Europe, the 4-to-1 pattern was completely dominant. Mail was also common in East Asia, primarily Japan, with several more patterns being utilised and an entire nomenclature developing around them.

Historically, in Europe, from the pre-Roman period on, the rings composing a piece of mail would be riveted closed to reduce the chance of the rings splitting open when subjected to a thrusting attack or a hit by an arrow.

Up until the 14th century European mail was made of alternating rows of round riveted rings and solid rings. Sometime during the 14th century European mail makers started to transition from round rivets to wedge shaped rivets but continued using alternating rows of solid rings. Eventually European mail makers stopped using solid rings and almost all European mail was made from wedge riveted rings only with no solid rings. Both were commonly made of wrought iron, but some later pieces were made of heat-treated steel. Wire for the riveted rings was formed by either of two methods. One was to hammer out wrought iron into plates and cut or slit the plates. These thin pieces were then pulled through a draw plate repeatedly until the desired diameter was achieved. Waterwheel powered drawing mills are pictured in several period manuscripts. Another method was to simply forge down an iron billet into a rod and then proceed to draw it out into wire. The solid links would have been made by punching from a sheet. Guild marks were often stamped on the rings to show their origin and craftsmanship. Forge welding was also used to create solid links, but there are few possible examples known; the only well documented example from Europe is that of the camail (mail neck-defence) of the 7th century Coppergate helmet. Outside of Europe this practice was more common such as "theta" links from India. Very few examples of historic butted mail have been found and it is generally accepted that butted mail was never in wide use historically except in Japan where mail ("kusari") was commonly made from "butted" links. Butted link mail was also used by the Moros of the Philippines in their mail and plate armours.

Mail is used as protective clothing for butchers against meat-packing equipment. Workers may wear up to of mail under their white coats. Butchers also commonly wear a single mail glove to protect themselves from self-inflicted injury while cutting meat, as do many oyster shuckers.

Scuba divers sometimes use mail to protect them from sharkbite, as do animal control officers for protection against the animals they handle. In 1980 marine biologist Jeremiah Sullivan patented his design for Neptunic full coverage chain mail shark resistant suits which he had developed for close encounters with sharks.
Shark expert and underwater filmmaker Valerie Taylor was among the first to develop and test shark suits in 1979 while diving with sharks.

Mail is widely used in industrial settings as shrapnel guards and splash guards in metal working operations.

Electrical applications for mail include RF leakage testing and being worn as a faraday cage suit by tesla coil enthusiasts and high voltage electrical workers.

Conventional textile-based ballistic vests are designed to stop soft-nosed bullets but offer little defense from knife attacks. Knife-resistant armour is designed to defend against knife attacks; some of these use layers of metal plates, mail and metallic wires.

Many historical reenactment groups, especially those whose focus is Antiquity or the Middle Ages, commonly use mail both as practical armour and for costuming. Mail is especially popular amongst those groups which use steel weapons. A modern hauberk made from 1.5Â mm diameter wire with 10Â mm inner diameter rings weighs roughly and contains 15,000â45,000 rings.

One of the drawbacks of mail is the uneven weight distribution; the stress falls mainly on shoulders. Weight can be better distributed by wearing a belt over the mail, which provides another point of support.

Mail worn today for re-enactment and recreational use can be made in a variety of styles and materials. Most recreational mail today is made of butted links which are galvanised or stainless steel. This is historically inaccurate but is much less expensive to procure and especially to maintain than historically accurate reproductions. Mail can also be made of titanium, aluminium, bronze, or copper. Riveted mail offers significantly better protection ability as well as historical accuracy than mail constructed with butted links. Riveted mail can be more labour-intensive and expensive to manufacture. Japanese mail ("kusari") is one of the few historically correct examples of mail being constructed with such "butted links".

Mail remained in use as a decorative and possibly high-status symbol with military overtones long after its practical usefulness had passed. It was frequently used for the epaulettes of military uniforms. It is still used in this form by the British Territorial Army.

Mail has applications in sculpture and jewellery, especially when made out of precious metals or colourful anodized metals. Mail artwork includes headdresses, decorative wall hangings, ornaments, chess sets, macramÃ©, and jewelry. For these non-traditional applications, hundreds of patterns (commonly referred to as "weaves") have been invented.

Large-linked mail is occasionally used as a fetish clothing material, with the large links intended to reveal â in part â the body beneath them.

In some films, knitted string spray-painted with a metallic paint is used instead of actual mail in order to cut down on cost (an example being "Monty Python and the Holy Grail", which was filmed on a very small budget). Films more dedicated to costume accuracy often use ABS plastic rings, for the lower cost and weight. Such ABS mail coats were made for "The Lord of the Rings" film trilogy, in addition to many metal coats. The metal coats are used rarely because of their weight, except in close-up filming where the appearance of ABS rings is distinguishable. A large scale example of the ABS mail used in the "Lord of the Rings" can be seen in the entrance to the Royal Armouries museum in Leeds in the form of a large curtain bearing the logo of the museum. It was acquired from the makers of the film's armour, Weta Workshop, when the museum hosted an exhibition of WETA armour from their films. For the film "Mad Max Beyond Thunderdome", Tina Turner is said to have worn actual mail and she complained how heavy this was. "Game of Thrones" makes use of mail, notably during the "Red Wedding" scene.


Typically worn under mail armour if thin or over mail armour if thick:

Can be worn over mail armour:

Others:



</doc>
<doc id="6697" url="https://en.wikipedia.org/wiki?curid=6697" title="Cerberus">
Cerberus

In Greek mythology, Cerberus (; "KÃ©rberos" ), often referred to as the hound of Hades, is a multi-headed dog that guards the gates of the Underworld to prevent the dead from leaving. Cerberus was the offspring of the monsters Echidna and Typhon, and is usually described as having three heads, a serpent for a tail, and snakes protruding from multiple parts of his body. Cerberus is primarily known for his capture by Heracles, one of Heracles' twelve labours.

Descriptions of Cerberus vary, including the number of his heads. Cerberus was usually three-headed, though not always. Cerberus had several multi-headed relatives. His father was the multi snake-headed Typhon, and Cerberus was the brother of three other multi-headed monsters, the multi-snake-headed Lernaean Hydra; Orthrus, the two-headed dog who guarded the Cattle of Geryon; and the Chimera, who had three heads: that of a lion, a goat, and a snake. And, like these close relatives, Cerberus was, with only the rare iconographic exception, multi-headed.

In the earliest description of Cerberus, Hesiod's "Theogony" (c. 8th â 7th century BC), Cerberus has fifty heads, while Pindar (c. 522 â c. 443 BC) gave him one hundred heads. However, later writers almost universally give Cerberus three heads. An exception is the Latin poet Horace's Cerberus which has a single dog head, and one hundred snake heads. Perhaps trying to reconcile these competing traditions, Apollodorus's Cerberus has three dog heads and the heads of "all sorts of snakes" along his back, while the Byzantine poet John Tzetzes (who probably based his account on Apollodorus) gives Cerberus fifty heads, three of which were dog heads, the rest being the "heads of other beasts of all sorts".

In art Cerberus is most commonly depicted with two dog heads (visible), never more than three, but occasionally with only one. On one of the two earliest depictions (c. 590â580 BC), a Corinthian cup from Argos (see below), now lost, Cerberus was shown as a normal single-headed dog. The first appearance of a three-headed Cerberus occurs on a mid-sixth-century BC Laconian cup (see below).

Horace's many snake-headed Cerberus followed a long tradition of Cerberus being part snake. This is perhaps already implied as early as in Hesiod's "Theogony", where Cerberus' mother is the half-snake Echidna, and his father the snake-headed Typhon. In art Cerberus is often shown as being part snake, for example the lost Corinthian cup showed snakes protruding from Cerberus' body, while the mid sixth-century BC Laconian cup gives Cerberus a snake for a tail. In the literary record, the first certain indication of Cerberus' serpentine nature comes from the rationalized account of Hecataeus of Miletus (fl. 500â494 BC), who makes Cerberus a large poisonous snake. Plato refers to Cerberus' composite nature, and Euphorion of Chalcis (3rd century BC) describes Cerberus as having multiple snake tails, and presumably in connection to his serpentine nature, associates Cerberus with the creation of the poisonous aconite plant. Virgil has snakes writhe around Cerberus' neck, Ovid's Cerberus has a venomous mouth, necks "vile with snakes", and "hair inwoven with the threatening snake", while Seneca gives Cerberus a mane consisting of snakes, and a single snake tail.

Cerberus was given various other traits. According to Euripides, Cerberus not only had three heads but three bodies, and according to Virgil he had multiple backs. Cerberus ate raw flesh (according to Hesiod), had eyes which flashed fire (according to Euphorion), a three-tongued mouth (according to Horace), and acute hearing (according to Seneca).

Cerberus' only mythology concerns his capture by Heracles. As early as Homer we learn that Heracles was sent by Eurystheus, the king of Tiryns, to bring back Cerberus from Hades the king of the underworld. According to Apollodorus, this was the twelfth and final labour imposed on Heracles. In a fragment from a lost play "Pirithous", (attributed to either Euripides or Critias) Heracles says that, although Eurystheus commanded him to bring back Cerberus, it was not from any desire to see Cerberus, but only because Eurystheus thought that the task was impossible.

Heracles was aided in his mission by his being an initiate of the Eleusinian Mysteries. Euripides has his initiation being "lucky" for Heracles in capturing Cerberus. And both Diodorus Siculus and Apollodorus say that Heracles was initiated into the Mysteries, in preparation for his descent into the underworld. According to Diodorus, Heracles went to Athens, where Musaeus, the son of Orpheus, was in charge of the initiation rites, while according to Apollodorus, he went to Eumolpus at Eleusis.

Heracles also had the help of Hermes, the usual guide of the underworld, as well as Athena. In the "Odyssey", Homer has Hermes and Athena as his guides. And Hermes and Athena are often shown with Heracles on vase paintings depicting Cerberus' capture. By most accounts, Heracles made his descent into the underworld through an entrance at Tainaron, the most famous of the various Greek entrances to the underworld. The place is first mentioned in connection with the Cerberus story in the rationalized account of Hecataeus of Miletus (fl. 500â494 BC), and Euripides, Seneca, and Apolodorus, all have Heracles descend into the underworld there. However Xenophon reports that Heracles was said to have descended at the Acherusian Chersonese near Heraclea Pontica, on the Black Sea, a place more usually associated with Heracles' exit from the underworld (see below). Heraclea, founded c. 560 BC, perhaps took its name from the association of its site with Heracles' Cerberian exploit.

While in the underworld, Heracles met the heroes Theseus and Pirithous, where the two companions were being held prisoner by Hades for attempting to carry off Hades' wife Persephone. Along with bringing back Cerberus, Heracles also managed (usually) to rescue Theseus, and in some versions Pirithous as well. According to Apollodorus, Heracles found Theseus and Pirithous near the gates of Hades, bound to the "Chair of Forgetfulness, to which they grew and were held fast by coils of serpents", and when they saw Heracles, "they stretched out their hands as if they should be raised from the dead by his might", and Heracles was able to free Theseus, but when he tried to raise up Pirithous, "the earth quaked and he let go."

The earliest evidence for the involvement of Theseus and Pirithous in the Cerberus story, is found on a shield-band relief (c. 560 BC) from Olympia, where Theseus and Pirithous (named) are seated together on a chair, arms held out in supplication, while Heracles approaches, about to draw his sword. The earliest literary mention of the rescue occurs in Euripides, where Heracles saves Theseus (with no mention of Pirithous). In the lost play "Pirithous", both heroes are rescued, while in the rationalized account of Philochorus, Heracles was able to rescue Theseus, but not Pirithous. In one place Diodorus says Heracles brought back both Theseus and Pirithous, by the favor of Persephone, while in another he says that Pirithous remained in Hades, or according to "some writers of myth" that neither Theseus, nor Pirithous returned. Both are rescued in Hyginus.

There are various versions of how Heracles accomplished Cerberus' capture. According to Apollodorus, Heracles asked Hades for Cerberus, and Hades told Heracles he would allow him to take Cerberus only if he "mastered him without the use of the weapons which he carried", and so, using his lion-skin as a shield, Heracles squeezed Cerberus around the head until he submitted.

In some early sources Cerberus' capture seems to involve Heracles fighting Hades. Homer ("Iliad" 5.395â397) has Hades injured by an arrow shot by Heracles. A scholium to the "Iliad" passage, explains that Hades had commanded that Heracles "master Cerberus without shield or Iron". Heracles did this, by (as in Apollodorus) using his lion-skin instead of his shield, and making stone points for his arrows, but when Hades still opposed him, Heracles shot Hades in anger. Consistent with the no iron requirement, on an early-sixth-century BC lost Corinthian cup, Heracles is shown attacking Hades with a stone, while the iconographic tradition, from c. 560 BC, often shows Heracles using his wooden club against Cerberus.

Euripides, has Amphitryon ask Heracles: "Did you conquer him in fight, or receive him from the goddess [i.e. Persephone]? To which, Heracles answers: "In fight", and the "Pirithous" fragment says that Heracles "overcame the beast by force". However, according to Diodorus, Persephone welcomed Heracles "like a brother" and gave Cerberus "in chains" to Heracles. Aristophanes, has Heracles seize Cerberus in a stranglehold and run off, while Seneca has Heracles again use his lion-skin as shield, and his wooden club, to subdue Cerberus, after which a quailing Hades and Persephone, allow Heracles to lead a chained and submissive Cerberus away. Cerberus is often shown being chained, and Ovid tells that Heracles dragged the three headed Cerberus with chains of adamant.

There were several locations which were said to be the place where Heracles brought up Cerberus from the underworld. The geographer Strabo (63/64 BC â c. AD 24) reports that "according to the myth writers" Cerberus was brought up at Tainaron, the same place where Euripides has Heracles enter the underworld. Seneca has Heracles enter and exit at Tainaron. Apollodorus, although he has Heracles enter at Tainaron, has him exit at Troezen. The geographer Pausanias tells us that there was a temple at Troezen with "altars to the gods said to rule under the earth", where it was said that, in addition to Cerberus being "dragged" up by Heracles, Semele was supposed to have been brought up out of the underworld by Dionysus.

Another tradition had Cerberus brought up at Heraclea Pontica (the same place which Xenophon had earlier associated with Heracles' descent) and the cause of the poisonous plant aconite which grew there in abundance. Herodorus of Heraclea and Euphorion said that when Heracles brought Cerberus up from the underworld at Heraclea, Cerberus "vomited bile" from which the aconite plant grew up. Ovid, also makes Cerberus the cause of the poisonous aconite, saying that on the "shores of Scythia", upon leaving the underworld, as Cerberus was being dragged by Heracles from a cave, dazzled by the unaccustomed daylight, Cerberus spewed out a "poison-foam", which made the aconite plants growing there poisonous. Seneca's Cerberus too, like Ovid's, reacts violently to his first sight of daylight. Enraged, the previously submissive Cerberus struggles furiously, and Heracles and Theseus must together drag Cerberus into the light.

Pausanias reports that according to local legend Cerberus was brought up through a chasm in the earth dedicated to Clymenus (Hades) next to the sanctuary of Chthonia at Hermione, and in Euripides' "Heracles", though Euripides does not say that Cerberus was brought out there, he has Cerberus kept for a while in the "grove of Chthonia" at Hermione. Pausanias also mentions that at Mount Laphystion in Boeotia, that there was a statue of Heracles Charops ("with bright eyes"), where the Boeotians said Heracles brought up Cerberus. Other locations which perhaps were also associated with Cerberus being brought out of the underworld include, Hierapolis, Thesprotia, and Emeia near Mycenae.

In some accounts, after bringing Cerberus up from the underworld, Heracles paraded the captured Cerberus through Greece. Euphorion has Heracles lead Cerberus through Midea in Argolis, as women and children watch in fear, and Diodorus Siculus says of Cerberus, that Heracles "carried him away to the amazement of all and exhibited him to men." Seneca has Juno complain of Heracles "highhandedly parading the black hound through Argive cities" and Heracles greeted by laurel-wreathed crowds, "singing" his praises.

Then, according to Apollodorus, Heracles showed Cerberus to Eurystheus, as commanded, after which he returned Cerberus to the underworld. However, according to Hesychius of Alexandria, Cerberus escaped, presumably returning to the underworld on his own.

The earliest mentions of Cerberus (c. 8th â 7th century BC) occur in Homer's "Iliad" and "Odyssey", and Hesiod's "Theogony". Homer does not name or describe Cerberus, but simply refers to Heracles being sent by Eurystheus to fetch the "hound of Hades", with Hermes and Athena as his guides, and, in a possible reference to Cerberus' capture, that Heracles shot Hades with an arrow. According to Hesiod, Cerberus was the offspring of the monsters Echidna and Typhon, was fifty-headed, ate raw flesh, and was the "brazen-voiced hound of Hades", who fawns on those that enter the house of Hades, but eats those who try to leave.

Stesichorus (c. 630 â 555 BC) apparently wrote a poem called "Cerberus", of which virtually nothing remains. However the early-sixth-century BC-lost Corinthian cup from Argos, which showed a single head, and snakes growing out from many places on his body, was possibly influenced by Stesichorus' poem. The mid-sixth-century BC cup from Laconia gives Cerberus three heads and a snake tail, which eventually becomes the standard representation.

Pindar (c. 522 â c. 443 BC) apparently gave Cerberus one hundred heads. Bacchylides (5th century BC) also mentions Heracles bringing Cerberus up from the underworld, with no further details. Sophocles (c. 495 â c. 405 BC), in his "Women of Trachis", makes Cerberus three-headed, and in his "Oedipus at Colonus", the Chorus asks that Oedipus be allowed to pass the gates of the underworld undisturbed by Cerberus, called here the "untamable Watcher of Hades". Euripides (c. 480 â 406 BC) describes Cerberus as three-headed, and three-bodied, says that Heracles entered the underworld at Tainaron, has Heracles say that Cerberus was not given to him by Persephone, but rather he fought and conquered Cerberus, "for I had been lucky enough to witness the rites of the initiated", an apparent reference to his initiation into the Eleusinian Mysteries, and says that the capture of Cerberus was the last of Heracles' labors. The lost play "Pirthous" (attributed to either Euripides or his late contemporary Critias) has Heracles say that he came to the underworld at the command of Eurystheus, who had ordered him to bring back Cerberus alive, not because he wanted to see Cerberus, but only because Eurystheus thought Heracles would not be able to accomplish the task, and that Heracles "overcame the beast" and "received favour from the gods".
Plato (c. 425 â 348 BC) refers to Cerberus' composite nature, citing Cerberus, along with Scylla and the Chimera, as an example from "ancient fables" of a creature composed of many animal forms "grown together in one". Euphorion of Chalcis (3rd century BC) describes Cerberus as having multiple snake tails, and eyes that flashed, like sparks from a blacksmith's forge, or the volcaninc Mount Etna. From Euphorion, also comes the first mention of a story which told that at Heraclea Pontica, where Cerberus was brought out of the underworld, by Heracles, Cerberus "vomited bile" from which the poisonous aconite plant grew up.

According to Diodorus Siculus (1st century BC), the capture of Cerberus was the eleventh of Heracles' labors, the twelfth and last being stealing the Apples of the Hesperides. Diodorus says that Heracles thought it best to first go to Athens to take part in the Eleusinian Mysteries, "Musaeus, the son of Orpheus, being at that time in charge of the initiatory rites", after which, he entered into the underworld "welcomed like a brother by Persephone", and "receiving the dog Cerberus in chains he carried him away to the amazement of all and exhibited him to men."

In Virgil's "Aeneid" (1st century BC), Aeneas and the Sibyl encounter Cerberus in a cave, where he "lay at vast length", filling the cave "from end to end", blocking the entrance to the underworld. Cerberus is described as "triple-throated", with "three fierce mouths", multiple "large backs", and serpents writhing around his neck. The Sybyl throws Cerberus a loaf laced with honey and herbs to induce sleep, enabling Aeneas to enter the underworld, and so apparently for Virgilâcontradicting HesiodâCerberus guarded the underworld against entrance. Later Virgil describes Cerberus, in his bloody cave, crouching over half-gnawed bones. In his "Georgics", Virgil refers to Cerberus, his "triple jaws agape" being tamed by Orpheus' playing his lyre.

Horace (65 â 8 BC) also refers to Cerberus yielding to Orphesus' lyre, here Cerberus has a single dog head, which "like a Fury's is fortified by a hundred snakes", with a "triple-tongued mouth" oozing "fetid breath and gore".

Ovid (43 BC â AD 17/18) has Cerberus' mouth produce venom, and like Euphorion, makes Cerberus the cause of the poisonous plant aconite. According to Ovid, Heracles dragged Cerberus from the underworld, emerging from a cave "where 'tis fabled, the plant grew / on soil infected by Cerberian teeth", and dazzled by the daylight, Cerberus spewed out a "poison-foam", which made the aconite plants growing there poisonous.
Seneca, in his tragedy "Hercules Furens" gives a detailed description of Cerberus and his capture.
Seneca's Cerberus has three heads, a mane of snakes, and a snake tail, with his three heads being covered in gore, and licked by the many snakes which surround them, and with hearing so acute that he can hear "even ghosts". Seneca has Heracles use his lion-skin as shield, and his wooden club, to beat Cerberus into submission, after which Hades and Persephone, quailing on their thrones, let Heracles lead a chained and submissive Cerberus away. But upon leaving the underworld, at his first sight of daylight, a frightened Cerberus struggles furiously, and Heracles, with the help of Theseus (who had been held captive by Hades, but released, at Heracles' request) drag Cerberus into the light. Seneca, like Diodorus, has Heracles parade the captured Cerberus through Greece.

Apollodorus' Cerberus has three dog-heads, a serpent for a tail, and the heads of many snakes on his back. According to Apollodorus, Heracles' twelfth and final labor was to bring back Cerberus from Hades. Heracles first went to Eumolpus to be initiated into the Eleusinian Mysteries. Upon his entering the underworld, all the dead flee Heracles except for Meleager and the Gorgon Medusa. Heracles drew his sword against Medusa, but Hermes told Heracles that the dead are mere "empty phantoms". Heracles asked Hades (here called Pluto) for Cerberus, and Hades said that Heracles could take Cerberus provided he was able to subdue him without using weapons. Heracles found Cerberus at the gates of Acheron, and with his arms around Cerberus, though being bitten by Cerberus' serpent tail, Heracles squeezed until Cerberus submitted. Heracles carried Cerberus away, showed him to Eurystheus, then returned Cerberus to the underworld.

In an apparently unique version of the story, related by the sixth-century AD Pseudo-Nonnus, Heracles descended into Hades to abduct Persephone, and killed Cerberus on his way back up.

The capture of Cerberus was a popular theme in ancient Greek and Roman art. The earliest depictions date from the beginning of the sixth century BC. One of the two earliest depictions, a Corinthian cup (c. 590â580 BC) from Argos (now lost), shows a naked Heracles, with quiver on his back and bow in his right hand, striding left, accompanied by Hermes. Heracles threatens Hades with a stone, who flees left, while a goddess, perhaps Persephone or possibly Athena, standing in front of Hades' throne, prevents the attack. Cerberus, with a single canine head and snakes rising from his head and body, flees right. On the far right a column indicates the entrance to Hades' palace. Many of the elements of this sceneâ Hermes, Athena, Hades, Persephone, and a column or porticoâ are common occurrences in later works. The other earliest depiction, a relief "pithos" fragment from Crete (c. 590â570 BC), is thought to show a single lion-headed Cerberus with a snake (open-mouthed) over his back being led to the right.

A mid-sixth-century BC Laconian cup by the Hunt Painter adds several new features to the scene which also become common in later works: three heads, a snake tail, Cerberus' chain and Heracles' club. Here Cerberus has three canine heads, is covered by a shaggy coat of snakes, and has a tail which ends in a snake head. He is being held on a chain leash by Heracles who holds his club raised over head.

In Greek art, the vast majority of depictions of Heracles and Cerberus occur on Attic vases. Although the lost Corinthian cup shows Cerberus with a single dog head, and the relief "pithos" fragment (c. 590â570 BC) apparently shows a single lion-headed Cerberus, in Attic vase painting Cerberus usually has two dog heads. In other art, as in the Laconian cup, Cerberus is usually three-headed. Occasionally in Roman art Cerberus is shown with a large central lion head and two smaller dog heads on either side.

As in the Corinthian and Laconian cups (and possibly the relief "pithos" fragment), Cerberus is often depicted as part snake. In Attic vase painting, Cerberus is usually shown with a snake for a tail or a tail which ends in the head of a snake. Snakes are also often shown rising from various parts of his body including snout, head, neck, back, ankles, and paws.

Two Attic amphoras from Vulci, one (c. 530â515 BC) by the Bucci Painter (Munich 1493), the other (c. 525â510 BC) by the Andokides painter (Louvre F204), in addition to the usual two heads and snake tail, show Cerberus with a mane down his necks and back, another typical Cerberian feature of Attic vase painting. Andokides' amphora also has a small snake curling up from each of Cerberus' two heads.

Besides this lion-like mane and the occasional lion-head mentioned above, Cerberus was sometimes shown with other leonine features. A pitcher (c. 530â500) shows Cerberus with mane and claws, while a first-century BC sardonyx cameo shows Cerberus with leonine body and paws. In addition, a limestone relief fragment from Taranto (c. 320â300 BC) shows Cerberus with three lion-like heads.

During the second quarter of the 5th century BC the capture of Cerberus disappears from Attic vase painting. After the early third century BC, the subject becomes rare everywhere until the Roman period. In Roman art the capture of Cerberus is usually shown together with other labors. Heracles and Cerberus are usually alone, with Heracles leading Cerberus.

The etymology of Cerberus' name is uncertain. Ogden refers to attempts to establish an Indo-European etymology as "not yet successful". It has been claimed to be related to the Sanskrit word à¤¸à¤°à¥à¤µà¤°à¤¾ "sarvarÄ", used as an epithet of one of the dogs of Yama, from a Proto-Indo-European word *"kÌÃ©rberos", meaning "spotted". Lincoln (1991), among others, critiques this etymology. Lincoln notes a similarity between Cerberus and the Norse mythological dog Garmr, relating both names to a Proto-Indo-European root "*ger-" "to growl" (perhaps with the suffixes "-*m/*b" and "-*r"). However, as Ogden observes, this analysis actually requires "Kerberos" and "Garmr" to be derived from two "different" Indo-European roots (*"ker-" and *"gher-" respectively), and so does not actually establish a relationship between the two names.

Though probably not Greek, Greek etymologies for Cerberus have been offered. An etymology given by Servius (the late-fourth-century commentator on Virgil)âbut rejected by Ogdenâderives Cerberus from the Greek word "creoboros" meaning "flesh-devouring". Another suggested etymology derives Cerberus from "Ker berethrou", meaning "evil of the pit".

At least as early as the 6th century BC, some ancient writers attempted to explain away various fantastical features of Greek mythology; included in these are various rationalized accounts of the Cerberus story. The earliest such account (late 6th century BC) is that of Hecataeus of Miletus. In his account Cerberus was not a dog at all, but rather simply a large venomous snake, which lived on Tainaron. The serpent was called the "hound of Hades" only because anyone bitten by it died immediately, and it was this snake that Heracles brought to Eurystheus. The geographer Pausanias (who preserves for us Hecataeus' version of the story) points out that, since Homer does not describe Cerberus, Hecataeus' account does not necessarily conflict with Homer, since Homer's "Hound of Hades" may not in fact refer to an actual dog.

Other rationalized accounts make Cerberus out to be a normal dog. According to Palaephatus (4th century BC) Cerberus was one of the two dogs who guarded the cattle of Geryon, the other being Orthrus. Geryon lived in a city named Tricranium (in Greek "Tricarenia," "Three-Heads"), from which name both Cerberus and Geryon came to be called "three-headed". Heracles killed Orthus, and drove away Geryon's cattle, with Cerberus following along behind. Molossus, a Mycenaen, offered to buy Cerberus from Eurystheus (presumably having received the dog, along with the cattle, from Heracles). But when Eurystheus refused, Molossus stole the dog and penned him up in a cave in Tainaron. Eurystheus commanded Heracles to find Cerberus and bring him back. After searching the entire Peloponnesus, Heracles found where it was said Cerberus was being held, went down into the cave, and brought up Cerberus, after which it was said: "Heracles descended through the cave into Hades and brought up Cerberus."

In the rationalized account of Philochorus, in which Heracles rescues Theseus, Perithous is eaten by Cerberus. In this version of the story, Aidoneus (i.e., "Hades") is the mortal king of the Molossians, with a wife named Persephone, a daughter named Kore (another name for the goddess Persephone) and a large mortal dog named Cerberus, with whom all suiters of his daughter were required to fight. After having stolen Helen, to be Theseus' wife, Theseus and Perithous, attempt to abduct Kore, for Perithous, but Aidoneus catches the two heroes, imprisons Theseus, and feeds Perithous to Cerberus. Later, while a guest of Aidoneus, Heracles asks Aidoneus to release Theseus, as a favor, which Aidoneus grants.

A 2nd-century AD Greek known as Heraclitus the paradoxographer (not to be confused with the 5th-century BC Greek philosopher Heraclitus) â claimed that Cerberus had two pups that were never away from their father, which made Cerberus appear to be three-headed.

Servius, a medieval commentator on Virgil's "Aeneid", derived Cerberus' name from the Greek word "creoboros" meaning "flesh-devouring" (see above), and held that Cerberus symbolized the corpse-consuming earth, with Heracles' triumph over Cerberus representing his victory over earthly desires. Later the mythographer Fulgentius, allegorizes Cerberus' three heads as representing the three origins of human strife: "nature, cause, and accident", and (drawing on the same flesh-devouring etymology as Servius) as symbolizing "the three agesâinfancy, youth, old age, at which death enters the world." The Byzantine historian and bishop Eusebius wrote that Cerberus was represented with three heads, because the positions of the sun above the earth are three â rising, midday, and setting.

The later Vatican Mythographers repeat and expand upon the traditions of Servius and Fulgentius. All three Vatican Mythographers repeat Servius' derivation of Cerberus' name from "creoboros". The Second Vatican Mythographer repeats (nearly word for word) what Fulgentius had to say about Cerberus, while the Third Vatican Mythographer, in another very similar passage to Fugentius', says (more specifically than Fugentius), that for "the philosophers" Cerberus represented hatred, his three heads symbolizing the three kinds of human hatred: natural, causal, and casual (i.e. accidental).

The Second and Third Vatican Mythographers, note that the three brothers Zeus, Poseidon and Hades each have tripartite insignia, associating Hades' three headed Cerberus, with Zeus' three-forked thunderbolt, and Poseidon's three-pronged trident, while the Third Vatican Mythographer adds that "some philosophers think of Cerberus as the tripartite earth: Asia, Africa, and Europe. This earth, swallowing up bodies, sends souls to Tartarus."

Virgil described Cerberus as "ravenous" ("fame rabida"), and a rapacious Cerberus became proverbial. Thus Cerberus came to symbolize avarice, and so, for example, in Dante's "Inferno," Cerberus is placed in the Third Circle of Hell, guarding over the gluttons, where he "rends the spirits, flays and quarters them," and Dante (perhaps echoing Servius' association of Cerbeus with earth) has his guide Virgil take up handfuls of earth and throw them into Cerberus' "rapacious gullets."

In the constellation Cerberus introduced by Johannes Hevelius in 1687, Cerberus is drawn as a three-headed snake, held in Hercules' hand (previously these stars had been depicted as a branch of the tree on which grew the Apples of the Hesperides).

In 1829 French naturalist Georges Cuvier gave the name "Cerberus" to a genus of Asian snakes, which are commonly called "dog-faced water snakes" in English.



</doc>
<doc id="6698" url="https://en.wikipedia.org/wiki?curid=6698" title="Camel case">
Camel case

Camel case (stylized as camelCase; also known as camel caps or more formally as medial capitals) is the practice of writing phrases without spaces or punctuation, indicating the separation of words with a single capitalized letter, and the first word starting with either case. Common examples include "iPhone", "JavaScript", and "eBay". It is also sometimes used in online usernames such as "johnSmith", and to make multi-word domain names more legible, for example in advertisements.

Camel case is often used as a naming convention in computer programming, but is an ambiguous definition due to the optional capitalization of the very first letter. Some programming styles prefer camel case with the first letter capitalised, others not. For clarity, this article calls the two alternatives upper camel case (initial uppercase letter, also known as Pascal case) and lower camel case (initial lowercase letter, also known as Dromedary case). Some people and organizations, notably Microsoft, use the term "camel case" only for lower camel case. "Pascal case" means only upper camel case.

Camel case is distinct from Title Case, which capitalises all words but retains the spaces between them, and from Tall Man lettering, which uses capitals to emphasize the differences between similar-looking product names such as "predniSONE" and "predniSOLONE". Camel case is also distinct from snake case, which uses underscores interspersed with lowercase letters (sometimes with the first letter capitalized). A combination of snake and camel case (identifiers "Written_Like_This") is recommended in the Ada 95 style guide.

The original name of the practice, used in media studies, grammars and the "Oxford English Dictionary", was "medial capitals". Other synonyms include:


The earliest known occurrence of the term "InterCaps" on Usenet is in an April 1990 post to the group alt.folklore.computers by Avi Rappoport. The earliest use of the name "Camel Case" occurs in 1995, in a post by Newton Love. Love has since said, "With the advent of programming languages having these sorts of constructs, the humpiness of the style made me call it HumpyCase at first, before I settled on CamelCase. I had been calling it CamelCase for years. ... The citation above was just the first time I had used the name on USENET."

The use of medial capitals as a convention in the regular spelling of everyday texts is rare, but is used in some languages as a solution to particular problems which arise when two words or segments are combined.

In Italian, pronouns can be suffixed to verbs, and because the honorific form of second-person pronouns is capitalized, this can produce a sentence like "non ho trovato il tempo di risponderLe" ("I have not found time to answer you" â where "Le" means "to you").

In German, the medial capital letter I, called "Binnen-I", is sometimes used in a word like "StudentInnen" ("students") to indicate that both "Studenten" ("male students") and "Studentinnen" ("female students") are intended simultaneously. However, mid-word capitalisation does not conform to German orthography; the previous example could be correctly written using parentheses as "Student(inn)en", analogous to "congress(wo)man" in English.

In Irish, camel case is used when an inflectional prefix is attached to a proper noun, for example ("in Galway"), from ("Galway"); ("the Scottish person"), from ("Scottish person"); and ("to Ireland"), from ("Ireland"). In recent Scottish Gaelic orthography, a hyphen has been inserted: .

This convention is also used by several written Bantu languages (e.g., "kiSwahili", "Swahili language"; "isiZulu", "Zulu language") and several indigenous languages of Mexico (e.g. Nahuatl, Totonacan, MixeâZoque, and some Oto-Manguean languages).

In Dutch, when capitalizing the digraph "ij", both the letter "I" and the letter "J" are capitalized, for example in the countryname "IJsland" ("Iceland").

In English, medial capitals are usually only found in Scottish or Irish "Mac-" or "Mc-" names, where for example "MacDonald, McDonald," and "Macdonald" are common spelling variants of the same name, and in Anglo-Norman "Fitz-" names, where for example both "FitzGerald" and "Fitzgerald" are found.

In their English style guide "The King's English", first published in 1906, H. W. and F. G. Fowler suggested that medial capitals could be used in triple compound words where hyphens would cause ambiguityâthe examples they give are "KingMark-like" (as against "King Mark-like") and "Anglo-SouthAmerican" (as against "Anglo-South American"). However, they described the system as "too hopelessly contrary to use at present."

In the scholarly transliteration of languages written in other scripts, medial capitals are used in similar situations. For example, in transliterated Hebrew, "ha'Ivri" means "the Hebrew person" or "the Jew" and "b'Yerushalayim" means "in Jerusalem". In Tibetan proper names like "rLobsang", the "r" stands for a prefix glyph in the original script that functions as tone marker rather than a normal letter. Another example is "tsIurku", a Latin transcription of the Chechen term for the capping stone of the characteristic Medieval defensive towers of Chechenia and Ingushetia; the capital letter "I" here denoting a phoneme distinct from the one transcribed as "i".

Medial capitals are traditionally used in abbreviations to reflect the capitalization that the words would have when written out in full, for example in the academic titles PhD or BSc. In German, the names of statutes are abbreviated using embedded capitals, e.g. StGB for Strafgesetzbuch (Criminal Code), PatG for Patentgesetz (Patent Act), BVerfG for Bundesverfassungsgericht (Federal Constitutional Court), or the very common GmbH, for Gesellschaft mit beschrÃ¤nkter Haftung (private limited company). In this context, there can even be three or more camel case capitals, e.g. in TzBfG for Teilzeit- und Befristungsgesetz (Act on Part-Time and Limited Term Occupations). In French, camel case acronyms such as OuLiPo (1960) were favored for a time as alternatives to initialisms.

Camel case is often used to transliterate initialisms into alphabets where two letters may be required to represent a single character of the original alphabet, e.g., DShK from Cyrillic ÐÐ¨Ð. 

The first systematic and widespread use of medial capitals for technical purposes was the notation for chemical formulae invented by the Swedish chemist Jacob Berzelius in 1813. To replace the multitude of naming and symbol conventions used by chemists until that time, he proposed to indicate each chemical element by a symbol of one or two letters, the first one being capitalized. The capitalization allowed formulae like "NaCl" to be written without spaces and still be parsed without ambiguity.

Berzelius' system continues to be used, augmented with three-letter symbols such as "Uue" for unconfirmed or unknown elements and abbreviations for some common substituents (especially in the field of organic chemistry, for instance "Et" for "ethyl-"). This has been further extended to describe the amino acid sequences of proteins and other similar domains.

Since the early 20th century, medial capitals have occasionally been used for corporate names and product trademarks, such as

In the 1970s and 1980s, medial capitals were adopted as a standard or alternative naming convention for multi-word identifiers in several programming languages. The precise origin of the convention in computer programming has not yet been settled. A 1954 conference proceedings occasionally informally referred to IBM's Speedcoding system as "SpeedCo". Christopher Strachey's paper on GPM (1965), shows a program that includes some medial capital identifiers, including "codice_1" and "codice_2".

Multiple-word descriptive identifiers with embedded spaces such as codice_3 or codice_4 cannot be used in most programming languages because the spaces between the words would be parsed as delimiters between tokens. The alternative of running the words together as in codice_5 or codice_6 is difficult to understand and possibly misleading; for example, codice_6 is an English word (able to be charted).

Some early programming languages, notably Lisp (1958) and COBOL (1959), addressed this problem by allowing a hyphen ("-") to be used between words of compound identifiers, as in "END-OF-FILE": Lisp because it worked well with prefix notation (a Lisp parser would not treat a hyphen in the middle of a symbol as a subtraction operator) and COBOL because its operators were individual English words. This convention remains in use in these languages, and is also common in program names entered on a command line, as in Unix.

However, this solution was not adequate for mathematically-oriented languages such as FORTRAN (1955) and ALGOL (1958), which used the hyphen as an infix subtraction operator. FORTRAN ignored blanks altogether, so programmers could use embedded spaces in variable names. However, this feature was not very useful since the early versions of the language restricted identifiers to no more than six characters.

Exacerbating the problem, common punched card character sets of the time were uppercase only and lacked other special characters. It was only in the late 1960s that the widespread adoption of the ASCII character set made both lowercase and the underscore character codice_8 universally available. Some languages, notably C, promptly adopted underscores as word separators, and identifiers such as codice_9 are still prevalent in C programs and libraries (as well as in later languages influenced by C, such as Perl and Python). However, some languages and programmers chose to avoid underscoresâamong other reasons to prevent confusing them with whitespaceâand adopted camel case instead.

Charles Simonyi, who worked at Xerox PARC in the 1970s and later oversaw the creation of Microsoft's Office suite of applications, invented and taught the use of Hungarian Notation, one version of which uses the lowercase letter(s) at the start of a (capitalized) variable name to denote its type. One account claims that the camel case style first became popular at Xerox PARC around 1978, with the Mesa programming language developed for the Xerox Alto computer. This machine lacked an underscore key (whose place was taken by a left arrow "â"), and the hyphen and space characters were not permitted in identifiers, leaving camel case as the only viable scheme for readable multiword names. The PARC Mesa Language Manual (1979) included a coding standard with specific rules for upper and lower camel case that was strictly followed by the Mesa libraries and the Alto operating system. Niklaus Wirth, the inventor of Pascal, came to appreciate camel case during a sabbatical at PARC and used it in Modula, his next programming language.

The Smalltalk language, which was developed originally on the Alto, also uses camel case instead of underscores. This language became quite popular in the early 1980s, and thus may also have been instrumental in spreading the style outside PARC.

Upper camel case is used in Wolfram Language in computer algebraic system Mathematica for predefined identifiers. User defined identifiers should start with a lower case letter. This avoids the conflict between predefined and user defined identifiers both today and in all future versions.

Whatever its origins in the computing field, the convention was used in the names of computer companies and their commercial brands, since the late 1970s â a trend that continues to this day:


In the 1980s and 1990s, after the advent of the personal computer exposed hacker culture to the world, camel case then became fashionable for corporate trade names in non-computer fields as well. Mainstream usage was well established by 1990:


During the dot-com bubble of the late 1990s, the lowercase prefixes "e" (for "electronic") and "i" (for "Internet", "information", "intelligent", etc.) became quite common, giving rise to names like Apple's iMac and the eBox software platform.

In 1998, Dave Yost suggested that chemists use medial capitals to aid readability of long chemical names, e.g. write AmidoPhosphoRibosylTransferase instead of amidophosphoribosyltransferase. This usage was not widely adopted.

Camel case is sometimes used for abbreviated names of certain neighborhoods, e.g. New York City neighborhoods "SoHo" ("So"uth of "Ho"uston Street) and "TriBeCa" ("Tri"angle "Be"low "Ca"nal Street) and San Francisco's "SoMa" ("So"uth of "Ma"rket). Such usages erode quickly, so the neighborhoods are now typically rendered as "Soho", "Tribeca", and "Soma".

Internal capitalization has also been used for other technical codes like HeLa (1983).

The use of medial caps for compound identifiers is recommended by the coding style guidelines of many organizations or software projects. For some languages (such as Mesa, Pascal, Modula, Java and Microsoft's .NET) this practice is recommended by the language developers or by authoritative manuals and has therefore become part of the language's "culture".

Style guidelines often distinguish between upper and lower camel case, typically specifying which variety should be used for specific kinds of entities: variables, record fields, methods, procedures, functions, subroutines, types, etc. These rules are sometimes supported by static analysis tools that check source code for adherence.

The original Hungarian notation for programming, for example, specifies that a lowercase abbreviation for the "usage type" (not data type) should prefix all variable names, with the remainder of the name in upper camel case; as such it is a form of lower camel case.

Programming identifiers often need to contain acronyms and initialisms that are already in uppercase, such as "old HTML file". By analogy with the title case rules, the natural camel case rendering would have the abbreviation all in uppercase, namely "oldHTMLFile". However, this approach is problematic when two acronyms occur together (e.g., "parse DBM XML" would become "parseDBMXML") or when the standard mandates lower camel case but the name begins with an abbreviation (e.g. "SQL server" would become "sQLServer"). For this reason, some programmers prefer to treat abbreviations as if they were lowercase words and write "oldHtmlFile", "parseDbmXml" or "sqlServer". However, this can make it harder to recognise that a given word is intended as an acronym.

Camel case is used in some wiki markup languages for terms that should be automatically linked to other wiki pages. This convention was originally used in Ward Cunningham's original wiki software, WikiWikiWeb, and can be activated in most other wikis. Some wiki engines such as TiddlyWiki, Trac and PmWiki make use of it in the default settings, but usually also provide a configuration mechanism or plugin to disable it. Wikipedia formerly used camel case linking as well, but switched to explicit link markup using square brackets and many other wiki sites have done the same. Some wikis that do not use camel case linking may still use the camel case as a naming convention, such as AboutUs.

The NIEM registry requires that XML data elements use upper camel case and XML attributes use lower camel case.

Most popular command-line interfaces and scripting languages cannot easily handle file names that contain embedded spaces (usually requiring the name to be put in quotes). Therefore, users of those systems often resort to camel case (or underscores, hyphens and other "safe" characters) for compound file names like MyJobResume.pdf.

Microblogging and social networking sites that limit the number of characters in a message are potential outlets for medial capitals. Using camel case between words reduces the number of spaces, and thus the number of characters, in a given message, allowing more content to fit into the limited space. Hashtags, especially long ones, often use camel case to maintain readability (e.g. #CollegeStudentProblems is easier to read than #collegestudentproblems).

In website URLs, spaces are percent-encoded as "%20", making the address longer and less human readable. By omitting spaces, camel case does not have this problem.

Camel case has been criticised as negatively impacting readability due to the removal of spaces and uppercasing of every word.

A 2009 study comparing snake case to camel case found that camel case identifiers could be recognised with higher accuracy among both programmers and non-programmers, and that programmers already trained in camel case were able to recognise those identifiers faster than underscored snake-case identifiers.

A 2010 follow-up study, under the same conditions but using an improved measurement method with use of eye-tracking equipment, indicates: "While results indicate no difference in accuracy between the two styles, subjects recognize identifiers in the underscore style more quickly."

Inconsistency of multi-word style, or combinations of style within a single name, can be confusing. An interesting example is "Creative Commons Attribution-ShareAlike 3.0 Unported License", which names a "thing" using three multi-word combination methods at once: space, dash and upper CamelCase. Difficulties arise when a new thing is created by combining things with different naming conventions. When the creation is dynamic and ongoing over time, it can be more confusing to try to transform from the individual native styles, to a consistent overall style.




</doc>
<doc id="6700" url="https://en.wikipedia.org/wiki?curid=6700" title="Cereal">
Cereal

A cereal is any grass cultivated (grown) for the edible components of its grain (botanically, a type of fruit called a caryopsis), composed of the endosperm, germ, and bran. The term may also refer to the resulting grain itself (specifically "cereal grain"). Cereal grain crops are grown in greater quantities and provide more food energy worldwide than any other type of crop and are therefore staple crops. Edible grains from other plant families, such as buckwheat (Polygonaceae), quinoa (Amaranthaceae) and chia (Lamiaceae), are referred to as pseudocereals.

In their natural, unprocessed, "whole grain" form, cereals are a rich source of vitamins, minerals, carbohydrates, fats, oils, and protein. When processed by the removal of the bran, and germ, the remaining endosperm is mostly carbohydrate. In some developing countries, grain in the form of rice, wheat, millet, or maize constitutes a majority of daily sustenance. In developed countries, cereal consumption is moderate and varied but still substantial.

The word "cereal" is derived from "Ceres", the Roman goddess of harvest and agriculture.

Agriculture allowed for the support of an increased population, leading to larger societies and eventually the development of cities. It also created the need for greater organization of political power (and the creation of social stratification), as decisions had to be made regarding labor and harvest allocation and access rights to water and land. Agriculture bred immobility, as populations settled down for long periods of time, which led to the accumulation of material goods.

Early Neolithic villages show evidence of the development of processing grain. The Levant is the ancient home of the ancestors of wheat, barley and peas, in which many of these villages were based. There is evidence of the cultivation of cereals in Syria approximately 9,000 years ago. During the same period, farmers in China began to farm rice and millet, using human-made floods and fires as part of their cultivation regimen. Fiber crops were domesticated as early as food crops, with China domesticating hemp, cotton being developed independently in Africa and South America, and Western Asia domesticating flax. The use of soil amendments, including manure, fish, compost and ashes, appears to have begun early, and developed independently in several areas of the world, including Mesopotamia, the Nile Valley and Eastern Asia.

The first cereal grains were domesticated by early primitive humans. About 8,000 years ago, they were domesticated by ancient farming communities in the Fertile Crescent region. Emmer wheat, einkorn wheat, and barley were three of the so-called Neolithic founder crops in the development of agriculture. Around the same time, millets and rices were starting to become domesticated in East Asia. Sorghum and millets were also being domesticated in sub-Saharan West Africa.

Cereals were the foundation of human civilization. Cereal frontiers coincided with civilizational frontiers. The term Fertile Crescent explicitly implies the spatial dependence of civilization on cereals. The Great Wall of China and the Roman limes demarcated the same northern limit of the cereal cultivation. The Silk Road stretched along the cereal belt of Eurasia. Numerous Chinese imperial edicts stated: âAgriculture is the foundation of this empire,â while the foundation of agriculture were the Five Grains.

Cereals determined how large and for how long an army could be mobilized. For this reason, Shang Yang called agriculture and war âthe One.â Guan Zhong, Chanakya (the author of "Arthashastra") and Hannibal expressed similar concepts. At the dawn of history, the Sumerians believed that if agriculture of a state declines, Inanna, the goddess of war, leaves this state. Several gods of antiquity combined the functions of what Shang Yang called âthe Oneâ â agriculture and war: the Hittite Sun goddess of Arinna, the Canaanite Lahmu and the Roman Janus. These were highly important gods in their time leaving their legacy until today. We still begin the year with the month of Janus (January). The Jews believe that Messiah will be born in the town of Lahmu (Bethlehem) and the Christians believe that he was already born there. Lahmu is the responsible why in Hebrew until today "bread" ("lehem") and "warfare" ("lehima") are of the same root. In fact, most persistent and flourishing empires throughout history in both hemispheres were centered in regions fertile for cereals.

Historian Max Ostrovsky argues that this historic pattern never changed, not even in the Industrial Age. He stresses that all modern great powers have traditionally remained first and foremost great "cereal" powers. The âfinest hourâ of the Axis powers âended precisely the moment they threw themselves against the two largest cereal lebensraumsâ (the United States and the USSR). The outcome of the Cold War followed the Soviet grave and long-lasting cereal crisis, exacerbated by the cereal embargo imposed on the USSR in 1980. And, called âthe grain basket of the world,â the most productive âcereal lebensraumâ dominates the world ever since.

Having analyzed the mechanism at work behind this pattern, Ostrovsky outlined that the cereal power determines the percentage of manpower available to non-agricultural sectors including the heavy industry vital for military power. He emphasized that chronologically the Industrial Revolution follows the modern Agricultural Revolution and spatially the world's industrial regions are bound to cereal regions. Taken from space, map of the global illumination is said to indicate by its brightest parts the industrial regions. These regions coincide with cereal regions. Ostrovsky formulized the universal indicator of national power which unambiguously demonstrates the unipolar international hierarchy of the present world order:

During the second half of the 20th century there was a significant increase in the production of high-yield cereal crops worldwide, especially wheat and rice, due to an initiative known as the Green Revolution. The strategies developed by the Green Revolution focused on fending off starvation and increasing yield-per-plant, and were very successful in raising overall yields of cereal grains, but did not give sufficient relevance to nutritional quality. These modern high yield-cereal crops tend have low quality proteins, with essential amino acid deficiencies, are high in carbohydrates, and lack balanced essential fatty acids, vitamins, minerals and other quality factors. So-called ancient grains and heirloom varieties have seen an increase in popularity with the "organic" movements of the early 21st century, but there is a tradeoff in yield-per-plant, putting pressure on resource-poor areas as food crops are replaced with cash crops.

While each individual species has its own peculiarities, the cultivation of all cereal crops is similar. Most are annual plants; consequently one planting yields one harvest. Wheat, rye, triticale, oats, barley, and spelt are the "cool-season" cereals. These are hardy plants that grow well in moderate weather and cease to grow in hot weather (approximately , but this varies by species and variety). The "warm-season" cereals are tender and prefer hot weather. Barley and rye are the hardiest cereals, able to overwinter in the subarctic and Siberia. Many cool-season cereals are grown in the tropics. However, some are only grown in cooler highlands, where it ' "may" ' be possible to grow multiple crops per year.

For the past few decades, there has also been increasing interest in perennial grain plants. This interest developed due to advantages in erosion control, reduced need for fertiliser, and potential lowered costs to the farmer. Though research is still in early stages, The Land Institute in Salina, Kansas has been able to create a few cultivars that produce a fairly good crop yield.

The warm-season cereals are grown in tropical lowlands year-round and in temperate climates during the frost-free season. Rice is commonly grown in flooded fields, though some strains are grown on dry land. Other warm climate cereals, such as sorghum, are adapted to arid conditions.

Cool-season cereals are well-adapted to temperate climates. Most varieties of a particular species are either winter or spring types. Winter varieties are sown in the autumn, germinate and grow vegetatively, then become dormant during winter. They resume growing in the springtime and mature in late spring or early summer. This cultivation system makes optimal use of water and frees the land for another crop early in the growing season.

Winter varieties do not flower until springtime because they require vernalization: exposure to low temperatures for a genetically determined length of time. Where winters are too warm for vernalization or exceed the hardiness of the crop (which varies by species and variety), farmers grow spring varieties. Spring cereals are planted in early springtime and mature later that same summer, without vernalization. Spring cereals typically require more irrigation and yield less than winter cereals.

Once the cereal plants have grown their seeds, they have completed their life cycle. The plants die, become brown, and dry. As soon as the parent plants and their seed kernels are reasonably dry, harvest can begin.

In developed countries, cereal crops are universally machine-harvested, typically using a combine harvester, which cuts, threshes, and winnows the grain during a single pass across the field. In developing countries, a variety of harvesting methods are in use, depending on the cost of labor, from combines to hand tools such as the scythe or grain cradle.

If a crop is harvested during humid weather, the grain may not dry adequately in the field to prevent spoilage during its storage. In this case, the grain is sent to a dehydrating facility, where artificial heat dries it.

In North America, farmers commonly deliver their newly harvested grain to a grain elevator, a large storage facility that consolidates the crops of many farmers. The farmer may sell the grain at the time of delivery or maintain ownership of a share of grain in the pool for later sale. Storage facilities should be protected from small grain pests, rodents and birds.

The following table shows the annual production of cereals in 1961, 1980, 2000, 2010, and 2018 ranked by 2018 production.

Maize, wheat, and rice together accounted for 89% of all cereal production worldwide in 2012, and 43% of all food calories in 2009, while the production of oats and triticale have drastically fallen from their 1960s levels.

Other cereals worthy of notice, but not included in FAO statistics, include:

Several other species of wheat have also been domesticated, some very early in the history of agriculture:

In 2013 global cereal production reached a record 2,521 million tonnes. A slight dip to 2,498 million tonnes was forecasted for 2014 by the FAO in July 2014.

Some grains are deficient in the essential amino acid, lysine. That is why many vegetarian cultures, in order to get a balanced diet, combine their diet of grains with legumes. Many legumes, however, are deficient in the essential amino acid methionine, which grains contain. Thus, a combination of legumes with grains forms a well-balanced diet for vegetarians. Common examples of such combinations are dal (lentils) with rice by South Indians and Bengalis, dal with wheat in Pakistan and North India, beans with corn tortillas, tofu with rice, and peanut butter with wheat bread (as sandwiches) in several other cultures, including the Americas. The amount of crude protein measured in grains is expressed as grain crude protein concentration.

Cereals contain exogenous opioid peptides called exorphins and include opioid food peptides like gluten exorphin. They mimic the actions of endorphines because they bind to the same opioid receptors in the brain.

The ISO has published a series of standards regarding cereal products which are covered by ICS 67.060.


</doc>
<doc id="6704" url="https://en.wikipedia.org/wiki?curid=6704" title="Christendom">
Christendom

Christendom historically refers to the "Christian world": Christian states, Christian-majority countries and the countries in which Christianity dominates or prevails. 

Since the spread of Christianity from the Levant to Europe and North Africa during the early Roman Empire, Christendom has been divided in the pre-existing Greek East and Latin West. Consequently, different versions of the Christian religion arose with their own beliefs and practices, centred around the cities of Rome (Western Christianity, whose community was called Western or Latin Christendom) and Constantinople (Eastern Christianity, whose community was called Eastern Christendom). From the 11th to 13th centuries, Latin Christendom rose to the central role of the Western world.

The term usually refers to the Middle Ages and to the Early Modern period during which the Christian world represented a geopolitical power that was juxtaposed with both the pagan and especially the Muslim world.

The Anglo-Saxon term "cristendom" appears to have been invented in the 9th century by a scribe somewhere in southern England, possibly at the court of king Alfred the Great of Wessex. The scribe was translating Paulus Orosius' book "History Against the Pagans" (c. 416) and in need for a term to express the concept of the universal culture focused on Jesus Christ. It had the sense now taken by "Christianity" (as is still the case with the cognate Dutch "christendom", where it denotes mostly the religion itself, just like the German "Christentum").

The current sense of the word of "lands where Christianity is the dominant religion" emerged in Late Middle English (by c. 1400).This semantic development happened independently in the languages of late medieval Europe, which leads to the confusing semantics of English "Christendom" equalling German "Christenheit", Dutch "christenheid", French "chrÃ©tientÃ©" vs. English "Christianity" equalling German "Christentum", Dutch "christendom", French "christianisme". The reason is the increasing fragmentation of Western Christianity at that time both theologically and politically.
"Christendom" as a geopolitical term is thus meaningful in the context of the Middle Ages, and arguably during the European wars of religion and the Ottoman wars in Europe.

Canadian theology professor Douglas John Hall stated (1997) that "Christendom" [...] means literally the dominion or sovereignty of the Christian religion." Thomas John Curry, Roman Catholic auxiliary bishop of Los Angeles, defined (2001) Christendom as "the system dating from the fourth century by which governments upheld and promoted Christianity." Curry states that the end of Christendom came about because modern governments refused to "uphold the teachings, customs, ethos, and practice of Christianity." British church historian Diarmaid MacCulloch described (2010) Christendom as "the union between Christianity and secular power."

The "Christian world" is also collectively known as the Corpus Christianum, translated as "the Christian body", meaning the community of all Christians. The Christian polity, embodying a less secular meaning, can be compatible with the idea of both a religious and a temporal body: "Corpus Christianum". The "Corpus Christianum" can be seen as a Christian equivalent of the Muslim "Ummah".

The word "Christendom" is also used with its other meaning to frame-true Christianity. A more secular meaning can denote the fact that the term "Christendom" refers to Christians as a group, the "political Christian world", as an informal cultural hegemony that Christianity has traditionally enjoyed in the West.
In its most broad term, it refers to the world's Christian-majority countries, which, share little in common aside from the predominance of the faith. Unlike the Muslim world, which has a geo-political and cultural definition that provides a primary identifier for a large swath of the world, Christendom is more complex.
There is a common and nonliteral sense of the word that is much like the terms "Western world", "known world" or "Free World". When Thomas F. Connolly said, "There isn't enough power in all Christendom to make that airplane what we want!", he was simply using a figure of speech, although it is true that during the Cold War, just as the totalitarianism of the Communist Bloc presented a contrast to the liberty of the Free World, the state atheism of the Communist Bloc contrasted with the religious freedom and the powerful religious institutions in North America and Western Europe. The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom"; many even attribute Christianity for being the link that created a unified European identity.

In the beginning of Christendom, early Christianity was a religion spread in the Greek/Roman world and beyond as a 1st-century Jewish sect, which historians refer to as Jewish Christianity. It may be divided into two distinct phases: the apostolic period, when the first apostles were alive and organizing the Church, and the post-apostolic period, when an early episcopal structure developed, whereby bishoprics were governed by bishops (overseers).

The post-apostolic period concerns the time roughly after the death of the apostles when bishops emerged as overseers of urban Christian populations. The earliest recorded use of the terms "Christianity" (Greek ) and "catholic" (Greek ), dates to this period, the 2nd century, attributed to Ignatius of Antioch "c." 107. Early Christendom would close at the end of imperial persecution of Christians after the ascension of Constantine the Great and the Edict of Milan in ADÂ 313 and the First Council of Nicaea in 325.

According to Malcolm Muggeridge (1980), Christ founded Christianity, but Constantine founded Christendom. Canadian theology professor Douglas John Hall dates the 'inauguration of Christendom' to the 4th century, with Constantine playing the primary role (so much so that he equates Christendom with "Constantinianism") and Theodosius I (Edict of Thessalonica, 380) and Justinian I secondary roles.

"Christendom" has referred to the medieval and renaissance notion of the "Christian world" as a sociopolitical polity. In essence, the earliest vision of Christendom was a vision of a Christian theocracy, a government founded upon and upholding Christian values, whose institutions are spread through and over with Christian doctrine. In this period, members of the Christian clergy wield political authority. The specific relationship between the political leaders and the clergy varied but, in theory, the national and political divisions were at times subsumed under the leadership of the church as an institution. This model of church-state relations was accepted by various Church leaders and political leaders in European history.

The Church gradually became a defining institution of the Empire. Emperor Constantine issued the Edict of Milan in 313 proclaiming toleration for the Christian religion, and convoked the First Council of Nicaea in 325 whose Nicene Creed included belief in "one holy catholic and apostolic Church". Emperor Theodosius I made Nicene Christianity the state church of the Roman Empire with the Edict of Thessalonica of 380.

As the Western Roman Empire disintegrated into feudal kingdoms and principalities, the concept of Christendom changed as the western church became one of five patriarchates of the Pentarchy and the Christians of the Eastern Roman Empire developed. The Byzantine Empire was the last bastion of Christendom. Christendom would take a turn with the rise of the Franks, a Germanic tribe who converted to the Christian faith and entered into communion with Rome.

On Christmas Day 800 AD, Pope Leo III crowned Charlemagne resulting in the creation of another Christian king beside the Christian emperor in the Byzantine state. The Carolingian Empire created a definition of "Christendom" in juxtaposition with the Byzantine Empire, that of a distributed versus centralized culture respectively.

The classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. In the Greek philosopher Plato's ideal state there are three major classes, which was representative of the idea of the âtripartite soulâ, which is expressive of three functions or capacities of the human soul: âreasonâ, âthe spirited elementâ, and âappetitesâ (or âpassionsâ). Will Durant made a convincing case that certain prominent features of Plato's ideal community where discernible in the organization, dogma and effectiveness of "the" Medieval Church in Europe:

... For a thousand years Europe was ruled by an order of guardians considerably like that which was visioned by our philosopher. During the Middle Ages it was customary to classify the population of Christendom into "laboratores" (workers), "bellatores" (soldiers), and "oratores" (clergy). The last group, though small in number, monopolized the instruments and opportunities of culture, and ruled with almost unlimited sway half of the most powerful continent on the globe. The clergy, like Plato's guardians, were placed in authority... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [800 AD onwards], the clergy were as free from family cares as even Plato could desire [for such guardians]... [Clerical] Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them..." "In the latter half of the period in which they ruled, the clergy were as free from family cares as even Plato could desire.

After the collapse of Charlemagne's empire, the southern remnants of the Holy Roman Empire became a collection of states loosely connected to the Holy See of Rome. Tensions between Pope Innocent III and secular rulers ran high, as the pontiff exerted control over their temporal counterparts in the west and vice versa. The pontificate of Innocent III is considered the height of temporal power of the papacy. The "Corpus Christianum" described the then-current notion of the community of all Christians united under the Roman Catholic Church. The community was to be guided by Christian values in its politics, economics and social life. Its legal basis was the "corpus iuris canonica" (body of canon law).

In the East, Christendom became more defined as the Byzantine Empire's gradual loss of territory to an expanding Islam and the muslim conquest of Persia. This caused Christianity to become important to the Byzantine identity. Before the EastâWest Schism which divided the Church religiously, there had been the notion of a "universal Christendom" that included the East and the West. After the EastâWest Schism, hopes of regaining religious unity with the West were ended by the Fourth Crusade, when Crusaders conquered the Byzantine capital of Constantinople and hastened the decline of the Byzantine Empire on the path to its destruction. With the breakup of the Byzantine Empire into individual nations with nationalist Orthodox Churches, the term Christendom described Western Europe, Catholicism, Orthodox Byzantines, and other Eastern rites of the Church.

The Catholic Church's peak of authority over all European Christians and their common endeavours of the Christian community â for example, the Crusades, the fight against the Moors in the Iberian Peninsula and against the Ottomans in the Balkans â helped to develop a sense of communal identity against the obstacle of Europe's deep political divisions. The popes, formally just the bishops of Rome, claimed to be the focus of all Christendom, which was largely recognised in Western Christendom from the 11th century until the Reformation, but not in Eastern Christendom. Moreover, this authority was also sometimes abused, and fostered the Inquisition and anti-Jewish pogroms, to root out divergent elements and create a religiously uniform community. Ultimately, the Inquisition was done away with by order of Pope Innocent III.

Christendom ultimately was led into specific crisis in the late Middle Ages, when the kings of France managed to establish a French national church during the 14th century and the papacy became ever more aligned with the Holy Roman Empire of the German Nation. Known as the Western Schism, western Christendom was a split between three men, who were driven by politics rather than any real theological disagreement for simultaneously claiming to be the true pope. The Avignon Papacy developed a reputation for corruption that estranged major parts of Western Christendom. The Avignon schism was ended by the Council of Constance.

Before the modern period, Christendom was in a general crisis at the time of the Renaissance Popes because of the moral laxity of these pontiffs and their willingness to seek and rely on temporal power as secular rulers did. Many in the Catholic Church's hierarchy in the Renaissance became increasingly entangled with insatiable greed for material wealth and temporal power, which led to many reform movements, some merely wanting a moral reformation of the Church's clergy, while others repudiated the Church and separated from it in order to form new sects. The Italian Renaissance produced ideas or institutions by which men living in society could be held together in harmony. In the early 16th century, Baldassare Castiglione (The Book of the Courtier) laid out his vision of the ideal gentleman and lady, while Machiavelli cast a jaundiced eye on "la veritÃ  effetuale delle cose" â the actual truth of things â in "The Prince", composed, humanist style, chiefly of parallel ancient and modern examples of VirtÃ¹. Some Protestant movements grew up along lines of mysticism or renaissance humanism (cf. Erasmus). The Catholic Church fell partly into general neglect under the Renaissance Popes, whose inability to govern the Church by showing personal example of high moral standards set the climate for what would ultimately become the Protestant Reformation. During the Renaissance, the papacy was mainly run by the wealthy families and also had strong secular interests. To safeguard Rome and the connected Papal States the popes became necessarily involved in temporal matters, even leading armies, as the great patron of arts Pope Julius II did. It during these intermediate times popes strove to make Rome the capital of Christendom while projecting it, through art, architecture, and literature, as the center of a Golden Age of unity, order, and peace.

Professor Frederick J. McGinness described Rome as essential in understanding the legacy the Church and its representatives encapsulated best by The Eternal City: No other city in Europe matches Rome in its traditions, history, legacies, and influence in the Western world. Rome in the Renaissance under the papacy not only acted as guardian and transmitter of these elements stemming from the Roman Empire but also assumed the role as artificer and interpreter of its myths and meanings for the peoples of Europe from the Middle Ages to modern times... Under the patronage of the popes, whose wealth and income were exceeded only by their ambitions, the city became a cultural center for master architects, sculptors, musicians, painters, and artisans of every kind...In its myth and message, Rome had become the sacred city of the popes, the prime symbol of a triumphant Catholicism, the center of orthodox Christianity, a new Jerusalem.

It is clearly noticeable that the popes of the Italian Renaissance have been subjected by many writers with an overly harsh tone. Pope Julius II, for example, was not only an effective secular leader in military affairs, a deviously effective politician but foremost one of the greatest patron of the Renaissance period and person who also encouraged open criticism from noted humanists.

The blossoming of renaissance humanism was made very much possible due to the universality of the institutions of Catholic Church and represented by personalities such as Pope Pius II, Nicolaus Copernicus, Leon Battista Alberti, Desiderius Erasmus, sir Thomas More, BartolomÃ© de Las Casas, Leonardo da Vinci and Teresa of Ãvila. George Santayana in his work "The Life of Reason" postulated the tenets of the all encompassing order the Church had brought and as the repository of the legacy of classical antiquity:

The enterprise of individuals or of small aristocratic bodies has meantime sown the world which we call civilised with some seeds and nuclei of order. There are scattered about a variety of churches, industries, academies, and governments. But the universal order once dreamt of and nominally almost established, the empire of universal peace, all-permeating rational art, and philosophical worship, is mentioned no more. An unformulated conception, the prerational ethics of private privilege and national unity, fills the background of men's minds. It represents feudal traditions rather than the tendency really involved in contemporary industry, science, or philanthropy. Those dark ages, from which our political practice is derived, had a political theory which we should do well to study; for their theory about a universal empire and a Catholic church was in turn the echo of a former age of reason, when a few men conscious of ruling the world had for a moment sought to survey it as a whole and to rule it justly.

Developments in western philosophy and European events brought change to the notion of the "Corpus Christianum". The Hundred Years' War accelerated the process of transforming France from a feudal monarchy to a centralized state. The rise of strong, centralized monarchies denoted the European transition from feudalism to capitalism. By the end of the Hundred Years' War, both France and England were able to raise enough money through taxation to create independent standing armies. In the Wars of the Roses, Henry Tudor took the crown of England. His heir, the absolute king Henry VIII establishing the English church.

In modern history, the Reformation and rise of modernity in the early 16th century entailed a change in the "Corpus Christianum". In the Holy Roman Empire, the Peace of Augsburg of 1555 officially ended the idea among secular leaders that all Christians must be united under one church. The principle of "cuius regio, eius religio" ("whose the region is, his religion") established the religious, political and geographic divisions of Christianity, and this was established with the Treaty of Westphalia in 1648, which legally ended the concept of a single Christian hegemony in the territories of the Holy Roman Empire, despite the Catholic Church's doctrine that it alone is the one true Church founded by Christ.
Subsequently, each government determined the religion of their own state. Christians living in states where their denomination was "not" the established one were guaranteed the right to practice their faith in public during allotted hours and in private at their will. At times there were mass expulsions of dissenting faiths as happened with the Salzburg Protestants. Some people passed as adhering to the official church, but instead lived as Nicodemites or crypto-protestants.

The European wars of religion are usually taken to have ended with the Treaty of Westphalia (1648), or arguably, including the Nine Years' War and the War of the Spanish Succession in this period, with the Treaty of Utrecht of 1713. In the 18th century, the focus shifts away from religious conflicts, either between Christian factions or against the external threat of Islamic factions.

The European Miracle, the Age of Enlightenment and the formation of the great colonial empires together with the beginning decline of the Ottoman Empire mark the end of the geopolitical "history of Christendom". Instead, the focus of Western history shifts to the development of the nation-state, accompanied by increasing atheism and secularism, culminating with the French Revolution and the Napoleonic Wars at the turn of the 19th century.

Writing in 1997, Canadian theology professor Douglas John Hall argued that Christendom had either fallen already or was in its death throes; although its end was gradual and not as clear to pin down as its 4th-century establishment, the "transition to the post-Constantinian, or post-Christendom, situation (...) has already been in process for a century or two," beginning with the 18th-century rationalist Enlightenment and the French Revolution (the first attempt to topple the Christian establishment). American Catholic bishop Thomas John Curry stated (2001) that the end of Christendom came about because modern governments refused to "uphold the teachings, customs, ethos, and practice of Christianity." He argued the First Amendment to the United States Constitution (1791) and the Second Vatican Council's Declaration on Religious Freedom (1965) are two of the most important documents setting the stage for its end. According to British historian Diarmaid MacCulloch (2010), Christendom was 'killed' by the First World War (1914â18), which led to the fall of the three main Christian empires (Russian, German and Austrian) of Europe, as well as the Ottoman Empire, rupturing the Eastern Christian communities that had existed on its territory. The Christian empires were replaced by secular, even anti-clerical republics seeking to definitively keep the churches out of politics. The only surviving monarchy with an established church, Britain, was severely damaged by the war, lost most of Ireland due to CatholicâProtestant infighting, and was starting to lose grip on its colonies.

Western culture, throughout most of its history, has been nearly equivalent to Christian culture, and many of the population of the Western hemisphere could broadly be described as cultural Christians. The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom"; many even attribute Christianity for being the link that created a unified European identity. Historian Paul Legutko of Stanford University said the Catholic Church is "at the center of the development of the values, ideas, science, laws, and institutions which constitute what we call Western civilization."

Though Western culture contained several polytheistic religions during its early years under the Greek and Roman Empires, as the centralized Roman power waned, the dominance of the Catholic Church was the only consistent force in Western Europe. Until the Age of Enlightenment, Christian culture guided the course of philosophy, literature, art, music and science. Christian disciplines of the respective arts have subsequently developed into Christian philosophy, Christian art, Christian music, Christian literature etc. Art and literature, law, education, and politics were preserved in the teachings of the Church, in an environment that, otherwise, would have probably seen their loss. The Church founded many cathedrals, universities, monasteries and seminaries, some of which continue to exist today. Medieval Christianity created the first modern universities. The Catholic Church established a hospital system in Medieval Europe that vastly improved upon the Roman "valetudinaria". These hospitals were established to cater to "particular social groups marginalized by poverty, sickness, and age," according to historian of hospitals, Guenter Risse. Christianity also had a strong impact on all other aspects of life: marriage and family, education, the humanities and sciences, the political and social order, the economy, and the arts.

Christianity had a significant impact on education and science and medicine as the church created the bases of the Western system of education, and was the sponsor of founding universities in the Western world as the university is generally regarded as an institution that has its origin in the Medieval Christian setting. Many clerics throughout history have made significant contributions to science and Jesuits in particular have made numerous significant contributions to the development of science. The cultural influence of Christianity includes social welfare, founding hospitals, economics (as the Protestant work ethic), natural law (which would later influence the creation of international law), politics, architecture, literature, personal hygiene, and family life. Christianity played a role in ending practices common among pagan societies, such as human sacrifice, slavery, infanticide and polygamy.

Christian literature is writing that deals with Christian themes and incorporates the Christian world view. This constitutes a huge body of extremely varied writing. Christian poetry is any poetry that contains Christian teachings, themes, or references. The influence of Christianity on poetry has been great in any area that Christianity has taken hold. Christian poems often directly reference the Bible, while others provide allegory.

Christian art is art produced in an attempt to illustrate, supplement and portray in tangible form the principles of Christianity. Virtually all Christian groupings use or have used art to some extent. The prominence of art and the media, style, and representations change; however, the unifying theme is ultimately the representation of the life and times of Jesus and in some cases the Old Testament. Depictions of saints are also common, especially in Anglicanism, Roman Catholicism, and Eastern Orthodoxy.

An illuminated manuscript is a manuscript in which the text is supplemented by the addition of decoration. The earliest surviving substantive illuminated manuscripts are from the period AD 400 to 600, primarily produced in Ireland, Constantinople and Italy. The majority of surviving manuscripts are from the Middle Ages, although many illuminated manuscripts survive from the 15th century Renaissance, along with a very limited number from Late Antiquity.

Most illuminated manuscripts were created as codices, which had superseded scrolls; some isolated single sheets survive. A very few illuminated manuscript fragments survive on papyrus. Most medieval manuscripts, illuminated or not, were written on parchment (most commonly of calf, sheep, or goat skin), but most manuscripts important enough to illuminate were written on the best quality of parchment, called vellum, traditionally made of unsplit calfskin, though high quality parchment from other skins was also called "parchment".

Christian art began, about two centuries after Christ, by borrowing motifs from Roman Imperial imagery, classical Greek and Roman religion and popular art. Religious images are used to some extent by the Abrahamic Christian faith, and often contain highly complex iconography, which reflects centuries of accumulated tradition. In the Late Antique period iconography began to be standardised, and to relate more closely to Biblical texts, although many gaps in the canonical Gospel narratives were plugged with matter from the apocryphal gospels. Eventually the Church would succeed in weeding most of these out, but some remain, like the ox and ass in the Nativity of Christ.

An icon is a religious work of art, most commonly a painting, from Eastern Christianity. Christianity has used symbolism from its very beginnings. In both East and West, numerous iconic types of Christ, Mary and saints and other subjects were developed; the number of named types of icons of Mary, with or without the infant Christ, was especially large in the East, whereas Christ Pantocrator was much the commonest image of Christ.

Christian symbolism invests objects or actions with an inner meaning expressing Christian ideas. Christianity has borrowed from the common stock of significant symbols known to most periods and to all regions of the world. Religious symbolism is effective when it appeals to both the intellect and the emotions. Especially important depictions of Mary include the Hodegetria and Panagia types. Traditional models evolved for narrative paintings, including large cycles covering the events of the Life of Christ, the Life of the Virgin, parts of the Old Testament, and, increasingly, the lives of popular saints. Especially in the West, a system of attributes developed for identifying individual figures of saints by a standard appearance and symbolic objects held by them; in the East they were more likely to identified by text labels.

Each saint has a story and a reason why he or she led an exemplary life. Symbols have been used to tell these stories throughout the history of the Church. A number of Christian saints are traditionally represented by a symbol or iconic motif associated with their life, termed an attribute or emblem, in order to identify them. The study of these forms part of iconography in Art history. They were particularly

Christian architecture encompasses a wide range of both secular and religious styles from the foundation of Christianity to the present day, influencing the design and construction of buildings and structures in Christian culture.

Buildings were at first adapted from those originally intended for other purposes but, with the rise of distinctively ecclesiastical architecture, church buildings came to influence secular ones which have often imitated religious architecture. In the 20th century, the use of new materials, such as concrete, as well as simpler styles has had its effect upon the design of churches and arguably the flow of influence has been reversed. From the birth of Christianity to the present, the most significant period of transformation for Christian architecture in the west was the Gothic cathedral. In the east, Byzantine architecture was a continuation of Roman architecture.

Christian philosophy is a term to describe the fusion of various fields of philosophy with the theological doctrines of Christianity. Scholasticism, which means "that [which] belongs to the school", and was a method of learning taught by the academics (or "school people") of medieval universities c. 1100â1500. Scholasticism originally started to reconcile the philosophy of the ancient classical philosophers with medieval Christian theology. Scholasticism is not a philosophy or theology in itself but a tool and method for learning which places emphasis on dialectical reasoning.

The Byzantine Empire, which was the most sophisticated culture during antiquity, suffered under Muslim conquests limiting its scientific prowess during the Medieval period. Christian Western Europe had suffered a catastrophic loss of knowledge following the fall of the Western Roman Empire. But thanks to the Church scholars such as Aquinas and Buridan, the West carried on at least the spirit of scientific inquiry which would later lead to Europe's taking the lead in science during the Scientific Revolution using translations of medieval works.

Medieval technology refers to the technology used in medieval Europe under Christian rule. After the Renaissance of the 12th century, medieval Europe saw a radical change in the rate of new inventions, innovations in the ways of managing traditional means of production, and economic growth. The period saw major technological advances, including the adoption of gunpowder and the astrolabe, the invention of spectacles, and greatly improved water mills, building techniques, agriculture in general, clocks, and ships. The latter advances made possible the dawn of the Age of Exploration. The development of water mills was impressive, and extended from agriculture to sawmills both for timber and stone, probably derived from Roman technology. By the time of the Domesday Book, most large villages in Britain had mills. They also were widely used in mining, as described by Georg Agricola in De Re Metallica for raising ore from shafts, crushing ore, and even powering bellows.

Significant in this respect were advances within the fields of navigation. The compass and astrolabe along with advances in shipbuilding, enabled the navigation of the World Oceans and thus domination of the worlds economic trade. Gutenbergâs printing press made possible a dissemination of knowledge to a wider population, that would not only lead to a gradually more egalitarian society, but one more able to dominate other cultures, drawing from a vast reserve of knowledge and experience.

During the Renaissance, great advances occurred in geography, astronomy, chemistry, physics, math, manufacturing, and engineering. The rediscovery of ancient scientific texts was accelerated after the Fall of Constantinople, and the invention of printing which would democratize learning and allow a faster propagation of new ideas. "Renaissance technology" is the set of artifacts and customs, spanning roughly the 14th through the 16th century. The era is marked by such profound technical advancements like the printing press, linear perspectivity, patent law, double shell domes or Bastion fortresses. Draw-books of the Renaissance artist-engineers such as Taccola and Leonardo da Vinci give a deep insight into the mechanical technology then known and applied.

Renaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement. The "Scientific Renaissance" was the early phase of the Scientific Revolution. In the two-phase model of early modern science: a "Scientific Renaissance" of the 15th and 16th centuries, focused on the restoration of the natural knowledge of the ancients; and a "Scientific Revolution" of the 17th century, when scientists shifted from recovery to innovation.

In 2009, according to the "EncyclopÃ¦dia Britannica", Christianity was the majority religion in Europe (including Russia) with 80%, Latin America with 92%, North America with 81%, and Oceania with 79%. There are also large Christian communities in other parts of the world, such as China, India and Central Asia, where Christianity is the second-largest religion after Islam. The United States is home to the world's largest Christian population, followed by Brazil and Mexico.
Many Christians not only live under, but also have an official status in, a state religion of the following nations: Argentina (Roman Catholic Church), Armenia (Armenian Apostolic Church), Costa Rica (Roman Catholic Church), Denmark (Church of Denmark), El Salvador (Roman Catholic Church), England (Church of England), Georgia (Georgian Orthodox church), Greece (Church of Greece), Iceland (Church of Iceland), Liechtenstein (Roman Catholic Church), Malta (Roman Catholic Church), Monaco (Roman Catholic Church), Romania (Romanian Orthodox Church), Norway (Church of Norway), Vatican City (Roman Catholic Church), Switzerland (Roman Catholic Church, Swiss Reformed Church and Christian Catholic Church of Switzerland).

The estimated number of Christians in the world ranges from 2.2 billion to 2.4 billion people. The faith represents approximately one-third of the world's population and is the largest religion in the world, with the three largest groups of Christians being the Catholic Church, Protestantism, and the Eastern Orthodox Church. The largest Christian denomination is the Catholic Church, with an estimated 1.2 billion adherents.

A religious order is a lineage of communities and organizations of people who live in some way set apart from society in accordance with their specific religious devotion, usually characterized by the principles of its founder's religious practice. In contrast, the term Holy Orders is used by many Christian churches to refer to ordination or to a group of individuals who are set apart for a special role or ministry. Historically, the word "order" designated an established civil body or corporation with a hierarchy, and ordination meant legal incorporation into an ordo. The word "holy" refers to the Church. In context, therefore, a holy order is set apart for ministry in the Church. Religious orders are composed of initiates (laity) and, in some traditions, ordained clergies.

Various organizations include:

Within the framework of Christianity, there are at least three possible definitions for Church law. One is the Torah/Mosaic Law (from what Christians consider to be the Old Testament) also called Divine Law or Biblical law. Another is the instructions of Jesus of Nazareth in the Gospel (sometimes referred to as the Law of Christ or the New Commandment or the New Covenant). A third is canon law which is the internal ecclesiastical law governing the Roman Catholic Church, the Eastern Orthodox churches, and the Anglican Communion of churches. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was initially a rule adopted by a council (From Greek "kanon" / ÎºÎ±Î½ÏÎ½, Hebrew kaneh / ×§× ×, for rule, standard, or measure); these canons formed the foundation of canon law.

Christian ethics in general has tended to stress the need for grace, mercy, and forgiveness because of human weakness and developed while Early Christians were subjects of the Roman Empire. From the time Nero blamed Christians for setting Rome ablaze (64 AD) until Galarius (311 AD), persecutions against Christians erupted periodically. Consequently, Early Christian ethics included discussions of how believers should relate to Roman authority and to the empire.

Under the Emperor Constantine I (312-337), Christianity became a legal religion. While some scholars debate whether Constantine's conversion to Christianity was authentic or simply matter of political expediency, Constantine's decree made the empire safe for Christian practice and belief. Consequently, issues of Christian doctrine, ethics and church practice were debated openly, see for example the First Council of Nicaea and the First seven Ecumenical Councils. By the time of Theodosius I (379-395), Christianity had become the state religion of the empire. With Christianity in power, ethical concerns broaden and included discussions of the proper role of the state.

Render unto Caesarâ¦ is the beginning of a phrase attributed to Jesus in the synoptic gospels which reads in full, ""Render unto Caesar the things which are Caesarâs, and unto God the things that are Godâs"". This phrase has become a widely quoted summary of the relationship between Christianity and secular authority. The gospels say that when Jesus gave his response, his interrogators "marvelled, and left him, and went their way." Time has not resolved an ambiguity in this phrase, and people continue to interpret this passage to support various positions that are poles apart. The traditional division, carefully determined, in Christian thought is the state and church have separate spheres of influence.

Thomas Aquinas thoroughly discussed that "human law" is positive law which means that it is natural law applied by governments to societies. All human laws were to be judged by their conformity to the natural law. An unjust law was in a sense no law at all. At this point, the natural law was not only used to pass judgment on the moral worth of various laws, but also to determine what the law said in the first place. This could result in some tension. Late ecclesiastical writers followed in his footsteps.

Christian democracy is a political ideology that seeks to apply Christian principles to public policy. It emerged in 19th-century Europe, largely under the influence of Catholic social teaching. In a number of countries, the democracy's Christian ethos has been diluted by secularisation. In practice, Christian democracy is often considered conservative on cultural, social and moral issues and progressive on fiscal and economic issues. In places, where their opponents have traditionally been secularist socialists and social democrats, Christian democratic parties are moderately conservative, whereas in other cultural and political environments they can lean to the left.

Attitudes and beliefs about the roles and responsibilities of women in Christianity vary considerably today as they have throughout the last two millennia â evolving along with or counter to the societies in which Christians have lived. The Bible and Christianity historically have been interpreted as excluding women from church leadership and placing them in submissive roles in marriage. Male leadership has been assumed in the church and within marriage, society and government.

Some contemporary writers describe the role of women in the life of the church as having been downplayed, overlooked, or denied throughout much of Christian history. Paradigm shifts in gender roles in society and also many churches has inspired reevaluation by many Christians of some long-held attitudes to the contrary. Christian egalitarians have increasingly argued for equal roles for men and women in marriage, as well as for the ordination of women to the clergy. Contemporary conservatives meanwhile have reasserted what has been termed a "complementarian" position, promoting the traditional belief that the Bible ordains different roles and responsibilities for women and men in the Church and family.

A Christian denomination is a distinct religious body within Christianity, identified by traits such as a name, organisation, leadership and doctrine. Worldwide, Christians are divided, often along ethnic and linguistic lines, into separate churches and traditions. Technically, divisions between one group and another are defined by church doctrine and church authority. Centering on language of "professed Christianity" and "true Christianity", issues that separate one group of followers of Jesus from another include:

Christianity is composed of, but not limited to, five major branches of Churches: Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, Protestantism, and Restorationism. Within Protestantism, Lutheranism, Anglicanism, Methodism, Baptists, Pentecostals, and Reformed Christianity are the largest traditions; many listings include Anglicans among Protestants while others list the Eastern Orthodox and Oriental Orthodox together as one group, thus the number of distinct major branches can vary between three and five depending on the listing. The Assyrian Church of the East (Nestorians) and the Old Catholic churches are also distinct Christian bodies of historic importance, but much smaller in adherents and geographic scope. Each of the branches has important subdivisions. Because the Protestant subdivisions do not maintain a common theology or earthly leadership, they are far more distinct than the subdivisions of the other four groupings. "Denomination" typically refers to one of the many Christian groupings including each of Protestant subdivisions.

In Christendom, the largest traditions are:








</doc>
<doc id="6710" url="https://en.wikipedia.org/wiki?curid=6710" title="Coyote">
Coyote

The coyote ("Canis latrans") is a species of canine native to North America. It is smaller than its close relative, the gray wolf, and slightly smaller than the closely related eastern wolf and red wolf. It fills much of the same ecological niche as the golden jackal does in Eurasia, though it is larger and more predatory, and it is sometimes called the "American jackal" by zoologists. Other names for the species, largely historical, include the "prairie wolf" and the "brush wolf".

The coyote is listed as least concern by the International Union for Conservation of Nature, due to its wide distribution and abundance throughout North America, southwards through Mexico and into Central America. The species is versatile, able to adapt to and expand into environments modified by humans. It is enlarging its range, with coyotes moving into urban areas in the eastern U.S., and was sighted in eastern Panama (across the Panama Canal from their home range) for the first time in 2013.

The coyote has 19 recognized subspecies. The average male weighs and the average female . Their fur color is predominantly light gray and red or fulvous interspersed with black and white, though it varies somewhat with geography. It is highly flexible in social organization, living either in a family unit or in loosely knit packs of unrelated individuals. Primarily carnivorous, its diet consists mainly of deer, rabbits, hares, rodents, birds, reptiles, amphibians, fish, and invertebrates, though it may also eat fruits and vegetables on occasion. Its characteristic vocalization is a howl made by solitary individuals. Humans are the coyote's greatest threat, followed by cougars and gray wolves. In spite of this, coyotes sometimes mate with gray, eastern, or red wolves, producing "coywolf" hybrids. In the northeastern regions of North America, the eastern coyote (a larger subspecies, though still smaller than wolves) is the result of various historical and recent matings with various types of wolves. Genetic studies show that most North American wolves contain some level of coyote DNA.

The coyote is a prominent character in Native American folklore, mainly in Aridoamerica, usually depicted as a trickster that alternately assumes the form of an actual coyote or a man. As with other trickster figures, the coyote uses deception and humor to rebel against social conventions. The animal was especially respected in Mesoamerican cosmology as a symbol of military might. After the European colonization of the Americas, it was seen in Anglo-American culture as a cowardly and untrustworthy animal. Unlike wolves (gray, eastern, or red), which have undergone an improvement of their public image, attitudes towards the coyote remain largely negative.

Coyote males average in weight, while females average , though size varies geographically. Northern subspecies, which average , tend to grow larger than the southern subspecies of Mexico, which average . Body length ranges on average from , and tail length , with females being shorter in both body length and height. The largest coyote on record was a male killed near Afton, Wyoming, on November19, 1937, which measured from nose to tail, and weighed . Scent glands are located at the upper side of the base of the tail and are a bluish-black color.

The color and texture of the coyote's fur varies somewhat geographically. The hair's predominant color is light gray and red or fulvous, interspersed around the body with black and white. Coyotes living at high elevations tend to have more black and gray shades than their desert-dwelling counterparts, which are more fulvous or whitish-gray. The coyote's fur consists of short, soft underfur and long, coarse guard hairs. The fur of northern subspecies is longer and denser than in southern forms, with the fur of some Mexican and Central American forms being almost hispid (bristly). Generally, adult coyotes (including coywolf hybrids) have a sable coat color, dark neonatal coat color, bushy tail with an active supracaudal gland, and a white facial mask. Albinism is extremely rare in coyotes; out of a total of 750,000 coyotes killed by federal and cooperative hunters between March22, 1938, and June30, 1945, only two were albinos.

The coyote is typically smaller than the gray wolf, but has longer ears and a relatively larger braincase, as well as a thinner frame, face, and muzzle. The scent glands are smaller than the gray wolf's, but are the same color. Its fur color variation is much less varied than that of a wolf. The coyote also carries its tail downwards when running or walking, rather than horizontally as the wolf does.

Coyote tracks can be distinguished from those of dogs by their more elongated, less rounded shape. Unlike dogs, the upper canines of coyotes extend past the mental foramina.

At the time of the European colonization of the Americas, coyotes were largely confined to open plains and arid regions of the western half of the continent. In early post-Columbian historical records, distinguishing between coyotes and wolves is often difficult. One record from 1750 in Kaskaskia, Illinois, written by a local priest, noted that the "wolves" encountered there were smaller and less daring than European wolves. Another account from the early 1800s in Edwards County mentioned wolves howling at night, though these were likely coyotes. This species was encountered several times during the Lewis and Clark Expedition (1804â1806), though it was already well known to European traders on the upper Missouri. Lewis, writing on 5Â May 1805, in northeastern Montana, described the coyote in these terms:
The coyote was first scientifically described by naturalist Thomas Say in SeptemberÂ 1819, on the site of Lewis and Clark's Council Bluffs, up the Missouri River from the mouth of the Platte during a government-sponsored expedition with Major Stephen Long. He had the first edition of the Lewis and Clark journals in hand, which contained Biddle's edited version of Lewis's observations dated 5Â May 1805. His account was published in 1823. Say was the first person to document the difference between a ""prairie wolf"" (coyote) and on the next page of his journal a wolf which he named "Canis nubilus" (Great Plains wolf). Say described the coyote as:
The earliest written reference to the species comes from the naturalist Francisco HernÃ¡ndez's "Plantas y Animales de la Nueva EspaÃ±a" (1651), where it is described as a "Spanish fox" or "jackal". The first published usage of the word "coyote" (which is a Spanish borrowing of its Nahuatl name "coyÅtl" ) comes from the historian Francisco Javier Clavijero's "Historia de MÃ©xico" in 1780. The first time it was used in English occurred in William Bullock's "Six months' residence and travels in Mexico" (1824), where it is variously transcribed as "cayjotte" and "cocyotie". The word's spelling was standardized as "coyote" by the 1880s. Alternative English names for the coyote include "prairie wolf", "brush wolf", "cased wolf", "little wolf" and "American jackal". Its binomial name "Canis latrans" translates to "barking dog", a reference to the many vocalizations they produce.

Xiaoming Wang and Richard H. Tedford, one of the foremost authorities on carnivore evolution, proposed that the genus "Canis" was the descendant of the coyote-like "Eucyon davisi" and its remains first appeared in the Miocene 6million years ago (Mya) in the southwestern US and Mexico. By the Pliocene (5Mya), the larger "Canis lepophagus" appeared in the same region and by the early Pleistocene (1Mya) "C.latrans" (the coyote) was in existence. They proposed that the progression from "Eucyon davisi" to "Clepophagus" to the coyote was linear evolution. Additionally, "C.latrans" and "C. aureus" are closely related to "C.edwardii", a species that appeared earliest spanning the mid-Blancan (late Pliocene) to the close of the Irvingtonian (late Pleistocene), and coyote remains indistinguishable from "C.latrans" were contemporaneous with "C.edwardii" in North America. Johnston describes "C.lepophagus" as having a more slender skull and skeleton than the modern coyote. Ronald Nowak found that the early populations had small, delicate, narrowly proportioned skulls that resemble small coyotes and appear to be ancestral to "C.latrans".

"C. lepophagus" was similar in weight to modern coyotes, but had shorter limb bones that indicates a less cursorial lifestyle. The coyote represents a more primitive form of "Canis" than the gray wolf, as shown by its relatively small size and its comparatively narrow skull and jaws, which lack the grasping power necessary to hold the large prey in which wolves specialize. This is further corroborated by the coyote's sagittal crest, which is low or totally flattened, thus indicating a weaker bite than the wolf's. The coyote is not a specialized carnivore as the wolf is, as shown by the larger chewing surfaces on the molars, reflecting the species' relative dependence on vegetable matter. In these respects, the coyote resembles the fox-like progenitors of the genus more so than the wolf.

The oldest fossils that fall within the range of the modern coyote date to 0.74â0.85 Ma (million years) in Hamilton Cave, West Virginia; 0.73 Ma in Irvington, California; 0.35â0.48 Ma in Porcupine Cave, Colorado and in Cumberland Cave, Pennsylvania. Modern coyotes arose 1,000 years after the Quaternary extinction event. Compared to their modern Holocene counterparts, Pleistocene coyotes ("C.l. orcutti") were larger and more robust, likely in response to larger competitors and prey. Pleistocene coyotes were likely more specialized carnivores than their descendants, as their teeth were more adapted to shearing meat, showing fewer grinding surfaces suited for processing vegetation. Their reduction in size occurred within 1000 years of the Quaternary extinction event, when their large prey died out. Furthermore, Pleistocene coyotes were unable to exploit the big-game hunting niche left vacant after the extinction of the dire wolf ("C.dirus"), as it was rapidly filled by gray wolves, which likely actively killed off the large coyotes, with natural selection favoring the modern gracile morph.

In 1993, a study proposed that the wolves of North America display skull traits more similar to the coyote than wolves from Eurasia. In 2010, a study found that the coyote was a basal member of the clade that included the Tibetan wolf, the domestic dog, the Mongolian wolf and the Eurasian wolf, with the Tibetan wolf diverging early from wolves and domestic dogs. In 2016, a whole-genome DNA study proposed, based on the assumptions made, that all of the North American wolves and coyotes diverged from a common ancestor less than 6,000â117,000 years ago. The study also indicated that all North American wolves have a significant amount of coyote ancestry and all coyotes some degree of wolf ancestry, and that the red wolf and eastern wolf are highly admixed with different proportions of gray wolf and coyote ancestry. The proposed timing of the wolf/coyote divergence conflicts with the finding of a coyote-like specimen in strata dated to 1 Mya.

Genetic studies relating to wolves or dogs have inferred phylogenetic relationships based on the only reference genome available, that of the Boxer dog. In 2017, the first reference genome of the wolf "Canis lupus lupus" was mapped to aid future research. In 2018, a study looked at the genomic structure and admixture of North American wolves, wolf-like canids, and coyotes using specimens from across their entire range that mapped the largest dataset of nuclear genome sequences against the wolf reference genome. The study supports the findings of previous studies that North American gray wolves and wolf-like canids were the result of complex gray wolf and coyote mixing. A polar wolf from Greenland and a coyote from Mexico represented the purest specimens. The coyotes from
Alaska, California, Alabama, and Quebec show almost no wolf ancestry. Coyotes from Missouri, Illinois, and Florida exhibit 5â10% wolf ancestry. There was 40%:60% wolf to coyote ancestry in red wolves, 60%:40% in Eastern timber wolves, and 75%:25% in the Great Lakes wolves. There was 10% coyote ancestry in Mexican wolves and the Atlantic Coast wolves, 5% in Pacific Coast and Yellowstone wolves, and less than 3% in Canadian archipelago wolves. If a third canid had been involved in the admixture of the North American wolf-like canids then its genetic signature would have been found in coyotes and wolves, which it has not.

In 2018, whole genome sequencing was used to compare members of genus "Canis". The study indicates that the common ancestor of the coyote and gray wolf has genetically admixed with a ghost population of an extinct unidentified canid. The canid was genetically close to the dhole and had evolved after the divergence of the African wild dog from the other canid species. The basal position of the coyote compared to the wolf is proposed to be due to the coyote retaining more of the mitochondrial genome of this unknown canid.

, 19 subspecies are recognized. Geographic variation in coyotes is not great, though taken as a whole, the eastern subspecies ("C. l. thamnos" and "C. l. frustor") are large, dark-colored animals, with a gradual paling in color and reduction in size westward and northward ("C. l. texensis", "C. l. latrans", "C. l. lestes", and "C. l. incolatus"), a brightening of ochraceous tonesâdeep orange or brownâtowards the Pacific coast ("C. l. ochropus", "C. l. umpquensis"), a reduction in size in Aridoamerica ("C. l. microdon", "C. l. mearnsi") and a general trend towards dark reddish colors and short muzzles in Mexican and Central American populations.

Coyotes have occasionally mated with domestic dogs, sometimes producing crosses colloquially known as "coydogs". Such matings are rare in the wild, as the mating cycles of dogs and coyotes do not coincide, and coyotes are usually antagonistic towards dogs. Hybridization usually only occurs when coyotes are expanding into areas where conspecifics are few, and dogs are the only alternatives. Even then, pup survival rates are lower than normal, as dogs do not form pair bonds with coyotes, thus making the rearing of pups more difficult. In captivity, F hybrids (first generation) tend to be more mischievous and less manageable as pups than dogs, and are less trustworthy on maturity than wolf-dog hybrids. Hybrids vary in appearance, but generally retain the coyote's usual characteristics. F hybrids tend to be intermediate in form between dogs and coyotes, while F hybrids (second generation) are more varied. Both F and F hybrids resemble their coyote parents in terms of shyness and intrasexual aggression. Hybrids are fertile and can be successfully bred through four generations. Melanistic coyotes owe their black pelts to a mutation that first arose in domestic dogs. A population of nonalbino white coyotes in Newfoundland owe their coloration to a melanocortin 1 receptor mutation inherited from Golden Retrievers.
Coyotes have hybridized with wolves to varying degrees, particularly in eastern North America. The so-called "eastern coyote" of northeastern North America probably originated in the aftermath of the extermination of gray and eastern wolves in the northeast, thus allowing coyotes to colonize former wolf ranges and mix with the remnant wolf populations. This hybrid is smaller than either the gray or eastern wolf, and holds smaller territories, but is in turn larger and holds more extensive home ranges than the typical western coyote. , the eastern coyote's genetic makeup is fairly uniform, with minimal influence from eastern wolves or western coyotes. Adult eastern coyotes are larger than western coyotes, with female eastern coyotes weighing 21% more than male western coyotes. Physical differences become more apparent by the age of 35 days, with eastern coyote pups having longer legs than their western counterparts. Differences in dental development also occurs, with tooth eruption being later, and in a different order in the eastern coyote. Aside from its size, the eastern coyote is physically similar to the western coyote. The four color phases range from dark brown to blond or reddish blond, though the most common phase is gray-brown, with reddish legs, ears, and flanks. No significant differences exist between eastern and western coyotes in aggression and fighting, though eastern coyotes tend to fight less, and are more playful. Unlike western coyote pups, in which fighting precedes play behavior, fighting among eastern coyote pups occurs after the onset of play. Eastern coyotes tend to reach sexual maturity at two years of age, much later than in western coyotes.

Eastern and red wolves are also products of varying degrees of wolf-coyote hybridization. The eastern wolf probably was a result of a wolf-coyote admixture, combined with extensive backcrossing with parent gray wolf populations. The red wolf may have originated during a time of declining wolf populations in the Southeastern Woodlands, forcing a wolf-coyote hybridization, as well as backcrossing with local parent coyote populations to the extent that about 75â80% of the modern red wolf's genome is of coyote derivation.

Like the Eurasian golden jackal, the coyote is gregarious, but not as dependent on conspecifics as more social canid species like wolves are. This is likely because the coyote is not a specialized hunter of large prey as the latter species is. The basic social unit of a coyote pack is a family containing a reproductive female. However, unrelated coyotes may join forces for companionship, or to bring down prey too large to attack singly. Such "nonfamily" packs are only temporary, and may consist of bachelor males, nonreproductive females and subadult young. Families are formed in midwinter, when females enter estrus. Pair bonding can occur 2â3 months before actual copulation takes place. The copulatory tie can last 5â45 minutes. A female entering estrus attracts males by scent marking and howling with increasing frequency. A single female in heat can attract up to seven reproductive males, which can follow her for as long as a month. Although some squabbling may occur among the males, once the female has selected a mate and copulates, the rejected males do not intervene, and move on once they detect other estrous females. Unlike the wolf, which has been known to practice both monogamous and bigamous matings, the coyote is strictly monogamous, even in areas with high coyote densities and abundant food. Females that fail to mate sometimes assist their sisters or mothers in raising their pups, or join their siblings until the next time they can mate. The newly mated pair then establishes a territory and either constructs their own den or cleans out abandoned badger, marmot, or skunk earths. During the pregnancy, the male frequently hunts alone and brings back food for the female. The female may line the den with dried grass or with fur pulled from her belly. The gestation period is 63 days, with an average litter size of six, though the number fluctuates depending on coyote population density and the abundance of food.

Coyote pups are born in dens, hollow trees, or under ledges, and weigh at birth. They are altricial, and are completely dependent on milk for their first 10 days. The incisors erupt at about 12 days, the canines at 16, and the second premolars at 21. Their eyes open after 10 days, by which point the pups become increasingly more mobile, walking by 20 days, and running at the age of six weeks. The parents begin supplementing the pup's diet with regurgitated solid food after 12â15 days. By the age of four to six weeks, when their milk teeth are fully functional, the pups are given small food items such as mice, rabbits, or pieces of ungulate carcasses, with lactation steadily decreasing after two months. Unlike wolf pups, coyote pups begin seriously fighting (as opposed to play fighting) prior to engaging in play behavior. A common play behavior includes the coyote "hip-slam". By three weeks of age, coyote pups bite each other with less inhibition than wolf pups. By the age of four to five weeks, pups have established dominance hierarchies, and are by then more likely to play rather than fight. The male plays an active role in feeding, grooming, and guarding the pups, but abandons them if the female goes missing before the pups are completely weaned. The den is abandoned by June to July, and the pups follow their parents in patrolling their territory and hunting. Pups may leave their families in August, though can remain for much longer. The pups attain adult dimensions at eight months, and gain adult weight a month later.

Individual feeding territories vary in size from , with the general concentration of coyotes in a given area depending on food abundance, adequate denning sites, and competition with conspecifics and other predators. The coyote generally does not defend its territory outside of the denning season, and is much less aggressive towards intruders than the wolf is, typically chasing and sparring with them, but rarely killing them. Conflicts between coyotes can arise during times of food shortage. Coyotes mark their territories by raised-leg urination and ground-scratching.

Like wolves, coyotes use a den (usually the deserted holes of other species) when gestating and rearing young, though they may occasionally give birth under sagebrushes in the open. Coyote dens can be located in canyons, washouts, coulees, banks, rock bluffs, or level ground. Some dens have been found under abandoned homestead shacks, grain bins, drainage pipes, railroad tracks, hollow logs, thickets, and thistles. The den is continuously dug and cleaned out by the female until the pups are born. Should the den be disturbed or infested with fleas, the pups are moved into another den. A coyote den can have several entrances and passages branching out from the main chamber. A single den can be used year after year.

While the popular consensus is that olfaction is very important for hunting, two studies that experimentally investigated the role of olfactory, auditory, and visual cues found that visual cues are the most important ones for hunting in red foxes and coyotes.
When hunting large prey, the coyote often works in pairs or small groups. Success in killing large ungulates depends on factors such as snow depth and crust density. Younger animals usually avoid participating in such hunts, with the breeding pair typically doing most of the work. Unlike the wolf, which attacks large prey from the rear, the coyote approaches from the front, lacerating its prey's head and throat. Like other canids, the coyote caches excess food. Coyotes catch mouse-sized rodents by pouncing, whereas ground squirrels are chased. Although coyotes can live in large groups, small prey is typically caught singly. Coyotes have been observed to kill porcupines in pairs, using their paws to flip the rodents on their backs, then attacking the soft underbelly. Only old and experienced coyotes can successfully prey on porcupines, with many predation attempts by young coyotes resulting in them being injured by their prey's quills. Coyotes sometimes urinate on their food, possibly to claim ownership over it. Recent evidence demonstrates that at least some coyotes have become more nocturnal in hunting, presumably to avoid humans.

Coyotes may occasionally form mutualistic hunting relationships with American badgers, assisting each other in digging up rodent prey. The relationship between the two species may occasionally border on apparent "friendship", as some coyotes have been observed laying their heads on their badger companions or licking their faces without protest. The amicable interactions between coyotes and badgers were known to pre-Columbian civilizations, as shown on a Mexican jar dated to 1250â1300 CE depicting the relationship between the two.

Food scraps, pet food, and animal feces may attract a coyote to a trash can.

Being both a gregarious and solitary animal, the variability of the coyote's visual and vocal repertoire is intermediate between that of the solitary foxes and the highly social wolf. The aggressive behavior of the coyote bears more similarities to that of foxes than it does that of wolves and dogs. An aggressive coyote arches its back and lowers its tail. Unlike dogs, which solicit playful behavior by performing a "play-bow" followed by a "play-leap", play in coyotes consists of a bow, followed by side-to-side head flexions and a series of "spins" and "dives". Although coyotes will sometimes bite their playmates' scruff as dogs do, they typically approach low, and make upward-directed bites. Pups fight each other regardless of sex, while among adults, aggression is typically reserved for members of the same sex. Combatants approach each other waving their tails and snarling with their jaws open, though fights are typically silent. Males tend to fight in a vertical stance, while females fight on all four paws. Fights among females tend to be more serious than ones among males, as females seize their opponents' forelegs, throat, and shoulders.

The coyote has been described as "the most vocal of all [wild] North American mammals". Its loudness and range of vocalizations was the cause for its binomial name "Canis latrans", meaning "barking dog". At least 11 different vocalizations are known in adult coyotes. These sounds are divided into three categories: agonistic and alarm, greeting, and contact. Vocalizations of the first category include woofs, growls, huffs, barks, bark howls, yelps, and high-frequency whines. Woofs are used as low-intensity threats or alarms, and are usually heard near den sites, prompting the pups to immediately retreat into their burrows. Growls are used as threats at short distances, but have also been heard among pups playing and copulating males. Huffs are high-intensity threat vocalizations produced by rapid expiration of air. Barks can be classed as both long-distance threat vocalizations and as alarm calls. Bark howls may serve similar functions. Yelps are emitted as a sign of submission, while high-frequency whines are produced by dominant animals acknowledging the submission of subordinates. Greeting vocalizations include low-frequency whines, 'wow-oo-wows', and group yip howls. Low-frequency whines are emitted by submissive animals, and are usually accompanied by tail wagging and muzzle nibbling. The sound known as 'wow-oo-wow' has been described as a "greeting song". The group yip howl is emitted when two or more pack members reunite, and may be the final act of a complex greeting ceremony. Contact calls include lone howls and group howls, as well as the previously mentioned group yip howls. The lone howl is the most iconic sound of the coyote, and may serve the purpose of announcing the presence of a lone individual separated from its pack. Group howls are used as both substitute group yip howls and as responses to either lone howls, group howls, or group yip howls.

Prior to the near extermination of wolves and cougars, the coyote was most numerous in grasslands inhabited by bison, pronghorn, elk, and other deer, doing particularly well in short-grass areas with prairie dogs, though it was just as much at home in semiarid areas with sagebrush and jackrabbits or in deserts inhabited by cactus, kangaroo rats, and rattlesnakes. As long as it was not in direct competition with the wolf, the coyote ranged from the Sonoran Desert to the alpine regions of adjoining mountains or the plains and mountainous areas of Alberta. With the extermination of the wolf, the coyote's range expanded to encompass broken forests from the tropics of Guatemala and the northern slope of Alaska.

Coyotes walk around per day, often along trails such as logging roads and paths; they may use iced-over rivers as travel routes in winter. They are often crepuscular, being more active around evening and the beginning of the night than during the day. Like many canids, coyotes are competent swimmers, reported to be able to travel at least across water.

The coyote is ecologically the North American equivalent of the Eurasian golden jackal. Likewise, the coyote is highly versatile in its choice of food, but is primarily carnivorous, with 90% of its diet consisting of meat. Prey species include bison (largely as carrion), white-tailed deer, mule deer, moose, elk, bighorn sheep, pronghorn, rabbits, hares, rodents, birds (especially galliforms, young water birds and pigeons and doves), amphibians (except toads), lizards, snakes, turtles and tortoises, fish, crustaceans, and insects. Coyotes may be picky over the prey they target, as animals such as shrews, moles, and brown rats do not occur in their diet in proportion to their numbers. However, terrestrial and/or burrowing small mammals such as ground squirrels and associated species (marmots, prairie dogs, chipmunks) as well as voles, pocket gophers, kangaroo rats and other ground-favoring rodents may be quite common foods, especially for lone coyotes. More unusual prey include fishers, young black bear cubs, harp seals and rattlesnakes. Coyotes kill rattlesnakes mostly for food (but also to protect their pups at their dens) by teasing the snakes until they stretch out and then biting their heads and snapping and shaking the snakes. Birds taken by coyotes may range in size from thrashers, larks and sparrows to adult wild turkeys and, possibly, brooding adult swans and pelicans. If working in packs or pairs, coyotes may have access to larger prey than lone individuals normally take, such as various prey weighing more than . In some cases, packs of coyotes have dispatched much larger prey such as adult "Odocoileus" deer, cow elk, pronghorns and wild sheep, although the young fawn, calves and lambs of these animals are considerably more often taken even by packs, as well as domestic sheep and domestic cattle. In some cases, coyotes can bring down prey weighing up to or more. When it comes to adult ungulates such as wild deer, they often exploit them when vulnerable such as those that are infirm, stuck in snow or ice, otherwise winter-weakened or heavily pregnant, whereas less wary domestic ungulates may be more easily exploited.

Although coyotes prefer fresh meat, they will scavenge when the opportunity presents itself. Excluding the insects, fruit, and grass eaten, the coyote requires an estimated of food daily, or annually. The coyote readily cannibalizes the carcasses of conspecifics, with coyote fat having been successfully used by coyote hunters as a lure or poisoned bait. The coyote's winter diet consists mainly of large ungulate carcasses, with very little plant matter. Rodent prey increases in importance during the spring, summer, and fall.

The coyote feeds on a variety of different produce, including blackberries, blueberries, peaches, pears, apples, prickly pears, chapotes, persimmons, peanuts, watermelons, cantaloupes, and carrots. During the winter and early spring, the coyote eats large quantities of grass, such as green wheat blades. It sometimes eats unusual items such as cotton cake, soybean meal, domestic animal droppings, beans, and cultivated grain such as maize, wheat, and sorghum.

In coastal California, coyotes now consume a higher percentage of marine-based food than their ancestors, which is thought to be due to the extirpation of the grizzly bear from this region. In Death Valley, coyotes may consume great quantities of hawkmoth caterpillars or beetles in the spring flowering months.

In areas where the ranges of coyotes and gray wolves overlap, interference competition and predation by wolves has been hypothesized to limit local coyote densities. Coyote ranges expanded during the 19th and 20th centuries following the extirpation of wolves, while coyotes were driven to extinction on Isle Royale after wolves colonized the island in the 1940s. One study conducted in Yellowstone National Park, where both species coexist, concluded that the coyote population in the Lamar River Valley declined by 39% following the reintroduction of wolves in the 1990s, while coyote populations in wolf inhabited areas of the Grand Teton National Park are 33% lower than in areas where they are absent. Wolves have been observed to not tolerate coyotes in their vicinity, though coyotes have been known to trail wolves to feed on their kills.

Coyotes may compete with cougars in some areas. In the eastern Sierra Nevadas, coyotes compete with cougars over mule deer. Cougars normally outcompete and dominate coyotes, and may kill them occasionally, thus reducing coyote predation pressure on smaller carnivores such as foxes and bobcats. Coyotes that are killed are sometimes not eaten, perhaps indicating that these compromise competitive interspecies interactions, however there are multiple confirmed cases of cougars also eating coyotes. In northeastern Mexico, cougar predation on coyotes continues apace but coyotes were absent from the prey spectrum of sympatric jaguars, apparently due to differing habitat usages.

Other than by gray wolves and cougars, predation on adult coyotes is relatively rare but multiple other predators can be occasional threats. In some cases, adult coyotes have been preyed upon by both American black and grizzly bears, American alligators, large Canada lynx and golden eagles. At kill sites and carrion, coyotes, especially if working alone, tend to be dominated by wolves, cougars, bears, wolverines and, usually but not always, eagles (i.e., bald and golden). When such larger, more powerful and/or more aggressive predators such as these come to a shared feeding site, a coyote may either try to fight, wait until the other predator is done or occasionally share a kill, but if a major danger such as wolves or an adult cougar is present, the coyote will tend to flee.

Coyotes rarely kill healthy adult red foxes, and have been observed to feed or den alongside them, though they often kill foxes caught in traps. Coyotes may kill fox kits, but this is not a major source of mortality. In southern California, coyotes frequently kill gray foxes, and these smaller canids tend to avoid areas with high coyote densities.

In some areas, coyotes share their ranges with bobcats. These two similarly-sized species rarely physically confront one another, though bobcat populations tend to diminish in areas with high coyote densities. However, several studies have demonstrated interference competition between coyotes and bobcats, and in all cases coyotes dominated the interaction. Multiple researchers reported instances of coyotes killing bobcats, whereas bobcats killing coyotes is more rare. Coyotes attack bobcats using a bite-and-shake method similar to what is used on medium-sized prey. Coyotes (both single individuals and groups) have been known to occasionally kill bobcats â in most cases, the bobcats were relatively small specimens, such as adult females and juveniles. However, coyote attacks (by an unknown number of coyotes) on adult male bobcats have occurred. In California, coyote and bobcat populations are not negatively correlated across different habitat types, but predation by coyotes is an important source of mortality in bobcats. Biologist Stanley Paul Young noted that in his entire trapping career, he had never successfully saved a captured bobcat from being killed by coyotes, and wrote of two incidents wherein coyotes chased bobcats up trees. Coyotes have been documented to directly kill Canada lynx on occasion, and compete with them for prey, especially snowshoe hares. In some areas, including central Alberta, lynx are more abundant where coyotes are few, thus interactions with coyotes appears to influence lynx populations more than the availability of snowshoe hares.

Due to the coyote's wide range and abundance throughout North America, it is listed as Least Concern by the International Union for Conservation of Nature (IUCN). The coyote's pre-Columbian range was limited to the Southwest and Plains regions of North America, and northern and central Mexico. By the 19th century, the species expanded north and east, expanding further after 1900, coinciding with land conversion and the extirpation of wolves. By this time, its range encompassed the entire North American continent, including all of the contiguous United States and Mexico, southward into Central America, and northward into most of Canada and Alaska. This expansion is ongoing, and the species now occupies the majority of areas between 8Â°N (Panama) and 70Â°N (northern Alaska).

Although it was once widely believed that coyotes are recent immigrants to southern Mexico and Central America, aided in their expansion by deforestation, Pleistocene and Early Holocene records, as well as records from the pre-Columbian period and early European colonization show that the animal was present in the area long before modern times. Nevertheless, range expansion did occur south of Costa Rica during the late 1970s and northern Panama in the early 1980s, following the expansion of cattle-grazing lands into tropical rain forests. The coyote is predicted to appear in northern Belize in the near future, as the habitat there is favorable to the species. Concerns have been raised of a possible expansion into South America through the Panamanian Isthmus, should the DariÃ©n Gap ever be closed by the Pan-American Highway. This fear was partially confirmed in January 2013, when the species was recorded in eastern Panama's Chepo District, beyond the Panama Canal.

A 2017 genetic study proposes that coyotes were originally not found in the area of the eastern United States. From the 1890s, dense forests were transformed into agricultural land and wolf control implemented on a large scale, leaving a niche for coyotes to disperse into. There were two major dispersals from two populations of genetically distinct coyotes. The first major dispersal to the northeast came in the early 20th century from those coyotes living in the northern Great Plains. These came to New England via the northern Great Lakes region and southern Canada, and to Pennsylvania via the southern Great Lakes region, meeting together in the 1940s in New York and Pennsylvania. These coyotes have hybridized with the remnant gray wolf and eastern wolf populations, which has added to coyote genetic diversity and may have assisted adaptation to the new niche. The second major dispersal to the southeast came in the mid-20th century from Texas and reached the Carolinas in the 1980s. These coyotes have hybridized with the remnant red wolf populations before the 1970s when the red wolf was extirpated in the wild, which has also added to coyote genetic diversity and may have assisted adaptation to this new niche as well. Both of these two major coyote dispersals have experienced rapid population growth and are forecast to meet along the mid-Atlantic coast. The study concludes that for coyotes the long range dispersal, gene flow from local populations, and rapid population growth may be inter-related.

Among large North American carnivores, the coyote probably carries the largest number of diseases and parasites, likely due to its wide range and varied diet. Viral diseases known to infect coyotes include rabies, canine distemper, infectious canine hepatitis, four strains of equine encephalitis, and oral papillomatosis. By the late 1970s, serious rabies outbreaks in coyotes had ceased to be a problem for over 60 years, though sporadic cases every 1â5 years did occur. Distemper causes the deaths of many pups in the wild, though some specimens can survive infection. "Tularemia", a bacterial disease, infects coyotes from tick bites and through their rodent and lagomorph prey, and can be deadly for pups.

Coyotes can be infected by both demodectic and sarcoptic mange, the latter being the most common. Mite infestations are rare and incidental in coyotes, while tick infestations are more common, with seasonal peaks depending on locality (MayâAugust in the Northwest, MarchâNovember in Arkansas). Coyotes are only rarely infested with lice, while fleas infest coyotes from puphood, though they may be more a source of irritation than serious illness. "Pulex simulans" is the most common species to infest coyotes, while "Ctenocephalides canis" tends to occur only in places where coyotes and dogs (its primary host) inhabit the same area. Although coyotes are rarely host to flukes, they can nevertheless have serious effects on coyotes, particularly "Nanophyetus salmincola", which can infect them with salmon poisoning disease, a disease with a 90% mortality rate. Trematode "Metorchis conjunctus" can also infect coyotes. Tapeworms have been recorded to infest 60â95% of all coyotes examined. The most common species to infest coyotes are "Taenia pisiformis" and "Taenia crassiceps", which uses cottontail rabbits as intermediate hosts. The largest species known in coyotes is "T. hydatigena", which enters coyotes through infected ungulates, and can grow to lengths of . Although once largely limited to wolves, "Echinococcus granulosus" has expanded to coyotes since the latter began colonizing former wolf ranges. The most frequent ascaroid roundworm in coyotes is "Toxascaris leonina", which dwells in the coyote's small intestine and has no ill effects, except for causing the host to eat more frequently. Hookworms of the genus "Ancylostoma" infest coyotes throughout their range, being particularly prevalent in humid areas. In areas of high moisture, such as coastal Texas, coyotes can carry up to 250 hookworms each. The blood-drinking "A. caninum" is particularly dangerous, as it damages the coyote through blood loss and lung congestion. A 10-day-old pup can die from being host to as few as 25 "A. caninum" worms.

Coyote features as a trickster figure and skin-walker in the folktales of some Native Americans, notably several nations in the Southwestern and Plains regions, where he alternately assumes the form of an actual coyote or that of a man. As with other trickster figures, Coyote acts as a picaresque hero who rebels against social convention through deception and humor. Folklorists such as Harris believe coyotes came to be seen as tricksters due to the animal's intelligence and adaptability. After the European colonization of the Americas, Anglo-American depictions of Coyote are of a cowardly and untrustworthy animal. Unlike the gray wolf, which has undergone a radical improvement of its public image, Anglo-American cultural attitudes towards the coyote remain largely negative.

In the Maidu creation story, Coyote introduces work, suffering, and death to the world. Zuni lore has Coyote bringing winter into the world by stealing light from the kachinas. The Chinook, Maidu, Pawnee, Tohono O'odham, and Ute portray the coyote as the companion of The Creator. A Tohono O'odham flood story has Coyote helping Montezuma survive a global deluge that destroys humanity. After The Creator creates humanity, Coyote and Montezuma teach people how to live. The Crow creation story portrays Old Man Coyote as The Creator. In The Dineh creation story, Coyote was present in the First World with First Man and First Woman, though a different version has it being created in the Fourth World. The Navajo Coyote brings death into the world, explaining that without death, too many people would exist, thus no room to plant corn.
Prior to the Spanish conquest of the Aztec Empire, Coyote played a significant role in Mesoamerican cosmology. The coyote symbolized military might in Classic era Teotihuacan, with warriors dressing up in coyote costumes to call upon its predatory power. The species continued to be linked to Central Mexican warrior cults in the centuries leading up to the post-Classic Aztec rule. In Aztec mythology, HuehuecÃ³yotl (meaning "old coyote"), the god of dance, music and carnality, is depicted in several codices as a man with a coyote's head. He is sometimes depicted as a womanizer, responsible for bringing war into the world by seducing Xochiquetzal, the goddess of love. Epigrapher David H. Kelley argued that the god Quetzalcoatl owed its origins to pre-Aztec Uto-Aztecan mythological depictions of the coyote, which is portrayed as mankind's "Elder Brother", a creator, seducer, trickster, and culture hero linked to the morning star.

Coyote attacks on humans are uncommon and rarely cause serious injuries, due to the relatively small size of the coyote, but have been increasingly frequent, especially in California. There have been only two confirmed fatal attacks: one on a three-year-old named Kelly Keen in Glendale, California and another on a nineteen-year-old named Taylor Mitchell in Nova Scotia, Canada. In the 30 years leading up to March 2006, at least 160 attacks occurred in the United States, mostly in the Los Angeles County area. Data from United States Department of Agriculture (USDA) Wildlife Services, the California Department of Fish and Game, and other sources show that while 41 attacks occurred during the period of 1988â1997, 48 attacks were verified from 1998 through 2003. The majority of these incidents occurred in Southern California near the suburban-wildland interface.

In the absence of the harassment of coyotes practiced by rural people, urban coyotes are losing their fear of humans, which is further worsened by people intentionally or unintentionally feeding coyotes. In such situations, some coyotes have begun to act aggressively toward humans, chasing joggers and bicyclists, confronting people walking their dogs, and stalking small children. Non-rabid coyotes in these areas sometimes target small children, mostly under the age of 10, though some adults have been bitten.

Although media reports of such attacks generally identify the animals in question as simply "coyotes", research into the genetics of the eastern coyote indicates those involved in attacks in northeast North America, including Pennsylvania, New York, New England, and eastern Canada, may have actually been coywolves, hybrids of "Canis latrans" and "C. lupus," not fully coyotes.

Coyotes are presently the most abundant livestock predators in western North America, causing the majority of sheep, goat, and cattle losses. For example, according to the National Agricultural Statistics Service, coyotes were responsible for 60.5% of the 224,000 sheep deaths attributed to predation in 2004. The total number of sheep deaths in 2004 comprised 2.22% of the total sheep and lamb population in the United States, which, according to the National Agricultural Statistics Service USDA report, totaled 4.66Â million and 7.80Â million heads respectively as of July 1, 2005. Because coyote populations are typically many times greater and more widely distributed than those of wolves, coyotes cause more overall predation losses. United States government agents routinely shoot, poison, trap, and kill about 90,000 coyotes each year to protect livestock. An Idaho census taken in 2005 showed that individual coyotes were 5% as likely to attack livestock as individual wolves. In Utah, more than 11,000 coyotes were killed for bounties totaling over $500,000 in the fiscal year ending June 30, 2017.

Livestock guardian dogs are commonly used to aggressively repel predators and have worked well in both fenced pasture and range operations. A 1986 survey of sheep producers in the USA found that 82% reported the use of dogs represented an economic asset.

Re-wilding cattle, which involves increasing the natural protective tendencies of cattle, is a method for controlling coyotes discussed by Temple Grandin of Colorado State University. This method is gaining popularity among producers who allow their herds to calve on the range and whose cattle graze open pastures throughout the year.

Coyotes typically bite the throat just behind the jaw and below the ear when attacking adult sheep or goats, with death commonly resulting from suffocation. Blood loss is usually a secondary cause of death. Calves and heavily fleeced sheep are killed by attacking the flanks or hindquarters, causing shock and blood loss. When attacking smaller prey, such as young lambs, the kill is made by biting the skull and spinal regions, causing massive tissue and bone damage. Small or young prey may be completely carried off, leaving only blood as evidence of a kill. Coyotes usually leave the hide and most of the skeleton of larger animals relatively intact, unless food is scarce, in which case they may leave only the largest bones. Scattered bits of wool, skin, and other parts are characteristic where coyotes feed extensively on larger carcasses.

Tracks are an important factor in distinguishing coyote from dog predation. Coyote tracks tend to be more oval-shaped and compact than those of domestic dogs, and their claw marks are less prominent and the tracks tend to follow a straight line more closely than those of dogs. With the exception of sighthounds, most dogs of similar weight to coyotes have a slightly shorter stride. Coyote kills can be distinguished from wolf kills by less damage to the underlying tissues in the former. Also, coyote scat tends to be smaller than wolf scat.

Coyotes are often attracted to dog food and animals that are small enough to appear as prey. Items such as garbage, pet food, and sometimes feeding stations for birds and squirrels attract coyotes into backyards. About three to five pets attacked by coyotes are brought into the Animal Urgent Care hospital of South Orange County (California) each week, the majority of which are dogs, since cats typically do not survive the attacks. Scat analysis collected near Claremont, California, revealed that coyotes relied heavily on pets as a food source in winter and spring. At one location in Southern California, coyotes began relying on a colony of feral cats as a food source. Over time, the coyotes killed most of the cats, and then continued to eat the cat food placed daily at the colony site by people who were maintaining the cat colony.
Coyotes usually attack smaller-sized dogs, but they have been known to attack even large, powerful breeds such as the Rottweiler in exceptional cases. Dogs larger than coyotes, such as greyhounds, are generally able to drive them off, and have been known to kill coyotes. Smaller breeds are more likely to suffer injury or death.

Coyote hunting is one of the most common forms of predator hunting that humans partake in. There are not many regulations with regard to the taking of the coyote which means there are many different methods that can be used to hunt the animal. The most common forms are trapping, calling, and hound hunting. Since coyotes are colorblind, seeing only in shades of gray and subtle blues, open camouflages, and plain patterns are ideal. The average male coyote weighs 8 to 20Â kg (18 to 44Â lbs) and the average female coyote 7 to 18Â kg (15 to 40Â lbs) a universal projectile that can perform between those weights is the .223 Remington. When hunting it is important the projectile expand in the target after the entry but before the exit, this way the projectile delivers the most energy. The .223 Remington has proven to deliver this energy effectively and reliably. Coyotes being the light and agile animals they are, they often leave a very light impression on terrain. The coyote's footprint is oblong, approximately 6.35Â cm (2.5-inches) long and 5.08Â cm (2-inches) wide. There are 4 claws in both their front and hind paws. The coyote's center pad is relatively shaped like that of a rounded triangle. Like the domestic dog the coyote's front paw is slightly larger than the hind paw. It is also important to note that the coyotes paw is most similar to that of the domestic dog.

Prior to the mid-19th century, coyote fur was considered worthless. This changed with the diminution of beavers, and by 1860, the hunting of coyotes for their fur became a great source of income (75 cents to $1.50 per skin) for wolfers in the Great Plains. Coyote pelts were of significant economic importance during the early 1950s, ranging in price from $5 to $25 per pelt, depending on locality. The coyote's fur is not durable enough to make rugs, but can be used for coats and jackets, scarves, or muffs. The majority of pelts are used for making trimmings, such as coat collars and sleeves for women's clothing. Coyote fur is sometimes dyed black as imitation silver fox.

Coyotes were occasionally eaten by trappers and mountain men during the western expansion. Coyotes sometimes featured in the feasts of the Plains Indians, and coyote pups were eaten by the indigenous people of San Gabriel, California. The taste of coyote meat has been likened to that of the wolf, and is more tender than pork when boiled. Coyote fat, when taken in the fall, has been used on occasion to grease leather or eaten as a spread.

Coyotes were probably semidomesticated by various pre-Columbian cultures. Some 19th-century writers wrote of coyotes being kept in native villages in the Great Plains. The coyote is easily tamed as a pup, but can become destructive as an adult. Both full-blooded and hybrid coyotes can be playful and confiding with their owners, but are suspicious and shy of strangers, though coyotes being tractable enough to be used for practical purposes like retrieving and pointing have been recorded. A tame coyote named "Butch", caught in the summer of 1945, had a short-lived career in cinema, appearing in "Smoky" and "Ramrod" before being shot while raiding a henhouse.






</doc>
<doc id="6711" url="https://en.wikipedia.org/wiki?curid=6711" title="Compressor (disambiguation)">
Compressor (disambiguation)

A compressor is a mechanical device that increases the pressure of a gas by reducing its volume. 

Compressor may also refer to:




</doc>
<doc id="6713" url="https://en.wikipedia.org/wiki?curid=6713" title="Conan the Barbarian">
Conan the Barbarian

Conan the Barbarian (also known as Conan the Cimmerian) is a fictional sword and sorcery hero who originated in pulp magazines and has since been adapted to books, comics, several films (including "Conan the Barbarian" and "Conan the Destroyer"), television programs (animated and live-action), video games, role-playing games, and other media. The character was created by writer Robert E. Howard in 1932 for a series of fantasy stories published in "Weird Tales" magazine.

Conan the Barbarian was created by Robert E. Howard in a series of fantasy stories published in "Weird Tales" in 1932. For months, Howard had been in search of a new character to market to the burgeoning pulp outlets of the early 1930s. In October 1931, he submitted the short story "People of the Dark" to Clayton Publications' new magazine, "Strange Tales of Mystery and Terror" (June 1932). "People of the Dark" is a remembrance story of "past lives", and in its first-person narrative the protagonist describes one of his previous incarnations; Conan is a black-haired barbarian hero who swears by a deity called Crom. Some Howard scholars believe this Conan to be a forerunner of the more famous character.

In February 1932, Howard vacationed at a border town on the lower Rio Grande. During this trip, he further conceived the character of Conan and also wrote the poem "Cimmeria", much of which echoes specific passages in Plutarch's "Lives". According to some scholars, Howard's conception of Conan and the Hyborian Age may have originated in Thomas Bulfinch's "The Outline of Mythology" (1913) which inspired Howard to "coalesce into a coherent whole his literary aspirations and the strong physical, autobiographical elements underlying the creation of Conan".

Having digested these prior influences after he returned from his trip, Howard rewrote a rejected story, "By This Axe I Rule!" (May 1929), replacing his existing character Kull of Atlantis with his new hero, and retitling it "The Phoenix on the Sword". Howard also wrote "The Scarlet Citadel" and "The Frost-Giant's Daughter", inspired by the Greek myth of Daphne, and submitted both stories to "Weird Tales" magazine. Although "The Frost-Giant's Daughter" was rejected, the magazine accepted "The Phoenix on the Sword", after it received the requested polishing, and published it in the December 1932 issue. "The Scarlet Citadel" was published the following month. .

"The Phoenix on the Sword" appeared in "Weird Tales" cover-dated DecemberÂ 1932. Editor Farnsworth Wright subsequently prompted Howard to write an 8,000-word essay for personal use detailing "the Hyborian Age", the fictional setting for Conan. Using this essay as his guideline, Howard began plotting "The Tower of the Elephant", a new Conan story that was the first to truly integrate his new conception of the Hyborian world.

The publication and success of "The Tower of the Elephant" spurred Howard to write many more Conan stories for "Weird Tales". By the time of Howard's suicide in 1936, he had written 21Â complete stories, 17Â ofÂ which had been published, as well as a number of unfinished fragments.

Following Howard's death, the copyright of the Conan stories passed through several hands. Eventually, under the guidance of L. Sprague de Camp and Lin Carter, the stories were edited, revised, and sometimes rewritten. For roughly 40 years, the original versions of Howard's Conan stories remained out of print. In 1977, the publisher Berkley Books issued three volumes using the earliest published form of the texts from "Weird Tales", but these failed to displace the edited versions. In the 1980s and 1990s, the copyright holders of the Conan franchise permitted Howard's stories to go out of print entirely, while continuing to sell Conan works by other authors.

In 2000, the British publisher Gollancz Science Fiction issued a two-volume, complete edition of Howard's Conan stories as part of its Fantasy Masterworks imprint, which included several stories that had never seen print in their original form. The Gollancz edition mostly used the versions of the stories as published in "Weird Tales".

The two volumes were combined and the stories restored to chronological order as "The Complete Chronicles of Conan: Centenary Edition" (Gollancz Science Fiction, 2006; edited and with an Afterword by Steve Jones).

In 2003, another British publisher, Wandering Star Books, made an effort both to restore Howard's original manuscripts and to provide a more scholarly and historical view of the Conan stories. It published hardcover editions in England, which were republished in the United States by the Del Rey imprint of Ballantine Books. The first book, "Conan of Cimmeria: Volume One (1932â1933)" (2003; published in the US as "The Coming of Conan the Cimmerian") includes Howard's notes on his fictional setting, as well as letters and poems concerning the genesis of his ideas. This was followed by "Conan of Cimmeria: Volume Two (1934)" (2004; published in the US as "The Bloody Crown of Conan") and "Conan of Cimmeria: Volume Three (1935â1936)" (2005; published in the US as "The Conquering Sword of Conan"). These three volumes combined include all of the original, unedited Conan stories.

The various stories of Conan the Barbarian occur in the pseudo-historical "Hyborian Age", set after the destruction of Atlantis and before the rise of any known ancient civilization. This is a specific epoch in a fictional timeline created by Howard for many of the low fantasy tales of his artificial legendary.

The reasons behind the invention of the Hyborian Age were perhaps commercial: Howard had an intense love for history and historical dramas; however, at the same time, he recognized the difficulties and the time-consuming research work needed in maintaining historical accuracyâand moreover, the poorly-stocked libraries in the rural part of Texas where Howard lived just didn't have the material needed for such historical research. By conceiving a timeless settingâ"a "vanished" age"âand by carefully choosing names that resembled human history, Howard shrewdly avoided the problem of historical anachronisms and his need for lengthy exposition.

According to "The Phoenix on the Sword", the adventures of Conan take place "Between the years when the oceans drank Atlantis and the gleaming cities, and the years of the rise of the Sons of Aryas."

Conan is a Cimmerian. From the writings of Robert E. Howard (The Hyborian Age among others) it is known that his Cimmerians are based on the Celts or perhaps the historic Cimmerians, based on the described geography and the existence of said people. He was born on a battlefield and is the son of a village blacksmith. Conan matured quickly as a youth and, by age fifteen, he was already a respected warrior who had participated in the destruction of the Aquilonian fortress of Venarium. After its demise, he was struck by wanderlust and began the adventures chronicled by Howard, encountering skulking monsters, evil wizards, tavern wenches, and beautiful princesses. He roamed throughout the Hyborian Age nations as a thief, outlaw, mercenary, and pirate. As he grew older, he began commanding vast units of warriors and escalating his ambitions. In his forties, he seized the crown from the tyrannical king of Aquilonia, the most powerful kingdom of the Hyborian Age, having strangled the previous ruler on the steps of his own throne. Conan's adventures often result in him performing heroic feats, though his motivation for doing so is largely to protect his own survival or for personal gain.

A conspicuous element of Conan's character is his chivalry. He is extremely reluctant to fight women (even when they fight him) and has a strong tendency to save a damsel in distress. In "Jewels of Gwahlur", he has to make a split-second decision whether to save the dancing girl Muriela or the chest of priceless gems which he spent months in search of. So, without hesitation, he rescues Muriela and allows for the treasure to be irrevocably lost. In "The Black Stranger", Conan saves the exile Zingaran Lady Belesa at considerable risk to himself, giving her as a parting gift his fortune in gems big enough to have a comfortable and wealthy life in Zingara, while asking for no favors in return. Reviewer Jennifer Bard also noted that when Conan is in a pirate crew or a robber gang led by another male, his tendency is to subvert and undermine the leader's authority, and eventually supplant (and often, kill) him (e.g. "Pool of the Black One", "A Witch Shall be Born", "Shadows in the Moonlight"). Conversely, in "Queen of the Black Coast", it is noted that Conan "generally agreed to Belit's plan. Hers was the mind that directed their raids, his the arm that carried out her ideas. It was a good life." And at the end of "Red Nails", Conan and Valeria seem to be headed towards a reasonably amicable piratical partnership.

George Baxter noted that "Conan's recorded history mentions him as being prominently involved, at one time or another, with four different pirate fraternities, on two different seas, as well being a noted leader of land robbers at three different locales. Yet, we hardly ever see him involved in, well, robbing people. To be sure, he speaks about it often and with complete candor: "We Kozaks took to plundering the outlying dominions of Koth, Zamora, and Turan impartially" he says in "Shadows in the Moonlight". But that was "before" the story began. And "We're bound for waters where the seaports are fat, and the merchant ships are crammed with plunder!" Conan declares at the end of "The Pool of the Black One". But this plundering will take place "after" the story ends. When we see Conan onstage, we see him do many other things: he intervenes in the politics and dynastic struggles of various kingdoms; he hunts for hidden treasure; he explores desert islands and lost cities; he fights countless terrible monsters and evil sorcerers; he saves countless beautiful women and makes them fall in love with him... What we virtually never see Conan do is engage in the proper business of an armed robber, on land or by seaâwhich is to attack people who never threatened or provoked you, take away their possessions by main force, and run your sword through them if they dare to resist. A bit messy business, that. Armchair adventurers, who like to enjoy a good yarn in the perfect safety and comfort of their suburban homes, might not have liked to read it."

Conan has "sullen", "smoldering", and "volcanic" blue eyes with a black "square-cut mane". Howard once describes him as having a hairy chest and, while comic book interpretations often portray Conan as wearing a loincloth or other minimalist clothing to give him a more barbaric image, Howard describes the character as wearing whatever garb is typical for the kingdom and culture in which Conan finds himself. Howard never gave a strict height or weight for Conan in a story, only describing him in loose terms like "giant" and "massive". In the tales, no human is ever described as being stronger than Conan, although several are mentioned as taller (such as the strangler Baal-pteor) or of larger bulk. In a letter to P. Schuyler Miller and John D. Clark in 1936, only three months before Howard's death, Conan is described as standing 6Â ft/183Â cm and weighing when he takes part in an attack on Venarium at only 14 years old, though being far from fully grown. Conan himself says in "Beyond the Black River" that he had "...not yet seen 15 snows".
Although Conan is muscular, Howard frequently compares his agility and way of moving to that of a panther (see, for instance, "Jewels of Gwahlur", "Beyond the Black River", or "Rogues in the House"). His skin is frequently characterized as bronzed from constant exposure to the sun. In his younger years, he is often depicted wearing a light chain shirt and a horned helmet, though appearances vary with different stories.

During his reign as king of Aquilonia, Conan was

Howard imagined the Cimmerians as a pre-Celtic people with mostly black hair and blue or grey eyes. Ethnically, the Cimmerians to which Conan belongs are descendants of the Atlanteans, though they do not remember their ancestry. In his fictional historical essay "The Hyborian Age", Howard describes how the people of Atlantisâthe land where his character King Kull originatedâhad to move east after a great cataclysm changed the face of the world and sank their island, settling where Ireland and Scotland would eventually be located. Thus they are (in Howard's work) the ancestors of the Irish and Scottish (the Celtic Gaels) and not the Picts, the other ancestor of modern Scots who also appear in Howard's work. In the same work, Howard also described how the Cimmerians eventually moved south and east after the age of Conan (presumably in the vicinity of the Black Sea, where the historical Cimmerians dwelt).

Despite his brutish appearance, Conan uses his brains as well as his brawn. The Cimmerian is a highly skilled warrior, possibly without peer with a sword, but his travels have given him vast experience in other trades, especially as a thief. He's also a talented commander, tactician, and strategist, as well as a born leader. In addition, Conan has advanced knowledge of languages and codes and is able to recognize, or even decipher, certain ancient or secret signs and writings. For example, in "Jewels of Gwahlur" Howard states: "In his roaming about the world the giant adventurer had picked up a wide smattering of knowledge, particularly including the speaking and reading of many alien tongues. Many a sheltered scholar would have been astonished at the Cimmerian's linguistic abilities." He also has incredible stamina, enabling him to go without sleep for a few days. In "A Witch Shall be Born", Conan fights armed men until he is overwhelmed, captured, and crucified, before going an entire night and day without water. However, Conan still possesses the strength to pull the nails from his feet, while hoisting himself into a horse's saddle and riding for ten miles.

Another noticeable trait is his sense of humor, largely absent in the comics and movies, but very much a part of Howard's original vision of the character (particularly apparent in "Xuthal of the Dusk", also known as "The Slithering Shadow.") His sense of humor can also be rather grimly ironic, as was demonstrated by how he unleashes his own version of justice on the treacherousâand ill-fatedâinnkeeper Aram Baksh in "Shadows in Zamboula".

He is a loyal friend to those true to him, with a barbaric code of conduct that often marks him as more honorable than the more sophisticated people he meets in his travels. Indeed, his straightforward nature and barbarism are constants in all the tales.

Conan is a formidable combatant both armed and unarmed. With his back to the wall, Conan is capable of engaging and killing opponents by the score. This is seen in several stories, such as "Queen of the Black Coast", "The Scarlet Citadel", and "A Witch Shall Be Born". Conan is not superhuman, though; he needed the providential help of Zelata's wolf to defeat four Nemedian soldiers in Howard's novel "The Hour of the Dragon". Some of his hardest victories have come from fighting single opponents of inhuman strength: one such as Thak, an ape-like humanoid from "Rogues in the House", or the strangler Baal-Pteor in "Shadows in Zamboula". Conan is far from untouchable and has been captured or defeated several times (on one occasion, knocking himself out drunkenly after running into a wall).

Howard frequently corresponded with H. P. Lovecraft, and the two would sometimes insert references or elements of each other's settings in their works. Later editors reworked many of the original Conan stories by Howard, thus diluting this connection. Nevertheless, many of Howard's unedited Conan stories are arguably part of the Cthulhu Mythos. Additionally, many of the Conan stories by Howard, de Camp, and Carter used geographical place names from Clark Ashton Smith's Hyperborean Cycle.





A number of untitled synopses for Conan stories also exist.


The character of Conan has proven durably popular, resulting in Conan stories by later writers such as Poul Anderson, Leonard Carpenter, Lin Carter, L. Sprague de Camp, Roland J. Green, John C. Hocking, Robert Jordan, Sean A. Moore, BjÃ¶rn Nyberg, Andrew J. Offutt, Steve Perry, John Maddox Roberts, Harry Turtledove, and Karl Edward Wagner. Some of these writers have finished incomplete Conan manuscripts by Howard. Others were created by rewriting Howard stories which originally featured entirely different characters from entirely different milieus. Most, however, are completely original works. In total, more than fifty novels and dozens of short stories featuring the Conan character have been written by authors other than Howard.

The Gnome Press edition (1950â1957) was the first hardcover collection of Howard's Conan stories, including all the original Howard material known to exist at the time, some left unpublished in his lifetime. The later volumes contain some stories rewritten by L. Sprague de Camp (like "The Treasure of Tranicos"), including several non-Conan Howard stories, mostly historical exotica situated in the Levant at the time of the Crusades, which he turned into Conan yarns. The Gnome edition also issued the first Conan story written by an author other than Howardâthe final volume published, which is by BjÃ¶rn Nyberg and revised by de Camp.

The Lancer/Ace editions (1966â1977), under the direction of de Camp and Lin Carter, were the first comprehensive paperbacks, compiling the material from the Gnome Press series together in chronological order with all the remaining original Howard material, including that left unpublished in his lifetime and fragments and outlines. These were completed by de Camp and Carter. The series also included Howard stories originally featuring other protagonists that were rewritten by de Camp as Conan stories. New Conan stories written entirely by de Camp and Carter were added as well. Lancer Books went out of business before bringing out the entire series, the publication of which was completed by Ace Books. Eight of the eventual twelve volumes published featured dynamic cover paintings by Frank Frazetta that, for many fans, presented the definitive, iconic impression of Conan and his world. For decades to come, most other portrayals of the Cimmerian and his imitators were heavily influenced by the cover paintings of this series.

Most editions after the Lancer/Ace series have been of either the original Howard stories or Conan material by others, but not both. The exception are the Ace Maroto editions (1978â1981), which include both new material by other authors and older material by Howard, though the latter are some of the non-Conan tales rewritten as Conan stories by de Camp. Notable later editions of the original Howard Conan stories include the Donald M. Grant editions (1974â1989, incomplete); Berkley editions (1977); Gollancz editions (2000â2006), and Wandering Star/Del Rey editions (2003â2005). Later series of new Conan material include the Bantam editions (1978â1982) and Tor editions (1982â2004).

In an attempt to provide a coherent timeline which fit the numerous adventures of Conan penned by Robert E. Howard and later writers, various "Conan chronologies" have been prepared by many people from the 1930s onward. Note that no consistent timeline has yet accommodated every single Conan story. The following are the principal theories that have been advanced over the years.

The very first Conan cinematic project was planned by Edward Summer. Summer envisioned a series of Conan films, much like the James Bond franchise. He outlined six stories for this film series, but none were ever made. An original screenplay by Summer and Roy Thomas was written, but their lore-authentic screen story was never filmed. However, the resulting film, "Conan the Barbarian" (1982), was a combination of director John Milius' ideas and plots from Conan stories (written also by Howard's successors, notably Lin Carter and L. Sprague de Camp). The addition of Nietzschean motto and Conan's life philosophy were crucial for bringing the spirit of Howard's literature to the screen.

The plot of "Conan the Barbarian" (1982) begins with Conan being enslaved by the Vanir raiders of Thulsa Doom, a malevolent warlord who is responsible for the slaying of Conan's parents and the genocide of his people. Later, Thulsa Doom becomes a cult leader of a religion that worships Set, a Snake God. The vengeful Conan, the archer Subotai and the thief Valeria set out on a quest to rescue a princess held captive by Thulsa Doom. The film was directed by John Milius and produced by Dino De Laurentiis. The character of Conan was played by Arnold Schwarzenegger and was his break-through role as an actor.

This film was followed by a less popular sequel, "Conan the Destroyer" in 1984. This sequel was a more typical fantasy-genre film and was even less faithful to Howard's Conan stories, being just a picaresque story of an assorted bunch of adventurers.

The third film in the "Conan" trilogy was planned for 1987 to be titled "Conan the Conqueror". The director was to be either Guy Hamilton or John Guillermin. Since Arnold Schwarzenegger was committed to the film "Predator" and De Laurentiis's contract with the star had expired after his obligation to "Red Sonja" and "Raw Deal", he wasn't keen to negotiate a new one; thus the third Conan film sank into development hell. The script was eventually turned into "Kull the Conqueror".

There were rumors in the late 1990s of another Conan sequel, a story about an older Conan titled "King Conan: Crown of Iron", but Schwarzenegger's election in 2003 as governor of California ended this project. Warner Bros. spent seven years trying to get the project off the ground. However, in June 2007 the rights reverted to Paradox Entertainment, though all drafts made under Warner remained with them. In August 2007, it was announced that Millennium Films had acquired the rights to the project. Production was aimed for a SpringÂ 2006 start, with the intention of having stories more faithful to the Robert E. Howard creation. In June 2009, Millennium hired Marcus Nispel to direct. In January 2010, Jason Momoa was selected for the role of Conan. The film was released in August 2011, and met poor critical reviews and box office results.

In 2012, producers Chris Morgan and Frederick Malmberg announced plans for a sequel to the 1982 "Conan the Barbarian" titled "The Legend of Conan", with Arnold Schwarzenegger reprising his role as Conan. A year later, "Deadline" reported that Andrea Berloff would write the script. Years passed since the initial announcement as Schwarzenegger worked on other films, but as late as 2016, Schwarzenegger affirmed his enthusiasm for making the film, saying, "Interest is high ... but we are not rushing." The script was finished, and Schwarzenegger and Morgan were meeting with possible directors. In April 2017, producer Chris Morgan stated that Universal had dropped the project, although there was a possibility of a TV show. The story of the film was supposed to be set 30 years after the first, with some inspiration from Clint Eastwood's "Unforgiven".

There have been three television series related to Conan:

Conan the Barbarian has appeared in comics nearly non-stop since 1970. The comics are arguably, apart from the books, the vehicle that had the greatest influence on the longevity and popularity of the character. Aside from an earlier and unofficial Conan comic published in Mexico, the two main publishers of Conan comics have been Marvel Comics and Dark Horse Comics. Marvel Comics launched "Conan the Barbarian" (1970â1993) and the classic "Savage Sword of Conan" (1974â1995). Dark Horse launched their "Conan" series in 2003. Dark Horse Comics is currently publishing compilations of the 1970s Marvel Comics series in trade paperback format.

Barack Obama, former President of the United States, is a collector of Conan the Barbarian comic books and a big fan of the character and appeared as a character in a comic book called "Barack the Barbarian" from Devil's Due.

Marvel Comics introduced a relatively lore-faithful version of Conan the Barbarian in 1970 with "Conan the Barbarian", written by Roy Thomas and illustrated by Barry Windsor-Smith. Smith was succeeded by penciller John Buscema, while Thomas continued to write for many years. Later writers included J.M. DeMatteis, Bruce Jones, Michael Fleisher, Doug Moench, Jim Owsley, Alan Zelenetz, Chuck Dixon and Don Kraar. In 1974, "Conan the Barbarian" series spawned the more adult-oriented, black-and-white comics magazine "Savage Sword of Conan", written by Thomas with art mostly by Buscema or Alfredo Alcala. Marvel also published several graphic novels starring the character , and a handbook with detailed information about the Hyborian world. Conan the Barbarian is also officially considered to be part of the larger Marvel Universe and has interacted with heroes and villains alike.

The Marvel Conan stories were also adapted as a newspaper comic strip which appeared daily and Sunday from 4Â September 1978 to 12Â April 1981. Originally written by Roy Thomas and illustrated by John Buscema, the strip was continued by several different Marvel artists and writers.

Dark Horse Comics began their comic adaptation of the Conan saga in 2003. Entitled simply "Conan", the series was first written by Kurt Busiek and pencilled by Cary Nord. Tim Truman replaced Busiek when Busiek signed an exclusive contract with DC Comics; however, Busiek issues were sometimes used for filler. This series is an interpretation of the original Conan material by Robert E. Howard with no connection whatsoever to the earlier Marvel comics or any Conan story not written or envisioned by Howard supplemented by wholly original material.

A second series, "Conan the Cimmerian" was released in 2008 by Tim Truman (writer) and TomÃ¡s Giorello (artist). The series ran for twenty-six issues, including an introductory "zero" issue.

Dark Horse's third series, "", began in December 2010 by Roy Thomas (writer) and Mike Hawthorne (artist) and ran for twelve issues.

A fourth series, "Conan the Barbarian", began in February 2012 by Brian Wood (writer) and Becky Cloonan (artist). It ran for twenty-five issues, and expanded on Robert E. Howard's "Queen of the Black Coast".

A fifth series, "Conan the Avenger", began in April 2014 by Fred Van Lente (writer) and Brian Ching (artist). It ran for twenty-five issues, and expanded on Robert E. Howard's The Snout in the Dark and A Witch Shall Be Born.

Dark Horse's sixth series, "Conan the Slayer", began in July 2016 by Cullen Bunn (writer) and Sergio DÃ¡vila (artist).

In 2018, Marvel reacquired the rights and started new runs of both Conan the Barbarian and Savage Sword of Conan in January/February 2019.




TSR, Inc. signed a license agreement in 1984 to publish Conan-related gaming material:

In 1988 Steve Jackson Games acquired a Conan license and started publishing Conan solo adventures for its "GURPS" generic system of rules as of 1988 and a "GURPS Conan" core rulebook in 1989:

In 2003 the British company Mongoose Publishing bought a license and acquired in turn the rights to make use of the Conan gaming franchise, publishing a Conan role-playing game from 2004 until 2010. The game ran the OGL System of rules that Mongoose established for its "OGL series" of games:

In 2010 Mongoose Publishing dropped the Conan license. In February 2015, another British company, Modiphius Entertainment, acquired the license, announcing plans to put out a new Conan role-playing game in August of that year. Actually, the core rulebook was not launched (via Kickstarter) until a whole year later, in February 2016, reaching by far all funds needed for publication. Long after the Kickstarter ended the core rulebook was launched in PDF format on January the 31st, 2017. The physical core rulebook finally started distribution in June 2017 :

Nine video games have been released based on the Conan mythos.







</doc>
<doc id="6715" url="https://en.wikipedia.org/wiki?curid=6715" title="Chris Marker">
Chris Marker

Chris Marker (; 29 July 1921 â 29 July 2012) was a French writer, photographer, documentary film director, multimedia artist and film essayist. His best known films are "La JetÃ©e" (1962), "A Grin Without a Cat" (1977) and "Sans Soleil" (1983). Marker is usually associated with the Left Bank subset of the French New Wave, that occurred in the late 1950s and 1960s, and included such other filmmakers as Alain Resnais, AgnÃ¨s Varda and Jacques Demy.

His friend and sometime collaborator Alain Resnais called him "the prototype of the twenty-first-century man." Film theorist Roy Armes has said of him: "Marker is unclassifiable because he is unique...The French Cinema has its dramatists and its poets, its technicians, and its autobiographers, but only has one true essayist: Chris Marker."

Marker was born Christian FranÃ§ois Bouche-Villeneuve. He was always elusive about his past and known to refuse interviews and not allow photographs to be taken of him; his place of birth is highly disputed. Some sources and Marker himself claim that he was born in Ulaanbaatar, Mongolia. Other sources say he was born in Belleville, Paris, and others, in Neuilly-sur-Seine. The 1949 edition of "Le CÅur Net" gives his birthday as 22 July. Film critic David Thomson has said, "Marker told me himself that Mongolia is correct. I have since concluded that Belleville is correctâbut that does not spoil the spiritual truth of Ulan Bator." When asked about his secretive nature, Marker said, "My films are enough for them [the audience]."

Marker was a philosophy student in France before World War II. During the German occupation of France, he joined the Maquis (FTP), a part of the French Resistance. At some point during the war he left France and joined the United States Air Force as a paratrooper, although some sources claim that this is not true. After the war, he began a career as a journalist, first writing for the journal "Esprit", a neo-Catholic, Marxist magazine where he met fellow journalist AndrÃ© Bazin. For "Esprit", Marker wrote political commentaries, poems, short stories, and (with Bazin) film reviews. He later became an early contributor to Bazin's "Cahiers du cinÃ©ma".

During this period, Marker began to travel around the world as a journalist and photographer, a vocation he pursued for the rest of his life. The French publishing company Ãditions du Seuil hired him as editor of the series "Petite PlanÃ¨te" ("Small World"). This collection devoted one edition to each country and included information and photographs. In 1949 Marker published his first novel, "Le Coeur net" ("The Forthright Spirit"), which was about aviation. In 1952 Marker published an illustrated essay on French writer Jean Giraudoux, "Giraudoux Par Lui-MÃªme".

During his early journalism career, Marker became increasingly interested in filmmaking and in the early 1950s experimented with photography. Around this time Marker met and befriended many members of the Left Bank Film Movement, including Alain Resnais, AgnÃ¨s Varda, Henri Colpi, Armand Gatti, and the novelists Marguerite Duras and Jean Cayrol. This group is often associated with the French New Wave directors who came to prominence during the same time period, and the groups were often friends and journalistic co-workers. The term "Left Bank" was first coined by film critic Richard Roud, who described them as having "fondness for a kind of Bohemian life and an impatience with the conformity of the Right Bank, a high degree of involvement in literature and the plastic arts, and a consequent interest in experimental filmmaking", as well as an identification with the political left. Anatole Dauman produced many of Marker's earliest films.

In 1952 Marker made his first film, "Olympia 52", a 16mm feature documentary about the 1952 Helsinki Olympic Games. In 1953 he collaborated with Resnais on the documentary "Statues Also Die". The film examines traditional African art such as sculptures and masks, and its decline with coming of Western colonialism. It won the 1954 Prix Jean Vigo, but was banned by French censors for its criticism of French colonialism.

After working as assistant director on Resnais's "Night and Fog" in 1955, Marker made "Sunday in Peking", a short documentary "film essay" in the style that characterized Marker's output for most of his career. Marker shot the film in two weeks while traveling through China with Armand Gatti in September 1955. In the film, Marker's commentary overlaps scenes from China, such as tombs that, contrary to Westernized understandings of Chinese legends, do not contain the remains of Ming Dynasty emperors.

After working on the commentary for Resnais's film "Le mystÃ¨re de l'atelier quinze" in 1957, Marker continued to refine his style with the feature documentary "Letter from Siberia". An essay film on the narrativization of Siberia, it contains Marker's signature commentary, which takes the form of a letter from the director, in the long tradition of epistolary treatments by French explorers of the "undeveloped" world. "Letter "looks at Siberia's movement into the 20th century and at some of the tribal cultural practices receding into the past. It combines footage Marker shot in Siberia with old newsreel footage, cartoon sequences, stills, and even an illustration of Alfred E. Neuman from "Mad Magazine" as well as a fake TV commercial as part of a humorous attack on Western mass culture. In producing a meta-commentary on narrativity and film, Marker uses the same brief filmic sequence three times but with different commentaryâthe first praising the Soviet Union, the second denouncing it, and the third taking an apparently neutral or "objective" stance.

In 1959 Marker made the animated film "Les Astronautes" with Walerian Borowczyk. The film was a combination of traditional drawings with still photography. In 1960 he made "Description d'un combat", a documentary on the State of Israel that reflects on its past and future. The film won the Golden Bear for Best Documentary at the 1961 Berlin Film Festival.

In January 1961, Marker traveled to Cuba and shot the film "Â¡Cuba SÃ­!" The film promotes and defends Fidel Castro and includes two interviews with him. It ends with an anti-American epilogue in which the United States is embarrassed by the Bay of Pigs Invasion fiasco, and was subsequently banned. The banned essay was included in Marker's first volume of collected film commentaries, "Commentaires I", published in 1961. The following year Marker published "CorÃ©ennes", a collection of photographs and essays on conditions in Korea.

Marker became known internationally for the short film "La JetÃ©e" ("The Pier") in 1962. It tells of a post-nuclear war experiment in time travel by using a series of filmed photographs developed as a photomontage of varying pace, with limited narration and sound effects. In the film, a survivor of a futuristic third World War is obsessed with distant and disconnected memories of a pier at the Orly Airport, the image of a mysterious woman, and a man's death. Scientists experimenting in time travel choose him for their studies, and the man travels back in time to contact the mysterious woman, and discovers that the man's death at the Orly Airport was his own. Except for one shot of the woman mentioned above sleeping and suddenly waking up, the film is composed entirely of photographs by Jean Chiabaud and stars Davos Hanich as the man, HÃ©lÃ¨ne ChÃ¢telain as the woman and filmmaker William Klein as a man from the future.

"La JetÃ©e" was the inspiration for Mamoru Oshii's 1987 debut live action feature "The Red Spectacles" (and later for parts of Oshii's 2001 film "Avalon") and also inspired Terry Gilliam's "12 Monkeys" (1995) and JonÃ¡s CuarÃ³n's "Year of the Nail" (2007). It also inspired many of Mira Nair's shots in her 2006 film "The Namesake".

While making "La JetÃ©e", Marker was simultaneously making the 150-minute documentary essay-film "Le joli mai", released in 1963. Beginning in the spring of 1962, Marker and his camera operator Pierre Lhomme shot 55 hours of footage interviewing random people on the streets of Paris. The questions, asked by the unseen Marker, range from their personal lives, as well as social and political issues of relevance at that time. As he had with montages of landscapes and indigenous art, Marker created a film essay that contrasted and juxtaposeed a variety of lives with his signature commentary (spoken by Marker's friends, singer-actor Yves Montand in the French version and Simone Signoret in the English version). The film has been compared to the "CinÃ©ma vÃ©ritÃ©" films of Jean Rouch, and criticized by its practitioners at the time. The term "CinÃ©ma vÃ©ritÃ©" was itself anathema to Marker, who never used it. It was shown in competition at the 1963 Venice Film Festival, where it won the award for Best First Work. It also won the Golden Dove Award at the Leipzig DOK Festival.

After the documentary "Le MystÃ¨re Koumiko" in 1965, Marker made "Si j'avais quatre dromadaires", an essay-film that, like "La JetÃ©e", is a photomontage of over 800 photographs Marker had taken over the previous 10 years in 26 countries. The commentary involves a conversation between a fictitious photographer and two friends, who discuss the photos. The film's title is an allusion to a poem by Guillaume Apollinaire. It was the last film in which Marker included "travel footage" for many years.

In 1967 Marker published his second volume of collected film essays, "Commentaires II". That same year, Marker organized the omnibus film "Loin du Vietnam", a protest against the Vietnam War with segments contributed by Marker, Jean-Luc Godard, Alain Resnais, AgnÃ¨s Varda, Claude Lelouch, William Klein, Michele Ray and Joris Ivens. The film includes footage of the war, from both sides, as well as anti-war protests in New York and Paris and other anti-war activities.

From this initial collection of filmmakers with left-wing political agendas, Marker created the group S.L.O.N. ("SociÃ©tÃ© pour le lancement des oeuvres nouvelles", "Society for launching new works", but also the Russian word for "elephant"). SLON was a film collective whose objectives were to make films and to encourage industrial workers to create film collectives of their own. Its members included Valerie Mayoux, Jean-Claude Lerner, Alain Adair and John Tooker. Marker is usually credited as director or co-director of all of the films made by SLON.

After the events of May 1968, Marker felt a moral obligation to abandon his own personal film career and devote himself to SLON and its activities. SLON's first film was about a strike at a RhodiacÃ©ta factory in France, "Ã bientÃ´t, j'espÃ¨re" ("RhodiacÃ©ta") in 1968. Later that year SLON made "La SixiÃ¨me face du pentagone", about an anti-war protest in Washington, D.C. and was a reaction to what SLON considered to be the unfair and censored reportage of such events on mainstream television. The film was shot by FranÃ§ois Reichenbach, who received co-director credit. "La Bataille des dix millions" was made in 1970 with Mayoux as co-director and Santiago Ãlvarez as cameraman and is about the 1970 sugar crop in Cuba and its disastrous effects on the country. In 1971, SLON made "Le Train en marche", a new prologue to Soviet filmmaker Aleksandr Medvedkin's 1935 film "Schastye", which had recently been re-released in France.

In 1974, SLON became I.S.K.R.A. ("Images, Sons, Kinescope, RÃ©alisations, Audiovisuelles", but also the name of Vladimir Lenin's political newspaper "Iskra," which also is a Russian word for "spark").

In 1974 returned to his personal work and made a film outside of ISKRA. "La Solitude du chanteur de fond" is a one-hour documentary about Marker's friend Yves Montand's benefit concert for Chilean refugees. The concert was Montand's first public performance in four years, and the documentary includes film clips from his long career as a singer and actor.

Marker had been working on a film about Chile with ISKRA since 1973. Marker had collaborated with Belgian sociologist Armand Mattelart and ISKRA members ValÃ©rie Mayoux and Jacqueline Meppiel to shoot and collect the visual materials, which Marker then edited together and provided the commentary for. The resulting film was the two and a half-hour documentary "La Spirale", released in 1975. The film chronicles events in Chile, beginning with the 1970 election of socialist President Salvador Allende until his murder and the resulting coup in 1973.

Marker then began work on one of his most ambitious films, "A Grin Without a Cat", released in 1977. The film's title refers to the Cheshire Cat from "Alice in Wonderland". The metaphor compares the promise of the global socialist movement before May 1968 (the grin) with its actual presence in the world after May 1968 (the cat). The film's original French title is "Le fond de l'air est rouge", which means "the air is essentially red", or "revolution is in the air", implying that the socialist movement was everywhere around the world.

The film was intended to be an all-encompassing portrait of political movements since May 1968, a summation of the work which he had taken part in for ten years. The film is divided into two parts: the first half focuses on the hopes and idealism before May 1968, and the second half on the disillusion and disappointments since those events. Marker begins the film with the Odessa Steps sequence from Sergei Eisenstein's film "The Battleship Potemkin", which Marker points out is a fictitious creation of Eisenstein which has still influenced the image of the historical event. Marker used very little commentary in this film, but the film's montage structure and preoccupation with memory make it a Marker film. Upon release, the film was criticized for not addressing many current issues of the New Left such as the woman's movement, sexual liberation and worker self-management. The film was re-released in the US in 2002.

In the late 1970s, Marker traveled extensively throughout the world, including an extended period in Japan. From this inspiration, he first published the photo-essay "Le DÃ©pays" in 1982, and then used the experience for his next film "Sans Soleil", released in 1982.

"Sans Soleil" stretches the limits of what could be called a documentary. It is an essay, a montage, mixing pieces of documentary with fiction and philosophical comments, creating an atmosphere of dream and science fiction. The main themes are Japan, Africa, memory and travel. A sequence in the middle of the film takes place in San Francisco, and heavily references Alfred Hitchcock's "Vertigo". Marker has said that "Vertigo" is the only film "capable of portraying impossible memory, insane memory." The film's commentary are credited to the fictitious cameraman Sandor Krasna, and read in the form of letters by an unnamed woman. Though centered around Japan, the film was also shot in such other countries as Guinea Bissau, Ireland and Iceland. "Sans Soleil" was shown at the 1983 Berlin Film Festival where it won the OCIC Award. It was also awarded the Sutherland Trophy at the 1983 British Film Institute Awards.

In 1984, Marker was invited by producer Serge Silberman to document the making of Akira Kurosawa's film "Ran". From this Marker made "A.K.", released in 1985. The film focuses more on Kurosawa's remote but polite personality than on the making of the film. The film was screened in the Un Certain Regard section at the 1985 Cannes Film Festival, before "Ran" itself had been released.

In 1985, Marker's long-time friend and neighbor Simone Signoret died of cancer. Marker then made the one-hour TV documentary "MÃ©moires pour Simone" as a tribute to her in 1986.

Beginning with "Sans Soleil", Marker developed a deep interest in digital technology. From 1985 to 1988, he worked on a conversational program (a prototypical chatbot) called "Dialector," which he wrote in Applesoft BASIC on an Apple II. He incorporated audiovisual elements in addition to the snippets of dialogue and poetry that "Computer" exchanged with the user. Version 6 of this program was revived from a floppy disk (with Marker's help and permission) and emulated online in 2015.

His interests in digital technology also led to his film "Level Five" (1996) and "Immemory" (1998, 2008), an interactive multimedia CD-ROM, produced for the Centre Pompidou (French language version) and from Exact Change (English version). Marker created a 19-minute multimedia piece in 2005 for the Museum of Modern Art in New York City titled "Owls at Noon Prelude: The Hollow Men" which was influenced by T. S. Eliot's poem.

Marker lived in Paris, and very rarely granted interviews. One exception was a lengthy interview with "LibÃ©ration" in 2003 in which he explained his approach to filmmaking. When asked for a picture of himself, he usually offered a photograph of a cat instead. (Marker was represented in Agnes Varda's 2008 documentary "The Beaches of Agnes" by a cartoon drawing of a cat, speaking in a technologically altered voice.) Marker's own cat was named "Guillaume-en-Ã©gypte". In 2009, Marker commissioned an to represent him in machinima works. The avatar was created by Exosius Woolley and first appeared in the short film / machinima, "Ouvroir the Movie by Chris Marker".

In the 2007 Criterion Collection release of "La JetÃ©e" and "Sans Soleil", Marker included a short essay, "Working on a Shoestring Budget". He confessed to shooting all of "Sans Soleil" with a silent film camera, and recording all the audio on a primitive audio cassette recorder. Marker also reminds the reader that only one short scene in "La JetÃ©e" is of a moving image, as Marker could only borrow a movie camera for one afternoon while working on the film.

From 2007 through 2011 Marker collaborated with the art dealer and publisher Peter Blum on a variety of projects that were exhibited at the Peter Blum galleries in New York City's Soho and Chelsea neighborhoods. Marker's works were also exhibited at the Peter Blum Gallery on 57th Street in 2014. These projects include several series of printed photographs titled "PASSENGERS", "Koreans", "Crush Art", "Quelle heure est-elle?", and "Staring Back"; a set of photogravures titled "After DÃ¼rer"; a book, "PASSENGERS"; and digital prints of movie posters, whose titles were often appropriated, including "Breathless", "Hiroshima Mon Amour", "Owl People", and "Rin Tin Tin". The video installations "Silent Movie" and "Owls at Noon Prelude: The Hollow Men" were exhibited at Peter Blum in 2009. These works were also exhibited at the 2014 & 2015 Venice Biennale, Whitechapel Gallery in London, the MIT List Visual Arts Center in Cambridge, Massachusetts, the Carpenter Center for the Visual Arts at Harvard University, the Moscow Photobiennale, Les Recontres d'Arles de la Photographie in Arles, France, the Centre de la Photographie in Geneva, Switzerland, the Walker Art Center in Minneapolis, Minnesota, the Wexner Center for the Arts in Columbus, Ohio, The Museum of Modern Art in New York, and the Pacific Film Archive in Berkeley, California. Since 2014 the artworks of the Estate of Chris Marker are represented by Peter Blum Gallery, New York.

Marker died on 29 July 2012, his 91st birthday.










</doc>
<doc id="6716" url="https://en.wikipedia.org/wiki?curid=6716" title="Cardinal vowels">
Cardinal vowels

Cardinal vowels are a set of reference vowels used by phoneticians in describing the sounds of languages. For instance, the vowel of the English word "feet" can be described with reference to cardinal vowel 1, , which is the cardinal vowel closest to it. It is often stated that to be able to use the cardinal vowel system effectively one must undergo training with an expert phonetician, working both on the recognition and the production of the vowels. Daniel Jones wrote "The values of cardinal vowels cannot be learnt from written descriptions; they should be learnt by oral instruction from a teacher who knows them".

A cardinal vowel is a vowel sound produced when the tongue is in an extreme position, either front or back, high or low. The current system was systematised by Daniel Jones in the early 20th century, though the idea goes back to earlier phoneticians, notably Ellis and Bell.

Cardinal vowels are not vowels of any particular language, but a measuring system. However, some languages contain vowel or vowels that are close to the cardinal vowel(s). An example of such language is Ngwe, which is spoken in Cameroon. It has been cited as a language with a vowel system that has 8 vowels which are rather similar to the 8 primary cardinal vowels (Ladefoged 1971:67).

Three of the cardinal vowelsâ, and âhave articulatory definitions. The vowel is produced with the tongue as far forward and as high in the mouth as is possible (without producing friction), with spread lips. The vowel is produced with the tongue as far back and as high in the mouth as is possible, with protruded lips. This sound can be approximated by adopting the posture to whistle a very low note, or to blow out a candle. And is produced with the tongue as low and as far back in the mouth as possible.

The other vowels are 'auditorily equidistant' between these three 'corner vowels', at four degrees of aperture or 'height': close (high tongue position), close-mid, open-mid, and open (low tongue position).

These degrees of aperture plus the front-back distinction define 8 reference points on a mixture of articulatory and auditory criteria. These eight vowels are known as the eight 'primary cardinal vowels', and vowels like these are common in the world's languages.

The lip positions can be reversed with the lip position for the corresponding vowel on the opposite side of the front-back dimension, so that e.g. Cardinal 1 can be produced with rounding somewhat similar to that of Cardinal 8 (though normally compressed rather than protruded); these are known as 'secondary cardinal vowels'. Sounds such as these are claimed to be less common in the world's languages. Other vowel sounds are also recognised on the vowel chart of the International Phonetic Alphabet.

In the , the cardinal vowels have the same numbers used above, but added to 300.

The usual explanation of the cardinal vowel system implies that the competent user can reliably distinguish between sixteen Primary and Secondary vowels plus a small number of central vowels. The provision of diacritics by the International Phonetic Association further implies that intermediate values may also be reliably recognized, so that a phonetician might be able to produce and recognize not only a close-mid front unrounded vowel and an open-mid front unrounded vowel but also a mid front unrounded vowel , a centralized mid front unrounded vowel , and so on. This suggests a range of vowels nearer to forty or fifty than to twenty in number. Empirical evidence for this ability in trained phoneticians is hard to come by.

Ladefoged, in a series of pioneering experiments published in the 1950s and 60s, studied how trained phoneticians coped with the vowels of a dialect of Scottish Gaelic. He asked eighteen phoneticians to listen to a recording of ten words spoken by a native speaker of Gaelic and to place the vowels on a cardinal vowel quadrilateral. He then studied the degree of agreement or disagreement among the phoneticians. Ladefoged himself drew attention to the fact that the phoneticians who were trained in the British tradition established by Daniel Jones were closer to each other in their judgments than those who had not had this training. However, the most striking result is the great divergence of judgments among "all" the listeners regarding vowels that were distant from Cardinal values.





</doc>
<doc id="6719" url="https://en.wikipedia.org/wiki?curid=6719" title="Columbia, Missouri">
Columbia, Missouri

Columbia is a city in the U.S. state of Missouri. It is the county seat of Boone County and home to the University of Missouri. Founded in 1821, it is the principal city of the five-county Columbia metropolitan area. It is Missouri's fourth most-populous and fastest growing city, with an estimated 123,195 residents in 2019.

As a Midwestern college town, Columbia has a reputation for progressive politics, persuasive journalism, and public art. The tripartite establishment of Stephens College (1833), the University of Missouri (1839), and Columbia College (1851), which surround the city's Downtown to the east, south, and north, has made the city a center of learning. At its center is 8th Street, also known as the Avenue of the Columns, which connects Francis Quadrangle and Jesse Hall to the Boone County Courthouse and the City Hall. Originally an agricultural town, the cultivation of the mind is Columbia's chief economic concern today. The city also depends on healthcare, insurance, and technology businesses, but has never been a major center of manufacturing. Companies such as Shelter Insurance, Carfax, Veterans United Home Loans, and Slackers CDs and Games, were founded in the city. Cultural institutions include the State Historical Society of Missouri, the Museum of Art and Archaeology, and the annual True/False Film Festival and the Roots N Blues Festival. The Missouri Tigers, the state's only major college athletic program, play football at Faurot Field and basketball at Mizzou Arena as members of the rigorous Southeastern Conference.

The city rests upon the forested hills and rolling prairies of Mid-Missouri, near the Missouri River valley, where the Ozark Mountains begin to transform into plains and savanna. Limestone forms bluffs and glades while rain dissolves the bedrock, creating caves and springs which water the Hinkson, Roche Perche, and Bonne Femme creeks. Surrounding the city, Rock Bridge Memorial State Park, Mark Twain National Forest, and Big Muddy National Fish and Wildlife Refuge form a greenbelt preserving sensitive and rare environments. The Columbia Agriculture Park is home to the Columbia Farmers Market.

The first humans who entered the area at least 12,000 years ago were nomadic hunters. Later, woodland tribes lived in villages along waterways and built mounds in high places. The Osage and Missouria nations were expelled by the exploration of French traders and the rapid settlement of American pioneers. The latter arrived by the Boone's Lick Road and hailed from the culture of the Upland South, especially Virginia, Kentucky, and Tennessee. From 1812, the Boonslick area played a pivotal role in Missouri's early history and the nation's westward expansion. German, Irish, and other European immigrants soon joined. The modern populace is unusually diverse, over 8% foreign-born. White and black people are the largest ethnicities, and people of Asian descent are the third-largest group. The city has been called the "Athens of Missouri" for its classic beauty and educational emphasis, but is more commonly called "CoMo".

Columbia's origins begin with the settlement of American pioneers from Kentucky and Virginia in an early 1800s region known as the Boonslick. Before 1815 settlement in the region was confined to small log forts because of the threat of Native American attack during the War of 1812. When the war ended settlers came on foot, horseback, and wagon, often moving entire households along the Boone's Lick Road and sometimes bringing enslaved African Americans. By 1818 it was clear that the increased population would necessitate a new county be created from territorial Howard County. The Moniteau Creek on the west and Cedar Creek on the east were obvious natural boundaries.

Believing it was only a matter of time before a county seat was chosen, the Smithton Land Company was formed to purchase over to established the village of Smithton near the present-day intersection of Walnut and Garth. In 1819 Smithton was a small cluster of log cabins in an ancient forest of oak and hickory; chief among them was the cabin of Richard Gentry, a trustee of the Smithton Company who would become first mayor of Columbia. In 1820, Boone County was formed and named after the recently deceased explorer Daniel Boone. The Missouri Legislature appointed John Gray, Jefferson Fulcher, Absalom Hicks, Lawrence Bass, and David Jackson as commissioners to select and establish a permanent county seat. Smithton never had more than twenty people, and it was quickly realized that well digging was difficult because of the bedrock.

Springs were discovered across the Flat Branch Creek, so in the spring of 1821 Columbia was laid out, and the inhabitants of Smithton moved their cabins to the new town. The first house in Columbia was built by Thomas Duly in 1820 at what became Fifth and Broadway. Columbia's permanence was ensured when it was chosen as county seat in 1821 and the Boone's Lick Road was rerouted down Broadway.

The roots of Columbia's three economic foundationsâeducation, medicine, and insuranceâ can be traced to the city's incorporation in 1821. Original plans for the town set aside land for a state university. In 1833, Columbia Baptist Female College opened, which later became Stephens College. Columbia College, distinct from today's and later to become the University of Missouri, was founded in 1839. When the state legislature decided to establish a state university, Columbia raised three times as much money as any competing city, and James S. Rollins donated the land that is today the Francis Quadrangle. Soon other educational institutions were founded in Columbia, such as Christian Female College, the first college for women west of the Mississippi, which later became Columbia College.

The city benefited from being a stagecoach stop of the Santa Fe and Oregon trails, and later from the MissouriâKansasâTexas Railroad. In 1822, William Jewell set up the first hospital. In 1830, the first newspaper began; in 1832, the first theater in the state was opened; and in 1835, the state's first agricultural fair was held. By 1839, the population of 13,000 and wealth of Boone County was exceeded in Missouri only by that of St. Louis County, which, at that time, included the City of St. Louis.

Columbia's infrastructure was relatively untouched by the Civil War. As a slave state, Missouri had many residents with Southern sympathies, but it stayed in the Union. The majority of the city was pro-Union; however, the surrounding agricultural areas of Boone County and the rest of central Missouri were decidedly pro-Confederate. Because of this, the University of Missouri became a base from which Union troops operated. No battles were fought within the city because the presence of Union troops dissuaded Confederate guerrillas from attacking, though several major battles occurred at nearby Boonville and Centralia.

After Reconstruction, race relations in Columbia followed the Southern pattern of increasing violence of whites against blacks in efforts to suppress voting and free movement: George Burke, a black man who worked at the university, was lynched in 1889. In the spring of 1923, James T. Scott, an African-American janitor at the University of Missouri, was arrested on allegations of raping a university professor's daughter. He was taken from the county jail and lynched on April 29 before a white mob of several hundred, hanged from the Old Stewart Road Bridge.
In the 21st century, a number of efforts have been undertaken to recognize Scott's death. In 2010 his death certificate was changed to reflect that he was never tried or convicted of charges, and that he had been lynched. In 2011 a headstone was put at his grave at Columbia Cemetery; it includes his wife's and parents' names and dates, to provide a fuller account of his life. In 2016, a marker was erected at the lynching site to memorialize Scott.

In 1963, University of Missouri System and the Columbia College system established their headquarters in Columbia. The insurance industry also became important to the local economy as several companies established headquarters in Columbia, including Shelter Insurance, Missouri Employers Mutual, and Columbia Insurance Group. State Farm Insurance has a regional office in Columbia. In addition, the now-defunct Silvey Insurance was a large local employer.

Columbia became a transportation crossroads when U.S. Route 63 and U.S. Route 40 (which was improved as present-day Interstate 70) were routed through the city. Soon after, the city opened the Columbia Regional Airport. By 2000, the city's population was nearly 85,000.
In 2017, Columbia was in the path of totality for the Solar eclipse of August 21, 2017. The city was expecting upwards of 400,000 tourists coming to view the eclipse.

Columbia, in northern mid-Missouri, is away from both St. Louis and Kansas City, and north of the state capital of Jefferson City. The city is near the Missouri River, between the Ozark Plateau and the Northern Plains.

According to the United States Census Bureau, the city has a total area of of which is land and is water.

The city generally slopes from the highest point in the Northeast to the lowest point in the Southwest towards the Missouri River. Prominent tributaries of the river are Perche Creek, Hinkson Creek, and Flat Branch Creek. Along these and other creeks in the area can be found large valleys, cliffs, and cave systems such as that in Rock Bridge State Park just south of the city. These creeks are largely responsible for numerous stream valleys giving Columbia hilly terrain similar to the Ozarks while also having prairie flatland typical of northern Missouri. Columbia also operates several greenbelts with trails and parks throughout town.

Large mammal found in the city include urbanized coyotes, red foxes, and numerous whitetail deer. Eastern gray squirrel, and other rodents are abundant, as well as cottontail rabbits and the nocturnal opossum and raccoon. Large bird species are abundant in parks and include the Canada goose, mallard duck, as well as shorebirds, including the great egret and great blue heron. Turkeys are also common in wooded areas and can occasionally be seen on the MKT recreation trail. Populations of bald eagles are found by the Missouri River. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern U.S. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. Columbia has large areas of forested and open land and many of these areas are home to wildlife.

Columbia has a humid continental climate (KÃ¶ppen "Dfa)" marked by sharp seasonal contrasts in temperature, and is in USDA Plant Hardiness Zone 6a. The monthly daily average temperature ranges from in January to in July, while the high reaches or exceeds on an average of 32 days per year, on two days, while four nights of sub- lows can be expected. Precipitation tends to be greatest and most frequent in the latter half of spring, when severe weather is also most common. Snow averages per season, mostly from December to March, with occasional November accumulation and falls in April being rarer; historically seasonal snow accumulation has ranged from in 2005â06 to in 1977â78. Extreme temperatures have ranged from on February 12, 1899 to on July 12 and 14, 1954. Readings of or are uncommon, the last occurrences being January 7, 2014 and July 31, 2012.

Columbia's most significant and well-known architecture is found in buildings located in its downtown area and on the university campuses. The University of Missouri's Jesse Hall and the neo-gothic Memorial Union have become icons of the city. The David R. Francis Quadrangle is an example of Thomas Jefferson's academic village concept.

Four historic districts located within the city are listed on the National Register of Historic Places: Downtown Columbia, the East Campus Neighborhood, Francis Quadrangle, and the North Ninth Street Historic District. The downtown skyline is relatively low and is dominated by the 10-story Tiger Hotel and the 15-story Paquin Tower.

Downtown Columbia is an area of approximately one square mile surrounded by the University of Missouri on the south, Stephens College to the east, and Columbia College on the north. The area serves as Columbia's financial and business district.
Since the early-21st century, a large number of high-rise apartment complexes have been built in downtown Columbia. Many of these buildings also offer mixed-use business and retail space on the lower levels. These developments have not been without criticism, with some expressing concern the buildings hurt the historic feel of the area, or that the city does not yet have the infrastructure to support them.

The city's historic residential core lies in a ring around downtown, extending especially to the west along Broadway, and south into the East Campus Neighborhood. The city government recognizes 63 neighborhood associations. The city's most dense commercial areas are primarily along Interstate 70, U.S. Route 63, Stadium Boulevard, Grindstone Parkway, and Downtown.

As of the census of 2010, 108,500 people, 43,065 households, and 21,418 families resided in the city. The population density was . There were 46,758 housing units at an average density of . The racial makeup of the city was 79.0% White, 11.3% African American, 0.3% Native American, 5.2% Asian, 0.1% Pacific Islander, 1.1% from other races, and 3.1% from two or more races. Hispanic or Latino of any race were 3.4% of the population.

There were 43,065 households, of which 26.1% had children under the age of 18 living with them, 35.6% were married couples living together, 10.6% had a female householder with no husband present, 3.5% had a male householder with no wife present, and 50.3% were non-families. 32.0% of all households were made up of individuals, and 6.6% had someone living alone who was 65 years of age or older. The average household size was 2.32 and the average family size was 2.94.

In the city the population was spread out, with 18.8% of residents under the age of 18; 27.3% between the ages of 18 and 24; 26.7% from 25 to 44; 18.6% from 45 to 64; and 8.5% who were 65 years of age or older. The median age in the city was 26.8 years. The gender makeup of the city was 48.3% male and 51.7% female.

As of the census of 2000, there were 84,531 people, 33,689 households, and 17,282 families residing in the city. The population density was 1,592.8 people per square mile (615.0/km). There were 35,916 housing units at an average density of 676.8 per square mile (261.3/km). The racial makeup of the city was 81.54% White, 10.85% Black or African American, 0.39% Native American, 4.30% Asian, 0.04% Pacific Islander, 0.81% from other races, and 2.07% from two or more races. Hispanic or Latino of any race were 2.05% of the population.

There were 33,689 households, out of which 26.1% had children under the age of 18 living with them, 38.2% were married couples living together, 10.3% had a female householder with no husband present, and 48.7% were non-families. 33.1% of all households were made up of individuals, and 6.5% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 2.92.

In the city, the population was spread out, with 19.7% under the age of 18, 26.7% from 18 to 24, 28.7% from 25 to 44, 16.2% from 45 to 64, and 8.6% who were 65 years of age or older. The median age was 27 years. For every 100 females, there were 91.8 males. For every 100 females age 18 and over, there were 89.1 males.

The median income for a household in the city was $33,729, and the median income for a family was $52,288. Males had a median income of $34,710 versus $26,694 for females. The per capita income for the city was $19,507. About 9.4% of families and 19.2% of the population were below the poverty line, including 14.8% of those under age 18 and 5.2% of those age 65 or over. However, traditional statistics of income and poverty can be misleading when applied to cities with high student populations, such as Columbia.

Columbia's economy is historically dominated by education, healthcare, and insurance. Jobs in government are also common, either in Columbia or a half-hour south in Jefferson City. The Columbia Regional Airport and the Missouri River Port of Rocheport connect the region with trade and transportation.

With a Gross Metropolitan Product of $9.6 billion in 2018, Columbia's economy makes up 3% of the Gross State Product of Missouri. Columbia's metro area economy is slightly larger than the economy of Rwanda. Insurance corporations headquartered in Columbia include Shelter Insurance and the Columbia Insurance Group. Other organizations include Veterans United Home Loans, MFA Incorporated, the Missouri State High School Activities Association, and MFA Oil. Companies such as Socket, Datastorm Technologies, Inc. (no longer existent), Slackers CDs and Games, Carfax, and MBS Textbook Exchange were all founded in Columbia.

According to Columbia's 2018 Comprehensive Annual Financial Report, the top employers in the city are:

The Missouri Theatre Center for the Arts and Jesse Auditorium are Columbia's largest fine arts venues. Ragtag Cinema annually hosts the True/False Film Festival, which brings in over 30,000 each year for a weekend of art, film, and music.

In 2008, filmmaker Todd Sklar completed "Box Elder", which was filmed entirely in and around Columbia and the University of Missouri. The city was also the set for a well-received horror film which was released in 2011.

The North Village Arts District, located on the north side of downtown is home to galleries, restaurants, theaters, bars, music venues, and the Missouri Contemporary Ballet.

The University of Missouri's Museum of Art and Archaeology displays 14,000 works of art and archaeological objects in five galleries for no charge to the public. Libraries include the Columbia Public Library, the University of Missouri Libraries, with over three million volumes in Ellis Library, and the State Historical Society of Missouri.

The "We Always Swing" Jazz Series and the Roots N Blues Festival is held in Columbia. "9th Street Summerfest" (Now hosted in Rose Park at Rose Music Hall) closes part of that street several nights each summer to hold outdoor performances and has featured Willie Nelson (2009), Snoop Dogg (2010), The Flaming Lips (2010), Weird Al Yankovic (2013), and others. The "University Concert Series" regularly includes musicians and dancers from various genres, typically in Jesse Hall. Other musical venues in town include the Missouri Theatre, the University's multipurpose Hearnes Center, the University's Mizzou Arena, The Blue Note, and Rose Music Hall. Shelter Gardens, a park on the campus of Shelter Insurance headquarters, also hosts outdoor performances during the summer.
The University of Missouri School of Music attracts hundreds of musicians to Columbia, student performances are held in Whitmore Recital Hall. Among many non-profit organizations for classical music are included the "Odyssey Chamber Music Series", "Missouri Symphony Society" and "Columbia Civic Orchestra". Founded in 2006, the "Plowman Chamber Music Competition" is a biennial competition held in March/April of odd-numbered years, considered to be one of the finest, top five chamber music competitions in the nation.

Columbia has multiple opportunities to watch and perform in theatrical productions. The city is home to Stephens College, a private institution known for performing arts. Their season includes multiple plays and musicals. The University of Missouri and Columbia College also present multiple productions a year.

The cities three public high schools are also known for their productions. Rock Bridge High School performs a musical in November and 2 plays in the spring. Hickman High School also performs a similar season with 2 musical performances (one in the fall, and one in the spring) and 2 plays (one in the winter, and one at the end of their school year). They are a Columbia staple when it comes to High School Theatre with a wide variety of performances such as Hairspray, Guys and Dolls, The Miracle Worker, and Shakespeareâs A Midsummer Nights Dream. The newest high school, Battle High, opened six years ago and also is known for their outstanding productions such as Annie, Beauty and the Beast, 9 to 5, and Tarzan. Battle presents a musical in the fall and a play in the spring, along with improv nights and more productions throughout the year. Ticket's to the High school shows are typically five to eight dollars for students and eight to ten dollars for adults.

The city is also home to the indoor/outdoor theatre Maplewood Barn Theatre in Nifong Park and other community theatre programs such as Columbia Entertainment Company, Talking Horse Productions, Pace Youth Theatre and TRYPS.

The University of Missouri's sports teams, the Missouri Tigers, play a significant role in the city's sports culture. Faurot Field at Memorial Stadium, which has a capacity of 71,168, hosts home football games. The Hearnes Center and Mizzou Arena are two other large sport and event venues, the latter being the home arena for Mizzou's basketball team. Taylor Stadium is host to their baseball team and was the regional host for the 2007 NCAA Baseball Championship. Columbia College has several men and women collegiate sports teams as well. In 2007, Columbia hosted the National Association of Intercollegiate Athletics Volleyball National Championship, which the Lady Cougars participated in.

Columbia also hosts the Show-Me State Games, a non-profit program of the Missouri Governor's Council on Physical Fitness and Health. They are the largest state games in the United States.

Situated midway between St. Louis and Kansas City, Columbians will often have allegiances to the professional sports teams housed there, such as the St. Louis Cardinals, the Kansas City Royals, the Kansas City Chiefs, the St. Louis Blues, Sporting Kansas City, and St. Louis FC.

Throughout the city are many parks and trails for public usage. Among the more popularly frequented is the MKT which is a spur that connects to the Katy Trail, meeting up just south of Columbia proper. The MKT ranked second in the nation for "Best Urban Trail" in the 2015 "USA Today"s 10 Best Readers' Choice Awards. This 10-foot wide trail built on the old railbed of the MKT railroad begins in downtown Columbia in Flat Branch Park at 4th and Cherry Streets. The all-weather crushed limestone surface provides opportunities for walking, jogging, running, and bicycling. Stephens Lake Park is the highlight of Columbia's park system and is known for its 11-acre fishing/swimming lake, mature trees, and historical significance in the community. It serves as the center for outdoor winter sports, a variety of community festivals such as the Roots N Blues Festival, and outdoor concert series at the amphitheater. Stephens Lake has reservable shelters, playgrounds, swimming beach and spraygrounds, art sculptures, waterfalls, and walking trails. Rock Bridge State Park is open year round giving visitors the chance to scramble, hike, and bicycle through a scenic environment. Rock Bridge State Park contains some of the most popular hiking trails in the state, including the Gans Creek Wild Area.

The city has two daily newspapers: the "Columbia Missourian" and the "Columbia Daily Tribune", both morning deliveries. The "Missourian" is directed by professional editors and staffed by Missouri School of Journalism students who do reporting, design, copy editing, information graphics, photography, and multimedia. The "Missourian" publishes the weekly city magazine, "Vox". With a daily circulation of nearly 20,000, the "Daily Tribune" is the most widely read newspaper in central Missouri. The University of Missouri has the independent but official student newspaper called "The Maneater", which is printed bi-weekly. The now-defunct "Prysms Weekly" was also published in Columbia. In late 2009, KCOU News launched full operations out of KCOU 88.1 FM on the MU Campus. The entirely student-run news organization airs a daily newscast, "The Pulse", weekdays from 4:30 to 5:30Â p.m.

The city has 14 radio stations and 4 television channels. Columbia's Public Access channel, Columbia Access Television, is commonly referred to by its acronym, CAT, or CAT-TV. The Education Access channel in Columbia, CPSTV, is managed by Columbia Public Schools as a function of the Columbia Public Schools Community Relations Department. The city's Government Access channel broadcast City Council, Planning and Zoning Commission, and Board of Adjustment meetings

Columbia has many bars and restaurants that provide diverse styles of cuisine, due in part to having three colleges. One such establishment is the historic Booches bar, restaurant, and pool hall, which was established in 1884 and is frequented by college students. Shakespeare's Pizza is known across the nation for its college town pizza.

The City of Columbia's current government was established by a home rule charter adopted by voters on November 11, 1974, which established a council-manager government that invested power in the city council. The city council is made up of seven members â six elected by each of Columbia's six single-member districts or wards, plus an at-large council member, the mayor, who is elected by all city voters. The mayor receives a $9,000 annual stipend, and the six remaining council members receive a $6,000 annual stipend. They are elected to staggered three-year terms. As well as serving as a voting member of the city council, the mayor is recognized as the head of city government for ceremonial purposes. Chief executive authority is invested in a hired city manager, who oversees the day-to-day operations of government.

Columbia is the county seat of Boone County, and houses the county court and government center. The city lies within Missouri's 4th congressional district. The 19th Missouri State Senate district covers all of Boone County. There are five Missouri House of Representatives districts (9, 21, 23, 24, and 25) in the city. The Columbia Police Department provides law enforcement across the city, while the Columbia Fire Department provides fire protection. The University of Missouri Police Department patrols areas on and around the MU campus and has jurisdiction throughout the state. The Public Service Joint Communications Center coordinates efforts between the two organizations as well as the Boone County Fire Protection District, which operates Urban Search and Rescue Missouri Task Force 1.
The population generally supports progressive causes such as the extensive city recycling programs and the decriminalization of cannabis both for medical and recreational use at the municipal level, though the scope of the latter of the two cannabis ordinances has since been restricted. The city is one of only four in the state to offer medical benefits to same-sex partners of city employees. The new health plan extends health benefits to unmarried heterosexual domestic partners of city employees.

On October 10, 2006, the city council approved an ordinance to prohibit smoking in public places, including restaurants and bars. The ordinance was passed over protest, and several amendments to the ordinance reflect this. Over half of residents possess at least a bachelor's degree, while over a quarter hold a graduate degree. Columbia is the thirteenth most-highly educated municipality in the United States.

Columbia and much of the surrounding area lies within the Columbia Public School District. The district enrolled more than 18,000 students and had a budget of $281 million for the 2019-20 school year. It is above the state average in both attendance percentage and graduation rate. The city operates four public high schools which cover grades 9â12: David H. Hickman High School, Rock Bridge High School, Muriel Battle High School, and Frederick Douglass High School. Rock Bridge is one of two Missouri high schools to receive a silver medal by U.S. News & World Report, putting it in the Top 3% of all high schools in the nation. Hickman has been on Newsweek magazine's list of Top 1,300 schools in the country for the past three years and has more named presidential scholars than any other public high school in the US. There are also several private high schools located in the city, including Christian Fellowship School, Columbia Independent School, Heritage Academy, Christian Chapel Academy, and Tolton High School.

CPS also manages six middle schools: West, Jefferson, Lange, Oakland, Smithton, and Gentry. There is also a seventh school that will open in the 2019-20 school year.

The city has three institutions of higher education: the University of Missouri, Stephens College, and Columbia College, all of which surround Downtown Columbia. The city is the headquarters of the University of Missouri System, which operates campuses in St. Louis, Kansas City, and Rolla. Moberly Area Community College, Central Methodist University, and William Woods University as well as operates satellite campuses in Columbia.

The Columbia Transit provides public bus and para-transit service, and is owned and operated by the city. In 2008, 1,414,400 passengers boarded along the system's six fixed routes and nine University of Missouri shuttle routes, and 27,000 boarded the Para-transit service. The system is constantly experiencing growth in service and technology. A $3.5 million project to renovate and expand the Wabash Station, a rail depot built in 1910 and converted into the city's transit center in the mid-1980s, was completed in summer of 2007. In 2007, a Transit Master Plan was created to address the future transit needs of the city and county with a comprehensive plan to add infrastructure in three key phases. The five to 15-year plan intends to add service along the southwest, southeast and northeast sections of Columbia and develop alternative transportation models for Boone County.

Columbia is also known for its MKT Trail, a spur of the Katy Trail State Park, which allows foot and bike traffic across the city, and, conceivably, the state. It consists of a soft gravel surface for running and biking. Columbia also is preparing to embark on construction of several new bike paths and street bike lanes thanks to a $25 million grant from the federal government. The city is also served by American Airlines and United Airlines at the Columbia Regional Airport, the only commercial airport in mid-Missouri.

I-70 (concurrent with US 40) and US 63 are the two main freeways used for travel to and from Columbia. Within the city, there are also three state highways: Routes 763 (Rangeline Street & College Avenue), 163 (Providence Road), and 740 (Stadium Boulevard).

Rail service is provided by the city-owned Columbia Terminal Railroad (COLT), which runs from the north side of Columbia to Centralia and a connection to the Norfolk Southern Railway. Columbia would be at the center of the proposed Missouri Hyperloop, reducing travel times to Kansas City and St. Louis to around 15 minutes.

Health care is a big part of Columbia's economy, with nearly one in six people working in a health-care related profession and a physician density that is about three times the United States average. The cityâs hospitals and supporting facilities are a large referral center for the state, and medical related trips to the city are common. There are three hospital systems within the city and five hospitals with a total of 1,105 beds.

The University of Missouri Health Care operates three hospitals in Columbia: the University of Missouri Hospital, the University of Missouri Women's and Children's Hospital (formerly Columbia Regional Hospital), and the Ellis Fischel Cancer Center. Boone Hospital Center is administered by BJC Healthcare and operates several clinics as well as outpatient locations. The Harry S. Truman Memorial Veterans' Hospital, adjacent to University Hospital, is administered by the United States Department of Veterans Affairs.

There are a large number of medical-related industries in Columbia. The University of Missouri School of Medicine uses university-owned facilities as teaching hospitals. The University of Missouri Research Reactor Center is the largest research reactor in the United States and produces radioisotopes used in nuclear medicine. The center serves as the sole supplier of the active ingredients in two U.S. Food and Drug Administration-approved radiopharmaceuticals and produces Fluorine-18 used in PET imaging with its cyclotron.

In accordance with the Columbia Sister Cities Program, which operates in conjunction with Sister Cities International, Columbia has been paired with five international sister cities in an attempt to foster cross-cultural understanding:







</doc>
<doc id="6720" url="https://en.wikipedia.org/wiki?curid=6720" title="Charlton Athletic F.C.">
Charlton Athletic F.C.

Charlton Athletic Football Club is an English professional association football club based in Charlton, south-east London. They currently compete in League One, the third tier of English football, having been relegated from the Championship in the 2019â20 season. The club was founded on 9 June 1905 when a number of youth clubs in south-east London, including East Street Mission and Blundell Mission, combined to form Charlton Athletic. Their home ground is the Valley, where the club have played since 1919, apart from one year in Catford, during 1923â24, and seven years at Crystal Palace and West Ham United between 1985 and 1992, due to financial issues, and then safety concerns raised by the local council. The club's fans formed the Valley Party, nominating candidates to stand in local elections, in a bid to return the club to The Valley.

Charlton turned professional in 1920 and first entered the Football League in 1921. Since then the club has had four separate periods in the top flight of English football: 1936â1957, 1986â1990, 1998â1999, and 2000â2007. Historically, Charlton's most successful period was the 1930s, when the club's highest league finishes were recorded, including runners-up of the First Division in 1937. After World War II, Charlton reached two consecutive FA Cup finals, losing in 1946, and winning in 1947.

The club's traditional kit consists of red shirts, white shorts and red socks, and their most commonly used nickname is "The Addicks". Charlton share local rivalries with fellow Kent clubs Gillingham and Dartford.

Charlton Athletic F.C. were formed on 9 June 1905 by a group of 15- to 17-year-olds in East Street, Charlton, which is now known as Eastmoor Street and no longer residential. Charlton spent most of the years before the First World War playing in youth leagues. They became a senior side in 1913 the same year that nearby Woolwich Arsenal relocated to North London. After the war, they joined the Kent League for one season (1919â20) before becoming professional, appointing Walter Rayner as the first full-time manager. They were accepted by the Southern League and played just a single season (1920â21) before being voted into the Football League. Charlton's first Football League match was against Exeter City in August 1921, which they won 1â0. In 1923, Charlton became "giant killers" in the FA Cup beating top flight sides Manchester City, West Bromwich Albion, and Preston North End before losing to eventual winners Bolton Wanderers in the Quarter-Finals. Later that year, it was proposed that Charlton merge with Catford Southend to create a larger team with bigger support. In the 1923â24 season Charlton played in Catford at The Mount stadium and wore the colours of "The Enders", light and dark blue vertical stripes. However, the move fell through and the Addicks returned to the Charlton area in 1924, returning to the traditional red and white colours in the process.

Charlton finished second bottom in the Football League in 1926 and were forced to apply for re-election which was successful. Three years later the Addicks won the Division Three championship in 1929<ref name="England 1928/29"></ref> and they remained at the Division Two level for four years. After relegation into the Third Division south at the end of the 1932â33 season the club appointed Jimmy Seed as manager and he oversaw the most successful period in Charlton's history either side of the Second World War. Seed, an ex-miner who had made a career as a footballer despite suffering the effects of poison gas in the First World War, remains the most successful manager in Charlton's history. He is commemorated in the name of a stand at the Valley. Seed was an innovative thinker about the game at a time when tactical formations were still relatively unsophisticated. He later recalled "a simple scheme that enabled us to pull several matches out of the fire" during the 1934â35 season: when the team was in trouble "the centre-half was to forsake his defensive role and go up into the attack to add weight to the five forwards." The organisation Seed brought to the team proved effective and the Addicks gained successive promotions from the Third Division to the First Division between 1934 and 1936, becoming the first club to ever do so. Charlton finally secured promotion to the First Division by beating local rivals West Ham United at the Boleyn Ground, with their centre-half John Oakes playing on despite concussion and a broken nose.

In 1937, Charlton finished runners up in the First Division,<ref name="1936/1937 English Division 1 (old) Table"></ref> in 1938 finished fourth<ref name="1937/1938 English Division 1 (old) Table"></ref> and 1939 finished third.<ref name="1938/1939 English Division 1 (old) Table"></ref> They were the most consistent team in the top flight of English football over the three seasons immediately before the Second World War. This continued during the war years and they won the Football League War Cup and appeared in finals.

Charlton reached the 1946 FA Cup Final, but lost 4â1 to Derby County at Wembley. Charlton's Bert Turner scored an own goal in the eightieth minute before equalising for the Addicks a minute later to take them into extra time, but they conceded three further goals in the extra period. When the full league programme resumed in 1946â47 Charlton could finish only 19th in the First Division, just above the relegation spots, but they made amends with their performance in the FA Cup, reaching the 1947 FA Cup Final. This time they were successful, beating Burnley 1â0, with Chris Duffy scoring the only goal of the day. In this period of renewed football attendances, Charlton became one of only thirteen English football teams to average over 40,000 as their attendance during a full season. The Valley was the largest football ground in the League, drawing crowds in excess of 70,000. However, in the 1950s little investment was made either for players or to The Valley, hampering the club's growth. In 1956, the then board undermined Jimmy Seed and asked for his resignation; Charlton were relegated the following year.

From the late 1950s until the early 1970s, Charlton remained a mainstay of the Second Division before relegation to the Third Division in 1972<ref name="England 1971/72"></ref> caused the team's support to drop, and even a promotion in 1975 back to the second division<ref name="England 1974/75"></ref> did little to re-invigorate the team's support and finances. In 1979â80 Charlton were relegated again to the Third Division,<ref name="England 1979/80"></ref> but won immediate promotion back to the Second Division in 1980â81.<ref name="England 1980/81"></ref> Even though it did not feel like it, this was a turning point in the club's history leading to a period of turbulence and change including further promotion and exile. A change in management and shortly after a change in club ownership led to severe problems, such as the reckless signing of former European Footballer of the Year Allan Simonsen, and the club looked like it would go out of business.

In 1984 financial matters came to a head and the club went into administration, to be reformed as Charlton Athletic. (1984) Ltd. although the club's finances were still far from secure. They were forced to leave the Valley just after the start of the 1985â86 season, after its safety was criticised by Football League officials in the wake of the Bradford City stadium fire.
The club began to groundshare with Crystal Palace at Selhurst Park and this arrangement looked to be for the long-term, as Charlton did not have enough funds to revamp the Valley to meet safety requirements.
Despite the move away from the Valley, Charlton were promoted to the First Division as Second Division runners-up at the end of 1985â86,<ref name="England 1985/86"></ref> and remained at this level for four years (achieving a highest league finish of 14th) often with late escapes, most notably against Leeds in 1987, where the Addicks triumphed in extra-time of the play-off final replay to secure their top flight place. In 1987 Charlton also returned to Wembley for the first time since the 1947 FA Cup final for the Full Members Cup final against Blackburn.
Eventually, Charlton were relegated in 1990 along with Sheffield Wednesday and bottom club Millwall. Manager Lennie Lawrence remained in charge for one more season before he accepted an offer to take charge of Middlesbrough. He was replaced by joint player-managers Alan Curbishley and Steve Gritt. The pair had unexpected success in their first season finishing just outside the play-offs, and 1992â93 began promisingly and Charlton looked good bets for promotion in the new Division One (the new name of the old Second Division following the formation of the Premier League). However, the club was forced to sell players such as Rob Lee to help pay for a return to the Valley, while club fans formed the Valley Party, nominating candidates to stand in local elections in 1990, pressing the local council to enable the club's return to the Valley - finally achieved in December 1992.

In March 1993, defender Tommy Caton, who had been out of action due to injury since January 1991, announced his retirement from playing on medical advice. He died suddenly at the end of the following month at the age of 30.

In 1995, new chairman Richard Murray appointed Alan Curbishley as sole manager of Charlton. Under his sole leadership Charlton made an appearance in the play-off in 1996 but were eliminated by Crystal Palace in the semi-finals and the following season brought a disappointing 15th-place finish. 1997â98 was Charlton's best season for years. They reached the Division One play-off final and battled against Sunderland in a thrilling game which ended with a 4â4 draw after extra time. Charlton won 7â6 on penalties, with the match described as "arguably the most dramatic game of football in Wembley's history", and were promoted to the Premier League.

Charlton's first Premier League campaign began promisingly (they went top after two games) but they were unable to keep up their good form and were soon battling relegation. The battle was lost on the final day of the season but the club's board kept faith in Curbishley, confident that they could bounce back. Curbishley rewarded the chairman's loyalty with the Division One title in 2000 which signalled a return to the Premier League.

After the club's return, Curbishley proved an astute spender and by 2003 he had succeeded in establishing Charlton in the top flight. Charlton spent much of the 2003â04 Premier League season challenging for a Champions League place, but a late-season slump in form and the sale of star player Scott Parker to Chelsea, left Charlton in seventh place, which was still the club's highest finish since the 1950s. Charlton were unable to build on this level of achievement and Curbishley departed in 2006, with the club still established as a solid mid-table side.

In May 2006, Iain Dowie was named as Curbishley's successor, but was sacked after 12 league matches in November 2006, with only two wins. Les Reed replaced Dowie as manager, however he too failed to improve Charlton's position in the league table and on Christmas Eve 2006, Reed was replaced by former player Alan Pardew. Although results did improve, Pardew was unable to keep Charlton up and relegation was confirmed in the penultimate match of the season.

Charlton's return to the second tier of English football was a disappointment, with their promotion campaign tailing off to an 11th-place finish. Early in the following season the Addicks were linked with a foreign takeover, but this was swiftly denied by the club. On 10 October 2008, Charlton received an indicative offer for the club from a Dubai-based diversified investment company. However, the deal later fell through. The full significance of this soon became apparent as the club recorded net losses of over Â£13Â million for that financial year. Pardew left on 22 November after a 2â5 home loss to Sheffield United that saw the team fall into the relegation places. Matters did not improve under caretaker manager Phil Parkinson, and the team went a club record 18 games without a win, a new club record, before finally achieving a 1â0 away victory over Norwich City in an FA Cup Third Round replay; Parkinson was hired on a permanent basis. The team were relegated to League One after a 2â2 draw against Blackpool on 18 April 2009.

After spending almost the entire 2009â10 season in the top six of League One, Charlton were defeated in the Football League One play-offs semi-final second leg on penalties against Swindon Town.
After a change in ownership, Parkinson and Charlton legend Mark Kinsella left after a poor run of results. Another Charlton legend, Chris Powell, was appointed manager of the club in January 2011, winning his first game in charge 2â0 over Plymouth at the Valley. This was Charlton's first league win since November. Powell's bright start continued with a further three victories, before running into a downturn which saw the club go 11 games in succession without a win. Yet the fans' respect for Powell saw him come under remarkably little criticism. The club's fortunes picked up towards the end of the season, but leaving them far short of the play-offs. In a busy summer, Powell brought in 19 new players and after a successful season, on 14 April 2012, Charlton Athletic won promotion back to the Championship with a 1â0 away win at Carlisle United. A week later, on 21 April 2012, they were confirmed as champions after a 2â1 home win over Wycombe Wanderers. Charlton then lifted the League One trophy on 5 May 2012, having been in the top position since 15 September 2011, and after recording a 3â2 victory over Hartlepool United, recorded their highest ever league points score of 101, the highest in any professional European league that year.

In the first season back in the Championship, the 2012â13 season saw Charlton finish ninth place with 65 points, just three points short of the play-off places to the Premier League.

In early January 2014 during the 2013â14 season, Belgian businessman Roland DuchÃ¢telet took over Charlton as owner in a deal worth Â£14million. This made Charlton a part of a network of football clubs owned by DuchÃ¢telet. On 11 March 2014, two days after an FA Cup quarter-final loss to Sheffield United, and with Charlton sitting bottom of the table, Powell was sacked and leaked private emails suggested that this was due to a rift with the owner.

New manager Jose Riga, despite having to join Charlton long after the transfer window had closed, was able to improve Charlton's form and eventually guide them to 18th place, successfully avoiding relegation. After Riga's departure to manage Blackpool, former Millwall player Bob Peeters was appointed as manager in May 2014 on a 12-month contract. Charlton started strong, but a long run of draws meant that after only 25 games in charge Peeters was dismissed with the team in 14th place. His replacement, Guy Luzon, ensured there was no relegation battle by winning most of the remaining matches, resulting in a 12th-place finish.

The 2015â16 season began promisingly but results under Luzon deteriorated and on 24 October 2015 after a 3â0 defeat at home to Brentford he was sacked. Luzon said in a "News Shopper" interview that he "was not the one who chose how to do the recruitment" as the reason why he failed as manager. Karel Fraeye was appointed "interim head coach", but was sacked after 14 games and just two wins, with the club then second from bottom in the Championship. On 14 January 2016, Jose Riga was appointed head coach for a second spell, but could not prevent Charlton from being relegated to League One for the 2016â17 season. Riga resigned at the end of the season. To many fans, the managerial changes and subsequent relegation to League One were symptomatic of the mismanagement of the club under DuchÃ¢telet's ownership and several protests began.

After a slow start to the new season, with the club in 15th place of League One, the club announced that it had "parted company" with Russell Slade in November 2016. Karl Robinson was appointed on a permanent basis soon after. He led the Addicks to an uneventful 13th-place finish. The following season Robinson had the team challenging for the play-offs, but a drop in form in March led him to resign by mutual consent. He was replaced by former player Lee Bowyer as caretaker manager who guided them to a 6th-place finish, but lost in the play-off semi-final.

Bowyer was appointed permanently in September on a one-year contract and after finishing third in the regular 2018-19 EFL League One season, Charlton beat Sunderland 2â1 in the League One play-off final to earn promotion back to the EFL Championship after a three-season absence. Bowyer later signed a new one-year contract following promotion, which was later extended to three years in January 2020.

On 29 November 2019, Charlton Athletic were acquired by East Street Investments (ESI) from Abu Dhabi, subject to approval from the English Football League (EFL). Approval was reportedly granted on 2 January 2020. However, on 10 March 2020, a public disagreement between the new owners erupted along with reports that the main investor was pulling out, and the EFL said the takeover had not been approved. The Valley and Charlton's training ground were still owned by DuchÃ¢telet, and a transfer embargo was in place as the new owners had not provided evidence of funding through to June 2021. On 20 April 2020, the EFL announced that the club had been placed under investigation for misconduct regarding the takeover. In June 2020, Charlton confirmed that ESI had been taken over by a consortium led by businessman Paul Elliott, and said it had contacted the EFL to finalise the ownership change. However, a legal dispute involving former ESI director Matt Southall continued. He attempted to regain control of the club to prevent Elliot's takeover from going ahead, but failed and was subsequently fined and dismissed for challenging the club's directors. On 7 August 2020 the EFL said three individuals including ESI owner Elliot and lawyer Chris Farnell had failed its Owners' and Directors' Test, leaving the club's ownership unclear; Charlton appealed against the decision. Meanwhile, Thomas Sandgaard, a Danish businessman based on Colorado, was reported to be negotiating to buy the club.

Charlton were relegated back to League One at the end of the 2019â20 season after finishing 22nd. Due to COVID-19 pandemic restrictions, the final games of the season were played behind closed doors, and the early games of the 2020-21 season were set to do the same. However, Charlton's first home game, against Doncaster Rovers on 19 September 2020, was proposed as a crowd pilot test, subject to government approval.

The club's first ground was Siemens Meadow (1905â1907), a patch of rough ground by the River Thames. This was over-shadowed by the Siemens Brothers Telegraph Works. Then followed Woolwich Common (1907â1908), Pound Park (1908â1913), and Angerstein Lane (1913â1915). After the end of the First World War, a chalk quarry known as the Swamps was identified as Charlton's new ground, and in the summer of 1919 work began to create the level playing area and remove debris from the site. The first match at this site, now known as the club's current ground The Valley, was in September 1919. Charlton stayed at The Valley until 1923, when the club moved to The Mount stadium in Catford as part of a proposed merger with Catford Southend Football Club. However, after this move collapsed in 1924 Charlton returned to The Valley.

During the 1930s and 1940s, significant improvements were made to the ground, making it one of the largest in the country at that time. In 1938 the highest attendance to date at the ground was recorded at over 75,000 for a FA Cup match against Aston Villa. During the 1940s and 1950s the attendance was often above 40,000, and Charlton had one of the largest support bases in the country. However, after the club's relegation little investment was made in The Valley as it fell into decline.

In the 1980s matters came to a head as the ownership of the club and The Valley was divided. The large East Terrace had been closed down by the authorities after the Bradford City stadium fire and the ground's owner wanted to use part of the site for housing. In September 1985, Charlton made the controversial move to ground-share with South London neighbours Crystal Palace at Selhurst Park. This move was unpopular with supporters and in the late 1980s significant steps were taken to bring about the club's return to The Valley.

A single issue political party, the Valley Party, contested the 1990 local Greenwich Borough Council elections on a ticket of reopening the stadium, capturing 11% of the vote, aiding the club's return. The Valley Gold investment scheme was created to help supporters fund the return to The Valley, and several players were also sold to raise funds. For the 1991â92 season and part of the 1992â93 season, the Addicks played at West Ham's Upton Park as Wimbledon had moved into Selhurst Park alongside Crystal Palace. Charlton finally returned to The Valley in December 1992, celebrating with a 1â0 victory against Portsmouth.

Since the return to The Valley, three sides of the ground have been completely redeveloped turning The Valley into a modern, all-seater stadium with a 27,111 capacity. There are plans in place to increase the ground's capacity to approximately 31,000 and even around 40,000 in the future.

The bulk of the club's support base comes from South East London and Kent, particularly the London boroughs of Greenwich, Bexley and Bromley. Supporters played a key role in the return of the club to The Valley in 1992 and were rewarded by being granted a voice on the board in the form of an elected supporter director. Any season ticket holder could put themselves forward for election, with a certain number of nominations, and votes were cast by all season ticket holders over the age of 18. The last such director, Ben Hayes, was elected in 2006 to serve until 2008, when the role was discontinued as a result of legal issues. Its functions were replaced by a fans forum, which met for the first time in December 2008 and is still active to this day.

Charlton's most common nickname is The Addicks. The most likely origin of this name is from a local fishmonger, Arthur "Ikey" Bryan, who rewarded the team with meals of haddock and chips.

The progression of the nickname can be seen in the book "The Addicks Cartoons: An Affectionate Look into the Early History of Charlton Athletic", which covers the pre-First World War history of Charlton through a narrative based on 56 cartoons which appeared in the now defunct Kentish Independent. The very first cartoon, from 31 October 1908, calls the team the Haddocks. By 1910, the name had changed to Addicks although it also appeared as Haddick. The club also have two other nicknames, The Red Robins, adopted in 1931, and The Valiants, chosen in a fan competition in the 1960s which also led to the adoption of the sword badge which is still in use. The Addicks nickname never went away and was revived by fans after the club lost its Valley home in 1985 and went into exile at Crystal Palace. It is now once again the official nickname of the club.

Charlton fans' chants have included "Valley, Floyd Road", a song noting the stadium's address to the tune of "Mull of Kintyre", and "The Red, Red Robin"

Charlton Athletic featured in the ITV one-off drama "Albert's Memorial", shown on 12 September 2010 and starring David Jason and David Warner.

In the long-running BBC sitcom "Only Fools and Horses", Rodney Charlton Trotter is named after the club.

Charlton's ground and the then manager, Alan Curbishley, made appearances in the Sky One TV series, "Dream Team".

Charlton Athletic has also featured in a number of book publications, in both the realm of fiction and factual/sports writing. These include works by Charlie Connelly and Paul Breen's work of popular fiction which is entitled "The Charlton Men". The book is set against Charlton's successful 2011â12 season when they won the League One title and promotion back to the Championship in concurrence with the 2011 London riots.

Timothy Young, the protagonist in "Out of the Shelter", a novel by David Lodge, supports Charlton Athletic. The book describes Timothy listening to Charlton's victory in the 1947 FA Cup Final on the radio.

Charlton have used a number of crests and badges during their history, although the current design has not been changed since 1968. The first known badge, from the 1930s, consisted of the letters CAF in the shape of a club from a pack of cards. In the 1940s, Charlton used a design featuring a robin sitting in a football within a shield, sometimes with the letters CAFC in the four-quarters of the shield, which was worn for the 1946 FA Cup Final. In the late 1940s and early 1950s, the crest of the former metropolitan borough of Greenwich was used as a symbol for the club but this was not used on the team's shirts.

In 1963, a competition was held to find a new badge for the club, and the winning entry was a hand holding a sword, which complied with Charlton's nickname of the time, the Valiants. Over the next five years modifications were made to this design, such as the addition of a circle surrounding the hand and sword and including the club's name in the badge. By 1968, the design had reached the one known today, and has been used continuously from this year, apart from a period in the 1970s when just the letters CAFC appeared on the team's shirts.

With the exception of one season, Charlton have always played in red and white. The colours had been chosen by the group of boys who had founded Charlton Athletic in 1905 after having to play their first matches in the borrowed kits of their local rivals Woolwich Arsenal, who also played in red and white. The exception came during the 1923â24 season when Charlton wore the colours of Catford Southend as part of the proposed move to Catford, which were light and dark blue stripes. However, after the move fell through, Charlton returned to wearing red and white as their home colours.

The sponsors were as follows:

Charlton's main rivals are their South London neighbours, Crystal Palace and Millwall. 

In 1985, Charlton was forced to ground-share with Crystal Palace after safety concerns at The Valley. They played their home fixtures at the Eagles' Selhurst Park stadium until 1991. The arrangement was seen by Crystal Palace chairman Ron Noades as essential for the future of football, but it was unpopular with both sets of fans. Charlton fans campaigned for a return to The Valley throughout their time at Selhurst Park. In 2005, Palace were relegated by Charlton at the Valley after a 2â2 draw. Palace needed a win to survive but with seven minutes left Charlton equalised, relegating their rivals. Post-match, there was a well-publicised altercation between the two chairmen of the respective clubs, Richard Murray and Simon Jordan. Since their first meeting in the Football League in 1925, Charlton have won 17, drawn 13 and lost 26 games against Palace. The teams last met in 2015, a 4â1 win for Palace in the League Cup.

Charlton are closest in proximity to Millwall than any other club, with The Valley and The Den being less than four miles () apart. They last met in July 2020, a 1â0 win for Millwall at the Valley. Since their first Football League game in 1921, Charlton have won 12, drawn 26 and lost 37. The Addicks have not beaten Millwall in the last twelve fixtures between the sides and their last win came in March 1996 at The Valley.

<ref name="2019/20 squad numbers revealed"></ref>



</doc>
