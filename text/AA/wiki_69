<doc id="8651" url="https://en.wikipedia.org/wiki?curid=8651" title="Dark matter">
Dark matter

Dark matter is a form of matter thought to account for approximately 85% of the matter in the universe and about a quarter of its total mass–energy density or about . Its presence is implied in a variety of astrophysical observations, including gravitational effects that cannot be explained by accepted theories of gravity unless more matter is present than can be seen. For this reason, most experts think that dark matter is abundant in the universe and that it has had a strong influence on its structure and evolution. Dark matter is called dark because it does not appear to interact with the electromagnetic field, which means it doesn't absorb, reflect or emit electromagnetic radiation, and is therefore difficult to detect.

Primary evidence for dark matter comes from calculations showing that many galaxies would fly apart, or that they would not have formed or would not move as they do, if they did not contain a large amount of unseen matter. Other lines of evidence include observations in gravitational lensing and in the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions, and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy of the universe contains 5% ordinary matter and energy, 27% dark matter and 68% of a form of energy known as dark energy. Thus, dark matter constitutes 85% of total mass, while dark energy plus dark matter constitute 95% of total mass–energy content.

Because dark matter has not yet been observed directly, if it exists, it must barely interact with ordinary baryonic matter and radiation, except through gravity. Most dark matter is thought to be non-baryonic in nature; it may be composed of some as-yet undiscovered subatomic particles. The primary candidate for dark matter is some new kind of elementary particle that has not yet been discovered, in particular, weakly interacting massive particles (WIMPs). Many experiments to directly detect and study dark matter particles are being actively undertaken, but none have yet succeeded. Dark matter is classified as "cold", "warm", or "hot" according to its velocity (more precisely, its free streaming length). Current models favor a cold dark matter scenario, in which structures emerge by gradual accumulation of particles.

Although the existence of dark matter is generally accepted by the scientific community, some astrophysicists, intrigued by certain observations which do not fit some dark matter theories, argue for various modifications of the standard laws of general relativity, such as modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity. These models attempt to account for all observations without invoking supplemental non-baryonic matter.

The hypothesis of dark matter has an elaborate history. In a talk given in 1884, Lord Kelvin estimated the number of dark bodies in the Milky Way from the observed velocity dispersion of the stars orbiting around the center of the galaxy. By using these measurements, he estimated the mass of the galaxy, which he determined is different from the mass of visible stars. Lord Kelvin thus concluded "many of our stars, perhaps a great majority of them, may be dark bodies". In 1906 Henri Poincaré in "The Milky Way and Theory of Gases" used "dark matter", or "matière obscure" in French, in discussing Kelvin's work.

The first to suggest the existence of dark matter using stellar velocities was Dutch astronomer Jacobus Kapteyn in 1922. Fellow Dutchman and radio astronomy pioneer Jan Oort also hypothesized the existence of dark matter in 1932. Oort was studying stellar motions in the local galactic neighborhood and found the mass in the galactic plane must be greater than what was observed, but this measurement was later determined to be erroneous.

In 1933, Swiss astrophysicist Fritz Zwicky, who studied galaxy clusters while working at the California Institute of Technology, made a similar inference. Zwicky applied the virial theorem to the Coma Cluster and obtained evidence of unseen mass he called "dunkle Materie" ('dark matter'). Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated the cluster had about 400 times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred some unseen matter provided the mass and associated gravitation attraction to hold the cluster together. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value of the Hubble constant; the same calculation today shows a smaller fraction, using greater values for luminous mass. Nonetheless, Zwicky did correctly conclude from his calculation that the bulk of the matter was dark.

Further indications the mass-to-light ratio was not unity came from measurements of galaxy rotation curves. In 1939, Horace W. Babcock reported the rotation curve for the Andromeda nebula (known now as the Andromeda Galaxy), which suggested the mass-to-luminosity ratio increases radially. He attributed it to either light absorption within the galaxy or modified dynamics in the outer portions of the spiral and not to the missing matter he had uncovered. Following Babcock's 1939 report of unexpectedly rapid rotation in the outskirts of the Andromeda galaxy and a mass-to-light ratio of 50; in 1940 Jan Oort discovered and wrote about the large non-visible halo of NGC 3115.

Vera Rubin, Kent Ford, and Ken Freeman's work in the 1960s and 1970s provided further strong evidence, also using galaxy rotation curves. Rubin and Ford worked with a new spectrograph to measure the velocity curve of edge-on spiral galaxies with greater accuracy. This result was confirmed in 1978. An influential paper presented Rubin and Ford's results in 1980. They showed most galaxies must contain about six times as much dark as visible mass; thus, by around 1980 the apparent need for dark matter was widely recognized as a major unsolved problem in astronomy.

At the same time Rubin and Ford were exploring optical rotation curves, radio astronomers were making use of new radio telescopes to map the 21 cm line of atomic hydrogen in nearby galaxies. The radial distribution of interstellar atomic hydrogen (H-I) often extends to much larger galactic radii than those accessible by optical studies, extending the sampling of rotation curves – and thus of the total mass distribution – to a new dynamical regime. Early mapping of Andromeda with the 300 foot telescope at Green Bank and the 250 foot dish at Jodrell Bank already showed the H-I rotation curve did not trace the expected Keplerian decline. As more sensitive receivers became available, Morton Roberts and Robert Whitehurst were able to trace the rotational velocity of Andromeda to 30 kpc, much beyond the optical measurements. Illustrating the advantage of tracing the gas disk at large radii, Figure 16 of that paper combines the optical data (the cluster of points at radii of less than 15 kpc with a single point further out) with the H-I data between 20–30 kpc, exhibiting the flatness of the outer galaxy rotation curve; the solid curve peaking at the center is the optical surface density, while the other curve shows the cumulative mass, still rising linearly at the outermost measurement. In parallel, the use of interferometric arrays for extragalactic H-I spectroscopy was being developed. In 1972, David Rogstad and Seth Shostak published H-I rotation curves of five spirals mapped with the Owens Valley interferometer; the rotation curves of all five were very flat, suggesting very large values of mass-to-light ratio in the outer parts of their extended H-I disks.

A stream of observations in the 1980s supported the presence of dark matter, including gravitational lensing of background objects by galaxy clusters, the temperature distribution of hot gas in galaxies and clusters, and the pattern of anisotropies in the cosmic microwave background. According to consensus among cosmologists, dark matter is composed primarily of a not yet characterized type of subatomic particle. The search for this particle, by a variety of means, is one of the major efforts in particle physics.

In standard cosmology, matter is anything whose energy density scales with the inverse cube of the scale factor, i.e., This is in contrast to radiation, which scales as the inverse fourth power of the scale factor and a cosmological constant, which is independent of "a". These scalings can be understood intuitively: For an ordinary particle in a cubical box, doubling the length of the sides of the box decreases the density (and hence energy density) by a factor of 8 (= 2). For radiation, the energy density decreases by a factor of 16 (= 2), because any act whose effect increases the scale factor must also cause a proportional redshift. A cosmological constant, as an intrinsic property of space, has a constant energy density regardless of the volume under consideration.

In principle, "dark matter" means all components of the universe which are not visible but still obey In practice, the term "dark matter" is often used to mean only the non-baryonic component of dark matter, i.e., excluding "missing baryons." Context will usually indicate which meaning is intended.

The arms of spiral galaxies rotate around the galactic center. The luminous mass density of a spiral galaxy decreases as one goes from the center to the outskirts. If luminous mass were all the matter, then we can model the galaxy as a point mass in the centre and test masses orbiting around it, similar to the Solar System. From Kepler's Second Law, it is expected that the rotation velocities will decrease with distance from the center, similar to the Solar System. This is not observed. Instead, the galaxy rotation curve remains flat as distance from the center increases.

If Kepler's laws are correct, then the obvious way to resolve this discrepancy is to conclude the mass distribution in spiral galaxies is not similar to that of the Solar System. In particular, there is a lot of non-luminous matter (dark matter) in the outskirts of the galaxy.

Stars in bound systems must obey the virial theorem. The theorem, together with the measured velocity distribution, can be used to measure the mass distribution in a bound system, such as elliptical galaxies or globular clusters. With some exceptions, velocity dispersion estimates of elliptical galaxies do not match the predicted velocity dispersion from the observed mass distribution, even assuming complicated distributions of stellar orbits.

As with galaxy rotation curves, the obvious way to resolve the discrepancy is to postulate the existence of non-luminous matter.

Galaxy clusters are particularly important for dark matter studies since their masses can be estimated in three independent ways:

Generally, these three methods are in reasonable agreement that dark matter outweighs visible matter by approximately 5 to 1.

One of the consequences of general relativity is massive objects (such as a cluster of galaxies) lying between a more distant source (such as a quasar) and an observer should act as a lens to bend the light from this source. The more massive an object, the more lensing is observed.

Strong lensing is the observed distortion of background galaxies into arcs when their light passes through such a gravitational lens. It has been observed around many distant clusters including Abell 1689. By measuring the distortion geometry, the mass of the intervening cluster can be obtained. In the dozens of cases where this has been done, the mass-to-light ratios obtained correspond to the dynamical dark matter measurements of clusters. Lensing can lead to multiple copies of an image. By analyzing the distribution of multiple image copies, scientists have been able to deduce and map the distribution of dark matter around the MACS J0416.1-2403 galaxy cluster.

Weak gravitational lensing investigates minute distortions of galaxies, using statistical analyses from vast galaxy surveys. By examining the apparent shear deformation of the adjacent background galaxies, the mean distribution of dark matter can be characterized. The mass-to-light ratios correspond to dark matter densities predicted by other large-scale structure measurements. Dark matter does not bend light itself; mass (in this case the mass of the dark matter) bends spacetime. Light follows the curvature of spacetime, resulting in the lensing effect.

Although both dark matter and ordinary matter are matter, they do not behave in the same way. In particular, in the early universe, ordinary matter was ionized and interacted strongly with radiation via Thomson scattering. Dark matter does not interact directly with radiation, but it does affect the CMB by its gravitational potential (mainly on large scales), and by its effects on the density and velocity of ordinary matter. Ordinary and dark matter perturbations, therefore, evolve differently with time and leave different imprints on the cosmic microwave background (CMB).

The cosmic microwave background is very close to a perfect blackbody but contains very small temperature anisotropies of a few parts in 100,000. A sky map of anisotropies can be decomposed into an angular power spectrum, which is observed to contain a series of acoustic peaks at near-equal spacing but different heights.
The series of peaks can be predicted for any assumed set of cosmological parameters by modern computer codes such as CMBFAST and CAMB, and matching theory to data, therefore, constrains cosmological parameters. The first peak mostly shows the density of baryonic matter, while the third peak relates mostly to the density of dark matter, measuring the density of matter and the density of atoms.

The CMB anisotropy was first discovered by COBE in 1992, though this had too coarse resolution to detect the acoustic peaks.
After the discovery of the first acoustic peak by the balloon-borne BOOMERanG experiment in 2000, the power spectrum was precisely observed by WMAP in 2003–2012, and even more precisely by the Planck spacecraft in 2013–2015. The results support the Lambda-CDM model.

The observed CMB angular power spectrum provides powerful evidence in support of dark matter, as its precise structure is well fitted by the Lambda-CDM model, but difficult to reproduce with any competing model such as modified Newtonian dynamics (MOND).

Structure formation refers to the period after the Big Bang when density perturbations collapsed to form stars, galaxies, and clusters. Prior to structure formation, the Friedmann solutions to general relativity describe a homogeneous universe. Later, small anisotropies gradually grew and condensed the homogeneous universe into stars, galaxies and larger structures. Ordinary matter is affected by radiation, which is the dominant element of the universe at very early times. As a result, its density perturbations are washed out and unable to condense into structure. If there were only ordinary matter in the universe, there would not have been enough time for density perturbations to grow into the galaxies and clusters currently seen.

Dark matter provides a solution to this problem because it is unaffected by radiation. Therefore, its density perturbations can grow first. The resulting gravitational potential acts as an attractive potential well for ordinary matter collapsing later, speeding up the structure formation process.

If dark matter does not exist, then the next most likely explanation must be general relativity – the prevailing theory of gravity – is incorrect and should be modified. The Bullet Cluster, the result of a recent collision of two galaxy clusters, provides a challenge for modified gravity theories because its apparent center of mass is far displaced from the baryonic center of mass. Standard dark matter models can easily explain this observation, but modified gravity has a much harder time, especially since the observational evidence is model-independent.

Type Ia supernovae can be used as standard candles to measure extragalactic distances, which can in turn be used to measure how fast the universe has expanded in the past. Data indicates the universe is expanding at an accelerating rate, the cause of which is usually ascribed to dark energy. Since observations indicate the universe is almost flat, it is expected the total energy density of everything in the universe should sum to 1 (). The measured dark energy density is ; the observed ordinary (baryonic) matter energy density is and the energy density of radiation is negligible. This leaves a missing which nonetheless behaves like matter (see technical definition section above) dark matter.

Baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe on large scales. These are predicted to arise in the Lambda-CDM model due to acoustic oscillations in the photon–baryon fluid of the early universe, and can be observed in the cosmic microwave background angular power spectrum. BAOs set up a preferred length scale for baryons. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (≈1 percent) preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130–160 Mpc. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe. The results support the Lambda-CDM model.

Large galaxy redshift surveys may be used to make a three-dimensional map of the galaxy distribution. These maps are slightly distorted because distances are estimated from observed redshifts; the redshift contains a contribution from the galaxy's so-called peculiar velocity in addition to the dominant Hubble expansion term. On average, superclusters are expanding more slowly than the cosmic mean due to their gravity, while voids are expanding faster than average. In a redshift map, galaxies in front of a supercluster have excess radial velocities towards it and have redshifts slightly higher than their distance would imply, while galaxies behind the supercluster have redshifts slightly low for their distance. This effect causes superclusters to appear squashed in the radial direction, and likewise voids are stretched. Their angular positions are unaffected. This effect is not detectable for any one structure since the true shape is not known, but can be measured by averaging over many structures. It was predicted quantitatively by Nick Kaiser in 1987, and first decisively measured in 2001 by the 2dF Galaxy Redshift Survey. Results are in agreement with the Lambda-CDM model.

In astronomical spectroscopy, the Lyman-alpha forest is the sum of the absorption lines arising from the Lyman-alpha transition of neutral hydrogen in the spectra of distant galaxies and quasars. Lyman-alpha forest observations can also constrain cosmological models. These constraints agree with those obtained from WMAP data.

There are various hypotheses about what dark matter could consist of, as set out in the table below.

Dark matter can refer to any substance which interacts predominantly via gravity with visible matter (e.g., stars and planets). Hence in principle it need not be composed of a new type of fundamental particle but could, at least in part, be made up of standard baryonic matter, such as protons or neutrons. However, for the reasons outlined below, most scientists think the dark matter is dominated by a non-baryonic component, which is likely composed of a currently unknown fundamental particle (or similar exotic state).

Baryons (protons and neutrons) make up ordinary stars and planets. However, baryonic matter also encompasses less common non-primordial black holes, neutron stars, faint old white dwarfs and brown dwarfs, collectively known as massive compact halo objects (MACHOs), which can be hard to detect.

However, multiple lines of evidence suggest the majority of dark matter is not made of baryons:

Candidates for non-baryonic dark matter are hypothetical particles such as axions, sterile neutrinos, weakly interacting massive particles (WIMPs), gravitationally-interacting massive particles (GIMPs), supersymmetric particles, or primordial black holes. The three neutrino types already observed are indeed abundant, and dark, and matter, but because their individual masses – however uncertain they may be – are almost certainly too tiny, they can only supply a small fraction of dark matter, due to limits derived from large-scale structure and high-redshift galaxies.

Unlike baryonic matter, nonbaryonic matter did not contribute to the formation of the elements in the early universe (Big Bang nucleosynthesis) and so its presence is revealed only via its gravitational effects, or weak lensing. In addition, if the particles of which it is composed are supersymmetric, they can undergo annihilation interactions with themselves, possibly resulting in observable by-products such as gamma rays and neutrinos (indirect detection).

If dark matter is composed of weakly-interacting particles, an obvious question is whether it can form objects equivalent to planets, stars, or black holes. Historically, the answer has been it cannot, because of two factors:

In 2015–2017 the idea dense dark matter was composed of primordial black holes, made a comeback following results of gravitational wave measurements which detected the merger of intermediate mass black holes. Black holes with about 30 solar masses are not predicted to form by either stellar collapse (typically less than 15 solar masses) or by the merger of black holes in galactic centers (millions or billions of solar masses). It was proposed the intermediate mass black holes causing the detected merger formed in the hot dense early phase of the universe due to denser regions collapsing. A later survey of about a thousand supernova detected no gravitational lensing events, when about eight would be expected if intermediate mass primordial black holes above a certain mass range accounted for the majority of dark matter.

The possibility atom-sized primordial black holes account for a significant fraction of dark matter was ruled out by measurements of positron and electron fluxes outside the Sun's heliosphere by the Voyager 1 spacecraft. Tiny black holes are theorized to emit Hawking radiation. However the detected fluxes were too low and did not have the expected energy spectrum suggesting tiny primordial black holes are not widespread enough to account for dark matter. Nonetheless, research and theories proposing dense dark matter accounts for dark matter continue as of 2018, including approaches to dark matter cooling, and the question remains unsettled. In 2019, the lack of microlensing effects in the observation of Andromeda suggests tiny black holes do not exist.

However, there still exists a largely unconstrained mass range smaller than that can be limited by optical microlensing observations, where primordial black holes may account for all dark matter.

Dark matter can be divided into "cold", "warm", and "hot" categories. These categories refer to velocity rather than an actual temperature, indicating how far corresponding objects moved due to random motions in the early universe, before they slowed due to cosmic expansion – this is an important distance called the "free streaming length" (FSL). Primordial density fluctuations smaller than this length get washed out as particles spread from overdense to underdense regions, while larger fluctuations are unaffected; therefore this length sets a minimum scale for later structure formation.

The categories are set with respect to the size of a protogalaxy (an object that later evolves into a dwarf galaxy): Dark matter particles are classified as cold, warm, or hot according to their FSL; much smaller (cold), similar to (warm), or much larger (hot) than a protogalaxy. Mixtures of the above are also possible: a theory of mixed dark matter was popular in the mid-1990s, but was rejected following the discovery of dark energy.

Cold dark matter leads to a bottom-up formation of structure with galaxies forming first and galaxy clusters at a latter stage, while hot dark matter would result in a top-down formation scenario with large matter aggregations forming early, later fragmenting into separate galaxies; the latter is excluded by high-redshift galaxy observations.

These categories also correspond to fluctuation spectrum effects and the interval following the Big Bang at which each type became non-relativistic. Davis "et al." wrote in 1985:

Another approximate dividing line is warm dark matter became non-relativistic when the universe was approximately 1 year old and 1 millionth of its present size and in the radiation-dominated era (photons and neutrinos), with a photon temperature 2.7 million Kelvins. Standard physical cosmology gives the particle horizon size as 2 "c t" (speed of light multiplied by time) in the radiation-dominated era, thus 2 light-years. A region of this size would expand to 2 million light-years today (absent structure formation). The actual FSL is approximately 5 times the above length, since it continues to grow slowly as particle velocities decrease inversely with the scale factor after they become non-relativistic. In this example the FSL would correspond to 10 million light-years, or 3 megaparsecs, today, around the size containing an average large galaxy.

The 2.7 million K photon temperature gives a typical photon energy of 250 electronvolts, thereby setting a typical mass scale for warm dark matter: particles much more massive than this, such as GeV–TeV mass WIMPs, would become non-relativistic much earlier than one year after the Big Bang and thus have FSLs much smaller than a protogalaxy, making them cold. Conversely, much lighter particles, such as neutrinos with masses of only a few eV, have FSLs much larger than a protogalaxy, thus qualifying them as hot.

Cold dark matter offers the simplest explanation for most cosmological observations. It is dark matter composed of constituents with an FSL much smaller than a protogalaxy. This is the focus for dark matter research, as hot dark matter does not seem capable of supporting galaxy or galaxy cluster formation, and most particle candidates slowed early.

The constituents of cold dark matter are unknown. Possibilities range from large objects like MACHOs (such as black holes and Preon stars) or RAMBOs (such as clusters of brown dwarfs), to new particles such as WIMPs and axions.

Studies of Big Bang nucleosynthesis and gravitational lensing convinced most cosmologists that MACHOs cannot make up more than a small fraction of dark matter. According to A. Peter: "... the only "really plausible" dark-matter candidates are new particles."

The 1997 DAMA/NaI experiment and its successor DAMA/LIBRA in 2013, claimed to directly detect dark matter particles passing through the Earth, but many researchers remain skeptical, as negative results from similar experiments seem incompatible with the DAMA results.

Many supersymmetric models offer dark matter candidates in the form of the WIMPy Lightest Supersymmetric Particle (LSP). Separately, heavy sterile neutrinos exist in non-supersymmetric extensions to the standard model which explain the small neutrino mass through the seesaw mechanism.

Warm dark matter comprises particles with an FSL comparable to the size of a protogalaxy. Predictions based on warm dark matter are similar to those for cold dark matter on large scales, but with less small-scale density perturbations. This reduces the predicted abundance of dwarf galaxies and may lead to lower density of dark matter in the central parts of large galaxies. Some researchers consider this a better fit to observations. A challenge for this model is the lack of particle candidates with the required mass ≈ 300 eV to 3000 eV.

No known particles can be categorized as warm dark matter. A postulated candidate is the sterile neutrino: A heavier, slower form of neutrino that does not interact through the weak force, unlike other neutrinos. Some modified gravity theories, such as scalar–tensor–vector gravity, require "warm" dark matter to make their equations work.

Hot dark matter consists of particles whose FSL is much larger than the size of a protogalaxy. The neutrino qualifies as such particle. They were discovered independently, long before the hunt for dark matter: they were postulated in 1930, and detected in 1956. Neutrinos' mass is less than 10 that of an electron. Neutrinos interact with normal matter only via gravity and the weak force, making them difficult to detect (the weak force only works over a small distance, thus a neutrino triggers a weak force event only if it hits a nucleus head-on). This makes them 'weakly interacting light particles' (WILPs), as opposed to WIMPs.

The three known flavours of neutrinos are the "electron", "muon", and "tau". Their masses are slightly different. Neutrinos oscillate among the flavours as they move. It is hard to determine an exact upper bound on the collective average mass of the three neutrinos (or for any of the three individually). For example, if the average neutrino mass were over 50 eV/c (less than 10 of the mass of an electron), the universe would collapse. CMB data and other methods indicate that their average mass probably does not exceed 0.3 eV/c. Thus, observed neutrinos cannot explain dark matter.

Because galaxy-size density fluctuations get washed out by free-streaming, hot dark matter implies the first objects that can form are huge supercluster-size pancakes, which then fragment into galaxies. Deep-field observations show instead that galaxies formed first, followed by clusters and superclusters as galaxies clump together.

If dark matter is made up of sub-atomic particles, then millions, possibly billions, of such particles must pass through every square centimeter of the Earth each second. Many experiments aim to test this hypothesis. Although WIMPs are popular search candidates, the Axion Dark Matter Experiment (ADMX) searches for axions. Another candidate is heavy hidden sector particles which only interact with ordinary matter via gravity.

These experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of dark matter particle annihilations or decays.

Direct detection experiments aim to observe low-energy recoils (typically a few keVs) of nuclei induced by interactions with particles of dark matter, which (in theory) are passing through the Earth. After such a recoil the nucleus will emit energy in the form of scintillation light or phonons, as they pass through sensitive detection apparatus. To do this effectively, it is crucial to maintain a low background, and so such experiments operate deep underground to reduce the interference from cosmic rays. Examples of underground laboratories with direct detection experiments include the Stawell mine, the Soudan mine, the SNOLAB underground laboratory at Sudbury, the Gran Sasso National Laboratory, the Canfranc Underground Laboratory, the Boulby Underground Laboratory, the Deep Underground Science and Engineering Laboratory and the China Jinping Underground Laboratory.

These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include ZEPLIN, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.

Currently there has been no well-established claim of dark matter detection from a direct detection experiment, leading instead to strong upper limits on the mass and interaction cross section with nucleons of such dark matter particles. The DAMA/NaI and more recent DAMA/LIBRA experimental collaborations have detected an annual modulation in the rate of events in their detectors, which they claim is due to dark matter. This results from the expectation that as the Earth orbits the Sun, the velocity of the detector relative to the dark matter halo will vary by a small amount. This claim is so far unconfirmed and in contradiction with negative results from other experiments such as LUX, SuperCDMS and XENON100.

A special case of direct detection experiments covers those with directional sensitivity. This is a search strategy based on the motion of the Solar System around the Galactic Center. A low-pressure time projection chamber makes it possible to access information on recoiling tracks and constrain WIMP-nucleus kinematics. WIMPs coming from the direction in which the Sun travels (approximately towards Cygnus) may then be separated from background, which should be isotropic. Directional dark matter experiments include DMTPC, DRIFT, Newage and MIMAC.

Indirect detection experiments search for the products of the self-annihilation or decay of dark matter particles in outer space. For example, in regions of high dark matter density (e.g., the centre of our galaxy) two dark matter particles could annihilate to produce gamma rays or Standard Model particle–antiparticle pairs. Alternatively, if the dark matter particle is unstable, it could decay into Standard Model (or other) particles. These processes could be detected indirectly through an excess of gamma rays, antiprotons or positrons emanating from high density regions in our galaxy or others. A major difficulty inherent in such searches is that various astrophysical sources can mimic the signal expected from dark matter, and so multiple signals are likely required for a conclusive discovery.

A few of the dark matter particles passing through the Sun or Earth may scatter off atoms and lose energy. Thus dark matter may accumulate at the center of these bodies, increasing the chance of collision/annihilation. This could produce a distinctive signal in the form of high-energy neutrinos. Such a signal would be strong indirect proof of WIMP dark matter. High-energy neutrino telescopes such as AMANDA, IceCube and ANTARES are searching for this signal.
The detection by LIGO in September 2015 of gravitational waves, opens the possibility of observing dark matter in a new way, particularly if it is in the form of primordial black holes.

Many experimental searches have been undertaken to look for such emission from dark matter annihilation or decay, examples of which follow.
The Energetic Gamma Ray Experiment Telescope observed more gamma rays in 2008 than expected from the Milky Way, but scientists concluded this was most likely due to incorrect estimation of the telescope's sensitivity.

The Fermi Gamma-ray Space Telescope is searching for similar gamma rays. In April 2012, an analysis of previously available data from its Large Area Telescope instrument produced statistical evidence of a 130 GeV signal in the gamma radiation coming from the center of the Milky Way. WIMP annihilation was seen as the most probable explanation.

At higher energies, ground-based gamma-ray telescopes have set limits on the annihilation of dark matter in dwarf spheroidal galaxies and in clusters of galaxies.

The PAMELA experiment (launched in 2006) detected excess positrons. They could be from dark matter annihilation or from pulsars. No excess antiprotons were observed.

In 2013 results from the Alpha Magnetic Spectrometer on the International Space Station indicated excess high-energy cosmic rays which could be due to dark matter annihilation.

An alternative approach to the detection of dark matter particles in nature is to produce them in a laboratory. Experiments with the Large Hadron Collider (LHC) may be able to detect dark matter particles produced in collisions of the LHC proton beams. Because a dark matter particle should have negligible interactions with normal visible matter, it may be detected indirectly as (large amounts of) missing energy and momentum that escape the detectors, provided other (non-negligible) collision products are detected. Constraints on dark matter also exist from the LEP experiment using a similar principle, but probing the interaction of dark matter particles with electrons rather than quarks. Any discovery from collider searches must be corroborated by discoveries in the indirect or direct detection sectors to prove that the particle discovered is, in fact, dark matter.

Because dark matter has not yet been conclusively identified, many other hypotheses have emerged aiming to explain the observational phenomena that dark matter was conceived to explain. The most common method is to modify general relativity. General relativity is well-tested on solar system scales, but its validity on galactic or cosmological scales has not been well proven. A suitable modification to general relativity can conceivably eliminate the need for dark matter. The best-known theories of this class are MOND and its relativistic generalization tensor-vector-scalar gravity (TeVeS), f(R) gravity, negative mass, dark fluid, and entropic gravity. Alternative theories abound.

A problem with alternative hypotheses is observational evidence for dark matter comes from so many independent approaches (see the "observational evidence" section above). Explaining any individual observation is possible but explaining all of them is very difficult. Nonetheless, there have been some scattered successes for alternative hypotheses, such as a 2016 test of gravitational lensing in entropic gravity.

The prevailing opinion among most astrophysicists is while modifications to general relativity can conceivably explain part of the observational evidence, there is probably enough data to conclude there must be some form of dark matter.

Mention of dark matter is made in works of fiction. In such cases, it is usually attributed extraordinary physical or magical properties. Such descriptions are often inconsistent with the hypothesized properties of dark matter in physics and cosmology.




</doc>
<doc id="8653" url="https://en.wikipedia.org/wiki?curid=8653" title="Ducati Motor Holding S.p.A.">
Ducati Motor Holding S.p.A.

Ducati Motor Holding S.p.A. () is the motorcycle-manufacturing division of Italian company Ducati, headquartered in Bologna, Italy. The company is owned by German automotive manufacturer Audi through its Italian subsidiary Lamborghini, with Audi itself owned by the Volkswagen Group.

In 1926 Antonio Cavalieri Ducati and his three sons, Adriano, Marcello, and Bruno Cavalieri Ducati, founded "Società Scientifica Radio Brevetti Ducati" in Bologna to produce vacuum tubes, condensers and other radio components. In 1935 they had become successful enough to enable construction of a new factory in the Borgo Panigale area of the city. Production was maintained during World War II, despite the Ducati factory being a repeated target of Allied bombing.It was finally destroyed by around 40 Consolidated B-24 Liberators on Thursday October 12, 1944 as part of the United States Air Force's Operation Pancake which involved some 700 aircraft flying from airfields in the Province of Foggia.
Meanwhile, at the small Turinese firm SIATA ("Societa Italiana per Applicazioni Tecniche Auto-Aviatorie"), Aldo Farinelli began developing a small pushrod engine for mounting on bicycles. Barely a month after the official liberation of Italy in 1944, SIATA announced its intention to sell this engine, called the "Cucciolo" (Italian for "puppy," in reference to the distinctive exhaust sound) to the public. The first Cucciolos were available alone, to be mounted on standard bicycles, by the buyer; however, businessmen soon bought the little engines in quantity, and offered complete motorized-bicycle units for sale.

In 1950, after more than 200,000 Cucciolos had been sold, in collaboration with SIATA, the Ducati firm finally offered its own Cucciolo-based motorcycle. This first Ducati motorcycle was a 48 cc bike weighing , with a top speed of , and had a giving just under . Ducati soon dropped the Cucciolo name in favor of "55M" and "65TL".

When the market moved toward larger motorcycles, Ducati management decided to respond, making an impression at an early-1952 Milan show, introducing their 65TS cycle and Cruiser (a four-stroke motor scooter). Despite being described as the most interesting new machine at the 1952 show, the Cruiser was not a great success, and only a few thousand were made over a two-year period before the model ceased production.

In 1953, management split the company into two separate entities, Ducati Meccanica SpA and Ducati Elettronica, in acknowledgment of its diverging motorcycle and electronics product lines. Dr. Giuseppe Montano took over as head of Ducati Meccanica SpA and the Borgo Panigale factory was modernized with government assistance. By 1954, Ducati Meccanica SpA had increased production to 120 bikes a day.

In the 1960s, Ducati earned its place in motorcycling history by producing the fastest 250 cc road bike then available, the Mach 1. In the 1970s Ducati began producing large-displacement V-twin motorcycles and in 1973, released a V-twin with the trademarked desmodromic valve design. In 1985, Cagiva bought Ducati and planned to rebadge Ducati motorcycles with the "Cagiva" name. By the time the purchase was completed, Cagiva kept the "Ducati" name on its motorcycles. Eleven years later, in 1996, Cagiva accepted the offer from Texas Pacific Group and sold a 51% stake in the company for US$325 million; then, in 1998, Texas Pacific Group bought most of the remaining 49% to become the sole owner of Ducati. In 1999, TPG issued an initial public offering of Ducati stock and renamed the company "Ducati Motor Holding SpA". TPG sold over 65% of its shares in Ducati, leaving TPG the majority shareholder. In December 2005, Ducati returned to Italian ownership with the sale of Texas Pacific's stake (minus one share) to Investindustrial Holdings, the investment fund of Carlo and Andrea Bonomi.

In April 2012, Volkswagen Group's Audi subsidiary announced its intention to buy Ducati for € (US$). Volkswagen chairman Ferdinand Piëch, a motorcycle enthusiast, had long coveted Ducati, and had regretted that he passed up an opportunity to buy the company from the Italian government in 1984. Analysts doubted a tiny motorcycle maker would have a meaningful effect on a company the size of Volkswagen, commenting that the acquisition has "a trophy feel to it," and, "is driven by VW's passion for nameplates rather than industrial or financial logic". Italian luxury car brand Lamborghini was strengthened under VW ownership. AUDI AG's Automobili Lamborghini S.p.A. subsidiary acquired 100 percent of the shares of Ducati Motor Holding S.p.A. on 19 July 2012 for € (US$).

Since 1926, Ducati has been owned by a number of groups and companies.

Ducati is best known for high-performance motorcycles characterized by large-capacity four-stroke, 90° V-twin engines, with a desmodromic valve design. Ducati refers to this configuration as L-twin because one cylinder is vertical while the other is horizontal, making it look like a letter "L". Modern Ducatis remain among the dominant performance motorcycles available today partly because of the desmodromic valve design, which is nearing its 50th year of use. Desmodromic valves are closed with a separate, dedicated cam lobe and lifter instead of the conventional valve springs used in most internal combustion engines in consumer vehicles. This allows the cams to have a more radical profile, thus opening and closing the valves more quickly without the risk of valve-float, which causes a loss of power that is likely when using a "passive" closing mechanism under the same conditions.

While most other manufacturers use wet clutches (with the spinning parts bathed in oil) Ducati previously used multiplate dry clutches in many of their motorcycles. The dry clutch eliminates the power loss from oil viscosity drag on the engine, even though the engagement may not be as smooth as the oil-bath versions, but the clutch plates can wear more rapidly. Ducati has converted to wet clutches across their current product lines.

Ducati also extensively uses a trellis frame, although Ducati's MotoGP project broke with this tradition by introducing a revolutionary carbon fibre frame for the Ducati Desmosedici GP9.

The chief designer of most Ducati motorcycles in the 1950s was Fabio Taglioni (1920–2001). His designs ranged from the small single-cylinder machines that were successful in the Italian 'street races' to the large-capacity twins of the 1980s. Ducati introduced the Pantah in 1979; its engine was updated in the 1990s in the Ducati SuperSport (SS) series. All modern Ducati engines are derivatives of the Pantah, which uses a toothed belt to actuate the engine's valves. Taglioni used the Cavallino Rampante (identified with the Ferrari brand) on his Ducati motorbikes. Taglioni chose this emblem of courage and daring as a sign of respect and admiration for Francesco Baracca, a heroic World War I fighter pilot who died during an air raid in 1918.

In 1973, Ducati commemorated its 1972 win at the Imola 200 with the production model green frame Ducati 750 SuperSport.

Ducati also targeted the offroad market with the two-stroke Regolarità 125, building 3,486 models from 1975 to 1979, but the bike was not successful.

In 1975, the company introduced the 860 GT, designed by noted car stylist Giorgetto Giugiaro. Its angular lines were unique, but raised handlebars made for an uncomfortable seating position at high speeds and also caused steering issues. The 860GT's angular styling was a sales disaster, and it was hurriedly re-designed for the 1976 season with a more rounded fuel tank.

In 1975 Ducati offered hand-built production racers, the 'square case' 750SS and later 900SS models, built in limited numbers. Sales of the 900SS proved so strong, and sales of the 860GT/GTE/GTS so weak, that production of the 900SS was ramped up, and it became Ducati's #1 selling model.

Ducati's liquid-cooled, multi-valve V-twins, made from 1985 on, are known as "Desmoquattro" ("desmodromic valve four"). These include the 851, 916 and 996, 999 and a few predecessors and derivatives.

The Ducati Paso was introduced in 1986 with the Paso 750, followed in 1989 with the Paso 906. The final version came in 1991 with the 907IE (Iniezione Elettronica), now without the name "Paso". The design was from the hand of Massimo Tamburini, who also designed the Ducati 916 and MV Agusta F4. The Paso was a typical "you love it, you hate it" bike. However, at that time it looked like that all-enclosed bodywork would be the future for all motorcycles. The Paso design was copied for the Moto Morini Dart 400 and 125. Together with Tamburini's Bimota DB1, they were enormously influential in terms of styling.

In 1993, Miguel Angel Galluzzi introduced the Ducati Monster, a naked bike with exposed trellis and engine. Today the Monster accounts for almost half of the company's worldwide sales. The Monster has undergone the most changes of any motorcycle that Ducati has ever produced.

In 1993, Pierre Terblanche, Massimo Bordi and Claudio Domenicali designed the Ducati Supermono. A 550 cc single-cylinder lightweight "Catalog Racer". Only 67 were built between 1993 and 1997.

In 1994, the company introduced the Ducati 916 model designed by Massimo Tamburini, a water-cooled version that allowed for higher output levels and a striking new bodywork that had aggressive lines, an underseat exhaust, and a single-sided swingarm. Ducati has since ceased production of the 916, supplanting it (and its progeny, the 748, 996 and 998) with the 749 and 999.

In 2006, the retro-styled Ducati PaulSmart 1000 LE was released, which shared styling cues with the 1973 750 SuperSport (itself a production replica of Paul Smart's 1972 race winning 750 Imola Desmo), as one of a SportClassic series representing the 750 GT, 750 Sport, and 750 SuperSport Ducati motorcycles.



Streetfighter





</doc>
<doc id="8654" url="https://en.wikipedia.org/wiki?curid=8654" title="Data General Nova">
Data General Nova

The Data General Nova is a series of 16-bit minicomputers released by the American company Data General. The Nova family was very popular in the 1970s and ultimately sold tens of thousands of examples.

The first model, known simply as "Nova", was released in 1969. The Nova was packaged into a single rack-mount case and had enough computing power to handle most simple tasks. The Nova became popular in science laboratories around the world. It was followed the next year by the SuperNOVA, which ran roughly four times as fast.

Introduced during a period of rapid progress in integrated circuit (IC, or "chip") design, the line went through several upgrades over the next five years, introducing the 800 and 1200, the Nova 2, Nova 3, and ultimately the Nova 4. A single-chip implementation was also introduced as the microNOVA in 1977, but did not see widespread use as the market moved to new microprocessor designs. Fairchild Semiconductor also introduced a microprocessor version of the Nova in 1977, the Fairchild 9440, but it also saw limited use in the market.

The Nova line was succeeded by the Data General Eclipse, which was similar in most ways but added virtual memory support and other features required by modern operating systems. A 32-bit upgrade of the Eclipse resulted in the Eclipse MV series of the 1980s.

Edson de Castro was the Product Manager of the pioneering Digital Equipment Corporation (DEC) PDP-8, a 12-bit computer generally considered by most to be the first true minicomputer. He also led the design of the upgraded PDP-8/I, which used early integrated circuits in place of individual transistors.

During the PDP-8/I process, de Castro had been visiting circuit board manufacturers who were making rapid strides in the complexity of the boards they could assemble. de Castro concluded that the 8/I could be produced using fully automated assembly on large boards, which would have been impossible only a year earlier. Others within DEC had become used to the smaller boards used in earlier machines and were concerned about tracking down problems when there were many components on a single board. For the 8/I, the decision was made to stay with small boards, using the new "flip-chip" packaging for a modest improvement in density.

During the period when the PDP-8 was being developed, the introduction of ASCII and its major update in 1967 led to a new generation of designs with word lengths that were multiples of 8 bits rather than multiples of 6 bits as in most previous designs. This led to mid-range designs working at 16-bit word lengths instead of DEC's current 12- and 18-bit lineups. de Castro was convinced that it was possible to improve upon the PDP-8 by building a 16-bit minicomputer CPU on a single 15-inch square board.

In 1967, de Castro began a new design effort known as "PDP-X" which included several advanced features. Among these was a single underlying design that could be used to build 8-, 16- and 32-bit platforms. This progressed to the point of producing several detailed architecture documents. Ken Olsen was not supportive of this project, feeling it did not offer sufficient advantages over the 12-bit PDP-8 and the 18-bit PDP-9. It was eventually canceled in the spring of 1968.

Cancelation of the PDP-X prompted de Castro to consider leaving DEC to build a system on his own. He was not alone; in late 1967 a group of like-minded engineers formed to consider such a machine. The group included Pat Green, a divisional manager, Richard Sogge, another hardware engineer, and a software engineer, Henry Burkhardt II. In contrast to the PDP-X, the new effort focussed on a single machine that could be brought to market quickly, as de Castro felt the PDP-X concept was far too ambitious for a small startup company.

Discussing it with the others at DEC, the initial concept led to an 8-bit machine which would be less costly to implement. At this time the group began talking with Herbert Richman, a salesman for Fairchild Semiconductor who knew the others through his contacts with DEC. Richman pointed out that the machine's internal word length didn't have to be the same as its external presentation; one could have a 16-bit machine that used a 4-bit arithmetic logic unit (ALU), for instance. This could be cheaply implemented with a single modern IC, which Fairchild just happened to be introducing at that time, in the form of the 74181.

This approach significantly reduced the complexity and cost of the main logic and is responsible for the Nova's low selling cost. The new design used a simple load–store architecture which would reemerge in the RISC designs in the 1980s. As the complexity of a flip-flop was being rapidly reduced as they were implemented in chips, they offset the lack of addressing modes of the load/store design by adding four general-purpose accumulators, instead of the single register that would be found in similar low-cost offerings.

In keeping with the original packaging concept of the 8/I, the Nova was based on two printed circuit boards, one for the CPU and another for various support systems. The boards were designed so they could be connected together using a printed circuit backplane, with minimal manual wiring, allowing all the boards to be built in an automated fashion. This greatly reduced costs over 8/I, which consisted of many smaller boards that had to be wired together at the backplane. The larger-board construction also made the Nova more reliable, which made it especially attractive for industrial or lab settings. Fairchild provided the medium-scale integration (MSI) chips used throughout the system.

Late in 1967, Richman introduced the group to New York-based lawyer Fred Adler, who began canvassing various funding sources for seed capital. By 1968, Adler had arranged a major funding deal with a consortium of venture capital funds from the Boston area, who agreed to provide an initial $400,000 investment with a second $400,000 available for production ramp-up. de Castro, Burkhart and Sogge quit DEC and started Data General (DG) on 15 April 1968. Green did not join them, considering the venture too risky, and Richman did not join until the product was up and running later in the year.

Work on the first system took about nine months, and the first sales efforts started that November. They had a bit of luck because the Fall Joint Computer Conference had been delayed until December that year, so they were able to bring a working unit to the Moscone Center where they ran a version of "Spacewar!". DG officially released the Nova in 1969 at a base price of US$3,995, advertising it as "the best small computer in the world." The basic model was not very useful out of the box, and adding RAM in the form of core memory typically brought the price up to $7,995.

The first sale was to a university in Texas, with the team hand-building an example which shipped out in February. However, this was in the midst of a strike in the airline industry and the machine never arrived. They sent a second example, which did arrive as the strike had ended, and in May the original one was finally delivered as well.

The system was successful from the start, with the 100th being sold after six months, and the 500th after 15 months. Sales accelerated as newer versions were introduced, and by 1975 the company had annual sales of $100 million.

Ken Olsen had publicly predicted that DG would fail, but with the release of the Nova it was clear that was not going to happen. By this time a number of other companies were talking about introducing 16-bit designs as well. Olsen decided these presented a threat to their 18-bit line as well as 12-bit, and began a new 16-bit design effort. This emerged in 1970 as the PDP-11, a much more complex design that was as different from the PDP-X as the Nova was. The two designs competed heavily in the market.

Rumors of the new system from DEC reached DG shortly after the Nova began shipping. In the spring of 1970 they hired a new designer, Larry Seligman, to leapfrog any possible machine in the making. Two major changes had taken place since the Nova was designed; one was that ICs continued to improve and offer higher densities, and another was that Intel was aggressively talking up semiconductor-based memories, promising 1000 bits on a single chip and running at much higher speeds than core.

Seligman's new design took advantage of both of these improvements. To start, the new ICs allowed the ALU to be expanded to a full 16-bit width, making the new design four times as fast at math as the original. In addition, a new smaller core memory was used that improved the cycle time from the original's 1,200 ns to 800 ns, offering a further improvement gain. Performance could be further improved by replacing the core with read-only memory; lacking core's read/write cycle, this could be accessed at 300 ns for a dramatic performance boost.

The resulting machine, known as the SuperNOVA, was released in 1970. Although the initial models still used core, the entire design was based on the premise that faster semiconductor memories would become available and the platform could make full use of them. This was introduced later the same year as the SuperNOVA SC, featuring semiconductor (SC) memory. The much higher performance memory allowed the CPU, which was synchronous with memory, to be further increased in speed to run at a 300 ns cycle time (3.3 MHz). This made it the fastest available minicomputer for many years. However, the new memory was also very expensive and ran hot, so it was not widely used.

While Seligman was working on the SuperNOVA, the company received a letter from Ron Gruner stating "I've read about your
product, I've read your ads, and I'm going to work for you. And I'm going to be at your offices in a week
to talk to you about that." He was hired on the spot.

By this point, the company had decided it needed a new version of their low-cost platform to take advantage of the changes in the market, and a new concept emerged where a single machine could be swapped out in the field by the customer if they needed to move from the low-cost to high-performance system. Gruner was put in charge of the low-cost machine while Seligman designed a matching high-performance version.

Gruner's low-cost model launched in 1970 as the Nova 1200, the 1200 referring to the use of the original Nova's 1,200 ns core memory. It also used the original 4-bit ALU, and was thus essentially a repackaged Nova. Seligman's repackaged SuperNOVA was released in 1971 as the Nova 800, resulting in the somewhat confusing naming where the lower-numbered model has higher performance. Both models were offered in a variety of cases, the 1200 with seven slots, the 1210 with four and the 1220 with fourteen.

By this time the PDP-11 was finally shipping. It offered a much richer instruction set architecture than the deliberately simple one in the Nova. Continuing improvement in IC designs, and especially their price–performance ratio, was eroding the value of the original simplified instructions. Seligman was put in charge of designing a new machine that would be compatible with the Nova while offering a much richer environment for those who wanted it. This concept shipped as the Data General Eclipse series, which offered the ability to add additional circuity to tailor the instruction set for scientific or data processing workloads. The Eclipse was successful in competing with the PDP-11 at the higher end of the market.

Around the same time, rumors of a new 32-bit machine from DEC began to surface. DG decided they had to have a similar product, and Gruner was put in charge of what became the Fountainhead Project. Given the scope of the project, they agreed that the entire effort should be handled off-site, and Gruner selected a location at Research Triangle Park in North Carolina. This design became very complex and was ultimately canceled years later.

While these efforts were underway, work on the Nova line continued.

The 840, first offered in 1973, also included a new paged memory system allowing for addresses of up to 17-bits. An index offset the base address into the larger 128 kword memory. Actually installing this much memory required considerable space; the 840 shipped in a large 14-slot case.

The next version was the Nova 2, with the first versions shipping in 1973. The Nova 2 was essentially a simplified version of the earlier machines as increasing chip densities allowed the CPU to be reduced in size. While the SuperNOVA used three 15×15" boards to implement the CPU and its memory, the Nova 2 fitted all of this onto a single board. ROM was used to store the boot code, which was then copied into core when the "program load" switch was flipped. Versions were available with four, seven and ten slots. Further improvements in core led to the 2/10 and 2/4, referring to their cycle times in milliseconds.

The Nova 3 of 1975 added two more registers, used to control access to a built-in stack. The processor was also re-implemented using TTL components, further increasing the performance of the system. The Nova 3 was offered in four-slot (the Nova 3/4) and twelve-slot (the Nova 3/12) versions.

It appears that Data General originally intended the Nova 3 to be the last of its line, planning to replace the Nova with the later Eclipse machines. However, continued demand led to a Nova 4 machine, this time based on four AMD Am2901 bit-slice ALUs. This machine was designed from the start to be both the Nova 4 and the Eclipse S/140, with different microcode for each. A floating-point co-processor was also available, taking up a separate slot. An additional option allowed for memory mapping, allowing programs to access up to 128 kwords of memory using bank switching. Unlike the earlier machines, the Nova 4 did not include a front panel console and instead relied on the terminal to emulate a console when needed.

There were three different versions of the Nova 4, the Nova 4/C, the Nova 4/S and the Nova 4/X. The Nova 4/C was a single-board implementation that included all of the memory (16 or 32 kwords). The Nova 4/S and 4/X used separate memory boards. The Nova 4/X had the on-board memory management unit (MMU) enabled to allow up to 128 kwords of memory to be used (the MMU was also installed in the Nova 4/S, but was disabled by firmware). Both the 4/S and the 4/X included a “prefetcher” to increase performance by fetching up to two instructions from memory before they were needed.

Data General also produced a series of single-chip implementations of the Nova processor as the microNOVA. Changes to the bus architecture limited speed dramatically, to the point where it was about one-half the speed of the original Nova. The original microNOVA with the “mN601” processor shipped in 1977. It was followed by the microNOVA MP/100 in 1979, which reduced the CPU to a single VLSI chip, the mN602. A larger version was also offered as the microNOVA MP/200, shipping the same year.

The microNOVA was later re-packaged in a PC-style case with two floppy disks as the Enterprise. Enterprise shipped in 1981, running RDOS, but the introduction of the IBM PC the same year made most other machines disappear under the radar.

The Nova influenced the design of both the Xerox Alto (1973) and Apple I (1976) computers, and its architecture was the basis for the Computervision CGP (Computervision Graphics Processor) series. Its external design has been reported to be the direct inspiration for the front panel of the MITS Altair (1975) microcomputer.

Data General followed up on the success of the original Nova with a series of faster designs. The Eclipse family of systems was later introduced with an extended upwardly compatible instruction set, and the MV-series further extended the Eclipse into a 32-bit architecture to compete with the DEC VAX. The development of the MV-series was documented in Tracy Kidder's popular 1981 book, "The Soul of a New Machine". Data General itself would later evolve into a vendor of Intel processor-based servers and storage arrays, eventually being purchased by EMC.

The Nova, unlike the PDP-8, was a load–store architecture. It had four 16-bit accumulator registers, of which two (2 and 3) could be used as index registers. There was a 15-bit program counter and a single-bit carry register. As with the PDP-8, current + zero page addressing was central. There was no stack register, but later Eclipse designs would utilize a dedicated hardware memory address for this function.

The earliest models of the Nova processed math serially in 4-bit packets, using a single 74181 bitslice ALU. A year after its introduction, this design was improved to include a full 16-bit parallel math unit using four 74181s, this design being referred to as the SuperNova. Future versions of the system added a stack unit and hardware multiply/divide.

The Nova 4 / Eclipse S/140 was based on four AMD 2901 bit-slice ALUs, with microcode in read-only memory, and was the first Nova designed for DRAM main memory only, without provision for magnetic core memory.

The first models were available with 8K words of magnetic core memory as an option, one that practically everyone had to buy, bringing the system cost up to $7,995.

This core memory board was organized in planar fashion as four groups of four banks, each bank carrying two sets of core in a 64 by 64 matrix; thus there were 64 x 64 = 4096 bits per set, x 2 sets giving 8,192 bits, x 4 banks giving 32,768 bits, x 4 groups giving a total of 131,072 bits, and this divided by the machine word size of 16 bits gave 8,192 words of memory.

The core on this 8K word memory board occupied a centrally located "board-on-a-board", 5.25" wide by 6.125" high, and was covered by a protective plate. It was surrounded by the necessary support driver read-write-rewrite circuitry. All of the core and the corresponding support electronics fit onto a single standard 15 x board. Up to 32K of such core RAM could be supported in one external expansion box. Semiconductor ROM was already available at the time, and RAM-less systems (i.e. with ROM only) became popular in many industrial settings. The original Nova machines ran at approximately 200 kHz, but its SuperNova was designed to run at up to 3 MHz when used with special semiconductor main memory.

The standardized backplane and I/O signals created a simple, efficient I/O design that made interfacing programmed I/O and Data Channel devices to the Nova simple compared to competing machines. In addition to its dedicated I/O bus structure, the Nova backplane had wire wrap pins that could be used for non-standard connectors or other special purposes.

The instruction format could be broadly categorized into one of three functions: <nowiki>1) register-to-register manipulation, 2) memory reference, and 3)</nowiki> input/output. Each instruction was contained in one word. The register-to-register manipulation was almost RISC-like in its bit-efficiency; and an instruction that manipulated register data could also perform tests, shifts and even elect to discard the result. Hardware options included an integer multiply and divide unit, a floating-point unit (single and double precision), and memory management.
The earliest Nova came with a BASIC interpreter on punched tape. As the product grew, Data General developed many languages for the Nova computers, running under a range of consistent operating systems. FORTRAN IV, ALGOL, Extended BASIC, Data General Business Basic, Interactive COBOL, and several assemblers were available from Data General. Third party vendors and the user community expanded the offerings with Forth, Lisp, BCPL, C, ALGOL, and other proprietary versions of COBOL and BASIC.

The machine instructions implemented below are the common set implemented by all of the Nova series processors. Specific models often implemented additional instructions, and some instructions were provided by optional hardware.

All arithmetic instructions operated between accumulators. For operations requiring two operands, one was taken from the source accumulator, and one from the destination accumulator, and the result was deposited in the destination accumulator. For single-operand operations, the operand was taken from the source register and the result replaced the destination register. For all single-operand opcodes, it was permissible for the source and destination accumulators to be the same, and the operation functioned as expected.

All arithmetic instructions included a "no-load" bit which, when set, suppressed the transfer of the result to the destination register; this was used in conjunction with the test options to perform a test without losing the existing contents of the destination register. In assembly language, adding a '#' to the opcode set the no-load bit.

The CPU contained a single-bit register called the carry bit, which after an arithmetic operation would contain the carry out of the most significant bit. The carry bit could be set to a desired value prior to performing the operation using a two-bit field in the instruction. The bit could be set, cleared, or complemented prior to performing the instruction. In assembly language, these options were specified by adding a letter to the opcode: 'O' — set the carry bit; 'Z' — clear the carry bit, 'C' — complement the carry bit, nothing — leave the carry bit alone. If the no-load bit was also specified, the specified carry value would be used for the computation, but the actual carry register would remain unaltered.

All arithmetic instructions included a two-bit field which could be used to specify a shift option, which would be applied to the result before it was loaded into the destination register. A single-bit left or right shift could be specified, or the two bytes of the result could be swapped. Shifts were 17-bit circular, with the carry bit "to the left" of the most significant bit. In other words, when a left shift was performed, the most significant bit of the result was shifted into the carry bit, and the previous contents of the carry bit were shifted into the least significant bit of the result. Byte swaps did not effect the carry bit. In assembly language, these options were specified by adding a letter to the opcode: 'L' — shift left; 'R' — shift right, 'S' — swap bytes; nothing — do not perform a shift or swap.

All arithmetic instructions included a three-bit field that could specify a test which was to be applied to the result of the operation. If the test evaluated to true, the next instruction in line was skipped. In assembly language, the test option was specified as a third operand to the instruction. The available tests were:

The actual arithmetic instructions were:

An example arithmetic instructions, with all options utilized, is:

ADDZR# 0,2,SNC

This decoded as: clear the carry bit; add the contents of AC2 (accumulator 2) to AC0; circularly shift the result one bit to the right; test the result to see if the carry bit is set and skip the next instruction if so. Discard the result after performing the test. In effect, this adds two numbers and tests to see if the result is odd or even.

The Nova instruction set contained a pair of instructions that transferred memory contents to accumulators and vice versa, two transfer-of-control instructions, and two instructions that tested the contents of a memory location. All memory reference instructions contained an eight-bit address field, and a two-bit field that specified the mode of memory addressing. The four modes were:

Obviously, mode 0 was only capable of addressing the first 256 memory words, given the eight-bit address field. This portion of memory was referred to as "page zero". Page zero memory words were considered precious to Nova assembly language programmers because of the small number available; only page zero locations could be addressed from anywhere in the program without resorting to indexed addressing, which required tying up accumulator 2 or 3 to use as an index register. In assembly language, a ".ZREL" directive caused the assembler to place the instructions and data words that followed it in page zero; an ".NREL" directive placed the following instructions and data words in "normal" memory. Later Nova models added instructions with extended addressing fields, which overcame this difficulty (at a performance penalty).

The assembler computed relative offsets for mode 1 automatically, although it was also possible to write it explicitly in the source. If a memory reference instruction referenced a memory address in .NREL space but no mode specifier, mode 1 was assumed and the assembler calculated the offset between the current instruction and the referenced location, and placed this in the instruction's address field (provided that the resulting value fit into the 8-bit field).

The two load and store instructions were:
Both of these instructions included an "indirect" bit. If this bit was set (done in assembly language by adding a '@' to the opcode), the contents of the target address were assumed to be a memory address itself, and that address would be referenced to do the load or store.

The two transfer-of-control instructions were:
As in the case of the load and store instructions, the jump instructions contained an indirect bit, which likewise was specified in assembly using the '@' character. In the case of an indirect jump, the processor retrieved the contents of the target location, and used the value as the memory address to jump to. However, unlike the load and store instructions, if the indirect address had the most significant bit set, it would perform a further cycle of indirection. On the Nova series processors prior to the Nova 3, there was no limit on the number of indirection cycles; an indirect address that referenced itself would result in an infinite indirect addressing loop, with the instruction never completing. (This could be alarming to users, since when in this condition, pressing the STOP switch on the front panel did nothing. It was necessary to reset the machine to break the loop.)

The two memory test instructions were:
As in the case of the load and store instructions, there was an indirect bit that would perform a single level of indirect addressing. These instructions were odd in that, on the Novas with magnetic core memory, the instruction was executed within the memory board itself. As was common at the time, the memory boards contained a "write-back" circuit to solve the destructive-read problem inherent to magnetic core memory. But the write-back mechanism also contained a mini arithmetic unit, which the processor used for several purposes. For the ISZ and DSZ instructions, the increment or decrement occurred between the memory location being read and the write-back; the CPU simply waited to be told if the result was zero or nonzero. These instructions were useful because they allowed a memory location to be used as a loop counter without tying up an accumulator, but they were slower than performing the equivalent arithmetic instructions.

Some examples of memory reference instructions:

LDA 1,COUNT

Transfers the contents of the memory location labeled COUNT into accumulator 1. Assuming that COUNT is in .NREL space, this instruction is equivalent to: LDA 1,1,(COUNT-(.+1))
where '.' represents the location of the LDA instruction.

JSR@ 0,17

Jump indirect to the memory address specified by the contents of location 17, in page zero space, and deposit the return address in accumulator 3. This was the standard method for making an RDOS system call on early Nova models; the assembly language mnemonic ".SYSTM" translated to this.

JMP 0,3

Jump to the memory location whose address is contained in accumulator 3. This was a common means of returning from a function or subroutine call, since the JSR instruction left the return address in accumulator 3.

STA 0,3,-1

Store the contents of accumulator 0 in the location that is one less than the address contained in accumulator 3.

DSZ COUNT

Decrement the value in the location labeled COUNT, and skip the next instruction if the result is zero. As in the case above, if COUNT is assumed to be in .NREL space, this is equivalent to: DSZ 1,(COUNT-(.+1))

The Novas implemented a channelized model for interfacing to I/O devices. In the model, each I/O device was expected to implement two flags, referred to as "Busy" and "Done", and three data and control registers, referred to as A, B, and C. I/O instructions were available to read and write the registers, and to send one of three signals to the device, referred to as "start", "clear", and "pulse". In general, sending a start signal initiated an I/O operation that had been set up by loading values into the A/B/C registers. The clear signal halted an I/O operation and cleared any resulting interrupt. The pulse signal was used to initiate ancillary operations on complex subsystems, such as seek operations on disk drives. Polled devices usually moved data directly between the device and the A register. DMA devices generally used the A register to specify the memory address, the B register to specify the number of words to be transferred, and the C register for control flags. Channel 63 referred to the CPU itself and was used for various special functions.

Each I/O instruction contained a six-bit channel number field, a four-bit to specify which register to read or write, and a two-bit field to specify which signal was to be sent. In assembly language, the signal was specified by adding a letter to the opcode: 'S' for start, 'C' for clear, 'P' for pulse, and nothing for no signal. The opcodes were:

In addition, four instructions were available to test the status of a device:

Starting a device caused it to set its busy flag. When the requested operation was completed, conventionally the device cleared its busy flag and set its done flag; most devices had their interrupt request mechanism wired to the done flag, so setting the done flag caused an interrupt (if interrupts were enabled and the device wasn't masked).

These instructions performed various CPU control and status functions. All of them were actually shorthand mnemonics for I/O instructions on channel 63, the CPU's self-referential I/O channel.

From the hardware standpoint, the interrupt mechanism was relatively simple, but also less flexible, than current CPU architectures. The backplane supported a single interrupt request line, which all devices capable of interrupting connected to. When a device needed to request an interrupt, it raised this line. The CPU took the interrupt as soon as it completed the current instruction. As stated above, a device was expected to raise its "done" I/O flag when it requested an interrupt, and the convention was that the device would clear its interrupt request when the CPU executed a I/O clear instruction on the device's channel number.

The CPU expected the operating system to place the address of its interrupt service routine into memory address 1. When a device interrupted, the CPU did an indirect jump through address 1, placing the return address into memory address 0, and disabling further interrupts. The interrupt handler would then perform an INTA instruction to discover the channel number of the interrupting device. This worked by raising an "acknowledge" signal on the backplane. The acknowledge signal was wired in a daisy-chain format across the backplane, such that it looped through each board on the bus. Any device requesting an interrupt was expected to block the further propagation of the acknowledge signal down the bus, so that if two or more devices had pending interrupts simultaneously, only the first one would see the acknowledge signal. That device then responded by placing its channel number on the data lines on the bus. This meant that, in the case of simultaneous interrupt requests, the device that had priority was determined by which one was physically closest to the CPU in the card cage.

After the interrupt had been processed and the service routine had sent the device an I/O clear, it resumed normal processing by enabling interrupts and then returning via an indirect jump through memory address 0. In order to prevent a pending interrupt from interrupting immediately before the return jump (which would cause the return address to be overwritten), the INTEN instruction had a one-instruction-cycle delay. When it was executed, interrupts would not be enabled until after the following instruction, which was expected to be the JMP@ 0 instruction, was executed.

The operating system's interrupt service routine then typically performed an indexed jump using the received channel number, to jump to the specific interrupt handling routine for the device. There were a few devices, notably the CPU's power-failure detection circuit, which did not respond to the INTA instruction. If the INTA returned a result of zero, the interrupt service routine had to poll all of the non-INTA-responding devices using the SKPDZ/SKPDN instructions to see which one interrupted.

The operating system could somewhat manage the ordering of interrupts by setting an interrupt mask using the MSKO instruction. This was intended to allow the operating system to determine which devices were permitted to interrupt at a given time. When this instruction was issued, a 16-bit interrupt mask was transmitted to all devices on the backplane. It was up to the device to decide what the mask actually meant to it; by convention, a device that was masked out was not supposed to raise the interrupt line, but the CPU had no means of enforcing this. Most devices that were maskable allowed the mask bit to be selected via a jumper on the board. There were devices that ignored the mask altogether.

On the systems having magnetic core memory (which retained its contents without power), recovery from a power failure was possible. A power failure detection circuit in the CPU issued an interrupt when loss of the main power coming into the computer was detected; from this point, the CPU had a short amount of time until a capacitor in the power supply lost its charge and the power to the CPU failed. This was enough time to stop I/O in progress, by issuing an IORST instruction, and then save the contents of the four accumulators and the carry bit to memory. When the power returned, if the CPU's front panel key switch was in the LOCK position, the CPU would start and perform an indirect jump through memory address 2. This was expected to be the address of an operating system service routine that would reload the accumulators and carry bit, and then resume normal processing. It was up to the service routine to figure out how to restart I/O operations that were aborted by the power failure.

As was the convention of the day, most Nova models provided a front panel console to control and monitor CPU functions. Models prior to the Nova 3 all relied on a canonical front panel layout, as shown in the Nova 840 panel photo above. The layout contained a keyed power switch, two rows of address and data display lamps, a row of data entry switches, and a row of function switches that activated various CPU functions when pressed. The address lamps always displayed the current value of the program counter, in binary. The data lamps displayed various values depending on which CPU function was active at the moment. To the left of the leftmost data lamp, an additional lamp displayed the current value of the carry bit. On most models the lamps were incandescent lamps which were soldered to the panel board; replacing burned-out lamps was a bane of existence for Data General field service engineers.

Each of the data switches controlled the value of one bit in a 16-bit value, and per Data General convention, they were numbered 0-15 from left to right. The data switches provided input to the CPU for various functions, and could also be read by a running program using the READS assembly language instruction. To reduce panel clutter and save money, the function switches were implemented as two-way momentary switches. When a function switch lever was lifted, it triggered the function whose name was printed above the switch on the panel; when the lever was pressed down, it activated the function whose name appeared below the switch. The switch lever returned to a neutral position when released.

Referencing the Nova 840 photo, the first four switches from the left performed the EXAMINE and DEPOSIT functions for the four accumulators. Pressing EXAMINE on one of these caused the current value of the accumulator to be displayed in binary by the data lamps. Pressing DEPOSIT transferred the binary value represented by the current settings of the data switches to the accumulator.

Going to the right, the next switch was the RESET/STOP switch. Pressing STOP caused the CPU to halt after completing the current instruction. Pressing RESET caused the CPU to halt immediately, cleared a number of CPU internal registers, and sent an I/O reset signal to all connected devices. The switch to the right of that was the START/CONTINUE switch. Pressing CONTINUE caused the CPU to resume executing at the instruction currently pointed at by the program counter. Pressing START transferred the value currently set in data switches 1-15 to the program counter, and then began executing from there.

The next two switches provided read and write access to memory from the front panel. Pressing EXAMINE transferred the value set in data switches 1-15 to the program counter, fetched the value in the corresponding memory location, and displayed its value in the data lamps. Pressing EXAMINE NEXT incremented the program counter and then performed an examine operation on that memory location, allowing the user to step through a series of memory locations. Pressing DEPOSIT wrote the value contained in the data switches to the memory location pointed at by the program counter. Pressing DEPOSIT NEXT first incremented the program counter and then deposited to the pointed-to memory location.

The INST STEP function caused the CPU to execute one instruction, at the current program counter location, and then halt. Since the program counter would be incremented as part of the instruction execution, this allowed the user to single-step through a program. MEMORY STEP, a misnomer, caused the CPU to run through a single clock cycle and halt. This was of little use to users and was generally only used by field service personnel for diagnostics.

PROGRAM LOAD was the mechanism usually used to boot a Nova. When this switch was triggered, it caused the 32-word boot ROM to be mapped over the first 32 words of memory, set the program counter to 0, and started the CPU. The boot ROM contained code that would read 256 words (512 bytes) of code from a selected I/O device into memory and then transfer control to the read-in code. The data switches 8-15 were used to tell the boot ROM which I/O channel to boot from. If switch 0 was off, the boot ROM would assume the device was a polled device (e.g., the paper tape reader) and run a polled input loop until 512 bytes had been read. If switch 0 was on, the boot ROM assumed the device was a DMA-capable device and it initiated a DMA data transfer. The boot ROM was not smart enough to position the device prior to initiating the transfer. This was a problem when rebooting after a crash; if the boot device was a disk drive, its heads had likely been left on a random cylinder. They had to be repositioned to cylinder 0, where RDOS wrote the first-level boot block, in order for the boot sequence to work. Conventionally this was done by cycling the drive through its load sequence, but users who got frustrated with the wait time (up to 5 minutes depending on the drive model) learned how to input from the front panel a drive "recalibrate" I/O code and single-step the CPU through it, an operation that took an experienced user only a few seconds.

The power switch was a 3-way keyed switch with positions marked OFF, ON, and LOCK. In the OFF position all power was removed from the CPU. Turning the key to ON applied power to the CPU. However, unlike current CPUs, the CPU did not start automatically when power was applied; the user had to use PROGRAM LOAD or some other method to start the CPU and initiate the boot sequence. Turning the switch to LOCK disabled the front panel function switches; by turning the switch to LOCK and removing the key, the user could render the CPU resistant to tampering. On systems with magnetic core memory, the LOCK position also enabled the auto power failure recovery function. The key could be removed in the OFF or LOCK positions.

The Nova 1200 executed core memory access instructions (LDA and STA) in 2.55 microseconds (μs). Use of read-only memory saved 0.4 μs. Accumulator instructions (ADD, SUB, COM, NEG, etc.) took 1.55 μs, MUL 2.55 μs, DIV 3.75 μs, ISZ 3.15-4.5 μs. On the later Eclipse MV/6000, LDA and STA took 0.44 μs, ADD, etc. took 0.33 μs, MUL 2.2 μs, DIV 3.19 μs, ISZ 1.32 μs, FAD 5.17 μs, FMMD 11.66 μs.

This is a minimal programming example in Nova assembly language. It is designed to run under RDOS and prints the string “Hello, world.” on the console.

Basic models of the Nova came without built-in hardware multiply and divide capability, to keep prices competitive. The following routine multiplies two 16-bit words to produce a 16-bit word result (overflow is ignored). It demonstrates combined use of ALU op, shift, and test (skip). Note that when this routine is called by jsr, AC3 holds the return address. This is used by the return instruction jmp 0,3. An idiomatic way to clear an accumulator is sub 0,0. Other single instructions can be arranged to load a specific set of useful constants (e.g. -2, -1, or +1).

The following routine prints the value of AC1 as a 16-digit binary number, on the RDOS console. It reveals further quirks of the Nova instruction set. For instance, there is no instruction to load an arbitrary “immediate” value into an accumulator (although memory reference instructions do encode such a value to form an effective address). Accumulators must generally be loaded from initialized memory locations (e.g. n16). Other contemporary machines such as the PDP-11, and practically all modern architectures, allow for immediate loads, although many such as ARM restrict the range of values that can be loaded immediately.

Because the RDOS .systm call macro implements a jsr, AC3 is overwritten by the return address for the .pchar function. Therefore, a temporary location is needed to preserve the return address of the caller of this function. For a recursive or otherwise re-entrant routine, a stack, hardware if available, software if not, must be used instead. The return instruction becomes jmp @ retrn which exploits the Nova's indirect addressing mode to load the return PC.

The constant definitions at the end show two assembler features: the assembler radix is octal by default (20 = sixteen), and character constants could be encoded as e.g. "0.

The Canadian Broadcasting Corporation in Montreal used the Nova 1200 for channel play-out automation up until the late 1980s. It was then replaced with refurbished Nova 4 units and these were in use until the mid 1990s.





</doc>
<doc id="8659" url="https://en.wikipedia.org/wiki?curid=8659" title="Protestant Church in the Netherlands">
Protestant Church in the Netherlands

The Protestant Church in the Netherlands (, abbreviated PKN) is the largest Protestant denomination in the Netherlands, being both Reformed (Calvinist) and Lutheran.

It was founded on 1 May 2004 as the merger of the vast majority of Dutch Reformed Church, the vast majority of the Reformed Churches in the Netherlands and the Evangelical Lutheran Church in the Kingdom of the Netherlands. The merger was the culmination of an organizational process started in 1961. Several orthodox Reformed and liberal churches did not merge into the new church.

The Protestant Church in the Netherlands (PKN) forms the country's second largest Christian denomination after the Roman Catholic Church, with approximately 1.6 million members as per the church official statistics or some 9.1% of the population in 2016. It is the traditional faith of the Dutch Royal Family – a remnant of historical dominance of the Dutch Reformed Church, the main predecessor of the Protestant Church.

The doctrine of the Protestant Church in the Netherlands is expressed in its creeds. In addition to holding the Apostles', the Nicene and the Athanasian Creeds of the universal church, it also holds to the confessions of its predecessor bodies. From the Lutheran tradition are the unaltered Augsburg Confession and Luther's Catechism. From the Reformed, the Heidelberg and Genevan Catechisms along with the Belgic Confession with the Canons of Dordt. The Church also acknowledges the Theological Declaration of Barmen and the Leuenberg Agreement. Ordination of women and blessings of same-sex marriages are allowed.

The PKN contains both liberal and conservative movements; although the liberal Remonstrants left talks when they could not agree with the unaltered adoption of the Canons of Dordt. Local congregations have far-reaching powers concerning "controversial" matters (such as admittance to holy communion or whether women are admitted as members of the congregation's consistory).

The polity of the Protestant Church in the Netherlands is a hybrid of presbyterian and congregationalist church governance. Church governance is organised along local, regional, and national lines. At the local level is the congregation. An individual congregation is led by a church council made of the minister along with elders and deacons elected by the congregation. At the regional level were 75 classical assemblies whose members are chosen by the church councils. As of May 1, 2018, these 75 classical assemblies are reorganized into 11 larger ones. At the national level is the General Synod which directs areas of common interest, such as theological education, ministry training and ecumenical cooperation.

The PKN has four different types of congregations:

Lutherans are a minority (about 1 percent) of the PKN's membership. To ensure that Lutherans are represented in the Church, the Lutheran congregations have their own synod. The Lutheran Synod also has representatives in the General Synod.

The Protestant Church in the Netherlands issues yearly reports regarding its membership and finances.

Its make-up by former affiliation of its congregations was as follows in 2017:

Trend shows that since 2011 identification with former denominations has been falling in favor of simply identifying as "Protestant".

Secularization, or the decline in religiosity, first became noticeable after 1960 in the Protestant rural areas of Friesland and Groningen. Then, it spread to Amsterdam, Rotterdam and the other large cities in the west. Finally the Catholic southern areas showed religious declines. A countervailing trend is produced by a religious revival in the Protestant Bible Belt, and the growth of Muslims and Hindu communities resulting from immigration and high birth rates.
Research in 2007 concluded that 42% of the members of the PKN were non-theists. Furthermore, in the PKN and several other smaller denominations of the Netherlands, one in six clergy were either agnostic or atheist. A minister of the PKN, Klaas Hendrikse once described God as "a word for experience, or human experience" and said that Jesus may have never existed.

Only those congregations belonging to the former Reformed Churches in the Netherlands have the legal right to secede from the PKN without losing its property and church during a transition period of 10 years. Seven congregations have so far decided to form the Continued Reformed Churches in the Netherlands. Two congregations have joined one of the other smaller Reformed churches in the Netherlands. Some minorities within congregations that joined the PKN decided to leave the church and associated themselves individually with one of the other Reformed churches.

Some congregations and members in the Dutch Reformed Church did not agree with the merger and have separated. They have organized themselves in the Restored Reformed Church. Estimations of their membership vary from 35,000 up to 70,000 people in about 120 local congregations. They disagree with the pluralism of the merged church which maintains, as they see it, contradicting Reformed and Lutheran confessions. This group also considers same-sex marriages and female clergy unbiblical.

In a meeting of eight Jewish and eight Protestant Dutch leaders in Israel in May 2011, a statement of cooperation was issued, indicating, for the most part, that the Protestant Church recognizes the issues involved with the Palestinian Christians and that this is sometimes at odds with support for the State of Israel, but standing up for the rights of the Palestinians does not detract from the emphasis on the safety of the State of Israel and vice versa.




</doc>
<doc id="8660" url="https://en.wikipedia.org/wiki?curid=8660" title="Christian Church (Disciples of Christ)">
Christian Church (Disciples of Christ)

The Christian Church (Disciples of Christ) is a Mainline Protestant Christian denomination in the United States and Canada. The denomination started with the Restoration Movement during the Second Great Awakening, first existing as a loose association of churches working towards Christian unity during the 19th century, then slowly forming quasi-denominational structures through missionary societies, regional associations, and an international convention. In 1968, the Disciples of Christ officially adopted a denominational structure at which time a group of churches left to remain nondenominational.

It is often referred to as The Christian Church, The Disciples of Christ, The Disciples, or the DOC The Christian Church was a charter participant in the formation of the World Council of Churches (WCC) and of the Federal Council of Churches (now the National Council of Churches), and it continues to be engaged in ecumenical conversations.

The Disciples' local churches are congregationally governed. In 2008 there were 679,563 members in 3,714 congregations in North America. By 2015, this number had declined to a baptized membership of 497,423 in 3,267 congregations, of whom about 306,905 were active members, while roughly 177,141 people attended Sunday services each week. In 2018, the denomination reported 380,248 members with 124,437 people in average worship attendance.

The name "Disciples of Christ" is shared by three other groups: the Churches of Christ, the Independent Christian churches and churches of Christ, and the Christian Congregation. They emerged from the same roots. The Stone-Campbell movement began as two separate threads, each without knowledge of the other, during the Second Great Awakening in the early 19th century. The first of these two groups, led by Barton W. Stone began at Cane Ridge, Bourbon County, Kentucky. The group called themselves simply "Christians". The second, began in western Pennsylvania and Virginia (now West Virginia), led by Thomas Campbell and his son, Alexander Campbell. Because the founders wanted to abandon all denominational labels, they used the biblical names for the followers of Jesus that they found in the Bible.

In 1801, the Cane Ridge Revival in Kentucky planted the seed for a movement in Kentucky and the Ohio River Valley to disassociate from denominationalism. In 1803 Stone and others withdrew from the Kentucky Presbytery and formed the Springfield Presbytery. The defining event of the Stone wing of the movement was the publication of the "Last Will and Testament of the Springfield Presbytery", at Cane Ridge, Kentucky, in 1804. "The Last Will" is a brief document in which Stone and five others announced their withdrawal from Presbyterianism and their intention to be solely part of the body of Christ. The writers appealed for the unity of all who follow Jesus, suggested the value of congregational self-governance, and lifted the Bible as the source for understanding the will of God. They denounced the divisive use of the Westminster Confession of Faith.

Soon, they adopted the name "Christian" to identify their group. Thus, the remnants of the Springfield Presbytery became the Christian Church. It is estimated that the Christian Church numbered about 12,000 by 1830.

Independently of Stone, the Campbell wing of the movement was launched when Thomas Campbell published the "Declaration and Address of the Christian Association of Washington," (Pennsylvania) in 1809. The Presbyterian Synod had suspended his ministerial credentials. In "The Declaration and Address" he set forth some of his convictions about the church of Jesus Christ, as he organized the Christian Association of Washington, not as a church but as an association of persons seeking to grow in faith. On May 4, 1811, however, the Christian Association constituted itself as a congregationally governed church. With the building it then constructed at Brush Run, it became known as Brush Run Church.

When their study of the New Testament led the reformers to begin to practice baptism by immersion, the nearby Redstone Baptist Association invited Brush Run Church to join with them for the purpose of fellowship. The reformers agreed provided that they would be "allowed to preach and to teach whatever they learned from the Scriptures."

Thus began a sojourn for the reformers among the Baptists within the Redstone Baptist Association (1815–1824). While the reformers and the Baptists shared the same beliefs in baptism by immersion and congregational polity, it was soon clear that the reformers were not traditional Baptists. Within the Redstone Association, the differences became intolerable to some of the Baptist leaders, when Alexander Campbell began publishing a journal, "The Christian Baptist," promoting reform. Campbell anticipated the conflict and moved his membership to a congregation of the Mahoning Baptist Association in 1824.
In 1827, the Mahoning Association appointed reformer Walter Scott as an Evangelist. Through Scott's efforts, the Mahoning Association grew rapidly. In 1828, Thomas Campbell visited several of the congregations formed by Scott and heard him preach. The elder Campbell realized that Scott was bringing an important new dimension to the movement with his approach to evangelism.

Several Baptist associations began disassociating congregations that refused to subscribe to the Philadelphia Confession. The Mahoning Association came under attack. In 1830, the Mahoning Baptist Association disbanded. Alexander ceased publication of "The Christian Baptist". In January 1831, he began publication of the "Millennial Harbinger".

The two groups united at High Street Meeting House, Lexington, Kentucky, with a handshake between Barton W. Stone and "Raccoon" John Smith, on Saturday, December 31, 1831. Smith had been chosen, by those present, to speak on behalf of the followers of the Campbells. While contemporaneous accounts are clear that the handshake took place on Saturday, some historians have changed the date of the merger to Sunday, January 1, 1832. The 1832 date has become generally accepted. The actual difference is about 20 hours.

Two representatives of those assembled were appointed to carry the news of the union to all the churches: John Rogers, for the Christians and "Raccoon" John Smith for the reformers. Despite some challenges, the merger succeeded.

With the merger, there was the challenge of what to call the new movement. Clearly, finding a Biblical, non-sectarian name was important. Stone wanted to continue to use the name "Christians." Alexander Campbell insisted upon "Disciples of Christ". Walter Scott and Thomas Campbell sided with Stone, but the younger Campbell had strong reasons and would not yield. As a result, both names were used. The confusion over names has been present ever since. Prior to the 1906 separation, congregations would typically be named "Disciples of Christ," "Christian Church," and "Church of Christ." However, there are different practices by each. More than the name separates each church. For example, the "Independent Christian Church" will not accept a woman as a minister while some of the "Disciples of Christ" congregations will. These different congregations (Disciples of Christ, Church of Christ, and Independent Church) share many of the same beliefs and practices but there are, in fact, some differences.

In 1849, the first National Convention was held at Cincinnati, Ohio. Alexander Campbell had concerns that holding conventions would lead the movement into divisive denominationalism. He did not attend the gathering. Among its actions, the convention elected Alexander Campbell its President and created the American Christian Missionary Society (ACMS).

The formation of a missionary society set the stage for further "co-operative" efforts. By the end of the century, the Foreign Christian Missionary Society and the Christian Women's Board of Missions were also engaged in missionary activities. Forming the ACMS did not reflect a consensus of the entire movement. Sponsorship of missionary activities became a divisive issue. In the succeeding decades, for some congregations and their leaders, co-operative work through missionary societies and the adoption of instrumental music in church worship was straying too far from their conception of the early church. After the American Civil War, the schism grew. While there was no disagreement over the need for evangelism, many believed that missionary societies were not authorized by scripture and would compromise the autonomy of local congregations. This became one important factor leading to the separation of the Churches of Christ from the Christian Church (Disciples of Christ).

From the beginning of the movement, the free exchange of ideas among the people was fostered by the journals published by its leaders. Alexander Campbell published "The Christian Baptist" and "The Millennial Harbinger". Barton W. Stone published "The Christian Messenger". In a respectful way, both men routinely published the contributions of others whose positions were radically different from their own.

Following Campbell's death in 1866, journals continued to keep the discussion and conversation alive. Between 1870 and 1900, two journals emerged as the most prominent. The "Christian Standard" was edited and published by Isaac Errett of Cincinnati. "The Christian Evangelist" was edited and published by J. H. Garrison from St. Louis. The two men enjoyed a friendly rivalry, and kept the dialog going within the movement. A third journal became part of the conversation with the publication in 1884 of "The Christian Oracle", later to become "The Christian Century", with an interdenominational appeal. In 1914, Garrison's Christian Publishing company was purchased by R. A. Long, who then established a non-profit corporation, "The Christian Board of Publication" as the Brotherhood publishing house.

In 1906, the U.S. Religious Census listed Churches of Christ for the first time as a group which was separate and distinct from the Disciples of Christ. However, the division had been growing for years, with published reports as early as 1883. The most obvious distinction between the two groups was the Churches of Christ rejecting the use of musical instruments in worship. The controversy over musical instruments began in 1860, when some congregations introduced organs, traditionally associated with wealthier, denominational churches. More basic were the underlying approaches to Biblical interpretation. The Churches of Christ permitted only those practices found in accounts of New Testament worship. They could find no New Testament documentation of the use of instrumental music in worship. The Disciples, by contrast, considered permissible any practices that the New Testament did not expressly forbid.

After the division, Disciples churches used "Christian Church" as the dominant designation for congregations. While music and the approach to missionary work were the most visible issues, there were also some deeper ones. The process that led to the separation had begun prior to the American Civil War.

Following the 1906 separation by the Churches of Christ, additional controversies arose. Should missionary efforts be cooperative or should they be independently sponsored by congregations? Should new methods of Biblical analysis, developed in the late 19th century, be embraced in the study and interpretation of the Bible? The "cooperative" churches were generally more likely to adopt the new biblical study methods.

During the first half of the 20th century, these opposing factions among the Christian Churches coexisted but with growing discomfort and tension. Among the cooperative churches, the three Missionary Societies merged into the United Christian Missionary Society in 1920. Human service ministries grew through the National Benevolent Association and provided assistance to orphans, the elderly and the disabled.
By mid century, the cooperative Christian Churches and the independent Christian Churches were following different paths.

Following World War II, it became obvious that the organizations that had been developed in previous decades no longer effectively met the needs of the postwar era. After a number of discussions throughout the 1950s, the 1960 International Convention of Christian Churches adopted a process to "restructure" the entire organization. The Commission on Restructure, chaired by Granville T. Walker, held its first meeting on October 30 & November 1, 1962. In 1968, the International Convention of Christian Churches (Disciples of Christ) adopted the commission's proposed "Provisional Design of the Christian Church (Disciples of Christ)." Soon the Provisional Design became "The Design."

Under the design, all churches in the 1968 yearbook of Christian Churches (Disciples of Christ) were automatically recognized as part of the Christian Church (Disciples of Christ). In the years that followed, many of the Independent Christian Church Congregations requested formal withdrawal from the yearbook. Many of those congregations became part of the Christian churches and churches of Christ.

As a congregational denomiation, each Disciple congregation determines the nature of its worship, study, Christian service, and witness to the world. Through belief in the priesthood of all believers, Disciples also practice freedom of interpretation among its members, with only baptism and confession of Christ as Lord required.

Early members of the Stone-Campbell Movement adopted the slogan "In essentials, Unity; In non-essentials, Liberty; and in all things, Charity." For modern disciples the one essential is the acceptance of Jesus Christ as Lord and Savior, and obedience to him in baptism. There is no requirement to give assent to any other statement of belief or creed. Nor is there any "official" interpretation of the Bible. Hierarchical doctrine was traditionally rejected by Disciples as human-made and divisive, and subsequently, freedom of belief and scriptural interpretation allows many Disciples to question or even deny beliefs common in doctrinal churches such as the Incarnation, the Trinity, and the Atonement. Beyond the essential commitment to follow Jesus, there is a tremendous freedom of belief and interpretation. As the basic teachings of Jesus are studied and applied to life, there is the freedom to interpret Jesus' teaching in different ways. As would be expected from such an approach, there is a wide diversity among Disciples in what individuals and congregations believe. It is not uncommon to find individuals who seemingly hold diametrically opposed beliefs within the same congregation affirming one another's journeys of faith as sisters and brothers in Christ.

Modern Disciples reject the use of creeds as "tests of faith," that is, as required beliefs, necessary to be accepted as a follower of Jesus. Although Disciples respect the great creeds of the church as informative affirmations of faith, they are never seen as binding. Since the adoption of The Design of the Christian Church (Disciples of Christ), in 1968, Disciples have celebrated a sense of unity in reading the preamble to the Design publicly.

Most congregations sing hymns, read from the Old and New Testaments, hear the word of God proclaimed through sermon or other medium and extend an invitation to become Christ's Disciples. 

As an integral part of worship in most Disciple congregations members celebrate Lord's Supper weekly. Through the observance of communion, individuals are invited to acknowledge their faults and sins, to remember the death and resurrection of Jesus Christ, to remember their baptism, and to give thanks for God's redeeming love. Because Disciples believe that the invitation to the table comes from Jesus Christ, communion is open to all who confess that Jesus Christ is Lord, regardless of their denominational affiliation. For most Disciples, communion is understood as the symbolic presence of Jesus within the gathered community.

Most Disciple congregations practice believer's baptism in the form of immersion, believing it to be the form used in the New Testament. The experiences of yielding to Christ in being buried with him in the waters of baptism and rising to a new life, have profound meaning for the church. While most congregations exclusively practice baptism by immersion, Disciples also accept other forms of baptism including infant baptism.

The Disciples celebrate their oneness with all who seek God through Jesus Christ, throughout time and regardless of location. In local communities, congregations share with churches of other denominations in joint worship and in community Christian service. Ecumenical cooperation and collaboration with other Christian Communions has long been practice by the Regions.

At the General Church level, the Christian Unity and Interfaith Ministries Unity (CUIM) coordinates the ecumenical and interfaith activities of the church. The Disciples continues to relate to the National Council of Churches and Canadian Council of Churches, both of which it was a founding member. It shares in the dialog and in the theological endeavors of the World Council of Churches. The Disciples has been a full participant in the Consultation on Church Union since it began in the 1960s. It continues to support those ongoing conversations which have taken on the title Churches Uniting in Christ.

The Disciples have two full communion partners: the United Church of Christ, since 1989, and the United Church of Canada, since 2019. These three denominations all share mutual full communion with each other. CUIM describes these partnerships as the proclamation of "mutual recognition of their sacraments and ordained ministry." Ordained Disciple ministers are able to directly serve in the United Church of Christ without having to seek additional qualifications.. 
Additionally, the Disciples combined their overseas ministries with the United Church of Christ in 1996. Known as Global Ministries, it is a common agency of both denominations with a joint staff and is a continuance of decades of cooperative work in global missions.

While the Disciples of Christ and United Church of Canada have entered full communion, the recentness of the agreement means that the provisions for mutual recognition of clergy are not yet finalized and adopted.

The Disciples believe in the priesthood of all believers, in that all people baptized are called to minister to others with diverse spiritual gifts. The Disciples view their Order of Ministry as a specific subset of all believers who are called with spiritual gifts specifically suited for pastoral ministry. Congregations use different terms to refer to persons in the Order of Ministry including Pastor and Reverend but most call them Ministers, including the denomination's governing documents.

Congregations sponsor members seeking ordination or commissioning as a Minister, and Regional Ministries organize committees to oversee the process. Ordination can be achieved by obtaining a Master of Divinity from a theological institution, which does not have to be an institution associated with the Disciples. Ordination can also be achieved through an "Apprentice" track which has candidates shadow ordained ministers. Finally, Ministers can be Commissioned, a shorter process for seminary students and those seeking short-term ministry in a Region. Regional requirements for ministry vary. Ordination is made official through a service which includes members of the church, clergy, and Regional Minister laying their hands on the candidate as the ordaining act. Ecumenical representatives are often included to emphasize the Disciples' desire for Christian unity.

Disciples recognize the ordinations of the United Church of Christ as do they for Disciples.

A General Commission on the Order of Ministry exists to interpret and review definitions of ministry, give oversight to Regions and congregations, provide other support, and maintain the standing of Regional Ministers and Ministers of General (National) Ministries.

In 2011, the denomination stated that "Disciples do not have a formal policy on same-sex marriage. Different congregations have the autonomy to discern on issues such as this one". In 2013, the Disciples of Christ voted in favor of a resolution affirming all members regardless of sexual orientation. After same-sex marriage was legalized in the US, the denomination reiterated that it leaves "all decisions of policy on same-sex marriage to local congregations".

The Disciples Alliance Q, an association of LGBTQ+ members, certifies congregations as "Open and Affirming" to show that they are accepting of all gender identities and sexual orientations. The process includes congregational workshops, information provided by the Alliance about what Open and Affirming means, and public witness to Open and Affirming ministries.

The structure of the Disciples is unique among Mainline Protestant churches. "The Design", the governing document of the denomination, describes three "expressions" of the church: congregational, regional, and general. Each of these expressions are "characterized by its integrity, self-governance, authority, rights, and responsibilities." In relating to each other, they work in covenant and not authority to support the ministry and work of the church.

Currently there are 31 regions of the Christian Church.

Congregations of the Disciples are self-governing in the tradition of congregational polity. They call their own Ministers, select their own leadership, own their own property, and manage their own affairs.

In Disciples congregations, the priesthood of all believers finds its expression in worship and Christian service. Typically, lay persons who have been elected and ordained as Elders preside with the church's Ministers in the celebration of the sacrament of Holy Communion. The Elders and Ministers provide spiritual oversight and care for members in partnership with one another.

The Regional Churches of the Christian Church provide resources for leadership development and opportunities for Christian fellowship beyond the local congregation. They have taken responsibility for the nurture and support of those individuals seeking to discern God's call to service as ordained or licensed ministers. Typically, they organize summer camping experiences for children and youth.

Regional churches assist congregations who are seeking ministers and ministers who are seeking congregations. Regional leadership is available on request to assist congregations that face conflict. Though they have no authority to direct the life of any congregation, the Regional Churches are analogous to the middle judicatories of other denominations.

The Christian Church (Disciples of Christ) at the "General Church" level consists of a number of self-governing agencies, which focus upon specific Christian witnesses to the world. The church agencies report to the General Assembly, which meets biennially in odd-numbered years and is an assembly of representatives selected by congregations and ordained ministers with standing in the denomination. The General Minister and President (GMP) is the lead pastor for the denomination and the chief executive officer of the legal corporation. Following the covenantal understanding of the denomination, the GMP does not have direct executive power over the General Ministries, regions, or congregations. The GMP is elected to a six-year term by the General Assembly.

The current General Minister and President is Teresa Hord Owens. When she was elected in 2017, Owens was the first black woman to lead a mainline denomination as their chief executive. Her presidency followed the presidency of Sharon E. Watkins, the first woman to lead a mainline denomination as their chief executive.

The General Ministries are:

One highly popular and respected General Agency program is the "Week of Compassion," named for the special offering to fund the program when it began in the 1950s. The Week of Compassion is the disaster relief and Third World development agency. It works closely with Church World Service and church related organizations in countries around the world where disasters strike, providing emergency aid.

The General Church has challenged the entire denomination to work for a 2020 Vision for the first two decades of the 21st Century. Together the denomination is well on the way to achieving its four foci:

The logo of the Christian Church (Disciples of Christ) is a red chalice with a white St. Andrew's Cross. The chalice represents the centrality of Communion to the life of the church. The cross of Saint Andrew is a reminder of the ministry of each person and the importance of evangelism, and recalls the denomination's Scottish Presbyterian ancestry.

After the 1968 General Assembly, the Administrative Committee charged a sub-committee with the task of proposing a symbol for the church. Hundreds of designs were submitted, but none seemed right. By November the Deputy General Minister and President, William Howland, suggested that the committee's staff consultant and chairperson agree on a specific proposal and bring it back to the committee: that meant Robert L. Friedly of the Office of Interpretation and Ronald E. Osborn.

On January 20, 1970, the two men sat down for lunch. With a red felt-tip pen, Osborn began to scrawl a Saint Andrew's cross circumscribed inside a chalice on his placemat.

Immediately, Friedly dispatched the crude drawing to Bruce Tilsley, a commercial artist and member of Central Christian Church of Denver, with the plea that he prepare an artistic version of the ideas. Tilsley responded with two or three sketches, from which was selected the now-familiar red chalice. Use of the proposed symbol became so prevalent that there was little debate when official adoption was considered at the 1971 General Assembly.

Because most congregations call themselves "Christian Churches," the chalice has become a simple way to identify Disciples of Christ Churches through signage, letterhead, and other forms of publicity.

The Christian Church (Disciples of Christ) has experienced a significant loss of membership since the middle of the 20th century. Membership peaked in 1958 at just under 2 million. In 1993, membership dropped below 1 million. In 2009, the denomination reported 658,869 members in 3,691 congregations. As of 2010, the five states with the highest adherence rates were Kansas, Missouri, Iowa, Kentucky and Oklahoma. The states with the largest absolute number of adherents were Missouri, Texas, Indiana, Kentucky and Ohio.

From the very beginnings of the movement, Disciples have founded institutions of higher learning. Alexander Campbell taught young leaders and founded Bethany College. The movement established similar schools, especially in the years following the American Civil War.

Because intellectual and religious freedom are important values for the Disciples of Christ, the colleges, universities, and seminaries founded by its congregations do not seek to indoctrinate students or faculty with a sectarian point of view.

In the 21st century, the relationship between the Christian Church (Disciples of Christ) and its affiliated universities is the purview of Higher Education and Leadership Ministries (HELM), an agency of the General Church.


The Disciples have four seminaries and divinity schools directly affiliated with the denomination. These institutions have an ecumenical student body, a reflection of the Disciples' focus on church unity. They are:

The Disciples have three additional institutions that provide supplementary education and community living for ecumenical theological institutions. They are:

The Disciples of Christ maintains ecumenical relations with the Pontifical Council for Promoting Christian Unity. It is also affiliated with other ecumenical organizations such as Churches Uniting in Christ, Christian Churches Together, the National Council of Churches and the World Council of Churches. It maintains Ordained Ministerial Partner Standing with the United Church of Christ, which means that clergy ordained in the Disciples of Christ may also serve in the United Church of Christ.







</doc>
<doc id="8662" url="https://en.wikipedia.org/wiki?curid=8662" title="David Rice Atchison">
David Rice Atchison

David Rice Atchison (August 11, 1807January 26, 1886) was a mid-19th century Democratic United States Senator from Missouri. He served as President pro tempore of the United States Senate for six years. Atchison served as a major general in the Missouri State Militia in 1838 during Missouri's Mormon War and as a Confederate brigadier general during the American Civil War under Major General Sterling Price in the Missouri Home Guard. He is best known for the claim that for 24 hours—Sunday, March 4, 1849 through noon on Monday—he may have been Acting President of the United States. This belief, however, is dismissed by nearly all historians, scholars, and biographers.

Atchison, owner of many slaves and a plantation, was a prominent pro-slavery activist and Border Ruffian leader, deeply involved with violence against abolitionists and other free-staters during the "Bleeding Kansas" events.

Atchison was born to William Atchison in Frogtown (later Kirklevington), which is now part of Lexington, Kentucky. He was educated at Transylvania University in Lexington, where his classmates included five future Democratic senators (Solomon Downs of Louisiana, Jesse Bright of Indiana, George Wallace Jones of Iowa, Edward Hannegan of Indiana, and Jefferson Davis of Mississippi). Atchison was admitted to the Kentucky bar in 1829.

In 1830 he moved to Liberty in Clay County in western Missouri, and set up practice there, where he also farmed. Atchison's law practice flourished, and his best-known client was Latter Day Saint Movement founder Joseph Smith. Atchison represented Smith in land disputes with non-Mormon settlers in Caldwell County and Daviess County.

Alexander William Doniphan joined Atchison's law practice in Liberty in May 1833. The two became fast friends and spent many leisure time hours playing cards, going to the horse races, hunting, fishing, attending social functions and political events. Atchison, already a member of the Liberty Blues, a volunteer militia in Missouri, got Doniphan to join.

Atchison was elected to the Missouri House of Representatives in 1834. He worked hard for the Platte Purchase, which extended the northwestern boundary of Missouri to the Missouri River in 1837.

When the earlier disputes broke out into the so-called Mormon War of 1838, Atchison was appointed a major general in the state militia and took part in suppression of the violence by both sides.

In 1838 he was re-elected to the Missouri State House of Representatives. In 1841, he was appointed a circuit court judge for the six-county area of the Platte Purchase. In 1843 he was named a county commissioner in Platte County, where he then lived.

In October 1843, Atchison was appointed to the U.S. Senate to fill the vacancy left by the death of Lewis F. Linn. He thus became the first senator from western Missouri. At age 36, he was the youngest senator from Missouri up to that time. Atchison was re-elected in 1849.

Atchison was very popular with his fellow Senate Democrats. When the Democrats took control of the Senate in December 1845, they chose Atchison as President pro tempore, placing him third in succession for the Presidency, and also giving him the duty of presiding over the Senate when the Vice President was absent. He was then only 38 years old and had served in the Senate just two years. In 1849 Atchison stepped down as President pro tempore in favor of William R. King. King in turn yielded the office back to Atchison in December 1852, since King had been elected Vice President of the United States. Atchison continued as President pro tempore until December 1854.

As a Senator, Atchison was a fervent advocate of slavery and territorial expansion. He supported the annexation of Texas and the U.S.-Mexican War. Atchison and Missouri's other Senator, the venerable Thomas Hart Benton, became rivals and finally enemies, though both were Democrats. Benton declared himself to be against slavery in 1849, and in 1851 Atchison allied with the Whigs to defeat Benton for re-election.

Benton, intending to challenge Atchison in 1854, began to agitate for territorial organization of the area west of Missouri (now the states of Kansas and Nebraska) so it could be opened to settlement. To counter this, Atchison proposed that the area be organized "and" that the section of the Missouri Compromise banning slavery there be repealed in favor of popular sovereignty, under which the settlers in each territory would decide themselves whether slavery would be allowed.

At Atchison's request, Senator Stephen Douglas of Illinois introduced the Kansas–Nebraska Act, which embodied this idea, in November 1853. The Act became law in May 1854, establishing the Territories of Kansas and Nebraska.

Both Douglas and Atchison had assumed that Nebraska would be settled by Free-State men from Iowa and Illinois, and Kansas by pro-slavery Missourians and other Southerners, thus preserving the numerical balance between free states and slave states. In 1854 Atchison helped found the town of Atchison, Kansas, as a pro-slavery settlement. The town (and county) were named for him.

In fact, while Southerners welcomed the opportunity to settle Kansas, very few actually chose to do so. Instead, most free-soilers preferred Kansas. Furthermore, anti-slavery activists throughout the North came to view Kansas as a battleground and formed societies to encourage free-soil settlers to go to Kansas and ensure that both Kansas and Nebraska would become free states.

It appeared as if the Kansas Territorial legislature to be elected in March 1855 would be controlled by free-soilers and ban slavery. This was viewed as a breach of faith by Atchison and his supporters. An angry Atchison called on pro-slavery Missourians to uphold slavery by force and "to kill every God-damned abolitionist in the district" if necessary. He recruited an immense mob of heavily armed Missourians, the infamous "Border Ruffians". On the election day, March 30, 1855, Atchison led 5,000 Border Ruffians into Kansas. They seized control of all polling places at gunpoint, cast tens of thousands of fraudulent votes for pro-slavery candidates, and elected a pro-slavery legislature.

The outrage was nonetheless accepted by the Federal government. When Territorial Governor Andrew Reeder objected, he was fired by President Pierce.

Despite this show of force, far more free-soilers than pro-slavery settlers migrated to Kansas. There were continual raids and ambushes by both sides in "Bleeding Kansas". But in spite of the best efforts of Atchison and the Ruffians, Kansas did reject slavery and finally became a free state in 1861.

Charles Sumner, in the epic "Crimes Against Kansas" speech on May 19, 1856, exposed Atchison's role in the invasion, tortures, and killings in Kansas. Speaking in the flamboyant style he and others used, lacing his prose with references to Roman history, Sumner compared Atchison to Roman Senator Catiline, who betrayed his country in a plot to overthrow the existing order. For two days, Sumner listed crime, after crime, in detail, complete with documentation by newspapers and letters of the time, showing the tortures and violence by Atchison and his men.

Two days later, Atchison gave his own speech, totally unaware as yet that he was exposed on Senate floor in such a fashion. Atchison's speech was to the Texas men he just met, hired and paid for, Atchison reveals in his speech, by "authorities in Washington". They are about to invade Lawrence Kansas. Atchison makes the men promise to kill and "draw blood," and boasts of his flag, which was red in color for "Southern Rights" and the color of blood. They would press "to blood" the spread of slavery into Kansas. He revealed in this speech that the immediate goal of the invasion was to stop the newspaper in Lawrence from publishing anti-slavery material. Atchison's men had made it a crime to publish anti-slavery newspapers in Kansas.

Atchison made it clear the men are to kill and draw blood, told the men they will be "well paid," and encouraged them to plunder from the homes that they invaded. That was after the hundreds of dozens of tortures and killings that Sumner had detailed in his Crimes Against Kansas speech. In other words, things were about to get much worse since Atchison had his hired men from Texas.

Atchison's Senate term expired on March 3, 1855. He sought election to another term, but the Democrats in the Missouri legislature were split between him and Benton, while the Whig minority put forward their own man. No Senator was elected until January 1857, when James S. Green was chosen.

When the First Transcontinental Railroad was proposed in the 1850s, Atchison called for it to be built along the central route (from St. Louis through Missouri, Kansas, and Utah), rather than the southern route (from New Orleans through Texas and New Mexico). Naturally, his suggested route went through Atchison.

Atchison and A. W. Doniphan would fall out over the politics preceding the Civil War and on which direction Missouri should proceed. Atchison favored secession, while Doniphan was torn and would remain for the most part non-committal. Privately Doniphan favored the Union, but found it hard to go against his friends and associates.

During the secession crisis in Missouri at the beginning of the American Civil War, Atchison sided with Missouri's pro-Confederate governor, Claiborne Jackson. He accepted an appointment as a major general in the Missouri State Guard. Atchison actively recruited State Guardsmen in northern Missouri and served with Missouri State Guard commander General Sterling Price in the summer campaign of 1861. In September 1861, Atchison led 3,500 State Guard recruits across the Missouri River to reinforce Price, and defeated Union troops that tried to block his force in the Battle of Liberty.

Atchison continued to serve through the end of 1861. In March 1862, Union forces in the Trans-Mississippi theater won a decisive victory at Pea Ridge in Arkansas and secured Union control of Missouri. Atchison then resigned from the army over reported strategy arguments with Price and moved to Texas for the duration of the Civil War. After the war he retired to his farm near Gower, and was noted to deny many of his pro-slavery public statements made prior to the Civil War. In addition, his retirement cottage outside of Plattsburg, Missouri burned to the ground before his death in 1886. This included the complete loss of his library containing books, documents, and letters which documented his role in the Mormon War, Indian affairs, pro-slavery activities, Civil War activities, and other legislation covering his career as a lawyer, senator, and soldier.

Inauguration Day—March 4—fell on a Sunday in 1849, and so President-elect Zachary Taylor did not take the presidential oath of office until the next day. Even so, the term of the outgoing president, James K. Polk, ended at noon on March 4. On March 2, outgoing vice president George M. Dallas relinquished his position as President of the Senate. Congress had previously chosen Atchison as President pro tempore. In 1849, according to the Presidential Succession Act of 1792, the Senate president pro tempore immediately followed the vice president in presidential line of succession. As Dallas's term also ended at noon on the 4th, and as neither Taylor nor Vice President-elect Millard Fillmore had been sworn-in to office on that day, it was claimed by some of Atchison's friends and colleagues that on March 4–5, 1849, Atchison was Acting President of the United States.

Historians, constitutional scholars and biographers all dismiss the claim. They point out that Atchison's Senate term had ended on March 3. When the Senate of the new Congress convened on March 5 to allow new senators and the new vice president to take the oath of office, the secretary of the Senate called members to order, as the Senate had no president pro tempore. Furthermore, the Constitution doesn't require the President-elect to take the oath of office to hold the office, just to execute the powers. Also, as Atchison never swore the presidential oath either, he could not have acted as President. Most historians and scholars assert that as soon as the outgoing President's term expires, the President-elect automatically assumes the office, although some claim instead that the office is vacant until the taking of the oath.

In September 1872, Atchison, who never himself claimed that he was technically president, told a reporter for the "Plattsburg Lever":

Atchison died on January 26, 1886, at his home near Gower, Missouri at the age of 78. He was buried at Greenlawn Cemetery in Plattsburg, Missouri. His grave marker reads "President of the United States for One Day."




 


</doc>
<doc id="8663" url="https://en.wikipedia.org/wiki?curid=8663" title="Daniel Gabriel Fahrenheit">
Daniel Gabriel Fahrenheit

Daniel Gabriel Fahrenheit FRS (; ; 24 May 1686 – 16 September 1736) was a physicist, inventor, and scientific instrument maker. Fahrenheit was born in Danzig (Gdańsk), then a predominantly German-speaking city in the Pomeranian Voivodeship of the Polish–Lithuanian Commonwealth, but lived most of his life in the Dutch Republic (1701–1736) and was one of the notable figures in the Golden Age of Dutch science and technology.

A pioneer of exact thermometry, he helped lay the foundations for the era of precision thermometry by inventing the mercury-in-glass thermometer (first widely used, practical, accurate thermometer) and Fahrenheit scale (first standardized temperature scale to be widely used). In other words, Fahrenheit's inventions ushered in the first revolution in the history of thermometry (branch of physics concerned with methods of temperature measurement). From the early 1710s until the beginnings of the electronic era, mercury-in-glass thermometers were among the most reliable and accurate thermometers ever invented.

Fahrenheit was born in Gdańsk (north Poland), then in the Polish–Lithuanian Commonwealth, but lived most of his life in the Dutch Republic. The Fahrenheits were a German Hanse merchant family who had lived in several Hanseatic cities. Fahrenheit's great-grandfather had lived in Rostock, and research suggests that the Fahrenheit family originated in Hildesheim. Daniel's grandfather moved from Kneiphof in Königsberg (present-day Kaliningrad) to Gdańsk and settled there as a merchant in 1650. His son, Daniel Fahrenheit (the father of Daniel Gabriel), married Concordia Schumann, daughter of a well-known Gdańsk business family. Daniel was the eldest of the five Fahrenheit children (two sons, three daughters) who survived childhood. His sister, Virginia Elisabeth Fahrenheit, married Benjamin Krüger and was the mother of Benjamin Ephraim Krüger, a clergyman and playwright.

Daniel Gabriel began training as a merchant in Amsterdam after his parents died on 14 August 1701 from eating poisonous mushrooms. However, Fahrenheit's interest in natural science led him to begin studies and experimentation in that field. From 1717, he traveled to Berlin, Halle, Leipzig, Dresden, Copenhagen, and also to his hometown, where his brother still lived. During that time, Fahrenheit met or was in contact with Ole Rømer, Christian Wolff, and Gottfried Leibniz. In 1717, Fahrenheit settled in The Hague as a glassblower, making barometers, altimeters, and thermometers. From 1718 onwards, he lectured in chemistry in Amsterdam. He visited England in 1724 and was the same year elected a Fellow of the Royal Society. From August 1736 Fahrenheit stayed in the house of Johannes Frisleven at Plein square in The Hague, in connection with an application for a patent at the States of Holland and West Friesland. At the beginning of September he became ill and on the 7th his health had deteriorated to such an extent that he had notary Willem Ruijsbroek come to draw up his will. On the 11th the notary came by again to make some changes. Five days after that Fahrenheit died at the age of fifty. Four days later he received a fourth-class funeral, which meant that he was destitute, in the Kloosterkerk in The Hague. (the Cloister or Monastery Church)

According to Fahrenheit's 1724 article, he determined his scale by reference to three fixed points of temperature. The lowest temperature was achieved by preparing a frigorific mixture of ice, water, and a salt ("ammonium chloride or even sea salt"), and waiting for the eutectic system to reach equilibrium temperature. The thermometer then was placed into the mixture and the liquid in the thermometer allowed to descend to its lowest point. The thermometer's reading there was taken as 0 °F. The second reference point was selected as the reading of the thermometer when it was placed in still water when ice was just forming on the surface. This was assigned as 30 °F. The third calibration point, taken as 90 °F, was selected as the thermometer's reading when the instrument was placed under the arm or in the mouth.

Fahrenheit came up with the idea that Mercury boils around 300 degrees on this temperature scale. Work by others showed that water boils about 180 degrees above its freezing point. The Fahrenheit scale later was redefined to make the freezing-to-boiling interval exactly 180 degrees, a convenient value as 180 is a highly composite number, meaning that it is evenly divisible into many fractions. It is because of the scale's redefinition that normal mean body temperature today is taken as 98.2 degrees, whereas it was 96 degrees on Fahrenheit's original scale.

The Fahrenheit scale was the primary temperature standard for climatic, industrial and medical purposes in English-speaking countries until the 1970s, nowadays replaced by the Celsius scale long used in the rest of the world, apart from the United States, where temperatures and weather reports are still broadcast in Fahrenheit.





</doc>
<doc id="8664" url="https://en.wikipedia.org/wiki?curid=8664" title="Freescale DragonBall">
Freescale DragonBall

Motorola/Freescale Semiconductor's DragonBall, or MC68328, is a microcontroller design based on the famous 68000 core, but implemented as an all-in-one low-power system for handheld computer use. It is supported by μClinux. It was designed by Motorola in Hong Kong and released in 1995.

The DragonBall's major design win was in numerous devices running the Palm OS platform. However, from Palm OS 5 onwards their use was superseded by ARM-based processors from Texas Instruments and Intel. 

The processor is capable of speeds of up to 16.58 MHz and can run up to 2.7 MIPS (million instructions per second), for the base 68328 and DragonBall EZ (MC68EZ328) model. It was extended to 33 MHz, 5.4 MIPS for the DragonBall VZ (MC68VZ328) model, and 66 MHz, 10.8 MIPS for the DragonBall Super VZ (MC68SZ328).

It is a 32-bit processor with 32-bit internal and external address bus (24-bit external address bus for EZ and VZ variants) and 32-bit data bus. It has many built-in functions, like a color and grayscale display controller, PC speaker sound, serial port with UART and IRDA support, UART bootstrap, real time clock, is able to directly access DRAM, Flash ROM, mask ROM, and has built-in support for touch screens.

The more recent DragonBall MX series microcontrollers, later renamed the Freescale i.MX (MC9328MX/MCIMX) series, are intended for similar application to the earlier DragonBall devices but are based on an ARM processor core instead of a 68000 core.



</doc>
<doc id="8667" url="https://en.wikipedia.org/wiki?curid=8667" title="Double-slit experiment">
Double-slit experiment

In modern physics, the double-slit experiment is a demonstration that light and matter can display characteristics of both classically defined waves and particles; moreover, it displays the fundamentally probabilistic nature of quantum mechanical phenomena. This type of experiment was first performed, using light, by Thomas Young in 1801, as a demonstration of the wave behavior of light. At that time it was thought that light consisted of "either" waves "or" particles. With the beginning of modern physics, about a hundred years later, it was realized that light could in fact show behavior characteristic of "both" waves "and" particles. In 1927, Davisson and Germer demonstrated that electrons show the same behavior, which was later extended to atoms and molecules. Thomas Young's experiment with light was part of classical physics well before quantum mechanics, and the concept of wave-particle duality. He believed it demonstrated that the wave theory of light was correct, and his experiment is sometimes referred to as Young's experiment or Young's slits.

The experiment belongs to a general class of "double path" experiments, in which a wave is split into two separate waves that later combine into a single wave. Changes in the path lengths of both waves result in a phase shift, creating an interference pattern. Another version is the Mach–Zehnder interferometer, which splits the beam with a mirror. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles (not waves); the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through. These results demonstrate the principle of wave–particle duality.

Other atomic-scale entities, such as electrons, are found to exhibit the same behavior when fired towards a double slit. Additionally, the detection of individual discrete impacts is observed to be inherently probabilistic, which is inexplicable using classical mechanics.

The experiment can be done with entities much larger than electrons and photons, although it becomes more difficult as size increases. The largest entities for which the double-slit experiment has been performed were molecules that each comprised 810 atoms (whose total mass was over 10,000 atomic mass units).

The double-slit experiment (and its variations) has become a classic thought experiment, for its clarity in expressing the central puzzles of quantum mechanics. Because it demonstrates the fundamental limitation of the ability of the observer to predict experimental results, Richard Feynman called it "a phenomenon which is impossible […] to explain in any classical way, and which has in it the heart of quantum mechanics. In reality, it contains the only mystery [of quantum mechanics]."

If light consisted strictly of ordinary or classical particles, and these particles were fired in a straight line through a slit and allowed to strike a screen on the other side, we would expect to see a pattern corresponding to the size and shape of the slit. However, when this "single-slit experiment" is actually performed, the pattern on the screen is a diffraction pattern in which the light is spread out. The smaller the slit, the greater the angle of spread. The top portion of the image shows the central portion of the pattern formed when a red laser illuminates a slit and, if one looks carefully, two faint side bands. More bands can be seen with a more highly refined apparatus. Diffraction explains the pattern as being the result of the interference of light waves from the slit.

If one illuminates two parallel slits, the light from the two slits again interferes. Here the interference is a more pronounced pattern with a series of alternating light and dark bands. The width of the bands is a property of the frequency of the illuminating light. (See the bottom photograph to the right.) When Thomas Young (1773–1829) first demonstrated this phenomenon, it indicated that light consists of waves, as the distribution of brightness can be explained by the alternately additive and subtractive interference of wavefronts. Young's experiment, performed in the early 1800s, played a vital part in the acceptance of the wave theory of light, vanquishing the corpuscular theory of light proposed by Isaac Newton, which had been the accepted model of light propagation in the 17th and 18th centuries. However, the later discovery of the photoelectric effect demonstrated that under different circumstances, light can behave as if it is composed of discrete particles. These seemingly contradictory discoveries made it necessary to go beyond classical physics and take the quantum nature of light into account.

Feynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment. He also proposed (as a thought experiment) that if detectors were placed before each slit, the interference pattern would disappear.

The Englert–Greenberger duality relation provides a detailed treatment of the mathematics of double-slit interference in the context of quantum mechanics.

A low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, by reducing the level of incident light until photon emission/absorption events were mostly non-overlapping.

A double-slit experiment was not performed with anything other than light until 1961, when Claus Jönsson of the University of Tübingen performed it with electron beams. In 1974, the Italian physicists Pier Giorgio Merli, Gian Franco Missiroli, and Giulio Pozzi repeated the experiment using single electrons and biprism (instead of slits), showing that each electron interferes with itself as predicted by quantum theory. In 2002, the single-electron version of the experiment was voted "the most beautiful experiment" by readers of "Physics World."

In 2012, Stefano Frabboni and co-workers eventually performed the double-slit experiment with electrons and real slits, following the original scheme proposed by Feynman. They sent single electrons onto nanofabricated slits (about 100 nm wide) and, by collecting the transmitted electrons with a single-electron detector, they could show the build-up of a double-slit interference pattern.

In 2019, single particle interference was demonstrated for antimatter by Marco Giammarchi and coworkers.

An important version of this experiment involves single particles (or waves—for consistency, they are called particles here). Sending particles through a double-slit apparatus one at a time results in single particles appearing on the screen, as expected. Remarkably, however, an interference pattern emerges when these particles are allowed to build up one by one (see the adjacent image). This demonstrates the wave–particle duality, which states that all matter exhibits both wave and particle properties: the particle is measured as a single pulse at a single position, while the wave describes the probability of absorbing the particle at a specific place on the screen. This phenomenon has been shown to occur with photons, electrons, atoms and even some molecules, including buckyballs. So experiments with electrons add confirmatory evidence to the view that electrons, protons, neutrons, and even larger entities that are ordinarily called particles nevertheless have their own wave nature and even a wavelength (related to their momentum).

The probability of detection is the square of the amplitude of the wave and can be calculated with classical waves (see below). The particles do not arrive at the screen in a predictable order, so knowing where all the previous particles appeared on the screen and in what order tells nothing about where a future particle will be detected. If there is a cancellation of waves at some point, that does not mean that a particle disappears; it will appear somewhere else. Ever since the origination of quantum mechanics, some theorists have searched for ways to incorporate additional determinants or "hidden variables" that, were they to become known, would account for the location of each individual impact with the target.

More complicated systems that involve two or more particles in superposition are not amenable to the above explanation.

A well-known thought experiment predicts that if particle detectors are positioned at the slits, showing through which slit a photon goes, the interference pattern will disappear. This which-way experiment illustrates the complementarity principle that photons can behave as either particles or waves, but cannot be observed as both at the same time.
Despite the importance of this thought experiment in the history of quantum mechanics (for example, see the discussion on ), technically feasible realizations of this experiment were not proposed until the 1970s. (Naive implementations of the textbook "gedanken" experiment are not possible because photons cannot be detected without absorbing the photon.) Currently, multiple experiments have been performed illustrating various aspects of complementarity.

An experiment performed in 1987 produced results that demonstrated that information could be obtained regarding which path a particle had taken without destroying the interference altogether. This showed the effect of measurements that disturbed the particles in transit to a lesser degree and thereby influenced the interference pattern only to a comparable extent. In other words, if one does not insist that the method used to determine which slit each photon passes through be completely reliable, one can still detect a (degraded) interference pattern.

Wheeler's delayed choice experiments demonstrate that extracting "which path" information after a particle passes through the slits can seem to retroactively alter its previous behavior at the slits.

Quantum eraser experiments demonstrate that wave behavior can be restored by erasing or otherwise making permanently unavailable the "which path" information.

A simple do-it-at-home illustration of the quantum eraser phenomenon was given in an article in "Scientific American". If one sets polarizers before each slit with their axes orthogonal to each other, the interference pattern will be eliminated. The polarizers can be considered as introducing which-path information to each beam. Introducing a third polarizer in front of the detector with an axis of 45° relative to the other polarizers "erases" this information, allowing the interference pattern to reappear. This can also be accounted for by considering the light to be a classical wave, and also when using circular polarizers and single photons. Implementations of the polarizers using entangled photon pairs have no classical explanation.

In a highly publicized experiment in 2012, researchers claimed to have identified the path each particle had taken without any adverse effects at all on the interference pattern generated by the particles. In order to do this, they used a setup such that particles coming to the screen were not from a point-like source, but from a source with two intensity maxima. However, commentators such as Svensson have pointed out that there is in fact no conflict between the weak measurements performed in this variant of the double-slit experiment and the Heisenberg uncertainty principle. Weak measurement followed by post-selection did not allow simultaneous position and momentum measurements for each individual particle, but rather allowed measurement of the average trajectory of the particles that arrived at different positions. In other words, the experimenters were creating a statistical map of the full trajectory landscape.

In 1967, Pfleegor and Mandel demonstrated two-source interference using two separate lasers as light sources.

It was shown experimentally in 1972 that in a double-slit system where only one slit was open at any time, interference was nonetheless observed provided the path difference was such that the detected photon could have come from either slit. The experimental conditions were such that the photon density in the system was much less than unity.

In 1999, the double-slit experiment was successfully performed with buckyball molecules (each of which comprises 60 carbon atoms). A buckyball is large enough (diameter about 0.7 nm, nearly half a million times larger than a proton) to be seen under an electron microscope.

In 2005, E. R. Eliel presented an experimental and theoretical study of the optical transmission of a thin metal screen perforated by two subwavelength slits, separated by many optical wavelengths. The total intensity of the far-field double-slit pattern is shown to be reduced or enhanced as a function of the wavelength of the incident light beam.

In 2012, researchers at the University of Nebraska–Lincoln performed the double-slit experiment with electrons as described by Richard Feynman, using new instruments that allowed control of the transmission of the two slits and the monitoring of single-electron detection events. Electrons were fired by an electron gun and passed through one or two slits of 62 nm wide × 4 μm tall.

In 2013, the double-slit experiment was successfully performed with molecules that each comprised 810 atoms (whose total mass was over 10,000 atomic mass units). The record was raised to 2000 atoms (25,000 amu) in 2019.

Hydrodynamic analogs have been developed that can recreate various aspects of quantum mechanical systems, including single-particle interference through a double-slit. A silicone oil droplet, bouncing along the surface of a liquid, self-propels via resonant interactions with its own wave field. The droplet gently sloshes the liquid with every bounce. At the same time, ripples from past bounces affect its course. The droplet's interaction with its own ripples, which form what is known as a pilot wave, causes it to exhibit behaviors previously thought to be peculiar to elementary particles – including behaviors customarily taken as evidence that elementary particles are spread through space like waves, without any specific location, until they are measured.

Behaviors mimicked via this hydrodynamic pilot-wave system include quantum single particle diffraction, tunneling, quantized orbits, orbital level splitting, spin, and multimodal statistics. It is also possible to infer uncertainty relations and exclusion principles. Videos are available illustrating various features of this system. (See the External links.)

However, more complicated systems that involve two or more particles in superposition are not amenable to such a simple, classically intuitive explanation. Accordingly, no hydrodynamic analog of entanglement has been developed. Nevertheless, optical analogs are possible.

Much of the behaviour of light can be modelled using classical wave theory. The Huygens–Fresnel principle is one such model; it states that each point on a wavefront generates a secondary wavelet, and that the disturbance at any subsequent point can be found by summing the contributions of the individual wavelets at that point. This summation needs to take into account the phase as well as the amplitude of the individual wavelets. Only the intensity of a light field can be measured—this is proportional to the square of the amplitude.

In the double-slit experiment, the two slits are illuminated by a single laser beam. If the width of the slits is small enough (less than the wavelength of the laser light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves.

If the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure below right. The path difference between two waves travelling at an angle is given by:

Where d is the distance between the two slits. When the two waves are in phase, i.e. the path difference is equal to an integral number of wavelengths, the summed amplitude, and therefore the summed intensity is maximum, and when they are in anti-phase, i.e. the path difference is equal to half a wavelength, one and a half wavelengths, etc., then the two waves cancel and the summed intensity is zero. This effect is known as interference. The interference fringe maxima occur at angles

where λ is the wavelength of the light. The angular spacing of the fringes, , is given by

The spacing of the fringes at a distance from the slits is given by

For example, if two slits are separated by 0.5 mm (), and are illuminated with a 0.6μm wavelength laser (), then at a distance of 1m (), the spacing of the fringes will be 1.2 mm.

If the width of the slits is greater than the wavelength, the Fraunhofer diffraction equation gives the intensity of the diffracted light as:

Where the sinc function is defined as sinc("x") = sin("x")/"x" for "x" ≠ 0, and sinc(0) = 1.

This is illustrated in the figure above, where the first pattern is the diffraction pattern of a single slit, given by the function in this equation, and the second figure shows the combined intensity of the light diffracted from the two slits, where the function represent the fine structure, and the coarser structure represents diffraction by the individual slits as described by the function.

Similar calculations for the near field can be done using the Fresnel diffraction equation. As the plane of observation gets closer to the plane in which the slits are located, the diffraction patterns associated with each slit decrease in size, so that the area in which interference occurs is reduced, and may vanish altogether when there is no overlap in the two diffracted patterns.

Like the Schrödinger's cat thought experiment, the double-slit experiment is often used to highlight the differences and similarities between the various interpretations of quantum mechanics.

The Copenhagen interpretation, put forth by some of the pioneers in the field of quantum mechanics, asserts that it is undesirable to posit anything that goes beyond the mathematical formulae and the kinds of physical apparatus and reactions that enable us to gain some knowledge of what goes on at the atomic scale. One of the mathematical constructs that enables experimenters to predict very accurately certain experimental results is sometimes called a probability wave. In its mathematical form it is analogous to the description of a physical wave, but its "crests" and "troughs" indicate levels of probability for the occurrence of certain phenomena (e.g., a spark of light at a certain point on a detector screen) that can be observed in the macro world of ordinary human experience.

The probability "wave" can be said to "pass through space" because the probability values that one can compute from its mathematical representation are dependent on time. One cannot speak of the location of any particle such as a photon between the time it is emitted and the time it is detected simply because in order to say that something is located somewhere at a certain time one has to detect it. The requirement for the eventual appearance of an interference pattern is that particles be emitted, and that there be a screen with at least two distinct paths for the particle to take from the emitter to the detection screen. Experiments observe nothing whatsoever between the time of emission of the particle and its arrival at the detection screen. If a ray tracing is next made as if a light wave (as understood in classical physics) is wide enough to take both paths, then that ray tracing will accurately predict the appearance of maxima and minima on the detector screen when many particles pass through the apparatus and gradually "paint" the expected interference pattern.

The Copenhagen interpretation is similar to the path integral formulation of quantum mechanics provided by Feynman. The path integral formulation replaces the classical notion of a single, unique trajectory for a system, with a sum over all possible trajectories. The trajectories are added together by using functional integration.

Each path is considered equally likely, and thus contributes the same amount. However, the phase of this contribution at any given point along the path is determined by the action along the path:

All these contributions are then added together, and the magnitude of the final result is squared, to get the probability distribution for the position of a particle:

As is always the case when calculating probability, the results must then be normalized by imposing:

To summarize, the probability distribution of the outcome is the normalized square of the norm of the superposition, over all paths from the point of origin to the final point, of waves propagating proportionally to the action along each path. The differences in the cumulative action along the different paths (and thus the relative phases of the contributions) produces the interference pattern observed by the double-slit experiment. Feynman stressed that his formulation is merely a mathematical description, not an attempt to describe a real process that we can measure.

According to the relational interpretation of quantum mechanics, first proposed by Carlo Rovelli, observations such as those in the double-slit experiment result specifically from the interaction between the observer (measuring device) and the object being observed (physically interacted with), not any absolute property possessed by the object. In the case of an electron, if it is initially "observed" at a particular slit, then the observer–particle (photon–electron) interaction includes information about the electron's position. This partially constrains the particle's eventual location at the screen. If it is "observed" (measured with a photon) not at a particular slit but rather at the screen, then there is no "which path" information as part of the interaction, so the electron's "observed" position on the screen is determined strictly by its probability function. This makes the resulting pattern on the screen the same as if each individual electron had passed through both slits. It has also been suggested that space and distance themselves are relational, and that an electron can appear to be in "two places at once"—for example, at both slits—because its spatial relations to particular points on the screen remain identical from both slit locations.

Physicist David Deutsch argues in his book "The Fabric of Reality" that the double-slit experiment is evidence for the many-worlds interpretation. However, since every interpretation of quantum mechanics is empirically indistinguishable, some scientists are skeptical of this claim.

An alternative to the standard understanding of quantum mechanics, the De Broglie–Bohm theory states that particles have precise locations at all times, and that their velocities are influenced by the wave-function. So while a single particle will travel through one particular slit in the double-slit experiment, the so-called "pilot wave" that influences it will travel through both. The two slit de Broglie-Bohm trajectories were first calculated by Chris Dewdney whilst working with Chris Philippidis and Basil Hiley at Birkbeck College (London). The de Broglie-Bohm theory produces the same statistical results as standard quantum mechanics, but dispenses with many of its conceptual difficulties.







</doc>
<doc id="8668" url="https://en.wikipedia.org/wiki?curid=8668" title="Dan Bricklin">
Dan Bricklin

Daniel Singer Bricklin (born on July 16, 1951), is an American businessman and engineer who is the co-creator, with Bob Frankston, of the VisiCalc spreadsheet program. He also founded Software Garden, Inc., of which he is currently president, and Trellix Corporation. He currently serves as the chief technology officer of Alpha Software.

His book, "Bricklin on Technology", was published by Wiley in May 2009. For his work with VisiCalc, Bricklin is often referred to as “the father of the Spreadsheet.”

Bricklin was born in a Jewish family in Philadelphia, where he attended Akiba Hebrew Academy. He began his college as a mathematics major, but soon switched to computer science. He earned a Bachelor of Science in electrical engineering and computer science from the Massachusetts Institute of Technology in 1973, where he was a resident of Bexley Hall.

Upon graduating from MIT, Bricklin worked for Digital Equipment Corporation (DEC) where he was part of the team that worked on WPS-8 until 1976, when he began working for FasFax, a cash register manufacturer. In 1977, he returned to education, and was awarded a Master of Business Administration from Harvard University in 1979.

While a student at Harvard Business School, Bricklin co-developed VisiCalc in 1979, making it the first electronic spreadsheet readily available for home and office use. It ran on an Apple II computer, and was considered a fourth generation software program. VisiCalc is widely credited for fueling the rapid growth of the personal computer industry. Instead of doing financial projections with manually calculated spreadsheets, and having to recalculate with every single cell in the sheet, VisiCalc allowed the user to change any cell, and have the entire sheet automatically recalculated. This could turn 20 hours of work into 15 minutes and allowed for more creativity.

In 1979, Bricklin and Frankston founded Software Arts, Inc., and began selling VisiCalc, via a separate company named VisiCorp. Along with co-founder Bob Frankston, he started writing versions of the program for the Tandy TRS-80, Commodore PET and the Atari 800. Soon after its launch, VisiCalc became a fast seller at $100.

Software Arts also published TK/Solver and "and "Spotlight","a desktop organizer for the I.B.M. Personal Computer."

Bricklin was awarded the Grace Murray Hopper Award in 1981 for VisiCalc. Bricklin could not patent VisiCalc, since software programs were not eligible for patent protection at the time.

Bricklin was chairman of Software Arts until 1985, the year that Software Arts was acquired by Lotus. He left and founded Software Garden.

Dan Bricklin founded Software Garden, a small consulting firm and developer of software applications, in 1985. The company's focus was to produce and market “Dan Bricklin's Demo Program”. The program allowed users to create demonstrations of their programs before they were even written, and was also used to create tutorials for Windows-based programs. Other versions released soon after included demo-it!. He remained the president of the company until he co-founded Slate Corporation in 1990. In 1992, he became the vice president of Phoenix-based Slate corporation, and developed "At Hand", a pen-based spreadsheet. When Slate closed in 1994, Bricklin returned to Software Garden.

His ""Dan Bricklin's Overall Viewer"" (described by "The New York Times" as "a visual way to display information in Windows-based software") was released in November 1994.

In 1995 Bricklin founded Trellix Corporation, named for "Trellix Site Builder".

Trellix was bought by Interland (now Web.com) in 2003, and Bricklin became Interland's chief technology officer until early 2004.
Bricklin continues to serve as president of Software Garden, a small company that develops and markets software tools he creates, as well as providing speaking and consulting services.

He has released Note Taker HD, an application that integrates handwritten notes on the Apple iPad tablet.

He is also developing wikiCalc, a collaborative, basic spreadsheet running on the Web.

He is currently the chief technology officer of Alpha Software in Burlington, Massachusetts, a company that creates tools to easily develop cross-platform mobile business applications.

In 1994, Bricklin was inducted as a Fellow of the Association for Computing Machinery. He is a founding trustee of the Massachusetts Technology Leadership Council and has served on the boards of the Software Publishers Association and the Boston Computer Society. He was also elected to be a member of the National Academy of Engineering.

In 1981, Bricklin was given a Grace Murray Hopper Award for VisiCalc.

In 1996, Bricklin was awarded by the IEEE Computer Society with the Computer Entrepreneur Award for pioneering the development and commercialization of the spreadsheet and the profound changes it fostered in business and industry.

In 2003, Bricklin was given the Wharton Infosys Business Transformation Award for being a technology change leader. He was recognized for having used information technology in an industry-transforming way. He has received an Honorary Doctor of Humane Letters from Newbury College.

In 2004, he was made a Fellow of the Computer History Museum "for advancing the utility of personal computers by developing the VisiCalc electronic spreadsheet."

Bricklin:



</doc>
<doc id="8674" url="https://en.wikipedia.org/wiki?curid=8674" title="Digital enhanced cordless telecommunications">
Digital enhanced cordless telecommunications

Digital enhanced cordless telecommunications (Digital European cordless telecommunications), usually known by the acronym DECT, is a standard primarily used for creating cordless telephone systems. It originated in Europe, where it is the universal standard, replacing earlier cordless phone standards, such as 900 MHz CT1 and CT2.

Beyond Europe, it has been adopted by Australia, and most countries in Asia and South America. North American adoption was delayed by United States radio frequency regulations. This forced development of a variation of DECT, called DECT 6.0, using a slightly different frequency range which makes these units incompatible with systems intended for use in other areas, even from the same manufacturer. DECT has almost universally replaced other standards in most countries where it is used, with the exception of North America.

DECT was originally intended for fast roaming between networked base stations and the first DECT product was Net wireless LAN. However, its most popular application is single-cell cordless phones connected to traditional analog telephone, primarily in home and small office systems, though gateways with multi-cell DECT and/or DECT repeaters are also available in many private branch exchange (PBX) systems for medium and large businesses produced by Panasonic, Mitel, Gigaset, Snom, BT Business, Spectralink, and RTX Telecom. DECT can also be used for purposes other than cordless phones, such as baby monitors and industrial sensors. The ULE Alliance's DECT ULE and its "HAN FUN" protocol are variants tailored for home security, automation, and the internet of things (IoT).

The DECT standard includes the generic access profile (GAP), a common interoperability profile for simple telephone capabilities, which most manufacturers implement. GAP-conformance enables DECT handsets and bases from different manufacturers to interoperate at the most basic level of functionality, that of making and receiving calls. Japan uses its own DECT variant, J-DECT, which is supported by the DECT forum.

The New Generation DECT (NG-DECT) standard, marketed as CAT-iq by the DECT Forum, provides a common set of advanced capabilities for handsets and base stations. CAT-iq allows interchangeability across IP-DECT base stations and handsets from different manufacturers, while maintaining backward-compatibility with GAP equipment. It also requires mandatory support for wideband audio.

The DECT standard was developed by ETSI in several phases, the first of which took place between 1988 and 1992 when the first round of standards were published. These were the ETS 300-175 series in nine parts defining the air interface, and ETS 300-176 defining how the units should be type approved. A technical report, ETR-178, was also published to explain the standard. Subsequent standards were developed and published by ETSI to cover interoperability profiles and standards for testing.

Named Digital European Cordless Telephone at its launch by CEPT in November 1987; its name was soon changed to Digital European Cordless Telecommunications, following a suggestion by Enrico Tosato of Italy, to reflect its broader range of application including data services. In 1995, due to its more global usage, the name was changed from European to Enhanced. DECT is recognized by the ITU as fulfilling the IMT-2000 requirements and thus qualifies as a 3G system. Within the IMT-2000 group of technologies, DECT is referred to as IMT-2000 Frequency Time (IMT-FT).

DECT was developed by ETSI but has since been adopted by many countries all over the World. The original DECT frequency band (1880–1900 MHz) is used in all countries in Europe. Outside Europe, it is used in most of Asia, Australia and South America. In the United States, the Federal Communications Commission in 2005 changed channelization and licensing costs in a nearby band (1920–1930 MHz, or 1.9 GHz), known as Unlicensed Personal Communications Services (UPCS), allowing DECT devices to be sold in the U.S. with only minimal changes. These channels are reserved exclusively for voice communication applications and therefore are less likely to experience interference from other wireless devices such as baby monitors and wireless networks.

The New Generation DECT (NG-DECT) standard was first published in 2007; it was developed by ETSI with guidance from the Home Gateway Initiative through the DECT Forum to support IP-DECT functions in home gateway/IP-PBX equipment. The ETSI TS 102 527 series comes in five parts and covers wideband audio and mandatory interoperability features between handsets and base stations. They were preceded by an explanatory technical report, ETSI TR 102 570. The DECT Forum maintains the CAT-iq trademark and certification program; CAT-iq wideband voice profile 1.0 and interoperability profiles 2.0/2.1 are based on the relevant parts of ETSI TS 102 527.

The DECT Ultra Low Energy (DECT ULE) standard was announced in January 2011 and the first commercial products were launched later that year by Dialog Semiconductor. The standard was created to enable home automation, security, healthcare and energy monitoring applications that are battery powered. Like DECT, DECT ULE standard uses the 1.9 GHz band, and so suffers less interference than Zigbee, Bluetooth, or Wi-Fi from microwave ovens, which all operate in the unlicensed 2.4 GHz ISM band. DECT ULE uses a simple star network topology, so many devices in the home are connected to a single control unit.

Future revisions of the standard (tentatively termed DECT-2020) are expected to include high reliability low-latency DECT ULE for industry machine-to-machine application, high bitrate ultra reliable low latency protocols for professional wireless audio applications using point-to-point or multicast communications, high-throughput QAM-1024 modulation; a long term evolution to OFDM (downlink) and OFDMA/SC-FDMA (uplink) modulation with a downlink rate of 1Gbit/s (tentatively termed DECT-5G) is being researched by the ETSI DECT committee. The effort aims to adopt the updated DECT protocols into the upcoming IMT-2020 standard, which defines Ultra-Reliable Low-Latency Communications (URLLC), Massive Machine Type Communications (MMTC), and enhanced Mobile Broadband (eMBB) services.

A new low-complexity audio codec, LC3plus, has been added as an option to the 2019 revision of the DECT standard. This codec is designed for high-quality voice and music applications, and supports scalable narrowband, wideband, super wideband, and fullband coding, with sample rates of 8, 16, 24, 32 and 48 kHz and audio bandwidth of up to 20 kHz.

OpenD is an open-source framework designed to provide a complete software implementation of DECT ULE protocols on reference hardware from Dialog Semiconductor and DSP Group; the project is maintained by the DECT forum.

The DECT standard originally envisaged three major areas of application:

Of these, the domestic application (cordless home telephones) has been extremely successful. The enterprise PABX market had some success, and all the major PABX vendors have offered DECT access options. The public access application did not succeed, since public cellular networks rapidly out-competed DECT by coupling their ubiquitous coverage with large increases in capacity and continuously falling costs. There has been only one major installation of DECT for public access: in early 1998 Telecom Italia launched a wide-area DECT network known as "Fido" after much regulatory delay, covering major cities in Italy. The service was promoted for only a few months and, having peaked at 142,000 subscribers, was shut down in 2001.

DECT has been used for wireless local loop as a substitute for copper pairs in the "last mile" in countries such as India and South Africa. By using directional antennas and sacrificing some traffic capacity, cell coverage could extend to over . One example is the corDECT standard.

The first data application for DECT was Net wireless LAN system by Olivetti, launched in 1993 and discontinued in 1995. A precursor to Wi-Fi, Net was a micro-cellular data-only network with fast roaming between base stations and 520 kbit/s transmission rates.

Data applications such as electronic cash terminals, traffic lights, and remote door openers also exist, but have been eclipsed by Wi-Fi, 3G and 4G which compete with DECT for both voice and data.

DECT 6.0 is a North American marketing term for DECT devices manufactured for the United States and Canada operating at 1.9 GHz. The "6.0" does not equate to a spectrum band; it was decided the term DECT 1.9 might have confused customers who equate larger numbers (such as the 2.4 and 5.8 in existing 2.4 GHz and 5.8 GHz cordless telephones) with later products. The term was coined by Rick Krupka, marketing director at Siemens and the DECT USA Working Group / Siemens ICM.

In North America, DECT suffers from deficiencies in comparison to DECT elsewhere, since the UPCS band (1920–1930 MHz) is not free from heavy interference. Bandwidth is half as wide as that used in Europe (1880–1900 MHz), the 4 mW average transmission power reduces range compared to the 10 mW permitted in Europe, and the commonplace lack of GAP compatibility among US vendors binds customers to a single vendor.

Before 1.9 GHz band was approved by the FCC in 2005, DECT could only operate in unlicensed Region 2
2.4 GHz and 900 MHz ISM bands; some users of Uniden WDECT 2.4 GHz phones reported interoperability issues with Wi-Fi equipment.

North-American products may not be used in Europe, Pakistan, Sri Lanka, and Africa, as they cause and suffer from interference with the local cellular networks. Use of such products is prohibited by European Telecommunications Authorities, PTA, Telecommunications Regulatory Commission of Sri Lanka and the Independent Communication Authority of South Africa. European DECT products may not be used in the United States and Canada, as they likewise cause and suffer from interference with American and Canadian cellular networks, and use is prohibited by the Federal Communications Commission and Industry Canada.

DECT 8.0 HD is a marketing designation for North American DECT devices certified with CAT-iq 2.0 "Multi Line" profile.

Cordless Advanced Technology—internet and quality (CAT-iq) is a certification program maintained by the DECT Forum. It is based on New Generation DECT (NG-DECT) series of standards from ETSI.

NG-DECT/CAT-iq contains features that expand the generic GAP profile with mandatory support for high quality wideband voice, enhanced security, calling party identification, multiple lines, parallel calls, and similar functions to facilitate VoIP calls through SIP and H.323 protocols.

There are several CAT-iq profiles which define supported voice features:

CAT-iq allows any DECT handset to communicate with a DECT base from a different vendor, providing full interoperability. CAT-iq 2.0/2.1 feature set is designed to support IP-DECT base stations found in office IP-PBX and home gateways.

The DECT standard specifies a means for a portable phone or "Portable Part" to access a fixed telephone network via radio. Base station or "Fixed Part" is used to terminate the radio link and provide access to a fixed line. A gateway is then used to connect calls to the fixed network, such as public switched telephone network (telephone jack), office PBX, ISDN, or VoIP over Ethernet connection.

Typical abilities of a domestic DECT Generic Access Profile (GAP) system include multiple handsets to one base station and one phone line socket. This allows several cordless telephones to be placed around the house, all operating from the same telephone jack. Additional handsets have a battery charger station that does not plug into the telephone system. Handsets can in many cases be used as intercoms, communicating between each other, and sometimes as walkie-talkies, intercommunicating without telephone line connection.

DECT operates in the 1880–1900 MHz band and defines ten frequency channels from 1881.792 MHz to 1897.344 MHz with a band gap of 1728 kHz.

DECT operates as a multicarrier frequency division multiple access (FDMA) and time division multiple access (TDMA) system. This means that the radio spectrum is divided into physical carriers in two dimensions: frequency and time. FDMA access provides up to 10 frequency channels, and TDMA access provides 24 time slots per every frame of 10ms. DECT uses time division duplex (TDD), which means that down- and uplink use the same frequency but different time slots. Thus a base station provides 12 duplex speech channels in each frame, with each time slot occupying any available channel thus 10 × 12 = 120 carriers are available, each carrying 32 kbit/s.

DECT also provides frequency-hopping spread spectrum over TDMA/TDD structure for ISM band applications. If frequency-hopping is avoided, each base station can provide up to 120 channels in the DECT spectrum before frequency reuse. Each timeslot can be assigned to a different channel in order to exploit advantages of frequency hopping and to avoid interference from other users in asynchronous fashion.

DECT allows interference-free wireless operation to around outdoors, much less indoors when separated by walls. Operates clearly in common congested domestic radio traffic situations, for instance, generally immune to interference from other DECT systems, Wi-Fi networks, video senders, Bluetooth technology, baby monitors and other wireless devices.

ETSI standards documentation ETSI EN 300 175 parts 1–8 (DECT), ETSI EN 300 444 (GAP) and ETSI TS 102 527 parts 1–5 (NG-DECT) prescribe the following technical properties:

The DECT physical layer uses FDMA/TDMA access with TDD.

Gaussian frequency-shift keying (GFSK) modulation is used: the binary one is coded with a frequency increase by 288 kHz, and the binary zero with frequency decrease of 288 kHz. With high quality connections, 2-, 4- or 8-level Differential PSK modulation (DBPSK, DQPSK or D8PSK), which is similar to QAM-2, QAM-4 and QAM-8, can be used to transmit 1, 2, or 3 bits per each symbol. QAM-16 and QAM-64 modulations with 4 and 8 bits per symbol can be used for user data (B-field) only, with resulting transmission speeds of up to 5,068Mbit/s.

DECT provides dynamic channel selection and assignment; the choice of transmission frequency and time slot is always made by the mobile terminal. In case of interference in the selected frequency channel, the mobile terminal (possibly from suggestion by the base station) can initiate either intracell handover, selecting another channel/transmitter on the same base, or intercell handover, selecting a different base station altogether. For this purpose, DECT devices scan all idle channels at regular 30s intervals to generate a received signal strength indication (RSSI) list. When a new channel is required, the mobile terminal (PP) or base station (FP) selects a channel with the minimum interference from the RSSI list.

The maximum allowed power for portable equipment as well as base stations is 250 mW. A portable device radiates an average of about 10 mW during a call as it is only using one of 24 time slots to transmit. In Europe, the power limit was expressed as effective radiated power (ERP), rather than the more commonly used equivalent isotropically radiated power (EIRP), permitting the use of high-gain directional antennas to produce much higher EIRP and hence long ranges.

The DECT media access control layer controls the physical layer and provides connection oriented, connectionless and broadcast services to the higher layers.

The DECT data link layer uses Link Access Protocol Control (LAPC), a specially designed variant of the ISDN data link protocol called LAPD. They are based on HDLC.

GFSK modulation uses a bit rate of 1152 kbit/s, with a frame of 10ms (11520bits) which contains 24 time slots. Each slots contains 480 bits, some of which are reserved for physical packets and the rest is guard space. Slots 0–11 are always used for downlink (FP to PP) and slots 12–23 are used for uplink (PP to FP).

There are several combinations of slots and corresponding types of physical packets with GFSK modulation:

The 420/424 bits of a GFSK basic packet (P32) contain the following fields:

The resulting full data rate is 32 kbit/s, available in both directions.

The DECT network layer always contains the following protocol entities:

Optionally it may also contain others:

All these communicate through a Link Control Entity (LCE).

The call control protocol is derived from ISDN DSS1, which is a Q.931-derived protocol. Many DECT-specific changes have been made.

The mobility management protocol includes the management of identities, authentication, location updating, on-air subscription and key allocation. It includes many elements similar to the GSM protocol, but also includes elements unique to DECT.

Unlike the GSM protocol, the DECT network specifications do not define cross-linkages between the operation of the entities (for example, Mobility Management and Call Control). The architecture presumes that such linkages will be designed into the interworking unit that connects the DECT access network to whatever mobility-enabled fixed network is involved. By keeping the entities separate, the handset is capable of responding to any combination of entity traffic, and this creates great flexibility in fixed network design without breaking full interoperability.

DECT GAP is an interoperability profile for DECT. The intent is that two different products from different manufacturers that both conform not only to the DECT standard, but also to the GAP profile defined within the DECT standard, are able to interoperate for basic calling. The DECT standard includes full testing suites for GAP, and GAP products on the market from different manufacturers are in practice interoperable for the basic functions.

The DECT media access control layer includes authentication of handsets to the base station using the DECT Standard Authentication Algorithm (DSAA). When registering the handset on the base, both record a shared 128-bit Unique Authentication Key (UAK). The base can request authentication by sending two random numbers to the handset, which calculates the response using the shared 128-bit key. The handset can also request authentication by sending a 64-bit random number to the base, which chooses a second random number, calculates the response using the shared key, and sends it back with the second random number.

The standard also provides encryption services with the DECT Standard Cipher (DSC). The encryption is fairly weak, using a 35-bit initialization vector and encrypting the voice stream with 64-bit encryption. While most of the DECT standard is publicly available, the part describing the DECT Standard Cipher was only available under a non-disclosure agreement to the phones' manufacturers from ETSI.

The properties of the DECT protocol make it hard to intercept a frame, modify it and send it later again, as DECT frames are based on time-division multiplexing and need to be transmitted at a specific point in time. Unfortunately very few DECT devices on the market implemented authentication and encryption procedures and even when encryption was used by the phone, it was possible to implement a man-in-the-middle attack impersonating a DECT base station and revert to unencrypted mode which allows calls to be listened to, recorded, and re-routed to a different destination.

After an unverified report of a successful attack in 2002, members of the deDECTed.org project actually did reverse engineer the DECT Standard Cipher in 2008, and as of 2010 there has been a viable attack on it that can recover the key.

In 2012, an improved authentication algorithm, the DECT Standard Authentication Algorithm 2 (DSAA2), and improved version of the encryption algorithm, the DECT Standard Cipher 2 (DSC2), both based on AES 128-bit encryption, were included as optional in the NG-DECT/CAT-iq suite.

DECT Forum also launched the DECT Security certification program which mandates the use of previously optional security features in the GAP profile, such as early encryption and base authentication.

Various access profiles have been defined in the DECT standard:

Other interoperability profiles exist in the DECT suite of standards, and in particular the DPRS (DECT Packet Radio Services) bring together a number of prior interoperability profiles for the use of DECT as a wireless LAN and wireless internet access service. With good range (up to indoors and using directional antennae outdoors), dedicated spectrum, high interference immunity, open interoperability and data speeds of around 500 kbit/s, DECT appeared at one time to be a superior alternative to Wi-Fi. The protocol capabilities built into the DECT networking protocol standards were particularly good at supporting fast roaming in the public space, between hotspots operated by competing but connected providers. The first DECT product to reach the market, Olivetti's Net, was a wireless LAN, and German firms Dosch & Amand and Hoeft & Wessel built niche businesses on the supply of data transmission systems based on DECT.

However, the timing of the availability of DECT, in the mid-1990s, was too early to find wide application for wireless data outside niche industrial applications. Whilst contemporary providers of Wi-Fi struggled with the same issues, providers of DECT retreated to the more immediately lucrative market for cordless telephones. A key weakness was also the inaccessibility of the U.S. market, due to FCC spectrum restrictions at that time. By the time mass applications for wireless Internet had emerged, and the U.S. had opened up to DECT, well into the new century, the industry had moved far ahead in terms of performance and DECT's time as a technically competitive wireless data transport had passed.

DECT uses UHF radio, similar to mobile phones, baby monitors, Wi-Fi, and other cordless telephone technologies. The UK Health Protection Agency (HPA) claims that due to a mobile phone's adaptive power ability, a DECT cordless phone's radiation could actually exceed the radiation of a mobile phone. A DECT cordless phone's radiation has an average output power of 10 mW but is in the form of 100 bursts per second of 250 mW, a strength comparable to some mobile phones.
Most studies have been unable to demonstrate any link to health effects, or have been inconclusive. Electromagnetic fields may have an effect on protein expression in laboratory settings but have not yet been demonstrated to have clinically significant effects in real-world settings. The World Health Organization has issued a statement on medical effects of mobile phones which acknowledges that the longer term effects (over several decades) require further research.





</doc>
<doc id="8676" url="https://en.wikipedia.org/wiki?curid=8676" title="Dhyāna">
Dhyāna

Dhyāna may refer to:




</doc>
<doc id="8677" url="https://en.wikipedia.org/wiki?curid=8677" title="December 30">
December 30






</doc>
<doc id="8678" url="https://en.wikipedia.org/wiki?curid=8678" title="Donn">
Donn

In Irish mythology, Donn ("the dark one", from ) is an ancestor of the Gaels and is believed to have been a god of the dead. Donn is said to dwell in Tech Duinn (the "house of Donn" or "house of the dark one"), where the souls of the dead gather. He may have originally been an aspect of the Dagda. Folklore about Donn survived into the modern era in parts of Ireland, in which he is said to be a phantom horseman riding a white horse.

A 9th-century poem says that Donn's dying wish was that all his descendants would gather at Tech Duinn after death: "To me, to my house, you shall all come after your deaths". The 10th-century tale "Airne Fíngein" ("Fíngen's Vigil") says that Tech Duinn is where the souls of the dead gather. In their translation of "Acallam na Senórach", Ann Dooley and Harry Roe commented that "to go to the House of Donn in Irish tradition means to die". This suggests that the pagan Gaels saw Donn as their ancestor and believed they would go to his abode when they died. Tech Duinn may have been thought of as a place where the souls of the dead gathered before travelling to their final destination in the otherworld, or before being reincarnated. According to Julius Caesar, the Gauls also claimed descent from a god whom he likened to Dīs Pater, the Roman god of the underworld.

The Christian writers who recorded the "Lebor Gabála Érenn" made Donn into Éber Donn one of the mythical Milesian ancestors of the Gaels. The Milesians invade Ireland and take it from the Tuatha Dé Danann. During their invasion, Donn slights Ériu, one of the eponymous goddesses of Ireland, and he drowns in a shipwreck off the southwest coast. Donn is then buried on a rocky island which becomes known as Tech Duinn. In the literature, Tech Duinn is said to lie at or beyond the western edge of Ireland. Tech Duinn is commonly identified with Bull Rock, an islet off the western tip of the Beara Peninsula. Bull Rock resembles a dolmen or portal tomb as it has a natural tunnel through it, allowing the sea to pass under it as if through a portal. In Ireland there was a belief that the souls of the dead departed westwards over the sea with the setting sun.

The "Metrical Dindshenchas" entry for “Tech Duinn” recounts the tale:Through the incantations of the druids a storm came upon them, and the ship wherein Donn was foundered. ‘Let his body be carried to yonder high rock’, says Amairgen: ‘his folk shall come to this spot.’ So hence it is called Tech Duinn: and for this cause, according to the heathen, the souls of sinners visit Tech Duinn before they go to hell, and give their blessing, ere they go, to the soul of Donn. But as for the righteous soul of a penitent, it beholds the place from afar, and is not borne astray. Such, at least, is the belief of the heathen. – Translation by E. Gwynn"

In the tale "Togail Bruidne Dá Derga" ("The Destruction of Dá Derga's Hostel"), king Conaire Mór meets his death in Bruiden Dá Derga (the "great hall or hostel of the red god"). On his way to the hostel, Conaire meets three red men riding red horses from the otherworld. They foretell his doom and tell him "we ride the horses of Donn ... although we are alive, we are dead". Donn is called "king of the dead" in the tale. It has been suggested that Dá Derga and Dá Derga's Hostel is another name for Donn and his abode. It may be a name for the death god in the context of violent death or sacrifice, hence the name "red god".

In the tale "Tochmarc Treblainne" ("The Wooing of Treblann"), the otherworld woman Treblann elopes with the mortal man Fráech, who sends her to safety in Tech Duinn while he embarks on a quest. In this tale, Donn is said to be the son or foster-son of the Dagda. Dáithí Ó hÓgáin notes similarities between the two and suggests that Donn was originally an epithet of the Dagda.

Donn is the father of Diarmuid Ua Duibhne, whom he gives to the god of youth, Aengus mac Óg, to raise.

Folklore about Donn survived into the early modern era. In County Limerick, a Donn Fírinne was said to dwell in the sacred hill of Cnoc Fírinne (Knockfeerina or Knockfierna), and folklore told of people being brought into the hill to be with Donn when they died. He was said to appear as a phantom horseman riding a white horse. He was also associated with the weather: thunder and lightning meant that Donn Fírinne was riding his horse through the sky, and if clouds were over the hill it meant that he was gathering them together to make rain. This imagery may have been influenced by the lore of Odin and his horse Sleipnir from the Norse settlers in Limerick. Donn Fírinne was also said to appear and warn anyone who interfered with his hill. On the west coast of County Clare there was a Donn na Duimhche or Donn Dumhach ("Donn of the dunes"), who "was also often encountered as a night-horseman". In later folklore, the name 'Donn' came to mean an 'otherworld lord' in general.

In modern Irish, "donn" is the word for the colour brown.



</doc>
<doc id="8681" url="https://en.wikipedia.org/wiki?curid=8681" title="Data compression ratio">
Data compression ratio

Data compression ratio, also known as compression power, is a measurement of the relative reduction in size of data representation produced by a data compression algorithm. It is typically expressed as the division of uncompressed size by compressed size.

Data compression ratio is defined as the ratio between the "uncompressed size" and "compressed size":

Thus, a representation that compresses a file's storage size from 10 MB to 2 MB has a compression ratio of 10/2 = 5, often notated as an explicit ratio, 5:1 (read "five" to "one"), or as an implicit ratio, 5/1. This formulation applies equally for compression, where the uncompressed size is that of the original; and for decompression, where the uncompressed size is that of the reproduction. 

Sometimes the "space savings" is given instead, which is defined as the reduction in size relative to the uncompressed size: 

Thus, a representation that compresses the storage size of a file from 10MB to 2MB yields a space savings of 1 - 2/10 = 0.8, often notated as a percentage, 80%.

For signals of indefinite size, such as streaming audio and video, the compression ratio is defined in terms of uncompressed and compressed data rates instead of data sizes: 

and instead of space savings, one speaks of data-rate savings, which is defined as the data-rate reduction relative to the uncompressed data rate: 

For example, uncompressed songs in CD format have a data rate of 16 bits/channel x 2 channels x 44.1 kHz ≅ 1.4 Mbit/s, whereas AAC files on an iPod are typically compressed to 128 kbit/s, yielding a compression ratio of 10.9, for a data-rate savings of 0.91, or 91%. 

When the uncompressed data rate is known, the compression ratio can be inferred from the compressed data rate.

Lossless compression of digitized data such as video, digitized film, and audio preserves all the information, but it does not generally achieve compression ratio much better than 2:1 because of the intrinsic entropy of the data. Compression algorithms which provide higher ratios either incur very large overheads or work only for specific data sequences (e.g. compressing a file with mostly zeros). In contrast, lossy compression (e.g. JPEG for images, or MP3 and Opus for audio) can achieve much higher compression ratios at the cost of a decrease in quality, such as Bluetooth audio streaming, as visual or audio compression artifacts from loss of important information are introduced. A compression ratio of at least 50:1 is needed to get 1080i video into a 20 Mbit/s MPEG transport stream.

The data compression ratio can serve as a measure of the complexity of a data set or signal. In particular it is used to approximate the algorithmic complexity. It is also used to see how much of a file is able to be compressed without increasing its original size.



</doc>
<doc id="8683" url="https://en.wikipedia.org/wiki?curid=8683" title="Disc jockey">
Disc jockey

A disc jockey, more commonly abbreviated as DJ, is a person who plays recorded music for an audience. Most common types of DJs include radio DJs, club DJs, who work at a nightclub or music festival, mobile DJs, who are hired to work at public and private events (weddings, parties, festivals), and turntablists who use record players, usually turntables, to manipulate sounds on phonograph records. Originally, the "disc" in "disc jockey" referred to vinyl records, but nowadays DJ is used as an all-encompassing term to also describe persons who mix music from other recording media such as cassettes, CDs or digital audio files on a CDJ, controller, or even a laptop. DJs may adopt the title "DJ" in front of their real names, adopted pseudonyms, or stage names.

DJs use audio equipment that can play at least two sources of recorded music simultaneously and mix them together to create seamless transitions between recordings and develop unique mixes of songs. Often, this involves aligning the beats of the music sources so their rhythms and tempos do not clash when played together and to enable a smooth transition from one song to another. DJs often use specialized DJ mixers, small audio mixers with crossfader and cue functions to blend or transition from one song to another. Mixers are also used to pre-listen to sources of recorded music in headphones and adjust upcoming tracks to mix with currently playing music. DJ software can be used with a DJ controller device to mix audio files on a computer instead of a console mixer. DJs may also use a microphone to speak to the audience; effects units such as reverb to create sound effects and electronic musical instruments such as drum machines and synthesizers.

Originally, the "disc" in "disc jockey" referred to gramophone records, but now "DJ" is used as an all-encompassing term to describe someone who mixes recorded music from any source, including vinyl records, cassettes, CDs, or digital audio files stored on USB stick or laptop. DJs typically perform for a live audience in a nightclub or dance club or a TV, radio broadcast audience, or an online radio audience. DJs also create mixes, remixes and tracks that are recorded for later sale and distribution. In hip hop music, DJs may create beats, using percussion breaks, basslines and other musical content sampled from pre-existing records. In hip hop, rappers and MCs use these beats to rap over. Some DJs adopt the title "DJ" as part of their names (e.g., DJ Jazzy Jeff, DJ Qbert, DJ Shadow and DJ Yoda). Professional DJs often specialize in a specific genre of music, such as techno, house or hip hop music. DJs typically have an extensive knowledge about the music they specialize in. Many DJs are avid music collectors of vintage, rare or obscure tracks and records.

Radio DJs or radio personalities introduce and play music broadcast on AM, FM, digital or Internet radio stations.

Club DJs, commonly referred as DJs in general, play music at musical events, such as parties at music venues or bars, music festivals, corporate and private events. Typically, club DJs mix music recordings from two or more sources using different mixing techniques in order to produce non-stopping flow of music. 

One key technique used for seamlessly transitioning from one song to another is beatmatching. A DJ who mostly plays and mixes one specific music genre is often given the title of that genre; for example, a DJ who plays hip hop music is called a hip hop DJ, a DJ who plays house music is a house DJ, a DJ who plays techno is called a techno DJ, and so on. 

The quality of a DJ performance (often called a DJ mix or DJ set) consists of two main features: technical skills, or how well can DJ operate the equipment and produce smooth transitions between two or more recordings and a playlist; and the ability of a DJ to select most suitable recordings, also known as "reading the crowd".

DJ Kool Herc, Grandmaster Flash, and Afrika Bambaataa were members of block party at South Bronx. Kool Herc played records such as James Brown's "Give It Up or Turnit a Loose", Jimmy Castor's "It's Just Begun" and Booker T. & the M.G.'s' "Melting Pot". Herc played Incredible Bong Band "Bongo rock", "Apach", and UK rock band Babe Ruth "The Mexican" also. With Bronx clubs struggling with street gangs, uptown DJs catering to an older disco crowd with different aspirations, and commercial radio also catering to a demographic distinct from teenagers in the Bronx, Herc's parties had a ready-made audience.

DJ Kool Herc developed the style that was the blueprint for hip hop music. Herc used the record to focus on a short, heavily percussive part in it: the "break". Since this part of the record was the one the dancers liked best, Herc isolated the break and prolonged it by changing between two record players. As one record reached the end of the break, he cued a second record back to the beginning of the break, which allowed him to extend a relatively short section of music into "five-minute loop of fury". This innovation had its roots in what Herc called "The Merry-Go-Round," a technique by which the deejay switched from break to break at the height of the party. This technique is specifically called "The Merry-Go-Round" because according to Herc, it takes one "back and forth with no slack."

In Jamaican music, a deejay (DJ) is a reggae or dancehall musician who sings and raps ("toasts") to an instrumental (or riddim). U Roy, Big Youth, and I Roy were famous deejays in Jamaica.

Turntablists, also called battle DJs, use turntables and DJ mixer to manipulate recorded sounds in order to produce new music. In essence, they use DJ equipment as a musical instrument. The most known turntablist technique is scratching. Turntablists often participate in DJ contests like DMC World DJ Championships and Red Bull 3Style.

A resident DJ performs at a venue on a regular basis or permanently. They would perform regularly (typically under an agreement) in a particular discotheque, a particular club, a particular event, or a particular broadcasting station. Residents have a decisive influence on the club or a series of events. Per agreement with the management or company, the DJ would have to perform under agreed times and dates. Typically, DJs perform as residents for two or three times in a week, for example, on Friday and Saturday. Also, DJs who make a steady income from a venue, are also considered resident DJs.

Examples for resident DJs are:



DJs use equipment that enables them to play multiple sources of recorded music and mix them to create seamless transitions and unique arrangements of songs. An important tool for DJs is the specialized DJ mixer, a small audio mixer with a crossfader and cue functions. The crossfader enables the DJ to blend or transition from one song to another. The cue knobs or switches allow the DJ to "listen" to a source of recorded music in headphones before playing it for the live club or broadcast audience. Previewing the music in headphones helps the DJ pick the next track they want to play, cue up the track to the desired starting location, and align the two tracks' beats in traditional situations where auto sync technology is not being used. This process ensures that the selected song will mix well with the currently playing music. DJs may align the beats of the music sources so their rhythms do not clash when they are played together to help create a smooth transition from one song to another. Other equipment may include a microphone, effects units such as reverb, and electronic musical instruments such as drum machines and synthesizers.
As music technology has progressed, DJs have adopted different types of equipment to play and mix music, all of which are still commonly used. Traditionally, DJs used two turntables plugged into a DJ mixer to mix music on vinyl records. As compact discs became popular media for publishing music, specialized high quality CD players known as CDJs were developed for DJs. CDJs can take the place of turntables or be used together with turntables. Many CDJs can now play digital music files from USB flash drives or SD cards in addition to CDs. With the spread of portable laptop, tablet, and smartphone computers, DJs began using software together with specialized sound cards and DJ controller hardware. DJ software can be used in conjunction with a hardware DJ mixer or be used instead of a hardware mixer.

Turntables allow DJs to play vinyl records. By adjusting the playback speed of the turntable, either by adjusting the speed knob, or by manipulating the platter (e.g., by slowing down the platter by putting a finger gently along the side), DJs can match the tempos of different records so their rhythms can be played together at the same time without clashing or make a smooth, seamless transition from one song to another. This technique is known as beatmatching. DJs typically replace the rubber mat on turntables that keeps the record moving in sync with the turntable with a slipmat that facilitates manipulating the playback of the record by hand. With the slipmat, the DJ can stop or slow down the record while the turntable is still spinning. Direct-drive turntables are the type preferred by DJs, with the Technics SL-1200 being the most popular model of turntables for DJs. Belt-drive turntables are less expensive, but they are not suitable for turntablism and DJing, because the belt-drive motor does not like being slowed down, as it can stretch out the belt (or in most cases, snap the belt, thus damaging the whole turntable). Some DJs, most commonly those who play hip hop music, go beyond merely mixing records and use turntables as musical instruments for scratching, beat juggling, and other turntablism techniques.

CDJs are high quality digital media players made for DJing. They often have large jog wheels and pitch controls to allow DJs to manipulate the playback of digital files for beatmatching similar to how DJs manipulate vinyl records on turntables. CDJs often have features such as loops and waveform displays similar to DJ software. Originally designed to play music from compact discs, they now can play digital music files stored on USB flash drives and SD cards. Some CDJs can also connect to a computer running DJ software to act as a DJ controller.

DJ mixers are small audio mixing consoles specialized for DJing. Most DJ mixers have far fewer channels than a mixer used by a record producer or audio engineer; whereas standard live sound mixers in small venues have 12 to 24 channels, and standard recording studio mixers have even more (as many as 72 on large boards), basic DJ mixers may have only two channels. While DJ mixers have many of the same features found on larger mixers (faders, equalization knobs, gain knobs, effects units, etc.), DJ mixers have a feature that is usually only found on DJ mixers: the crossfader. The crossfader is a type of fader that is mounted horizontally. DJs used the crossfader to mix two or more sound sources. The midpoint of the crossfader's travel is a 50/50 mix of the two channels (on a two channel mixer). The far left side of the crossfader provides only the channel A sound source. The far right side provides only the channel B sound source (e.g., record player number 2). Positions in between the two extremes provide different mixes of the two channels. Some DJs use a computer with DJ software and a DJ controller instead of an analog DJ mixer to mix music, although DJ software can be used in conjunction with a hardware DJ mixer.

DJs generally use higher quality headphones than those designed for music consumers. DJ headphones have other properties useful for DJs, such as designs that acoustically isolate the sounds of the headphones from the outside environment (hard shell headphones), flexible headbands and pivot joints to allow DJs to listen to one side of the headphones, while turning the other headphone away (so they can monitor the mix in the club), and replaceable cables. Replaceable cables enables DJs to buy new cables if a cable becomes frayed, worn, or damaged, or if a cable is accidentally cut.

Closed-back headphones are highly recommended for DJs to block outside noise as the environment of DJ usually tend to be very noisy. Standard headphones have 3.5mm jack but DJ equipment usually requires ¼ inch jack. Most of specialized DJ Headphones have an adapter to switch between 3.5mm jack and ¼ inch jack. Detachable coiled cables are perfect for DJ Headphones.

DJs have changed their equipment as new technologies are introduced. The earliest DJs in pop music, in 1970s discos, used record turntables, vinyl records and audio consoles. In the 1970s, DJs would have to lug heavy direct drive turntables and crates of records to clubs and shows. In the 1980s, many DJs transitioned to compact cassettes. In the 1990s and 2000s, many DJs switched to using digital audio such as CDs and MP3 files. As technological advances made it practical to store large collections of digital music files on a laptop computer, DJ software was developed so DJs could use a laptop as a source of music instead of transporting CDs or vinyl records to gigs. Unlike most music player software designed for regular consumers, DJ software can play at least two audio files simultaneously, display the waveforms of the files on screen and enable the DJ to listen to either source.

The waveforms allow the DJ see what is coming next in the music and how the playback of different files is aligned. The software analyzes music files to identify their tempo and where the beats are. The analyzed information can be used by the DJ to help manually beatmatch like with vinyl records or the software can automatically synchronize the beats. Digital signal processing algorithms in software allow DJs to adjust the tempo of recordings independently of their pitch (and musical key, a feature known as "keylock". Some software analyzes the loudness of the music for automatic normalization with ReplayGain and detects the musical key. Additionally, DJ software can store cue points, set loops, and apply effects.

As tablet computers and smartphones became widespread, DJ software was written to run on these devices in addition to laptops.
DJ software requires specialized hardware in addition to a computer to fully take advantage of its features. The consumer grade, regular sound card integrated into most computer motherboards can only output two channels (one stereo pair). However, DJs need to be able to output at least four channels (two stereo pairs, thus Left and Right for input 1 and Left and Right for input 2), either unmixed signals to send to a DJ mixer or a main output plus a headphone output. Additionally, DJ sound cards output higher quality signals than the sound cards built into consumer-grade computer motherboards.

Special vinyl records (or CDs/digital files played with CDJs) can be used with DJ software to play digital music files with DJ software as if they were pressed onto vinyl, allowing turntablism techniques to be used with digital files. These vinyl records do not have music recordings pressed on to them. Instead, they are pressed with a special signal, referred to as "timecode", to control DJ software. The DJ software interprets changes in the playback speed, direction, and position of the timecode signal and manipulates the digital files it is playing in the same way that the turntable manipulates the timecode record.

This requires a specialized DJ sound card with at least 4 channels (2 stereo pairs) of inputs and outputs. With this setup, the DJ software typically outputs unmixed signals from the music files to an external hardware DJ mixer. Some DJ mixers have integrated USB sound cards that allow DJ software to connect directly to the mixer without requiring a separate sound card.

A DJ software can be used to mix audio files on the computer instead of a separate hardware mixer. When mixing on a computer, DJs often use a DJ controller device that mimics the layout of two turntables plus a DJ mixer to control the software rather than the computer keyboard & touchpad on a laptop, or the touchscreen on a tablet computer or smartphone. Many DJ controllers have an integrated sound card with 4 output channels (2 stereo pairs) that allows the DJ to use headphones to preview music before playing it on the main output.


Several techniques are used by DJs as a means to better mix and blend recorded music. These techniques primarily include the cueing, equalization and audio mixing of two or more sound sources. The complexity and frequency of special techniques depends largely on the setting in which a DJ is working. Radio DJs are less likely to focus on advanced music-mixing procedures than club DJs, who rely on a smooth transition between songs using a range of techniques. However, some radio DJs are experienced club DJs, so they use the same sophisticated mixing techniques.
Club DJ turntable techniques include beatmatching, phrasing and slip-cueing to preserve energy on a dance floor. Turntablism embodies the art of cutting, beat juggling, scratching, needle drops, phase shifting, back spinning and more to perform the transitions and overdubs of samples in a more creative manner (although turntablism is often considered a use of the turntable as a musical instrument rather than a tool for blending recorded music). Professional DJs may use harmonic mixing to choose songs that are in compatible musical keys.

Recent advances in technology in both DJ hardware and software can provide assisted or automatic completion of some traditional DJ techniques and skills. Examples include phrasing and beatmatching, which can be partially or completely automated by utilizing DJ software that performs automatic synchronization of sound recordings, a feature commonly labelled "sync". Most DJ mixers now include a beat-counter which analyzes the tempo of an incoming sound source and displays its tempo in beats per minute (BPM), which may assist with beatmatching analog sound sources.

In the past, being a DJ has largely been a self-taught craft but with the complexities of new technologies and the convergence with music production methods, there are a growing number of schools and organizations that offer instruction on the techniques.

In DJ culture, miming refers to the practice of DJ's pantomiming the actions of live-mixing a set on stage while a pre-recorded mix plays over the sound system. Miming mixing in a live performance is considered to be controversial within DJ culture. Some within the DJ community say that miming is increasingly used as a technique by celebrity model DJs who may lack mixing skills, but can draw big crowds to a venue.

During a DJ tour for the release of the French group Justice's "A Cross the Universe" in November 2008, controversy arose when a photograph of Augé DJing with an unplugged Akai MPD24 surfaced. The photograph sparked accusations that Justice's live sets were faked. Augé has since said that the equipment was unplugged very briefly before being reattached and the band put a three-photo set of the incident on their MySpace page. After a 2013 Disclosure concert, the duo was criticized for pretending to live mix to a playback of a pre-recorded track. Disclosure's Guy Lawrence said they did not deliberately intend to mislead their audience, and cited miming by other DJs such as David Guetta.

The term "disc jockey" was ostensibly coined by radio gossip commentator Walter Winchell in 1935, and the phrase first appeared in print in a 1941 "Variety" magazine, used to describe radio personalities who introduced phonograph records on the air. Playing recorded music for dancing and parties rose with the mass marketing of home phonographs in the late 19th century. British radio disc jockey Jimmy Savile hosted his first live dance party in 1943 using a single turntable and a makeshift sound system. Four years later, Savile began using two turnables welded together to form a single DJ console. In 1947, the Whiskey A Go-Go opened in Paris as the first discotheque. In the 1960s, Rudy Bozak began making the first DJ mixers, mixing consoles specialized for DJing.

In the late 1960s to early 1970s Jamaican sound system culture, producer and sound system operator (DJ), (Jamaican) King Tubby and producer Lee "Scratch" Perry were pioneers of the genre known as dub music. They experimented with tape-based composition; emphasized repetitive rhythmic structures (often stripped of their harmonic elements); electronically manipulated spatiality; sonically manipulated pre-recorded musical materials from mass media; and remixed music among other innovative techniques. It is widely known that the Jamaican dancehall culture has had and continues to have a significant impact on the American hip hop culture.

DJ turntablism has origins in the invention of direct-drive turntables. Early belt-drive turntables were unsuitable for turntablism and mixing, since they had a slow start-up time, and they were prone to wear-and-tear and breakage, as the belt would break from backspinning or scratching. The first direct-drive turntable was invented by engineer at Matsushita (now Panasonic), based in Osaka, Japan. It eliminated belts, and instead employed a motor to directly drive a platter on which a vinyl record rests. In 1969, Matsushita released it as the SP-10, the first direct-drive turntable on the market, and the first in their influential Technics series of turntables.

In 1972, Technics started making their SL-1200 turntable, which became the most popular turntable for DJs due to its high torque direct drive design. The SL-1200 had a rapid start and its durable direct drive enabled DJs to manipulate the platter, as with scratching techniques. Hip hop DJs began using the Technics SL-1200s as musical instruments to manipulate records with turntablism techniques such as scratching and beat juggling rather than merely mixing records. These techniques were developed in the 1970s by DJ Kool Herc, Grand Wizard Theodore, and Afrika Bambaataa, as they experimented with Technics direct-drive decks, finding that the motor would continue to spin at the correct RPM even if the DJ wiggled the record back and forth on the platter. Although Technics stopped producing the SL-1200 in 2010, they remain the most popular DJ turntable due to their high build quality and durability.

In 1980, Japanese company Roland released the TR-808, an analog rhythm/drum machine, which has unique artificial sounds, such as its booming bass and sharp snare, and a metronome-like rhythm. Yellow Magic Orchestra's use of the instrument in 1980 influenced hip hop pioneer Afrika Bambaataa, after which the TR-808 would be widely adopted by hip hop DJs, with 808 sounds remaining central to hip hop music ever since. The Roland TB-303, a bass synthesizer released in 1981, had a similar impact on electronic dance music genres such as techno and house music, along with Roland's TR-808 and TR-909 drum machines.

In 1982, the Compact Disc (CD) format was released, popularizing digital audio. In 1998, the first MP3 digital audio player, the Eiger Labs MPMan F10, was introduced. In January of that same year at the BeOS Developer Conference, N2IT demonstrated FinalScratch, the first digital DJ system to allow DJs control of MP3 files through special time-coded vinyl records or CDs. While it would take some time for this novel concept to catch on with the "die-hard Vinyl DJs," this would become the first step in the Digital DJ revolution. Manufacturers joined with computer DJing pioneers to offer professional endorsements, the first being Professor Jam (a.k.a. William P. Rader), who went on to develop the industry's first dedicated computer DJ convention and learning program, the "CPS (Computerized Performance System) DJ Summit", to help spread the word about the advantages of this emerging technology.

In 2001, Pioneer DJ began producing the CDJ-1000 CD player, making the use of digital music recordings with traditional DJ techniques practical for the first time. As the 2000s progressed, laptop computers became more powerful and affordable. DJ software, specialized DJ sound cards, and DJ controllers were developed for DJs to use laptops as a source of music rather than turntables or CDJs. In the 2010s, like laptops before them, tablet computers and smartphones became more powerful & affordable. DJ software was written to run on these more portable devices instead of laptops, although laptops remain the more common type of computer for DJing.

In Western popular music, women musicians have achieved great success in singing and songwriting roles, however, there are relatively few women DJs or turntablists. Part of this may stem from a general low percentage of women in audio technology-related jobs. A 2013 "Sound on Sound" article stated that there are "...few women in record production and sound engineering." Ncube states that "[n]inety-five percent of music producers are male, and although there are female producers achieving great things in music, they are less well-known than their male counterparts." The vast majority of students in music technology programs are male. In hip hop music, the low percentage of women DJs and turntablists may stem from the overall male domination of the entire hip hop music industry. Most of the top rappers, MCs, DJs, record producers and music executives are men. There are a small number of high-profile women, but they are rare. 

In 2007 Mark Katz's article "Men, Women, and Turntables: Gender and the DJ Battle," stated that "very few women [do turntablism] battle[s]; the matter has been a topic of conversation among hip-hop DJs for years." In 2010 Rebekah Farrugia states "the male-centricity of EDM culture" contributes to "a marginalisation of women in these [EDM] spaces." While turntablism and broader DJ practices should not be conflated, Katz suggests use or lack of use of the turntable broadly by women across genres and disciplines is impacted upon by what he defines as "male technophilia." Historian Ruth Oldenziel concurs in her writing on engineering with this idea of socialization as a central factor in the lack of engagement with technology. She explains: "an exclusive focus on women's supposed failure to enter the field … is insufficient for understanding how our stereotypical notions have come into being; it tends to put the burden of proof entirely on women and to blame them for their supposedly inadequate socialization, their lack of aspiration, and their want of masculine values. An equally challenging question is why and how boys have come to love things technical, how boys have historically been socialized as technophiles."

Lucy Green has focused on gender in relation to musical performers and creators, and specifically on educational frameworks as they relate to both. She suggests that women's alienation from "areas that have a strong technological tendency such as DJing, sound engineering and producing" are "not necessarily about her dislike of these instruments but relates to the interrupting effect of their dominantly masculine delineations." Despite this, women and girls do increasingly engage in turntable and DJ practices, individually and collectively, and "carve out spaces for themselves in EDM and DJ Culture". A 2015 article cited a number of prominent female DJs: Hannah Wants, Ellen Allien, Miss Kittin, Monika Kruse, Nicole Moudaber, B.Traits, Magda, Nina Kraviz, Nervo, and Annie Mac. Two years later, another article brings out a list with world-famous female DJs including Nastia, tINY, Nora En Pure, Anja Schneider, Peggy Gou, Maya Jane Coles, and Eli & Fur.

Female DJ The Black Madonna has been called "one of the world’s most exciting turntablists." Her stage name The Black Madonna is a tribute to her mother's favorite Catholic saint. In 2018, The Black Madonna played herself as an in-residence DJ for the video game "Grand Theft Auto Online", as part of the "After Hours" DLC.

There are various projects dedicated to the promotion and support of these practices such as Female DJs London. Some artists and collectives go beyond these practices to be more gender inclusive. For example, Discwoman, a New York-based collective and booking agency, describe themselves as "representing and showcasing cis women, trans women and genderqueer talent."





</doc>
<doc id="8687" url="https://en.wikipedia.org/wiki?curid=8687" title="Detroit">
Detroit

Detroit (, ; ) is the largest and most populous city in the U.S. state of Michigan, the largest U.S. city on the United States–Canada border, and the seat of Wayne County. The municipality of Detroit had a 2019 estimated population of 670,031, making it the 24th-most populous city in the United States. The metropolitan area, known as Metro Detroit, is home to 4.3 million people, making it the second-largest in the Midwest after the Chicago metropolitan area, and 14th largest in the United States. Regarded as a major cultural center, Detroit is known for its contributions to music and as a repository for art, architecture and design.

Detroit is a major port on the Detroit River, one of the four major straits that connect the Great Lakes system to the Saint Lawrence Seaway. The Detroit Metropolitan Airport is among the most important hubs in the United States. The City of Detroit anchors the second-largest regional economy in the Midwest, behind Chicago and ahead of Minneapolis–Saint Paul, and the 13th-largest in the United States. Detroit and its neighboring Canadian city Windsor are connected through a highway tunnel, railway tunnel, and the Ambassador Bridge, which is the second busiest international crossing in North America, after San Diego–Tijuana. Detroit is best known as the center of the U.S. automobile industry, and the "Big Three" auto manufacturers General Motors, Ford, and Fiat Chrysler are all headquartered in Metro Detroit.

In 1701, Antoine de la Mothe Cadillac founded Fort Pontchartrain du Détroit, the future city of Detroit. During the 19th century, it became an important industrial hub at the center of the Great Lakes region. The city became the 4th-largest in the nation in 1920, after only New York City, Chicago and Philadelphia with the influence of the booming auto industry. With expansion of the auto industry in the early 20th century, the city and its suburbs experienced rapid growth, and by the 1940s, the city remained as the fourth-largest in the country. However, due to industrial restructuring, the loss of jobs in the auto industry, and rapid suburbanization, Detroit lost considerable population from the late 20th century to the present. Since reaching a peak of 1.85 million at the 1950 census, Detroit's population has declined by more than 60 percent. In 2013, Detroit became the largest U.S. city to file for bankruptcy, which it successfully exited in December 2014, when the city government regained control of Detroit's finances.

Detroit's diverse culture has had both local and international influence, particularly in music, with the city giving rise to the genres of Motown and techno, and playing an important role in the development of jazz, hip-hop, rock, and punk music. The rapid growth of Detroit in its boom years resulted in a globally unique stock of architectural monuments and historic places. Since the 2000s conservation efforts have managed to save many architectural pieces and achieved several large-scale revitalizations, including the restoration of several historic theatres and entertainment venues, high-rise renovations, new sports stadiums, and a riverfront revitalization project. More recently, the population of Downtown Detroit, Midtown Detroit, and various other neighborhoods has increased. An increasingly popular tourist destination, Detroit receives 19 million visitors per year. In 2015, Detroit was named a "City of Design" by UNESCO, the first U.S. city to receive that designation.

Paleo-Indian people inhabited areas near Detroit as early as 11,000 years ago including the culture referred to as the Mound-builders. In the 17th century, the region was inhabited by Huron, Odawa, Potawatomi and Iroquois peoples.

The first Europeans did not penetrate into the region and reach the straits of Detroit until French missionaries and traders worked their way around the League of the Iroquois, with whom they were at war, and other Iroquoian tribes in the 1630s. The Huron and Neutral peoples held the north side of Lake Erie until the 1650s, when the Iroquois pushed both and the Erie people away from the lake and its beaver-rich feeder streams in the Beaver Wars of 1649–1655. By the 1670s, the war-weakened Iroquois laid claim to as far south as the Ohio River valley in northern Kentucky as hunting grounds, and had absorbed many other Iroquoian peoples after defeating them in war. For the next hundred years, virtually no British, colonist, or French action was contemplated without consultation with, or consideration of the Iroquois' likely response. When the French and Indian War evicted the Kingdom of France from Canada, it removed one barrier to British colonists migrating west. 
British negotiations with the Iroquois would both prove critical and lead to a Crown policy limiting settlements below the Great Lakes and west of the Alleghenies. Many colonial American would-be migrants resented this restraint and became supporters of the American Revolution. The 1778 raids and resultant 1779 decisive Sullivan Expedition reopened the Ohio Country to westward emigration, which began almost immediately. By 1800 white settlers were pouring westwards.

The city was named by French colonists, referring to the Detroit River (, meaning "the strait of Lake Erie"), linking Lake Huron and Lake Erie; in the historical context, the strait included the St. Clair River, Lake St. Clair and the Detroit River.

On July 24, 1701, the French explorer Antoine de la Mothe Cadillac, along with more than a hundred other settlers, began constructing a small fort on the north bank of the Detroit River. Cadillac would later name the settlement Fort Pontchartrain du Détroit, after Louis Phélypeaux, comte de Pontchartrain, Minister of Marine under Louis XIV. A church was soon founded here, and the parish was known as Sainte Anne de Détroit. France offered free land to colonists to attract families to Detroit; when it reached a population of 800 in 1765, this was the largest European settlement between Montreal and New Orleans, both also French settlements, in the former colonies of New France and La Louisiane, respectively.

By 1773, after the addition of Anglo-American settlers, the population of Detroit was 1,400. By 1778, its population reached 2,144 and it was the third-largest city in what was known as the Province of Quebec since the British takeover of French colonies following their victory in the Seven Years' War.

The region's economy was based on the lucrative fur trade, in which numerous Native American people had important roles as trappers and traders. Today the flag of Detroit reflects its French colonial heritage. Descendants of the earliest French and French-Canadian settlers formed a cohesive community, who gradually were superseded as the dominant population after more Anglo-American settlers arrived in the early 19th century with American westward migration. Living along the shores of Lakes St. Clair, and south to Monroe and downriver suburbs, the ethnic French Canadians of Detroit, also known as Muskrat French in reference to the fur trade, remain a subculture in the region in the 21st century.

During the French and Indian War (1754–63), the North American front of the Seven Years' War between Britain and France, British troops gained control of the settlement in 1760, and shortened its name to "Detroit". Several regional Native American tribes, such as the Potowatomi, Ojibwe and Huron, launched Pontiac's Rebellion (1763), and conducted a siege of Fort Detroit, but failed to capture it. In defeat, France ceded its territory in North America east of the Mississippi to Britain following the war.

Following the American Revolutionary War and United States independence, Britain ceded Detroit along with other territory in the area under the Jay Treaty (1796), which established the northern border with its colony of Canada. In 1805, fire destroyed most of the Detroit settlement, which had primarily buildings made of wood. One stone fort, a river warehouse, and brick chimneys of former wooden homes were the sole structures to survive. Of the 600 Detroit residents in this area, none died in the fire.

From 1805 to 1847, Detroit was the capital of Michigan (first the territory, then the state). William Hull, the United States commander at Detroit surrendered without a fight to British troops and their Native American allies during the War of 1812 in the Siege of Detroit, believing his forces were vastly outnumbered. The Battle of Frenchtown (January 18–23, 1813) was part of a U.S. effort to retake the city, and U.S. troops suffered their highest fatalities of any battle in the war. This battle is commemorated at River Raisin National Battlefield Park south of Detroit in Monroe County. Detroit was recaptured by the United States later that year.

The settlement was incorporated as a city in 1815. As the city expanded, a geometric street plan developed by Augustus B. Woodward was followed, featuring grand boulevards as in Paris.

Prior to the American Civil War, the city's access to the Canada–US border made it a key stop for refugee slaves gaining freedom in the North along the Underground Railroad. Many went across the Detroit River to Canada to escape pursuit by slave catchers. An estimated 20,000 to 30,000 African-American refugees settled in Canada. George DeBaptiste was considered to be the "president" of the Detroit Underground Railroad, William Lambert the "vice president" or "secretary", and Laura Haviland the "superintendent".

Numerous men from Detroit volunteered to fight for the Union during the American Civil War, including the 24th Michigan Infantry Regiment. It was part of the legendary Iron Brigade, which fought with distinction and suffered 82% casualties at the Battle of Gettysburg in 1863. When the First Volunteer Infantry Regiment arrived to fortify Washington, D.C., President Abraham Lincoln is quoted as saying "Thank God for Michigan!" George Armstrong Custer led the Michigan Brigade during the Civil War and called them the "Wolverines".

During the late 19th century, wealthy industry and shipping magnates commissioned design and construction of several Gilded Age mansions east and west of the current downtown, along the major avenues of the Woodward plan. Most notable among them was the David Whitney House at 4421 Woodward Avenue, and the grand avenue became a favored address for mansions. During this period some referred to Detroit as the "Paris of the West" for its architecture, grand avenues in the Paris style, and for Washington Boulevard, recently electrified by Thomas Edison. The city had grown steadily from the 1830s with the rise of shipping, shipbuilding, and manufacturing industries. Strategically located along the Great Lakes waterway, Detroit emerged as a major port and transportation hub.

In 1896, a thriving carriage trade prompted Henry Ford to build his first automobile in a rented workshop on Mack Avenue. During this growth period, Detroit expanded its borders by annexing all or part of several surrounding villages and townships.

In 1903, Henry Ford founded the Ford Motor Company. Ford's manufacturing—and those of automotive pioneers William C. Durant, the Dodge Brothers, Packard, and Walter Chrysler—established Detroit's status in the early 20th century as the world's automotive capital. The growth of the auto industry was reflected by changes in businesses throughout the Midwest and nation, with the development of garages to service vehicles and gas stations, as well as factories for parts and tires.

With the rapid growth of industrial workers in the auto factories, labor unions such as the American Federation of Labor and the United Auto Workers fought to organize workers to gain them better working conditions and wages. They initiated strikes and other tactics in support of improvements such as the 8-hour day/40-hour work week, increased wages, greater benefits and improved working conditions. The labor activism during those years increased influence of union leaders in the city such as Jimmy Hoffa of the Teamsters and Walter Reuther of the Autoworkers.

Due to the booming auto industry, Detroit became the 4th-largest in the nation in 1920, following New York City, Chicago and Philadelphia.

The prohibition of alcohol from 1920 to 1933 resulted in the Detroit River becoming a major conduit for smuggling of illegal Canadian spirits.

Detroit, like many places in the United States, developed racial conflict and discrimination in the 20th century following the rapid demographic changes as hundreds of thousands of new workers were attracted to the industrial city; in a short period it became the 4th-largest city in the nation. The Great Migration brought rural blacks from the South; they were outnumbered by southern whites who also migrated to the city. Immigration brought southern and eastern Europeans of Catholic and Jewish faith; these new groups competed with native-born whites for jobs and housing in the booming city.

Detroit was one of the major Midwest cities that was a site for the dramatic urban revival of the Ku Klux Klan beginning in 1915. "By the 1920s the city had become a stronghold of the KKK," whose members primarily opposed Catholic and Jewish immigrants, but also practiced discrimination against black Americans. Even after the decline of the KKK in the late 1920s, the Black Legion, a secret vigilante group, was active in the Detroit area in the 1930s. One-third of its estimated 20,000 to 30,000 members in Michigan were based in the city. It was defeated after numerous prosecutions following the kidnapping and murder in 1936 of Charles Poole, a Catholic organizer with the federal Works Progress Administration. Some 49 men of the Black Legion were convicted of numerous crimes, with many sentenced to life in prison for murder.

In the 1940s the world's "first urban depressed freeway" ever built, the Davison, was constructed in Detroit. During World War II, the government encouraged retooling of the American automobile industry in support of the Allied powers, leading to Detroit's key role in the American Arsenal of Democracy.

Jobs expanded so rapidly due to the defense buildup in World War II that 400,000 people migrated to the city from 1941 to 1943, including 50,000 blacks in the second wave of the Great Migration, and 350,000 whites, many of them from the South. Whites, including ethnic Europeans, feared black competition for jobs and scarce housing. The federal government prohibited discrimination in defense work, but when in June 1943 Packard promoted three black people to work next to whites on its assembly lines, 25,000 white workers walked off the job.

The Detroit race riot of 1943 took place in June, three weeks after the Packard plant protest, beginning with an altercation at Belle Isle. Blacks suffered 25 deaths (of a total of 34), three quarters of 600 wounded, and most of the losses due to property damage. Rioters moved through the city, and young whites traveled across town to attack more settled blacks in their neighborhood of Paradise Valley.

Industrial mergers in the 1950s, especially in the automobile sector, increased oligopoly in the American auto industry. Detroit manufacturers such as Packard and Hudson merged into other companies and eventually disappeared. At its peak population of 1,849,568, in the 1950 Census, the city was the 5th-largest in the United States, after New York City, Chicago, Philadelphia and Los Angeles.

In this postwar era, the auto industry continued to create opportunities for many African Americans from the South, who continued with their Great Migration to Detroit and other northern and western cities to escape the strict Jim Crow laws and racial discrimination policies of the South. As before the war, competition had been fierce for employment, housing, and land. Racial discrimination took place in employment, keeping the work force and better jobs predominantly white. These unequal opportunities in employment resulted in unequal housing opportunities for the majority of the black community. Despite changes in demographics, for instance, Detroit's police force, fire department, and other city jobs were held by predominately white residents.

The surge in Detroit's black population with the Great Migration augmented the strain on housing scarcity. Black people were often turned away from bank loans to obtain housing and interest rates, and rents were unfairly inflated to prevent their moving into white neighborhoods. Such discrimination also took place due to redlining by banks and federal housing policy, which limited the ability of blacks to improve their housing, and encouraged white people to guard the racial divide that defined their neighborhoods. This marginalized the agency of black Detroiters—another important aspect in the history of postwar Detroit.

As in other major American cities in the postwar era, construction of a federally subsidized, extensive highway and freeway system around Detroit, and pent-up demand for new housing stimulated suburbanization; highways made commuting by car easier. However, this construction had negative implications for many urban residents. Highways were constructed through neighborhoods of poor and minority residents who had less political power to oppose them. (The neighborhoods were mostly low income or considered blighted, made up of older housing, where investment had been lacking due to racial redlining, so the highways were presented as a kind of urban renewal.) They displaced residents with little consideration of the effects of breaking up functioning neighborhoods.

In 1956, Detroit's last heavily used electric streetcar line, which traveled along the length of Woodward Avenue, was removed and replaced with gas-powered buses. It was the last line of what had once been a 534-mile network of electric streetcars. In 1941 at peak times, a streetcar ran on Woodward Avenue every 60 seconds.

All of these changes in the area's transportation system favored low-density, auto-oriented development rather than high-density urban development. Industry also moved to the suburbs, seeking large plots of land for single story factories. By the 21st century, the metro Detroit area had developed as one of the most sprawling job markets in the United States; combined with poor public transport, this resulted in many new jobs being beyond the reach of urban low-income workers.
In 1950, the city held about one-third of the state's population, anchored by its industries and workers. Over the next sixty years, the city's population declined to less than 10 percent of the state's population. During the same time period, the sprawling Detroit metropolitan area, which surrounds and includes the city, grew to contain more than half of Michigan's population. The shift of population and jobs eroded Detroit's tax base.

In June 1963, Rev. Martin Luther King Jr. gave a major speech as part of a civil rights march in Detroit that foreshadowed his "I Have a Dream" speech in Washington, D.C., two months later. While the civil rights movement gained significant federal civil rights laws in 1964 and 1965, longstanding inequities resulted in confrontations between the police and inner city black youth who wanted change.

Longstanding tensions in Detroit culminated in the Twelfth Street riot in July 1967. Governor George W. Romney ordered the Michigan National Guard into Detroit, and President Johnson sent in U.S. Army troops. The result was 43 dead, 467 injured, over 7,200 arrests, and more than 2,000 buildings destroyed, mostly in black residential and business areas. Thousands of small businesses closed permanently or relocated to safer neighborhoods. The affected district lay in ruins for decades. It was the most costly riot in the United States.

On August 18, 1970, the NAACP filed suit against Michigan state officials, including Governor William Milliken, charging "de facto" public school segregation. The NAACP argued that although schools were not legally segregated, the city of Detroit and its surrounding counties had enacted policies to maintain racial segregation in public schools. The NAACP also suggested a direct relationship between unfair housing practices and educational segregation, as the composition of students in the schools followed segregated neighborhoods. The District Court held all levels of government accountable for the segregation in its ruling. The Sixth Circuit Court affirmed some of the decision, holding that it was the state's responsibility to integrate across the segregated metropolitan area. The U.S. Supreme Court took up the case February 27, 1974. The subsequent "Milliken v. Bradley" decision had nationwide influence. In a narrow decision, the US Supreme Court found schools were a subject of local control, and suburbs could not be forced to solve problems in the city's school district.

"Milliken was perhaps the greatest missed opportunity of that period," said Myron Orfield, professor of law at the University of Minnesota. "Had that gone the other way, it would have opened the door to fixing nearly all of Detroit's current problems." John Mogk, a professor of law and an expert in urban planning at Wayne State University in Detroit, says, 
"Everybody thinks that it was the riots [in 1967] that caused the white families to leave. Some people were leaving at that time but, really, it was after Milliken that you saw mass flight to the suburbs. If the case had gone the other way, it is likely that Detroit would not have experienced the steep decline in its tax base that has occurred since then."

In November 1973, the city elected Coleman Young as its first black mayor. After taking office, Young emphasized increasing racial diversity in the police department, which was predominately white. Young also worked to improve Detroit's transportation system, but tension between Young and his suburban counterparts over regional matters was problematic throughout his mayoral term. In 1976, the federal government offered $600 million for building a regional rapid transit system, under a single regional authority. But the inability of Detroit and its suburban neighbors to solve conflicts over transit planning resulted in the region losing the majority of funding for rapid transit.

Following the failure to reach a regional agreement over the larger system, the city moved forward with construction of the elevated downtown circulator portion of the system, which became known as the Detroit People Mover.

The gasoline crises of 1973 and 1979 also affected Detroit and the U.S. auto industry. Buyers chose smaller, more fuel-efficient cars made by foreign makers as the price of gas rose. Efforts to revive the city were stymied by the struggles of the auto industry, as their sales and market share declined. Automakers laid off thousands of employees and closed plants in the city, further eroding the tax base. To counteract this, the city used eminent domain to build two large new auto assembly plants in the city.

As mayor, Young sought to revive the city by seeking to increase investment in the city's declining downtown. The Renaissance Center, a mixed-use office and retail complex, opened in 1977. This group of skyscrapers was an attempt to keep businesses in downtown. Young also gave city support to other large developments to attract middle and upper-class residents back to the city. Despite the Renaissance Center and other projects, the downtown area continued to lose businesses to the automobile-dependent suburbs. Major stores and hotels closed, and many large office buildings went vacant. Young was criticized for being too focused on downtown development and not doing enough to lower the city's high crime rate and improve city services to residents.

Previously a major population center and site of worldwide automobile manufacturing, Detroit has suffered a long economic decline produced by numerous factors. Like many industrial American cities, Detroit peak population was in 1950, before postwar suburbanization took effect. The peak population was 1.8 million people.

Following suburbanization, industrial restructuring, and loss of jobs (as described above), by the 2010 census, the city had less than 40 percent of that number, with just over 700,000 residents. The city has declined in population in each census since 1950.

High unemployment was compounded by middle-class flight to the suburbs, and some residents leaving the state to find work. The result for the city was a higher proportion of poor in its population, reduced tax base, depressed property values, abandoned buildings, abandoned neighborhoods, high crime rates, and a pronounced demographic imbalance.

On August 16, 1987, Northwest Airlines Flight 255 crashed near Detroit, killing all but one of the 155 people on board, as well as two people on the ground.
From 1982 to 1988, Detroit held the Detroit Grand Prix, at the Detroit street circuit, The original circuit had seventeen corners in . The winners, by year are: John Watson, Michele Alboreto, Nelson Piquet, Keke Rosberg, Ayrton Senna, Ayrton Senna, Ayrton Senna.

In 1993 Young retired as Detroit's longest-serving mayor, deciding not to seek a sixth term. That year the city elected Dennis Archer, a former Michigan Supreme Court justice. Archer prioritized downtown development and easing tensions with Detroit's suburban neighbors. A referendum to allow casino gambling in the city passed in 1996; several temporary casino facilities opened in 1999, and permanent downtown casinos with hotels opened in 2007–08.

Campus Martius, a reconfiguration of downtown's main intersection as a new park, was opened in 2004. The park has been cited as one of the best public spaces in the United States. The city's riverfront on the Detroit River has been the focus of redevelopment, following successful examples of other older industrial cities. In 2001, the first portion of the International Riverfront was completed as a part of the city's 300th anniversary celebration. Miles of associated parks and landscaping have been completed in succeeding years. In 2011, the Port Authority Passenger Terminal opened, with the riverwalk connecting Hart Plaza to the Renaissance Center.

Since 2006, $9 billion has been invested in downtown and surrounding neighborhoods; $5.2 billion of which has come in 2013 and 2014. Construction activity, particularly rehabilitation of historic downtown buildings, has increased markedly. The number of vacant downtown buildings has dropped from nearly 50 to around 13. Among the most notable redevelopment projects are the Book Cadillac Hotel and Fort Shelby Hotel; the David Broderick Tower; and the David Whitney Building.

Little Caesars Arena, a new home for the Detroit Red Wings and the Detroit Pistons, with attached residential, hotel, and retail use, opened on September 5, 2017. The plans for the project call for mixed-use residential on the blocks surrounding the arena and the renovation of the vacant 14-story Eddystone Hotel. It will be a part of The District Detroit, a group of places owned by Olympia Entertainment Inc., including Comerica Park and the Detroit Opera House, among others.

Detroit's protracted decline has resulted in severe urban decay, with thousands of empty buildings around the city, referred to as greyfield. Some parts of Detroit are so sparsely populated the city has difficulty providing municipal services. The city has demolished abandoned homes and buildings, planting grass and trees, and considered removing street lighting from large portions of the city, in order to encourage the small population in certain areas to move to more populated areas. Roughly half of the owners of Detroit's 305,000 properties failed to pay their 2011 tax bills, resulting in about $246 million in taxes and fees going uncollected, nearly half of which was due to Detroit. The rest of the money would have been earmarked for Wayne County, Detroit Public Schools, and the library system.

In September 2008, Mayor Kwame Kilpatrick (who had served for six years) resigned following felony convictions. In 2013, Kilpatrick was convicted on 24 federal felony counts, including mail fraud, wire fraud, and racketeering, and was sentenced to 28 years in federal prison. The former mayor's activities cost the city an estimated $20 million. In 2013, felony bribery charges were brought against seven building inspectors. In 2016, further corruption charges were brought against 12 principals, a former school superintendent and supply vendor for a $12 million kickback scheme. Law professor Peter Henning argues Detroit's corruption is not unusual for a city its size, especially when compared with Chicago.

The city's financial crisis resulted in Michigan taking over administrative control of its government. The state governor declared a financial emergency in March 2013, appointing Kevyn Orr as emergency manager. On July 18, 2013, Detroit became the largest U.S. city to file for bankruptcy. It was declared bankrupt by U.S. District Court on December 3, 2013, in light of the city's $18.5 billion debt and its inability to fully repay its thousands of creditors. On November 7, 2014, the city's plan for exiting bankruptcy was approved. The following month, on December 11, the city officially exited bankruptcy. The plan allowed the city to eliminate $7 billion in debt and invest $1.7 billion into improved city services.

One of the largest post-bankruptcy efforts to improve city services has been work to fix the city's broken street lighting system. At one time it was estimated that 40% of lights were not working, which resulted in public safety issues and abandonment of housing. The plan called for replacing outdated high pressure sodium lights with 65,000 LED lights. Construction began in late 2014 and finished in December 2016; Detroit is the largest U.S city with all LED street lighting.

In the 2010s, several initiatives were taken by Detroit's citizens and new residents to improve the cityscape by renovating and revitalizing neighborhoods. Such projects include volunteer renovation groups and various urban gardening movements.

The well-known symbol of the city's decades-long demise, the Michigan Central Station, was long vacant. The city renovated it with new windows, elevators and facilities since 2015. Several other landmark buildings have been privately renovated and adapted as condominiums, hotels, offices, or for cultural uses. Detroit is mentioned as a city of renaissance and has reversed many of the trends of the prior decades.

Detroit is the center of a three-county urban area (with a population of 3,734,090 within an area of according to the 2010 United States Census), six-county metropolitan statistical area (population of 4,296,250 in an area of as of the 2010 census), and a nine-county combined statistical area (population of 5.3 million within ).

According to the U.S. Census Bureau, the city has a total area of , of which is land and is water. Detroit is the principal city in Metro Detroit and Southeast Michigan. It is situated in the Midwestern United States and the Great Lakes region.

The Detroit River International Wildlife Refuge is the only international wildlife preserve in North America, and is uniquely located in the heart of a major metropolitan area. The Refuge includes islands, coastal wetlands, marshes, shoals, and waterfront lands along of the Detroit River and Western Lake Erie shoreline.

The city slopes gently from the northwest to southeast on a till plain composed largely of glacial and lake clay. The most notable topographical feature in the city is the Detroit Moraine, a broad clay ridge on which the older portions of Detroit and Windsor are located, rising approximately above the river at its highest point. The highest elevation in the city is directly north of Gorham Playground on the northwest side approximately three blocks south of 8 Mile Road, at a height of . Detroit's lowest elevation is along the Detroit River, at a surface height of .
Belle Isle Park is a island park in the Detroit River, between Detroit and Windsor, Ontario. It is connected to the mainland by the MacArthur Bridge in Detroit. Belle Isle Park contains such attractions as the James Scott Memorial Fountain, the Belle Isle Conservatory, the Detroit Yacht Club on an adjacent island, a half-mile (800 m) beach, a golf course, a nature center, monuments, and gardens. The city skyline may be viewed from the island.

Three road systems cross the city: the original French template, with avenues radiating from the waterfront, and true north–south roads based on the Northwest Ordinance township system. The city is north of Windsor, Ontario. Detroit is the only major city along the Canada–U.S. border in which one travels south in order to cross into Canada.

Detroit has four border crossings: the Ambassador Bridge and the Detroit–Windsor Tunnel provide motor vehicle thoroughfares, with the Michigan Central Railway Tunnel providing railroad access to and from Canada. The fourth border crossing is the Detroit–Windsor Truck Ferry, near the Windsor Salt Mine and Zug Island. Near Zug Island, the southwest part of the city was developed over a salt mine that is below the surface. The Detroit salt mine run by the Detroit Salt Company has over of roads within.

Detroit and the rest of southeastern Michigan have a hot-summer humid continental climate (Köppen: "Dfa") which is influenced by the Great Lakes like other places in the state; the city and close-in suburbs are part of USDA Hardiness zone 6b, while the more distant northern and western suburbs generally are included in zone 6a. Winters are cold, with moderate snowfall and temperatures not rising above freezing on an average 44 days annually, while dropping to or below on an average 4.4 days a year; summers are warm to hot with temperatures exceeding on 12 days. The warm season runs from May to September. The monthly daily mean temperature ranges from in January to in July. Official temperature extremes range from on July 24, 1934, down to on January 21, 1984; the record low maximum is on January 19, 1994, while, conversely the record high minimum is on August 1, 2006, the most recent of five occurrences. A decade or two may pass between readings of or higher, which last occurred July 17, 2012. The average window for freezing temperatures is October 20 thru April 22, allowing a growing season of 180 days.

Precipitation is moderate and somewhat evenly distributed throughout the year, although the warmer months such as May and June average more, averaging annually, but historically ranging from in 1963 to in 2011. Snowfall, which typically falls in measurable amounts between November 15 through April 4 (occasionally in October and very rarely in May), averages per season, although historically ranging from in 1881–82 to in 2013–14. A thick snowpack is not often seen, with an average of only 27.5 days with or more of snow cover. Thunderstorms are frequent in the Detroit area. These usually occur during spring and summer.

Seen in panorama, Detroit's waterfront shows a variety of architectural styles. The post modern Neo-Gothic spires of the One Detroit Center (1993) were designed to refer to the city's Art Deco skyscrapers. Together with the Renaissance Center, these buildings form a distinctive and recognizable skyline. Examples of the Art Deco style include the Guardian Building and Penobscot Building downtown, as well as the Fisher Building and Cadillac Place in the New Center area near Wayne State University. Among the city's prominent structures are United States' largest Fox Theatre, the Detroit Opera House, and the Detroit Institute of Arts, all built in the early 20th century.

While the Downtown and New Center areas contain high-rise buildings, the majority of the surrounding city consists of low-rise structures and single-family homes. Outside of the city's core, residential high-rises are found in upper-class neighborhoods such as the East Riverfront, extending toward Grosse Pointe, and the Palmer Park neighborhood just west of Woodward. The University Commons-Palmer Park district in northwest Detroit, near the University of Detroit Mercy and Marygrove College, anchors historic neighborhoods including Palmer Woods, Sherwood Forest, and the University District.

Forty-two significant structures or sites are listed on the National Register of Historic Places. Neighborhoods constructed prior to World War II feature the architecture of the times, with wood-frame and brick houses in the working-class neighborhoods, larger brick homes in middle-class neighborhoods, and ornate mansions in upper-class neighborhoods such as Brush Park, Woodbridge, Indian Village, Palmer Woods, Boston-Edison, and others.

Some of the oldest neighborhoods are along the major Woodward and East Jefferson corridors, which formed spines of the city. Some newer residential construction may also be found along the Woodward corridor, and in the far west and northeast. The oldest extant neighborhoods include West Canfield and Brush Park. There have been multi-million dollar restorations of existing homes and construction of new homes and condominiums here.

The city has one of United States' largest surviving collections of late 19th- and early 20th-century buildings. Architecturally significant churches and cathedrals in the city include St. Joseph's, Old St. Mary's, the Sweetest Heart of Mary, and the Cathedral of the Most Blessed Sacrament.

The city has substantial activity in urban design, historic preservation, and architecture. A number of downtown redevelopment projects—of which Campus Martius Park is one of the most notable—have revitalized parts of the city. Grand Circus Park and historic district is near the city's theater district; Ford Field, home of the Detroit Lions, and Comerica Park, home of the Detroit Tigers. Other projects include the demolition of the Ford Auditorium off Jefferson Street.

The Detroit International Riverfront includes a partially completed three-and-one-half mile riverfront promenade with a combination of parks, residential buildings, and commercial areas. It extends from Hart Plaza to the MacArthur Bridge, which connects to Belle Isle Park, the largest island park in a U.S. city. The riverfront includes Tri-Centennial State Park and Harbor, Michigan's first urban state park. The second phase is a extension from Hart Plaza to the Ambassador Bridge for a total of of parkway from bridge to bridge. Civic planners envision the pedestrian parks will stimulate residential redevelopment of riverfront properties condemned under eminent domain.

Other major parks include River Rouge (in the southwest side), the largest park in Detroit; Palmer (north of Highland Park) and Chene Park (on the east river downtown).

Detroit has a variety of neighborhood types. The revitalized Downtown, Midtown, and New Center areas feature many historic buildings and are high density, while further out, particularly in the northeast and on the fringes, high vacancy levels are problematic, for which a number of solutions have been proposed. In 2007, Downtown Detroit was recognized as the best city neighborhood in which to retire among the United States' largest metro areas by CNN Money Magazine editors.

Lafayette Park is a revitalized neighborhood on the city's east side, part of the Ludwig Mies van der Rohe residential district. The development was originally called the Gratiot Park. Planned by Mies van der Rohe, Ludwig Hilberseimer and Alfred Caldwell it includes a landscaped, park with no through traffic, in which these and other low-rise apartment buildings are situated. Immigrants have contributed to the city's neighborhood revitalization, especially in southwest Detroit. Southwest Detroit has experienced a thriving economy in recent years, as evidenced by new housing, increased business openings and the recently opened Mexicantown International Welcome Center.

The city has numerous neighborhoods consisting of vacant properties resulting in low inhabited density in those areas, stretching city services and infrastructure. These neighborhoods are concentrated in the northeast and on the city's fringes. A 2009 parcel survey found about a quarter of residential lots in the city to be undeveloped or vacant, and about 10% of the city's housing to be unoccupied. The survey also reported that most (86%) of the city's homes are in good condition with a minority (9%) in fair condition needing only minor repairs.

To deal with vacancy issues, the city has begun demolishing the derelict houses, razing 3,000 of the total 10,000 in 2010, but the resulting low density creates a strain on the city's infrastructure. To remedy this, a number of solutions have been proposed including resident relocation from more sparsely populated neighborhoods and converting unused space to urban agricultural use, including Hantz Woodlands, though the city expects to be in the planning stages for up to another two years.

Public funding and private investment have also been made with promises to rehabilitate neighborhoods. In April 2008, the city announced a $300-million stimulus plan to create jobs and revitalize neighborhoods, financed by city bonds and paid for by earmarking about 15% of the wagering tax. The city's working plans for neighborhood revitalizations include 7-Mile/Livernois, Brightmoor, East English Village, Grand River/Greenfield, North End, and Osborn. Private organizations have pledged substantial funding to the efforts. Additionally, the city has cleared a section of land for large-scale neighborhood construction, which the city is calling the "Far Eastside Plan". In 2011, Mayor Dave Bing announced a plan to categorize neighborhoods by their needs and prioritize the most needed services for those neighborhoods.

In the 2010 United States Census, the city had 713,777 residents, ranking it the 18th most populous city in the United States.

Of the large shrinking cities in the United States, Detroit has had the most dramatic decline in population of the past 60 years (down 1,135,791) and the second largest percentage decline (down 61.4%). While the drop in Detroit's population has been ongoing since 1950, the most dramatic period was the significant 25% decline between the 2000 and 2010 Census.

The population collapse has resulted in large numbers of abandoned homes and commercial buildings, and areas of the city hit hard by urban decay.

Detroit's 713,777 residents represent 269,445 households, and 162,924 families residing in the city. The population density was 5,144.3 people per square mile (1,895/km). There were 349,170 housing units at an average density of 2,516.5 units per square mile (971.6/km). Housing density has declined. The city has demolished thousands of Detroit's abandoned houses, planting some areas and in others allowing the growth of urban prairie.

Of the 269,445 households, 34.4% had children under the age of 18 living with them, 21.5% were married couples living together, 31.4% had a female householder with no husband present, 39.5% were non-families, 34.0% were made up of individuals, and 3.9% had someone living alone who was 65 years of age or older. Average household size was 2.59, and average family size was 3.36.

There was a wide distribution of age in the city, with 31.1% under the age of 18, 9.7% from 18 to 24, 29.5% from 25 to 44, 19.3% from 45 to 64, and 10.4% 65 years of age or older. The median age was 31 years. For every 100 females, there were 89.1 males. For every 100 females age 18 and over, there were 83.5 males.

According to a 2014 study, 67% of the population of the city identified themselves as Christians, with 49% professing attendance at Protestant churches, and 16% professing Roman Catholic beliefs, while 24% claim no religious affiliation. Other religions collectively make up about 8% of the population.

The loss of industrial and working-class jobs in the city has resulted in high rates of poverty and associated problems. From 2000 to 2009, the city's estimated median household income fell from $29,526 to $26,098. the mean income of Detroit is below the overall U.S. average by several thousand dollars. Of every three Detroit residents, one lives in poverty. Luke Bergmann, author of "Getting Ghost: Two Young Lives and the Struggle for the Soul of an American City", said in 2010, "Detroit is now one of the poorest big cities in the country."

In the 2010 American Community Survey, median household income in the city was $25,787, and the median income for a family was $31,011. The per capita income for the city was $14,118. 32.3% of families had income at or below the federally defined poverty level. Out of the total population, 53.6% of those under the age of 18 and 19.8% of those 65 and older had income at or below the federally defined poverty line.

Oakland County in Metro Detroit, once rated amongst the wealthiest US counties per household, is no longer shown in the top 25 listing of "Forbes" magazine. But internal county statistical methods—based on measuring per capita income for counties with more than one million residents—show Oakland is still within the top 12, slipping from the 4th-most affluent such county in the U.S. in 2004 to 11th-most affluent in 2009. Detroit dominates Wayne County, which has an average household income of about $38,000, compared to Oakland County's $62,000.

Much of Detroit's history is racially-charged and rooted in the effects of structural and individualized racism. Beginning with the rise of the automobile industry, the city's population increased more than sixfold during the first half of the 20th century as an influx of European, Middle Eastern (Lebanese, Assyrian/Chaldean), and Southern migrants brought their families to the city. With this economic boom following World War I, the African American population grew from a mere 6,000 in 1910 to more than 120,000 by 1930. This influx of thousands of African Americans in the 20th century became known as the Great Migration. Many of the original white families in Detroit saw this increase in diversity as a threat to their way of life and made it their mission to isolate black people from their neighborhoods, workplaces, and public institutions. Perhaps one of the most overt examples of neighborhood discrimination occurred in 1925 when African American physician Ossian Sweet found his home surrounded by an angry mob of his hostile white neighbors violently protesting his new move into a traditionally white neighborhood. Sweet and ten of his family members and friends were put on trial for murder as one of the mob members throwing rocks at the newly purchased house was shot and killed by someone firing out of a second floor window. Many middle-class families experienced the same kind of hostility as they sought the security of homeownership and the potential for upward mobility.

Detroit has a relatively large Mexican-American population. In the early 20th century, thousands of Mexicans came to Detroit to work in agricultural, automotive, and steel jobs. During the Mexican Repatriation of the 1930s many Mexicans in Detroit were willingly repatriated or forced to repatriate. By the 1940s much of the Mexican community began to settle what is now Mexicantown.

After World War II, many people from Appalachia also settled in Detroit. Appalachians formed communities and their children acquired southern accents. Many Lithuanians also settled in Detroit during the World War II era, especially on the city's Southwest side in the West Vernor area, where the renovated Lithuanian Hall reopened in 2006.

By 1940, 80% of Detroit deeds contained restrictive covenants prohibiting African Americans from buying houses they could afford. These discriminatory tactics were successful as a majority of black people in Detroit resorted to living in all black neighborhoods such as Black Bottom and Paradise Valley. At this time, white people still made up about 90.4% of the city's population. From the 1940s to the 1970s a second wave of black people moved to Detroit in search of employment and with the desire to escape the Jim Crow laws enforcing segregation in the south. However, they soon found themselves once again excluded from many opportunities in Detroit—through violence and policy perpetuating economic discrimination (e.g., redlining). White residents attacked black homes: breaking windows, starting fires, and detonating bombs. An especially grueling result of this increasing competition between black and white people was the Riot of 1943 that had violent ramifications. This era of intolerance made it almost impossible for African Americans to be successful without access to proper housing or the economic stability to maintain their homes and the conditions of many neighborhoods began to decline. In 1948, the landmark Supreme Court case of Shelley v Kraemer outlawed restrictive covenants and while racism in housing did not disappear, it allowed affluent black families to begin moving to traditionally white neighborhoods. Many white families with the financial ability moved to the suburbs of Detroit taking their jobs and tax dollars with them. By 1950, much of the city's white population had moved to the suburbs as macrostructural processes such as "white flight" and "suburbanization" led to a complete population shift.

The Detroit Riot of 1967 is considered to be one of the greatest racial turning points in the history of the city. The ramifications of the uprising were widespread as there were many allegations of white police brutality towards African Americans and over $36 million of insured property was lost. Discrimination and deindustrialization in tandem with racial tensions that had been intensifying in the previous years boiled over and led to an event considered to be the most damaging in Detroit's history.

The population of Latinos significantly increased in the 1990s due to immigration from Jalisco. By 2010 Detroit had 48,679 Hispanics, including 36,452 Mexicans: a 70% increase from 1990. While African Americans previously comprised only 13% of Michigan's population, by 2010 they made up nearly 82% of Detroit's population. The next largest population groups were white people, at 10%, and Hispanics, at 6%. In 2001,103,000 Jews, or about 1.9% of the population, were living in the Detroit area, in both Detroit and Ann Arbor.

According to the 2010 census, segregation in Detroit has decreased in absolute and relative terms and in the first decade of the 21st century, about two-thirds of the total black population in the metropolitan area resided within the city limits of Detroit. The number of integrated neighborhoods increased from 100 in 2000 to 204 in 2010. Detroit also moved down the ranking from number one most segregated city to number four. A 2011 op-ed in "The New York Times" attributed the decreased segregation rating to the overall exodus from the city, cautioning that these areas may soon become more segregated. This pattern already happened in the 1970s, when apparent integration was a precursor to white flight and resegregation. Over a 60-year period, white flight occurred in the city. According to an estimate of the Michigan Metropolitan Information Center, from 2008 to 2009 the percentage of non-Hispanic White residents increased from 8.4% to 13.3%. As the city has become more gentrified, some empty nesters and many young white people have moved into the city, increasing housing values and once again forcing African Americans to move. Gentrification in Detroit has become a rather controversial issues as reinvestment will hopefully lead to economic growth and an increase in population; however, it has already forced many black families to relocate to the suburbs. Despite revitalization efforts, Detroit remains one of the most racially segregated cities in the United States. One of the implications of racial segregation, which correlates with class segregation, may correlate to overall worse health for some populations.

As of 2002, of all of the municipalities in the Wayne County-Oakland County-Macomb County area, Detroit had the second largest Asian population. As of that year Detroit's percentage of Asians was 1%, far lower than the 13.3% of Troy. By 2000 Troy had the largest Asian American population in the tricounty area, surpassing Detroit.

There are four areas in Detroit with significant Asian and Asian American populations. Northeast Detroit has population of Hmong with a smaller group of Lao people. A portion of Detroit next to eastern Hamtramck includes Bangladeshi Americans, Indian Americans, and Pakistani Americans; nearly all of the Bangladeshi population in Detroit lives in that area. Many of those residents own small businesses or work in blue collar jobs, and the population is mostly Muslim. The area north of Downtown Detroit, including the region around the Henry Ford Hospital, the Detroit Medical Center, and Wayne State University, has transient Asian national origin residents who are university students or hospital workers. Few of them have permanent residency after schooling ends. They are mostly Chinese and Indian but the population also includes Filipinos, Koreans, and Pakistanis. In Southwest Detroit and western Detroit there are smaller, scattered Asian communities including an area in the westside adjacent to Dearborn and Redford Township that has a mostly Indian Asian population, and a community of Vietnamese and Laotians in Southwest Detroit.

, the city has one of the U.S.'s largest concentrations of Hmong Americans. In 2006, the city had about 4,000 Hmong and other Asian immigrant families. Most Hmong live east of Coleman Young Airport near Osborn High School. Hmong immigrant families generally have lower incomes than those of suburban Asian families.

Several major corporations are based in the city, including three Fortune 500 companies. The most heavily represented sectors are manufacturing (particularly automotive), finance, technology, and health care. The most significant companies based in Detroit include General Motors, Quicken Loans, Ally Financial, Compuware, Shinola, American Axle, Little Caesars, DTE Energy, Lowe Campbell Ewald, Blue Cross Blue Shield of Michigan, and Rossetti Architects.

About 80,500 people work in downtown Detroit, comprising one-fifth of the city's employment base. Aside from the numerous Detroit-based companies listed above, downtown contains large offices for Comerica, Chrysler, Fifth Third Bank, HP Enterprise, Deloitte, PricewaterhouseCoopers, KPMG, and Ernst & Young. Ford Motor Company is in the adjacent city of Dearborn.

Thousands more employees work in Midtown, north of the central business district. Midtown's anchors are the city's largest single employer Detroit Medical Center, Wayne State University, and the Henry Ford Health System in New Center. Midtown is also home to watchmaker Shinola and an array of small and startup companies. New Center bases TechTown, a research and business incubator hub that is part of the WSU system. Like downtown and Corktown, Midtown also has a fast-growing retailing and restaurant scene.

A number of the city's downtown employers are relatively new, as there has been a marked trend of companies moving from satellite suburbs around Metropolitan Detroit into the downtown core. Compuware completed its world headquarters in downtown in 2003. OnStar, Blue Cross Blue Shield, and HP Enterprise Services are at the Renaissance Center. PricewaterhouseCoopers Plaza offices are adjacent to Ford Field, and Ernst & Young completed its office building at One Kennedy Square in 2006. Perhaps most prominently, in 2010, Quicken Loans, one of the largest mortgage lenders, relocated its world headquarters and 4,000 employees to downtown Detroit, consolidating its suburban offices. In July 2012, the U.S. Patent and Trademark Office opened its Elijah J. McCoy Satellite Office in the Rivertown/Warehouse District as its first location outside Washington, D.C.'s metropolitan area.

In April 2014, the United States Department of Labor reported the city's unemployment rate at 14.5%.

The city of Detroit and other private-public partnerships have attempted to catalyze the region's growth by facilitating the building and historical rehabilitation of residential high-rises in the downtown, creating a zone that offers many business tax incentives, creating recreational spaces such as the Detroit RiverWalk, Campus Martius Park, Dequindre Cut Greenway, and Green Alleys in Midtown. The city itself has cleared sections of land while retaining a number of historically significant vacant buildings in order to spur redevelopment; even though it has struggled with finances, the city issued bonds in 2008 to provide funding for ongoing work to demolish blighted properties. Two years earlier, downtown reported $1.3 billion in restorations and new developments which increased the number of construction jobs in the city. In the decade prior to 2006, downtown gained more than $15 billion in new investment from private and public sectors.

Despite the city's recent financial issues, many developers remain unfazed by Detroit's problems. Midtown is one of the most successful areas within Detroit to have a residential occupancy rate of 96%. Numerous developments have been recently completed or are in various stages of construction. These include the $82 million reconstruction of downtown's David Whitney Building (now an Aloft Hotel and luxury residences), the Woodward Garden Block Development in Midtown, the residential conversion of the David Broderick Tower in downtown, the rehabilitation of the Book Cadillac Hotel (now a Westin and luxury condos) and Fort Shelby Hotel (now Doubletree) also in downtown, and various smaller projects.

Downtown's population of young professionals is growing and retail is expanding. A study in 2007 found out that Downtown's new residents are predominantly young professionals (57% are ages 25 to 34, 45% have bachelor's degrees, and 34% have a master's or professional degree), a trend which has hastened over the last decade. John Varvatos is set to open a downtown store in 2015, and Restoration Hardware is rumored to be opening a store nearby.

On July 25, 2013, Meijer, a midwestern retail chain, opened its first supercenter store in Detroit,; this was a $20 million, 190,000-square-foot store in the northern portion of the city and it also is the centerpiece of a new $72 million shopping center named Gateway Marketplace. On June 11, 2015, Meijer opened its second supercenter store in the city.

On May 21, 2014, JPMorgan Chase announced it was injecting $100 million over five years into Detroit's economy, providing development funding for a variety of projects that would increase employment. It is the largest commitment made to any one city by the nation's biggest bank. Of the $100 million, $50 million will go toward development projects, $25 million will go toward city blight removal, $12.5 million will go for job training, $7 million will go for small businesses in the city, and $5.5 million will go toward the M-1 light rail project (Qline). On May 19, 2015, JPMorgan Chase announced it has invested $32 million for two redevelopment projects in the city's Capitol Park district, the Capitol Park Lofts (the former Capitol Park Building) and the Detroit Savings Bank building at 1212 Griswold. Those investments are separate from Chase's five-year, $100-million commitment. On May 10, 2017, J.P. Morgan Chase & Co. announced a $50 million increase in the $100 million investment the firm committed to economic development and neighborhood stabilization in Detroit by 2019. Half of the $150 million will be grants and the other half is going to toward a variety of loan funds for small business growth, mixed-use real estate development and residential housing projects.

In the central portions of Detroit, the population of young professionals, artists, and other transplants is growing and retail is expanding. This dynamic is luring additional new residents, and former residents returning from other cities, to the city's Downtown along with the revitalized Midtown and New Center areas.

A desire to be closer to the urban scene has also attracted some young professionals to reside in inner ring suburbs such as Ferndale and Royal Oak, Michigan. Detroit's proximity to Windsor, Ontario, provides for views and nightlife, along with Ontario's minimum drinking age of 19. A 2011 study by Walk Score recognized Detroit for its above average walkability among large U.S. cities. About two-thirds of suburban residents occasionally dine and attend cultural events or take in professional games in the city of Detroit.

Known as the world's automotive center, "Detroit" is a metonym for that industry. Detroit's auto industry, some of which was converted to wartime defense production, was an important element of the American "Arsenal of Democracy" supporting the Allied powers during World War II. It is an important source of popular music legacies celebrated by the city's two familiar nicknames, the "Motor City" and "Motown". Other nicknames arose in the 20th century, including "City of Champions," beginning in the 1930s for its successes in individual and team sport; "The D"; "Hockeytown" (a trademark owned by the city's NHL club, the Red Wings); "Rock City" (after the Kiss song "Detroit Rock City"); and "The 313" (its telephone area code).

Live music has been a prominent feature of Detroit's nightlife since the late 1940s, bringing the city recognition under the nickname 'Motown'. The metropolitan area has many nationally prominent live music venues. Concerts hosted by Live Nation perform throughout the Detroit area. Large concerts are held at DTE Energy Music Theatre. The city's theatre venue circuit is the United States' second largest and hosts Broadway performances.

The city of Detroit has a rich musical heritage and has contributed to a number of different genres over the decades leading into the new millennium. Important music events in the city include: the Detroit International Jazz Festival, the Detroit Electronic Music Festival, the Motor City Music Conference (MC2), the Urban Organic Music Conference, the Concert of Colors, and the hip-hop Summer Jamz festival.

In the 1940s, Detroit blues artist John Lee Hooker became a long-term resident in the city's southwest Delray neighborhood. Hooker, among other important blues musicians migrated from his home in Mississippi bringing the Delta blues to northern cities like Detroit. Hooker recorded for Fortune Records, the biggest pre-Motown blues/soul label. During the 1950s, the city became a center for jazz, with stars performing in the Black Bottom neighborhood. Prominent emerging Jazz musicians of the 1960s included: trumpet player Donald Byrd who attended Cass Tech and performed with Art Blakey and the Jazz Messengers early in his career and Saxophonist Pepper Adams who enjoyed a solo career and accompanied Byrd on several albums. The Graystone International Jazz Museum documents jazz in Detroit.

Other, prominent Motor City R&B stars in the 1950s and early 1960s was Nolan Strong, Andre Williams and Nathaniel Mayer – who all scored local and national hits on the Fortune Records label. According to Smokey Robinson, Strong was a primary influence on his voice as a teenager. The Fortune label, a family-operated label on Third Avenue in Detroit, was owned by the husband and wife team of Jack Brown and Devora Brown. Fortune, which also released country, gospel and rockabilly LPs and 45s, laid the groundwork for Motown, which became Detroit's most legendary record label.

Berry Gordy, Jr. founded Motown Records which rose to prominence during the 1960s and early 1970s with acts such as Stevie Wonder, The Temptations, The Four Tops, Smokey Robinson & The Miracles, Diana Ross & The Supremes, the Jackson 5, Martha and the Vandellas, The Spinners, Gladys Knight & the Pips, The Marvelettes, The Elgins, The Monitors, The Velvelettes and Marvin Gaye. Artists were backed by in-house vocalists The Andantes and The Funk Brothers, the Motown house band that was featured in Paul Justman's 2002 documentary film "Standing in the Shadows of Motown", based on Allan Slutsky's book of the same name.

The Motown Sound played an important role in the crossover appeal with popular music, since it was the first African American owned record label to primarily feature African-American artists. Gordy moved Motown to Los Angeles in 1972 to pursue film production, but the company has since returned to Detroit. Aretha Franklin, another Detroit R&B star, carried the Motown Sound; however, she did not record with Berry's Motown Label.

Local artists and bands rose to prominence in the 1960s and 70s including: the MC5, The Stooges, Bob Seger, Amboy Dukes featuring Ted Nugent, Mitch Ryder and The Detroit Wheels, Rare Earth, Alice Cooper, and Suzi Quatro. The group Kiss emphasized the city's connection with rock in the song "Detroit Rock City" and the movie produced in 1999. In the 1980s, Detroit was an important center of the hardcore punk rock underground with many nationally known bands coming out of the city and its suburbs, such as The Necros, The Meatmen, and Negative Approach.
In the 1990s and the new millennium, the city has produced a number of influential hip hop artists, including Eminem, the hip-hop artist with the highest cumulative sales, hip-hop producer J Dilla, rapper and producer Esham and hip hop duo Insane Clown Posse. The city is also home to rappers Big Sean and Danny Brown. The band Sponge toured and produced music, with artists such as Kid Rock and Uncle Kracker. The city also has an active garage rock genre that has generated national attention with acts such as: The White Stripes, The Von Bondies, The Detroit Cobras, The Dirtbombs, Electric Six, and The Hard Lessons.

Detroit is cited as the birthplace of techno music in the early 1980s. The city also lends its name to an early and pioneering genre of electronic dance music, "Detroit techno". Featuring science fiction imagery and robotic themes, its futuristic style was greatly influenced by the geography of Detroit's urban decline and its industrial past. Prominent Detroit techno artists include Juan Atkins, Derrick May, Kevin Saunderson, and Jeff Mills. The Detroit Electronic Music Festival, now known as "Movement", occurs annually in late May on Memorial Day Weekend, and takes place in Hart Plaza. In the early years (2000–2002), this was a landmark event, boasting over a million estimated attendees annually, coming from all over the world to celebrate Techno music in the city of its birth.

Major theaters in Detroit include the Fox Theatre (5,174 seats), Music Hall (1,770 seats), the Gem Theatre (451 seats), Masonic Temple Theatre (4,404 seats), the Detroit Opera House (2,765 seats), the Fisher Theatre (2,089 seats), The Fillmore Detroit (2,200 seats), Saint Andrew's Hall, the Majestic Theater, and Orchestra Hall (2,286 seats) which hosts the renowned Detroit Symphony Orchestra. The Nederlander Organization, the largest controller of Broadway productions in New York City, originated with the purchase of the Detroit Opera House in 1922 by the Nederlander family.

Motown Motion Picture Studios with produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus for a film industry expected to employ over 4,000 people in the metro area.

Because of its unique culture, distinctive architecture, and revitalization and urban renewal efforts in the 21st century, Detroit has enjoyed increased prominence as a tourist destination in recent years. "The New York Times" listed Detroit as the 9th-best destination in its list of "52 Places to Go in 2017", while travel guide publisher "Lonely Planet" named Detroit the second-best city in the world to visit in 2018.

Many of the area's prominent museums are in the historic cultural center neighborhood around Wayne State University and the College for Creative Studies. These museums include the Detroit Institute of Arts, the Detroit Historical Museum, Charles H. Wright Museum of African American History, the Detroit Science Center, as well as the main branch of the Detroit Public Library. Other cultural highlights include Motown Historical Museum, the Ford Piquette Avenue Plant museum (birthplace of the Ford Model T and the world's oldest car factory building open to the public), the Pewabic Pottery studio and school, the Tuskegee Airmen Museum, Fort Wayne, the Dossin Great Lakes Museum, the Museum of Contemporary Art Detroit (MOCAD), the Contemporary Art Institute of Detroit (CAID), and the Belle Isle Conservatory.

In 2010, the G.R. N'Namdi Gallery opened in a complex in Midtown. Important history of America and the Detroit area are exhibited at The Henry Ford in Dearborn, the United States' largest indoor-outdoor museum complex. The Detroit Historical Society provides information about tours of area churches, skyscrapers, and mansions. Inside Detroit, meanwhile, hosts tours, educational programming, and a downtown welcome center. Other sites of interest are the Detroit Zoo in Royal Oak, the Cranbrook Art Museum in Bloomfield Hills, the Anna Scripps Whitcomb Conservatory on Belle Isle, and Walter P. Chrysler Museum in Auburn Hills.

The city's Greektown and three downtown casino resort hotels serve as part of an entertainment hub. The Eastern Market farmer's distribution center is the largest open-air flowerbed market in the United States and has more than 150 foods and specialty businesses. On Saturdays, about 45,000 people shop the city's historic Eastern Market. The Midtown and the New Center area are centered on Wayne State University and Henry Ford Hospital. Midtown has about 50,000 residents and attracts millions of visitors each year to its museums and cultural centers; for example, the Detroit Festival of the Arts in Midtown draws about 350,000 people.

Annual summer events include the Electronic Music Festival, International Jazz Festival, the Woodward Dream Cruise, the African World Festival, the country music Hoedown, Noel Night, and Dally in the Alley. Within downtown, Campus Martius Park hosts large events, including the annual Motown Winter Blast. As the world's traditional automotive center, the city hosts the North American International Auto Show. Held since 1924, America's Thanksgiving Parade is one of the nation's largest. River Days, a five-day summer festival on the International Riverfront lead up to the Windsor–Detroit International Freedom Festival fireworks, which draw super sized-crowds ranging from hundreds of thousands to over three million people.

An important civic sculpture in Detroit is "The Spirit of Detroit" by Marshall Fredericks at the Coleman Young Municipal Center. The image is often used as a symbol of Detroit and the statue itself is occasionally dressed in sports jerseys to celebrate when a Detroit team is doing well. A memorial to Joe Louis at the intersection of Jefferson and Woodward Avenues was dedicated on October 16, 1986. The sculpture, commissioned by "Sports Illustrated" and executed by Robert Graham, is a long arm with a fisted hand suspended by a pyramidal framework.

Artist Tyree Guyton created the controversial street art exhibit known as the Heidelberg Project in 1986, using found objects including cars, clothing and shoes found in the neighborhood near and on Heidelberg Street on the near East Side of Detroit. Guyton continues to work with neighborhood residents and tourists in constantly evolving the neighborhood-wide art installation.

Detroit is one of 13 U.S. metropolitan areas that are home to professional teams representing the four major sports in North America. Since 2017, all of these teams play in the city limits of Detroit itself, a distinction shared with only three other U.S. cities. Detroit is the only U.S. city to have its four major sports teams play within its downtown district.

There are three active major sports venues in the city: Comerica Park (home of the Major League Baseball team Detroit Tigers), Ford Field (home of the NFL's Detroit Lions), and Little Caesars Arena (home of the NHL's Detroit Red Wings and the NBA's Detroit Pistons). A 1996 marketing campaign promoted the nickname "Hockeytown".

The Detroit Tigers have won four World Series titles. The Detroit Red Wings have won 11 Stanley Cups (the most by an American NHL franchise). The Detroit Lions have won 4 NFL titles. The Detroit Pistons have won three NBA titles. With the Pistons' first of three NBA titles in 1989, the city of Detroit has won titles in all four of the major professional sports leagues. Two new downtown stadiums for the Detroit Tigers and Detroit Lions opened in 2000 and 2002, respectively, returning the Lions to the city proper.

In college sports, Detroit's central location within the Mid-American Conference has made it a frequent site for the league's championship events. While the MAC Basketball Tournament moved permanently to Cleveland starting in 2000, the MAC Football Championship Game has been played at Ford Field in Detroit since 2004, and annually attracts 25,000 to 30,000 fans. The University of Detroit Mercy has an NCAA Division I program, and Wayne State University has both NCAA Division I and II programs. The NCAA football Quick Lane Bowl is held at Ford Field each December.

The local soccer team is called the Detroit City Football Club and was founded in 2012. The team plays in the National Premier Soccer League, and its nickname is "Le Rouge".

The city hosted the 2005 MLB All-Star Game, 2006 Super Bowl XL, 2006 and 2012 World Series, WrestleMania 23 in 2007, and the NCAA Final Four in April 2009. 
The city hosted the Detroit Indy Grand Prix on Belle Isle Park from 1989 to 2001, 2007 to 2008, and 2012 and beyond. In 2007, open-wheel racing returned to Belle Isle with both Indy Racing League and American Le Mans Series Racing.

Detroit is one of eight American cities to have won titles in all four major leagues (MLB, NFL, NHL and NBA), though of the eight it is the only one to have not won a Super Bowl title (all of the Lions' titles came prior to the start of the Super Bowl era). In the years following the mid-1930s, Detroit was referred to as the "City of Champions" after the Tigers, Lions, and Red Wings captured the three major professional sports championships in existence at the time in a seven-month period of time (the Tigers won the World Series in October 1935; the Lions won the NFL championship in December 1935; the Red Wings won the Stanley Cup in April 1936). In 1932, Eddie "The Midnight Express" Tolan from Detroit won the 100- and 200-meter races and two gold medals at the 1932 Summer Olympics. Joe Louis won the heavyweight championship of the world in 1937.

Detroit has made the most bids to host the Summer Olympics without ever being awarded the games: seven unsuccessful bids for the 1944, 1952, 1956, 1960, 1964, 1968 and 1972 games.

The city is governed pursuant to the "Home Rule Charter of the City of Detroit". The government of Detroit, Michigan is run by a mayor, the nine-member Detroit City Council, the eleven-member Board of Police Commissioners, and a clerk. All of these officers are elected on a nonpartisan ballot, with the exception of four of the police commissioners, who are appointer by the mayor. Detroit has a "strong mayoral" system, with the mayor approving departmental appointments. The council approves budgets, but the mayor is not obligated to adhere to any earmarking. The city clerk supervises elections and is formally charged with the maintenance of municipal records. City ordinances and substantially large contracts must be approved by the council. The "Detroit City Code" is the codification of Detroit's local ordinances.

The city clerk supervises elections and is formally charged with the maintenance of municipal records. Municipal elections for mayor, city council and city clerk are held at four-year intervals, in the year after presidential elections. Following a November 2009 referendum, seven council members will be elected from districts beginning in 2013 while two will continue to be elected at-large.

Detroit's courts are state-administered and elections are nonpartisan. The Probate Court for Wayne County is in the Coleman A. Young Municipal Center in downtown Detroit. The Circuit Court is across Gratiot Avenue in the Frank Murphy Hall of Justice, in downtown Detroit. The city is home to the Thirty-Sixth District Court, as well as the First District of the Michigan Court of Appeals and the United States District Court for the Eastern District of Michigan. The city provides law enforcement through the Detroit Police Department and emergency services through the Detroit Fire Department.

Detroit has struggled with high crime for decades. The number of homicides peaked in 1974 at 714 and again in 1991 with 615. The murder rate for the city have gone up and down throughout the years averaging over 400 murders with a population of over 1,000,000 residents. The crime rate however has been above the nation average since the 1970s 
Crime has since decreased and, in 2014, the murder rate was 43.4 per 100,000, lower than in St. Louis.

About half of all murders in Michigan in 2015 occurred in Detroit. Although the rate of violent crime dropped 11% in 2008, violent crime in Detroit has not declined as much as the national average from 2007 to 2011. The violent crime rate is one of the highest in the United States. Neighborhoodscout.com reported a crime rate of 62.18 per 1,000 residents for property crimes, and 16.73 per 1,000 for violent crimes (compared to national figures of 32 per 1,000 for property crimes and 5 per 1,000 for violent crime in 2008). Annual statistics released by the Detroit Police Department for 2016 indicate that while the city's overall crime rate declined that year, the murder rate rose from 2015. In 2016 there were 302 homicides in Detroit, a 2.37% increase in the number of murder victims from the preceding year.

The city's downtown typically has lower crime than national and state averages. According to a 2007 analysis, Detroit officials note about 65 to 70 percent of homicides in the city were drug related, with the rate of unsolved murders roughly 70%.

Areas of the city adjacent to the Detroit River are also patrolled by the United States Border Patrol.

In 2012, crime in the city was among the reasons for more expensive car insurance.

Beginning with its incorporation in 1802, Detroit has had a total of 74 mayors. Detroit's last mayor from the Republican Party was Louis Miriani, who served from 1957 to 1962. In 1973, the city elected its first black mayor, Coleman Young. Despite development efforts, his combative style during his five terms in office was not well received by many suburban residents. Mayor Dennis Archer, a former Michigan Supreme Court Justice, refocused the city's attention on redevelopment with a plan to permit three casinos downtown. By 2008, three major casino resort hotels established operations in the city.

In 2000, the city requested an investigation by the United States Justice Department into the Detroit Police Department which was concluded in 2003 over allegations regarding its use of force and civil rights violations. The city proceeded with a major reorganization of the Detroit Police Department.

Detroit is sometimes referred to as the sanctuary city because it has "anti-profiling ordinances that generally prohibit local police from asking about the immigration status of people who are not suspected of any crime."

In March 2013, Governor Rick Snyder declared a financial emergency in the city, stating the city has a $327 million budget deficit and faces more than $14 billion in long-term debt. It has been making ends meet on a month-to-month basis with the help of bond money held in a state escrow account and has instituted mandatory unpaid days off for many city workers. Those troubles, along with underfunded city services, such as police and fire departments, and ineffective turnaround plans from Mayor Bing and the City Council led the state of Michigan to appoint an emergency manager for Detroit on March 14, 2013. On June 14, 2013, Detroit defaulted on $2.5 billion of debt by withholding $39.7 million in interest payments, while Emergency Manager Kevyn Orr met with bondholders and other creditors in an attempt to restructure the city's $18.5 billion debt and avoid bankruptcy. On July 18, 2013, the City of Detroit filed for Chapter 9 bankruptcy protection. It was declared bankrupt by U.S. judge Stephen Rhodes on December 3, with its $18.5 billion debt; he said in accepting the city's contention it is broke and negotiations with its thousands of creditors were infeasible. The city levies an income tax of 2.4 percent on residents and 1.2 percent on nonresidents.

Detroit is home to several institutions of higher learning including Wayne State University, a national research university with medical and law schools in the Midtown area offering hundreds of academic degrees and programs. The University of Detroit Mercy, in Northwest Detroit in the University District, is a prominent Roman Catholic co-educational university affiliated with the Society of Jesus (the Jesuits) and the Sisters of Mercy. The University of Detroit Mercy offers more than a hundred academic degrees and programs of study including business, dentistry, law, engineering, architecture, nursing and allied health professions. The University of Detroit Mercy School of Law is Downtown across from the Renaissance Center.

Sacred Heart Major Seminary, founded in 1919, is affiliated with Pontifical University of Saint Thomas Aquinas, "Angelicum" in Rome and offers pontifical degrees as well as civil undergraduate and graduate degrees. Sacred Heart Major Seminary offers a variety of academic programs for both clerical and lay students. Other institutions in the city include the College for Creative Studies, Marygrove College and Wayne County Community College. In June 2009, the Michigan State University College of Osteopathic Medicine which is based in East Lansing opened a satellite campus at the Detroit Medical Center. The University of Michigan was established in 1817 in Detroit and later moved to Ann Arbor in 1837. In 1959, University of Michigan–Dearborn was established in neighboring Dearborn.

 many K-12 students in Detroit frequently change schools, with some children having been enrolled in seven schools before finishing their K-12 careers. There is a concentration of senior high schools and charter schools in the Downtown Detroit area, which had wealthier residents and more gentrification relative to other parts of Detroit: Downtown, northwest Detroit, and northeast Detroit have 1,894, 3,742, and 6,018 students of high school age each, respectively, while they have 11, three, and two high schools each, respectively.

With about 66,000 public school students (2011–12), the Detroit Public Schools (DPS) district is the largest school district in Michigan. Detroit has an additional 56,000 charter school students for a combined enrollment of about 122,000 students. there are about as many students in charter schools as there are in district schools. DPS continues to have the majority of the special education pupils. In addition, some Detroit students, as of 2016, attend public schools in other municipalities.

In 1999, the Michigan Legislature removed the locally elected board of education amid allegations of mismanagement and replaced it with a reform board appointed by the mayor and governor. The elected board of education was re-established following a city referendum in 2005. The first election of the new 11-member board of education occurred on November 8, 2005.

Due to growing Detroit charter schools enrollment as well as a continued exodus of population, the city planned to close many public schools. State officials report a 68% graduation rate for Detroit's public schools adjusted for those who change schools. Traditional public and charter school students in the city have performed poorly on standardized tests. Circa 2009 and 2011, while Detroit traditional public schools scored a record low on national tests, the publicly funded charter schools did even worse than the traditional public schools. there were 30,000 excess openings in Detroit traditional public and charter schools, bearing in mind the number of K-12-aged children in the city. In 2016, Kate Zernike of "The New York Times" stated school performance did not improve despite the proliferation of charters, describing the situation as "lots of choice, with no good choice."

Detroit public schools students scored the lowest on tests of reading and writing of all major cities in the United States in 2015. Among eighth-graders, only 27% showed basic proficiency in math and 44% in reading. Nearly half of Detroit's adults are functionally illiterate.

Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.

In the 1964–1965 school year there were about 110 Catholic grade schools in Detroit, Hamtramck, and Highland Park and 55 Catholic high schools in those three cities. The Catholic school population in Detroit has decreased due to the increase of charter schools, increasing tuition at Catholic schools, the small number of African-American Catholics, White Catholics moving to suburbs, and the decreased number of teaching nuns.

The "Detroit Free Press" and "The Detroit News" are the major daily newspapers, both broadsheet publications published together under a joint operating agreement called the Detroit Newspaper Partnership. Media philanthropy includes the "Detroit Free Press" high school journalism program and the Old Newsboys' Goodfellow Fund of Detroit. In March 2009, the two newspapers reduced home delivery to three days a week, print reduced newsstand issues of the papers on non-delivery days and focus resources on Internet-based news delivery. The "Metro Times", founded in 1980, is a weekly publication, covering news, arts & entertainment.

Also founded in 1935 and based in Detroit the "Michigan Chronicle" is one of the oldest and most respected African-American weekly newspapers in America. Covering politics, entertainment, sports and community events. The Detroit television market is the 11th largest in the United States; according to estimates that do not include audiences in large areas of Ontario, Canada (Windsor and its surrounding area on broadcast and cable TV, as well as several other cable markets in Ontario, such as the city of Ottawa) which receive and watch Detroit television stations.

Detroit has the 11th largest radio market in the United States, though this ranking does not take into account Canadian audiences. Nearby Canadian stations such as Windsor's CKLW (whose jingles formerly proclaimed "CKLW-the Motor City") are popular in Detroit.

"Hardcore Pawn", a U.S. documentary reality television series produced for truTV, features the day-to-day operations of American Jewelry and Loan, a family-owned pawn shop on Greenfield Road.

Within the city of Detroit, there are over a dozen major hospitals which include the Detroit Medical Center (DMC), Henry Ford Health System, St. John Health System, and the John D. Dingell VA Medical Center. The DMC, a regional Level I trauma center, consists of Detroit Receiving Hospital and University Health Center, Children's Hospital of Michigan, Harper University Hospital, Hutzel Women's Hospital, Kresge Eye Institute, Rehabilitation Institute of Michigan, Sinai-Grace Hospital, and the Karmanos Cancer Institute. The DMC has more than 2,000 licensed beds and 3,000 affiliated physicians. It is the largest private employer in the City of Detroit. The center is staffed by physicians from the Wayne State University School of Medicine, the largest single-campus medical school in the United States, and the United States' fourth largest medical school overall.

Detroit Medical Center formally became a part of Vanguard Health Systems on December 30, 2010, as a for profit corporation. Vanguard has agreed to invest nearly $1.5 B in the Detroit Medical Center complex which will include $417 M to retire debts, at least $350 M in capital expenditures and an additional $500 M for new capital investment. Vanguard has agreed to assume all debts and pension obligations. The metro area has many other hospitals including William Beaumont Hospital, St. Joseph's, and University of Michigan Medical Center.
In 2011, Detroit Medical Center and Henry Ford Health System substantially increased investments in medical research facilities and hospitals in the city's Midtown and New Center.

In 2012, two major construction projects were begun in New Center, the Henry Ford Health System started the first phase of a $500 million, 300-acre revitalization project, with the construction of a new $30 million, 275,000-square-foot, "Medical Distribution Center" for Cardinal Health, Inc. and Wayne State University started construction on a new $93 million, 207,000-square-foot, Integrative Biosciences Center (IBio). As many as 500 researchers, and staff will work out of the IBio Center.

With its proximity to Canada and its facilities, ports, major highways, rail connections and international airports, Detroit is an important transportation hub. The city has three international border crossings, the Ambassador Bridge, Detroit–Windsor Tunnel and Michigan Central Railway Tunnel, linking Detroit to Windsor, Ontario. The Ambassador Bridge is the single busiest border crossing in North America, carrying 27% of the total trade between the U.S. and Canada.

On February 18, 2015, Canadian Transport Minister Lisa Raitt announced Canada has agreed to pay the entire cost to build a $250 million U.S. Customs plaza adjacent to the planned new Detroit–Windsor bridge, now the Gordie Howe International Bridge. Canada had already planned to pay for 95% of the bridge, which will cost $2.1 billion, and is expected to open in 2022 or 2023. "This allows Canada and Michigan to move the project forward immediately to its next steps which include further design work and property acquisition on the U.S. side of the border," Raitt said in a statement issued after she spoke in the House of Commons.

Mass transit in the region is provided by bus services. The Detroit Department of Transportation (DDOT) provides service within city limits up to the outer edges of the city. From there, the Suburban Mobility Authority for Regional Transportation (SMART) provides service to the suburbs and the city regionally with local routes and SMART's FAST service. FAST is a new service provided by SMART which offers limited stops along major corridors throughout the Detroit metropolitan area connecting the suburbs to downtown. The new high-frequency service travels along three of Detroit's busiest corridors, Gratiot, Woodward, and Michigan, and only stops at designated FAST stops. Cross border service between the downtown areas of Windsor and Detroit is provided by Transit Windsor via the Tunnel Bus.

An elevated rail system known as the People Mover, completed in 1987, provides daily service around a loop downtown. The QLINE serves as a link between the Detroit People Mover and Detroit Amtrak station via Woodward Avenue. The SEMCOG Commuter Rail line will extend from Detroit's New Center, connecting to Ann Arbor via Dearborn, Wayne, and Ypsilanti when it is opened.

The Regional Transit Authority (RTA) was established by an act of the Michigan legislature in December 2012 to oversee and coordinate all existing regional mass transit operations, and to develop new transit services in the region. The RTA's first project was the introduction of RelfeX, a limited-stop, cross-county bus service connecting downtown and midtown Detroit with Oakland county via Woodward avenue.

Amtrak provides service to Detroit, operating its "Wolverine" service between Chicago and Pontiac. The Amtrak station is in New Center north of downtown. The "J. W. Westcott II", which delivers mail to lake freighters on the Detroit River, is a floating post office.

The city of Detroit has a higher than average percentage of households without a car. In 2016, 24.7 percent of Detroit households lacked a car, much higher than the national average of 8.7. Detroit averaged 1.15 cars per household in 2016, compared to a national average of 1.8.

Freight railroad operations in the city of Detroit are provided by Canadian National Railway, Canadian Pacific Railway, Conrail Shared Assets, CSX Transportation and Norfolk Southern Railway, each of which have local yards within the city. Detroit is also served by the Delray Connecting Railroad and Detroit Connecting Railroad shortlines.

Detroit Metropolitan Wayne County Airport (DTW), the principal airport serving Detroit, is in nearby Romulus. DTW is a primary hub for Delta Air Lines (following its acquisition of Northwest Airlines), and a secondary hub for Spirit Airlines. The airport is connected to Downtown Detroit by the Suburban Mobility Authority for Regional Transportation (SMART) FAST Michigan route.

Coleman A. Young International Airport (DET), previously called Detroit City Airport, is on Detroit's northeast side; the airport now maintains only charter service and general aviation. Willow Run Airport, in far-western Wayne County near Ypsilanti, is a general aviation and cargo airport.

Metro Detroit has an extensive toll-free network of freeways administered by the Michigan Department of Transportation. Four major Interstate Highways surround the city. Detroit is connected via Interstate 75 (I-75) and I-96 to Kings Highway 401 and to major Southern Ontario cities such as London, Ontario and the Greater Toronto Area. I-75 (Chrysler and Fisher freeways) is the region's main north–south route, serving Flint, Pontiac, Troy, and Detroit, before continuing south (as the Detroit–Toledo and Seaway Freeways) to serve many of the communities along the shore of Lake Erie.

I-94 (Edsel Ford Freeway) runs east–west through Detroit and serves Ann Arbor to the west (where it continues to Chicago) and Port Huron to the northeast. The stretch of the I-94 freeway from Ypsilanti to Detroit was one of America's earlier limited-access highways. Henry Ford built it to link the factories at Willow Run and Dearborn during World War II. A portion was known as the Willow Run Expressway. The I-96 freeway runs northwest–southeast through Livingston, Oakland and Wayne counties and (as the Jeffries Freeway through Wayne County) has its eastern terminus in downtown Detroit.

I-275 runs north–south from I-75 in the south to the junction of I-96 and I-696 in the north, providing a bypass through the western suburbs of Detroit. I-375 is a short spur route in downtown Detroit, an extension of the Chrysler Freeway. I-696 (Reuther Freeway) runs east–west from the junction of I-96 and I-275, providing a route through the northern suburbs of Detroit. Taken together, I-275 and I-696 form a semicircle around Detroit. Michigan state highways designated with the letter M serve to connect major freeways.

Detroit has a floating post office. In 1948, The J. W. Westcott II became a floating post office servicing the Port of Detroit. Its zip code is 48222. Originally established in 1874 as a maritime reporting agency to inform other vessels about port conditions, the J. W. Westcott II is still in operation today.







</doc>
<doc id="8688" url="https://en.wikipedia.org/wiki?curid=8688" title="Deccan Traps">
Deccan Traps

The Deccan Traps are a large igneous province of west-central India (17–24°N, 73–74°E). They are one of the largest volcanic features on Earth. They consist of multiple layers of solidified flood basalt that together are more than thick, cover an area of , and have a volume of . Originally, the Deccan Traps may have covered , with a correspondingly larger original volume.

The term "trap" has been used in geology since 1785–1795 for such rock formations. It is derived from the Swedish word for stairs ("trappa") and refers to the step-like hills forming the landscape of the region.

The Deccan Traps began forming 66.25 million years ago, at the end of the Cretaceous period. The bulk of the volcanic eruption occurred at the Western Ghats some 66 million years ago. This series of eruptions may have lasted fewer than 30,000 years. 

The original area covered by the lava flows is estimated to have been as large as , approximately half the size of modern India. The Deccan Traps region was reduced to its current size by erosion and plate tectonics; the present area of directly observable lava flows is around .

The release of volcanic gases, particularly sulfur dioxide, during the formation of the traps may have contributed to climate change. Data points to an average drop in temperature of about in this period.

Because of its magnitude, scientists have speculated that the gases released during the formation of the Deccan Traps played a major role in the Cretaceous–Paleogene (K–Pg) extinction event (also known as the Cretaceous–Tertiary or K–T extinction). It has been theorized that sudden cooling due to sulfurous volcanic gases released by the formation of the traps and toxic gas emissions may have contributed significantly to the K–Pg, as well as other, mass extinctions. However, the current consensus among the scientific community is that the extinction was primarily triggered by the Chicxulub impact event in North America, which would have produced a sunlight-blocking dust cloud that killed much of the plant life and reduced global temperature (this cooling is called an impact winter).

Work published in 2014 by geologist Gerta Keller and others on the timing of the Deccan volcanism suggests the extinction may have been caused by both the volcanism and the impact event. This was followed by a similar study in 2015, both of which consider the hypothesis that the impact exacerbated or induced the Deccan volcanism, since the events occur at antipodes.

However, the impact theory is still the best supported and has been determined by various reviews to be the consensus view.

Within the Deccan Traps at least 95% of the lavas are tholeiitic basalts. Other rock types present include: alkali basalt, nephelinite, lamprophyre, and carbonatite.

Mantle xenoliths have been described from Kachchh (northwestern India) and elsewhere in the western Deccan.

The Deccan Traps are famous for the beds of fossils that have been found between layers of lava. Particularly well known species include the frog "Oxyglossus pusillus" (Owen) of the Eocene of India and the toothed frog "Indobatrachus", an early lineage of modern frogs, which is now placed in the Australian family Myobatrachidae. The Infratrappean Beds and Intertrappean Beds also contain fossil freshwater molluscs.

It is postulated that the Deccan Traps eruption was associated with a deep mantle plume. The area of long-term eruption (the hotspot), known as the Réunion hotspot, is suspected of both causing the Deccan Traps eruption and opening the rift that once separated the Seychelles plateau from India. Seafloor spreading at the boundary between the Indian and African Plates subsequently pushed India north over the plume, which now lies under Réunion island in the Indian Ocean, southwest of India. The mantle plume model has, however, been challenged.

Data continues to emerge that support the plume model. The motion of the Indian tectonic plate and the eruptive history of the Deccan traps show strong correlations. Based on data from marine magnetic profiles, a pulse of unusually rapid plate motion began at the same time as the first pulse of Deccan flood basalts, which is dated at 67 million years ago. The spreading rate rapidly increased and reached a maximum at the same time as the peak basaltic eruptions. The spreading rate then dropped off, with the decrease occurring around 63 million years ago, by which time the main phase of Deccan volcanism ended. This correlation is seen as driven by plume dynamics.

The motions of the Indian and African plates have also been shown to be coupled, the common element being the position of these plates relative to the location of the Réunion plume head. The onset of accelerated motion of India coincides with a large slowing of the rate of counterclockwise rotation of Africa. The close correlations between the plate motions suggest that they were both driven by the force of the Réunion plume.

There is some evidence to link the Deccan Traps eruption to the contemporaneous asteroid impact that created the nearly antipodal Chicxulub crater in the Mexican state of Yucatán. Although the Deccan Traps began erupting well before the impact, argon-argon dating suggests that the impact may have caused an increase in permeability that allowed magma to reach the surface and produced the most voluminous flows, accounting for around 70% of the volume. The combination of the asteroid impact and the resulting increase in eruptive volume may have been responsible for the mass extinctions that occurred at the time that separates the Cretaceous and Paleogene periods, known as the K–Pg boundary.

A more recent discovery appears to demonstrate the scope of the destruction from the impact alone, however. In a March 2019 article in the Proceedings of the National Academy of Sciences, an international team of twelve scientists revealed the contents of the Tanis fossil site discovered near Bowman, North Dakota that appeared to show a devastating mass destruction of an ancient lake and its inhabitants at the time of the Chicxulub impact. In the paper, the group claims that the geology of the site is strewn with fossilized trees and remains of fish and other animals. The lead researcher, Robert A. DePalma of the University of Kansas, was quoted in the New York Times as stating that “[Y]ou would be blind to miss the carcasses sticking out... It is impossible to miss when you see the outcrop.” Evidence correlating this find to the Chicxulub impact included tektites bearing "the unique chemical signature of other tektites associated with the Chicxulub event" found in the gills of fish fossils and embedded in amber, an iridium-rich top layer that is considered another signature of the event, and an atypical lack of scavenging of the dead fish and animals that suggested few other species survived the event to feed off the mass death. The exact mechanism of the site's destruction has been debated as either an impact-caused tsunami or lake and river seiche activity triggered by post-impact earthquakes, though there has yet been no firm conclusion upon which researchers have settled.

A geological structure that exists in the sea floor off the west coast of India has been suggested as a possible impact crater, in this context called the Shiva crater. It has also been dated at approximately 66 million years ago, potentially matching the Deccan traps. The researchers claiming that this feature is an impact crater suggest that the impact may have been the triggering event for the Deccan Traps as well as contributing to the acceleration of the Indian plate in the early Paleogene. However, the current consensus in the Earth science community is that this feature is unlikely to be an actual impact crater.




</doc>
<doc id="8690" url="https://en.wikipedia.org/wiki?curid=8690" title="Don't ask, don't tell">
Don't ask, don't tell

"Don't ask, don't tell" (DADT) was the official United States policy on military service by gay men, bisexuals, and lesbians, instituted by the Clinton Administration. The policy was issued under Department of Defense Directive 1304.26 on December 21, 1993 and was in effect from February 28, 1994 until September 20, 2011. The policy prohibited military personnel from discriminating against or harassing closeted homosexual or bisexual service members or applicants, while barring openly gay, lesbian, or bisexual persons from military service. This relaxation of legal restrictions on service by gays and lesbians in the armed forces was mandated by United States federal law (), which was signed November 30, 1993. The policy prohibited people who "demonstrate a propensity or intent to engage in homosexual acts" from serving in the armed forces of the United States, because their presence "would create an unacceptable risk to the high standards of morale, good order and discipline, and unit cohesion that are the essence of military capability".

The act prohibited any homosexual or bisexual person from disclosing their sexual orientation or from speaking about any homosexual relationships, including marriages or other familial attributes, while serving in the United States armed forces. The act specified that service members who disclose that they are homosexual or engage in homosexual conduct should be separated (discharged) except when a service member's conduct was "for the purpose of avoiding or terminating military service" or when it "would not be in the best interest of the armed forces". Since DADT ended in 2011, persons who are openly homosexual and bisexual have been able to serve.

The "don't ask" part of the DADT policy specified that superiors should not initiate investigation of a service member's orientation without witnessing disallowed behaviors, though credible evidence of homosexual behavior could be used to initiate an investigation. Unauthorized investigations and harassment of suspected servicemen and women led to an expansion of the policy to "don't ask, don't tell, don't pursue, don't harass".

Beginning in the early 2000s, several legal challenges to DADT were filed, and legislation to repeal DADT was enacted in December 2010, specifying that the policy would remain in place until the President, the Secretary of Defense, and the Chairman of the Joint Chiefs of Staff certified that repeal would not harm military readiness, followed by a 60-day waiting period. A July 6, 2011, ruling from a federal appeals court barred further enforcement of the U.S. military's ban on openly gay service members. President Barack Obama, Secretary of Defense Leon Panetta, and Chairman of the Joint Chiefs of Staff Admiral Mike Mullen sent that certification to Congress on July 22, 2011, which set the end of DADT to September 20, 2011.

Engaging in homosexual activity has been grounds for discharge from the American military since the Revolutionary War. Policies based on sexual orientation appeared as the United States prepared to enter World War II. When the military added psychiatric screening to its induction process, it included homosexuality as a disqualifying trait, then seen as a form of psychopathology. When the army issued revised mobilization regulations in 1942, it distinguished "homosexual" recruits from "normal" recruits for the first time. Before the buildup to the war, gay service members were court-martialed, imprisoned, and dishonorably discharged; but in wartime, commanding officers found it difficult to convene court-martial boards of commissioned officers and the administrative blue discharge became the military's standard method for handling gay and lesbian personnel. In 1944, a new policy directive decreed that homosexuals were to be committed to military hospitals, examined by psychiatrists and discharged under Regulation 615-360, section 8.

In 1947, blue discharges were discontinued and two new classifications were created: "general" and "undesirable". Under such a system, a serviceman or woman found to be gay but who had not committed any sexual acts while in service would tend to receive an undesirable discharge. Those found guilty of engaging in sexual conduct were usually dishonorably discharged. A 1957 U.S. Navy study known as the Crittenden Report dismissed the charge that homosexuals constitute a security risk, but advocated stringent anti-homosexual policies because "Homosexuality is wrong, it is evil, and it is to be branded as such." It remained secret until 1976. Fannie Mae Clackum was the first service member to successfully appeal such a discharge, winning eight years of back pay from the US Court of Claims in 1960.

From the 1950s through the Vietnam War, some notable gay service members avoided discharges despite pre-screening efforts, and when personnel shortages occurred, homosexuals were allowed to serve.

The gay and lesbian rights movement in the 1970s and 1980s raised the issue by publicizing several noteworthy dismissals of gay service members. Sgt. Leonard Matlovich appeared on the cover of "Time" in 1975. In 1982 the Department of Defense issued a policy stating that, "Homosexuality is incompatible with military service." It cited the military's need "to maintain discipline, good order, and morale" and "to prevent breaches of security". In 1988, in response to a campaign against lesbians at the Marines' Parris Island Depot, activists launched the Gay and Lesbian Military Freedom Project (MFP) to advocate for an end to the exclusion of gays and lesbians from the armed forces. In 1989, reports commissioned by the Personnel Security Research and Education Center (PERSEREC), an arm of the Pentagon, were discovered in the process of Joseph Steffan's lawsuit fighting his forced resignation from the U.S. Naval Academy. One report said that "having a same-gender or an opposite-gender orientation is unrelated to job performance in the same way as is being left- or right-handed." Other lawsuits fighting discharges highlighted the service record of service members like Tracy Thorne and Margarethe (Grethe) Cammermeyer. The MFP began lobbying Congress in 1990, and in 1991 Senator Brock Adams (D-Washington) and Rep. Barbara Boxer introduced the Military Freedom Act, legislation to end the ban completely. Adams and Rep. Pat Schroeder (D-Colorado) re-introduced it the next year. In July 1991, Secretary of Defense Dick Cheney, in the context of the outing of his press aide Pete Williams, dismissed the idea that gays posed a security risk as "a bit of an old chestnut" in testimony before the House Budget Committee. In response to his comment, several major newspapers endorsed ending the ban, including "USA Today", the "Los Angeles Times", and the "Detroit Free Press". In June 1992, the General Accounting Office released a report that members of Congress had requested two years earlier estimating the costs associated with the ban on gays and lesbians in the military at $27 million annually.

During the 1992 U.S. presidential election campaign, the civil rights of gays and lesbians, particularly their open service in the military, attracted some press attention, and all candidates for the Democratic presidential nomination supported ending the ban on military service by gays and lesbians, but the Republicans did not make a political issue of that position. In an August cover letter to all his senior officers, Gen. Carl Mundy, Jr., Commandant of the Marine Corps, praised a position paper authored by a Marine Corps chaplain that said that "In the unique, intensely close environment of the military, homosexual conduct can threaten the lives, including the physical (e.g. AIDS) and psychological well-being of others". Mundy called it "extremely insightful" and said it offered "a sound basis for discussion of the issue". The murder of gay U.S. Navy petty officer Allen R. Schindler, Jr. on October 27, 1992, brought calls from advocates of allowing open service by gays and lesbians for prompt action from the incoming Clinton administration.

The policy was introduced as a compromise measure in 1993 by President Bill Clinton who campaigned in 1992 on the promise to allow all citizens to serve in the military regardless of sexual orientation. Commander Craig Quigley, a Navy spokesman, expressed the opposition of many in the military at the time when he said, "Homosexuals are notoriously promiscuous" and that in shared shower situations, heterosexuals would have an "uncomfortable feeling of someone watching".

During the 1993 policy debate, the National Defense Research Institute prepared a study for the Office of the Secretary of Defense published as "Sexual Orientation and U.S. Military Personnel Policy: Options and Assessment". It concluded that "circumstances could exist under which the ban on homosexuals could be lifted with little or no adverse consequences for recruitment and retention" if the policy were implemented with care, principally because many factors contribute to individual enlistment and re-enlistment decisions. On May 5, 1993, Gregory M. Herek, associate research psychologist at the University of California at Davis and an authority on public attitudes toward lesbians and gay men, testified before the House Armed Services Committee on behalf of several professional associations. He stated, "The research data show that there is nothing about lesbians and gay men that makes them inherently unfit for military service, and there is nothing about heterosexuals that makes them inherently unable to work and live with gay people in close quarters." Herek added, "The assumption that heterosexuals cannot overcome their prejudices toward gay people is a mistaken one."

In Congress, Democratic Senator Sam Nunn of Georgia led the contingent that favored maintaining the absolute ban on gays. Reformers were led by Democratic Congressman Barney Frank of Massachusetts, who favored modification (but ultimately voted for the defense authorization bill with the gay ban language), and Barry Goldwater, a former Republican Senator and a retired Major General, who argued on behalf of allowing service by open gays and lesbians. In a June 1993 "Washington Post" opinion piece, Goldwater wrote: "You don't have to be straight to shoot straight".

Congress rushed to enact the existing gay ban policy into federal law, outflanking Clinton's planned repeal effort. Clinton called for legislation to overturn the ban, but encountered intense opposition from the Joint Chiefs of Staff, members of Congress, and portions of the public. DADT emerged as a compromise policy. Congress included text in the National Defense Authorization Act for Fiscal Year 1994 (passed in 1993) requiring the military to abide by regulations essentially identical to the 1982 absolute ban policy. The Clinton Administration on December 21, 1993, issued Defense Directive 1304.26, which directed that military applicants were not to be asked about their sexual orientation. This policy is now known as "Don't Ask, Don't Tell". The phrase was coined by Charles Moskos, a military sociologist.

In accordance with the December 21, 1993, Department of Defense Directive 1332.14, it was legal policy (10 U.S.C. § 654) that homosexuality was incompatible with military service and that persons who engaged in homosexual acts or stated that they are homosexual or bisexual were to be discharged. The Uniform Code of Military Justice, passed by Congress in 1950 and signed by President Harry S Truman, established the policies and procedures for discharging service members.

The full name of the policy at the time was "Don't Ask, Don't Tell, Don't Pursue". The "Don't Ask" provision mandated that military or appointed officials will not ask about or require members to reveal their sexual orientation. The "Don't Tell" stated that a member may be discharged for claiming to be a homosexual or bisexual or making a statement indicating a tendency towards or intent to engage in homosexual activities. The "Don’t Pursue" established what was minimally required for an investigation to be initiated. A "Don’t Harass" provision was added to the policy later. It ensured that the military would not allow harassment or violence against service members for any reason.

The Servicemembers Legal Defense Network was founded in 1993 to advocate an end to discrimination on the basis of sexual orientation in the U.S. Armed Forces.

DADT was upheld by five federal Courts of Appeal. The Supreme Court, in "Rumsfeld v. Forum for Academic and Institutional Rights, Inc." (2006), unanimously held that the federal government could constitutionally withhold funding from universities, no matter what their nondiscrimination policies might be, for refusing to give military recruiters access to school resources. An association of law schools had argued that allowing military recruiting at their institutions compromised their ability to exercise their free speech rights in opposition to discrimination based on sexual orientation as represented by DADT.

In January 1998, Senior Chief Petty Officer Timothy R. McVeigh (not to be confused with convicted Oklahoma City bomber, Timothy J. McVeigh) won a preliminary injunction from a U.S. district court that prevented his discharge from the U.S. Navy for "homosexual conduct" after 17 years of service. His lawsuit did not challenge the DADT policy, but asked the court to hold the military accountable for adhering to the policy's particulars. The Navy had investigated McVeigh's sexual orientation based on his AOL email account name and user profile. District Judge Stanley Sporkin ruled in "McVeigh v. Cohen" that the Navy had violated its own DADT guidelines: "Suggestions of sexual orientation in a private, anonymous email account did not give the Navy a sufficient reason to investigate to determine whether to commence discharge proceedings." He called the Navy's investigation "a search and destroy mission" against McVeigh. The case also attracted attention because a navy paralegal had misrepresented himself when querying AOL for information about McVeigh's account. Frank Rich linked the two issues: "McVeigh is as clear-cut a victim of a witch hunt as could be imagined, and that witch hunt could expand exponentially if the military wants to add on-line fishing to its invasion of service members' privacy." AOL apologized to McVeigh and paid him damages. McVeigh reached a settlement with the Navy that paid his legal expenses and allowed him to retire with full benefits in July. "The New York Times" called Sporkin's ruling "a victory for gay rights, with implications for the millions of people who use computer on-line services".

In April 2006, Margaret Witt, a major in the United States Air Force who was being investigated for homosexuality, filed suit in the United States District Court for the Western District of Washington seeking declaratory and injunctive relief on the grounds that DADT violates substantive due process, the Equal Protection Clause, and procedural due process. In July 2007 the Secretary of the Air Force ordered her honorable discharge. Dismissed by the district court, the case was heard on appeal, and the Ninth Circuit issued its ruling on May 21, 2008. Its decision in "Witt v. Department of the Air Force" reinstated Witt's substantive-due-process and procedural-due-process claims and affirmed the dismissal of her Equal Protection claim. The Ninth Circuit, analyzing the Supreme Court decision in "Lawrence v. Texas" (2003), determined that DADT had to be subjected to heightened scrutiny, meaning that there must be an "important" governmental interest at issue, that DADT must "significantly" further the governmental interest, and that there can be no less intrusive way for the government to advance that interest.

The Obama administration declined to appeal, allowing a May 3, 2009, deadline to pass, leaving "Witt" as binding on the entire Ninth Circuit, and returning the case to the District Court. On September 24, 2010, District Judge Ronald B. Leighton ruled that Witt's constitutional rights had been violated by her discharge and that she must be reinstated to the Air Force.

The government filed an appeal with the Ninth Circuit on November 23, but made no attempt to have the trial court's ruling stayed pending the outcome. In a settlement announced on May 10, 2011, the Air Force agreed to drop its appeal and remove Witt's discharge from her military record. She will retire with full benefits.

In 2010, a lawsuit filed in 2004 by the Log Cabin Republicans (LCR), the nation's largest Republican gay organization, went to trial. Challenging the constitutionality of DADT, the plaintiffs stated that the policy violates the rights of gay military members to free speech, due process and open association. The government argued that DADT was necessary to advance a legitimate governmental interest. Plaintiffs introduced statements by President Barack Obama, from prepared remarks, that DADT "doesn't contribute to our national security", "weakens our national security", and that reversal is "essential for our national security". According to plaintiffs, these statements alone satisfied their burden of proof on the due process claims.

On September 9, 2010, Judge Virginia A. Phillips ruled in "Log Cabin Republicans v. United States of America" that the ban on service by openly gay service members was an unconstitutional violation of the First and Fifth Amendments. On October 12, 2010, she granted an immediate worldwide injunction prohibiting the Department of Defense from enforcing the "Don't Ask Don't Tell" policy and ordered the military to suspend and discontinue any investigation or discharge, separation, or other proceedings based on it. The Department of Justice appealed her decision and requested a stay of her injunction, which Phillips denied but which the Ninth Circuit Court of Appeals granted on October 20
and stayed pending appeal on November 1. The U.S. Supreme Court refused to overrule the stay.
District Court neither anticipated questions of
constitutional law nor formulated a rule broader than is required by the facts. The
constitutional issues regarding DADT are well-defined, and the District Court
focused specifically on the relevant inquiry of whether the statute impermissibly
infringed upon substantive due process rights with regard to a protected area of
individual liberty. Engaging in a careful and detailed review of the facts presented
to it at trial, the District Court properly concluded that the Government put forward
no persuasive evidence to demonstrate that the statute is a valid exercise of
congressional authority to legislate in the realm of protected liberty interests. See
Log Cabin, 716 F. Supp. 2d at 923. Hypothetical questions were neither presented
nor answered in reaching this decision.
On October 19, 2010, military recruiters were told they could accept openly gay applicants. On October 20, 2010, Lt. Daniel Choi, an openly gay man honorably discharged under DADT, re-enlisted in the U.S. Army.

Following passage of the Don't Ask, Don't Tell Repeal Act of 2010, the Justice Department asked the Ninth Circuit to suspend LCR's suit in light of the legislative repeal. LCR opposed the request, noting that gay personnel were still subject to discharge. On January 28, 2011, the Court denied the Justice Department's request. The Obama administration responded by requesting that the policy be allowed to stay in place while they completed the process of assuring that its end would not impact combat readiness. On March 28, the LCR filed a brief asking that the court deny the administration's request.

In 2011, while waiting for certification, several service members were discharged under DADT at their own insistence, until July 6 when a three-judge panel of the Ninth Circuit Court of Appeals re-instated Judge Phillips' injunction barring further enforcement of the U.S. military's ban on openly gay service members. On July 11, the appeals court asked the DOJ to inform the court if it intended to proceed with its appeal. On July 14, the Justice Department filed a motion "to avoid short-circuiting the repeal process established by Congress during the final stages of the implementation of the repeal". and warning of "significant immediate harms on the government". On July 15, the Ninth Circuit restored most of the DADT policy, but continued to prohibit the government from discharging or investigating openly gay personnel. Following the implementation of DADT's repeal, a panel of three judges of the Ninth Circuit Court of Appeals vacated the Phillips ruling.

Following the July 1999 murder of Army Pfc. Barry Winchell, apparently motivated by anti-gay bias, President Clinton issued an executive order modifying the Uniform Code of Military Justice to permit evidence of a hate crime to be admitted during the sentencing phase of a trial. In December, Secretary of Defense William Cohen ordered a review of DADT to determine if the policy's anti-gay harassment component was being observed. When that review found anti-gay sentiments were widely expressed and tolerated in the military, the DOD adopted a new anti-harassment policy in July 2000, though its effectiveness was disputed. On December 7, 1999, Hillary Clinton told an audience of gay supporters that "Gays and lesbians already serve with distinction in our nation's armed forces and should not face discrimination. Fitness to serve should be based on an individual's conduct, not their sexual orientation." Later that month, retired Gen. Carl E. Mundy Jr. defended the implementation of DADT against what he called the "politicization" of the issue by both Clintons. He cited discharge statistics for the Marines for the past 5 years that showed 75% were based on "voluntary admission of homosexuality" and 49% occurred during the first 6 months of service, when new recruits were most likely to reevaluate their decision to enlist. He also argued against any change in the policy, writing in the "New York Times": "Conduct that is widely rejected by a majority of Americans can undermine the trust that is essential to creating and maintaining the sense of unity that is critical to the success of a military organization operating under the very different and difficult demands of combat." The conviction of Winchell's murderer, according to the "New York Times", "galvanized opposition" to DADT, an issue that had "largely vanished from public debate". Opponents of the policy focused on punishing harassment in the military rather than the policy itself, which Sen. Chuck Hagel defended on December 25: "The U.S. armed forces aren't some social experiment."

The principal candidates for the Democratic presidential nomination in 2000, Al Gore and Bill Bradley, both endorsed military service by open gays and lesbians, provoking opposition from high-ranking retired military officers, notably the recently retired commandant of the Marine Corps, Gen. Charles C. Krulak. He and others objected to Gore's statement that he would use support for ending DADT as a "litmus test" when considering candidates for the Joint Chiefs of Staff. The 2000 Democratic Party platform was silent on the issue, while the Republican Party platform that year said: "We affirm that homosexuality is incompatible with military service." Following the election of George W. Bush in 2000, observers expected him to avoid any changes to DADT, since his nominee for Secretary of State Colin Powell had participated in its creation.

In February 2004 members of the British Armed Forces, Lt Rolf Kurth and Lt Cdr Craig Jones along with Aaron Belkin, Director of the Center for the Study of Sexual Minorities in the Military met with members of Congress and spoke at the National Defense University. They spoke about their experience of the current situation in the UK. The UK lifted the gay ban on members serving in their forces in 2000.

In July 2004, the American Psychological Association issued a statement that DADT "discriminates on the basis of sexual orientation" and that "Empirical evidence fails to show that sexual orientation is germane to any aspect of military effectiveness including unit cohesion, morale, recruitment and retention." It said that the U.S. military's track record overcoming past racial and gender discrimination demonstrated its ability to integrate groups previously excluded. The Republican Party platform that year reiterated its support for the policy—"We affirm traditional military culture, and we affirm that homosexuality is incompatible with military service."—while the Democratic Party maintained its silence.

In February 2005, the Government Accountability Office released estimates of the cost of DADT. It reported at least $95.4 million in recruiting costs and at least $95.1 million for training replacements for the 9,488 troops discharged from 1994 through 2003, while noting that the true figures might be higher. In September, as part of its campaign to demonstrate that the military allowed open homosexuals to serve when its manpower requirements were greatest, the Center for the Study of Sexual Minorities in the Military (now the Palm Center) reported that army regulations allowed the active duty deployment of Army Reservists and National Guard troops who claim to be or who are accused of being gay. A U.S. Army Forces Command spokesperson said the regulation was intended to prevent Reservists and National Guard members from pretending to be gay to escape combat. Advocates of ending DADT repeatedly publicized discharges of highly trained gay and lesbian personnel, especially those in positions with critical shortages, including fifty-nine Arabic speakers and nine Persian speakers. Elaine Donnelly, president of the Center for Military Readiness, later argued that the military's failure to ask about sexual orientation at recruitment was the cause of the discharges: [Y]ou could reduce this number to zero
or near zero if the Department of Defense dropped Don't Ask, Don't Tell... We should not be training people who are not eligible to be in the Armed Forces."

In February 2006, a University of California Blue Ribbon Commission that included Lawrence Korb, a former assistant defense secretary during the Reagan administration, William Perry, Secretary of Defense in the Clinton administration, and professors from the United States Military Academy released their assessment of the GAO's analysis of the cost of DADT released a year earlier. The commission report stated that the GAO did not take into account the value the military lost from the departures. They said that that total cost was closer to $363 million, including $14.3 million for "separation travel" following a service member's discharge, $17.8 million for training officers, $252.4 million for training enlistees, and $79.3 million in recruiting costs.

In 2006, Soulforce, a national LGBT rights organization, organized its Right to Serve Campaign, in which gay men and lesbians in several cities attempted to enlist in the Armed Forces or National Guard. Donnelly of the Center for Military Readiness stated in September: "I think the people involved here do not have the best interests of the military at heart. They never have. They are promoting an agenda to normalize homosexuality in America using the military as a battering ram to promote that broader agenda." She said that "pro-homosexual activists ... are creating media events all over the country and even internationally."

In 2006, a speaking tour of gay former service members, organized by SLDN, Log Cabin Republicans, and Meehan, visited 18 colleges and universities. Patrick Guerriero, executive director of Log Cabin, thought the repeal movement was gaining "new traction" but "Ultimately", said, "we think it's going to take a Republican with strong military credentials to make a shift in the policy." Elaine Donnelly called such efforts "a big P.R. campaign" and said that "The law is there to protect good order and discipline in the military, and it's not going to change."

In December 2006, Zogby International released the results of a poll of military personnel conducted in October 2006 that found that 26% favored allowing gays and lesbians to serve openly in the military, 37% were opposed, while 37% expressed no preference or were unsure. Of respondents who had experience with gay people in their unit, 6% said their presence had a positive impact on their personal morale, 66% said no impact, and 28% said negative impact. Regarding overall unit morale, 3% said positive impact, 64% no impact, and 27% negative impact.

Retired Chairman of the Joint Chiefs of Staff General John Shalikashvili and former Senator and Secretary of Defense William Cohen opposed the policy in January 2007: "I now believe that if gay men and lesbians served openly in the United States military, they would not undermine the efficacy of the armed forces" Shalikashvili wrote. "Our military has been stretched thin by our deployments in the Middle East, and we must welcome the service of any American who is willing and able to do the job." Shalikashvili cited the recent "Zogby poll of more than 500 service members returning from Afghanistan and Iraq, three quarters of whom said they were comfortable interacting with gay people. The debate took a different turn in March when Gen. Peter Pace, Chairman of the Joint Chiefs of Staff, told the editorial board of the "Chicago Tribune" he supported DADT because "homosexual acts between two individuals are immoral and ... we should not condone immoral acts." His remarks became, according to the "Tribune", "a huge news story on radio, television and the Internet during the day and showed how sensitive the Pentagon's policy has become." Sen. John Warner, who backed DADT, said "I respectfully, but strongly, disagree with the chairman's view that homosexuality is immoral", and Pace expressed regret for expressing his personal views and said that DADT "does not make a judgment about the morality of individual acts." Massachusetts Governor Mitt Romney, then in the early stages of his campaign for the 2008 Republican presidential nomination, defended DADT:

That summer, after U.S. senator Larry Craig was arrested for lewd conduct in a men's restroom, conservative commentator Michael Medved argued that any liberalization of DADT would "compromise restroom integrity and security". He wrote: "The national shudder of discomfort and queasiness associated with any introduction of homosexual eroticism into public men's rooms should make us more determined than ever to resist the injection of those lurid attitudes into the even more explosive situation of the U.S. military."

In November 2007, 28 retired generals and admirals urged Congress to repeal the policy, citing evidence that 65,000 gay men and women were serving in the armed forces and that there were over a million gay veterans. On November 17, 2008, 104 retired generals and admirals signed a similar statement. In December, SLDN arranged for "60 Minutes" to interview Darren Manzella, an Army medic who served in Iraq after coming out to his unit.

On May 4, 2008, while Chairman of the Joint Chiefs of Staff Admiral Mike Mullen addressed the graduating cadets at West Point, a cadet asked what would happen if the next administration were supportive of legislation allowing gays to serve openly. Mullen responded, "Congress, and not the military, is responsible for DADT." Previously, during his Senate confirmation hearing in 2007, Mullen told lawmakers, "I really think it is for the American people to come forward, really through this body, to both debate that policy and make changes, if that's appropriate." He went on to say, "I'd love to have Congress make its own decisions" with respect to considering repeal.

In May 2009, when a committee of military law experts at the Palm Center, an anti-DADT research institute, concluded that the President could issue an Executive Order to suspend homosexual conduct discharges, Obama rejected that option and said he wanted Congress to change the law.

On July 5, 2009, Colin Powell told CNN that the policy was "correct for the time" but that "sixteen years have now gone by, and I think a lot has changed with respect to attitudes within our country, and therefore I think this is a policy and a law that should be reviewed." Interviewed for the same broadcast, Mullen said the policy would continue to be implemented until the law was repealed, and that his advice was to "move in a measured way... At a time when we're fighting two conflicts there is a great deal of pressure on our forces and their families." In September, "Joint Force Quarterly" published an article by an Air Force colonel that disputed the argument that unit cohesion is compromised by the presence of openly gay personnel.

In October 2009, the Commission on Military Justice, known as the Cox Commission, repeated its 2001 recommendation that Article 125 of the Uniform Code of Military Justice, which bans sodomy, be repealed, noting that "most acts of consensual sodomy committed by consenting military personnel are not prosecuted, creating a perception that prosecution of this sexual behavior is arbitrary."

In January 2010, the White House and congressional officials started work on repealing the ban by inserting language into the 2011 defense authorization bill. During Obama's State of the Union Address on January 27, 2010, he said that he would work with Congress and the military to enact a repeal of the gay ban law and for the first time set a timetable for repeal.

At a February 2, 2010, congressional hearing, Senator John McCain read from a letter signed by "over one thousand former general and flag officers". It said: "We firmly believe that this law, which Congress passed to protect good order, discipline and morale in the unique environment of the armed forces, deserves continued support." The signature campaign had been organized by Elaine Donnelly of the Center for Military Readiness, a longtime supporter of a traditional all-male and all-heterosexual military. Servicemembers United, a veterans group opposed to DADT, issued a report critical of the letter's legitimacy. They said that among those signing the letter were officers who had no knowledge of their inclusion or who had refused to be included, and even one instance of a general's widow who signed her husband's name to the letter though he had died before the survey was published. The average age of the officers whose names were listed as signing the letter was 74, the oldest was 98, and Servicemembers United noted that "only a small fraction of these officers have even served in the military during the 'Don't Ask, Don't Tell' period, much less in the 21st century military."

The Center for American Progress issued a report in March 2010 that said a smooth implementation of an end to DADT required eight specified changes to the military's internal regulations. On March 25, 2010, Defense Secretary Gates announced new rules mandating that only flag officers could initiate discharge proceedings and imposing more stringent rules of evidence on discharge proceedings.

The underlying justifications for DADT have been subjected to increasing suspicion and outright rejection by the early 21st century. Mounting evidence obtained from the integration efforts of foreign militaries, surveys of U.S. military personnel, and studies conducted by the DoD gave credence to the view that the presence of open homosexuals within the military would not be detrimental at all to the armed forces. A DoD study conducted at the behest of Secretary of Defense Robert Gates in 2010 supports this most.

The DoD working group conducting the study considered the impact that lifting the ban would have on unit cohesion and effectiveness, good order and discipline, and military morale. The study included a survey that revealed significant differences between respondents who believed they had served with homosexual troops and those who did not believe they had. In analyzing such data, the DoD working group concluded that it was actually generalized perceptions of homosexual troops that led to the perceived unrest that would occur without DADT. Ultimately, the study deemed the overall risk to military effectiveness of lifting the ban to be low. Citing the ability of the armed forces to adjust to the previous integration of African-Americans and women, the DoD study asserted that the United States military could adjust as had it before in history without an impending serious effect.

In March 2005, Rep. Martin T. Meehan introduced the Military Readiness Enhancement Act in the House. It aimed "to amend title 10, United States Code, to enhance the readiness of the Armed Forces by replacing the current policy concerning homosexuality in the Armed Forces, referred to as 'Don't ask, don't tell,' with a policy of nondiscrimination on the basis of sexual orientation". As of 2006, it had 105 Democrats and 4 Republicans as co-sponsors. He introduced the bill again in 2007 and 2009.

During the 2008 U.S. presidential election campaign, Senator Barack Obama advocated a full repeal of the laws barring gays and lesbians from serving in the military. Nineteen days after his election, Obama's advisers announced that plans to repeal the policy might be delayed until 2010, because Obama "first wants to confer with the Joint Chiefs of Staff and his new political appointees at the Pentagon to reach a consensus, and then present legislation to Congress". As president he advocated a policy change to allow gay personnel to serve openly in the armed forces, stating that the U.S. government has spent millions of dollars replacing troops expelled from the military, including language experts fluent in Arabic, because of DADT. On the eve of the National Equality March in Washington, D.C., October 10, 2009, Obama stated in a speech before the Human Rights Campaign that he would end the ban, but he offered no timetable. Obama said in his 2010 State of the Union Address: "This year, I will work with Congress and our military to finally repeal the law that denies gay Americans the right to serve the country they love because of who they are." This statement was quickly followed up by Defense Secretary Robert Gates and Joint Chiefs Chairman Michael Mullen voicing their support for a repeal of DADT.

Democrats in both houses of Congress first attempted to end DADT by amending the Defense Authorization Act. On May 27, 2010, on a 234–194 vote, the U.S. House of Representatives approved the Murphy amendment to the National Defense Authorization Act for Fiscal Year 2011. It provided for repeal of the DADT policy and created a process for lifting the policy, including a U.S. Department of Defense study and certification by key officials that the change in policy would not harm military readiness followed by a waiting period of 60 days. The amended defense bill passed the House on May 28, 2010. On September 21, 2010, John McCain led a successful filibuster against the debate on the Defense Authorization Act, in which 56 Senators voted to end debate, four short of the 60 votes required. Some advocates for repeal, including the Palm Center, OutServe, and Knights Out, opposed any attempt to block the passage of NDAA if it failed to include DADT repeal language. The Human Rights Campaign, the Center for American Progress, Servicemembers United and SLDN refused to concede that possibility.
The American Civil Liberties Union (ACLU) filed a lawsuit, "Collins v. United States", against the Department of Defense in November 2010 seeking full compensation for those discharged under the policy.

On November 30, 2010, the Joint Chiefs of Staff released the "Don't Ask, Don't Tell" Comprehensive Review Working Group (CRWG) report authored by Jeh C. Johnson, General Counsel of the Department of Defense, and Army General Carter F. Ham. It outlined a path to the implementation of repeal of DADT. The report indicated that there was a low risk of service disruptions due to repealing the ban, provided time was provided for proper implementation and training. It included the results of a survey of 115,000 active-duty and reserve service members. Across all service branches, 30 percent thought that integrating gays into the military would have negative consequences. In the Marine Corps and combat specialties, the percentage with that negative assessment ranged from 40 to 60 percent. The CRWG also said that 69 percent of all those surveyed believed they had already worked with a gay or lesbian and of those, 92 percent reported that the impact of that person's presence was positive or neutral. The same day, in response to the CRWG, 30 professors and scholars, most from military institutions, issued a joint statement saying that the CRWG "echoes more than 20 studies, including studies by military researchers, all of which reach the same conclusion: allowing gays and lesbians to serve openly will not harm the military ... We hope that our collective statement underscores that the debate about the evidence is now officially over..." The Family Research Council's president, Tony Perkins, interpreted the CRWG data differently, writing that it "reveals that 40 percent of Marines and 25 percent of the Army could leave".

Gates encouraged Congress to act quickly to repeal the law so that the military could carefully adjust rather than face a court decision requiring it to lift the policy immediately. The United States Senate held two days of hearings on December 2 and 3, 2010, to consider the CRWG report. Defense Secretary Robert Gates, Joint Chiefs Chairman Michael Mullen urged immediate repeal. The heads of the Marine Corps, Army, and Navy all advised against immediate repeal and expressed varied views on its eventual repeal. Oliver North, writing in "National Review" the next week, said that Gates' testimony showed "a deeply misguided commitment to political correctness". He interpreted the CRWG's data as indicating a high risk that large numbers of resignations would follow the repeal of DADT. Service members, especially combat troops, he wrote, "deserve better than to be treated like lab rats in Mr. Obama's radical social experiment".

On December 9, 2010, another filibuster prevented debate on the Defense Authorization Act. In response to that vote, Senators Joe Lieberman and Susan Collins introduced a bill that included the policy-related portions of the Defense Authorization Act that they considered more likely to pass as a stand-alone bill. It passed the House on a vote of 250 to 175 on December 15, 2010. On December 18, 2010, the Senate voted to end debate on its version of the bill by a cloture vote of 63–33. The final Senate vote was held later that same day, with the measure passing by a vote of 65–31.

U.S. Secretary of Defense Robert Gates released a statement following the vote indicating that the planning for implementation of a policy repeal would begin right away and would continue until Gates certified that conditions were met for orderly repeal of the policy. President Obama signed the repeal into law on December 22, 2010.

The repeal act established a process for ending the DADT policy. The President, the Secretary of Defense and the Chairman of the Joint Chiefs of Staff were required to certify in writing that they had reviewed the Pentagon's report on the effects of DADT repeal, that the appropriate regulations had been reviewed and drafted, and that implementation of repeal regulations "is consistent with the standards of military readiness, military effectiveness, unit cohesion, and recruiting and retention of the Armed Forces". Once certification was given, DADT would be lifted after a 60-day waiting period.

Representative Duncan D. Hunter announced plans in January 2011 to introduce a bill designed to delay the end of DADT. His proposed legislation required all of the chiefs of the armed services to submit the certification at the time required only of the President, Defense Secretary and Joint Chiefs Chairman. In April, Perkins of the Family Research Council argued that the Pentagon was misrepresenting its own survey data and that hearings by the House Armed Services Committee, now under Republican control, could persuade Obama to withhold certification. Congressional efforts to prevent the change in policy from going into effect continued into May and June 2011.

On January 29, 2011, Pentagon officials stated that the training process to prepare troops for the end of DADT would begin in February and would proceed quickly, though they suggested that it might not be completed in 2011. On the same day, the DOD announced it would not offer any additional compensation to service members who had been discharged under DADT, who received half of the separation pay other honorably discharged service members received.

In May 2011, the U.S. Army reprimanded three colonels for performing a skit in March 2011 at a function at Yongsan Garrison, South Korea, that mocked the repeal.

In May 2011, revelations that an April Navy memo relating to its DADT training guidelines contemplated allowing same-sex weddings in base chapels and allowing chaplains to officiate if they so chose resulted in a letter of protest from 63 Republican congressman, citing the Defense of Marriage Act (DOMA) as controlling the use of federal property. Tony Perkins of the Family Research Council said the guidelines "make it even more uncomfortable for men and women of faith to perform their duties". A Pentagon spokesperson replied that DOMA "does not limit the type of religious ceremonies a chaplain may perform in a chapel on a military installation", and a Navy spokesperson said that "A chaplain can conduct a same-sex ceremony if it is in the tenets of his faith". A few days later the Navy rescinded its earlier instructions "pending additional legal and policy review and interdepartmental coordination".

While waiting for certification, several service members were discharged at their own insistence until a July 6 ruling from a federal appeals court barred further enforcement of the U.S. military's ban on openly gay service members, which the military promptly did.

Anticipating the lifting of DADT, some active duty service members wearing civilian clothes marched in San Diego's gay pride parade on July 16. The DOD noted that participation "does not constitute a declaration of sexual orientation".

President Obama, Secretary of Defense Leon Panetta, and Admiral Mike Mullen, Chairman of the Joint Chiefs of Staff, sent the certification required by the Repeal Act to Congress on July 22, 2011, setting the end of DADT for September 20, 2011. A Pentagon spokesman said that service members discharged under DADT would be able to re-apply to rejoin the military then.

At the end of August 2011, the DOD approved the distribution of the magazine produced by OutServe, an organization of gay and lesbian service members, at Army and Air Force base exchanges beginning with the September 20 issue, coinciding with the end of DADT.

On September 20, Air force officials announced that 22 Air Force Instructions were "updated as a result of the repeal of DADT". On September 30, 2011, the Department of Defense modified regulations to reflect the repeal by deleting "homosexual conduct" as a ground for administrative separation.

On the eve of repeal, US Air Force 1st Lt. Josh Seefried, one of the founders of OutServe, an organization of LGBT troops, revealed his identity after two years of hiding behind a pseudonym. Senior Airman Randy Phillips, after conducting a social media campaign seeking encouragement coming out and already out to his military co-workers, came out to his father on the evening of September 19. When the video of their conversation he posted on YouTube went viral, it made him, in one journalist's estimation, "the poster boy for the DADT repeal". The moment the repeal took effect at midnight on September 19, US Navy Lt. Gary C. Ross married his same-sex partner of eleven and a half years, Dan Swezy, making them the first same-sex military couple to legally marry in the United States. Retired Rear Adm. Alan M. Steinman became the highest-ranking person to come out immediately following the end of DADT. HBO produced a World of Wonder documentary, "The Strange History of Don't Ask, Don't Tell", and premiered it on September 20. "Variety" called it "an unapologetic piece of liberal advocacy" and "a testament to what formidable opponents ignorance and prejudice can be". Discharge proceedings on the grounds of homosexuality, some begun years earlier, came to an end.

In the weeks that followed, a series of firsts attracted press attention to the impact of the repeal. The Marine Corps were the first branch of the armed services to recruit from the LGBTQ community. Reservist Jeremy Johnson became the first person discharged under DADT to re-enlist. Jase Daniels became the first to return to active duty, re-joining the Navy as a third class petty officer. On December 2, Air Force intelligence officer Ginger Wallace became the first open LGBT service member to have a same-sex partner participate in the "pinning-on" ceremony that marked her promotion to colonel. On December 23, after 80 days at sea, US Navy Petty Officer 2nd Class Marissa Gaeta won the right to the traditional "first kiss" upon returning to port and shared it with her same-sex partner. On January 20, 2012, U.S. service members deployed to Bagram, Afghanistan, produced a video in support of the It Gets Better Project, which aims to support LGBT at-risk youth. Widespread news coverage continued even months after the repeal date, when a photograph of Marine Sgt. Brandon Morgan kissing his partner at a February 22, 2012, homecoming celebration on Marine Corps Base Hawaii went viral. When asked for her comment, a spokesperson for the Marine Corps said: "It's your typical homecoming photo."
On September 30, 2011, Under Secretary of Defense Clifford Stanley announced the DOD's policy that military chaplains are allowed to perform same-sex marriages "on or off a military installation" where local law permits them. His memo noted that "a chaplain is not required to participate in or officiate a private ceremony if doing so would be in variance with the tenets of his or her religion" and "a military chaplain's participation in a private ceremony does not constitute an endorsement of the ceremony by DoD". Some religious groups announced that their chaplains would not participate in such weddings, including an organization of evangelical Protestants, the Chaplain Alliance for Religious Liberty and Roman Catholics led by Archbishop Timothy Broglio of the Archdiocese for the Military Services, USA.

In late October 2011, speaking at the Air Force Academy, Col. Gary Packard, leader of the team that drafted the DOD's repeal implementation plan, said: "The best quote I've heard so far is, 'Well, some people's Facebook status changed, but that was about it.'" In late November, discussing the repeal of DADT and its implementation, Marine Gen. James F. Amos said "I'm very pleased with how it has gone" and called it a "non-event". He said his earlier public opposition was appropriate based on ongoing combat operations and the negative assessment of the policy given by 56% of combat troops under his command in the Department of Defense's November 2010 survey. A Defense Department spokesperson said implementation of repeal occurred without incident and added: "We attribute this success to our comprehensive pre-repeal training program, combined with the continued close monitoring and enforcement of standards by our military leaders at all levels."

In December 2011, Congress considered two DADT-related amendments in the course of work on the National Defense Authorization Act for 2012. The Senate approved 97-3, an amendment removing the prohibition on sodomy found in Article 125 of the Uniform Code of Military Justice as recommended by the Comprehensive Review Working Group (CRWG) a year earlier. The House approved an amendment banning same-sex marriages from being performed at military bases or by military employees, including chaplains and other employees of the military when "acting in an official capacity". Neither amendment appeared in the final legislation.

In July 2012, the Department of Defense granted permission for military personnel to wear their uniforms while participating in the San Diego Pride Parade. This was the first time that U.S. military personnel were permitted to wear their service uniforms in such a parade.

Marking the first anniversary of the passage of the Repeal Act, television news networks reported no incidents in the three months since DADT ended. One aired video of a social gathering for gay service members at a base in Afghanistan. Another reported on the experience of lesbian and gay troops, including some rejection after coming out to colleagues.

The Palm Center, a think tank that studies issues of sexuality and the military, released a study in September 2012 that found no negative consequences, nor any effect on military effectiveness from DADT repeal. This study began six months following repeal and concluded at the one year mark. The study included surveys of 553 generals and admirals who had opposed repeal, experts who supported DADT, and more than 60 heterosexual, gay, lesbian and bisexual active duty service personnel.

On January 7, 2013, the ACLU reached a settlement with the federal government in "Collins v. United States". It provided for the payment of full separation pay to service members discharged under DADT since November 10, 2004, who had previously been granted only half that.

Several candidates for the 2012 Republican presidential nomination called for the restoration of DADT, including Michele Bachmann, Rick Perry, and Rick Santorum. Newt Gingrich called for an extensive review of DADT's repeal.

Ron Paul, having voted for the Repeal Act, maintained his support for allowing military service by open homosexuals. Herman Cain called the issue "a distraction" and opposed reinstating DADT. Mitt Romney said that the winding down of military operations in Iraq and Afghanistan obviated his opposition to the repeal and said he was not proposing any change to policy.

On September 22, 2011, the audience at a Republican candidates' debate booed a U.S. soldier posted in Iraq who asked a question via video about the repeal of DADT, and none of the candidates noticed or responded to the crowd's behavior. Two days later, Obama commented on the incident while addressing a dinner of the Human Rights Campaign: "You want to be commander in chief? You can start by standing up for the men and women who wear the uniform of the United States, even when it's not politically convenient".

In June 2012, Rep. Howard McKeon, Republican chair of the House Armed Services Committee, said he considered the repeal of DADT a settled issue and if Romney became president would not advocate its reinstatement, though others in his party might.

In 1993, "Time" reported that 44% of those polled supported openly gay servicemembers, and in 1994, a CNN poll indicated 53% of Americans believed gays and lesbians should be permitted to serve openly.

According to a December 2010 "The Washington Post"-ABC News poll 77% of Americans said gays and lesbians who publicly disclose their sexual orientation should be able to serve in the military. That number showed little change from polls over the previous two years, but represented the highest level of support in a Post-ABC poll. The support also cut across partisan and ideological lines, with majorities of Democrats (86%), Republicans (74%), independents (74%), liberals (92%), conservatives (67%), white evangelical Protestants (70%) and non-religious (84%) in favor of homosexuals serving openly.

A November 2010 survey by the Pew Research Center found that 58% of the U.S. public favored allowing gays and lesbians to serve openly in the military, while less than half as many (27%) were opposed. According to a November 2010 CNN/Opinion Research Corporation poll, 72% of adult Americans favored permitting people who are openly gay or lesbian to serve in the military, while 23% opposed it. "The main difference between the CNN poll and the Pew poll is in the number of respondents who told pollsters that they didn't have an opinion on this topic – 16 percent in the Pew poll compared to only five percent in the CNN survey", said CNN Polling Director Keating Holland. "The two polls report virtually the same number who say they oppose gays serving openly in the military, which suggests that there are some people who favor that change in policy but for some reason were reluctant to admit that to the Pew interviewers. That happens occasionally on topics where moral issues and equal-treatment issues intersect."

A February 2010 Quinnipiac University Polling Institute national poll showed 57% of American voters favored gays serving openly, compared to 36% opposed, while 66% said not allowing openly gay personnel to serve is discrimination, compared to 31% who did not see it as discrimination. A CBS News/"The New York Times" national poll done at the same time showed 58% of Americans favored gays serving openly, compared to 28% opposed.

Chaplain groups and religious organizations took various positions on DADT. Some felt that the policy needed to be withdrawn to make the military more inclusive. The Southern Baptist Convention battled the repeal of DADT, warning that their endorsements for chaplains might be withdrawn if the repeal took place. They took the position that allowing gay men and women to serve in the military without restriction would have a negative impact on the ability of chaplains who think homosexuality is a sin to speak freely regarding their religious beliefs. The Roman Catholic Church called for the retention of the policy, but had no plans to withdraw its priests from serving as military chaplains. Sixty-five retired chaplains signed a letter opposing repeal, stating that repeal would make it impossible for chaplains whose faith teaches that same-sex behavior is immoral to minister to military service members. Other religious organizations and agencies called the repeal of the policy a "non-event" or "non-issue" for chaplains, claiming that chaplains have always supported military service personnel, whether or not they agree with all their actions or beliefs.

After the policy was introduced in 1993, the military discharged over 13,000 troops from the military under DADT. The number of discharges per fiscal year under DADT dropped sharply after the September 11 attacks and remained comparatively low through to the repeal. Discharges exceeded 600 every year until 2009.

In November 2019, both Rhode Island and New York State signed into law and implemented restoring military benefits to gay and lesbian military veterans. An estimated approximately 100,000 individuals were affected by the "don't ask don't tell policy" (since it was repealed in September 2011).





</doc>
<doc id="8691" url="https://en.wikipedia.org/wiki?curid=8691" title="Divination">
Divination

Divination (from Latin "divinare", 'to foresee, to foretell, to predict, to prophesy', related to "divinus", 'divine'), or "to be inspired by a god," is the attempt to gain insight into a question or situation by way of an occultic, standardized process or ritual. Used in various forms throughout history, diviners ascertain their interpretations of how a querent should proceed by reading signs, events, or omens, or through alleged contact with a supernatural agency.
Divination can be seen as a systematic method with which to organize what appear to be disjointed, random facets of existence such that they provide insight into a problem at hand. If a distinction is to be made between divination and fortune-telling, divination has a more formal or ritualistic element and often contains a more social character, usually in a religious context, as seen in traditional African medicine. Fortune-telling, on the other hand, is a more everyday practice for personal purposes. Particular divination methods vary by culture and religion.

Divination has long been criticized. In the modern era, it has been dismissed by the scientific community and skeptics as being superstition; experiments do not support the idea that divination techniques can actually predict the future more reliably or precisely than would be possible without it. In antiquity it was attacked by philosophers such as the Academic Skeptic Cicero in "De Divinatione" and the Pyrrhonist Sextus Empiricus in "Against the Astrologers". The satirist, Lucian, devoted a witty essay to "Alexander the false prophet".

The Oracle of Amun at the Siwa Oasis was made famous when Alexander the Great visited it after conquering Egypt from Persia in 332 BC.

Both oracles and seers in ancient Greece practiced divination. Oracles were the conduits for the gods on earth; their prophecies were understood to be the will of the gods verbatim. Because of the high demand for oracle consultations and the oracles’ limited work schedule, they were not the main source of divination for the ancient Greeks. That role fell to the seers ().

Seers were not in direct contact with the gods; instead, they were interpreters of signs provided by the gods. Seers used many methods to explicate the will of the gods including extispicy, bird signs, etc. They were more numerous than the oracles and did not keep a limited schedule; thus, they were highly valued by all Greeks, not just those with the capacity to travel to Delphi or other such distant sites.

The disadvantage of seers was that only direct yes-or-no questions could be answered. Oracles could answer more generalized questions, and seers often had to perform several sacrifices in order to get the most consistent answer. For example, if a general wanted to know if the omens were proper for him to advance on the enemy, he would ask his seer both that question and if it were better for him to remain on the defensive. If the seer gave consistent answers, the advice was considered valid.

During battle, generals would frequently ask seers at both the campground (a process called the "hiera") and at the battlefield (called the "sphagia"). The hiera entailed the seer slaughtering a sheep and examining its liver for answers regarding a more generic question; the sphagia involved killing a young female goat by slitting its throat and noting the animal's last movements and blood flow. The battlefield sacrifice only occurred when two armies prepared for battle against each other. Neither force would advance until the seer revealed appropriate omens.

Because the seers had such power over influential individuals in ancient Greece, many were skeptical of the accuracy and honesty of the seers. The degree to which seers were honest depends entirely on the individual seers. Despite the doubt surrounding individual seers, the craft as a whole was well regarded and trusted by the Greeks.

The divination method of casting lots (Cleromancy) was used by the remaining eleven disciples of Jesus in to select a replacement for Judas Iscariot. Therefore, divination was arguably an accepted practice in the early church. However, divination became viewed as a pagan practice by Christian emperors during ancient Rome.

In 692 the Quinisext Council, also known as the "Council in Trullo" in the Eastern Orthodox Church, passed canons to eliminate pagan and divination practices. Fortune-telling and other forms of divination were widespread through the Middle Ages. In the constitution of 1572 and public regulations of 1661 of Kur-Saxony, capital punishment was used on those predicting the future. Laws forbidding divination practice continue to this day.

Småland is famous for Årsgång, a practice which occurred until the early 19th century in some parts of Småland. Generally occurring on Christmas and New Year's Eve, it is a practice in which one would fast and keep themselves away from light in a room until midnight to then complete a set of complex events to interpret symbols encountered throughout the journey to foresee the coming year.

Divination was a central component of ancient Mesoamerican religious life. Many Aztec gods, including central creator gods, were described as diviners and were closely associated with sorcery. Tezcatlipoca is the patron of sorcerers and practitioners of magic. His name means "smoking mirror," a reference to a device used for divinatory scrying. In the Mayan "Popol Vuh", the creator gods Xmucane and Xpiacoc perform divinatory hand casting during the creation of people.

Every civilization that developed in pre-Columbian Mexico, from the Olmecs to the Aztecs, practiced divination in daily life, both public and private. Scrying through the use of reflective water surfaces, mirrors, or the casting of lots were among the most widespread forms of divinatory practice. Visions derived from hallucinogens were another important form of divination, and are still widely used among contemporary diviners of Mexico. Among the more common hallucinogenic plants used in divination are morning glory, jimson weed, and peyote.

Although Japan retains a history of traditional and local methods of divination, such as "onmyōdō", contemporary divination in Japan, called "uranai", derives from outside sources. Contemporary methods of divination in Japan include both Western and Chinese astrology, geomancy or feng shui, tarot cards, I Ching (Book of Changes), and physiognomy (methods of reading the body to identify traits). Rather than indicate cultural appropriation, understood as inappropriate acts of appropriation by a dominant culture in the context of colonization or inequality, Japanese divination represents instances of unique and creative amalgamation of cultural elements. This concept may be referred to as syncretism, creolization, or cultural hybridity. In the example of feng shui, Japanese adaptations of feng shui extend outside the traditional form, featuring such hybrids as "car feng shui," "workplace feng shui," "makeup feng shui," and even "toilet feng shui."

In Japan, divination methods include Futomani from the Shinto tradition.

Personality typing as a form of divination has been prevalent in Japan since the 1980s. Various methods exist for divining personality type. Each attempt to reveal glimpses of an individual's destiny, productive and inhibiting traits, future parenting techniques, and compatibility in marriage. Personality type is increasingly important for young Japanese, who consider personality the driving factor of compatibility, given the ongoing marriage drought and birth rate decline in Japan.

An import to Japan, Chinese zodiac signs based on the birth year in 12 year cycles (rat, ox, tiger, hare, dragon, snake, horse, sheep, monkey, cock, dog, and boar) are frequently combined with other forms of divination, such as so-called 'celestial types' based on the planets (Saturn, Venus, Mars, Jupiter, Mercury, or Uranus). Personality can also be divined using cardinal directions, the four elements (water, earth, fire, air), and yin-yang. Names can also lend important personality information under name classification which asserts that names bearing certain Japanese vowel sounds (a, i, u, e, o) share common characteristics. Numerology, which utilizes methods of diving 'birth numbers' from significant numbers such as birth date, may also reveal character traits of individuals.

Individuals can also assess their own and others' personalities according to physical characteristics. Blood type remains a popular form of divination from physiology. Stemming from Western influences, body reading or "ninsou", determines personality traits based on body measurements. The face is the most commonly analyzed feature, with eye size, pupil shape, mouth shape, and eyebrow shape representing the most important traits. An upturned mouth may be cheerful, and a triangle eyebrow may indicate that someone is strong-willed.

Methods of assessment in daily life may include self-taken measurements or quizzes. As such, magazines targeted at women in their early-to-mid twenties feature the highest concentration of personality assessment guides. There are approximately 144 different women's magazines, known as "nihon zashi koukoku kyoukai", published in Japan aimed at this audience.

The adaptation of the Western divination method of tarot cards into Japanese culture presents a particularly unique example of contemporary divination as this adaptation mingles with Japan's robust visual culture. Japanese tarot cards are created by professional artists, advertisers, and fans of tarot. One tarot card collector claimed to have accumulated more than 1,500 Japan-made decks of tarot cards. 

Japanese tarot cards fall into diverse categories such as: 


The images on tarot cards may come from images from Japanese popular culture, such as characters from manga and anime including Hello Kitty, or may feature cultural symbols. Tarot cards may adapt the images of Japanese historical figures, such as high priestess Himiko (170–248CE) or imperial court wizard Abe no Seimei (921–1005CE) . Still others may feature images of cultural displacement, such as English knights, pentagrams, the Jewish Torah, or invented glyphs. The introduction of such cards began by the 1930s and reached prominence 1970s. Japanese tarot cards were originally created by men, often based on the Rider-Waite-Smith tarot published by the Rider Company in London in 1909. Since, the practice of Japanese tarot has become overwhelmingly feminine and intertwined with kawaii culture. Referring to the cuteness of tarot cards, Japanese model Kuromiya Niina was quoted as saying "because the images are cute, even holding them is enjoyable." While these differences exist, Japanese tarot cards function similarly to their Western counterparts. Cards are shuffled and cut into piles then used to forecast the future, for spiritual reflection, or as a tool for self-understanding.

As seen previous to this section, many different cultures around the world use divination as a way of understanding the future. The most common act of divination in the Bao’an village in Taiwan is called the Poe, Bao’an is not the actual name of the village, but for privacy purposes that is what it will be called. The Poe translated to English means “moon boards”. The Poe consists of two wood or bamboo blocks cut into the shape of a crescent moon. The one edge is rounded while the other is flat; the two are mirror images. Both crescents are held out in one's palms and while kneeling, they are raised to the forehead level. Once in this position, the blocks are dropped and the future can be understood depending on their landing. If both fall flat side up or both fall rounded side up, that can be taken as a failure of the deity to agree. If the blocks land one rounded and one flat, the deity agrees. “Laughing poe” is when rounded sides land down and they rock before coming to a standstill. “Negative poe” is seen when the flat sides fall downward and abruptly stop, this indicates anger. When there is a positive fall, it is called “sacred poe”, although the negative falls are not usually taken seriously. As the blocks are being dropped the question is said in a murmur, and if the answer is yes, the blocks are dropped again. To make sure the answer is definitely a yes, the blocks must fall in a “yes” position three times in a row.
A more serious type of divination is the Kiō-á. There is a small wooden chair, and around the sides of the chair are small pieces of wood that can move up and down in their sockets, this causes a clicking sounds when the chair is moved in any way. Two men hold this chair by its legs before an altar, while the incense is being burned, and the supernatural agent is asked to descend into the chair. It is seen that it is in the chair by an onset of motion. Eventually, the chair crashes onto a table prepared with wood chips and burlap. The characters on the table are then traced and these are said to be written by the god who possessed the chair, these characters are then interpreted.

Divination is widespread throughout Africa. Among many examples it is one of central tenets of Serer religion in Senegal. Only those who have been initiated as Saltigues (the Serer high priests and priestesses) can divine the future. These are the "hereditary rain priests" whose role is both religious and medicinal.

Specialized diviners called Ob'guega (doctor of Oguega oracle), as well as Ob'Oronmila (doctor of Oronmila oracle) from the Edo people of West Africa for thousands, have used divination as a means of foretelling the past, present and future. These diviners are initiated and trained in Iha (divination) of either Ominigbon or Oronmila (Benin Orunmila).

The Yoruba people of West Africa are internationally known for having developed the Ifá system, an intricate process of divination that is performed by an "Awo", an initiated priest or priestess of Orunmila, the spirit of the Yoruba oracle.





</doc>
<doc id="8693" url="https://en.wikipedia.org/wiki?curid=8693" title="Diets of Nuremberg">
Diets of Nuremberg

The Diets of Nuremberg, also called the Imperial Diets of Nuremberg, took place at different times between the Middle Ages and the 17th century. 

The first Diet of Nuremberg, in 1211, elected the future emperor Frederick II of Hohenstaufen as German king.

At the Diet of 1356 the Emperor Charles IV issued the Golden Bull of 1356, which required each Holy Roman Emperor to summon the first Imperial Diet after his election at Nuremberg. Apart from that, a number of other diets were held there.

Important to Protestantism were the Diets of 1522 ("First Diet of Nuremberg"), 1524 ("Second Diet of Nuremberg") and 1532 ("Third Diet of Nuremberg").

This Diet has become known mostly for the reaction of the papacy to the decision made on Luther at the Diet of Worms the previous year. The new pope, Adrian VI, sent his nuncio Francesco Chieregati to the Diet, to insist both that the Edict of Worms be executed, and that action be taken promptly against Luther. This demand, however, was coupled with a promise of thorough reform in the Roman hierarchy, frankly admitting the partial guilt of the Vatican in the decline of the Church.

In the recess drafted on 9 February 1523, however, the German princes rejected this appeal. Using Adrian's admissions, they declared that they could not have it appear 'as though they wished to oppress evangelical truth and assist unchristian and evil abuses.'

This Diet generally took the same line as the previous one. The Estates reiterated their decision from the previous Diet. The Cardinal-legate, Campeggio, who was present, showed his disgust at the behaviour of the Estates. On 18 April, the Estates decided to call 'a general gathering of the German nation', to meet at Speyer the following year and to decide what would be done until the meeting of the general council of the Church which they demanded. This resulted in the Diet of Speyer (1526), which in turn was followed by the Diet of Speyer (1529). The latter included the Protestation at Speyer.



</doc>
<doc id="8695" url="https://en.wikipedia.org/wiki?curid=8695" title="Dr. Strangelove">
Dr. Strangelove

Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, more commonly known simply as Dr. Strangelove, is a 1964 black comedy film that satirizes the Cold War fears of a nuclear conflict between the Soviet Union and the United States. The film was directed, produced, and co-written by Stanley Kubrick and stars Peter Sellers, George C. Scott, Sterling Hayden, and Slim Pickens. Production took place in the United Kingdom. The film is loosely based on Peter George's thriller novel "Red Alert" (1958).

The story concerns an unhinged United States Air Force general who orders a first strike nuclear attack on the Soviet Union. It separately follows the President of the United States, his advisors, the Joint Chiefs of Staff and a Royal Air Force (RAF) exchange officer as they attempt to prevent the crew of a B-52 plane (who were ordered by the general) from nuking the Soviets and starting an atomic holocaust.

The film is often considered one of the best comedies ever made, as well as one of the greatest films of all time. In 1998, the American Film Institute ranked it twenty-sixth in its list of the best American movies (in the 2007 edition, the film ranked thirty-ninth), and in 2000, it was listed as number three on its list of the funniest American films. In 1989, the United States Library of Congress included "Dr. Strangelove" as one of the first twenty-five films selected for preservation in the National Film Registry for being "culturally, historically, or aesthetically significant". 

United States Air Force Brigadier General Jack D. Ripper is commander of Burpelson Air Force Base, which houses the Strategic Air Command (SAC) 843rd Bomb Wing, flying B-52 bombers armed with hydrogen bombs. The 843rd Wing is flying on airborne alert, two hours from their targets inside the USSR.

General Ripper orders his executive officer, Group Captain Lionel Mandrake of the UK Royal Air Force, to put the base on alert, and to issue "Wing Attack Plan R" to the patrolling aircraft, one of which is commanded by Major T. J. "King" Kong. All of the aircraft commence an attack flight on the USSR and set their radios to allow communications only through their CRM 114 discriminators, which was designed to accept only communications preceded by a secret three-letter code known only to General Ripper. Mandrake discovers that no war order has been issued by the Pentagon and he tries to stop Ripper, who locks them both in his office. Ripper tells Mandrake that he believes the Soviets have been using fluoridation of the American water supplies to pollute the "precious bodily fluids" of Americans. Mandrake realizes that Ripper has gone insane.

In the War Room at the Pentagon, General Buck Turgidson briefs President Merkin Muffley and other officers about how Plan R enables a senior officer to launch a strike against the Soviets if all superiors have been killed in a first strike on the United States. Turgidson reports that his men are trying every possible three-letter CRM code to issue the stand-down order, but that could take over two days and the planes are due to reach their targets in a couple of hours. Muffley orders the U.S. Army to storm the base and arrest General Ripper. Turgidson then attempts to convince Muffley to let the attack continue, but Muffley refuses to be party to a nuclear first strike. Instead, he brings Soviet ambassador Alexei de Sadeski into the War Room to telephone Soviet Premier Dimitri Kissov on the "hot line". Muffley warns the Premier of the impending attack and offers to reveal the positions of the bombers and targets so that the Soviets can protect themselves.

After a heated discussion in Russian with the Premier, the ambassador informs President Muffley that the Soviet Union has created a doomsday machine, which consists of many buried bombs jacketed with "cobalt-thorium G" connected to a computer network set to detonate them automatically should any nuclear attack strike the country. Within two months after detonation, the cobalt-thorium G would encircle the planet in a radioactive "doomsday shroud", wiping out all human and animal life, and rendering the surface of the Earth uninhabitable. The device cannot be deactivated, as it is programmed to explode if any such attempt is made. When the President's wheelchair-bound scientific advisor, the former Nazi German Dr. Strangelove, points out that such a doomsday machine would only be an effective deterrent if everyone knew about it, de Sadeski replies that the Soviet Premier had planned to reveal its existence to the world the following week.

Meanwhile, U.S. Army troops arrive at Burpelson, and General Ripper shoots and kills himself. Mandrake identifies Ripper's CRM code from his desk blotter ("OPE," a variant of both "Peace on Earth" and "Purity of Essence") and relays this code to the Pentagon. Using the recall code, SAC successfully recalls all of the bombers except one whose radio equipment has been destroyed. The Soviets attempt to find it, but its commanding officer, Major Kong, with his fuel dwindling, has switched to a closer backup target. As the plane approaches the new target, the crew is unable to open the damaged bomb bay doors. Kong enters the bomb bay and repairs the broken electrical wiring while sitting on the H-bomb, whereupon the doors open and the bomb is dropped. With Kong straddling it, the bomb falls and detonates over a Soviet missile site.

Back in the War Room, Dr. Strangelove recommends that the President gather several hundred thousand people to live in deep underground mines where the radiation will not penetrate. He suggests a 10:1 female-to-male ratio for a breeding program to repopulate the Earth once the radiation has subsided. Turgidson, worried that the Soviets will do the same, warns about a "mineshaft gap", while Alexei secretly photographs the war room. Dr. Strangelove declares he has a plan, but then rises from his wheelchair and announces "Mein Führer, I can walk!" as the Doomsday Machine activates. The film ends with a montage of many nuclear explosions, accompanied by Vera Lynn's version of the World War II song "We'll Meet Again".


Columbia Pictures agreed to finance the film if Peter Sellers played at least four major roles. The condition stemmed from the studio's opinion that much of the success of Kubrick's previous film "Lolita" (1962) was based on Sellers's performance in which his single character assumes a number of identities. Sellers had also played three roles in "The Mouse That Roared" (1959). Kubrick accepted the demand, later explaining that "such crass and grotesque stipulations are the "sine qua non" of the motion-picture business".

Sellers ended up playing three of the four roles written for him. He had been expected to play Air Force Major T. J. "King" Kong, the B-52 Stratofortress aircraft commander, but from the beginning, Sellers was reluctant. He felt his workload was too heavy, and he worried he would not properly portray the character's Texas accent. Kubrick pleaded with him, and he asked the screenwriter Terry Southern (who had been raised in Texas) to record a tape with Kong's lines spoken in the correct accent. Using Southern's tape, Sellers managed to get the accent right, and he started acting in the scenes in the aircraft, but then sprained his ankle and he could not work in the cramped cockpit set.

Sellers is said to have improvised much of his dialogue, with Kubrick incorporating the ad-libs into the written screenplay so the improvised lines became part of the canonical screenplay, a practice known as retroscripting. 

According to film critic Alexander Walker, the author of biographies of both Sellers and Kubrick, the role of Group Captain Lionel Mandrake was the easiest of the three for Sellers to play, since he was aided by his experience of mimicking his superiors while serving in the Royal Air Force during World War II. There is also a heavy resemblance to Sellers' friend and occasional co-star Terry-Thomas and the prosthetic-limbed RAF ace Sir Douglas Bader.

For his performance as President Merkin Muffley, Sellers assumed the accent of an American Midwesterner. Sellers drew inspiration for the role from Adlai Stevenson, a former Illinois governor who was the Democratic candidate for the 1952 and 1956 presidential elections and the U.N. ambassador during the Cuban Missile Crisis.

In early takes, Sellers faked cold symptoms to emphasize the character's apparent weakness. That caused frequent laughter among the film crew, ruining several takes. Kubrick ultimately found this comic portrayal inappropriate, feeling that Muffley should be a serious character. In later takes Sellers played the role straight, though the President's cold is still evident in several scenes.

In keeping with Kubrick's satirical character names, a "merkin" is a pubic hair wig. The president is bald, and his last name is "Muffley"; both are additional homages to a merkin.

Dr. Strangelove is an ex-Nazi scientist, suggesting Operation Paperclip, the US effort to recruit top German technical talent at the end of World War II. He serves as President Muffley's scientific adviser in the War Room. When General Turgidson wonders aloud what kind of name "Strangelove" is, saying to Mr. Staines (Jack Creley) that it is not a "Kraut name", Staines responds that Strangelove's original German surname was "Merkwürdigliebe" ("Strange love" in German) and that "he changed it when he became a citizen". Twice in the film, Strangelove accidentally addresses the president as "Mein Führer". Dr. Strangelove did not appear in the book "Red Alert".

The character is an amalgamation of RAND Corporation strategist Herman Kahn, mathematician and Manhattan Project principal John von Neumann, rocket scientist Wernher von Braun (a central figure in Nazi Germany's rocket development program recruited to the US after the war), and Edward Teller, the "father of the hydrogen bomb". It has been claimed that the character was based on Henry Kissinger, but Kubrick and Sellers denied this; Sellers said, "Strangelove was never modeled after Kissinger—that's a popular misconception. It was always Wernher von Braun." Furthermore, Henry Kissinger points out in his memoirs that at the time of the writing of "Dr. Strangelove", he was an unknown academic.

The wheelchair-using Strangelove furthers a Kubrick trope of the menacing, seated antagonist, first depicted in "Lolita" through the character "Dr. Zaempf". Strangelove's accent was influenced by that of Austrian-American photographer Weegee, who worked for Kubrick as a special photographic effects consultant. Strangelove's appearance echoes the mad scientist archetype as seen in the character Rotwang in Fritz Lang's film "Metropolis" (1927). Sellers's Strangelove takes from Rotwang the single black gloved hand (which, in Rotwang's case is mechanical, because of a lab accident), the wild hair and, most important, his ability to avoid being controlled by political power. According to Alexander Walker, Sellers improvised Dr. Strangelove's lapse into the Nazi salute, borrowing one of Kubrick's black leather gloves for the uncontrollable hand that makes the gesture. Dr. Strangelove apparently suffers from alien hand syndrome. Kubrick wore the gloves on the set to avoid being burned when handling hot lights, and Sellers, recognizing the potential connection to Lang's work, found them to be menacing.

Slim Pickens, an established character actor and veteran of many Western films, was eventually chosen to replace Sellers as Major Kong after Sellers' injury. Terry Southern's biographer, Lee Hill, said the part was originally written with John Wayne in mind, and that Wayne was offered the role after Sellers was injured, but he immediately turned it down. Dan Blocker of the "Bonanza" western television series was approached to play the part, but according to Southern, Blocker's agent rejected the script as being "too pinko". Kubrick then recruited Pickens, whom he knew from his brief involvement in a Marlon Brando western film project that was eventually filmed as "One-Eyed Jacks".

His fellow actor James Earl Jones recalls, "He was Major Kong on and off the set—he didn't change a thing—his temperament, his language, his behavior." Pickens was not told that the movie was a black comedy, and he was only given the script for scenes he was in, to get him to play it "straight".

Kubrick's biographer John Baxter explained, in the documentary "Inside the Making of Dr. Strangelove":
Pickens, who had previously played only supporting and character roles, said that his appearance as Maj. Kong greatly improved his career. He later commented, "After "Dr. Strangelove" the roles, the dressing rooms, and the checks all started getting bigger."

Kubrick tricked Scott into playing the role of Gen. Turgidson far more ridiculously than Scott was comfortable doing. Kubrick talked Scott into doing over-the-top "practice" takes, which Kubrick told Scott would never be used, as a way to warm up for the "real" takes. Kubrick used these takes in the final film, causing Scott to swear never to work with Kubrick again.

During the filming, Kubrick and Scott had different opinions regarding certain scenes, but Kubrick got Scott to conform largely by repeatedly beating him at chess, which they played frequently on the set. Scott, a skilled player himself, later said that while he and Kubrick may not have always seen eye to eye, he respected Kubrick immensely for his skill at chess.

Stanley Kubrick started with nothing but a vague idea to make a thriller about a nuclear accident that built on the widespread Cold War fear for survival. While doing research, Kubrick gradually became aware of the subtle and paradoxical "balance of terror" between nuclear powers. At Kubrick's request, Alastair Buchan (the head of the Institute for Strategic Studies) recommended the thriller novel "Red Alert" by Peter George. Kubrick was impressed with the book, which had also been praised by game theorist and future Nobel Prize in Economics winner Thomas Schelling in an article written for the "Bulletin of the Atomic Scientists" and reprinted in "The Observer", and immediately bought the film rights. In 2006, Schelling wrote that conversations between Kubrick, Schelling, and George in late 1960 about a treatment of "Red Alert" updated with intercontinental missiles eventually led to the making of the film.

In collaboration with George, Kubrick started writing a screenplay based on the book. While writing the screenplay, they benefited from some brief consultations with Schelling and, later, Herman Kahn. In following the tone of the book, Kubrick originally intended to film the story as a serious drama. However, as he later explained during interviews, he began to see comedy inherent in the idea of mutual assured destruction as he wrote the first draft. Kubrick said:

Among the titles that Kubrick considered for the film were "Dr. Doomsday or: How to Start World War III Without Even Trying", "Dr. Strangelove's Secret Uses of Uranus", and "Wonderful Bomb". After deciding to make the film a black comedy, Kubrick brought in Terry Southern as a co-writer in late 1962. The choice was influenced by reading Southern's comic novel "The Magic Christian", which Kubrick had received as a gift from Peter Sellers, and which itself became a Sellers film in 1969. Southern made important contributions to the film, but his role led to a rift between Kubrick and Peter George; after "Life" magazine published a photo-essay on Southern in August 1964 which implied that Southern had been the script's principal author—a misperception neither Kubrick nor Southern did much to dispel—Peter George wrote an indignant letter to the magazine, published in its September 1964 issue, in which he pointed out that he had both written the film's source novel and collaborated on various incarnations of the script over a period of ten months, whereas "Southern was briefly employed ... to do some additional rewriting for Kubrick and myself and fittingly received a screenplay credit in behind Mr. Kubrick and myself".

"Dr. Strangelove" was filmed at Shepperton Studios, near London, as Sellers was in the middle of a divorce at the time and unable to leave England. The sets occupied three main sound stages: the Pentagon War Room, the B-52 Stratofortress bomber and the last one containing both the motel room and General Ripper's office and outside corridor. The studio's buildings were also used as the Air Force base exterior. The film's set design was done by Ken Adam, the production designer of several "James Bond" films (at the time he had already worked on "Dr. No"). The black and white cinematography was by Gilbert Taylor, and the film was edited by Anthony Harvey and an uncredited Kubrick. The original musical score for the film was composed by Laurie Johnson and the special effects were by Wally Veevers. The theme of the chorus from the bomb run scene is a modification of "When Johnny Comes Marching Home". Sellers and Kubrick got on famously during the film's production and shared a love of photography.

For the War Room, Ken Adam first designed a two-level set which Kubrick initially liked, only to decide later that it was not what he wanted. Adam next began work on the design that was used in the film, an expressionist set that was compared with "The Cabinet of Dr. Caligari" and Fritz Lang's "Metropolis". It was an enormous concrete room ( long and wide, with a -high ceiling) suggesting a bomb shelter, with a triangular shape (based on Kubrick's idea that this particular shape would prove the most resistant against an explosion). One side of the room was covered with gigantic strategic maps reflecting in a shiny black floor inspired by dance scenes in Fred Astaire films. In the middle of the room there was a large circular table lit from above by a circle of lamps, suggesting a poker table. Kubrick insisted that the table would be covered with green baize (although this could not be seen in the black and white film) to reinforce the actors' impression that they are playing 'a game of poker for the fate of the world.' Kubrick asked Adam to build the set ceiling in concrete to force the director of photography to use only the on-set lights from the circle of lamps. Moreover, each lamp in the circle of lights was carefully placed and tested until Kubrick was happy with the result.

Lacking cooperation from the Pentagon in the making of the film, the set designers reconstructed the aircraft cockpit to the best of their ability by comparing the cockpit of a B-29 Superfortress and a single photograph of the cockpit of a B-52 and relating this to the geometry of the B-52's fuselage. The B-52 was state-of-the-art in the 1960s, and its cockpit was off-limits to the film crew. When some United States Air Force personnel were invited to view the reconstructed B-52 cockpit, they said that "it was absolutely correct, even to the little black box which was the CRM." It was so accurate that Kubrick was concerned about whether Adam's team had carried out all its research legally.

In several shots of the B-52 flying over the polar ice en route to Russia, the shadow of the actual camera plane, a Boeing B-17 Flying Fortress, is visible on the icecap below. The B-52 was a scale model composited into the Arctic footage, which was sped up to create a sense of jet speed. Home movie footage included in "Inside the Making of Dr. Strangelove" on the 2001 Special Edition DVD release of the film shows clips of the B-17 with a cursive "Dr. Strangelove" painted over the rear entry hatch on the right side of the fuselage.

In 1967, some of the flying footage from "Dr. Strangelove" was re-used in The Beatles' television film "Magical Mystery Tour". As told by editor Roy Benson in the BBC Radio Documentary "Celluloid Beatles", the production team of "Magical Mystery Tour" lacked footage to cover the sequence for the song "Flying". Benson had access to the aerial footage filmed for the B52 sequences of "Dr. Strangelove", which was stored at Shepperton Studios. The use of the footage prompted Kubrick to call Benson to complain.

"Red Alert" author Peter George collaborated on the screenplay with Kubrick and satirist Terry Southern. "Red Alert" was more solemn than its film version, and it did not include the character Dr. Strangelove, though the main plot and technical elements were quite similar. A novelization of the actual film, rather than a reprint of the original novel, was published by Peter George, based on an early draft in which the narrative is bookended by the account of aliens, who, having arrived at a desolated Earth, try to piece together what has happened. It was reissued in October 2015 by Candy Jar Books, featuring never-before-published material on Strangelove's early career.

During the filming of "Dr. Strangelove", Stanley Kubrick learned that "Fail Safe", a film with a similar theme, was being produced. Although "Fail Safe" was to be an ultrarealistic thriller, Kubrick feared that its plot resemblance would damage his film's box office potential, especially if it were released first. Indeed, the novel "Fail-Safe" (on which the film is based) is so similar to "Red Alert" that Peter George sued on charges of plagiarism and settled out of court. 
What worried Kubrick the most was that "Fail Safe" boasted the acclaimed director Sidney Lumet and the first-rate dramatic actors Henry Fonda as the American president and Walter Matthau as the advisor to the Pentagon, Professor Groeteschele. Kubrick decided to throw a legal wrench into "Fail Safe"s production gears. Lumet recalled in the documentary "Inside the Making of Dr. Strangelove": "We started casting. Fonda was already set ... which of course meant a big commitment in terms of money. I was set, Walter [Bernstein, the screenwriter] was set ... And suddenly, this lawsuit arrived, filed by Stanley Kubrick and Columbia Pictures."

Kubrick argued that "Fail Safe"s own source novel "Fail-Safe" (1960) had been plagiarized from Peter George's "Red Alert", to which Kubrick owned creative rights. He pointed out unmistakable similarities in intentions between the characters Groeteschele and Strangelove. The plan worked, and the suit was settled out of court, with the agreement that Columbia Pictures, which had financed and was distributing "Strangelove", also buy "Fail Safe", which had been an independently financed production. Kubrick insisted that the studio release his movie first, and "Fail Safe" opened eight months after "Dr. Strangelove", to critical acclaim but mediocre ticket sales.

The end of the film shows Dr. Strangelove exclaiming, ""Mein Führer," I can walk!" before cutting to footage of nuclear explosions, with Vera Lynn and her audience singing "We'll Meet Again". This footage comes from nuclear tests such as shot BAKER of Operation Crossroads at Bikini Atoll, the Trinity test, a test from Operation Sandstone and the hydrogen bomb tests from Operation Redwing and Operation Ivy. In some shots, old warships (such as the German heavy cruiser "Prinz Eugen"), which were used as targets, are plainly visible. In others, the smoke trails of rockets used to create a calibration backdrop can be seen.

Former "Goon Show" writer and friend of Sellers Spike Milligan was credited with suggesting Vera Lynn's song for the ending.

It was originally planned for the film to end with a scene that depicted everyone in the War Room involved in a pie fight. Accounts vary as to why the pie fight was cut. In a 1969 interview, Kubrick said, "I decided it was farce and not consistent with the satiric tone of the rest of the film." Critic Alexander Walker observed that "the cream pies were flying around so thickly that people lost definition, and you couldn't really say whom you were looking at." Nile Southern, son of screenwriter Terry Southern, suggested the fight was intended to be less jovial: "Since they were laughing, it was unusable, because instead of having that totally black, which would have been amazing, like, this blizzard, which in a sense is metaphorical for all of the missiles that are coming, as well, you just have these guys having a good old time. So, as Kubrick later said, 'it was a disaster of Homeric proportions.

A first test screening of the film was scheduled for November 22, 1963, the day of the assassination of John F. Kennedy. The film was just weeks from its scheduled premiere, but because of the assassination, the release was delayed until late January 1964, as it was felt that the public was in no mood for such a film any sooner.

During post-production, one line by Slim Pickens, "a fella could have a pretty good weekend in Dallas with all that stuff," was dubbed to change "Dallas" to "Vegas," since Dallas was where Kennedy was killed. The original reference to Dallas survives in the English audio of the French-subtitled version of the film.

The assassination also serves as another possible reason that the pie-fight scene was cut. In the scene, after Muffley takes a pie in the face, General Turgidson exclaims: "Gentlemen! Our gallant young president has been struck down in his prime!" Editor Anthony Harvey stated that the scene "would have stayed, except that Columbia Pictures were horrified, and thought it would offend the president's family." Kubrick and others have said that the scene had already been cut before preview night because it was inconsistent with the rest of the film.

In 1994, the film was rereleased. While the 1964 release used a 1.85:1 aspect ratio, the new print was in the slightly squarer 1.66:1 (5:3) ratio that Kubrick had originally intended.

"Dr. Strangelove" takes passing shots at numerous contemporary Cold War attitudes, such as the "missile gap", but it primarily focuses its satire on the theory of mutual assured destruction (MAD), in which each side is supposed to be deterred from a nuclear war by the prospect of a universal cataclysmic disaster regardless of who "won". Military strategist and former physicist Herman Kahn, in the book "On Thermonuclear War" (1960), used the theoretical example of a "doomsday machine" to illustrate the limitations of MAD, which was developed by John von Neumann.

The concept of such a machine is consistent with MAD doctrine when it is logically pursued to its conclusion. It thus worried Kahn that the military might like the idea of a doomsday machine and build one. Kahn, a leading critic of MAD and President Eisenhower's administration's doctrine of massive retaliation upon the slightest provocation by the USSR, considered MAD to be foolish bravado, and urged America to instead plan for proportionality, and thus even a limited nuclear war. With this logical reasoning, Kahn became one of the architects of the flexible response doctrine, which, while superficially resembling MAD, allowed for responding to a limited nuclear strike with a proportional, or calibrated, return of fire (see "On Escalation").

Kahn educated Kubrick on the concept of the semirealistic "cobalt-thorium G" doomsday machine, and then Kubrick used the concept for the film. Kahn in his writings and talks would often come across as cold and calculating, for example, with his use of the term "megadeaths" and in his willingness to estimate how many human lives the United States could lose and still rebuild economically. Kahn's cold analytical attitude towards millions of deaths is reflected in Turgidson's remark to the president about the outcome of a preemptive nuclear war: "Mr. President, I'm not saying we wouldn't get our hair mussed. But I do say no more than ten to twenty million killed, tops, uh, depending on the breaks." Turgidson has a binder that is labelled "World Targets in Megadeaths", a term coined in 1953 by Kahn and popularized in his 1960 book "On Thermonuclear War".

The post-hoc planning in the film, by Dr. Strangelove, done the MAD policy has clearly broken down, to keep the human race alive and to regenerate from populations sheltered in mineshafts, is a parody of those strict adherents of the MAD doctrine who are opposed to the prior creation of fallout shelters on ideological grounds. To such adherents, talk of survival takes the "Assured Destruction" out of "Mutual Assured Destruction", hence no preparations should be conducted for fear of "destabilizing" the MAD doctrine. Moreover, it is also somewhat of a parody of Nelson Rockefeller, Edward Teller, Herman Kahn, and Chet Holifield's November 1961 popularization of a similar plan to spend billions of dollars on a nationwide network of highly protective concrete-lined underground fallout shelters, capable of holding millions of people and to be built any such nuclear exchange began. These extensive and therefore wildly expensive preparations were the fullest conceivable implementation of President Kennedy's, month prior, September 1961 advocacy in favor of the comparatively more modest, individual and community fallout shelters, as it appeared in "Life" magazine, which was in the context of shelters being on the minds of the public at the time due to the Berlin Crisis. The Kennedy administration would later go on to expand the nascent United States civil defense efforts, including the assessment of millions of homes and to create a network of thousands of well known, black and yellow plaqued, community fallout shelters. This was done, not with a massive construction effort but by the relatively cheap re-purposing of existing buildings and stocking them with CD V-700 geiger counters etc. In 1962 the Kennedy administration would found the American Civil Defense Association to organize this, comparatively far more cost-effective, shelter effort.

The fallout-shelter-network proposal, mentioned in the film, with its inherently high radiation protection characteristics, has similarities and contrasts to that of the very real and robust Swiss civil defense network. Switzerland has an overcapacity of nuclear fallout shelters for the country's population size, and by law, new homes must still be built with a fallout shelter. If the US did that, it would violate the spirit of MAD and according to MAD adherents, allegedly destabilize the situation because the US could launch a first strike and its population would largely survive a retaliatory second strike (see MAD § Theory).

To refute early 1960s novels and Hollywood films like "Fail-Safe" and "Dr. Strangelove", which raised questions about US control over nuclear weapons, the Air Force produced a documentary film, "SAC Command Post", to demonstrate its responsiveness to presidential command and its tight control over nuclear weapons. However, later academic research into declassified documents showed that U.S. military commanders had been given presidentially-authorized pre-delegation for the use of nuclear weapons during the early Cold War, showing that that aspect of the film's plot was plausible.

The characters of Buck Turgidson and Jack Ripper both deride the real-life Gen. Curtis LeMay of the Strategic Air Command.

In the months following the film's release, director Stanley Kubrick received a fan letter from Legrace G. Benson of the Department of History of Art at Cornell University interpreting the film as being sexually-layered. The director wrote back to Benson and confirmed the interpretation, "Seriously, you are the first one who seems to have noticed the sexual framework from intromission (the planes going in) to the last spasm (Kong's ride down and detonation at target)."

Sexual metaphors often popped up when the nuclear analysts that Kubrick consulted were discussing strategy, such as when Bernard Brodie compared his not attacking cities/withhold plan following belligerent escalation to coitus interruptus in an internally circulated memorandum at the RAND Corporation (spoofed in the film as the "BLAND Corporation"), while he described the SAC plan of massive retaliation as "going all the way". That led RAND scholar Herman Kahn, whom Kubrick consulted, to quip to an assembled group of "massive retaliation" SAC officers, "Gentlemen, you do not have a war plan. You have a Wargasm!".

The film was a popular success, earning US$4,420,000 in rentals in North America during its initial theatrical release.

"Dr. Strangelove" is Kubrick's highest-rated film on Rotten Tomatoes, holding a 98% approval rating based on 88 reviews, with an average rating of 9.14/10. The site's critical consensus reads, "Stanley Kubrick's brilliant Cold War satire remains as funny and razor-sharp today as it was in 1964." The film also holds a score of 96 out of 100 on Metacritic, based on 11 reviews, indicating "universal acclaim". The film is ranked number 7 in the All-Time High Scores chart of Metacritic's Video/DVD section. It was selected for preservation in the United States National Film Registry.

"Dr. Strangelove" is on Roger Ebert's list of "The Great Movies", and he described it as "arguably the best political satire of the century". One of the most celebrated of all film comedies, it is the only comedy to make the top 10 in any of the "Sight & Sound" polls of best films. John Patterson of "The Guardian" wrote, "There had been nothing in comedy like "Dr Strangelove" ever before. All the gods before whom the America of the stolid, paranoid 50s had genuflected—the Bomb, the Pentagon, the National Security State, the President himself, Texan masculinity and the alleged Commie menace of water-fluoridation—went into the wood-chipper and never got the same respect ever again." It is also listed as number 26 on "Empire's 500 Greatest Movies of All Time", and in 2010 it was listed by "Time" magazine as one of the 100 best films since the publication's inception in 1923. The Writers Guild of America ranked its screenplay the 12th best ever written.

In 2000, readers of "Total Film" magazine voted it the 24th greatest comedic film of all time.

The film ranked #32 on "TV Guide"s list of the 50 Greatest Movies on TV (and Video).

American Film Institute included the film as #26 in AFI's 100 Years...100 Movies, #3 in AFI's 100 Years...100 Laughs, #64 in AFI's 100 Years...100 Movie Quotes ("Gentlemen, you can't fight in here! This is the War Room!") and #39 in AFI's 100 Years...100 Movies (10th Anniversary Edition).

In 1995, Kubrick enlisted Terry Southern to script a sequel titled "Son of Strangelove". Kubrick had Terry Gilliam in mind to direct. The script was never completed, but index cards laying out the story's basic structure were found among Southern's papers after he died in October 1995. It was set largely in underground bunkers, where Dr. Strangelove had taken refuge with a group of women.

In 2013, Gilliam commented, "I was told after Kubrick died—by someone who had been dealing with him—that he had been interested in trying to do another "Strangelove" with me directing. I never knew about that until after he died but I would have loved to."





</doc>
<doc id="8697" url="https://en.wikipedia.org/wiki?curid=8697" title="DNA ligase">
DNA ligase

DNA ligase is a specific type of enzyme, a ligase, () that facilitates the joining of DNA strands together by catalyzing the formation of a phosphodiester bond. It plays a role in repairing single-strand breaks in duplex DNA in living organisms, but some forms (such as DNA ligase IV) may specifically repair double-strand breaks (i.e. a break in both complementary strands of DNA). Single-strand breaks are repaired by DNA ligase using the complementary strand of the double helix as a template, with DNA ligase creating the final phosphodiester bond to fully repair the DNA.

DNA ligase is used in both DNA repair and DNA replication (see "Mammalian ligases"). In addition, DNA ligase has extensive use in molecular biology laboratories for recombinant DNA experiments (see "Research applications"). Purified DNA ligase is used in gene cloning to join DNA molecules together to form recombinant DNA.

The mechanism of DNA ligase is to form two covalent phosphodiester bonds between 3' hydroxyl ends of one nucleotide ("acceptor"), with the 5' phosphate end of another ("donor"). Two ATP molecules are consumed for each phosphodiester bond formed. AMP is required for the ligase reaction, which proceeds in four steps:


Ligase will also work with blunt ends, although higher enzyme concentrations and different reaction conditions are required.

The "E. coli" DNA ligase is encoded by the "lig" gene. DNA ligase in "E. coli", as well as most prokaryotes, uses energy gained by cleaving nicotinamide adenine dinucleotide (NAD) to create the phosphodiester bond. It does not ligate blunt-ended DNA except under conditions of molecular crowding with polyethylene glycol, and cannot join RNA to DNA efficiently.

The activity of E. coli DNA ligase can be enhanced by DNA polymerase at the right concentrations. Enhancement only works when the concentrations of the DNA polymerase 1 are much lower than the DNA fragments to be ligated. When the concentrations of Pol I DNA polymerases are higher, it has an adverse effect on E. coli DNA ligase

The DNA ligase from bacteriophage T4 (a bacteriophage that infects "Escherichia coli" bacteria). The T4 ligase is the most-commonly used in laboratory research. It can ligate either cohesive or blunt ends of DNA, oligonucleotides, as well as RNA and RNA-DNA hybrids, but not single-stranded nucleic acids. It can also ligate blunt-ended DNA with much greater efficiency than "E. coli" DNA ligase. Unlike "E. coli" DNA ligase, T4 DNA ligase cannot utilize NAD and it has an absolute requirement for ATP as a cofactor. Some engineering has been done to improve the "in vitro" activity of T4 DNA ligase; one successful approach, for example, tested T4 DNA ligase fused to several alternative DNA binding proteins and found that the constructs with either p50 or NF-kB as fusion partners were over 160% more active in blunt-end ligations for cloning purposes than wild type T4 DNA ligase. A typical reaction for inserting a fragment into a plasmid vector would use about 0.01 (sticky ends) to 1 (blunt ends) units of ligase. The optimal incubation temperature for T4 DNA ligase is 16 °C.

In mammals, there are four specific types of ligase.


DNA ligase from eukaryotes and some microbes uses adenosine triphosphate (ATP) rather than NAD.

Derived from a thermophilic bacterium, the enzyme is stable and active at much higher temperatures than conventional DNA ligases. Its half-life is 48 hours at 65 °C and greater than 1 hour at 95 °C. Ampligase DNA Ligase has been shown to be active for at least 500 thermal cycles (94 °C/80 °C) or 16 hours of cycling. This exceptional thermostability permits extremely high hybridization stringency and ligation specificity.

There are at least three different units used to measure the activity of DNA ligase:

DNA ligases have become indispensable tools in modern molecular biology research for generating recombinant DNA sequences. For example, DNA ligases are used with restriction enzymes to insert DNA fragments, often genes, into plasmids.

Controlling the optimal temperature is a vital aspect of performing efficient recombination experiments involving the ligation of cohesive-ended fragments. Most experiments use T4 DNA Ligase (isolated from bacteriophage T4), which is most active at 37 °C. However, for optimal ligation efficiency with cohesive-ended fragments ("sticky ends"), the optimal enzyme temperature needs to be balanced with the melting temperature T of the sticky ends being ligated, the homologous pairing of the sticky ends will not be stable because the high temperature disrupts hydrogen bonding. A ligation reaction is most efficient when the sticky ends are already stably annealed, and disruption of the annealing ends would therefore result in low ligation efficiency. The shorter the overhang, the lower the T.

Since blunt-ended DNA fragments have no cohesive ends to anneal, the melting temperature is not a factor to consider within the normal temperature range of the ligation reaction. The limiting factor in blunt end ligation is not the activity of the ligase but rather the number of alignments between DNA fragment ends that occur. The most efficient ligation temperature for blunt-ended DNA would therefore be the temperature at which the greatest number of alignments can occur. The majority of blunt-ended ligations are carried out at 14-25 °C overnight. The absence of stably annealed ends also means that the ligation efficiency is lowered, requiring a higher ligase concentration to be used.

A novel use of DNA ligase can be seen in the field of nano chemistry, specifically in DNA origami.  DNA based self-assembly principles have proven useful for organizing nanoscale objects, such as biomolecules, nanomachines, nanoelectronic and photonic component. Assembly of such nano structure requires the creation of an intricate mesh of DNA molecules. Although DNA self-assembly is possible without any outside help using different substrates such as provision of catatonic surface of Aluminium foil, DNA ligase can provide the enzymatic assistance that is required to make DNA lattice structure from DNA over hangs.

The first DNA ligase was purified and characterized in 1967 by the Gellert, Lehman, Richardson, and Hurwitz laboratories. It was first purified and characterized by Weiss and Richardson using a six-step chromatographic-fractionation process beginning with elimination of cell debris and addition of streptomycin, followed by several Diethylaminoethyl (DEAE)-cellulose column washes and a final phosphocellulose fractionation. The final extract contained 10% of the activity initially recorded in the "E. coli "media; along the process it was discovered that ATP and Mg++ were necessary to optimize the reaction. The common commercially available DNA ligases were originally discovered in bacteriophage T4, "E. coli" and other bacteria.

Genetic deficiencies in human DNA ligases have been associated with clinical syndromes marked by immunodeficiency, radiation sensitivity, and developmental abnormalities,  LIG4 syndrome (Ligase IV syndrome) is a rare disease associated with mutations in DNA ligase 4 and interferes with dsDNA break-repair mechanisms. Ligase IV syndrome causes immunodeficiency in individuals and is commonly associated with microcephaly and marrow hypoplasia. A list of prevalent diseases caused by lack of or malfunctioning of DNA ligase is as follows.

Xeroderma pigmentosum, which is commonly known as XP, is an inherited condition characterized by an extreme sensitivity to ultraviolet (UV) rays from sunlight. This condition mostly affects the eyes and areas of skin exposed to the sun. Some affected individuals also have problems involving the nervous system.

Mutations in the ATM gene cause ataxia–telangiectasia. The ATM gene provides instructions for making a protein that helps control cell division and is involved in DNA repair. This protein plays an important role in the normal development and activity of several body systems, including the nervous system and immune system. The ATM protein assists cells in recognizing damaged or broken DNA strands and coordinates DNA repair by activating enzymes that fix the broken strands. Efficient repair of damaged DNA strands helps maintain the stability of the cell's genetic information. Affected children typically develop difficulty walking, problems with balance and hand coordination, involuntary jerking movements (chorea), muscle twitches (myoclonus), and disturbances in nerve function (neuropathy). The movement problems typically cause people to require wheelchair assistance by adolescence. People with this disorder also have slurred speech and trouble moving their eyes to look side-to-side (oculomotor apraxia).

Fanconi anemia (FA) is a rare, inherited blood disorder that leads to bone marrow failure. FA prevents bone marrow from making enough new blood cells for the body to work normally. FA also can cause the bone marrow to make many faulty blood cells. This can lead to serious health problems, such as leukemia.

Bloom syndrome results in skin that is sensitive to sun exposure, and usually the development of a butterfly-shaped patch of reddened skin across the nose and cheeks. A skin rash can also appear on other areas that are typically exposed to the sun, such as the back of the hands and the forearms. Small clusters of enlarged blood vessels (telangiectases) often appear in the rash; telangiectases can also occur in the eyes. Other skin features include patches of skin that are lighter or darker than the surrounding areas (hypopigmentation or hyperpigmentation respectively). These patches appear on areas of the skin that are not exposed to the sun, and their development is not related to the rashes.

In recent studies, human DNA ligase I was used in Computer-aided drug design to identify DNA ligase inhibitors as possible therapeutic agents to treat cancer. Since excessive cell growth is a hallmark of cancer development, targeted chemotherapy that disrupts the functioning of DNA ligase can impede adjuvant cancer forms. Furthermore, it has been shown that DNA ligases can be broadly divided into two categories, namely, ATP- and NAD-dependent. Previous research has shown that although NAD-dependent DNA ligases have been discovered in sporadic cellular or viral niches outside the bacterial domain of life, there is no instance in which a NAD-dependent ligase is present in a eukaryotic organism. The presence solely in non-eukaryotic organisms, unique substrate specificity, and distinctive domain structure of NAD+ dependent compared with ATP-dependent human DNA ligases together make NAD-dependent ligases ideal targets for the development of new antibacterial drugs.




</doc>
<doc id="8699" url="https://en.wikipedia.org/wiki?curid=8699" title="Dewey Decimal Classification">
Dewey Decimal Classification

The Dewey Decimal Classification (DDC), colloquially the Dewey Decimal System, is a proprietary library classification system first published in the United States by Melvil Dewey in 1876. Originally described in a four-page pamphlet, it has been expanded to multiple volumes and revised through 23 major editions, the latest printed in 2011. It is also available in an abridged version suitable for smaller libraries. OCLC, a non-profit cooperative that serves libraries, currently maintains the system and licenses online access to WebDewey, a continuously updated version for catalogers.

The Decimal Classification introduced the concepts of "relative location" and "relative index" which allow new books to be added to a library in their appropriate location based on subject. Libraries previously had given books permanent shelf locations that were related to the order of acquisition rather than topic. The classification's notation makes use of three-digit numbers for main classes, with fractional decimals allowing expansion for further detail. Numbers are flexible to the degree that they can be expanded in linear fashion to cover special aspects of general subjects. A library assigns a classification number that unambiguously locates a particular volume in a position relative to other books in the library, on the basis of its subject. The number makes it possible to find any book and to return it to its proper place on the library shelves. The classification system is used in 200,000 libraries in at least 135 countries.

Melvil Dewey (1851–1931) was an American librarian and self-declared reformer. He was a founding member of the American Library Association and can be credited with the promotion of card systems in libraries and business. He developed the ideas for his library classification system in 1873 while working at Amherst College library. He applied the classification to the books in that library, until in 1876 he had a first version of the classification. In 1876, he published the classification in pamphlet form with the title "A Classification and Subject Index for Cataloguing and Arranging the Books and Pamphlets of a Library."
He used the pamphlet, published in more than one version during the year, to solicit comments from other librarians. It is not known who received copies or how many commented as only one copy with comments has survived, that of Ernest Cushing Richardson. His classification system was mentioned in an article in the first issue of the "Library Journal" and in an article by Dewey in the Department of Education publication "Public Libraries in America" in 1876. In March 1876, he applied for, and received copyright on the first edition of the index. The edition was 44 pages in length, with 2,000 index entries, and was printed in 200 copies.

The second edition of the Dewey Decimal system, published in 1885 with the title "", comprised 314 pages, with 10,000 index entries. Five hundred copies were produced. Editions 3–14, published between 1888 and 1942, used a variant of this same title. Dewey modified and expanded his system considerably for the second edition. In an introduction to that edition Dewey states that "nearly 100 persons hav [spelling of 'have' per English-language spelling reform, which Dewey championed] contributed criticisms and suggestions".

One of the innovations of the Dewey Decimal system was that of positioning books on the shelves in relation to other books on similar topics. When the system was first introduced, most libraries in the US used fixed positioning: each book was assigned a permanent shelf position based on the book's height and date of acquisition. Library stacks were generally closed to all but the most privileged patrons, so shelf browsing was not considered of importance. The use of the Dewey Decimal system increased during the early 20th century as librarians were convinced of the advantages of relative positioning and of open shelf access for patrons.

New editions were readied as supplies of previously published editions were exhausted, even though some editions provided little change from the previous, as they were primarily needed to fulfill demand. In the next decade, three editions followed closely on: the 3rd (1888), 4th (1891), and 5th (1894). Editions 6 through 11 were published from 1899 to 1922. The 6th edition was published in a record 7,600 copies, although subsequent editions were much lower. During this time, the size of the volume grew, and edition 12 swelled to 1243 pages, an increase of 25% over the previous edition.

In response to the needs of smaller libraries which were finding the expanded classification schedules difficult to use, in 1894, the first abridged edition of the Dewey Decimal system was produced. The abridged edition generally parallels the full edition, and has been developed for most full editions since that date. By popular request, in 1930, the Library of Congress began to print Dewey Classification numbers on nearly all of its cards, thus making the system immediately available to all libraries making use of the Library of Congress card sets.

Dewey's was not the only library classification available, although it was the most complete. Charles Ammi Cutter published the Expansive Classification in 1882, with initial encouragement from Melvil Dewey. Cutter's system was not adopted by many libraries, with one major exception: it was used as the basis for the Library of Congress Classification system.

In 1895, the International Institute of Bibliography, located in Belgium and led by Paul Otlet, contacted Dewey about the possibility of translating the classification into French, and using the classification system for bibliographies (as opposed to its use for books in libraries). This would have required some changes to the classification, which was under copyright. Dewey gave permission for the creation of a version intended for bibliographies, and also for its translation into French. Dewey did not agree, however, to allow the International Institute of Bibliography to later create an English version of the resulting classification, considering that a violation of their agreement, as well as a violation of Dewey's copyright. Shortly after Dewey's death in 1931, however, an agreement was reached between the committee overseeing the development of the Decimal Classification and the developers of the French "Classification Decimal". The English version was published as the Universal Decimal Classification and is still in use today.

According to a study done in 1927, the Dewey system was used in the US in approximately 96% of responding public libraries and 89% of the college libraries. After the death of Melvil Dewey in 1931, administration of the classification was under the Decimal Classification Committee of the Lake Placid Club Education Foundation, and the editorial body was the Decimal Classification Editorial Policy Committee with participation of the American Library Association (ALA), Library of Congress, and Forest Press. By the 14th edition in 1942, the Dewey Decimal Classification index was over 1,900 pages in length and was published in two volumes.

The growth of the classification to date had led to significant criticism from medium and large libraries which were too large to use the abridged edition but found the full classification overwhelming. Dewey had intended issuing the classification in three editions: the library edition, which would be the fullest edition; the bibliographic edition, in English and French, which was to be used for the organization of bibliographies rather than of books on the shelf; and the abridged edition. In 1933, the bibliographic edition became the Universal Decimal Classification, which left the library and abridged versions as the formal Dewey Decimal Classification editions. The 15th edition, edited by Milton Ferguson, implemented the growing concept of the "standard edition", designed for the majority of general libraries but not attempting to satisfy the needs of the very largest or of special libraries. It also reduced the size of the Dewey system by over half, from 1,900 to 700 pages. This revision was so radical that an advisory committee was formed right away for the 16th and 17th editions. The 16th and 17th editions, under the editorship of the Library of Congress, grew again to two volumes. However, by now, the Dewey Decimal system had established itself as a classification for general libraries, with the Library of Congress Classification having gained acceptance for large research libraries.

The first electronic version of "Dewey" was created in 1993. Hard-copy editions continue to be issued at intervals; the online WebDewey and Abridged WebDewey are updated quarterly.

Dewey and a small editorial staff managed the administration of the very early editions. Beginning in 1922, the Lake Placid Club Educational Foundation, a not-for-profit organization founded by Melvil Dewey, managed administrative affairs. The ALA set up a Special Advisory Committee on the Decimal Classification as part of the Cataloging and Classification division of ALA in 1952. The previous Decimal Classification Committee was changed to the Decimal Classification Editorial Policy Committee, with participation of the ALA Division of Cataloging and Classification, and of the Library of Congress.

Melvil Dewey edited the first three editions of the classification system and oversaw the revisions of all editions until his death in 1931. May Seymour became editor in 1891 and served until her death in 1921. She was followed by Dorcas Fellows, who was editor until her death in 1938. Constantin J. Mazney edited the 14th edition. Milton Ferguson functioned as editor from 1949 to 1951. The 16th edition in 1958 was edited under an agreement between the Library of Congress and Forest Press, with David Haykin as director. Editions 16–19 were edited by Benjamin A. Custer and the editor of edition 20 was John P. Comaromi. Joan Mitchell was editor until 2013, covering editions 21 to 23. In 2013 Michael Panzer of OCLC became Editor-in-Chief. The Dewey Editorial Program Manager since 2016 has been Dr. Rebecca Green.

Dewey himself held copyright in editions 1 to 6 (1876–1919). Copyright in editions 7–10 was held by the publisher, The Library Bureau. On the death of May Seymour, Dewey conveyed the "copyrights and control of all editions" to the Lake Placid Club Educational Foundation, a non-profit chartered in 1922. The Online Computer Library Center (OCLC) of Dublin, Ohio, US, acquired the trademark and copyrights associated with the Dewey Decimal Classification system when it bought Forest Press in 1988. In 2003 the Dewey Decimal Classification came to the attention of the U.S. press when OCLC sued the Library Hotel for trademark infringement for using the classification system as the hotel theme. The case was settled shortly thereafter.

The OCLC has maintained the classification since 1988, and also publishes new editions of the system. The editorial staff responsible for updates is based partly at the Library of Congress and partly at OCLC. Their work is reviewed by the Decimal Classification Editorial Policy Committee, a ten-member international board which meets twice each year. The four-volume unabridged edition was published approximately every six years, with the last edition (DDC 23) published in mid-2011. In 2017 the editorial staff announced that the English edition of DDC will no longer be printed, in favor of using the frequently updated WebDewey. An experimental version of Dewey in RDF was previously available at dewey.info beginning in 2009, but has not been available since 2015.

In addition to the full version, a single-volume abridged edition designed for libraries with 20,000 titles or fewer has been made available since 1895. The last printed English abridged edition, Abridged Edition 15, was published in early 2012.

The Dewey Decimal Classification organizes library materials by discipline or field of study. Main divisions include philosophy, social sciences, science, technology, and history. The scheme comprises ten classes, each divided into ten divisions, each having ten sections. The system's notation uses Indo-Arabic numbers, with three whole numbers making up the main classes and sub-classes and decimals designating further divisions. The classification structure is hierarchical and the notation follows the same hierarchy. Libraries not needing the full level of detail of the classification can trim right-most decimal digits from the class number to obtain more general classifications. For example:

The classification was originally enumerative, meaning that it listed all of the classes explicitly in the schedules. Over time it added some aspects of a faceted classification scheme, allowing classifiers to construct a number by combining a class number for a topic with an entry from a separate table. Tables cover commonly used elements such as geographical and temporal aspects, language, and bibliographic forms. For example, a class number could be constructed using 330 for economics + .9 for geographic treatment + .04 for Europe to create the class 330.94 European economy. Or one could combine the class 973 (for the United States) + .05 (for periodical publications on the topic) to arrive at the number 973.05 for periodicals concerning the United States generally. The classification also makes use of mnemonics in some areas, such that the number 5 represents the country Italy in classification numbers like 945 (history of Italy), 450 (Italian language), 195 (Italian philosophy). The combination of faceting and mnemonics makes the classification "synthetic" in nature, with meaning built into parts of the classification number.

The Dewey Decimal Classification has a number for all subjects, including fiction, although many libraries maintain a separate fiction section shelved by alphabetical order of the author's surname. Each assigned number consists of two parts: a class number (from the Dewey system) and a book number, which "prevents confusion of different books on the same subject". A common form of the book number is called a Cutter number, which represents the author and distinguishes the book from other books on the same topic.



The Relative Index (or, as Dewey spelled it, "Relativ Index") is an alphabetical index to the classification, for use both by classifiers and by library users when seeking books by topic. The index was "relative" because the index entries pointed to the class numbers, not to the page numbers of the printed classification schedule. In this way, the Dewey Decimal Classification itself had the same relative positioning as the library shelf and could be used either as an entry point to the classification, by catalogers, or as an index to a Dewey-classed library itself.

Dewey Decimal Classification numbers formed the basis of the Universal Decimal Classification (UDC), which combines the basic Dewey numbers with selected punctuation marks (comma, colon, parentheses, etc.). Adaptations of the system for specific regions outside the English-speaking world include the Korean Decimal Classification, the New Classification Scheme for Chinese Libraries, and the Nippon Decimal Classification (Japanese).

Despite its widespread usage, the classification has been criticized for its complexity and limited capability for amendment. In particular, the arrangement of subheadings has been described as archaic and biased towards an Anglo-American world view. In 2007–08, the Maricopa County Library District in Arizona abandoned the DDC in favor of the Book Industry Standards and Communications (BISAC) system commonly used by commercial bookstores, in an effort to make their libraries more accessible for patrons. Several other libraries across the United States and other countries (including Canada and the Netherlands) followed suit. The DDC has also been criticized for being a proprietary system licensed by a single entity (OCLC), making it expensive to adopt. However, book classification critic Justin Newlan stands by the Dewey Decimal System, stating that newer and more advanced book classification systems "are too confusing to understand for newcomers".

In 1932, topics relating to homosexuality were first added to the system under 132 (mental derangements) and 159.9 (abnormal psychology). In 1952, homosexuality was also included under 301.424 (the study of sexes in society). In 1989, it was added to 363.49 (social problems), a classification that continues in the current edition.

In 1996, homosexuality was added to 306.7 (sexual relations); this remains the preferred location in the current edition. Although books can also be found under 616.8583 (sexual practices viewed as medical disorders), the official direction states:
The top-level class for religion heavily favors Christianity, dedicating nearly all of the 200 division to it: the world's thousands of other religions were listed under the 290s. For example, Islam is under just DDC 297, despite being almost as large as Christianity by population. The entire 200 section has remained largely unchanged since DDC 1, since restructuring would pose significant work to existing libraries. The motivation for this change is ideological rather than technical, as appending significant figures can add space as needed.

The placement of topics related to women have shown implicit bias as well, but have proven simpler to amend than the religion schema. Some changes made so far have been in numerical proximity, altering the placement of topics relative to each other. For example: in previous DDC versions, some categories regarding women were adjacent to categories on etiquette; the placement of these categories next to each other imposes an association of etiquette with women, rather than being gender neutral. This was changed in DDC version 17.




</doc>
<doc id="8702" url="https://en.wikipedia.org/wiki?curid=8702" title="Duḥkha">
Duḥkha

Duḥkha (; Sanskrit:दुःख; Pāli: dukkha) is an important concept in Hinduism and Buddhism , commonly translated as "suffering", "unhappiness", "pain", "unsatisfactoriness" or "stress". It refers to the fundamental unsatisfactoriness and painfulness of mundane life. It is the first of the Four Noble Truths and it is one of the three marks of existence. The term also appears in scriptures of Hinduism, such as the Upanishads, in discussions of moksha (spiritual liberation).

"Duḥkha" (Sanskrit; Pali "dukkha") is a term found in ancient Indian literature, meaning anything that is "uneasy, uncomfortable, unpleasant, difficult, causing pain or sadness". It is also a concept in Indian religions about the nature of life that innately includes the "unpleasant", "suffering", "pain", "sorrow", "distress", "grief" or "misery." The term "duḥkha" does not have a one-word English translation, and embodies diverse aspects of unpleasant human experiences. It is opposed to the word "sukha", meaning "happiness," "comfort" or "ease."

The word is commonly explained as a derivation from Aryan terminology for an axle hole, referring to an axle hole which is not in the center and leads to a bumpy, uncomfortable ride. According to Winthrop Sargeant,

Joseph Goldstein, American vipassana teacher and writer, explains the etymology as follows:
However, according to Monier Monier-Williams, the actual roots of the Pali term "dukkha" appear to be Sanskrit दुस्- ("dus-", "bad") + स्था ("stha", "to stand"). Regular phonological changes in the development of Sanskrit into the various Prakrits led to a shift from "dus-sthā" to "duḥkha" to "dukkha".

Contemporary translators of Buddhist texts use a variety of English words to convey the aspects of "duḥkha". Early Western translators of Buddhist texts (before the 1970s) typically translated the Pali term "dukkha" as "suffering." Later translators have emphasized that "suffering" is too limited a translation for the term duḥkha, and have preferred to either leave the term untranslated or to clarify that translation with terms such as anxiety, distress, frustration, unease, unsatisfactoriness, etc. Many contemporary teachers, scholars, and translators have used the term "unsatisfactoriness" to emphasize the subtlest aspects of dukkha. Contemporary translators have used a variety of English words to translate the term "duḥkha",
and many translators prefer to leave the term untranslated.

Within the Buddhist sutras, duḥkha is divided in three categories:

Various sutras sum up how life in this "mundane world" is regarded to be "duḥkha", starting with "samsara", the ongoing process of death and rebirth itself:

"Duḥkha" is one of the three marks of existence, namely "duḥkha" ("suffering"), "anatman" (not-self), "anitya" ("impermanence").

The Buddhist tradition emphasizes the importance of developing insight into the nature of "duḥkha", the conditions that cause it, and how it can be overcome. This process is formulated in the teachings on the Four Noble Truths.

In Hindu literature, the earliest Upaniads — the and the — in all likelihood predate the advent of Buddhism. In these scriptures of Hinduism, the Sanskrit word "dukha" (दुःख) appears in the sense of "suffering, sorrow, distress", and in the context of a spiritual pursuit and liberation through the knowledge of Atman (soul/self).

The verse 4.4.14 of the states:

The verse 7.26.2 of the states:
The concept of sorrow and suffering, and self-knowledge as a means to overcome it, appears extensively with other terms in the pre-Buddhist Upanishads. The term "Duhkha" also appears in many other middle and later post-Buddhist Upanishads such as the verse 6.20 of Shvetashvatara Upanishad, as well as in the Bhagavada Gita, all in the context of moksha. The term also appears in the foundational Sutras of the six schools of Hindu philosophy, such as the opening lines of "Samkhya karika" of the Samkhya school.

Both Hinduism and Buddhism emphasize that one overcomes "dukha" through the development of understanding. However, the two religions widely differ in the nature of that understanding. Hinduism emphasizes the understanding and acceptance of Atman (self, soul) and Brahman, while Buddhism emphasizes the understanding and acceptance of Anatta (Anatman, non-self, non-soul) as each discusses the means to liberation from "Duḥkha".

According to the Silk Road philologist, Christopher I. Beckwith, the ancient Greek philosopher, Pyrrho, based his new philosophy, Pyrrhonism, on elements of Early Buddhism, most particularly the Buddhist three marks of existence. Pyrrho accompanied Alexander the Great on his Indian campaign, spending about 18 months in Taxila studying Indian philosophy. Diogenes Laërtius' biography of Pyrrho reports that Pyrrho based his philosophy on what he learned there:

...he even went as far as the Gymnosophists, in India, and the Magi. Owing to which circumstance, he seems to have taken a noble line in philosophy, introducing the doctrine of acatalepsy (incomprehensibility), and of the necessity of epoche (suspending one's judgment)...

A summary of Pyrrho's philosophy was preserved by Eusebius, quoting Aristocles, quoting Pyrrho's student Timon, in what is known as the "Aristocles passage."

"Whoever wants to live well (eudaimonia) must consider these three questions: First, how are "pragmata" (ethical matters, affairs, topics) by nature? Secondly, what attitude should we adopt towards them? Thirdly, what will be the outcome for those who have this attitude?" Pyrrho's answer is that "As for "pragmata" they are all adiaphora (undifferentiated by a logical differentia), "astathmēta" (unstable, unbalanced, not measurable), and "anepikrita" (unjudged, unfixed, undecidable). Therefore, neither our sense-perceptions nor our "doxai" (views, theories, beliefs) tell us the truth or lie; so we certainly should not rely on them. Rather, we should be "adoxastoi" (without views), "aklineis" (uninclined toward this side or that), and "akradantoi" (unwavering in our refusal to choose), saying about every single one that it no more is than it is not or it both is and is not or it neither is nor is not.

According to Beckwith's analysis of the Aristocles Passage, Pyrrho translated "dukkha" into Greek as "astathmēta". This gives insight into what "dukkha" meant in Early Buddhism. 

...although the sense of duḥkha in Normative Buddhism is traditionally given as 'suffering', that and similar interpretations are highly unlikely for Early Buddhism. Significantly, Monier-Williams himself doubts the usual explanation of duḥkha and presents an alternative one immediately after it, namely: duḥ-stha “'standing badly,’ unsteady, disquieted (lit. and fig.); uneasy", and so on. This form is also attested, and makes much better sense as the opposite of the Rig Veda sense of sukha, which Monier-Williams gives in full as “(said to be fr. 5. su + 3. kha , and to mean originally 'having a good axle-hole'; possibly a Prakrit form of su-stha37 q.v.; cf. duḥkha) running
swiftly or easily (only applied to cars or chariots, superl[ative] sukhátama), easy"... The most important point here is that duḥ + stha literally means 'dis-/ bad- + stand-’, that is, 'badly standing, unsteady' and is therefore virtually identical to the literal meaning of Greek astathmēta, from a- + sta- 'not- + stand', both evidently meaning 'unstable'. This strongly suggests that Pyrrho’s middle term is in origin a simple calque.




</doc>
<doc id="8703" url="https://en.wikipedia.org/wiki?curid=8703" title="Darwin Awards">
Darwin Awards

The Darwin Awards are a tongue-in-cheek honor originating in Usenet newsgroup discussions around 1985. They recognize individuals who have supposedly contributed to human evolution by selecting themselves out of the gene pool via death or sterility by their own actions.

The project became more formalized with the creation of a website in 1993, followed by a series of books starting in 2000 by Wendy Northcutt. The criterion for the awards states: "In the spirit of Charles Darwin, the Darwin Awards commemorate individuals who protect our gene pool by making the ultimate sacrifice of their own lives. Darwin Award winners eliminate themselves in an extraordinarily idiotic manner, thereby improving our species' chances of long-term survival."

Accidental self-sterilisation also qualifies; however, the site notes: "Of necessity, the award is usually bestowed posthumously." The candidate is disqualified, though, if "innocent bystanders" are killed in the process, as they might have contributed positively to the gene pool. The logical problem presented by award winners who may have already reproduced is not addressed in the selection process due to the difficulty of ascertaining whether or not a person has children; the Darwin Award rules state that the presence of offspring does not disqualify a nominee.

The origin of the Darwin Awards can be traced back to posts on Usenet group discussions as early as 1985. An early post, on August 7, 1985, describes the awards as being, "given posthumously to people who have made the supreme sacrifice to keep their genes out of our pool. Style counts, not everyone who dies from their own stupidity can win." This early post cites an example of a person who pulled a vending machine over his head and was crushed to death trying to break into it. Another widely distributed early story mentioning the Darwin Awards is the JATO Rocket Car, which describes a man who strapped a jet-assisted take-off unit to his Chevrolet Impala in the Arizona desert and who died on the side of a cliff as his car achieved speeds of . This story was later confirmed to be an urban legend by the Arizona Department of Public Safety. Wendy Northcutt says the official Darwin Awards website run by Northcutt does its best to confirm all stories submitted, listing them as, "confirmed true by Darwin." Many of the viral emails circulating the Internet, however, are hoaxes and urban legends.

The website and collection of books were started in 1993 by Wendy Northcutt, who at the time was a graduate in molecular biology from the University of California, Berkeley. She went on to study neurobiology at Stanford University, doing research on cancer and telomerase. In her spare time, she organised chain letters from family members into the original Darwin Awards website hosted in her personal account space at Stanford. She eventually left the bench in 1998 and devoted herself full-time to her website and books in September 1999. By 2002, the website received 7 million page hits per month.

She encountered some difficulty in publishing the first book, since most publishers would only offer her a deal if she agreed to remove the stories from the internet. Northcutt refused to do so, saying, "It was a community! I could not do that. Even though it might have cost me a lot of money, I kept saying no." She eventually found a publisher who agreed to print a book containing only 10% of the material gathered for the website. The first book turned out to be a success, and was listed on "The New York Times" best-seller list for six months.

Not all of the feedback from the stories Northcutt published was positive, and she occasionally received email from people who knew the deceased. One such person wrote, "This is horrible. It has shocked our community to the core. You should remove this." But Northcutt said "I can't. It's just too stupid." Northcutt kept the stories on the website and in her books, citing them as a "funny-but-true safety guide", and mentioning that children who read the book are going to be much more careful around explosives.

The website also recognises, with Honourable Mentions, individuals who survive their misadventures with their reproductive capacity intact. One example of this is Larry Walters, who attached helium-filled weather balloons to a lawn chair and floated far above Long Beach, California, in July 1982. He reached an altitude of but survived, to be later fined for crossing controlled airspace. (Walters later fell into depression and committed suicide.) Another notable honourable mention was given to the two men who attempted to burgle the home of footballer Duncan Ferguson (who had four convictions for assault and had served six months in Glasgow's Barlinnie Prison) in 2001, with one burglar requiring three days' hospitalisation after being confronted by the player.
A 2014 study published in the British Medical Journal found that between 1995 and 2014 males represented 88.7% of Darwin Award winners (see figure).

A 2006 comedy film, "The Darwin Awards", written and directed by Finn Taylor, was based on the website and many of the Darwin Awards stories.

Northcutt has stated five requirements for a Darwin Award:

This may be subject to dispute. Potential awardees may be out of the gene pool because of age; others have already reproduced before their deaths. To avoid debates about the possibility of in-vitro fertilization, artificial insemination, or cloning, the original Darwin Awards book applied the following "deserted island" test to potential winners: If the person were unable to reproduce when stranded on a deserted island with a fertile member of the opposite sex, he or she would be considered sterile. Winners of the award, in general, either are dead or have become unable to use their sexual organs.

The candidate's foolishness must be unique and sensational, likely because the award is intended to be funny. A number of foolish but common activities, such as smoking in bed, are excluded from consideration. In contrast, self-immolation caused by smoking after being administered a flammable ointment in a hospital and specifically told not to smoke is grounds for nomination. One "Honourable Mention" (a man who attempted suicide by swallowing nitroglycerine pills, and then tried to detonate them by running into a wall) is noted to be in this category, despite being intentional and self-inflicted (i.e. attempted suicide), which would normally disqualify the inductee.

Killing a friend with a hand grenade would not be eligible, but killing oneself while manufacturing a home made chimney-cleaning device from a grenade would be eligible. To earn a Darwin Award, one must have killed oneself, or rendered oneself sterile; merely causing death to a third party is insufficient.

The nominee must be at least past the legal driving age and free of mental defect (Northcutt considers injury or death caused by mental defect to be tragic, rather than amusing, and routinely disqualifies such entries). After much discussion, a small category regarding deaths below this age limit also exists. Entry into this category requires that the peers of the candidate be of the opinion that the actions of the person in question were above and beyond the limits of reason.

However, in 2011, the awards targeted a 16-year-old boy in Leeds who died stealing copper wiring (the standard minimum driving age in Great Britain being 17). In 2012, Northcutt made similar light of a 14-year-old girl in Brazil who was killed while leaning out of a school bus window; however, she was "disqualified" for the award itself because of the likely public objection due to the girl's age, which Northcutt asserts is based on "magical thinking".

The story must be documented by reliable sources: e.g., reputable newspaper articles, confirmed television reports, or responsible eyewitnesses. If a story is found to be untrue, it is disqualified, but particularly amusing ones are placed in the urban legend section of the archives. Despite this requirement, many of the stories are fictional, often appearing as "original submissions" and presenting no further sources than unverified (and unreliable) "eyewitnesses". Most such stories on Northcutt's Darwin Awards site are filed in the Personal Accounts section.

In addition, later revisions to the qualification criteria add several requirements that have not been made into formalised ‘rules’:




</doc>
<doc id="8704" url="https://en.wikipedia.org/wiki?curid=8704" title="Outline of dance">
Outline of dance

The following outline is provided as an overview of and topical guide to dance:

Dance – human movement either used as a form of expression or presented in a social, spiritual or performance setting. Choreography is the art of making dances, and the person who does this is called a choreographer. Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet. A great many dances and dance styles are performed to dance music.

Dance (also called "dancing") can fit the following categories:


Some other things can be named "dance" metaphorically; see dance (disambiguation)

Type of dance – a particular dance or dance style. There are many varieties of dance. Dance categories are not mutually exclusive. For example, tango is traditionally a "partner dance". While it is mostly "social dance", its ballroom form may be "competitive dance", as in DanceSport. At the same time it is enjoyed as "performance dance", whereby it may well be a "solo dance".



History of dance



Dance science








</doc>
<doc id="8706" url="https://en.wikipedia.org/wiki?curid=8706" title="DCM">
DCM

DCM may refer to:






</doc>
<doc id="8707" url="https://en.wikipedia.org/wiki?curid=8707" title="DKW">
DKW

DKW (Dampf-Kraft-Wagen, , also Deutsche Kinder-Wagen . Das-Kleine-Wunder, or Des-Knaben-Wunsch, ) is a German car and motorcycle marque. DKW was one of the four companies that formed Auto Union in 1932 and is hence an ancestor of the modern day Audi company.

In 1916, Danish engineer Jørgen Skafte Rasmussen founded a factory in Zschopau, Saxony, Germany, to produce steam fittings. That year he attempted to produce a steam-driven car, called the DKW. Although unsuccessful, he made a two-stroke toy engine in 1919, called "Des Knaben Wunsch" – "the boy's wish". He put a slightly modified version of this engine into a motorcycle and called it "Das Kleine Wunder" – "the little wonder" the initials from this becoming the DKW brand: by the late 1920s, DKW was the world's largest motorcycle manufacturer.

In September 1924, DKW bought , saving them from Germany's hyperinflation economic crisis. Rudolf Slaby became chief-engineer at DKW.
In 1932, DKW merged with Audi, Horch and Wanderer to form Auto Union. After World War II, DKW moved to West Germany. The original factory became MZ. Auto Union came under Daimler-Benz ownership in 1957 and was purchased by the Volkswagen Group in 1964. The last German-built DKW car was the F102, which ceased production in 1966. Its successor, the four-stroke F103, was marketed under the Audi brand, another Auto Union marque.

DKW-badged cars continued to be built under license in Brazil and Argentina until 1967 and 1969 respectively. The DKW trademark is currently owned by Auto Union GmbH, a wholly owned subsidiary of Audi AG which also owns the rights to other historical trademarks and intellectual property of the Auto Union combine.

DKW cars were made from 1928 until 1966, apart from the interruption caused by the Second World War. DKWs always used two-stroke engines, reflecting the company's position by the end of the 1920s as the world's largest producer of motorcycles. The first DKW car, the small and rather crude Typ P, emerged on 7 May 1928 and the model continued to be built at the company's Spandau (Berlin) plant, first as a roadster and later as a stylish if basic sports car, until 1931.

More significant was a series of inexpensive cars built 300 km (185 miles) to the south in Zwickau in the plant acquired by the company's owner in 1928 when he had become the majority owner in Audi Werke AG. Models F1 to F8 (F for Front) were built between 1931 and 1942, with successor models reappearing after the end of the war in 1945. They were the first volume production cars in Europe with front wheel drive, and were powered by transversely mounted two-cylinder two-stroke engines. Displacement was 584 or 692 cc: claimed maximum power was initially 15 PS, and from 1931 a choice between 18 or . These models had a generator that doubled as a starter, mounted directly on the crankshaft, known as a Dynastart. DKWs from Zwickau notched up approximately 218,000 units between 1931 and 1942. Most cars were sold on the home market and over 85% of DKWs produced in the 1930s were the little F series cars: DKW reached second place in German sales by 1934 and stayed there, accounting for 189,369 of the cars sold between 1931 and 1938, more than 16% of the market.

Between 1929 and 1940, DKW produced a less well remembered but technically intriguing series of rear-wheel drive cars called (among other names) "Schwebeklasse" and "Sonderklasse" with two-stroke V4 engines. Engine displacement was 1,000 cc, later 1,100 cc. The engines had two extra cylinders for forced induction, so they appeared like V6 engines but without spark plugs on the front cylinder pair.

In 1939, DKW made a prototype with the first three-cylinder engine, with a displacement of 900 cc and producing . With a streamlined body, the car could run at . It was put into production after World War II, first as an Industrieverband Fahrzeugbau (IFA) F9 (later Wartburg) in Zwickau, East Germany, and shortly afterwards in DKW-form from Düsseldorf as the 3=6 or F91.

DKW engines were used by Saab as a model for the Saab two-stroke in its Saab 92 car manufacturing venture, in 1947.

As Auto Union was based in Saxony in what became the German Democratic Republic (East Germany), it took some time for it to regroup after the war. The company was registered in West Germany as Auto Union GmbH in 1949, first as a spare-part provider, but soon to take up production of the RT 125 motorcycle and a new delivery van, called a "Schnellaster" F800. Their first line of production took place in Düsseldorf. This van used the same engine as the last F8 made before the war.

Their first car was the F89 using the body from the prototype F9 made before the war and the two-cylinder two-stroke engine from the last F8. Production went on until it was replaced by the successful three-cylinder engine that came with the F91. The F91 was in production 1953–1955, and was replaced by the larger F93 in 1956. The F91 and F93 had 900 cc three-cylinder two-stroke engines, the first ones delivering , the last . The ignition system comprised three independent sets of points and coils, one for each cylinder, with the points mounted in a cluster around a single lobed cam at the front end of the crankshaft. The cooling system was of the free convection type assisted by a fan driven from a pulley mounted at the front end of the crankshaft.

The F93 was produced until 1959, and was replaced by the Auto-Union 1000. These models were produced with a 1,000 cc two-stroke engine, with a choice between or S versions until 1963. During this transition, production was moved from Düsseldorf to Ingolstadt, where Audi still has its production. From 1957, the cars could be fitted with a saxomat, an automatic clutch, the only small car then offering this feature. The last versions of the Auto-Union 1000S had disc brakes as option, an early development for this technology. A sporting 2+2 seater version was available as the Auto-Union 1000 SP from 1957 to 1964, the first years only as a coupé and from 1962 also as a convertible.

In 1956, the very rare DKW Monza was put into small-scale production on a private initiative, with a sporting two-seater body of glassfiber on a standard F93 frame. It was first called Solitude, but got its final name from the long-distance speed records it made on the Autodromo Nazionale Monza in Italy in November 1956. Running in Fédération Internationale de l'Automobile (FIA) class G, it set records including 48 hours at an average speed of , 10,000 km at and 72 hours at . The car was first produced by in Stuttgart, then by Massholder in Heidelberg and lastly by Robert Schenk in Stuttgart. The number produced is said to be around 230 and production finished by the end of 1958.
A more successful range of cars was sold from 1959, the Junior/F12 series based on a modern concept from the late 1950s. The range consists of Junior (basic model) made from 1959 to 1961, Junior de Luxe (a little enhanced) from 1961 to 1963, F11 (a little larger) and F12 (larger and bigger engine) from 1963 to 1965, and F12 Roadster from 1964 to 1965. The Junior/F12 series became quite popular, and many cars were produced. An assembly plant was licensed in Ballincollig, County Cork, Ireland between 1952 and c.1964 and roughly 4,000 vehicles were assembled, ranging from saloons, vans and motorbikes to commercial combine harvesters. This was the only DKW factory outside Germany in Europe and for many years after its closure its large DKW sign could be visible on the wall of the factory. The building was demolished in the late 2000s and was redeveloped into a German Aldi store and a McDonald's drive-thru.

All the three-cylinder two-stroke post-war cars had some sporting potential and formed the basis for many rally victories in the 1950s and early 1960s. This made DKW the most winning car brand in the European rally league for several years during the fifties.

In 1960, DKW developed a V6 engine by combining two three-cylinder two-stroke engines, with a capacity of 1,000 cc. The capacity was increased and the final V6 in 1966 had a capacity of 1,300 cc, which developed at 5,000 rpm using the standard configuration with two carburettors. A four-carburettor version produced , a six-carburettor one . It weighed only . The V6 was planned to be used in the DKW Munga and the F102. About 100 engines were built for testing purposes and 13 DKW F102 and some Mungas were fitted with the V6 engine in the 1960s.

The last DKW was the F102, coming into production in 1964 as a replacement for the old-looking AU1000. However, the F102 sold poorly, largely due to its two-stroke engine technology which was at the limit of its development. Auto Union's parent, Daimler-Benz, decided to offload the company to Volkswagen. The car was re-engineered with a four-stroke engine and relaunched as the Audi F103. This marked the end of the DKW marque for cars, and the rebirth of the Audi name.

From 1956 to 1961, Dutch importer Hart, Nibbrig & Greve assembled cars in an abandoned asphalt factory in Sassenheim, where they employed about 120 workers, two transporter, that collected SKD kits from Duesseldorf and built about 13.500 cars. When the DKW plant moved the import of SKD kits stopped, as it became too expensive.

From 1956 to 1967, DKW cars were made in Brazil by the local company Vemag ("Veículos e Máquinas Agrícolas S.A.", "Vehicles and Agricultural Machinery Inc."). Vemag was assembling Scania-Vabis trucks, but Scania Vabis became an independent company in July 1960. The original plans were to build the Candango off-roader (Munga), a utility vehicle and a four-door sedan, called Vemaguet and Belcar respectively. The first model built was the 900 cc F91 Universal but the Belcar and Vemaguet names were applied later.
In 1958, the F94 four-door sedan and station wagon were launched, in the early 1960s renamed Belcar and Vemaguet. The company also produced a luxury coupe (the DKW Fissore) and the off-road Munga (locally called Candango). In 1960 Vemag cars received the larger one-litre, engine from the Auto Union 1000.

Vemag had a successful official racing team, with the coupe GT Malzoni, with fiberglass body. This project was the foundation of the long-lasting Brazilian sports car brand Puma. The Brazilian F94 line has been improved with several cosmetic changes and became more and more different from the German and Argentine models. Vemag had no capital to invest in new products and came under governmental pressure to merge. In 1964-1965 Volkswagen gradually took over Auto Union, a minority holder in Vemag, and in 1967 Volkswagen bought the remainder of the stock. VW quickly began phasing out DKW-Vemag production and introduced the Volkswagen 1600 sedan to the old Vemag plant, after a total of 109,343 DKW-Vemag cars had been built.

DKW vehicles were made in Argentina from 1960 to 1969 by IASF S.A. (Industria Automotriz Santa Fe Sociedad Anónima) in Sauce Viejo, Santa Fe. The most beautiful were the Cupé Fissore, which had many famous owners (Julio Sosa, César Luis Menotti, and others). Other models are the Auto Union 1000 S Sedán (21,797 made until 1969) and the Auto Union 1000 Universal S (6,396 made until 1969). and the Auto Union Combi/Pick-up.
The last version of the Auto Union Combi/Pick-up (DKW F1000 L), launched in 1969, survived a few months and was bought out by IME, which continued production until 1979.

The DKW Munga was built by Auto Union in Ingolstadt. Production began in October 1956 and ended in December 1968, with 46,750 cars built.

From 1949 to 1962, DKW produced the "Schnellaster" with a trailing-arm rear suspension system with springs in the cross bar assembly. Spanish subsidiary IMOSA produced a modern successor introduced in 1963, the DKW F 1000 L. This van started with the three-cylinder 1,000 cc engine, but later received a Mercedes-Benz Diesel engine and was renamed a Mercedes-Benz in 1975.

During the late 1920s and until WWII broke out, DKW was the world's largest motorcycle manufacturer and the pioneer of front wheel drive automobiles with their DKW Front, along with the Citroen Traction Avant. In 1931, Arnold Zoller started building split-singles and this concept made DKW the dominant racing motorcycle in the Lightweight and Junior classes between the wars. This included off-road events like the International Six Days Trial where the marque scored some considerable inter-war year successes alongside Bavarian Motor Works At the same time, the company also had some success with super-charged racing motorcycles which because of their light weight were particularly successful in the ISDT

The motorcycle branch produced famous models such as the RT 125 pre- and post-World War II, and after the war with production at the original factory in GDR becoming MZ it made 175, 250 and 350 (cc) models. As war reparations, the design drawings of the RT125 were given to Harley-Davidson in the US and BSA in the UK. The Harley-Davidson version was known loosely as the Hummer (Hummer is really just a few specific years, but generally people call the Harley lightweights Hummers), while BSA used them for the Bantam. IFA and later MZ models continued in production until the 1990s, when economics brought production of the two stroke to an end. Other manufacturers copied the DKW design, officially or otherwise. This can be seen in the similarity of many small two-stroke motorcycles from the 1950s, including from Yamaha, Voskhod, Maserati, and Polish WSK.






</doc>
<doc id="8708" url="https://en.wikipedia.org/wiki?curid=8708" title="Doctor Syn">
Doctor Syn

The Reverend Doctor Christopher Syn is the smuggler hero of a series of novels by Russell Thorndike. The first book, "Doctor Syn: A Tale of the Romney Marsh" was published in 1915. The story idea came from smuggling in the 18th-century Romney Marsh, where brandy and tobacco were brought in at night by boat from France to avoid high tax. Minor battles were fought, sometimes at night, between gangs of smugglers, such as the Hawkhurst Gang, and the Revenue, supported by the army and local militias in the South, Kent and the West, Sussex.

Christopher Syn, born 1729, is portrayed as a brilliant scholar from Queen's College, Oxford, possessing swashbuckling skills such as riding, fencing, and seamanship. He was content to live the quiet life of a country vicar in Dymchurch-under-the-Wall under the patronage of Sir Charles Cobtree, the father of his best friend Anthony Cobtree, until his beautiful young Spanish wife Imogene was seduced by and eloped with Nicholas Tappitt, whom Dr. Syn had considered a close friend.

Christopher Syn set out on a quest for revenge, always managing to reach the eloped pair's destinations ahead of them just in time to terrify them against landing and facing him in a deliberate campaign of terror. While sailing from Spain to America in pursuit, his ship was captured by the pirate ship "The Sulphur Pit", commanded by Captain Satan. In a one-on-one fight, Syn defeated and killed Captain Satan to take command of his ship and crew; among them was Mr. Mipps, a former Royal Navy carpenter with whom Syn had become friends in England after rescuing him from the Customs men. Mipps swore loyalty to Syn from that time onward.

With Mipps at his side, Syn turned to piracy and became a great success. Later, when his crew refused to let Syn leave, Syn and Mipps slipped away in one of the ship's boats; unknown to Syn, Mipps had arranged a convenient "accident" in the ship's powder magazine with an exploding barrel of gunpowder, eliminating witnesses of Syn's piratical acts.

Mipps then joined Syn in his quest for revenge, pursuing Tappitt and Imogene throughout the thirteen American colonies (supposedly preaching the gospel to the Indians) and around the world (as part of a whaling voyage) afterwards. Mipps was with him in the Caribbean when Dr. Syn turned again to piracy, assuming the name of Captain Clegg (taking the name "Clegg" from a certain vicious biting fly he had encountered in America)., "Clegg" hijacked his enemy Tappitt's own ship and crew and sailed off with them (renaming the ship the "Imogene") to become the most infamous pirate of the day.

However, a mulatto who escaped the destruction of Syn's previous ship stowed away in Clegg's ship and accused him before the crew; Clegg quelled the potential mutiny by having the mulatto's tongue cut out, marooning him on a coral reef and violently killing Yellow Pete, the ship's Chinese cook, who represented the crew in their wish to rescue the mulatto. Afterwards, realizing that Clegg had become too notorious, Syn decided to abandon his quest and return to England, and Mipps set up a second "accidental" explosion to destroy the "Imogene" and her crew.

Syn returned to England on the night of a storm (13 November 1775) that wrecked his brig off the English coast in sight of Dymchurch. That night he went to the house of his old friend (and now squire) Anthony Cobtree. When news came that the local vicar had drowned while trying to save victims of the shipwreck, Squire Cobtree offered the post to Christopher Syn. Syn accepted and settled down to a more respectable life as the vicar of Dymchurch and Dean of Peculiars in Romney Marsh, Kent, resuming his original name.

Mipps arrived in Dymchurch with the intent of settling down. Syn made him the village sexton upon condition that Mipps "remember to forget" (that Syn had been Clegg and that they had known each other before), and that Mipps never get involved with the local smugglers.

Syn soon became aware that his parishioners were smuggling goods from France to avoid the excessive customs duties the government charged. Learning from Mipps (who, contrary to Syn's orders, had become a leader of the smugglers) that certain townsfolk had been ambushed and captured during a smuggling run, Syn purchased the great black stallion Gehenna from gypsy horse-traders and raced to their rescue. A suit of clothing borrowed from a scarecrow made an improvised disguise, and Syn and Mipps were able to rescue the townsfolk from the Dragoons.

After this, Syn decided that he could only protect his people by becoming their leader. He created a more elaborate scarecrow costume, with eerie luminous paint. Riding Gehenna at night, the respectable Dr. Syn became "The Scarecrow", the feared head of the smugglers. Together with Mipps, he organized the smugglers into a well-organized band of "Night Riders", also called "The Devil Riders", with macabre disguises and code-names.

Syn's cunning was so great that the smugglers outwitted the government forces for many years. A hidden stable watched over by Mother Handaway, the local "witch" (who believed the Scarecrow to be The Devil in living form), was the hiding place for the horses of the Scarecrow and his lieutenants, Mipps and the local highwayman Jimmie Bone (who, being as good a horseman as Syn and of similar build, was sometimes called upon to impersonate the Scarecrow when Syn either had to be elsewhere or seen in the same place.).

Shortly after the first appearances of the Scarecrow, Nicholas Tappitt (using the name "Colonel Delacourt") and the ailing Imogene returned to England, ending up in Dymchurch. Recognizing Syn as Clegg, Tappitt realized that Syn and the Scarecrow were the same and helped the authorities set a trap for him, hoping to both rid himself of his enemy and claim the reward for his capture. The trap was sprung, but Squire Cobtree's daughter Charlotte, who had fallen in love with Syn and also learned his secret identities as both Clegg and the Scarecrow, was the tragic victim when she dressed in the Scarecrow's disguise and was fatally wounded as a result. Tappitt was then suspected of being the Scarecrow, and a Customs officer and three constables came to arrest him. In the ensuing fight, Tappitt killed the Customs man and the constables subdued and arrested Tappitt for murdering the Customs officer.

After Imogene's death in Syn's arms (during which she revealed to him that he had a son by her who was missing somewhere in America), Syn fought a final duel with Tappitt in his jail cell, defeating him. Syn then struck a bargain with Tappitt: If Tappitt confessed to being the notorious pirate Clegg, then Syn would look after and care for Tappitt and Imogene's new-born infant daughter (also named Imogene). Tappitt agreed, and "Captain Clegg" was hanged and later "buried without benefit of clergy at a cross-roads hard by the Kent Ditch."

Many years later, Captain Collyer, a Royal Navy officer assigned to smash the local smuggling ring, uncovered the deception and Dr. Syn's true identity, thanks in part to the tongueless mulatto (who had been rescued by Collyer years before and who had been serving Collyer as a "ferret" seeking out hidden contraband) who recognized Syn as Clegg. Syn evaded capture while at the same time making sure that Imogene and Squire Cobtree's son Denis (who had fallen in love with Imogene) would have a happy life together (they were eventually married), but was murdered in revenge by the mulatto, who then mysteriously managed to escape, leaving Syn harpooned through the neck. As a last mark of respect, Collyer ordered that Syn be buried at sea, rather than have his body hung in chains.

Mipps escaped in the confusion of Syn's death and disappeared from England, but it is said that a little man very much like him is living out his days in a Buddhist Monastery somewhere in the Malay Peninsula, delighting the monks with recounting the adventures of Doctor Syn and the eerie stories of the Romney Marsh and the mysterious Scarecrow and his Night Riders.

The Dr. Syn books detail his adventures and attempts to help the people of Dymchurch and the surrounding area evade the Excise tax. There are:

Note that the "first" book, "Doctor Syn," is actually the final story chronologically; the others proceed in published sequence.

An expanded version of "Doctor Syn Returns" titled "The Scarecrow Rides" was published for the US market by The Dial Press in 1935 and later re-printed in paperback by Black Curtain Press in 2013 ().

In 1960, American author William Buchanan reworked Thorndike's "Further Adventures of Doctor Syn" under the title "Christopher Syn" (New York, Abelard Schuman), giving Thorndike co-authorship credit; this version provides a different conclusion and some conflation, renaming and even removal of the supporting characters. "Christopher Syn" became the basis for the 1962 Disney production (see below); there was also a novelization of the Disney theatrical version, titled "Doctor Syn, Alias the Scarecrow" and written by Vic Crume.

Three film adaptations have been made of Dr. Syn's exploits.

The first, "Doctor Syn" (1937), starred the actor George Arliss in the title role and was his last film.

"Captain Clegg" (1962), known as "Night Creatures" in the U. S., was produced by Hammer Film Productions with actor Peter Cushing in the lead role, directed by Peter Graham Scott. In the screenplay by Anthony Hinds, the main character's name was changed from Doctor Syn to Parson Blyss to avoid rights problems with Disney's forthcoming version, and "Captain Clegg"'s screenplay follows the novel "Doctor Syn" and the screenplay of the 1937 film closely with the exception of a tightening of the plot. In the Arliss movie "Doctor Syn", Syn escapes to sea with Mipps and the rest of the Dymchurch smugglers, whereas "Captain Clegg" ends more faithfully to the novel, with Parson Blyss being killed by the mulatto (who is then killed by Mipps) and then being carried to and buried in Captain Clegg's empty grave by Mipps. "Captain Clegg" was released in the UK on DVD and Blu-ray in 2014; "Night Creatures" was never released on videotape in the United States, but is included in the 2014 two-disc DVD collection "The Hammer Horror Series".

"The Scarecrow of Romney Marsh" (1963) was produced for the "Walt Disney's Wonderful World of Color" TV series. It was shot on location in England and was directed by James Neilson. It stars Patrick McGoohan in the title role, with George Cole as Mipps and Sean Scully as John Banks, the younger son of Squire Banks (Michael Hordern). St Clement's Church in Old Romney doubled as Dr Syn's Dymchurch parish church in the production, and Disney funded the repair of the building in order to use it as a filming location.

Part One dealt with the arrival of General Pugh (Geoffrey Keen), who had been ordered by the War Office to smash the smuggling ring and prevent the Scarecrow from rescuing a Dymchurch man captured by a naval press gang as bait to trap the Scarecrow. Part Two depicted The Scarecrow dealing with the traitorous Joe Ransley (Patrick Wymark). Part Three showed how the Scarecrow rescued Harry Banks (David Buck) and American Simon Bates (Tony Britton) from General Pugh's clutches in Dover Castle.

While originally conceived and edited for American television (and announced in an advertisement by NBC in the Tuesday, July 9, 1963 issue of "The Hollywood Reporter"), "The Scarecrow of Romney Marsh" was re-edited for a British theatrical run before the American television debut. Retitled "Dr. Syn, Alias the Scarecrow", the British theatrical version was released on a double bill with "The Sword in the Stone", and ran during the 1963 Christmas season (advertised in the January 1964 issue of "Photoplay"). This version was shown in Europe as well as Central and South America through 1966.

In the 1970s, the production was re-edited again for its first American theatrical release, on double bills with both "Snow White and the Seven Dwarfs" and "Treasure Island". (The VHS version of the 1980s, sharing the removal of the Scarecrow's laugh from Terry Gilkyson's title song, was expanded to include the story material from all three TV episodes, while retaining feature film structure and credits; it was available for a relatively short amount of time.) Shortly after the US theatrical run, it was re-edited once more for a two-part presentation on Disney's television series in the 1970s, simply omitting the middle segment. The original three-part version was first shown as part of "Walt Disney's Wonderful World of Color" on February 9, 16 and 23, 1964. Later it was included in a late 1980s "Wonderful World of Disney" syndication rerun package, and cablecast in 1990s on the Disney Channel. This version generally followed the storyline of "The Further Adventures of Dr. Syn" and made it clear that Syn did not die or stage his own death: at film's end, he is having a cup of tea with the Squire, who admits to now owing a debt of gratitude to the Scarecrow.

On November 11, 2008 The Walt Disney Company released a limited pressing of 39,500 issues of "The Scarecrow of Romney Marsh" in DVD format for the first time as a part of the "" collection, and was now called "Dr. Syn: The Scarecrow of Romney Marsh". The issue sold out in three weeks, but as of February 17, 2009 the DVD was made available for members of the Disney Movie Club for $29.95. The two-disc set includes the American television version and the original British theatrical version "Dr. Syn, Alias the Scarecrow" in widescreen format. It also includes the original introductions by Walt Disney (in which he erroneously indicates that Dr. Syn was an actual historical figure) and a documentary on Disney's interest in the property. In October 2019, the Disney Movie Club released the film on Blu-ray, this time titled "The Scarecrow of Romney Marsh." The single disc contains the three episodes as originally broadcast in 1963, with Walt Disney's introductions (but with none of the supplemental features that appear on previous releases).

Made in 1974, "Carry On Dick", of the celebrated "Carry On" series of films, followed the same premise of a country vicar (Sid James) who is secretly an outlaw, in this case the highwayman Dick Turpin.

In 2001 a stage adaptation titled "Doctor Syn" was performed at churches throughout the Romney Marsh, the final night being performed in Dymchurch itself. The cast featured Daniel Thorndike (the author's son), Michael Fields, Steven Povey and Ben Barton, along with various amateurs from the area.


Rufus Sewell read a 10-part audio adaptation combining and abridging "Doctor Syn on the High Seas" and "Doctor Syn Returns" for BBC Radio, broadcast on BBC Radio 7 in December 2006 and repeated in June 2007.

A 10-part audio adaptation of "The Further Adventures of Doctor Syn" (combining and abridging "The Further Adventures of Doctor Syn" and "The Shadow of Doctor Syn") read by Rufus Sewell was performed on BBC Radio 7 in December 2007.

In April 2009, a third series was announced for broadcast later in 2009. BBC Radio 7 broadcast the six-part series, an abridged reading by Rufus Sewell of the original "Doctor Syn" novel, from January 4, 2010 to January 11.

John Paul Jones of Led Zeppelin reinterpreted elements of the Doctor Syn story as his "No Quarter" fantasy sequence in Led Zeppelin's concert film "The Song Remains the Same".

A three-issue adaptation of the Disney production was published by Gold Key Comics under the "Scarecrow of Romney Marsh" title, spanning April 1964 through October 1965.

A much abridged revision of the adventures of Dr. Syn appeared as a short comic serialized in the monthly publication "Disney Adventures". The new story features the heroic Doctor and his young sidekick protecting innocent villagers from corrupt government officials and soldiers. "Disney Adventures" would also produce a crossover story with the "Pirates of the Caribbean" franchise where Dr. Syn meets Captain Jack Sparrow.

Doctor Syn appears in the "League of Extraordinary Gentlemen" series as a member of the league gathered by Lemuel Gulliver. His alter ego, Captain Clegg, also makes appearances, where he is mentioned to have had a brief romantic liaison with future teammate Fanny Hill. In the 2003 film adaptation of "League", Dr. Syn can be spotted in one of the portraits hanging on the wall in M's library.

A "Days of Syn" festival is held even-numbered years by Dymchurch residents for fund-raising. The 2006 "Days of Syn" was on 26–28 August (UK August Bank Holiday weekend) and featured a talk on Dr. Syn at the Anglican church at 6:30 p.m. On Sunday at 3 p.m. there was a church service where Dr. Syn and the cast appeared in period costume. On Monday, starting at the Bowery Hall, scenes were reenacted from "Doctor Syn", and again during the day along the Dymchurch shoreline and in the Ocean pub.

In 2009, discussions took place to build a 100 ft high statue of "The Scarecrow" on a site in the centre of Romney Marsh. This had not been done by 2016.

Doctor Syn is also the name given to one of the locomotives on the Romney, Hythe and Dymchurch Railway.

Doctor Syn also inspired novelist George Chittenden who captures smuggling on the Kent coast in his highly praised ("sources?") debut "The Boy Who Led Them", which follows the rise and fall of a smuggling gang leader further down the coast in the notorious town of Deal.




</doc>
<doc id="8709" url="https://en.wikipedia.org/wiki?curid=8709" title="Dhrystone">
Dhrystone

Dhrystone is a synthetic computing benchmark program developed in 1984 by Reinhold P. Weicker intended to be representative of system (integer) programming. The Dhrystone grew to become representative of general processor (CPU) performance. The name "Dhrystone" is a pun on a different benchmark algorithm called Whetstone.

With Dhrystone, Weicker gathered meta-data from a broad range of software, including programs written in FORTRAN, PL/1, SAL, ALGOL 68, and Pascal. He then characterized these programs in terms of various common constructs: procedure calls, pointer indirections, assignments, etc. From this he wrote the Dhrystone benchmark to correspond to a representative mix. Dhrystone was published in Ada, with the C version for Unix developed by Rick Richardson ("version 1.1") greatly contributing to its popularity.

The Dhrystone benchmark contains no floating point operations, thus the name is a pun on the then-popular Whetstone benchmark for floating point operations. The output from the benchmark is the number of Dhrystones per second (the number of iterations of the main code loop per second).

Both Whetstone and Dhrystone are "synthetic" benchmarks, meaning that they are simple programs that are carefully designed to statistically mimic the processor usage of some common set of programs. Whetstone, developed in 1972, originally strove to mimic typical Algol 60 programs based on measurements from 1970, but eventually became most popular in its Fortran version, reflecting the highly numerical orientation of computing in the 1960s.

Dhrystone's eventual importance as an indicator of general-purpose ("integer") performance of new computers made it a target for commercial compiler writers. Various modern compiler static code analysis techniques (such as elimination of dead code: for example, code which uses the processor but produces internal results which are not used or output) make the use and design of synthetic benchmarks more difficult. Version 2.0 of the benchmark, released by Weicker and Richardson in March 1988, had a number of changes intended to foil a range of compiler techniques. Yet it was carefully crafted so as not to change the underlying benchmark. This effort to foil compilers was only partly successful. Dhrystone 2.1, released in May of the same year, had some minor changes and remains the current definition of Dhrystone.

Other than issues related to compiler optimization, various other issues have been cited with the Dhrystone. Most of these, including the small code size and small data set size, were understood at the time of its publication in 1984. More subtle is the slight over-representation of string operations, which is largely language-related: both Ada and Pascal have strings as normal variables in the language, whereas C does not, so what was simple variable assignment in reference benchmarks became buffer copy operations in the C library. Another issue is that the score reported does not include information which is critical when comparing systems such as which compiler was used, and what optimizations.

Dhrystone remains remarkably resilient as a simple benchmark, but its continuing value in establishing true performance is questionable. It is easy to use, well documented, fully self-contained, well understood, and can be made to work on almost any system. In particular, it has remained in broad use in the embedded computing world, though the recently developed EEMBC benchmark suite, HINT, Stream, and even Bytemark are widely quoted and used, as well as more specific benchmarks for the memory subsystem (Cachebench), TCP/IP (TTCP), and many others.

Dhrystone may represent a result more meaningfully than MIPS (million instructions per second) because instruction count comparisons between different instruction sets (e.g. RISC vs. CISC) can confound simple comparisons. For example, the same high-level task may require many more instructions on a RISC machine, but might execute faster than a single CISC instruction. Thus, the Dhrystone score counts only the number of program iteration completions per second, allowing individual machines to perform this calculation in a machine-specific way. Another common representation of the Dhrystone benchmark is the DMIPS (Dhrystone MIPS) obtained when the Dhrystone score is divided by 1757 (the number of Dhrystones per second obtained on the VAX 11/780, nominally a 1 MIPS machine).

Another way to represent results is in DMIPS/MHz, where DMIPS result is further divided by CPU frequency, to allow for easier comparison of CPUs running at different clock rates.

Using Dhrystone as a benchmark has pitfalls: 






</doc>
<doc id="8713" url="https://en.wikipedia.org/wiki?curid=8713" title="Dave Winer">
Dave Winer

Dave Winer (born May 2, 1955 in Queens, New York City) is an American software developer, entrepreneur, and writer who resides in New York City. Winer is noted for his contributions to outliners, scripting, content management, and web services, as well as blogging and podcasting. He is the founder of the software companies Living Videotext, Userland Software and Small Picture Inc., a former contributing editor for the Web magazine HotWired, the author of the "Scripting News" weblog, a former research fellow at Harvard Law School, and current visiting scholar at New York University's Arthur L. Carter Journalism Institute.

Winer was born on May 2, 1955, in Queens, New York City, the son of Eve Winer, Ph.D., a school psychologist, and Leon Winer, Ph.D., a former professor of the Columbia University Graduate School of Business. Winer is also the grandnephew of German novelist Arno Schmidt and a relative of Hedy Lamarr. He graduated from the Bronx High School of Science in 1972. Winer received a BA in Mathematics from Tulane University in New Orleans in 1976. In 1978 he received an MS in Computer Science from the University of Wisconsin–Madison.

In 1979 Dave Winer became an employee of Personal Software, where he worked on his own product idea named VisiText, which was his first attempt to build a commercial product around an "expand and collapse" outline display and which ultimately established outliners as a software product. In 1981 he left the company and founded Living Videotext to develop this still-unfinished product. The company was based in Mountain View, CA, and grew to more than 50 employees.

ThinkTank, which was based on VisiText, was released in 1983 for Apple II and was promoted as an "idea processor." It became the "first popular outline processor, the one that made the term generic." A ThinkTank release for the IBM PC followed in 1984, as well as releases for the Macintosh 128K and 512K. Ready, a RAM resident outliner for the IBM PC released in 1985, was commercially successful but soon succumbed to the competing Sidekick product by Borland. MORE, released for Apple's Macintosh in 1986, combined an outliner and a presentation program. It became "uncontested in the marketplace" and won the MacUser's Editor's Choice Award for "Best Product" in 1986.

In 1987, at the height of the company's success, Winer sold Living Videotext to Symantec for an undisclosed but substantial transfer of stock that "made his fortune." Winer continued to work at Symantec's Living Videotext division, but after six months he left the company in pursuit of other challenges.

Winer founded UserLand Software in 1988 and served as the company's CEO until 2002.

UserLand's original flagship product, Frontier, was a system-level scripting environment for the Mac, Winer's pioneering weblog, "Scripting News", takes its name from this early interest. Frontier was an outliner-based scripting language, echoing Winer's longstanding interest in outliners and anticipating code-folding editors of the late 1990s.
Winer became interested in web publishing while helping automate the production process of the strikers' online newspaper during San Francisco's newspaper strike of November 1994, According to Newsweek, through this experience, he "revolutionized Net publishing." Winer subsequently shifted the company's focus to online publishing products, enthusiastically promoting and experimenting with these products while building his websites and developing new features. One of these products was Frontier's NewsPage Suite of 1997, which supported the publication of Winer's "Scripting News" and was adopted by a handful of users who "began playing around with their own sites in the Scripting News vein." These users included notably Chris Gulker and Jorn Barger, who envisaged blogging as a networked practice among users of the software.

Winer was named a Seybold Fellow in 1997, to assist the executives and editors that comprised the Seybold Institute in ensuring "the highest quality and topicality" in their educational program, the Seybold Seminars; the honour was bestowed for his "pioneering work in web-based publishing systems." Keen to enter the "competitive arena of high-end Web development," Winer then came to collaborate with Microsoft and jointly developed the XML-RPC protocol. This led to the creation of SOAP, which he co-authored with Microsoft's Don Box, Bob Atkinson, and Mohsen Al-Ghosein.

In December 1997, acting on the desire to "offer much more timely information," Winer designed and implemented an XML syndication format for use on his "Scripting News" weblog, thus making an early contribution to the history of web syndication technology. By December 2000, competing dialects of RSS included several varieties of Netscape's RSS, Winer's RSS 0.92, and an RDF-based RSS 1.0. Winer continued to develop the branch of the RSS fork originating from RSS 0.92, releasing in 2002 a version called RSS 2.0. Winer's advocacy of web syndication in general and RSS 2.0 in particular convinced many news organizations to syndicate their news content in that format. For example, in early 2002 "The New York Times" entered an agreement with UserLand to syndicate many of their articles in RSS 2.0 format. Winer resisted calls by technologists to have the shortcomings of RSS 2.0 improved. Instead, he froze the format and turned its ownership over to Harvard University.

With products and services based on UserLand's Frontier system, Winer became a leader in blogging tools from 1999 onwards, as well as a "leading evangelist of weblogs." In 2000 Winer developed the Outline Processor Markup Language OPML, an XML format for outlines, which originally served as the native file format for Radio UserLand's outliner application and has since been adopted for other uses, the most common being to exchange lists of web feeds between web feed aggregators. UserLand was the first to add an "enclosure" tag in its RSS, modifying its blog software and its aggregator so that bloggers could easily link to an audio file (see podcasting and history of podcasting).

In February 2002 Winer was named one of the "Top Ten Technology Innovators" by InfoWorld.

In June 2002 Winer underwent life-saving bypass surgery to prevent a heart attack and as a consequence stepped down as CEO of UserLand shortly after. He remained the firm's majority shareholder, however, and claimed personal ownership of Weblogs.com.

As "one of the most prolific content generators in Web history," Winer has enjoyed a long career as a writer and has come to be counted among Silicon Valley's "most influential web voices."

Winer started "DaveNet", "a stream-of-consciousness newsletter distributed by e-mail" in November 1994 and maintained Web archives of the "goofy and informative" 800-word essays since January 1995, which earned him a Cool Site of the Day award in March 1995. From the start, the "Internet newsletter" "DaveNet" was widely read among industry leaders and analysts, who experienced it as a "real community." Dissatisfied with the quality of the coverage that the Mac and, especially, his own Frontier software received in the trade press, Winer saw "DaveNet" as an opportunity to "bypass" the conventional news channels of the software business. Satisfied with his success, he "reveled in the new direct email line he had established with his colleagues and peers, and in his ability to circumvent the media." In the early years, Winer often used "DaveNet" to vent his grievances against Apple's management, and as a consequence of his strident criticism came to be seen as "the most notorious of the disgruntled Apple developers." Redacted "DaveNet" columns were published weekly by the web magazine "HotWired" between June 1995 and May 1996. "DaveNet" was discontinued in 2004.

Winer's "Scripting News", described as "one of the [web's] oldest blogs," launched in February 1997 and earned him titles such as "protoblogger" and "forefather of blogging." "Scripting News" started as "a home for links, offhand observations, and ephemera" and allowed Winer to mix "his roles as a widely read pundit and an ambitious entrepreneur." Offering an "as-it-happened portrait of the work of writing software for the Web in the 1990s," the site became an "established must-read for industry insiders." "Scripting News" continues to be updated regularly.

Winer spent one year as a resident fellow at the Harvard Law School's Berkman Center for Internet & Society, where he worked on using weblogs in education. While there, he launched "Weblogs at Harvard Law School" using UserLand software, and held the first BloggerCon conferences. Winer's fellowship ended in June 2004.

In 2010 Winer was appointed Visiting Scholar at New York University's Arthur L. Carter Journalism Institute.

On December 19, 2012, Winer co-founded Small Picture, Inc. with Kyle Shank; Small Picture is a corporation that builds two outlining products, Little Outliner and Fargo. Little Outliner, an entry-level outliner designed to teach new users about outliners, which launched on March 25, 2013. Fargo, the company's "primary product", launched less than a month later, on April 17, 2013. Fargo is a free browser-based outliner which syncs with a user's Dropbox account. Small Picture has stated that in future it may offer paid-for services to Fargo users.

In February 1996, while working as a columnist for HotWired, Winer organized 24 Hours of Democracy, an online protest against the recently passed Communications Decency Act. As part of the protest, over 1,000 people, among them Microsoft chairman Bill Gates, posted essays to the Web on the subject of democracy, civil liberty and freedom of speech.

In December 1999, Winer became the "proprietor of a growing free blog service" at EditThisPage.com, hosting "approximately 20,000 sites" in February 2001. The service closed in December 2005.

Winer has been given "credit for the invention of the podcasting model." Having received user requests for audioblogging features since October 2000, especially from Adam Curry, Winer decided to include new functionality in RSS 0.92 by defining a new element called "enclosure," which would pass the address of a media file to the RSS aggregator. He demonstrated the RSS enclosure feature on January 11, 2001 by enclosing a Grateful Dead song in his "Scripting News" weblog.

Winer's weblogging product, Radio Userland, the program favored by Curry, had a built-in aggregator and thus provided both the "send" and "receive" components of what was then called audioblogging. In July 2003 Winer challenged other aggregator developers to provide support for enclosures. In October 2003, Kevin Marks demonstrated a script to download RSS enclosures and pass them to iTunes for transfer to an iPod. Curry then offered an RSS-to-iPod script that moved MP3 files from Radio UserLand to iTunes. The term "podcasting" was suggested by Ben Hammersley in February 2004.

Winer also has an occasional podcast, Morning Coffee Notes, which has featured guests such as Doc Searls, Mike Kowalchik, Jason Calacanis, Steve Gillmor, Peter Rojas, Cecile Andrews, Adam Curry, Betsy Devine and others.

BloggerCon is a user-focused conference for the blogger community. BloggerCon I (October 2003) and II (April 2004), were organized by Dave Winer and friends at Harvard Law School's Berkman Center for the Internet and Society in Cambridge, Mass. BloggerCon III met at Stanford Law School on November 6, 2004.

Weblogs.com provided a free ping-server used by many blogging applications, as well as free hosting to many bloggers. After leaving Userland, Winer claimed personal ownership of the site, and in mid-June 2004 he shut down its free blog-hosting service, citing lack of resources and personal problems. A swift and orderly migration off Winer's server was facilitated by Rogers Cadenhead, whom Winer then hired to port the server to a more stable platform. In October, 2005, VeriSign bought the Weblogs.com ping-server from Winer and promised that its free services would remain free. The podcasting-related web site audio.weblogs.com was also included in the $2.3 million deal.

Winer opened his self-described "commons for sharing outlines, feeds, and taxonomy" in May 2006. The site allowed users to publish and syndicate blogrolls and aggregator subscriptions using OPML. Winer suspended its service in January 2008.

Since 2009, Winer has collaborated with New York University's associate professor of journalism Jay Rosen on "Rebooting the News", a weekly podcast on technology and innovation in journalism. It was announced on July 1, 2011 that the show would be on break, as NYU itself was, from June to September. However, no new episodes have been released since, making show #94 released on May 23, 2011 the last.




</doc>
<doc id="8714" url="https://en.wikipedia.org/wiki?curid=8714" title="December 10">
December 10





</doc>
<doc id="8715" url="https://en.wikipedia.org/wiki?curid=8715" title="Taiko">
Taiko

Taiko have a mythological origin in Japanese folklore, but historical records suggest that taiko were introduced to Japan through Korean and Chinese cultural influence as early as the 6th century CE. Some taiko are similar to instruments originating from India. Archaeological evidence also supports the view that taiko were present in Japan during the 6th century in the Kofun period. Their function has varied throughout history, ranging from communication, military action, theatrical accompaniment, and religious ceremony to both festival and concert performances. In modern times, taiko have also played a central role in social movements for minorities both within and outside Japan.

"Kumi-daiko" performance, characterized by an ensemble playing on different drums, was developed in 1951 through the work of Daihachi Oguchi and has continued with groups such as Kodo. Other performance styles, such as "hachijō-daiko", have also emerged from specific communities in Japan. "Kumi-daiko" performance groups are active not only in Japan, but also in the United States, Australia, Canada, Europe, Taiwan, and Brazil. Taiko performance consists of many components in technical rhythm, form, stick grip, clothing, and the particular instrumentation. Ensembles typically use different types of barrel-shaped "nagadō-daiko" as well as smaller "shime-daiko". Many groups accompany the drums with vocals, strings, and woodwind instruments.

The origin of the instruments is unclear, though there have been many suggestions. Historical accounts, of which the earliest date from 588 CE, note that young Japanese men traveled to Korea to study the kakko, a drum that originated in South China. This study and appropriation of Chinese instruments may have influenced the emergence of taiko. Certain court music styles, especially gigaku and gagaku, arrived in Japan through both Korea and China. In both traditions, dancers were accompanied by several instruments that included drums similar to taiko. Certain percussive patterns and terminology in togaku, an early dance and music style in Japan, in addition to physical features of the kakko, also reflect influence from both China and India on drum use in gagaku performance.

Archaeological evidence shows that taiko were used in Japan as early as the 6th century CE, during the latter part of the Kofun period, and were likely used for communication, in festivals, and in other rituals. This evidence was substantiated by the discovery of haniwa statues in the Sawa District of Gunma Prefecture. Two of these figures are depicted playing drums; one of them, wearing skins, is equipped with a barrel-shaped drum hung from his shoulder and uses a stick to play the drum at hip height. This statue is titled "Man Beating the Taiko" and is considered the oldest evidence of taiko performance in Japan. Similarities between the playing style demonstrated by this haniwa and known music traditions in Korea and China further suggest influences from these regions.

The "Nihon Shoki", the second oldest book of Japanese classical history, contains a mythological story describing the origin of taiko. The myth tells how Amaterasu, who had sealed herself inside a cave in anger, was beckoned out by an elder goddess Ame-no-Uzume when others had failed. Ame-no-Uzume accomplished this by emptying out a barrel of sake and dancing furiously on top of it. Historians regard her performance as the mythological creation of taiko music.

In feudal Japan, taiko were often used to motivate troops, call out orders or announcements, and set a marching pace; marches were usually set to six paces per beat of the drum. During the 16th-century Warring States period, specific drum calls were used to communicate orders for retreating and advancing. Other rhythms and techniques were detailed in period texts. According to the war chronicle "Gunji Yoshū", nine sets of five beats would summon an ally to battle, while nine sets of three beats, sped up three or four times, was the call to advance and pursue an enemy. Folklore from the 16th century on the legendary 6th-century Emperor Keitai offers a story that he obtained a large drum from China, which he named . The Emperor was thought to have used it to both encourage his own army and intimidate his enemies.

Taiko have been incorporated in Japanese theatre for rhythmic needs, general atmosphere, and in certain settings decoration. In the kabuki play "The Tale of Shiroishi and the Taihei Chronicles", scenes in the pleasure quarters are accompanied by taiko to create dramatic tension. Noh theatre also feature taiko where performance consists of highly specific rhythmic patterns. The school of drumming, for example, contains 65 basic patterns in addition to 25 special patterns; these patterns are categorized in several classes. Differences between these patterns include changes in tempo, accent, dynamics, pitch, and function in the theatrical performance. Patterns are also often connected together in progressions.

Taiko continue to be used in gagaku, a classical music tradition typically performed at the Tokyo Imperial Palace in addition to local temples and shrines. In gagaku, one component of the art form is traditional dance, which is guided in part by the rhythm set by the taiko. Taiko have played an important role in many local festivals across Japan. They are also used to accompany religious ritual music. In kagura, which is a category of music and dances stemming from Shinto practices, taiko frequently appear alongside other performers during local festivals. In Buddhist traditions, taiko are used for ritual dances that are a part of the Bon Festival. Taiko, along with other instruments, are featured atop towers that are adorned with red-and-white cloth and serve to provide rhythms for the dancers who are encircled around the performers.

In addition to the instruments, the term "taiko" also refers to the performance itself, and commonly to one style called "kumi-daiko", or ensemble-style playing (as opposed to festival performances, rituals, or theatrical use of the drums). "Kumi-daiko" was developed by Daihachi Oguchi in 1951. He is considered a master performer and helped transform taiko performance from its roots in traditional settings in festivals and shrines. Oguchi was trained as a jazz musician in Nagano, and at one point, a relative gave him an old piece of written taiko music. Unable to read the traditional and esoteric notation, Oguchi found help to transcribe the piece, and on his own added rhythms and transformed the work to accommodate multiple taiko players on different-sized instruments. Each instrument served a specific purpose that established present-day conventions in "kumi-daiko" performance.

Oguchi's ensemble, Osuwa Daiko, incorporated these alterations and other drums into their performances. They also devised novel pieces that were intended for non-religious performances. Several other groups emerged in Japan through the 1950s and 1960s. Oedo Sukeroku Daiko was formed in Tokyo in 1959 under Seidō Kobayashi, and has been referred to as the first taiko group who toured professionally. Globally, "kumi-daiko" performance became more visible during the 1964 Summer Olympics in Tokyo, when it was featured during the Festival of Arts event.

"Kumi-daiko" was also developed through the leadership of , who gathered young men who were willing to devote their entire lifestyle to taiko playing and took them to Sado Island for training where Den and his family had settled in 1968. Den chose the island based on a desire to reinvigorate the folk arts in Japan, particularly taiko; he became inspired by a drumming tradition unique to Sado called that required considerable strength to play well. Den called the group "Za Ondekoza" or Ondekoza for short, and implemented a rigorous set of exercises for its members including long-distance running. In 1975, Ondekoza was the first taiko group to tour in the United States. Their first performance occurred just after the group finished running the Boston Marathon while wearing their traditional uniforms. In 1981, some members of Ondekoza split from Den and formed another group called Kodo under the leadership of Eitetsu Hayashi. Kodo continued to use Sado Island for rigorous training and communal living, and went on to popularize taiko through frequent touring and collaborations with other musical performers. Kodo is one of the most recognized taiko groups both in Japan and worldwide.

Estimates of the number of taiko groups in Japan vary up to 5000 active in Japan, but more conservative assessments place the number closer to 800 based on membership in the Nippon Taiko Foundation, the largest national organization of taiko groups. Some pieces that have emerged from early "kumi-daiko" groups that continue to be performed include "Yatai-bayashi" from Ondekoza, from Osuwa Daiko, and from Kodo.

Taiko have been developed into a broad range of percussion instruments that are used in both Japanese folk and classical musical traditions. An early classification system based on shape and tension was advanced by Francis Taylor Piggott in 1909. Taiko are generally classified based on the construction process, or the specific context in which the drum is used, but some are not classified, such as the toy den-den daiko.

With few exceptions, taiko have a drum shell with heads on both sides of the body, and a sealed resonating cavity. The head may be fastened to the shell using a number of different systems, such as using ropes. Taiko may be either tunable or non-tunable depending on the system used.

Taiko are categorized into three types based on construction process. "Byō-uchi-daiko" are constructed with the drumhead nailed to the body. "Shime-daiko" are classically constructed with the skin placed over iron or steel rings, which are then tightened with ropes. Contemporary "shime-daiko" are tensioned using bolts or turnbuckles systems attached to the drum body. "Tsuzumi" are also rope-tensioned drums, but have a distinct hourglass shape and their skins are made using deerskin.

"Byō-uchi-daiko" were historically made only using a single piece of wood; they continue to be made in this manner, but are also constructed from staves of wood. Larger drums can be made using a single piece of wood, but at a much greater cost due to the difficulty in finding appropriate trees. The preferred wood is the Japanese zelkova or "keyaki", but a number of other woods, and even wine barrels, have been used to create taiko. "Byō-uchi-daiko" cannot be tuned.

The typical "byō-uchi-daiko" is the "nagadō-daiko", an elongated drum that is roughly shaped like a wine barrel. "Nagadō-daiko" are available in a variety of sizes, and their head diameter is traditionally measured in shaku (units of roughly 30 cm). Head diameters range from . are the smallest of these drums and are usually about in diameter. The "chū-daiko " is a medium-sized "nagadō-daiko" ranging from , and weighing about . vary in size, and are often as large as in diameter. Some "ō-daiko" are difficult to move due to their size, and therefore permanently remain inside the performance space, such as temple or shrine. "Ō-daiko" means "large drum" and for a given ensemble, the term refers to their largest drum. The other type of "byō-uchi-daiko" is called a and can be any drum constructed such that the head diameter is greater than the length of the body.

"Shime-daiko" are a set of smaller, roughly snare drum-sized instrument that are tunable. The tensioning system usually consists of hemp cords or rope, but bolt or turnbuckle systems have been used as well. , sometimes referred to as "taiko" in the context of theater, have thinner heads than other kinds of shime-daiko. The head includes a patch of deerskin placed in the center, and in performance, drum strokes are generally restricted to this area. The is a heavier type of "shime-daiko". They are available in sizes 1–5, and are named according to their number: "namitsuke" (1), "nichō-gakke" (2), "sanchō-gakke" (3), "yonchō-gakke" (4), and "gochō-gakke" (5). The "namitsuke" has the thinnest skins and the shortest body in terms of height; thickness and tension of skins, as well as body height, increase toward the "gochō-gakke". The head diameters of all "shime-daiko" sizes are around .

"Okedō-daiko" or simply "okedō", are a type of "shime-daiko" that are stave-constructed using narrower strips of wood, have a tube-shaped frame. Like other "shime-daiko", drum heads are attached by metal hoops and fastened by rope or cords. "Okedō" can be played using the same drumsticks (called "bachi") as "shime-daiko", but can also be hand-played. "Okedō" come in short- and long-bodied types.

"Tsuzumi" are a class of hourglass-shaped drums. The drum body is shaped on a spool and the inner body carved by hand. Their skins can be made from cowhide, horsehide, or deerskin. While the "ō-tsuzumi" skins are made from cowhide, "ko-tsuzumi" are made from horsehide. While some classify "tsuzumi" as a type of taiko, others have described them as a drum entirely separate from taiko.

Taiko can also be categorized by the context in which they are used. The "miya-daiko", for instance, is constructed in the same manner as other "byō-uchi-daiko", but is distinguished by an ornamental stand and is used for ceremonial purposes at Buddhist temples. The (a "ko-daiko") and (a "nagadō-daiko" with a cigar-shaped body) are used in sumo and festivals respectively.
Several drums, categorized as "gagakki", are used in the Japanese theatrical form, gagaku. The lead instrument of the ensemble is the kakko, which is a smaller "shime-daiko" with heads made of deerskin, and is placed horizontally on a stand during performance. A "tsuzumi", called the "san-no-tsuzumi" is another small drum in gagaku that is placed horizontally and struck with a thin stick. are the largest drums of the ensemble, and have heads that are about in diameter. During performance, the drum is placed on a tall pedestals and surrounded by a rim decoratively painted with flames and adorned with mystical figures such as wyverns. "Dadaiko" are played while standing, and are usually only played on the downbeat of the music. The is a smaller drum that produces a lower sound, its head measuring about in diameter. It is used in ensembles that accompany bugaku, a traditional dance performed at the Tokyo Imperial Palace and in religious contexts. "Tsuri-daiko" are suspended on a small stand, and are played sitting down. "Tsuri-daiko" performers typically use shorter mallets covered in leather knobs instead of bachi. They can be played simultaneously by two performers; while one performer plays on the head, another performer uses bachi on the body of the drum.

The larger "ō-tsuzumi" and smaller "ko-tsuzumi" are used in the opening and dances of Noh theater. Both drums are struck using the fingers; players can also adjust pitch by manually applying pressure to the ropes on the drum. The color of the cords of these drums also indicates the skill of the musician: Orange and red for amateur players, light blue for performers with expertise, and lilac for masters of the instrument. "Nagauta-shime daiko" or "uta daiko" are also featured in Noh performance.

Many taiko in Noh are also featured in kabuki performance and are used in a similar manner. In addition to the "ō-tsuzumi", "ko-tsuzumi", and "nagauta-shime daiko", Kabuki performances make use of the larger "ō-daiko" offstage to help set the atmosphere for different scenes.

Taiko construction has several stages, including making and shaping of the drum body (or shell), preparing the drum skin, and tuning the skin to the drumhead. Variations in the construction process often occur in the latter two parts of this process. Historically, "byō-uchi-daiko" were crafted from trunks of the Japanese zelkova tree that were dried out over years, using techniques to prevent splitting. A master carpenter then carved out the rough shape of the drum body with a chisel; the texture of the wood after carving softened the tone of the drum. In contemporary times, taiko are carved out on a large lathe using wood staves or logs that can be shaped to fit drum bodies of various sizes. Drumheads can be left to air-dry over a period of years, but some companies use large, smoke-filled warehouses to hasten the drying process. After drying is complete, the inside of the drum is worked with a deep-grooved chisel and sanded. Lastly, handles are placed onto the drum. These are used to carry smaller drums and they serve an ornamental purpose for larger drums.
The skins or heads of taiko are generally made from cowhide from Holstein cows aged about three or four years. Skins also come from horses, and bull skin is preferred for larger drums. Thinner skins are preferred for smaller taiko, and thicker skins are used for larger ones. On some drumheads, a patch of deer skin placed in the center serves as the target for many strokes during performance. Before fitting it to the drum body the hair is removed from the hide by soaking it in a river or stream for about a month; winter months are preferred as colder temperatures better facilitate hair removal. To stretch the skin over the drum properly, one process requires the body to be held on a platform with several hydraulic jacks underneath it. The edges of the cowhide are secured to an apparatus below the jacks, and the jacks stretch the skin incrementally to precisely apply tension across the drumhead. Other forms of stretching use rope or cords with wooden dowels or an iron wheel to create appropriate tension. Small tension adjustments can be made during this process using small pieces of bamboo that twist around the ropes. Particularly large drumheads are sometimes stretched by having several workers, clad in stockings, hop rhythmically atop it, forming a circle along the edge. After the skin has dried, tacks, called "byō", are added to the appropriate drums to secure it; "chū-daiko" require about 300 of them for each side. After the body and skin have been finished, excess hide is cut off and the drum can be stained as needed.

Several companies specialize in the production of taiko. One such company that created drums exclusively for the Emperor of Japan, Miyamoto Unosuke Shoten in Tokyo, has been making taiko since 1861. The Asano Taiko Corporation is another major taiko-producing organization, and has been producing taiko for over 400 years. The family-owned business started in Mattō, Ishikawa, and, aside from military equipment, made taiko for Noh theater and later expanded to creating instruments for festivals during the Meiji period. Asano currently maintains an entire complex of large buildings referred to as Asano Taiko Village, and the company reports producing up to 8000 drums each year. As of 2012, there is approximately one major taiko production company in each prefecture of Japan, with some regions having several companies. Of the manufacturers in Naniwa, Taikoya Matabē is one of the most successful and is thought to have brought considerable recognition to the community and attracted many drum makers there. Umetsu Daiko, a company that operates in Hakata, has been producing taiko since 1821.

Taiko performance styles vary widely across groups in terms of the number of performers, repertoire, instrument choices, and stage techniques. Nevertheless, a number of early groups have had broad influence on the tradition. For instance, many pieces developed by Ondekoza and Kodo are considered standard in many taiko groups.

Kata is the posture and movement associated with taiko performance. The notion is similar to that of kata in martial arts: for example, both traditions include the idea that the hara is the center of being. Author Shawn Bender argues that kata is the primary feature that distinguishes different taiko groups from one another and is a key factor in judging the quality of performance. For this reason, many practice rooms intended for taiko contain mirrors to provide visual feedback to players. An important part of kata in taiko is keeping the body stabilized while performing and can be accomplished by keeping a wide, low stance with the legs, with the left knee bent over the toes and keeping the right leg straight. It is important that the hips face the drum and the shoulders are relaxed. Some teachers note a tendency to rely on the upper body while playing and emphasize the importance of the holistic use of the body during performance.

Some groups in Japan, particularly those active in Tokyo, also emphasize the importance of the lively and spirited "iki" aesthetic. In taiko, it refers to very specific kinds of movement while performing that evoke the sophistication stemming from the mercantile and artisan classes active during the Edo period (1603–1868).
The sticks for playing taiko are called "bachi", and are made in various sizes and from different kinds of wood such as white oak, bamboo, and Japanese magnolia. "Bachi" are also held in a number of different styles. In "kumi-daiko", it is common for a player to hold their sticks in a relaxed manner between the V-shape of the index finger and thumb, which points to the player. There are other grips that allow performers to play much more technically difficult rhythms, such as the "shime" grip, which is similar to a matched grip: the "bachi" are gripped at the back end, and the fulcrum rests between the performer's index finger and thumb, while the other fingers remain relaxed and slightly curled around the stick.

Performance in some groups is also guided by principles based on Zen Buddhism. For instance, among other concepts, the San Francisco Taiko Dojo is guided by emphasizing communication, respect, and harmony. The way the "bachi" are held can also be significant; for some groups, "bachi" represent a spiritual link between the body and the sky. Some physical parts of taiko, like the drum body, its skin, and the tacks also hold symbolic significance in Buddhism.

"Kumi-daiko" groups consist primarily of percussive instruments where each of the drums plays a specific role. Of the different kinds of taiko, the most common in groups is the "nagadō-daiko". "Chū-daiko" are common in taiko groups and represent the main rhythm of the group, whereas "shime-daiko" set and change tempo. A "shime-daiko" often plays the Jiuchi, a base rhythm holding together the ensemble. "Ō-daiko" provide a steady, underlying pulse and serve as a counter-rhythm to the other parts. It is common for performances to begin with a single stroke roll called an "". The player starts slowly, leaving considerable space between strikes, gradually shortening the interval between hits, until the drummer is playing a rapid roll of hits. Oroshi are also played as a part of theatrical performance, such as in Noh theater.

Drums are not the only instruments played in the ensemble; other Japanese instruments are also used. Other kinds of percussion instruments include the , a hand-sized gong played with a small mallet. In kabuki, the shamisen, a plucked string instrument, often accompanies taiko during the theatrical performance. "Kumi-daiko" performances can also feature woodwinds such as the shakuhachi and the shinobue.

Voiced calls or shouts called kakegoe and kiai are also common in taiko performance. They are used as encouragement to other players or cues for transition or change in dynamics such as an increase in tempo. In contrast, the philosophical concept of ma, or the space between drum strikes, is also important in shaping rhythmic phrases and creating appropriate contrast.

There is a wide variety of traditional clothing that players wear during taiko performance. Common in many "kumi-daiko" groups is the use of the happi, a decorative, thin-fabric coat, and traditional headbands called hachimaki. Tabi, , and are also typical. During his time with the group Ondekoza, Eitetsu Hayashi suggested that a loincloth called a fundoshi be worn when performing for French fashion designer Pierre Cardin, who saw Ondekoza perform for him in 1975. The Japanese group Kodo has sometimes worn fundoshi for its performances.

Taiko performance is generally taught orally and through demonstration. Historically, general patterns for taiko were written down, such as in the 1512 encyclopedia called the "Taigensho", but written scores for taiko pieces are generally unavailable. One reason for the adherence to an oral tradition is that, from group to group, the rhythmic patterns in a given piece are often performed differently. Furthermore, ethnomusicologist William P. Malm observed that Japanese players within a group could not usefully predict one another using written notation, and instead did so through listening. In Japan, printed parts are not used during lessons.

Orally, patterns of onomatopoeia called kuchi shōga are taught from teacher to student that convey the rhythm and timbre of drum strikes for a particular piece. For example, represents a single strike to the center of the drum, where as represents two successive strikes, first by the right and then the left, and lasts the same amount of time as one "don" strike. Some taiko pieces, such as "Yatai-bayashi", include patterns that are difficult to represent in Western musical notation. The exact words used can also differ from region to region.

More recently, Japanese publications have emerged in an attempt to standardize taiko performance. The Nippon Taiko Foundation was formed in 1979; its primary goals were to foster good relations among taiko groups in Japan and to both publicize and teach how to perform taiko. Daihachi Oguchi, the leader of the Foundation, wrote "Japan Taiko" with other teachers in 1994 out of concern that correct form in performance would degrade over time. The instructional publication described the different drums used in "kumi-daiko" performance, methods of gripping, correct form, and suggestions on instrumentation. The book also contains practice exercises and transcribed pieces from Oguchi's group, Osuwa Daiko. While there were similar textbooks published before 1994, this publication had much more visibility due to the Foundation's scope.

The system of fundamentals "Japan Taiko" put forward was not widely adopted because taiko performance varied substantially across Japan. An updated 2001 publication from the Foundation, called the , describes regional variations that depart from the main techniques taught in the textbook. The creators of the text maintained that mastering a set of prescribed basics should be compatible with learning local traditions.

Aside from "kumi-daiko" performance, a number of folk traditions that use taiko have been recognized in different regions in Japan. Some of these include from Sado Island, ' from the town of Kokura, and ' from Iwate Prefecture.

A variety of folk dances originating from Okinawa, known collectively as eisa, often make use of the taiko. Some performers use drums while dancing, and generally speaking, perform in one of two styles: groups on the Yokatsu Peninsula and on Hamahiga Island use small, single-sided drums called whereas groups near the city of Okinawa generally use "shime-daiko". Use of "shime-daiko" over "pāranku" has spread throughout the island, and is considered the dominant style. Small "nagadō-daiko", referred to as "ō-daiko" within the tradition, are also used and are worn in front of the performer. These drum dances are not limited to Okinawa and have appeared in places containing Okinawan communities such as in São Paulo, Hawaii, and large cities on the Japanese mainland.

 is a taiko tradition originating on the island of Hachijō-jima. Two styles of "Hachijō-daiko" emerged and have been popularized among residents: an older tradition based on a historical account, and a newer tradition influenced by mainland groups and practiced by the majority of the islanders.

The "Hachijō-daiko" tradition was documented as early as 1849 based on a journal kept by an exile named Kakuso Kizan. He mentioned some of its unique features, such as "a taiko is suspended from a tree while women and children gathered around", and observed that a player used either side of the drum while performing. Illustrations from Kizan's journal show features of "Hachijō-daiko". These illustrations also featured women performing, which is unusual as taiko performance elsewhere during this period was typically reserved for men. Teachers of the tradition have noted that the majority of its performers were women; one estimate asserts that female performers outnumbered males by three to one.
The first style of Hachijō-daiko is thought to descend directly from the style reported by Kizan. This style is called "Kumaoji-daiko", named after its creator Okuyama Kumaoji, a central performer of the style. "Kumaoji-daiko" has two players on a single drum, one of whom, called the , provides the underlying beat. The other player, called the , builds on this rhythmical foundation with unique and typically improvised rhythms. While there are specific types of underlying rhythms, the accompanying player is free to express an original musical beat. "Kumaoji-daiko" also features an unusual positioning for taiko: the drums are sometimes suspended from ropes, and historically, sometimes drums were suspended from trees.

The contemporary style of "Hachijō-daiko" is called , which differs from "Kumaoji-daiko" in multiple ways. For instance, while the lead and accompanying roles are still present, "shin-daiko" performances use larger drums exclusively on stands. "Shin-daiko" emphasizes a more powerful sound, and consequently, performers use larger bachi made out of stronger wood. Looser clothing is worn by "shin-daiko" performers compared to kimono worn by "Kumaoji-daiko" performers; the looser clothing in "shin-daiko" allow performers to adopt more open stances and larger movements with the legs and arms. Rhythms used for the accompanying "shita-byōshi" role can also differ. One type of rhythm, called "yūkichi", consists of the following: 

This rhythm is found in both styles, but is always played faster in "shin-daiko". Another type of rhythm, called "honbadaki", is unique to "shin-daiko" and also contains a song which is performed in standard Japanese.

 is a style that has spread amongst groups through Kodo, and is formally known as . The word "miyake" comes from Miyake-jima, part of the Izu Islands, and the word "Kamitsuki" refers to the village where the tradition came from. Miyake-style taiko came out of performances for — a traditional festival held annually in July on Miyake Island since 1820 honoring the deity Gozu Tennō. In this festival, players perform on taiko while portable shrines are carried around town. The style itself is characterized in a number of ways. A "nagadō-daiko" is typically set low to the ground and played by two performers, one on each side; instead of sitting, performers stand and hold a stance that is also very low to the ground, almost to the point of kneeling.

Taiko groups in Australia began forming in the 1990s. The first group, called Ataru Taru Taiko, was formed in 1995 by Paulene Thomas, Harold Gent, and Kaomori Kamei. TaikOz was later formed by percussionist Ian Cleworth and Riley Lee, a former Ondekoza member, and has been performing in Australia since 1997. They are known for their work in generating interest in performing taiko among Australian audiences, such as by developing a complete education program with both formal and informal classes, and have a strong fan base. Cleworth and other members of the group have developed several original pieces.

The introduction of "kumi-daiko" performance in Brazil can be traced back to the 1970s and 1980s in São Paulo. Tangue Setsuko founded an eponymous taiko dojo and was Brazil's first taiko group; Setsuo Kinoshita later formed the group Wadaiko Sho. Brazilian groups have combined native and African drumming techniques with taiko performance. One such piece developed by Kinoshita is called "Taiko de Samba", which emphasizes both Brazilian and Japanese aesthetics in percussion traditions. Taiko was also popularized in Brazil from 2002 through the work of Yukihisa Oda, a Japanese native who visited Brazil several times through the Japan International Cooperation Agency.

The Brazilian Association of Taiko (ABT) suggests that there are about 150 taiko groups in Brazil and that about 10–15% of players are non-Japanese; Izumo Honda, coordinator of a large annual festival in São Paulo, estimated that about 60% of all taiko performers in Brazil are women.

Taiko emerged in the United States in the late 1960s. The first group, San Francisco Taiko Dojo, was formed in 1968 by Seiichi Tanaka, a postwar immigrant who studied taiko in Japan and brought the styles and teachings to the US. A year later, a few members of Senshin Buddhist Temple in Los Angeles led by its minister Masao Kodani initiated another group called Kinnara Taiko. San Jose Taiko later formed in 1973 in Japantown, San Jose, under Roy and PJ Hirabayashi. Taiko started to branch out to the eastern US in the late 1970s. This included formation of Denver Taiko in 1976 and Soh Daiko in New York City in 1979. Many of these early groups lacked the resources to equip each member with a drum and resorted to makeshift percussion materials such as rubber tires or creating taiko out of wine barrels.

Japanese-Canadian taiko began in 1979 with Katari Taiko, and was inspired by the San Jose Taiko group. Its early membership was predominantly female. Katari Taiko and future groups were thought to represent an opportunity for younger, third-generation Japanese Canadians to explore their roots, redevelop a sense of ethnic community, and expand taiko into other musical traditions.

There are no official counts or estimates of the number of active taiko groups in the United States or Canada, as there is no governing body for taiko groups in either country. Unofficial estimates have been made. In 1989, there were as many as 30 groups in the US and Canada, seven of which were in California. One estimate suggested that around 120 groups were active in the US and Canada as of 2001, many of which could be traced to the San Francisco Taiko Dojo; later estimates in 2005 and 2006 suggested there were about 200 groups in the United States alone.

The Cirque du Soleil shows "Mystère" in Las Vegas and "Dralion" have featured taiko performance. Taiko performance has also been featured in commercial productions such as the 2005 Mitsubishi Eclipse ad campaign, and in events such as the 2009 Academy Awards and 2011 Grammy Awards.

From 2005 to 2006, the Japanese American National Museum held an exhibition called "Big Drum: Taiko in the United States". The exhibition covered several topics related to taiko in the United States, such as the formation of performance groups, their construction using available materials, and social movements. Visitors were able to play smaller drums.

Certain peoples have used taiko to advance social or cultural movements, both within Japan and elsewhere in the world.

Taiko performance has frequently been viewed as an art form dominated by men. Historians of taiko argue that its performance comes from masculine traditions. Those who developed ensemble-style taiko in Japan were men, and through the influence of Ondekoza, the ideal taiko player was epitomized in images of the masculine peasant class, particularly through the character Muhōmatsu in the 1958 film "Rickshaw Man". Masculine roots have also been attributed to perceived capacity for "spectacular bodily performance" where women's bodies are sometimes judged as unable to meet the physical demands of playing.

Before the 1980s, it was uncommon for Japanese women to perform on traditional instruments, including taiko, as their participation had been systematically restricted; an exception was the San Francisco Taiko Dojo under the guidance of Grand master Seiichi Tanaka, who was the first to admit females to the art form. In Ondekoza and in the early performances of Kodo, women performed only dance routines either during or between taiko performances. Thereafter, female participation in "kumi-daiko" started to rise dramatically, and by the 1990s, women equaled and possibly exceeded representation by men. While the proportion of women in taiko has become substantial, some have expressed concern that women still do not perform in the same roles as their male counterparts and that taiko performance continues to be a male-dominated profession. For instance, a member of Kodo was informed by the director of the group's apprentice program that women were permitted to play, but could only play "as women". Other women in the apprentice program recognized a gender disparity in performance roles, such as what pieces they were allowed to perform, or in physical terms based on a male standard.

Female taiko performance has also served as a response to gendered stereotypes of Japanese women as being quiet, subservient, or a femme fatale. Through performance, some groups believe they are helping to redefine not only the role of women in taiko, but how women are perceived more generally.

Those involved in the construction of taiko are usually considered part of the burakumin, a marginalized minority class in Japanese society, particularly those working with leather or animal skins. Prejudice against this class dates back to the Tokugawa period in terms of legal discrimination and treatment as social outcasts. Although official discrimination ended with the Tokugawa era, the burakumin have continued to face social discrimination, such as scrutiny by employers or in marriage arrangements. Drum makers have used their trade and success as a means to advocate for an end to discriminatory practices against their class.

The , representing the contributions of burakumin, is found in Naniwa Ward in Osaka, home to a large proportion of burakumin. Among other features, the road contains taiko-shaped benches representing their traditions in taiko manufacturing and leatherworking, and their influence on national culture. The road ends at the Osaka Human Rights Museum, which exhibits the history of systematic discrimination against the burakumin. The road and museum were developed in part due an advocacy campaign led by the Buraku Liberation League and a taiko group of younger performers called .

Taiko performance was an important part of cultural development by third-generation Japanese residents in North America, who are called "sansei". During World War II, second-generation Japanese residents, called "nisei" faced internment in the United States and in Canada on the basis of their race. During and after the war, Japanese residents were discouraged from activities such as speaking Japanese or forming ethnic communities. Subsequently, sansei could not engage in Japanese culture and instead were raised to assimilate into more normative activities. There were also prevailing stereotypes of Japanese people, which sansei sought to escape or subvert. During the 1960s in the United States, the civil rights movement influenced sansei to reexamine their heritage by engaging in Japanese culture in their communities; one such approach was through taiko performance. Groups such as San Jose Taiko were organized to fulfill a need for solidarity and to have a medium to express their experiences as Japanese-Americans. Later generations have adopted taiko in programs or workshops established by sansei; social scientist Hideyo Konagaya remarks that this attraction to taiko among other Japanese art forms may be due to its accessibility and energetic nature. Konagaya has also argued that the resurgence of taiko in the United States and Japan are differently motivated: in Japan, performance was meant to represent the need to recapture sacred traditions, while in the United States it was meant to be an explicit representation of masculinity and power in Japanese-American men.

A number of performers and groups, including several early leaders, have been recognized for their contributions to taiko performance. Daihachi Oguchi was best known for developing "kumi-daiko" performance. Oguchi founded the first "kumi-daiko" group called Osuwa Daiko in 1951, and facilitated the popularization of taiko performance groups in Japan.

Seidō Kobayashi is the leader of the Tokyo-based taiko group Oedo Sukeroku Taiko as of December 2014. Kobayashi founded the group in 1959 and was the first group to tour professionally. Kobayashi is considered a master performer of taiko. He is also known for asserting intellectual control of the group's performance style, which has influenced performance for many groups, particularly in North America.

In 1968, Seiichi Tanaka founded the San Francisco Taiko Dojo and is regarded as the Grandfather of Taiko and primary developer of taiko performance in the United States. He was a recipient of a 2001 National Heritage Fellowship awarded by the National Endowment for the Arts and since 2013 is the only taiko professional presented with the Order of the Rising Sun 5th Order: Gold and Silver Rays by Emperor Akihito of Japan, in recognition of Grandmaster Seiichi Tanaka's contributions to the fostering of US-Japan relations as well as the promotion of Japanese cultural understanding in the United States.

In 1969, founded Ondekoza, a group well known for making taiko performance internationally visible and for its artistic contributions to the tradition. Den was also known for developing a communal living and training facility for Ondekoza on Sado Island in Japan, which had a reputation for its intensity and broad education programs in folklore and music.

Performers and groups beyond the early practitioners have also been noted. Eitetsu Hayashi is best known for his solo performance work. When he was 19, Hayashi joined Ondekoza, a group later expanded and re-founded as Kodo, one of the best known and most influential taiko performance groups in the world. Hayashi soon left the group to begin a solo career and has performed in venues such as Carnegie Hall in 1984, the first featured taiko performer there. He was awarded the 47th Education Minister's Art Encouragement Prize, a national award, in 1997 as well as the 8th Award for the Promotion of Traditional Japanese Culture from the Japan Arts Foundation in 2001.





</doc>
<doc id="8716" url="https://en.wikipedia.org/wiki?curid=8716" title="Dolly Parton">
Dolly Parton

Dolly Rebecca Parton (born January 19, 1946) is an American singer, songwriter, multi-instrumentalist, record producer, actress, author, businesswoman, and humanitarian, known primarily for her work in country music. After achieving success as a songwriter for others, Parton made her album debut in 1967 with "Hello, I'm Dolly". With steady success during the remainder of the 1960s (both as a solo artist and with a series of duet albums with Porter Wagoner), her sales and chart peak came during the 1970s and continued into the 1980s. Parton's albums in the 1990s did not sell as well, but she achieved commercial success again in the new millennium and has released albums on various independent labels since 2000, including her own label, Dolly Records.

Parton's music includes Recording Industry Association of America (RIAA)-certified gold, platinum and multi-platinum awards. She has had 25 songs reach No. 1 on the "Billboard" country music charts, a record for a female artist (tied with Reba McEntire). She has 44 career Top 10 country albums, a record for any artist, and she has 110 career-charted singles over the past 40 years. She has garnered ten Grammy Awards and 49 nominations, including the Lifetime Achievement Award and a 2020 win with for KING & COUNTRY for their collaboration on "God Only Knows"; 10 Country Music Association Awards, including Entertainer of the Year and is one of only seven female artists to win the Country Music Association's Entertainer of the Year Award; five Academy of Country Music Awards, also including a nod for Entertainer of the Year; four People’s Choice Awards; and three American Music Awards.

In 1999, Parton was inducted into the Country Music Hall of Fame. She has composed over 3,000 songs, including "I Will Always Love You" (a two-time U.S. country chart-topper, as well as an international pop hit for Whitney Houston), "Jolene", "Coat of Many Colors", and "9 to 5". She is also a select group to have received at least one nomination from the Academy Awards, Grammy Awards, Tony Awards, and Emmy Awards. As an actress, she has starred in films such as "9 to 5" (1980) and "The Best Little Whorehouse in Texas" (1982), for which she earned Golden Globe nominations for Best Actress, as well as "Rhinestone" (1984), "Steel Magnolias" (1989), "Straight Talk" (1992) and "Joyful Noise" (2012).

Dolly Rebecca Parton was born January 19, 1946, in a one-room cabin on the banks of the Little Pigeon River in Pittman Center, Tennessee. She is the fourth of 12 children born to Avie Lee Caroline (née Owens; 1923–2003) and Robert Lee Parton Sr. (1921–2000). Her father, known as "Lee", worked in the mountains of East Tennessee, first as a sharecropper and later tending his own small tobacco farm and acreage. He also worked construction jobs to supplement the farm's small income. Lee was illiterate but Dolly Parton often says despite that fact, he was one of the smartest people she's known in regards to business and making a profit. Avie Lee was homemaker for the large family. Her 11 pregnancies (the tenth being twins) in 20 years made her a mother of 12 by age 35. Often in poor health, she still managed to keep house and entertain her children with songs and tales of mountain folklore. Avie Lee's father, Jake Owens, was a Pentecostal preacher, so Parton and her siblings all attended church regularly. Parton has long credited her father for her business savvy, and her mother's family for her musical abilities.
While Dolly Parton was still very young, her family moved to a farm on nearby Locust Ridge. Most of her cherished memories of youth happened there, and it is the place about which she wrote the song "My Tennessee Mountain Home" in the 1970s. Parton bought back the Locust Ridge property in the 1980s. Two of her siblings are no longer living; Larry died shortly after birth in 1955, and Floyd died in 2018.

Dolly Parton's middle name comes from her maternal great-great-grandmother Rebecca (Dunn) Whitted. She has described her family as "dirt poor." Parton's father paid the doctor who helped deliver her with a bag of cornmeal. She outlined her family's poverty in her early songs "Coat of Many Colors" and "In the Good Old Days (When Times Were Bad)". They lived in a rustic, one-bedroom cabin in Locust Ridge, just north of the Greenbrier Valley of the Great Smoky Mountains, a predominantly Pentecostal area.
Music played an important role in her early life. She was brought up in the Church of God (Cleveland, Tennessee), the church her grandfather, Jake Robert Owens, pastored. Her earliest public performances were in the church, beginning at age six. At seven, she started playing a homemade guitar. When she was eight, her uncle bought her first real guitar.

Parton began performing as a child, singing on local radio and television programs in the East Tennessee area. By ten, she was appearing on "The Cas Walker Show" on both WIVK Radio and WBIR-TV in Knoxville, Tennessee. At 13, she was recording (the single "Puppy Love") on a small Louisiana label, Goldband Records, and appeared at the Grand Ole Opry, where she first met Johnny Cash, who encouraged her to follow her own instincts regarding her career.

After graduating from Sevier County High School in 1964, Parton moved to Nashville the next day. Her initial success came as a songwriter, having signed with Combine Publishing shortly after her arrival; with her frequent songwriting partner, her uncle Bill Owens, she wrote several charting singles during this time, including two top-10 hits: Bill Phillips's "Put It Off Until Tomorrow" (1966) and Skeeter Davis's "Fuel to the Flame" (1967). Her songs were recorded by many other artists during this period, including Kitty Wells and Hank Williams Jr. She signed with Monument Records in 1965, at age 19; she initially was pitched as a bubblegum pop singer. She released a string of singles, but the only one that charted, "Happy, Happy Birthday Baby", did not crack the "Billboard" Hot 100. Although she expressed a desire to record country material, Monument resisted, thinking her unique voice with its strong vibrato was not suited to the genre.

After her composition "Put It Off Until Tomorrow", as recorded by Bill Phillips (with Parton, uncredited, on harmony), went to number six on the country chart in 1966, the label relented and allowed her to record country. Her first country single, "Dumb Blonde" (composed by Curly Putman, one of the few songs during this era that she recorded but did not write), reached number 24 on the country chart in 1967, followed by "Something Fishy", which went to number 17. The two songs appeared on her first full-length album, "Hello, I'm Dolly".

In 1967, musician and country music entertainer Porter Wagoner invited Parton to join his organization, offering her a regular spot on his weekly syndicated television program "The Porter Wagoner Show", and in his road show. As documented in her 1994 autobiography, initially, much of Wagoner's audience was unhappy that Norma Jean, the performer whom Parton had replaced, had left the show, and was reluctant to accept Parton (sometimes chanting loudly for Norma Jean from the audience). With Wagoner's assistance, however, Parton was eventually accepted. Wagoner convinced his label, RCA Victor, to sign her. RCA decided to protect their investment by releasing her first single as a duet with Wagoner. That song, a remake of Tom Paxton's "The Last Thing on My Mind", released in late 1967, reached the country top 10 in January 1968, launching a six-year streak of virtually uninterrupted top-10 singles for the pair.

Parton's first solo single for RCA Victor, "Just Because I'm a Woman", was released in the summer of 1968 and was a moderate chart hit, reaching number 17. For the next two years, none of her solo efforts – even "In the Good Old Days (When Times Were Bad)", which later became a standard – were as successful as her duets with Wagoner. The duo was named Vocal Group of the Year in 1968 by the Country Music Association, but Parton's solo records were continually ignored. Wagoner had a significant financial stake in her future; as of 1969, he was her co-producer and owned nearly half of Owe-Par, the publishing company Parton had founded with Bill Owens.

By 1970, both Parton and Wagoner had grown frustrated by her lack of solo chart success. Wagoner persuaded Parton to record Jimmie Rodgers' "Mule Skinner Blues", a gimmick that worked. The record shot to number three, followed closely, in February 1971, by her first number-one single, "Joshua". For the next two years, she had numerous solo hits – including her signature song "Coat of Many Colors" (number four, 1971) – in addition to her duets. Top-20 singles included "The Right Combination" and "Burning the Midnight Oil" (both duets with Wagoner, 1971); "Lost Forever in Your Kiss", (with Wagoner) "Touch Your Woman", (1972) "My Tennessee Mountain Home" and "Travelin' Man" (1973).

Although her solo singles and the Wagoner duets were successful, her biggest hit of this period was "Jolene". Released in late 1973, it topped the country chart in February 1974 and reached the lower regions of the Hot 100. (It eventually also charted in the U.K., reaching number seven in 1976, representing Parton's first U.K. success). Parton, who had always envisioned a solo career, made the decision to leave Wagoner's organization; the pair performed their last duet concert in April 1974, and she stopped appearing on his TV show in mid-1974, although they remained affiliated. He helped produce her records through 1975. The pair continued to release duet albums, their final release being 1975's "Say Forever You'll Be Mine".

In 1974, her song, "I Will Always Love You", written about her professional break from Wagoner, went to number one on the country chart. Around the same time, Elvis Presley indicated that he wanted to record the song. Parton was interested until Presley's manager, Colonel Tom Parker, told her that it was standard procedure for the songwriter to sign over half of the publishing rights to any song recorded by Presley. Parton refused. That decision has been credited with helping to make her many millions of dollars in royalties from the song over the years. Parton had three solo singles reach number one on the country chart in 1974 ("Jolene", "I Will Always Love You" and "Love Is Like a Butterfly"), as well as the duet with Porter Wagoner, "Please Don't Stop Loving Me". In a 2019 episode of the Sky Arts music series "Brian Johnson: A Life on the Road", Parton described finding old cassette tapes and realizing that she'd composed both "Jolene" and "I Will Always Love You" in the same songwriting session, telling Johnson "Buddy, that was a good night." Parton again topped the singles chart in 1975 with "The Bargain Store".

Between 1974 and 1980 Parton had a series of country hits, with eight singles reaching number one. Her influence on pop culture is reflected by the many performers covering her songs, including mainstream and crossover artists such as Olivia Newton-John, Emmylou Harris, and Linda Ronstadt.

Parton began to embark on a high-profile crossover campaign, attempting to aim her music in a more mainstream direction and increase her visibility outside of the confines of country music. In 1976, she began working closely with Sandy Gallin, who served as her personal manager for the next 25 years. With her 1976 album "All I Can Do", which she co-produced with Porter Wagoner, Parton began taking more of an active role in production, and began specifically aiming her music in a more mainstream, pop direction. Her first entirely self-produced effort, "New Harvest...First Gathering" (1977), highlighted her pop sensibilities, both in terms of choice of songs – the album contained covers of the pop and R&B classics "My Girl" and "Higher and Higher" – and production. Though the album was well received and topped the U.S. country albums chart, neither it nor its single "Light of a Clear Blue Morning" made much of an impression on the pop charts.

After "New Harvest"'s disappointing crossover performance, Parton turned to high-profile pop producer Gary Klein for her next album. The result, 1977's "Here You Come Again," became her first million-seller, topping the country album chart and reaching number 20 on the pop chart. The Barry Mann-Cynthia Weil-penned title track topped the country singles chart, and became Parton's first top-ten single on the pop chart (#3). A second single, the double A-sided "Two Doors Down"/"It's All Wrong, But It's All Right" topped the country chart and crossed over to the pop Top 20. For the remainder of the 1970s and into the early 1980s, many of her subsequent singles moved up on both charts simultaneously. Her albums during this period were developed specifically for pop-crossover success.
In 1978, Parton won a Grammy Award for Best Female Country Vocal Performance for her "Here You Come Again" album. She continued to have hits with "Heartbreaker", (1978) "Baby I'm Burning" (1979) and "You're the Only One," (1979) all of which charted in the pop Top 40 and topped the country chart. "Sweet Summer Lovin'" (1979) became the first Parton single in two years to not top the country chart (though it did reach the Top 10). During this period, her visibility continued to increase, with multiple television appearances. A highly publicized candid interview on a "Barbara Walters Special" in 1977 (timed to coincide with "Here You Come Again"'s release) was followed by appearances in 1978 on Cher's ABC television special, and her own joint special with Carol Burnett on CBS, "Carol and Dolly in Nashville".

Parton served as one of three co-hosts (along with Roy Clark and Glen Campbell) on the CBS special "Fifty Years of Country Music". In 1979, Parton hosted the NBC special "The Seventies: An Explosion of Country Music", performed live at the Ford Theatre in Washington, D.C., and whose audience included President Jimmy Carter.
Her commercial success grew in 1980, with three consecutive country chart number-one hits: the Donna Summer-written "Starting Over Again", "Old Flames Can't Hold a Candle to You", and "9 to 5", which topped the country and pop charts in early 1981. She had another Top 10 single that year with "Making Plans", a single released from a 1980 reunion album with Porter Wagoner.
The theme song to the 1980 feature film "9 to 5", in which she starred along with Jane Fonda and Lily Tomlin, "9 to 5", not only reached number one on the country chart, but also, in February 1981, reached number one on the pop and the adult-contemporary charts, giving her a triple number-one hit. Parton became one of the few female country singers to have a number-one single on the country and pop charts simultaneously. It also received a nomination for an Academy Award for Best Original Song. Her singles continued to appear consistently in the country Top 10. Between 1981 and 1985, she had 12 Top-10 hits; half of them hit number one. She continued to make inroads on the pop chart as well. A re-recorded version of "I Will Always Love You," from the feature film "The Best Little Whorehouse in Texas" (1982) scraped the Top 50 that year and her duet with Kenny Rogers, "Islands in the Stream" (written by the Bee Gees and produced by Barry Gibb), spent two weeks at number one in 1983.

In the mid-1980s, her record sales were still relatively strong, with "Save the Last Dance for Me", "Downtown", "Tennessee Homesick Blues" (1984), "Real Love" (another duet with Kenny Rogers), "Don't Call It Love" (1985) and "Think About Love" (1986) all reaching the country Top 10 ("Tennessee Homesick Blues" and "Think About Love" reached number one; "Real Love" also reached number one on the country chart and became a modest crossover hit). However, RCA Records did not renew her contract after it expired in 1986, and she signed with Columbia Records in 1987.

Along with Emmylou Harris and Linda Ronstadt, she released "Trio" (1987) to critical acclaim. The album revitalized Parton's music career, spending five weeks at number one on "Billboard's" Country Albums chart, and also reached the top 10 on "Billboard"'s Top-200 Albums chart. It sold several million copies and produced four Top 10 country hits, including Phil Spector's "To Know Him Is to Love Him", which went to number one. "Trio" won the Grammy Award for Best Country Performance by a Duo or Group with Vocal and was nominated for a Grammy Award for Album of the Year. After a further attempt at pop success with "Rainbow," (1987) including the single "The River Unbroken", Parton focused on recording country material. "White Limozeen" (1989) produced two number one hits in "Why'd You Come in Here Lookin' Like That" and "Yellow Roses". Although Parton's career appeared to be revived, it was actually just a brief revival before contemporary country music came in the early 1990s and moved most veteran artists off the charts.

A duet with Ricky Van Shelton, "Rockin' Years" (1991) reached number one, though Parton's greatest commercial fortune of the decade came when Whitney Houston recorded "I Will Always Love You" for the soundtrack of the feature film "The Bodyguard" (1992). Both the single and the album were massively successful. Parton's soundtrack album from the 1992 film, "Straight Talk", however, was less successful. But her 1993 album "Slow Dancing with the Moon" won critical acclaim and did well on the charts, reaching number four on the country albums chart, and number 16 on the "Billboard" 200 album chart. She recorded "The Day I Fall in Love" as a duet with James Ingram for the feature film "Beethoven's 2nd" (1993). The songwriters (Ingram, Carole Bayer Sager, and Clif Magness) were nominated for an Academy Award for Best Original Song, and Parton and Ingram performed the song at the awards telecast. Similar to her earlier collaborative album with Harris and Ronstadt, Parton released "Honky Tonk Angels" in the fall of 1993 with Loretta Lynn and Tammy Wynette. It was certified as a gold album by the Recording Industry Association of America and helped revive both Wynette and Lynn's careers. Also in 1994, Parton contributed the song "You Gotta Be My Baby" to the AIDS benefit album "Red Hot + Country" produced by the Red Hot Organization. A live acoustic album, "", featuring stripped-down versions of some of her hits, as well as some traditional songs, was released in late 1994.

Parton's recorded music during the mid- to late-1990s remained steady and somewhat eclectic. Her 1995 re-recording of "I Will Always Love You" (performed as a duet with Vince Gill), from her album "Something Special." won the Country Music Association's Vocal Event of the Year Award. The following year, "Treasures", an album of covers of 1960s/70s hits was released, and featured a diverse collection of material, including songs by Mac Davis, Pete Seeger, Kris Kristofferson, Cat Stevens, and Neil Young. Her recording of Stevens' "Peace Train" was later re-mixed and released as a dance single, reaching "Billboard's "dance singles chart. Her 1998 country-rock album "Hungry Again" was made up entirely of her own compositions. Although neither of the album's two singles, "(Why Don't More Women Sing) Honky Tonk Songs" and "Salt in my Tears", charted, videos for both songs received significant airplay on CMT. A second and more contemporary collaboration with Harris and Ronstadt, "Trio II", was released in early 1999. Its cover of Neil Young's song "After the Gold Rush" won a Grammy Award for Best Country Collaboration with Vocals. Parton also was inducted into the Country Music Hall of Fame in 1999.

Parton recorded a series of bluegrass-inspired albums, beginning with "The Grass Is Blue" (1999), winning a Grammy Award for Best Bluegrass Album; and "Little Sparrow" (2001), with its cover of Collective Soul's "Shine" winning a Grammy Award for Best Female Country Vocal Performance. The third, "Halos & Horns" (2002) included a bluegrass version of the Led Zeppelin song "Stairway to Heaven". In 2005, she released "Those Were The Days" consisting of her interpretations of hits from the folk-rock era of the late 1960s and early 1970s, including "Imagine", "Where Do the Children Play?", "Crimson and Clover", and "Where Have All the Flowers Gone?"

Parton earned her second Academy Award nomination for Best Original Song for "Travelin' Thru," which she wrote specifically for the feature film "Transamerica." (2005) Due to the song's (and film's) acceptance of a transgender woman, Parton received death threats. She returned to number one on the country chart later in 2005 by lending her distinctive harmonies to the Brad Paisley ballad, "When I Get Where I'm Going".

In September 2007, Parton released her first single from her own record company, Dolly Records, titled, "Better Get to Livin'", which eventually peaked at number 48 on "Billboard"'s Hot Country Songs chart. It was followed by the studio album "Backwoods Barbie", which was released on February 26, 2008, and reached number two on the country chart. The album's debut at number 17 on the all-genre "Billboard" 200 albums chart was the highest in her career. "Backwoods Barbie" produced four additional singles, including the title track, written as part of her score for "9 to 5: The Musical", an adaptation of her feature film. After the sudden death of Michael Jackson, whom Parton knew personally, she released a video in which she somberly told of her feelings on Jackson and his death.

On October 27, 2009, Parton released a four-CD box set, "Dolly", which featured 99 songs and spanned most of her career. She released her second live DVD and album, "Live From London" in October 2009, which was filmed during her sold-out 2008 concerts at London's The O2 Arena.

On August 10, 2010, with longtime friend Billy Ray Cyrus, Parton released the album "Brother Clyde". Parton is featured on "The Right Time", which she co-wrote with Cyrus and Morris Joseph Tancredi.

On January 6, 2011, Parton announced that her new album would be titled "Better Day". In February 2011, she announced that she would embark on the Better Day World Tour on July 17, 2011, with shows in northern Europe and the U.S. The album's lead-off single, "Together You and I", was released on May 23, 2011, and "Better Day" was released on June 28, 2011. In 2011, Parton voiced the character Dolly Gnome in the animated film "Gnomeo & Juliet".

On February 11, 2012, after the sudden death of Whitney Houston, Dolly Parton stated, "Mine is only one of the millions of hearts broken over the death of Whitney Houston. I will always be grateful and in awe of the wonderful performance she did on my song, and I can truly say from the bottom of my heart, 'Whitney, I will always love you. You will be missed.'" In 2013, Parton joined Lulu Roman for a re-recording of "I Will Always Love You" for Roman's album, "At Last".

In 2013, Parton and Kenny Rogers reunited for the title song of his album "You Can't Make Old Friends." For their performance, they were nominated at the 2014 Grammy Awards for Grammy Award for Best Country Duo/Group Performance.

In 2014, Parton embarked on the Blue Smoke World Tour in support of her 42nd studio album, "Blue Smoke". The album was first released in Australia
& New Zealand on January 31 to coincide with tour dates there in February, and reached the top 10 in both countries. It was released in the United States on May 13, and debuted at number six on the "Billboard" 200 chart, making it her first top-10 album and her highest-charting solo album ever; it also reached the number two on the U.S. country chart. The album was released in Europe on June 9, and reached number two on the UK album chart.

On June 29, 2014, Parton performed for the first time at the UK Glastonbury Festival performing songs such as "Jolene", "9 to 5" and "Coat of Many Colors" to a crowd of more than 180,000.

On March 6, 2016, Parton announced that she would be embarking on a tour in support of her new album, "Pure & Simple". The tour was one of Parton's biggest tours within the United States in more than 25 years. Sixty-four dates were planned in the United States and Canada, visiting the most requested markets missed on previous tours.

In the fall of 2016, she released "Jolene" as a single with the "a cappella" group Pentatonix and performed on "" with Pentatonix and Miley Cyrus in November 2016.

Also in 2016, Parton was one of 30 artists to perform on "Forever Country", a mash-up of the songs, "Take Me Home, Country Roads", "On the Road Again" and her own "I Will Always Love You". The song celebrates 50 years of the CMA Awards. At the ceremony itself, Parton was honored with the Willie Nelson Lifetime Achievement Award, which was presented by Lily Tomlin and preceded by a tribute featuring Jennifer Nettles, Pentatonix, Reba McEntire, Kacey Musgraves, Carrie Underwood and Martina McBride.

In 2017, Parton appeared on "Rainbow", the third studio album by Kesha performing a duet of "Old Flames Can't Hold a Candle to You". The track had been co-written by Kesha's mother Pebe Sebert. It was previously a hit for Parton and was included on her 1980 album "Dolly, Dolly, Dolly". She also co-wrote and provided featuring vocals on the song "Rainbowland" on "Younger Now", the sixth album by her goddaughter Miley Cyrus.

On June 25, 2019, "The New York Times Magazine" listed Parton as one of the hundreds of artists whose material was destroyed in the 2008 Universal fire.

In July 2019, Parton made an unannounced appearance at the Newport Folk Festival, and performed several songs accompanied by The Highwomen and Linda Perry.

In 2020, Parton received worldwide attention after posting four pictures in which she showed how she would present herself on the social media platforms LinkedIn, Facebook, Instagram and Twitter. The original post on Instagram went viral after celebrities posted their own versions of the so-called Dolly Parton challenge on social media.

On April 10, 2020, Parton re-released 93 songs from six of her classic albums. Little Sparrow, Halos & Horns, For God and Country, Better Day, Those Were The Days, and Live and Well are all available for online listening.

On May 27, 2020, Parton released a brand new song called "When Life Is Good Again". This song was released to help keep the spirits up of those affected by the 2020 COVID-19 pandemic. Parton also released a music video for "When Life Is Good Again" which premiered on TIME 100 talks on May 28, 2020.

In August 2020, Parton announced plans to release her first holiday album in 30 years, "A Holly Dolly Christmas", in October 2020. 

In 1998, "Nashville Business" ranked her the wealthiest country-music star. , her net worth is estimated at $500 million. She was also on "The Love Boat" in 1977, a short cameo in episode 13 as the boat captain's silent wife.

Parton is a prolific songwriter, having begun by writing country-music songs with strong elements of folk music, based on her upbringing in humble mountain surroundings and reflecting her family's Christian background. Her songs "Coat of Many Colors", "I Will Always Love You", and "Jolene", among others, have become classics. On November 4, 2003, Parton was honored as a BMI Icon at the 2003 BMI Country Awards.
Parton has earned over 35 BMI Pop and Country Awards. In 2001, she was inducted into the Songwriters Hall of Fame. In a 2009 interview on CNN's "Larry King Live", she said she had written "at least 3,000" songs, having written seriously since the age of seven. Parton also said she writes something every day, be it a song or an idea.

Parton's songwriting has been featured prominently in several films. In addition to the title song for "9 to 5," she also recorded a second version of "I Will Always Love You" for "The Best Little Whorehouse in Texas" (1982). The second version was a number one country hit and also reached number 53 on the pop charts.
"I Will Always Love You" has been covered by many country artists, including Ronstadt on "Prisoner In Disguise" (1975), Kenny Rogers on "Vote for Love" (1996), and LeAnn Rimes on "" (1997). Whitney Houston performed it on "The Bodyguard" soundtrack and her version became the best-selling hit both written and performed by a female vocalist, with worldwide sales of over 12 million copies. In addition, the song has been translated into Italian and performed by the Welsh opera singer Katherine Jenkins.

As a songwriter, Parton has twice been nominated for an Academy Award for Best Original Song, for "9 to 5" and "Travelin' Thru" (2005) from the film "Transamerica". "Travelin' Thru" won Best Original Song at the 2005 Phoenix Film Critics Society Awards. It was also nominated for both the 2005 Golden Globe Award for Best Original Song and the 2005 Broadcast Film Critics Association Award (also known as the Critics' Choice Awards) for Best Song. A cover of "Love Is Like A Butterfly" by Clare Torry was used as the theme music for the British TV show "Butterflies".

Parton wrote the score (and Patricia Resnick the book) for "", a musical-theater adaptation of Parton's feature film "9 to 5" (1980). The musical ran at the Ahmanson Theatre, Los Angeles in late 2008. It opened on Broadway at the Marquis Theatre in New York City, on April 30, 2009, to mixed reviews.
The title track of her 2008 album "Backwoods Barbie" was written for the musical's character Doralee. Although her score (as well as the musical debut of actress Allison Janney) was praised, the show struggled, closing on September 6, 2009, after 24 previews and 148 performances. Parton received nominations for Drama Desk Award for Outstanding Music and Drama Desk Award for Outstanding Lyrics, as well as a nomination for Tony Award for Best Original Score.
Developing the musical was not a quick process. According to the public-radio program "Studio 360" (October 29, 2005), in October 2005 Parton was in the midst of composing the songs for a Broadway musical theater adaptation of the film. In late June 2007, "9 to 5: The Musical" was read for industry presentations. The readings starred Megan Hilty, Allison Janney, Stephanie J. Block, Bebe Neuwirth, and Marc Kudisch. Ambassador Theatre Group announced a 2012 UK tour for "Dolly Parton's 9 to 5: The Musical", commencing at Manchester Opera House, on October 12, 2012.

Parton invested much of her earnings into business ventures in her native East Tennessee, notably Pigeon Forge. She is a co-owner of The Dollywood Company, which operates the theme park Dollywood (a former Silver Dollar City), a dinner theater, Dolly Parton's Stampede, the waterpark Dollywood's Splash Country, and the Dream More Resort and Spa, all in Pigeon Forge. Dollywood is the 24th-most-popular theme park in the United States, with 3 million visitors per year.
The Dolly Parton's Stampede business has venues in Branson, Missouri, and Myrtle Beach, South Carolina. A former location in Orlando, Florida, closed in January 2008 after the land and building were sold to a developer. Starting in June 2011, the Myrtle Beach location became Pirates Voyage Fun, Feast and Adventure; Parton appeared for the opening, and the South Carolina General Assembly declared June 3, 2011, as Dolly Parton Day.

On January 19, 2012, Parton's 66th birthday, Gaylord Opryland and Dollywood announced plans to open a $50 million water and snow park, a family-friendly destination in Nashville that is open all year. On September 29, 2012, Parton officially withdrew her support for the Nashville park due to the restructuring of Gaylord Entertainment Company after its merger with Marriott International.
On June 12, 2015, it was announced that the Dollywood Company had purchased the Lumberjack Feud Dinner Show in Pigeon Forge. The show, which opened in June 2011, was owned and operated by Rob Scheer until the close of the 2015 season. The new, renovated show by the Dollywood Company opened in 2016.

Parton was a co-owner of Sandollar Productions, with Sandy Gallin, her former manager. A film and television production company, it produced the documentary "" (1989), which won an Academy Award for Best Documentary (Feature); the television series "Babes" (1990–91) and "Buffy the Vampire Slayer" (1997–2003); and the feature films "Father of the Bride" (1991), "Father of the Bride: Part II" (1995) "Straight Talk" (1992) (in which Parton starred), and "Sabrina" (1995), among other shows. In a 2009 interview, singer Connie Francis revealed that Dolly had been contacting her for years in an attempt to film the singer's life story. Francis turned down Parton's offers, as she was already in negotiations with singer Gloria Estefan to produce the film, a collaboration now ended. After the retirement of her partner, Sandy Gallin, Parton briefly operated Dolly Parton's Southern Light Productions and in 2015 she announced her new production company would be called Dixie Pixie Productions and produce the movies-of-week in development with NBC Television and Magnolia Hill Productions.

In addition to her performing appearances on "The Porter Wagoner Show" in the 1960s and into the 1970s, her two self-titled television variety shows in the 1970s and 1980s, and on "American Idol" in 2008 and other guest appearances, Parton has had television roles. In 1979, she received an Emmy award nomination as "Outstanding Supporting Actress in a Variety Program" for her guest appearance in a Cher special. During the mid-1970s, Parton wanted to expand her audience base. Although her first attempt, the television variety show "Dolly!" (1976–77), had high ratings, it lasted only one season, with Parton requesting to be released from her contract because of the stress it was causing on her vocal cords (she later tried a second television variety show, also titled "Dolly" (1987–88); it too lasted only one season).

In her first feature film, Parton portrayed a secretary in a leading role with Jane Fonda and Lily Tomlin in the comedy film "9 to 5" (1980). The movie highlights the discrimination of women in a working environment and created awareness of the National Association of Working Women (9–5).She received nominations for a Golden Globe Award for Best Actress – Motion Picture Musical or Comedy and a Golden Globe Award for New Star of the Year – Actress. Parton wrote and recorded the film's title song. It received nominations for an Academy Award for Best Song and a Golden Globe Award for Best Original Song. Released as a single, the song won both the Grammy Award for Best Female Country Vocal Performance and the Grammy Award for Best Country Song. It also reached No. 1 on the Hot 100 chart and it was No. 78 on the "AFI's 100 Years...100 Songs" list released by the American Film Institute in 2004. "9 to 5" became a major box office success, grossing over $3.9 million its opening weekend, and over $103 million worldwide. Parton was named Top Female Box Office Star by the "Motion Picture Herald" in both 1981 and 1982 due to the film's success.

In late 1981, Parton began filming her second film, the musical film "The Best Little Whorehouse in Texas" (1982). The film earned her a second nomination for a Golden Globe Award for Best Actress – Motion Picture Musical or Comedy. The film was greeted with positive critical reviews and became a commercial success, earning over $69 million worldwide. After a two-year hiatus from films, Parton was teamed with Sylvester Stallone for "Rhinestone" (1984). A comedy film about a country music star's efforts to mould an unknown into a music sensation, the film was a critical and financial failure, making just over $21 million on a $28 million budget.

In 1989, Parton returned to film acting in "Steel Magnolias" (1989), based on the play of the same name by Robert Harling. The film was popular with critics and audiences, grossing over $95 million inside the U.S. She starred in the television movies "A Smoky Mountain Christmas" (1986); "Wild Texas Wind" (1991); "Unlikely Angel" (1996), portraying an angel sent back to earth following a deadly car crash; and "Blue Valley Songbird" (1999), where her character lives through her music. Parton starred along with James Woods in "Straight Talk" (1992), which received mixed reviews, and grossed a mild $21 million at the box office.

Parton's 1987 variety show "Dolly" lasted only one season. She made a cameo appearance as herself in "The Beverly Hillbillies" (1993), an adaptation of the long-running TV sitcom of the same name (1962–71). Parton has done voice work for animation for television series, playing herself in "Alvin and the Chipmunks" (episode "Urban Chipmunk", 1983) and the character Katrina Eloise "Murph" Murphy (Ms. Frizzle's first cousin) in "The Magic School Bus" (episode "The Family Holiday Special", 1994). She also has guest-starred in several sitcoms, including a 1990 episode of "Designing Women" (episode "The First Day of the Last Decade of the Entire Twentieth Century") as herself, the guardian movie star of Charlene's baby. She made a guest appearance on "Reba" (episode "Reba's Rules of Real Estate") portraying a real-estate agency owner and on "The Simpsons" (episode "Sunday, Cruddy Sunday", 1999). She appeared as herself in 2000 on the Halloween episode of Bette Midler's short-lived sitcom "Bette," and on episode 14 of "Babes" (produced by Sandollar Productions, Parton and Sandy Gallin's joint production company). She made cameo appearances on the Disney Channel as "Aunt Dolly", visiting Hannah and her family in fellow Tennessean and real-life goddaughter Miley Cyrus's series "Hannah Montana" (episodes "Good Golly, Miss Dolly", 2006, "I Will Always Loathe You", 2007, and "Kiss It All Goodbye", 2010). She was nominated for Outstanding Guest Actress in a Comedy Series.

Parton appeared as an overprotective mother in the comedy "Frank McKlusky, C.I.". (2002) She made a cameo appearance in the comedy film "", starring Sandra Bullock. She was featured in "The Book Lady" (2008), a documentary about her campaign for children's literacy. Parton expected to reprise her television role as Hannah's godmother in the musical comedy film "" (2009), but the character was omitted from the screenplay. She had a voice role in the comedy family film "Gnomeo & Juliet" (2011), a computer-animated film with garden gnomes about William Shakespeare's "Romeo and Juliet".

"Dolly Parton's Coat of Many Colors", a made-for-TV film based on Parton's song of the same name, and featuring narration by Parton, aired on NBC in December 2015, with child actress Alyvia Alyn Lind portraying the young Parton. Parton also had a cameo in , which aired in November 2016. She co-starred with Queen Latifah in the musical film "Joyful Noise" (2012), playing a choir director's widow who joins forces with Latifah's character, a mother of two teens, to save a small Georgia town's gospel choir.

In June 2018, Parton announced an eight-part Netflix series, featuring her music career. She is its executive producer and co-star. The series, called "Dolly Parton's Heartstrings", aired in November 2019.

Parton is the subject of the NPR podcast "Dolly Parton's America". It is hosted by Jad Abumrad, who also hosts Radiolab.

In December 2019, the biographical documentary "Here I Am" was added to the catalog of the Netflix streaming service. The documentary, a co-production of Netflix and the BBC, takes its name from Parton's 1971 song.

Parton is the fourth of 12 children; her siblings in order are Willadeene, David, Denver, Bobby, Stella, Cassie, Randy, Larry, Floyd, Frieda and Rachel.

On May 30, 1966, Parton and Carl Thomas Dean (born in Nashville, Tennessee) were married in Ringgold, Georgia. Although Parton does not use Dean's surname professionally, she has stated that her passport says "Dolly Parton Dean" and that she sometimes uses Dean when signing contracts.
Dean, who is retired from running an asphalt road-paving business in Nashville, has always shunned publicity and rarely accompanies his wife to any events. According to Parton, he has seen her perform only once. She also has said in interviews that, although it appears they spend little time together, it is simply that nobody sees him publicly. She has commented on Dean's romantic side, saying that he does spontaneous things to surprise her and sometimes even writes poems for her. In 2011 Parton said, "We're really proud of our marriage. It's the first for both of us. And the last."

On May 6, 2016, Parton announced that she and her husband would renew their vows in honor of their 50th wedding anniversary later in the month.

Parton and Dean helped raise several of Parton's younger siblings in Nashville, leading her nieces and nephews to refer to her as "Aunt Granny," a moniker that later lent its name to one of Parton's Dollywood restaurants. As she suffered from endometriosis, a condition which eventually required her to undergo a hysterectomy, the couple have no children of their own. Parton is the godmother of performer Miley Cyrus.

Parton has turned down several offers to pose nude for "Playboy" magazine, but did appear on the cover of the October 1978 issue wearing a Playboy bunny outfit, complete with ears (the issue featured Lawrence Grobel's extensive and candid interview with Parton, representing one of her earliest high-profile interviews with the mainstream press). The association of breasts with Parton's public image is illustrated in the naming of Dolly the sheep after her, since the sheep was cloned from a cell taken from an adult ewe's mammary gland. In Mobile, Alabama, the General W.K. Wilson Jr. Bridge is commonly called "the Dolly Parton Bridge" due to its arches resembling her bust. The Hernando de Soto Bridge over the Mississippi River at Memphis is also sometimes called this for the same reason.

Parton is known for having undergone considerable plastic surgery. On a 2003 episode of "The Oprah Winfrey Show", Winfrey asked what kind of cosmetic surgery Parton had undergone. Parton replied that cosmetic surgery was imperative in keeping with her famous image.
Parton has repeatedly joked about her physical image and surgeries, saying, "It takes a lot of money to look this cheap." Her breasts have garnered her mentions in several songs, including "Dolly Parton's Hits" by Bobby Braddock, "Marty Feldman Eyes" by Bruce Baum (a parody of "Bette Davis Eyes"), "No Show Jones" by George Jones and Merle Haggard, and "Make Me Proud" by Drake ft. Nicki Minaj. When asked about future plastic surgeries, she famously said, "If I see something sagging, bagging or dragging, I'll get it nipped, tucked or sucked." Parton's feminine escapism is acknowledged in her words, "Womanhood was a difficult thing to get a grip on in those hills, unless you were a man."

Since the mid-1980s, Parton has supported many charitable efforts, particularly in the area of literacy, primarily through her Dollywood Foundation.
Her literacy program, Dolly Parton's Imagination Library, which kicked off in Rotherham, United Kingdom, a part of the Dollywood Foundation, mails one book per month to each enrolled child from the time of their birth until they enter kindergarten. Currently, over 1600 local communities provide the Imagination Library to almost 850,000 children each month across the U.S., Canada, the UK, Australia, and the Republic of Ireland. In 2018, Parton was honored by the Library of Congress on account of the "charity sending out its 100 millionth book".
In 2006, Parton published a cookbook, "Dolly's Dixie Fixin's: Love, Laughter and Lots of Good Food".

The Dollywood Foundation, funded from Parton's profits, has been noted for bringing jobs and tax revenues to a previously depressed region. Parton also has worked to raise money for several other causes, including the American Red Cross and HIV/AIDS-related charities.
In December 2006, Parton pledged $500,000 toward a proposed $90-million hospital and cancer center to be constructed in Sevierville in the name of Robert F. Thomas, the physician who delivered her. She announced a benefit concert to raise additional funds for the project. The concert played to about 8,000 people. That same year, Emmylou Harris and she had allowed their music to be used in a PETA ad campaign that encouraged pet owners to keep their dogs indoors rather than chained outside.

In 2003, her efforts to preserve the bald eagle through the American Eagle Foundation's sanctuary at Dollywood earned her the Partnership Award from the U.S. Fish and Wildlife Service. Parton received the Woodrow Wilson Award for Public Service from the Woodrow Wilson International Center for Scholars of the Smithsonian Institution at a ceremony in Nashville on November 8, 2007. In February 2018, she donated her 100 millionth free book, a copy of Parton's children's picture book "Coat of Many Colors". It was donated to the Library of Congress in Washington, D.C.

For her work in literacy, Parton has received various awards, including Association of American Publishers Honors Award (2000), Good Housekeeping Seal of Approval (2001) (the first time the seal had been awarded to a person), American Association of School Administrators – Galaxy Award (2002), National State Teachers of the Year – Chasing Rainbows Award (2002), and Parents as Teachers National Center – Child and Family Advocacy Award (2003).

On May 8, 2009, Parton gave the commencement speech at the graduation ceremony for the University of Tennessee, Knoxville's College of Arts and Sciences. During the ceremony, she received an honorary Doctor of Humane Letters from the university. It was only the second honorary degree given by the university, and in presenting the degree, the university's Chancellor, Jimmy G. Cheek, said, "Because of her career not just as a musician and entertainer, but for her role as a cultural ambassador, philanthropist and lifelong advocate for education, it is fitting that she be honored with an honorary degree from the flagship educational institution of her home state."

In response to the 2016 Great Smoky Mountains wildfires, Parton was one of a number of country music artists who participated in a telethon to raise money for victims of the fires. This was held in Nashville on December 9. In addition, Parton hosted her own telethon for the victims on December 13 and reportedly raised around $9 million.

In response to the COVID-19 pandemic, Parton donated $1 million towards research at Vanderbilt University and encouraged those who can afford it to make similar donations. Parton has been a generous donor to VUMC (Vanderbilt University School of Medicine). Among her gifts was a contribution to the Monroe Carell Jr. Children’s Hospital at Vanderbilt Pediatric Cancer Program in honor of a lifelong friend, Professor Naji Abumrad, of Vanderbilt University Medical Center, and her niece, Hannah Dennison, who was successfully treated for leukemia as a child at Children’s Hospital.

Parton is one of the most-honored female country performers of all time. The Record Industry Association of America has certified 25 of her single or album releases as either Gold Record, Platinum Record or Multi-Platinum Record. She has had 26 songs reach No. 1 on the Billboard country charts, a record for a female artist. She has 42 career top-10 country albums, a record for any artist, and 110 career-charted singles over the past 40 years. All-inclusive sales of singles, albums, collaboration records, compilation usage, and paid digital downloads during Parton's career have reportedly topped 100 million records around the world.

Parton has earned nine Grammy Awards (including her 2011 Lifetime Achievement Grammy) and a total of 49 Grammy Award nominations, the 2nd most nominations of any female artist in the history of the prestigious awards.

At the American Music Awards, she has won three awards out of 18 nominations. At the Country Music Association, she has won 10 awards out of 42 nominations. At the Academy of Country Music, she has won seven awards and 39 nominations. She is one of only six female artists (including Reba McEntire, Barbara Mandrell, Shania Twain, Loretta Lynn, and Taylor Swift), to win the Country Music Association's highest honor, Entertainer of the Year (1978). She also has been nominated for two Academy Awards and a Tony Award. She was nominated for an Emmy Award for her appearance in a 1978 Cher television special. She was awarded a star on the Hollywood Walk of Fame for her music in 1984, located at 6712 Hollywood Boulevard in Hollywood, California; a star on the Nashville StarWalk for Grammy winners; and a bronze sculpture on the courthouse lawn in Sevierville. She has called that statue of herself in her hometown "the greatest honor", because it came from the people who knew her. Parton was inducted into the Grand Ole Opry in 1969, and in 1986 was named one of "Ms. Magazine"'s Women of the Year. In 1986, she was inducted into the Nashville Songwriters Hall of Fame.

In 1999, Parton received country music's highest honor, an induction into the Country Music Hall of Fame. She received an honorary doctorate degree from Carson-Newman College (Jefferson City, Tennessee) in 1990. This was followed by induction into the National Academy of Popular Music/Songwriters Hall of Fame in 2001. In 2002, she ranked No. 4 in CMT's 40 Greatest Women of Country Music.

Parton has received 46 Grammy Award nominations, tying her with Bruce Springsteen for the most Grammy nominations and positioning her in tenth place overall.

Parton was honored in 2003 with a tribute album called "Just Because I'm a Woman: Songs of Dolly Parton." The artists who recorded versions of Parton's songs included Melissa Etheridge ("I Will Always Love You"), Alison Krauss ("9 to 5"), Shania Twain ("Coat of Many Colors"), Meshell Ndegeocello ("Two Doors Down"), Norah Jones ("The Grass is Blue"), and Sinéad O'Connor ("Dagger Through the Heart"). Parton herself contributed a re-recording of the title song, originally the title song for her first RCA album in 1968. Parton was awarded the Living Legend Medal by the U.S. Library of Congress on April 14, 2004, for her contributions to the cultural heritage of the United States. She is also the focus of a Library of Congress collection exploring the influences of country music on her life and career. The collection contains images, articles, sheet music, and more.
In 2005, she was honored with the National Medal of Arts, the highest honor given by the U.S. government for excellence in the arts. The award is presented by the U.S. President. On December 3, 2006, Parton received the Kennedy Center Honors from the John F. Kennedy Center for the Performing Arts for her lifetime of contributions to the arts. During the show, some of country music's biggest names came to show their admiration. Carrie Underwood performed "Islands in the Stream" with Rogers, Parton's original duet partner. Krauss performed "Jolene" and duetted "Coat of Many Colors" with Twain. McEntire and Reese Witherspoon also came to pay tribute. On November 16, 2010, Parton accepted the Liseberg Applause Award, the theme park industry's most prestigious honor, on behalf of Dollywood theme park during a ceremony held at IAAPA Attractions Expo 2010 in Orlando, Florida.

In 2015, a newly discovered species of fungus found growing in the southern Appalachians was named "Japewiella dollypartoniana" in honor of Parton's music and her efforts to bring national and global attention to that region.

In 2018, Parton received a second star on the Hollywood Walk of Fame, inducted alongside Linda Ronstadt and Emmylou Harris in recognition of their work as a trio.

Parton was also recognized in the Guinness World Records 2018 Edition for holding records for the Most Decades with a Top 20 hit on Billboard's Hot Country Songs Chart and Most Hits on Billboard's Hot Country Songs Chart by a Female Artist.

In 2020, Parton received a Grammy award for her collaboration with For King & Country on their song, "God Only Knows."

During her career, Parton has gained induction into numerous Halls of Fame. Those honors include:

Theatrical releases






</doc>
<doc id="8717" url="https://en.wikipedia.org/wiki?curid=8717" title="Diprotodon">
Diprotodon

Diprotodon, meaning in Greek "two forward teeth", is an extinct genus of diprotodontid marsupial native to Australia during the Pleistocene epoch. The genus is currently considered monotypic, containing only Diprotodon optatum, the largest known marsupial to have ever existed. It is considered one of the core species of the "Australian megafauna", which ranged throughout the continent during the Pleistocene. "Diprotodon" existed from about 1.6 million years ago until extinction some 44,000 years ago.

"Diprotodon" species fossils have been found in sites across mainland Australia, including complete skulls and skeletons, and foot impressions. The largest specimens were hippopotamus-sized: about from nose to tail, standing tall at the shoulder and weighing about . Diprotodonts may have been depicted on Aboriginal rock art images in Quinkan traditional country (Queensland, Australia). "Diprotodon" became extinct sometime after 44,000 years ago, after the initial settlement of the continent, the role of human and climactic factors in its extinction are uncertain and contested.

The closest surviving relatives of "Diprotodon" are the wombats and the koala, and "Diprotodon" have sometimes been referred to as "giant wombats" in the popular press. Diprotodonts are suggested to have inspired legends of the bunyip, as some Aboriginal tribes identify "Diprotodon" bones as those of "bunyips".

The first recorded "Diprotodon" remains were discovered in a cave near Wellington, New South Wales, in the early 1830s by bushman George Ranken and Major Thomas Mitchell; the latter sent them to England for study by Sir Richard Owen. In the 1840s, Ludwig Leichhardt discovered many "Diprotodon" bones eroding from the banks of creeks in the Darling Downs of Queensland, and when reporting the find to Owen, commented that the remains were so well preserved, he expected to find living examples in the then-unexplored central regions of Australia.

The majority of fossil finds are of demographic groups indicative of diprotodonts dying in drought conditions. For example, hundreds of individuals were found in Lake Callabonna with well-preserved lower bodies, but crushed and distorted heads. Several family groups are thought to have sunk in mud while crossing the drying lake bed. Other finds consist of age groupings of young or old animals, which are first to die during a drought.

In 2012, a significant group of about 40 was found at Eulo, south-west Queensland.

"Diprotodon" was named by Owen (1838). It was assigned to the Diprotodontidae by McKenna and Bell (1997). The historical classification of "Diprotodon" consisted of eight species ("D. optatum" Owen, 1838; "D. australis" Owen, 1844; "D. annextans" McCoy, 1861; "D. minor" Huxley, 1862; "D. longiceps" McCoy 1865; "D. loderi" Krefft, 1873a; "D. bennettii" Krefft, 1873b (nec "D. bennettii" Owen, 1877); and "D. bennettii" Owen, 1877 (nec "D. bennettii" Krefft, 1873b); based on size or slight morphological differences of single specimens collected from isolated geographic regions. Bimodal dental sizes, rather than a continuum of tooth sizes, and identical male and female dental morphology, indicate sexual dimorphism instead of separate species, thus providing strong evidence that the eight species are synonyms for "D. optatum". "Diprotodon" is ultimately thought to have evolved from "Euryzygoma dunensis," a smaller diprotodontid known from the Pliocene of eastern Australia, and a smaller form of "Diprotodon", labelled "D." ?"optatum", intermediate in size between the two taxa, is known from the Early Pleistocene (1.77–0.78 Ma) in Nelson Bay near Portland, Victoria

"Diprotodon" superficially resembled a rhinoceros without a horn. Its feet turned inwards like a wombat's, giving it a pigeon-toed appearance. It had strong claws on the front feet and its pouch opening faced backwards.

Until recently, how many species of "Diprotodon" had existed was unknown. Eight species are described, although many researchers believed these actually represented only three at most, while some estimated about 20 in total could exist.

John Walter Gregory collected stories of mystical creatures in Aboriginal myths and legends and considered the possible connections between them and extinct species. He reported a story of a "big, heavy land animal, with a single horn on its forehead" as a possible reference to "Diprotodon"; the presence of a horn on the rostrum of the species is not scientifically acknowledged.
The skull of "Diprotodon" has large endocranial sinus cavities, which separate the relatively small cranial vault from the outer part of the skull. These significantly lighten the skull while providing large areas for muscle attachment and reduce load stress.

Recent research compared the variation between all of the described "Diprotodon" species with the variation in one of Australia's largest living marsupials, the eastern grey kangaroo, and found the range was comparable, with a near continent-wide distribution. This left only two possible "Diprotodon" species differing only in size with the smaller being around half the size of the larger. According to Gause’s "competitive exclusion principle", no two species with identical ecological requirements can coexist in a stable environment. However, both the small and large diprotodonts coexisted throughout the Pleistocene and the size difference is similar to other sexually dimorphic living marsupials. Further evidence is the battle damage common in competing males found on the larger specimens, but absent from the smaller. Dental morphology also supports sexual dimorphism, with highly sexually dimorphic marsupials, such as the grey kangaroo, having different tooth sizes between males and females, but both sexes having the same dental morphology. An identical dental morphology occurs in the large and small "Diprotodon". The taxonomic implication is that Owen's original "Diprotodon optatum" is the only valid species.

A single sexually dimorphic species allows behavioural interpretations. All sexually dimorphic species of over exhibit a polygynous breeding strategy. A modern example of this is the gender segregation of elephants, where females and the young form family groups, while lone males fight for the right to mate with all the females of the group. This behaviour is consistent with fossil finds where adult/juvenile fossil assemblages usually contain only female adult remains.
Cyclic variations in the strontium isotope ratios within the tooth enamel of a 300,000-year-old fossil imply that a population of "Diprotodon" undertook regular, seasonal migrations across the Darling Downs, making it the only known extinct or extant metatherian known to migrate annually. The carbon-13 enamel content was found to have little variation, suggesting a relatively consistent diet through the course of a year containing a mix of both C and C plants. A finite element method analysis of the skull estimated it had a bite force of around 4500 Newtons at the first molar to over 11,000 N at the fourth molar, values which were described as "exceptionally high", suggesting that "Diprotodon" was capable of processing tough, fibrous food.

Some modern researchers, including Richard Roberts and Tim Flannery, argue that diprotodonts, along with a wide range of other Australian megafauna, became extinct shortly after humans arrived in Australia about 50,000 years ago. Others, including Steve Wroe, note that records in the Australian Pleistocene are rare, and there is not enough data to definitively determine the time of extinction of many of the species, with many of the species having no confirmed record within the last 100,000 years. They suggest that many of the extinctions had been staggered over the course of the late Middle Pleistocene and early Late Pleistocene, prior to human arrival, due to climatic stress. "Diprotodon" is one of several species with confirmed dates post-dating human arrival on the continent, with the latest high-reliability date being around 44 kyr BP.

Some earlier researchers, including Richard Wright, argued on the contrary that diprotodont remains from several sites, such as Tambar Springs and Trinkey and Lime Springs, suggest that "Diprotodon" survived much longer, into the Holocene. Other more recent researchers, including Lesley Head and Judith Field, favour an extinction date of 28,000 to 30,000 years ago, which would mean that humans coexisted with "Diprotodon" for some 20,000 years. However, opponents of "late extinction" theories have interpreted such late dates based on indirect dating methods as artifacts resulting from redeposition of skeletal material into more recent strata, and recent direct dating results obtained with new technologies have tended to confirm this interpretation.

Three theories have been advanced to explain the mass extinction.

Australia, as with many other areas of the globe, underwent significant ecological change from predominantly forests to open landscapes during the late Neogene, in correlation with increased cooling. There has been considerable aridification of the Australian interior since the Late Miocene. The recent ice ages produced no significant glaciation in mainland Australia, but long periods of cold and increased aridification. These climatic changes have been suggested as a contributing factor in extinction.

The overkill theory is that human hunters killed and ate the diprotodonts, causing their extinction. The extinctions appear to have coincided with the arrival of humans on the continent. Similar hunting-out happened with the megafauna of New Zealand, Madagascar, and many smaller islands around the world (such as New Caledonia, the Greater Antilles). Critics of this theory regard it as simplistic, arguing that (unlike New Zealand and America), little direct evidence of hunting has been found, and the dates on which the theory rests are too uncertain to be reliable. Linear striated cuts on a "Diprotodon" tooth from south-eastern Australia initially suggested to have been etched by people are now thought to be bite marks from a spotted‐tailed quoll. A partial juvenile radius of "Diprotodon optatum" over 47,000 years old was found at the Warratyi rock shelter in South Australia north of Adelaide. Due to the shelter's location on a steep escarpment and the lack of bite marks on the bone, it is thought that humans transported the bone to the site. At present, this represents the only known interaction between humans and "Diprotodon".

An examination of swamp sediment cores spanning the last 130,000 years from Lynch's Crater in Queensland suggests that hunting may have been the primary cause of the extinction. Analysis of "Sporormiella" fungal spores (which are found in the dung of herbivores and are used as a proxy for their abundance) in the cores shows that the "Spororniella" records in that region virtually disappeared about 41,000 years ago, at a time when climate changes were minimal; the change was accompanied by an increase in charcoal, and was followed by a transition from rainforest to fire-tolerant sclerophyll vegetation. The high-resolution chronology of the changes indicates that fire increased about a century after the disappearance of browsing megafauna, probably due to accumulation of fuel. Grass increased over the next several centuries; sclerophyll vegetation increased following a lag of another century, and a sclerophyll forest developed about a thousand years later. Earlier increases in sclerophyll vegetation during shifts to cooler, drier conditions about 120,000 and 75,000 years ago did not have any obvious impact on megafaunal abundance. A study on "Sporormiella" in cores going back 135,000 years at the Caledonia Fen wetland in Alpine National Park, found that "Sporomiella" levels rose after the end of the last interglacial, followed by a sharp drop around 76–60 kya, associated with an increased proportion of wetland plants and further grassland and herbfield; a second sharp drop was observed around 52–45 kya, suggested to represent the megafaunal extinction interval. Charcoal was found throughout the core, suggesting that the arrival of humans and the extinction of megafauna did not change the fire regime at this locality. However, the use of "Sporormiella" as a megafaunal proxy has been criticised, noting that "Sporormiella" is found sporadically in the dung of various herbivores, including extant emus and kangaroos, not just megafauna, its presence depends on a variety of factors, often unrelated to megafaunal abundance, and that in Cuddie Springs, a well known megafaunal site, the densities of "Sporormiella" were consistently low.

The third theory says that humans indirectly caused the extinction of diprotodonts by destroying the ecosystem on which they depended. In particular, early Aboriginal people are thought to have been fire-stick farmers using fire regularly and persistently to drive game, open up dense thickets of vegetation, and create fresh green regrowth for both humans and game animals to eat. Evidence for the fire hypothesis is the sudden increase in widespread ash deposits at the time that people arrived in Australia, as well as land-management and hunting practices of modern Aboriginal people as recorded by the earliest European settlers. Evidence against the hypothesis is the fact that humans appear to have eliminated the megafauna of Tasmania without using fire to modify the environment there.





</doc>
<doc id="8718" url="https://en.wikipedia.org/wiki?curid=8718" title="Dirk Benedict">
Dirk Benedict

Dirk Benedict (born Dirk Niewoehner Metzger on March 1, 1945) is an American movie, television and stage actor and author. He is best known for playing the characters Lieutenant Starbuck in the original "Battlestar Galactica" film and television series and Lieutenant Templeton "Faceman" Peck in "The A-Team" television series. He is the author of "Confessions of a Kamikaze Cowboy" and "And Then We Went Fishing".

Benedict was born Dirk Niewoehner in Helena, Montana, the son of Priscilla Mella (née Metzger), an accountant, and George Edward Niewoehner, a lawyer. He grew up in White Sulphur Springs, Montana. He graduated from Whitman College in 1967.
Benedict allegedly chose his stage name from a serving of Eggs Benedict he had prior to his acting career. He is of German extraction.

Benedict's film debut was in the 1972 film "Georgia, Georgia". When the New York run for "Butterflies Are Free" ended, he received an offer to repeat his performance in Hawaii, opposite Barbara Rush. While there, he appeared as a guest lead on "Hawaii Five-O". The producers of a horror film called "Sssssss" (1973) saw Benedict's performance in "Hawaii Five-O" and promptly cast him as the lead in that movie. He next played the psychotic wife-beating husband of Twiggy in her American film debut, "W" (1974). Benedict starred in the television series "Chopper One", which aired for one season in 1974. He made two appearances in "Charlie's Angels". He also appeared on the "Donny & Marie" variety show.

Benedict's career break came in 1978 when he appeared as Lieutenant Starbuck in the movie and television series "Battlestar Galactica". The same year Benedict starred in the TV movie "Cruise Into Terror", and appeared in the ensemble movie "Scavenger Hunt" the following year.

In 1980, Benedict starred alongside Linda Blair in an action-comedy movie called "Ruckus". In 1983, Dirk gained further popularity as con-man Lieutenant Templeton "Face" Peck in 1980s action television series "The A-Team". He played "Faceman" from to , although the series didn't air until January 1983, and the final episode wasn't shown until 1987 rebroadcasts. The episode "Steel" includes a scene at Universal Studios where Face is seen looking bemused as a Cylon walks by him as an in-joke to his previous role in "Battlestar Galactica". The clip is incorporated into the series' opening credit sequence from season 3 onward.

In 1986, Benedict starred as low-life band manager Harry Smilac in the movie "Body Slam" along with Lou Albano, Roddy Piper, and cameo appearances by Freddie Blassie, Ric Flair, and Bruno Sammartino. His character Smilac ends up managing the pro-wrestler "Quick Rick" Roberts (Piper) and faces opposition by Captain Lou and his wrestling tag-team "the Cannibals".

In 1987, Benedict took the title role of Shakespeare's "Hamlet" at the Abbey Theatre in Manhattan. Both his performance and the entire production were lambasted by critics. Benedict starred in the 1989 TV movie "Trenchcoat in Paradise".

In 1991, Benedict starred in "Blue Tornado," playing Alex, call sign Fireball, an Italian Air Force fighter pilot. Benedict published an autobiography, "Confessions of a Kamikaze Cowboy: A True Story of Discovery, Acting, Health, Illness, Recovery, and Life" (Avery Publishing ). In 1993, Benedict starred in "Shadow Force".

Benedict also appeared as Jake Barnes in the 1996 action-adventure film "Alaska".

In 2000, Benedict wrote and directed his first screenplay, "Cahoots". Benedict appeared in the 2006 German film "Goldene Zeiten" ("Golden Times") in a dual role, playing an American former TV star as well as a German lookalike who impersonates him.

In 2006, he wrote an online essay criticizing the then-airing "Battlestar Galactica" re-imagined series and, especially, its casting of a woman as his character, Starbuck, writing that "the war against masculinity has been won" and that "a television show based on hope, spiritual faith, and family is unimagined and regurgitated as a show of despair, sexual violence and family dysfunction".

He appeared as a contestant on the 2007 U.K. series of "Celebrity Big Brother". He arrived on launch night in a replica of the "A-Team" van, smoking a cigar and accompanied by the "A-Team" theme tune.

In 2010, Benedict starred in a stage production of "Prescription: Murder" playing Lieutenant Columbo for the Middle Ground Theatre Company in the UK. Benedict also made a cameo appearance in the 2010 film adaptation of "The A-Team" as Pensacola Prisoner Milt.

In 2019, Benedict took on the role of Jack Strange in the B movie Space Ninjas written and directed by Scott McQuaid. Dirk plays an eccentric T.V. host of a show called 'Stranger Than Fiction', which is like a hybrid of the Twilight Zone and the X-Files. The movie is a sci-fi, comedy, horror that follows a bunch of high school students trying to survive the night from a Space Ninja invasion.

In the 1970s, Benedict survived a prostate tumor believed to have been cancerous. Having rejected conventional medical treatment, he credited his survival to the adoption of a macrobiotic diet recommended to him by actress Gloria Swanson.
In 1986, he married Toni Hudson, an actress with whom he has two sons, George and Roland. Hudson had previously appeared as Dana in the fourth season "A-Team" episode titled "Blood, Sweat and Cheers". They divorced in 1995. In 1998, Benedict learned that he also has another son, John Talbert (born 1968), from an earlier relationship, who had been placed for adoption. With the help of his adoptive parents, Talbert discovered and contacted his birth parents.



</doc>
<doc id="8724" url="https://en.wikipedia.org/wiki?curid=8724" title="Doppler effect">
Doppler effect

The Doppler effect (or the Doppler shift) is the change in frequency of a wave in relation to an observer who is moving relative to the wave source. It is named after the Austrian physicist Christian Doppler, who described the phenomenon in 1842.

A common example of Doppler shift is the change of pitch heard when a vehicle sounding a horn approaches and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.

The reason for the Doppler effect is that when the source of the waves is moving towards the observer, each successive wave crest is emitted from a position closer to the observer than the crest of the previous wave. Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrivals of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are traveling, the distance between successive wave fronts is reduced, so the waves "bunch together". Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves "spread out".

For waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium, such as light or gravity in general relativity, only the relative difference in velocity between the observer and the source needs to be considered.

Doppler first proposed this effect in 1842 in his treatise ""Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels"" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called "effet Doppler-Fizeau" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).

In classical physics, where the speeds of source and the receiver relative to the medium are lower than the velocity of waves in the medium, the relationship between observed frequency formula_1 and emitted frequency formula_2 is given by:

Note this relationship predicts that the frequency will decrease if either source or receiver is moving away from the other.

Equivalently, under the assumption that the source is either directly approaching or receding from the observer:

If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.

If the speeds formula_7 and formula_5 are small compared to the speed of the wave, the relationship between observed frequency formula_1 and emitted frequency formula_2 is approximately
Given formula_19

we divide for formula_6

formula_21

Since formula_22 we can substitute the geometric expansion:

formula_23

With an observer stationary relative to the medium, if a moving source is emitting waves with an actual frequency formula_2 (in this case, the wavelength is changed, the transmission velocity of the wave keeps constant; note that the "transmission velocity" of the wave does not depend on the "velocity of the source"), then the observer detects waves with a frequency formula_1 given by

A similar analysis for a moving "observer" and a stationary source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves and hence the "transmission velocity" of the wave [with respect to the observer] is changed) yields the observed frequency:

A similar analysis for a moving "observer" and a moving source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves and hence the "transmission velocity" of the wave [with respect to the observer] is changed) yields the observed frequency:

Assuming a stationary observer and a source moving at the speed of sound, the Doppler equation predicts a perceived momentary infinite frequency by an observer in front of a source traveling at the speed of sound. All the peaks are at the same place, so the wavelength is zero and the frequency is infinite. This overlay of all the waves produces a shock wave which for sound waves is known as a sonic boom.

When the source moves faster than the wave speed the source outruns the wave. The equation can give negative frequency values, but -500 Hz is pretty much the same as +500 Hz as far as an observer is concerned. 

Lord Rayleigh predicted the following effect in his classic book on sound: if the source is moving toward the observer at twice the speed of sound, a musical piece emitted by that source would be heard in correct time and tune, but "backwards". The Doppler effect with sound is only clearly heard with objects moving at high speed, as change in frequency of musical tone involves a speed of around 40 meters per second, and smaller changes in frequency can easily be confused by changes in the amplitude of the sounds from moving emitters. Neil A Downie has demonstrated how the Doppler effect can be made much more easily audible by using an ultrasonic (e.g. 40 kHz) emitter on the moving object. The observer then uses a heterodyne frequency converter, as used in many bat detectors, to listen to a band around 40 kHz. In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second.

An acoustic Doppler current profiler (ADCP) is a hydroacoustic current meter similar to a sonar, used to measure water current velocities over a depth range using the Doppler effect of sound waves scattered back from particles within the water column. The term ADCP is a generic term for all acoustic current profilers, although the abbreviation originates from an instrument series introduced by RD Instruments in the 1980s. The working frequencies range of ADCPs range from 38 kHz to several Megahertz. The device used in the air for wind speed profiling using sound is known as SODAR and works with the same underlying principles.

Dynamic real-time path planning in robotics to aid the movement of robots in a sophisticated environment with moving obstacles often take help of Doppler effect. Such applications are specially used for competitive robotics where the environment is constantly changing, such as robosoccer.

A siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:

In other words, if the siren approached the observer directly, the pitch would remain constant, at a higher than stationary pitch, until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial velocity does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:

where formula_31 is the angle between the object's forward velocity and the line of sight from the object to the observer.

The Doppler effect for electromagnetic waves such as light is of great use in astronomy and results in either a so-called redshift or blueshift. It has been used to measure the speed at which stars and galaxies are approaching or receding from us; that is, their radial velocities. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. This redshift and blueshift happens on a very small scale. If an object was moving toward earth, there would not be a noticeable difference in visible light, to the unaided eye.

Note that redshift is also used to measure the expansion of space, but that this is not truly a Doppler effect. Rather, redshifting due to the expansion of space is known as cosmological redshift, which can be derived purely from the Robertson-Walker metric under the formalism of General Relativity. Having said this, it also happens that there "are" detectable Doppler effects on cosmological scales, which, if incorrectly interpreted as cosmological in origin, lead to the observation of redshift-space distortions.

The use of the Doppler effect for light in astronomy depends on our knowledge that the spectra of stars are not homogeneous. They exhibit absorption lines at well defined frequencies that are correlated with the energies required to excite electrons in various elements from one level to another. The Doppler effect is recognizable in the fact that the absorption lines are not always at the frequencies that are obtained from the spectrum of a stationary light source. Since blue light has a higher frequency than red light, the spectral lines of an approaching astronomical light source exhibit a blueshift and those of a receding astronomical light source exhibit a redshift.

Among the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and −260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial velocity means the star is receding from the Sun, negative that it is approaching.

The Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target — e.g. a motor car, as police use radar to detect speeding motorists — as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's velocity. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.

Because the doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative velocity formula_32 is twice that from the same target emitting a wave:

An echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, abnormal communications between the left and right side of the heart, leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.

Although "Doppler" has become synonymous with "velocity measurement" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift ("when" the received signal arrives).

Velocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.

Instruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.

Developed originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.

Fast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed progressively during transmission, is used so the satellite receives a constant frequency signal. After realizing that the Dopper shift had not been considered before launch of the Huygens probe of the 2005 Cassini–Huygens mission, the probe trajectory was altered to approach Titan in such a way that its transmissions traveled perpendicular to its direction of motion relative to Cassini, greatly reducing the Doppler shift.

Doppler shift of the direct path can be estimated by the following formula:

formula_34

where formula_35 is the velocity of the mobile station, formula_36 is the wavelength of the carrier, formula_37 is the elevation angle of the satellite and formula_31 is the driving direction with respect to the satellite.

The additional Doppler shift due to the satellite moving can be described as:

formula_39

where formula_40 is the relative speed of the satellite.

The Leslie speaker, most commonly associated with and predominantly used with the famous Hammond organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.

A laser Doppler vibrometer (LDV) is a non-contact instrument for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.

During the segmentation of vertebrate embryos, waves of gene expression sweep across the presomitic mesoderm, the tissue from which the precursors of the vertebrae (somites) are formed. A new somite is formed upon arrival of a wave at the anterior end of the presomitic mesoderm. In zebrafish, it has been shown that the shortening of the presomitic mesoderm during segmentation leads to a Doppler effect as the anterior end of the tissue moves into the waves. This Doppler effect contributes to the period of segmentation.

Since 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The size of the Doppler shift depends on the refractive index of the medium a wave is traveling through. But some materials are capable of negative refraction, which should lead to a Doppler shift that works in a direction opposite that of a conventional Doppler shift. First experiment that detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003. Later inverse Doppler effect was observed in some inhomogeneous materials and predicted inside Vavilov–Cherenkov cone.




</doc>
<doc id="8727" url="https://en.wikipedia.org/wiki?curid=8727" title="ΔT">
ΔT

In precise timekeeping, Δ"T (Delta "T, delta-"T, delta"T, or D"T") is a measure of the cumulative effect of the departure of the Earth's rotation period from the fixed-length day of atomic time. Formally it is the time difference obtained by subtracting Universal Time (UT, defined by the Earth's rotation) from Terrestrial Time (TT, independent of the Earth's rotation): . The value of ΔT for the start of 1902 was approximately zero; for 2002 it was about 64 seconds. So the Earth's rotations over that century took about 64 seconds longer than would be required for days of atomic time. As well as this long-term drift in the length of the day there are short-term fluctuations in the length of day () which are dealt with separately.

The Earth's rotational speed is , and a day corresponds to one period . A rotational acceleration gives a rate of change of the period of , which is usually expressed as . This has units of 1/time, and is commonly quoted as milliseconds-per-day per century (written as ms/day/cy, understood as (ms/day)/cy). Integrating gives an expression for Δ"T" against time.

Universal Time is a time scale based on the Earth's rotation, which is somewhat irregular over short periods (days up to a century), thus any time based on it cannot have an accuracy better than 1 in 10. However, a larger, more consistent effect has been observed over many centuries: Earth's rate of rotation is inexorably slowing down. This observed change in the rate of rotation is attributable to two primary forces, one decreasing and one increasing the Earth's rate of rotation. Over the long term, the dominating force is tidal friction, which is slowing the rate of rotation, contributing about ms/day/cy or ms/cy, which is equal to the very small fractional change day/day. The most important force acting in the opposite direction, to speed up the rate, is believed to be a result of the melting of continental ice sheets at the end of the last glacial period. This removed their tremendous weight, allowing the land under them to begin to rebound upward in the polar regions, an effect that is still occurring today and will continue until isostatic equilibrium is reached. This "post-glacial rebound" brings mass closer to the rotational axis of the Earth, which makes the Earth spin faster, according to the law of conservation of angular momentum, similar to an ice skater pulling their arms in to spin faster. Models estimate this effect to contribute about −0.6 ms/day/cy. Combining these two effects, the net acceleration (actually a deceleration) of the rotation of the Earth, or the change in the length of the mean solar day (LOD), is +1.7 ms/day/cy or +62 s/cy or +46.5 ns/day. This matches the average rate derived from astronomical records over the past 27 centuries.

Terrestrial Time is a theoretical uniform time scale, defined to provide continuity with the former Ephemeris Time (ET). ET was an independent time-variable, proposed (and its adoption agreed) in the period 1948–52 with the intent of forming a gravitationally uniform time scale as far as was feasible at that time, and depending for its definition on Simon Newcomb's "Tables of the Sun" (1895), interpreted in a new way to accommodate certain observed discrepancies. Newcomb's tables formed the basis of all astronomical ephemerides of the Sun from 1900 through 1983: they were originally expressed (and published) in terms of Greenwich Mean Time and the mean solar day, but later, in respect of the period 1960–1983, they were treated as expressed in terms of ET, in accordance with the adopted ET proposal of 1948–52. ET, in turn, can now be seen (in light of modern results) as close to the average mean solar time between 1750 and 1890 (centered on 1820), because that was the period during which the observations on which Newcomb's tables were based were performed. While TT is strictly uniform (being based on the SI second, every second is the same as every other second), it is in practice realised by International Atomic Time (TAI) with an accuracy of about 1 part in 10.

Earth's rate of rotation must be integrated to obtain time, which is Earth's angular position (specifically, the orientation of the meridian of Greenwich relative to the fictitious mean sun). Integrating +1.7 ms/d/cy and centering the resulting parabola on the year 1820 yields (to a first approximation) seconds for Δ"T". Smoothed historical measurements of Δ"T" using total solar eclipses are about +17190 s in the year −500 (501 BC), +10580 s in 0 (1 BC), +5710 s in 500, +1570 s in 1000, and +200  s in 1500. After the invention of the telescope, measurements were made by observing occultations of stars by the Moon, which allowed the derivation of more closely spaced and more accurate values for Δ"T". Δ"T" continued to decrease until it reached a plateau of +11 ± 6 s between 1680 and 1866. For about three decades immediately before 1902 it was negative, reaching −6.64 s. Then it increased to +63.83 s in January 2000 and +68.97 s in January 2018 and +69.361 s in January 2020, after even a slight decrease from 69.358 s in July 2019 to 69.338 s in September and October 2019 and a new increase in November and December 2019. This will require the addition of an ever-greater number of leap seconds to UTC as long as UTC tracks UT1 with one-second adjustments. (The SI second as now used for UTC, when adopted, was already a little shorter than the current value of the second of mean solar time.) Physically, the meridian of Greenwich in Universal Time is almost always to the east of the meridian in Terrestrial Time, both in the past and in the future. +17190 s or about  h corresponds to 71.625°E. This means that in the year −500 (501 BC), Earth's faster rotation would cause a total solar eclipse to occur 71.625° to the east of the location calculated using the uniform TT.

All values of Δ"T" before 1955 depend on observations of the Moon, either via eclipses or occultations. The angular momentum lost by the Earth due to friction induced by the Moon's tidal effect is transferred to the Moon, increasing its angular momentum, which means that its moment arm (approximately its distance from the Earth, i.e. precisely the semi-major axis of the Moon's orbit) is increased (for the time being about +3.8 cm/year), which via Kepler's laws of planetary motion causes the Moon to revolve around the Earth at a slower rate. The cited values of Δ"T" assume that the lunar acceleration (actually a deceleration, that is a negative acceleration) due to this effect is = −26″/cy, where is the mean sidereal angular motion of the Moon. This is close to the best estimate for as of 2002 of −25.858 ± 0.003″/cy, so Δ"T" need not be recalculated given the uncertainties and smoothing applied to its current values. Nowadays, UT is the observed orientation of the Earth relative to an inertial reference frame formed by extra-galactic radio sources, modified by an adopted ratio between sidereal time and solar time. Its measurement by several observatories is coordinated by the International Earth Rotation and Reference Systems Service (IERS).

Tidal deceleration rates have varied over the history of the Earth-Moon system. Analysis of layering in fossil mollusc shells from 70 million years ago, in the Late Cretaceous period, shows that there were 372 days a year, and thus that the day was about 23.5 hours long then. Based on geological studies of "tidal rhythmites," the day was 21.9±0.4 hours long 620 million years ago and there were 13.1±0.1 synodic months/year and 400±7 solar days/year. The average recession rate of the Moon between then and now has been 2.17±0.31 cm/year, which is about half the present rate. The present high rate may be due to near resonance between natural ocean frequencies and tidal frequencies.





</doc>
<doc id="8728" url="https://en.wikipedia.org/wiki?curid=8728" title="December 22">
December 22





</doc>
<doc id="8729" url="https://en.wikipedia.org/wiki?curid=8729" title="David Deutsch">
David Deutsch

David Elieser Deutsch (; born 18 May 1953) is a British physicist at the University of Oxford. He is a Visiting Professor in the Department of Atomic and Laser Physics at the Centre for Quantum Computation (CQC) in the Clarendon Laboratory of the University of Oxford. He pioneered the field of quantum computation by formulating a description for a quantum Turing machine, as well as specifying an algorithm designed to run on a quantum computer. He has also proposed the use of entangled states and Bell's theorem for quantum key distribution
and is a proponent of the many-worlds interpretation of quantum mechanics.

In 2009, Deutsch expounded a new criterion for scientific explanation, which is to formulate invariants: 'State an explanation [publicly, so that it can be dated and verified by others later] that remains invariant [in the face of apparent change, new information, or unexpected conditions]'.

Deutsch was born in Haifa in Israel on 18 May 1953, the son of Oskar and Tikva Deutsch. David attended Geneva House school in Cricklewood (his parents owned and ran the Alma restaurant on Cricklewood Broadway, NW2), followed by The William Ellis School in Highgate (then a voluntary aided school in north London) before reading Natural Sciences at Clare College, Cambridge and taking Part III of the Mathematical Tripos. He went on to Wolfson College, Oxford for his doctorate in theoretical physics and wrote his thesis on quantum field theory in curved space-time supervised by Dennis Sciama and Philip Candelas.

His work on quantum algorithms began with a 1985 paper, later expanded in 1992 along with Richard Jozsa to produce the Deutsch–Jozsa algorithm, one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm. In his 1985 paper, he also suggests the use of entangled states and Bell's theorem for quantum key distribution. In his nomination for election as a Fellow of the Royal Society (FRS) in 2008, his contributions were described as:

"[having] laid the foundations of the quantum theory of computation, and has subsequently made or participated in many of the most important advances in the field, including the discovery of the first quantum algorithms, the theory of quantum logic gates and quantum computational networks, the first quantum error-correction scheme, and several fundamental quantum universality results. He has set the agenda for worldwide research efforts in this new, interdisciplinary field, made progress in understanding its philosophical implications (via a variant of the many-universes interpretation) and made it comprehensible to the general public, notably in his book The Fabric of Reality."

Since 2012, he has been working on constructor theory, an attempt at generalizing the quantum theory of computation to cover not just computation but all physical processes. Together with Chiara Marletto, he published a paper in December 2014 entitled "Constructor theory of information", that conjectures that information can be expressed solely in terms of which transformations of physical systems are possible and which are impossible.
In his 1997 book "The Fabric of Reality", Deutsch details his "Theory of Everything". It aims not at the reduction of everything to particle physics, but rather mutual support among multiversal, computational, epistemological, and evolutionary principles. His theory of everything is somewhat emergentist rather than reductive. There are four strands to his theory:


Deutsch's second book, "The Beginning of Infinity: Explanations that Transform the World", was published on 31 March 2011. In this book, Deutsch views the Enlightenment of the 18th century as near the beginning of a potentially unending sequence of purposeful knowledge creation. He examines the nature of memes and how and why creativity evolved in humans.

"The Fabric of Reality" was shortlisted for the Rhone-Poulenc science book award in 1998. 
Deutsch was awarded the Dirac Prize of the Institute of Physics in 1998, and the Edge of Computation Science Prize in 2005. In 2017, he received the Dirac Medal of the International Centre for Theoretical Physics (ICTP). Deutsch is related to Paul Dirac through his doctoral advisor Dennis Sciama, whose doctoral advisor was Dirac. Deutsch was elected a Fellow of the Royal Society (FRS) in 2008.

Deutsch is an atheist. He is also a founding member of the parenting and educational method known as Taking Children Seriously.




</doc>
<doc id="8730" url="https://en.wikipedia.org/wiki?curid=8730" title="Volkssturm">
Volkssturm

The Volkssturm (, "people's storm") was a national militia established by Nazi Germany during the last months of World War II. It was not set up by the German Army, the ground component of the combined German "Wehrmacht" armed forces, but by the Nazi Party on the orders of Adolf Hitler, and its existence was only officially announced on 16 October 1944. It was staffed by conscripting males between the ages of 16 and 60 years who were not already serving in some military unit. The "Volkssturm" comprised one of the final components of the total war promulgated by Propaganda Minister Joseph Goebbels, part of a Nazi endeavor to overcome their enemies' military strength through force of will.

The new "Volkssturm" drew inspiration from the old Prussian "Landsturm" of 1813–1815, that fought in the liberation wars against Napoleon, mainly as guerrilla forces. Plans to form a "Landsturm" national militia in eastern Germany as a last resort to boost fighting strength were first proposed in 1944 by General Heinz Guderian, chief of the General Staff. The Army did not have enough men to resist the Soviet onslaught. So, additional categories of men were called into service, including those in non-essential jobs, those previously deemed unfit, over-age, or under-age, and those recovering from wounds. The "Volkssturm" had existed, on paper, since around 1925, but it was only after Hitler ordered Martin Bormann to recruit six million men for this militia that the group became a physical reality. The intended strength of "six million" was never attained.

Joseph Goebbels and other propagandists depicted the "Volkssturm" as an outburst of enthusiasm and the will to resist. While it had some marginal effect on morale, it was undermined by the recruits' visible lack of uniforms and weaponry. Nazi themes of death, transcendence, and commemoration were given full play to encourage the fight. Many German civilians realized that this was a desperate attempt to turn the course of the war. Sardonic old men would remark, "We old monkeys are the Führer’s newest weapon" (in German this rhymes: ""Wir alten Affen sind des Führers neue Waffen""). A popular joke about the "Volkssturm" went "Why is the Volkssturm Germany's most precious resource? Because its members have silver in their hair, gold in their mouths, and lead in their bones."

For these militia units to be effective, they needed not only strength in numbers, but also fanaticism. During the early stages of "Volkssturm" planning, it became apparent that units lacking morale would lack combat effectiveness. To generate fanaticism, "Volkssturm" units were placed under direct command of the local Nazi party officials, the "Gauleiter" and "Kreisleiter". The new "Volkssturm" was also to become a nationwide organization, with Heinrich Himmler, as Replacement Army commander, responsible for armament and training. Though nominally under party control, "Volkssturm" units were placed under "Wehrmacht" command when engaged in action. Aware that a "people's army" would not be able to withstand the onslaught of the modern army wielded by the Allies, Hitler issued the following order towards the end of 1944: 

With the Nazi Party in charge of organizing the "Volkssturm", each "Gauleiter", or Nazi Party District Leader, was charged with the leadership, enrollment, and organization of the "Volkssturm" in their district. The largest "Volkssturm" unit seems to have corresponded to the next smaller territorial subdivision of the Nazi Party organization—the "Kreis". The basic unit was a battalion of 642 men. Units were mostly composed of members of the Hitler Youth, invalids, the elderly, or men who had previously been considered unfit for military service. On 12 February 1945, the Nazis conscripted German women and girls into the auxiliaries of the "Volkssturm". Correspondingly, girls as young as 14 years were trained in the use of small arms, panzerfaust, machine guns, and hand grenades from December 1944 through May 1945. 

Municipal organization:


Each "Gauleiter" and "Kreisleiter" had a "Volkssturm" Chief of Staff.

From the militia's inception until the spring of 1945, Himmler and Bormann engaged in a power-struggle over the jurisdictional control over the "Volkssturm" regarding security and police powers in Germany and the occupied territories; a contest which Himmler and his SS more or less won on one level (police and security), but lost to Bormann on another (mobilizing reserve forces). Historian David Yelton described the situation as two ranking officers at the helm of a sinking ship fighting over command.

Benito Mussolini suggested, through his son Vittorio, then general secretary of the Republican Fascist party's German branch, that 30,000 Italians should be added to the Volkssturm in defense of Germany. However, no evidence exists that this offer was implemented.

The "Volkssturm" "uniform" was only a black armband with the German words "Deutscher Volkssturm Wehrmacht". Some variants of the above-mentioned armband worn by Volkssturm members were simply red or yellow cloth-bands with "Deutscher Volkssturm Wehrmacht" printed in black on them. On top of the identification armbands, there were black rank-patches, either with or without silver pips, worn on the collar of the Volkssturm members' uniform. These were characteristically derived from the rank insignia of the various paramilitary organizations of the Nazi Party, which had control over them, and not of the regular "Wehrmacht".

The German government tried to issue as many of its members as possible with military uniforms of all sorts, ranging from field-gray to camouflage types. Most members of the "Volkssturm", especially elderly members, had no uniform and were not supplied, so they generally wore either work-attire (including railway workers, policemen and firemen) or their civilian clothing and usually carried with them their own personal rucksacks, blankets and cooking-equipment, etc.

The simple paramilitary insignia of the "Volkssturm" were as follows:

Typically, members of the "Volkssturm" received only very basic military training. It included a brief indoctrination and training on the use of basic weapons such as the Karabiner 98k rifle and "Panzerfaust". Because of continuous fighting and weapon shortages, weapon training was often minimal. There was also a lack of instructors, meaning that weapons training was sometimes done by World War I veterans drafted into service themselves. Often "Volkssturm" members were only able to familiarize themselves with their weapons when in actual combat.

There was no standardization of any kind and units were issued only what equipment was available. This was true of every form of equipment—"Volkssturm" members were required to bring their own uniforms and culinary equipment etc. This resulted in the units looking very ragged and, instead of boosting civilian morale, it often reminded people of Germany's desperate state. Armament was equally haphazard: though some Karabiner 98ks were on hand, members were also issued older Gewehr 98s and 19th-century Gewehr 71s and Steyr-Mannlicher M1888s, as well as Dreyse M1907 pistols. In addition there was a plethora of Soviet, British, Belgian, French, Italian, and other weapons that had been captured by German forces during the war. The Germans had also developed cheap but reasonably effective "Volkssturm" weapons, such as MP 3008 machine pistols and Volkssturmgewehr rifles. These were completely stamped and machine-pressed constructions (in the 1940s, industrial processes were much cruder than today, so a firearm needed great amounts of semi-artisanal work to be actually reliable). The "Volkssturm" troops were nominally supplied when and where possible by both the "Wehrmacht" and the SS.

When units had completed their training and received armament, members took a customary oath to Hitler and were then dispatched into combat. Unlike most English-speaking countries, Germany had universal military service for all young men for several generations, so many of the older members would have had at least basic military training from when they served in the German Army and many would have been veterans of the First World War. "Volkssturm" units were supposed to be used only in their own districts, but many were sent directly to the front lines. Ultimately, it was their charge to confront the overwhelming power of the British, Canadian, Soviet, American, and French armies alongside "Wehrmacht" forces to either turn the tide of the war or set a shining example for future generations of Germans and expunge the defeat of 1918 by fighting to the last, dying before surrendering. It was an apocalyptic goal which some of those assigned to the "Volkssturm" took to heart. Unremittingly fanatical members of the "Volkssturm" refused to abandon the Nazi ethos unto the dying days of Nazi Germany, and in a number of instances took brutal "police actions" against German civilians deemed defeatists or cowards.

On some occasions, members of the "Volkssturm" showed tremendous courage and a determined will to resist, more so even than soldiers in the "Wehrmacht". The "Volkssturm" battalion 25/235 for instance, started out with 400 men but fought on until there were only 10 men remaining. Fighting at Küstrin between 30 January to 29 March 1945, militia units made up mostly of the "Volkssturm" resisted for nearly two months. Losses were upwards of 60 percent for the "Volkssturm" at Kolberg, roughly 1900 of them died at Breslau, and during the Battle of Königsberg (Kaliningrad), another 2400 members of the "Volkssturm" were killed. At other times along the western front particularly, "Volkssturm" troops would cast their arms aside and disappear into the chaos. Youthful ardor and fanaticism among Hitler Youth members fighting with the "Volkssturm" or an insatiable sense of duty from old men proved tragic sometimes. An example shared by historian Stephen Fritz is instructive in this case:

Not every "Volkssturm" unit was suicidal or apocalyptic in outlook as the war drew closer to its end. Many of them lost their enthusiasm for the fight when it became clear that the Allies had won, prompting them to lay down their weapons and surrender – they also feared being captured by Allied forces and tortured or executed as partisans. Duty to their communities and sparing them from horrors like at Bad Windsheim also played a part in their capitulation, as did self-preservation.

Their most extensive use was during the Battle of Berlin, where "Volkssturm" units fought in many parts of the city. This battle was particularly devastating to its formations, however; many members fought to the death out of fear of being captured by the Soviets, holding out to the very end, which was in keeping with their covenant. Nonetheless, a force of over 2.5 million Soviet troops, equipped with 6,250 tanks and over 40,000 artillery pieces were assigned to capture the city, and the diminished remnants of the "Wehrmacht" were no match for them. Meanwhile, Hitler denounced every perceived "betrayal" to the inhabitants of the "Führerbunker". Not eager to die what was thought to be a pointless death, many older members of the "Volkssturm" looked for places to hide from the approaching Soviet Army. Juxtaposed against the tragic image of Berlin holding out against all odds was the frequent exodus and capitulation of "Wehrmacht" soldiers and members of the "Volkssturm" in southern and western Germany.

In the Battle for Berlin, "Volkssturm" units were used by the German high command as a last-ditch attempt to defend Berlin. The "Volkssturm" had a strength of about 60,000 in the Berlin area formed into 92 battalions, of which about 30 battalions of "Volkssturm I" (those with some weapons) were sent to forward positions, while those of "Volkssturm II" (those without weapons) remained in the inner city. One of the few substantive fighting units left to defend Berlin was the LVI Panzer Corps, which occupied the southeastern sector of the town, whereas the remaining parts of the city were being defended by what remained of the SS, the "Volkssturm", and the Hitler Youth formations.

One notable and unusual "Volkssturm" unit in the Battle for Berlin was the 3/115 Siemensstadt Battalion. It comprised 770 men, mainly First World War veterans in their 50s who were reasonably fit factory workers, with experienced officers. Unlike most "Volkssturm" units it was quite well equipped and trained. It was formed into three rifle companies, a support company (with two infantry support guns, four infantry mortars and heavy machine guns), and a heavy weapons company (with four Soviet M-20 howitzers and a French De Bange 220 mm mortar). The battalion first engaged Soviet troops at Friedrichsfelde on 21 April and saw the heaviest fighting over the following two days. It held out until 2 May, by which time it was down to just 50 rifles and two light machine guns. The survivors fell back to join other "Volkssturm" units. 26 men from the battalion were awarded the Iron Cross. Allied bombing had reduced Berlin to rubble; meanwhile the final stand in Berlin dwindled to fighting against highly trained, battle-hardened Soviet troops on the brink of final victory, who viewed resistance fighters like the "Volkssturm" as terrorists in much the same way the Wehrmacht once had viewed potential partisans during Operation Barbarossa. Red Army soldiers called the Hitler Youth formations and members of the "Volkssturm" still fighting to the end in Berlin "totals" for being part of Germany's total mobilization effort.

While Iron Crosses were being handed out in places like Berlin, other cities and towns like Parchim and Mecklenburg witnessed old elites, acting as military commandants over the Hitler Youth and "Volkssturm", asserting themselves and demanding that the defensive fighting stop so as to spare lives and property. Despite their efforts, the last four months of the war were an exercise in futility for the "Volkssturm", and the Nazi leadership's insistence to continue the fight to the bitter end contributed to an additional 1.23 million (approximated) deaths, half of them German military personnel and the other half from the "Volkssturm". The figure put forward by the historian Stephen Fritz does not match the observations of Richard J. Evans, who reported 175,000 "Volkssturm" members killed when fighting the professional armies of the western Allies and the Soviet Union. Evans' figures are based on the members listed in the index cards and reported as killed, while Martin Sorge pointed out that this figure did not include the 30,000 listed as presumed missing or dead in a 1963 report.

Interrogated members of the "Volkssturm"—when questioned as to where the regular forces had gone—revealed that German soldiers surrendered to the Americans and British instead of the Red Army for fear of reprisals related to the atrocities they had committed in the Soviet Union.





Other nations:



</doc>
<doc id="8731" url="https://en.wikipedia.org/wiki?curid=8731" title="Director's cut">
Director's cut

A director's cut is an edited version of a film (or television episode, music video, commercial, or video game) that is supposed to represent the director's own approved edit. "Cut" explicitly refers to the process of film editing; in preparing a film for release, the director's cut is preceded by the assembly and rough editor's cut and usually followed by the final cut meant for the public film release.

Director's cuts of film are not generally released to the public, because on most films the director does not have a final cut privilege. Anybody with money invested in the film, such as the production company, distributors, and/or studio can impose changes that they think will make the film profit more at the box office. This sometimes means a happier ending or less ambiguity, or excluding scenes that would earn a more audience-restricting rating, but more often means that the film is simply shortened to provide more screenings per day.

With the rise of home video, the phrase became more generically used as a marketing term (including mediums such as comic books and music albums, neither of which actually have directors), and the most commonly seen form of director's cut is a cut where extra scenes and characters are added in, often making the director's cut considerably longer than the final cut.

Traditionally, the "director's cut" is not, by definition, the director's ideal or preferred cut. The editing process of a film is broken into stages: First is the assembly/rough cut, where all selected takes are put together in the order in which they should appear in the film. Next, the editor's cut is reduced from the rough cut; the editor may be guided by his own tastes or following notes from the director or producers. Eventually is the final cut, which actually gets released or broadcast. In between the editor's cut and the final cut can come any number of fine cuts, including the director's cut. The director's cut may include unsatisfactory takes, a preliminary soundtrack, a lack of desired pick-up shots etc., which the director would not like to be shown but uses as a placeholder until satisfactory replacements can be inserted. This is still how the term is used within the film industry, as well as commercials, television, and music videos.

The trend of releasing alternate cuts of films for artistic reasons became prominent in the 1970s; in 1974, the "director's cut" of "The Wild Bunch" was shown theatrically in Los Angeles to sold-out audiences. The theatrical release of the film had cut ten minutes to get an R-rating, but this cut was hailed as superior and has now become the definitive one. Other early examples include George Lucas's first two films being re-released following the success of "Star Wars", in cuts which more closely resembled his vision, or Peter Bogdanovich re-cutting "The Last Picture Show" several times. Charlie Chaplin also re-released all of his films in the 1970s, several of which were re-cut (Chaplin's re-release of "The Gold Rush" in the 1940s is almost certainly the earliest prominent example of a director's re-cut film being released to the public). A theatrical re-release of "Close Encounters of the Third Kind" used the phrase "Special Edition" to describe a cut which was closer to Spielberg's intent but had a compromised ending demanded by the studio. As the home video industry rose in the early 1980s, video releases of director's cuts were sometimes created for the small but dedicated cult fan market. Los Angeles cable station Z Channel is also cited as significant in the popularization of alternate cuts. Early examples of films released in this manner include Michael Cimino's "Heaven's Gate", where a longer cut was recalled from theaters but subsequently shown on cable and eventually released to home video; James Cameron's "Aliens", where a video release restored 20 minutes the studio had insisted on cutting; James Cameron's "The Abyss", where Cameron voluntarily made cuts to the theatrical version for pacing but restored them for a video release; Shim Hyung-rae's "Yonggary", where Shim made cuts to the theatrical version for how close to the American "Godzilla" it was and added in aliens and another monster, Cyker. That version was released stateside as "Reptilian" and, most famously, Ridley Scott's "Blade Runner", where an alternate workprint version was released to fan acclaim, ultimately resulting in the 1992 recut, the first film to use the term "Director's Cut" as a marketing description (and the first time it was used to describe a cut that the director was not involved in preparing).

Once distributors discovered that consumers would buy alternate versions of films, it became common for films to receive multiple releases. There is no standardization for labelling, leading to so-called "director's cuts" of films despite where the director prefers the theatrically released version, or when the director had actual final cut privilege. These were often assembled by simply restoring deleted scenes, sometimes adding as much as a half-hour to the length of the film without regard to pacing and storytelling.

As a result, the "director's cut" is often considered a mixed bag, with an equal share of supporters and detractors. Roger Ebert approved of the use of the label in unsuccessful films that had been tampered with by studio executives, such as Sergio Leone's original cut of "Once Upon a Time in America", and the moderately successful theatrical version of "Daredevil", which were altered by studio interference for their theatrical release. Other well-received director's cuts include Ridley Scott's "Kingdom of Heaven" (with "Empire" magazine stating: "The added 45 minutes in the Director’s Cut are like pieces missing from a beautiful but incomplete puzzle"), or Sam Peckinpah's "Pat Garrett and Billy the Kid", where the restored 115 minute cut is closer to the director's intent than the theatrical 105 minute cut (the actual director's cut was 122 minutes; it was never completed to Peckinpah's satisfaction, but was used as a guide for the restoration that was done after his death).

However, Ebert considers adding such material to a successful film a waste. Even Ridley Scott stated on the director's commentary track of "Alien" that the original theatrical release was his director's cut, and that the new version was released as a marketing ploy. Director Peter Bogdanovich, no stranger to director's cuts himself, cites "Red River" as an example where "MGM have a version of Howard Hawks's "Red River" that they're calling the Director's Cut and it is absolutely not the director's cut. It's a cut the director didn't want, an earlier cut that was junked. They assume because it was longer that it's a director's cut. Capra cut two reels off "Lost Horizon" because it didn't work and then someone tried to put it back. There are certainly mistakes and stupidities in reconstructing pictures." 

In rare instances, such as Peter Weir's "Picnic at Hanging Rock", John Cassavetes's "The Killing of a Chinese Bookie", and Blake Edwards's "Darling Lili", changes made to a director's cut result in a shorter, more compact cut. This generally happens when a distributor insists that a film be completed in order to meet a release date, but sometimes it is the result of removing scenes that the distributor insisted on inserting, as opposed to restoring scenes they insisted on cutting.

Another way that released director's cuts can be compromised is when directors were never allowed to even shoot their vision, and thus when the film is re-cut, they must make do with the footage that exists. Examples of this include Terry Zwigoff's "Bad Santa", Brian Helgeland's "Payback", and most notably the Richard Donner re-cut of "Superman II". Donner completed about 75% of the shooting of the sequel during the shooting of the first one but was fired from the project. of the film includes, among other things, screen test footage of stars Christopher Reeve and Margot Kidder, footage used in the first film, and entire scenes that were shot by replacement director Richard Lester which Donner dislikes but were required for story purposes.

Some directors explicitly dislike the phrase "director's cut" because it implies that they disapprove of the theatrically released cut. James Cameron and Peter Jackson are two directors who publicly reject the label, preferring "extended edition" or "special edition". While Jackson considers the theatrical releases of the "Lord of the Rings" and "Hobbit" trilogies to be a final "director's cut" within the constraints of theatrical exhibition, the extended cuts were produced so that fans of the material could see nearly all of the scenes shot for the script to develop more of J. R. R. Tolkien's world but that were originally cut for running time or other reasons. New music and special effects were also added to the cuts. Cameron specified "what I put into theaters is the Director's cut. Nothing was cut that I didn't want cut. All the extra scenes we've added back in are just a bonus for the fans." (though referring specifically to "Avatar", he has expressed similar feelings on all of his films besides "").

Special editions such as George Lucas's "Star Wars" films, and Steven Spielberg's "E.T. the Extra-Terrestrial", in which special effects are redone in addition to a new edit, have also caused controversy. ("See " Changes in "Star Wars" re-releases and "E.T. the Extra-Terrestrial: The 20th Anniversary").

Extended or special editions can also apply to films that have been extended for television or cut out to fill time slots and long advertisement breaks, against the explicit wishes of the director, such as the TV versions of "Dune" (1984), "The Warriors" (1979), "Superman" (1978) and the "Harry Potter" films. 

"" was released (March 25, 2016), an extended cut dubbed the "Ultimate Edition", which features 31 minutes of additional footage, was released digitally on June 28, 2016, and on Blu-ray on July 19, 2016. 

The film "Caligula" exists in at least 10 different officially released versions, ranging from a sub-90 minute television edit version of TV-14 (later TV-MA) for cable television to an unrated full pornographic version exceeding 3.5 hours. This is believed to be the most distinct versions of a single film. Among major studio films, the record is believed to be held by "Blade Runner"; the magazine "Video Watchdog" counted no less than seven distinct versions in a 1993 issue, before director Ridley Scott later released a "Final Cut" in 2007, bringing the supposed grand total to eight differing versions.

When released on DVD and Blu-Ray in 2019, "" featured an Extended Cut with 7 minutes of additional footage. This is the first time since "Harry Potter and the Chamber of Secrets", that a Harry Potter film has had one.

The music video for the 2006 Academy Award-nominated song "Listen", performed by Beyoncé, received a director's cut by Diane Martel. This version of the video was later included on Knowles' "B'Day Anthology Video Album" (2007). Linkin Park has a director's cut version for their music video "Faint" (directed by Mark Romanek) in which one of the band members spray paints the words "En Proceso" on a wall, as well as Hoobastank also having one for 2004's "The Reason" which omits the woman getting hit by the car. Britney Spears' music video for 2007's "Gimme More" was first released as a director's cut on iTunes, with the official video released 3 days later. Many other director's cut music videos contain sexual content that can't be shown on TV thus creating alternative scenes, such as Thirty Seconds to Mars's "Hurricane", and in some cases, alternative videos, such as in the case of Spears' 2008 video for "Womanizer".

As the trend became more widely recognized, the term "director's cut" became increasingly used as a colloquialism to refer to an expanded version of other things, including video games, music, and comic books. This confusing usage only served to further reduce the artistic value of a "director's cut", and it is currently rarely used in those ways.

For video games, these expanded versions, also referred as "complete editions", will have additions to the gameplay or additional game modes and features outside the main portion of the game. 

As is the case with certain high-profile Japanese-produced games, the game designers may take the liberty to revise their product for the overseas market with additional features during the localization process. These features are later added back to the native market in a re-release of a game in what is often referred as the international version of the game. This was the case with the overseas versions of "Final Fantasy VII", "Metal Gear Solid" and "Rogue Galaxy", which contained additional features (such as new difficulty settings for "Metal Gear Solid"), resulting in re-released versions of those respective games in Japan ("Final Fantasy VII International", "" and "Rogue Galaxy: Director's Cut"). In the case of "" and "", the American versions were released first, followed by the Japanese versions and then the European versions, with each regional release offering new content not found in the previous one. All of the added content from the Japanese and European versions of those games were included in the expanded editions titled "" and "".

They also, similar to movies, will occasionally include extra, uncensored or alternate versions of cutscenes, as was the case with "". In markets with strict censorship, a later relaxing of those laws occasional will result in the game being rereleased with the "Special/Uncut Edition" tag added to differentiate between the originally released censored version and the current uncensored edition.

Several of the "Pokémon" games have also received director's cuts and have used the term "extension," though "remake" and "third version" are also often used by many fans. These include "" (Japan only), "Pokémon Yellow" (for "Pokémon Red" and "Green"/"Blue"), "Pokémon Crystal" (for "Pokémon Gold" and "Silver"), "Pokémon Emerald" (for "Pokémon Ruby" and "Sapphire"), "Pokémon Platinum" (for ""Pokémon Diamond" and "Pearl""), Pokémon Black 2 and Pokémon White 2, and "Pokémon Ultra Sun and Ultra Moon".

"Director's cuts" in music are rarely released. A few exceptions include Guided by Voices' 1994 album "Bee Thousand", which was re-released as a three disc vinyl LP Director's cut in 2004, and Fall Out Boy's 2003 album "Take This to Your Grave", which was re-released as a Director's cut in 2005 with two extra tracks.

In 2011 British singer Kate Bush released the album titled "Director's Cut". It is made up of songs from her earlier albums "The Sensual World" and "The Red Shoes" which have been remixed and restructured, three of which were re-recorded completely.




</doc>
<doc id="8733" url="https://en.wikipedia.org/wiki?curid=8733" title="Digital video">
Digital video

Digital video is an electronic representation of moving visual images (video) in the form of encoded digital data. This is in contrast to analog video, which represents moving visual images with analog signals. Digital video comprises a series of digital images displayed in rapid succession.

Digital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard definition component video signal in digital form. In addition to uncompressed formats, popular compressed digital video formats today include H.264 and MPEG-4. Modern interconnect standards for digital video include HDMI, DisplayPort, Digital Visual Interface (DVI) and serial digital interface (SDI).

Digital video can be copied with no degradation in quality. In contrast, when analog sources are copied, they experience generation loss. Digital video can be stored on digital media such as Blu-ray Disc, on computer data storage or streamed over the Internet to end users who watch content on a desktop computer screen or a digital smart TV. In everyday practice, digital video content such as TV shows and movies also includes a digital audio soundtrack.

The basis for digital video cameras are metal-oxide-semiconductor (MOS) image sensors. The first practical semiconductor image sensor was the charge-coupled device (CCD), invented in 1969, based on MOS capacitor technology. Following the commercialization of CCD sensors during the late 1970s to early 1980s, the entertainment industry slowly began transitioning to digital imaging and digital video over the next two decades. The CCD was followed by the CMOS active-pixel sensor (CMOS sensor), developed in the 1990s.

The earliest forms of digital video coding began in the 1970s, with uncompressed pulse-code modulation (PCM) video, requiring high bitrates between 45140 Mbps for standard definition (SD) content. Practical digital video coding was eventually made possible with the discrete cosine transform (DCT), a form of lossy compression. DCT compression was first proposed by Nasir Ahmed in 1972, and then developed by Ahmed with T. Natarajan and K. R. Rao at the University of Texas in 1973. DCT would later become the standard for digital video compression since the late 1980s.

The first digital video coding standard was H.120, created by the CCITT (now ITU-T) in 1984. H.120 was not practical, due to weak performance. H.120 was based on differential pulse-code modulation (DPCM), a lossless compression algorithm that was inefficient for video coding. During the late 1980s, a number of companies began experimenting with DCT, a much more efficient form of compression for video coding. The CCITT received 14 proposals for DCT-based video compression formats, in contrast to a single proposal based on vector quantization (VQ) compression. The H.261 standard was developed based on DCT compression. H.261 was the first practical video coding standard. Since H.261, DCT compression has been adopted by all the major video coding standards that followed.

MPEG-1, developed by the Motion Picture Experts Group (MPEG), followed in 1991, and it was designed to compress VHS-quality video. It was succeeded in 1994 by MPEG-2/H.262, which became the standard video format for DVD and SD digital television. It was followed by MPEG-4/H.263 in 1999, and then in 2003 it was followed by H.264/MPEG-4 AVC, which has become the most widely used video coding standard.

Starting in the late 1970s to the early 1980s, several types of video production equipment that were digital in their internal workings were introduced. These included time base correctors (TBC) and digital video effects (DVE) units. They operated by taking a standard analog composite video input and digitizing it internally. This made it easier to either correct or enhance the video signal, as in the case of a TBC, or to manipulate and add effects to the video, in the case of a DVE unit. The digitized and processed video information was then converted back to standard analog video for output.

Later on in the 1970s, manufacturers of professional video broadcast equipment, such as Bosch (through their Fernseh division) and Ampex developed prototype digital videotape recorders (VTR) in their research and development labs. Bosch's machine used a modified 1 inch type B videotape transport, and recorded an early form of CCIR 601 digital video. Ampex's prototype digital video recorder used a modified 2 inch Quadruplex videotape VTR (an Ampex AVR-3), but fitted with custom digital video electronics, and a special "octaplex" 8-head headwheel (regular analog 2" Quad machines only used 4 heads). Like standard 2" Quad, the audio on the Ampex prototype digital machine, nicknamed by its developers as "Annie", still recorded the audio in analog as linear tracks on the tape. None of these machines from these manufacturers were ever marketed commercially.

Digital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard definition component video signal in digital form. Component video connections required 3 cables and most television facilities were wired for composite NTSC or PAL video using one cable. Due this incompatibility and also due to the cost of the recorder, D1 was used primarily by large television networks and other component-video capable video studios.

In 1988, Sony and Ampex co-developed and released the D2 digital videocassette format, which recorded video digitally without compression in ITU-601 format, much like D1. But D2 had the major difference of encoding the video in composite form to the NTSC standard, thereby only requiring single-cable composite video connections to and from a D2 VCR, making it a perfect fit for the majority of television facilities at the time. D2 was a successful format in the television broadcast industry throughout the late '80s and the '90s. D2 was also widely used in that era as the master tape format for mastering laserdiscs.

D1 & D2 would eventually be replaced by cheaper systems using video compression, most notably Sony's Digital Betacam that were introduced into the network's television studios. Other examples of digital video formats utilizing compression were Ampex's DCT (the first to employ such when introduced in 1992), the industry-standard DV and MiniDV and its professional variations, Sony's DVCAM and Panasonic's DVCPRO, and Betacam SX, a lower-cost variant of Digital Betacam using MPEG-2 compression.

One of the first digital video products to run on personal computers was "PACo: The PICS Animation Compiler" from The Company of Science & Art in Providence, RI, which was developed starting in 1990 and first shipped in May 1991. PACo could stream unlimited-length video with synchronized sound from a single file (with the ".CAV" file extension) on CD-ROM. Creation required a Mac; playback was possible on Macs, PCs, and Sun SPARCstations.

QuickTime, Apple Computer's multimedia framework appeared in June 1991. Audio Video Interleave from Microsoft followed in 1992. Initial consumer-level content creation tools were crude, requiring an analog video source to be digitized to a computer-readable format. While low-quality at first, consumer digital video increased rapidly in quality, first with the introduction of playback standards such as MPEG-1 and MPEG-2 (adopted for use in television transmission and DVD media), and then the introduction of the DV tape format allowing recordings in the format to be transferred direct to digital video files using a FireWire port on an editing computer. This simplified the process, allowing non-linear editing systems (NLE) to be deployed cheaply and widely on desktop computers with no external playback or recording equipment needed.

The widespread adoption of digital video and accompanying compression formats has reduced the bandwidth needed for a high-definition video signal (with HDV and AVCHD, as well as several commercial variants such as DVCPRO-HD, all using less bandwidth than a standard definition analog signal). These savings have increased the number of channels available on cable television and direct broadcast satellite systems, created opportunities for spectrum reallocation of terrestrial television broadcast frequencies, made tapeless camcorders based on flash memory possible among other innovations and efficiencies.

Digital video comprises a series of digital images displayed in rapid succession. In the context of video these images are called frames. The rate at which frames are displayed is known as the frame rate and is measured in frames per second (FPS). Every frame is an orthogonal bitmap digital image and so comprises a raster of pixels. Pixels have only one property, their color. The color of a pixel is represented by a fixed number of bits. The more bits the more subtle variations of colors can be reproduced. This is called the color depth of the video.

In interlaced video each "frame" is composed of two halves of an image. The first half contains only the odd-numbered lines of a full frame. The second half contains only the even-numbered lines. Those halves are referred to individually as "fields". Two consecutive fields compose a full frame. If an interlaced video has a frame rate of 30 frames per second the field rate is 60 fields per second. All the properties discussed here apply equally to interlaced video but one should be careful not to confuse the fields-per-second rate with the frames-per-second rate.

By its definition, bit rate is a measure of the rate of information content of the digital video stream. In the case of uncompressed video, bit rate corresponds directly to the quality of the video as bit rate is proportional to every property that affects the video quality. Bit rate is an important property when transmitting video because the transmission link must be capable of supporting that bit rate. Bit rate is also important when dealing with the storage of video because, as shown above, the video size is proportional to the bit rate and the duration. Video compression is used to greatly reduce the bit rate while having a lesser effect on quality. 

Bits per pixel (BPP) is a measure of the efficiency of compression. A true-color video with no compression at all may have a BPP of 24 bits/pixel. Chroma subsampling can reduce the BPP to 16 or 12 bits/pixel. Applying jpeg compression on every frame can reduce the BPP to 8 or even 1 bits/pixel. Applying video compression algorithms like MPEG1, MPEG2 or MPEG4 allows for fractional BPP values.

BPP represents the "average" bits per pixel. There are compression algorithms that keep the BPP almost constant throughout the entire duration of the video. In this case, we also get video output with a constant bitrate (CBR). This CBR video is suitable for real-time, non-buffered, fixed bandwidth video streaming (e.g. in videoconferencing). As not all frames can be compressed at the same level, because quality is more severely impacted for scenes of high complexity, some algorithms try to constantly adjust the BPP. They keep it high while compressing complex scenes and low for less demanding scenes. This way, one gets the best quality at the smallest average bit rate (and the smallest file size, accordingly). This method produces a variable bitrate because it tracks the variations of the BPP.

Standard film stocks typically record at 24 frames per second. For video, there are two frame rate standards: NTSC, at 30/1.001 (about 29.97) frames per second (about 59.94 fields per second), and PAL, 25 frames per second (50 fields per second). Digital video cameras come in two different image capture formats: interlaced and progressive scan. Interlaced cameras record the image in alternating sets of lines: the odd-numbered lines are scanned, and then the even-numbered lines are scanned, then the odd-numbered lines are scanned again, and so on. One set of odd or even lines is referred to as a "field", and a consecutive pairing of two fields of opposite parity is called a "frame". Progressive scan cameras record all lines in each frame as a single unit. Thus, interlaced video captures samples the scene motion twice as often as progressive video does, for the same frame rate. Progressive-scan generally produces a slightly sharper image. However, motion may not be as smooth as interlaced video.

Digital video can be copied with no generation loss which degrades quality in analog systems. However a change in parameters like frame size or a change of the digital format can decrease the quality of the video due to image scaling and transcoding losses. Digital video can be manipulated and edited on a non-linear editing systems frequently implemented using commodity computer hardware and software.

Digital video has a significantly lower cost than 35 mm film. In comparison to the high cost of film stock, the digital media used for digital video recording, such as flash memory or hard disk drive, used for recording digital video is very inexpensive. Digital video also allows footage to be viewed on location without the expensive and time-consuming chemical processing required by film. Network transfer of digital video makes physical deliveries of tapes and film reels unnecessary. 

Digital television (including higher quality HDTV) was introduced in most developed countries in early 2000s. Digital video is used in modern mobile phones and video conferencing systems. Digital video is used for Internet distribution of media, including streaming video and peer-to-peer movie distribution. 

Many types of video compression exist for serving digital video over the internet and on optical disks. The file sizes of digital video used for professional editing are generally not practical for these purposes, and the video requires further compression with codecs.

, the highest resolution demonstrated for digital video generation is 35 megapixels (8192 x 4320). The highest speed is attained in industrial and scientific high speed cameras that are capable of filming 1024x1024 video at up to 1 million frames per second for brief periods of recording.

Live digital video consumes bandwidth. Recorded digital video consumes data storage. The amount of bandwidth or storage required is determined by the frame size, color depth and frame rate. Each pixel consumes a number of bits determined by the color depth. The data required to represent a frame of data is determined by multiplying by the number of pixels in the image. The bandwidth is determined by multiplying the storage requirement for a frame by the frame rate. The overall storage requirements for a program can then be determined by multiplying bandwidth by the duration of the program.

These calculations are accurate for uncompressed video but because of the relatively high bit rate of uncompressed video, video compression is extensively used. In the case of compressed video, each frame requires a small percentage of the original bits. Note that it is not necessary that all frames are equally compressed by the same percentage. In practice, they are not so it is useful to consider the "average" factor of compression for "all" the frames taken together.

Purpose-built digital video interfaces

General-purpose interfaces use to carry digital video

The following interface has been designed for carrying MPEG-Transport compressed video:

Compressed video is also carried using UDP-IP over Ethernet. Two approaches exist for this:

Other methods of carrying video over IP






</doc>
<doc id="8735" url="https://en.wikipedia.org/wiki?curid=8735" title="BIND">
BIND

BIND (, or named (pronounced "name-dee": , short for "name daemon"), is an implementation of the Domain Name System (DNS) of the Internet. It performs both of the main DNS server roles, acting as an authoritative name server for domains, and acting as a recursive resolver in the network. As of 2015, it is the most widely used domain name server software, and is the "de facto" standard on Unix-like operating systems.

The software was originally designed at the University of California, Berkeley (UCB) in the early 1980s. The name originates as an acronym of "Berkeley Internet Name Domain", reflecting the application's use within UCB. The software consists, most prominently, of the DNS server component, called "named", a contracted form of "name daemon". In addition, the suite contains various administration tools, and a DNS resolver interface library. The latest version of BIND is BIND 9, first released in 2000. BIND 9 is actively maintained, with new releases issued several times a year.

Starting in 2009, the Internet Software Consortium (ISC) developed a software suite, initially called BIND10. With release version 1.2.0 the project was renamed "Bundy" to terminate ISC involvement in the project.

BIND 9 is intended to be fully compliant with the IETF DNS standards and draft standards. Important features of BIND 9 include: TSIG, nsupdate, IPv6, RNDC (remote name daemon control), views, multiprocessor support, Response Rate Limiting (RRL), DNSSEC, and broad portability. RNDC enables remote configuration updates, using a shared secret to provide encryption for local and remote terminals during each session.

While earlier versions of BIND offered no mechanism to store and retrieve zone data in anything other than flat text files, in 2007 BIND 9.4 DLZ provided a compile-time option for zone storage in a variety of database formats including LDAP, Berkeley DB, PostgreSQL, MySQL, and ODBC.

BIND 10 planned to make the data store modular, so that a variety of databases may be connected.
In 2016 ISC added support for the 'dyndb' interface, contributed by RedHat, with BIND version 9.11.0. 

Security issues that are discovered in BIND 9 are patched and publicly disclosed in keeping with common principles of open source software. A complete list of security defects that have been discovered and disclosed in BIND9 is maintained by Internet Systems Consortium, the current authors of the software.

The BIND 4 and BIND 8 releases both had serious security vulnerabilities. Use of these ancient versions, or any un-maintained, non-supported version is strongly discouraged. BIND 9 was a complete rewrite, in part to mitigate these ongoing security issues. The downloads page on the ISC web site clearly shows which versions are currently maintained and which are end of life.

Originally written by four graduate students at the Computer Systems Research Group at the University of California, Berkeley (UCB), BIND was first released with Berkeley Software Distribution 4.3BSD. Paul Vixie started maintaining it in 1988 while working for Digital Equipment Corporation. , the Internet Systems Consortium maintains, updates, and writes new versions of BIND.

BIND was written by Douglas Terry, Mark Painter, David Riggle and Songnian Zhou in the early 1980s at the University of California, Berkeley as a result of a DARPA grant. The acronym "BIND" is for "Berkeley Internet Name Domain", from a technical paper published in 1984.

Versions of BIND through 4.8.3 were maintained by the Computer Systems Research Group (CSRG) at UC Berkeley.

In the mid-1980s, Paul Vixie of DEC took over BIND development, releasing versions 4.9 and 4.9.1. Vixie continued to work on BIND after leaving DEC. BIND Version 4.9.2 was sponsored by Vixie Enterprises. Vixie eventually founded the ISC, which became the entity responsible for BIND versions starting with 4.9.3.

BIND 8 was released by ISC in May 1997.

Version 9 was developed by Nominum, Inc. under an ISC outsourcing contract, and the first version was released 9 October 2000. It was written from scratch in part to address the architectural difficulties with auditing the earlier BIND code bases, and also to support DNSSEC (DNS Security Extensions). The development of BIND 9 took place under a combination of commercial and military contracts. Most of the features of BIND 9 were funded by UNIX vendors who wanted to ensure that BIND stayed competitive with Microsoft's DNS offerings; the DNSSEC features were funded by the US military, which regarded DNS security as important. BIND 9 was released in September 2000.

In 2009, ISC started an effort to develop a new version of the software suite, called BIND10. In addition to DNS service, the BIND10 suite also included IPv4 and IPv6 DHCP server components. In April 2014, with the BIND10 release 1.2.0 the ISC concluded its development work of the project and renamed the project to "Bundy", moving the source code repository to GitHub for further development by outside public efforts.. ISC discontinued its involvement in the project due to cost-cutting measures. The development of DHCP components was split off to become a new Kea project.





</doc>
<doc id="8736" url="https://en.wikipedia.org/wiki?curid=8736" title="Djbdns">
Djbdns

The djbdns software package is a DNS implementation. It was created by Daniel J. Bernstein in response to his frustrations with repeated security holes in the widely used BIND DNS software. As a challenge, Bernstein offered a $1000 prize for the first person to find a security hole in djbdns, which was awarded in March 2009 to Matthew Dempsky.

, djbdns's tinydns component was the second most popular DNS server in terms of the number of domains for which it was the authoritative server, and third most popular in terms of the number of DNS hosts running it.

djbdns has never been vulnerable to the widespread cache poisoning vulnerability reported in July 2008, but it has been discovered that it is vulnerable to a related attack.

The source code has not been centrally managed since its release in 2001, and was released into the public domain in 2007. As of March 2009, there are a number of forks, one of which is dbndns (part of the Debian Project), and more than a dozen patches to modify the released version.

While djbdns does not directly support DNSSEC, there are third party patches to add DNSSEC support to djbdns' authoritative-only tinydns component.

The djbdns software consists of servers, clients, and miscellaneous configuration tools.



In djbdns, different features and services are split off into separate programs. For example, zone transfers, zone file parsing, caching, and recursive resolving are implemented as separate programs. The result of these design decisions is a reduction in code size and complexity of the daemon program that provides the core function of answering lookup requests. Bernstein asserts that this is true to the spirit of the Unix operating system, and makes security verification much simpler.

On December 28, 2007, Bernstein released djbdns into the public domain. Previously the package was distributed free of charge as license-free software. However this did not permit the distribution of modified versions of djbdns, which was one of the core principles of open-source software. Consequently, it was not included in Linux distributions which required all components to be open-source.




</doc>
<doc id="8741" url="https://en.wikipedia.org/wiki?curid=8741" title="Dylan (programming language)">
Dylan (programming language)

Dylan is a multi-paradigm programming language that includes support for functional and object-oriented programming (OOP), and is dynamic and reflective while providing a programming model designed to support generating efficient machine code, including fine-grained control over dynamic and static behaviors. It was created in the early 1990s by a group led by Apple Computer.

A concise and thorough overview of the language may be found in the Dylan Reference Manual.

Dylan derives from Scheme and Common Lisp and adds an integrated object system derived from the Common Lisp Object System (CLOS). In Dylan, all values (including numbers, characters, functions, and classes) are first-class objects. Dylan supports multiple inheritance, polymorphism, multiple dispatch, keyword arguments, object introspection, pattern-based syntax extension macros, and many other advanced features. Programs can express fine-grained control over dynamism, admitting programs that occupy a continuum between dynamic and static programming and supporting evolutionary development (allowing for rapid prototyping followed by incremental refinement and optimization).

Dylan's main design goal is to be a dynamic language well-suited for developing commercial software. Dylan attempts to address potential performance issues by introducing "natural" limits to the full flexibility of Lisp systems, allowing the compiler to clearly understand compilable units, such as libraries.

Dylan derives much of its semantics from Scheme and other Lisps; some Dylan implementations were initially built within extant Lisp systems. However, Dylan has an ALGOL-like syntax instead of a Lisp-like prefix syntax.

Dylan was created in the early 1990s by a group led by Apple Computer. At one time in its development, it was intended for use with the Apple Newton computer, but the Dylan implementation did not reach sufficient maturity in time, and Newton instead used a mix of C and the NewtonScript developed by Walter Smith. Apple ended their Dylan development effort in 1995, though they made a "technology release" version available (Apple Dylan TR1) that included an advanced integrated development environment (IDE).

Two other groups contributed to the design of the language and developed implementations: Harlequin released a commercial IDE for Microsoft Windows and Carnegie Mellon University released an open source compiler for Unix systems called Gwydion Dylan. Both of these implementations are now open source. The Harlequin implementation is now named Open Dylan and is maintained by a group of volunteers, the Dylan Hackers.

The Dylan language was code-named Ralph. James Joaquin chose the name Dylan for "DYnamic LANguage."

Many of Dylan's syntax features come from its Lisp heritage. Originally, Dylan used a Lisp-like prefix syntax, which was based on s-expressions. By the time the language design was completed, the syntax was changed to an ALGOL-like syntax, with the expectation that it would be more familiar to a wider audience of programmers. The syntax was designed by Michael Kahl. It is described in great detail in the Dylan Reference Manual.

Dylan is not case sensitive. Dylan's lexical syntax allows the use of a naming convention where hyphen-minus signs are used to connect the parts of multiple-word identifiers (sometimes called "lisp-case" or "kebab case"). This convention is common in Lisp languages but cannot be used in programming languages that treat any hyphen-minus that is not part of a numeric literal as a single lexical token, even when not surrounded by whitespace characters.

Besides alphanumeric characters and hyphen-minus signs, Dylan allows certain non-alphanumerical characters as part of identifiers. Identifiers may not consist of these non-alphanumeric characters or of numeric characters alone. If there is any ambiguity, whitespace is used.

A simple class with several slots:
define class <point> (<object>)
end class <point>;
By convention, classes are named with less-than and greater-than signs used as angle brackets, e.g. the class named codice_1 in the code example.

In codice_2 both codice_3 and codice_1 are optional. This is true for all codice_5 clauses. For example, you may write codice_6 or just codice_5 to terminate an codice_8 statement.

The same class, rewritten in the most minimal way possible:
define class <point> (<object>)
end;
The slots are now both typed as codice_9. The slots must be initialized manually.

By convention, constant names begin with "$":
define constant $pi :: <double-float> = 3.1415927d0;
A factorial function:

define function factorial (n :: <integer>) => (n! :: <integer>)
end;
Here, codice_10 and codice_11 are just normal identifiers.

There is no explicit return statement. The result of a method or function is the last expression evaluated. It is a common style to leave off the semicolon after an expression in return position.

In many object-oriented languages, classes are the main means of encapsulation and modularity; each class defines a namespace and controls which definitions are externally visible. Further, classes in many languages define an indivisible unit that must be used as a whole. For example, using a codice_12 concatenation function requires importing and compiling against all of codice_12.

Some languages, including Dylan, also include a separate, explicit namespace or module system that performs encapsulation in a more general way.

In Dylan, the concepts of compile-unit and import-unit are separated, and classes have nothing specifically to do with either. A "library" defines items that should be compiled and handled together, while a "module" defines a namespace. Classes can be placed together in modules, or cut across them, as the programmer wishes. Often the complete definition for a class does not exist in a single module, but is spread across several that are optionally collected together. Different programs can have different definitions of the same class, including only what they need.

For example, consider an add-on library for regex support on codice_12. In some languages, for the functionality to be included in strings, the functionality must be added to the codice_12 namespace. As soon as this occurs, the codice_12 class becomes larger, and functions that don't need to use regex still must "pay" for it in increased library size. For this reason, these sorts of add-ons are typically placed in their own namespaces and objects. The downside to this approach is that the new functions are no longer a "part of" codice_12; instead, it is isolated in its own set of functions that must be called separately. Instead of codice_18, which would be the natural organization from an OO viewpoint, something like codice_19 is used, which effectively reverses the ordering.

Under Dylan, many interfaces can be defined for the same code, for instance the String concatenation method could be placed in both the String interface, and the "concat" interface which collects together all of the different concatenation functions from various classes. This is more commonly used in math libraries, where functions tend to be applicable to widely differing object types.

A more practical use of the interface construct is to build public and private versions of a module, something that other languages include as a "bolt on" feature that invariably causes problems and adds syntax. Under Dylan, every function call can be simply places in the "Private" or "Development" interface, and collect up publicly accessible functions in codice_20. Under Java or C++ the visibility of an object is defined in the code, meaning that to support a similar change, a programmer would be forced to rewrite the definitions fully, and could not have two versions at the same time.

Classes in Dylan describe codice_21 (data members, fields, ivars, etc.) of objects in a fashion similar to most OO languages. All access to slots are via methods, as in Smalltalk. Default getter and setter methods are automatically generated based on the slot names. In contrast with most other OO languages, other methods applicable to the class are often defined outside of the class, and thus class definitions in Dylan typically include the definition of the storage only. For instance:
define class <window> (<view>)
end class;
In this example, the class "codice_22" is defined. The <class name> syntax is convention only, to make the class names stand out—the angle brackets are merely part of the class name. In contrast, in some languages the convention is to capitalize the first letter of the class name or to prefix the name with a "C" or "T" (for example). codice_22 inherits from a single class, codice_24, and contains two slots, codice_25 holding a string for the window title, and codice_26 holding an X-Y point for a corner of the window. In this example, the title has been given a default value, while the position has not. The optional "init-keyword" syntax allows the programmer to specify the initial value of the slot when instantiating an object of the class.

In languages such as C++ or Java, the class would also define its interface. In this case the definition above has no explicit instructions, so in both languages access to the slots and methods is considered codice_27, meaning they can be used only by subclasses. To allow unrelated code to use the window instances, they must be declared codice_28.

In Dylan, these sorts of visibility rules are not considered part of the code, but of the module/interface system. This adds considerable flexibility. For instance, one interface used during early development could declare everything public, whereas one used in testing and deployment could limit this. With C++ or Java these changes would require changes to the source code, so people won't do it, whereas in Dylan this is a fully unrelated concept.

Although this example does not use it, Dylan also supports multiple inheritance.

In Dylan, methods are not intrinsically associated with any specific class; methods can be thought of as existing outside of classes. Like CLOS, Dylan is based on multiple dispatch (multimethods), where the specific method to be called is chosen based on the types of all its arguments. The method need not be known at compile time, the understanding being that the required function may be available, or not, based on a user's preferences.

Under Java the same methods would be isolated in a specific class. To use that functionality the programmer is forced to "import" that class and refer to it explicitly to call the method. If that class is unavailable, or unknown at compile time, the application simply won't compile.

In Dylan, code is isolated from storage in "functions". Many classes have methods that call their own functions, thereby looking and feeling like most other OO languages. However code may also be located in "generic functions", meaning they are not attached to a specific class, and can be called natively by anyone. Linking a specific generic function to a method in a class is accomplished thusly:
define method turn-blue (w :: <window>)
end method;
This definition is similar to those in other languages, and would likely be encapsulated within the codice_22 class. Note the := setter call, which is syntactic sugar for codice_30.

The utility of generic methods comes into its own when you consider more "generic" examples. For instance, one common function in most languages is the codice_31, which returns some human-readable form for the object. For instance, a window might return its title and its position in parens, while a string would return itself. In Dylan these methods could all be collected into a single module called "codice_31", thereby removing this code from the definition of the class itself. If a specific object did not support a codice_31, it could be easily added in the codice_31 module.

This whole concept might strike some readers as very odd. The code to handle codice_31 for a window isn't defined in codice_22? This might not make any sense until you consider how Dylan handles the call of the codice_31. In most languages when the program is compiled the codice_31 for codice_22 is looked up and replaced with a pointer (more or less) to the method. In Dylan this occurs when the program is first run; the runtime builds a table of method-name/parameters details and looks up methods dynamically via this table. That means that a function for a specific method can be located anywhere, not just in the compile-time unit. In the end the programmer is given considerable flexibility in terms of where to place their code, collecting it along class lines where appropriate, and functional lines where it's not.

The implication here is that a programmer can add functionality to existing classes by defining functions in a separate file. For instance, you might wish to add spell checking to all codice_40s, which in most languages would require access to the source code of the string class—and such basic classes are rarely given out in source form. In Dylan (and other "extensible languages") the spell checking method could be added in the codice_41 module, defining all of the classes on which it can be applied via the codice_42 construct. In this case the actual functionality might be defined in a single generic function, which takes a string and returns the errors. When the codice_41 module is compiled into your program, all strings (and other objects) will get the added functionality.

Apple Dylan is the implementation of Dylan produced by Apple Computer. It was originally developed for the Apple Newton product.



</doc>
<doc id="8742" url="https://en.wikipedia.org/wiki?curid=8742" title="Dublin Core">
Dublin Core

The Dublin Core™, also known as the Dublin Core™ Metadata Element Set, is a set of fifteen "core" elements (properties) for describing resources. This fifteen-element Dublin Core™ has been formally standardized as ISO 15836, ANSI/NISO Z39.85, and IETF RFC 5013. The core properties are part of a larger set of DCMI Metadata Terms. "Dublin Core™" is also used as an adjective for Dublin Core™ metadata, a style of metadata that draws on multiple RDF vocabularies, packaged and constrained in Dublin Core™ application profiles.

The resources described using the Dublin Core may be digital resources (video, images, web pages, etc), as well as physical resources such as books or CDs, and objects like artworks.Dublin Core metadata may be used for multiple purposes, from simple resource description to combining metadata vocabularies of different metadata standards, to providing interoperability for metadata vocabularies in the linked data cloud and Semantic Web implementations.

"Dublin" refers to Dublin, Ohio, USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop, hosted by the OCLC (known at that time as Online Computer Library Center), a library consortium based in Dublin, and the National Center for Supercomputing Applications (NCSA). "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources". The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from librarianship, computer science, text encoding, museums, and other related fields of scholarship and practice.

Starting in 2000, the Dublin Core community focused on "application profiles" – the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium's work on a generic data model for metadata, the Resource Description Framework (RDF), was maturing. As part of an extended set of DCMI metadata terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the linked data movement.

The Dublin Core Metadata Initiative (DCMI) provides an open forum for the development of interoperable online metadata standards for a broad range of purposes and of business models. DCMI's activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.

Currently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.

The Dublin Core standard originally included two levels: Simple and Qualified. "Simple Dublin Core" comprised 15 elements; "Qualified Dublin Core" included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.

Since 2012, the two have been incorporated into the "DCMI Metadata Terms" as a single set of terms using the RDF data model. The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.

The original DCMES Version 1.1 consists of 15 metadata elements, defined this way in the original specification:

Each Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became a NISO standards, Z39.85, and IETF RFC 5013 in 2007, ISO 15836 standard in 2009 and is used as a base-level data element set for the description of learning resources in the ISO/IEC 19788-2 Metadata for learning resources (MLR) – Part 2: Dublin Core elements, prepared by the ISO/IEC JTC1 SC36.

Full information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.

On the "archive form" web page for WebCite it says, in part: "Metadata (optional): These are Dublin Core elements. [...]".

(Superseded in 2008 by the DCMI Metadata Terms.) Subsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the DCMES was begun. The additional terms were identified, generally in working groups of the DCMI, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.

Element refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the "Dumb-Down Principle", states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.

In addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to a "human reader".

Audience, Provenance and RightsHolder are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core. DCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.

The DCMI Metadata Terms lists the current set of the Dublin Core vocabulary. This set includes the fifteen terms of the DCMES (in "italic"), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as RDF properties.

Syntax choices for metadata expressed with the Dublin Core elements depend on context. Dublin Core concepts and semantics are designed to be syntax independent and apply to a variety of contexts, as long as the metadata is in a form suitable for interpretation by both machines and people.

The Dublin Core Abstract Model provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model helps implementers get a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntaxes.

One Document Type Definition based on Dublin Core is the Open Source Metadata Framework (OMF) specification. OMF is in turn used by Rarian (superseding ScrollKeeper), which is used by the GNOME desktop and KDE help browsers and the ScrollServer documentation server.

PBCore is also based on Dublin Core. The Zope CMF's Metadata products, used by the Plone, ERP5, the Nuxeo CPS Content management systems, SimpleDL, and Fedora Commons also implement Dublin Core. The EPUB e-book format uses Dublin Core metadata in the OPF file.

The Australian Government Locator Service (AGLS) metadata standard is an application profile of Dublin Core.






</doc>
<doc id="8743" url="https://en.wikipedia.org/wiki?curid=8743" title="Document Object Model">
Document Object Model

The Document Object Model (DOM) is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure where in each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. Nodes can have event handlers attached to them. Once an event is triggered, the event handlers get executed.

The principal standardization of the DOM was handled by the World Wide Web Consortium, which last developed a recommendation in 2004. WHATWG took over development of the standard, publishing it as a living document. The W3C now publishes stable snapshots of the WHATWG standard.

The history of the Document Object Model is intertwined with the history of the "browser wars" of the late 1990s between Netscape Navigator and Microsoft Internet Explorer, as well as with that of JavaScript and JScript, the first scripting languages to be widely implemented in the JavaScript engines of web browsers.

JavaScript was released by Netscape Communications in 1995 within Netscape Navigator 2.0. Netscape's competitor, Microsoft, released Internet Explorer 3.0 the following year with a reimplementation of JavaScript called JScript. JavaScript and JScript let web developers create web pages with client-side interactivity. The limited facilities for detecting user-generated events and modifying the HTML document in the first generation of these languages eventually became known as "DOM Level 0" or "Legacy DOM." No independent standard was developed for DOM Level 0, but it was partly described in the specifications for HTML 4.

Legacy DOM was limited in the kinds of elements that could be accessed. Form, link and image elements could be referenced with a hierarchical name that began with the root document object. A hierarchical name could make use of either the names or the sequential index of the traversed elements. For example, a form input element could be accessed as either codice_1 or codice_2.

The Legacy DOM enabled client-side form validation and the popular "rollover" effect.

In 1997, Netscape and Microsoft released version 4.0 of Netscape Navigator and Internet Explorer respectively, adding support for Dynamic HTML (DHTML) functionality enabling changes to a loaded HTML document. DHTML required extensions to the rudimentary document object that was available in the Legacy DOM implementations. Although the Legacy DOM implementations were largely compatible since JScript was based on JavaScript, the DHTML DOM extensions were developed in parallel by each browser maker and remained incompatible. These versions of the DOM became known as the "Intermediate DOM."

After the standardization of ECMAScript, the W3C DOM Working Group began drafting a standard DOM specification. The completed specification, known as "DOM Level 1", became a W3C Recommendation in late 1998. By 2005, large parts of W3C DOM were well-supported by common ECMAScript-enabled browsers, including Microsoft Internet Explorer version 6 (from 2001), Opera, Safari and Gecko-based browsers (like Mozilla, Firefox, SeaMonkey and Camino).

The W3C DOM Working Group published its final recommendation and subsequently disbanded in 2004. Development efforts migrated to the WHATWG, which continues to maintain a living standard. In 2009, the Web Applications group reorganized DOM activities at the W3C. In 2013, due to a lack of progress and the impending release of HTML5, the DOM Level 4 specification was reassigned to the HTML Working Group to expedite its completion. Meanwhile, in 2015, the Web Applications group was disbanded and DOM stewardship passed to the Web Platform group. Beginning with the publication of DOM Level 4 in 2015, the W3C creates new recommendations based on snapshots of the WHATWG standard.


To render a document such as a HTML page, most web browsers use an internal model similar to the DOM. The nodes of every document are organized in a tree structure, called the "DOM tree", with the topmost node named as "Document object". When an HTML page is rendered in browsers, the browser downloads the HTML into local memory and automatically parses it to display the page on screen.

When a web page is loaded, the browser creates a Document Object Model of the page, which is an object oriented representation of an HTML document that acts as an interface between JavaScript and the document itself. This allows the creation of dynamic web pages, because within a page JavaScript can:


Because the DOM supports navigation in any direction (e.g., parent and previous sibling) and allows for arbitrary modifications, an implementation must at least buffer the document that has been read so far (or some parsed form of it).

Web browsers rely on layout engines to parse HTML into a DOM. Some layout engines, such as Trident/MSHTML, are associated primarily or exclusively with a particular browser, such as Internet Explorer. Others, including Blink, WebKit, and Gecko, are shared by a number of browsers, such as Google Chrome, Opera, Safari, and Firefox. The different layout engines implement the DOM standards to varying degrees of compliance.

DOM implementations:


APIs that expose DOM implementations:


Inspection tools:





</doc>
<doc id="8745" url="https://en.wikipedia.org/wiki?curid=8745" title="Design pattern">
Design pattern

A design pattern is the re-usable form of a solution to a design problem. The idea was introduced by the architect Christopher Alexander and has been adapted for various other disciplines, notably software engineering.

An organized collection of design patterns that relate to a particular field is called a pattern language. This language gives a common terminology for discussing the situations designers are faced with.

Documenting a pattern requires explaining why a particular situation causes problems, and how the components of the pattern relate to each other to give the solution. Christopher Alexander describes common design problems as arising from "conflicting forces" — such as the conflict between wanting a room to be sunny and wanting it not to overheat on summer afternoons. A pattern would not tell the designer how many windows to put in the room; instead, it would propose a set of values to guide the designer toward a decision that is best for their particular application. Alexander, for example, suggests that enough windows should be included to direct light all around the room. He considers this a good solution because he believes it increases the enjoyment of the room by its occupants. Other authors might come to different conclusions, if they place higher value on heating costs, or material costs. These values, used by the pattern's author to determine which solution is "best", must also be documented within the pattern.

Pattern documentation should also explain when it is applicable. Since two houses may be very different from one another, a design pattern for houses must be broad enough to apply to both of them, but not so vague that it doesn't help the designer make decisions. The range of situations in which a pattern can be used is called its context. Some examples might be "all houses", "all two-story houses", or "all places where people spend time".

For instance, in Christopher Alexander's work, bus stops and waiting rooms in a surgery center are both within the context for the pattern "A PLACE TO WAIT".


Business models also have design patterns.





</doc>
<doc id="8748" url="https://en.wikipedia.org/wiki?curid=8748" title="N,N-Dimethyltryptamine">
N,N-Dimethyltryptamine

N","N"-Dimethyltryptamine (DMT or N","N"-DMT) is a chemical substance that occurs in many plants and animals and which is both a derivative and a structural analog of tryptamine. It can be consumed as a psychedelic drug and has historically been prepared by various cultures for ritual purposes as an entheogen. DMT is illegal in most countries.

DMT has a rapid onset, intense effects, and a relatively short duration of action. For those reasons, DMT was known as the "business trip" during the 1960s in the United States, as a user could access the full depth of a psychedelic experience in considerably less time than with other substances such as LSD or magic mushrooms. DMT can be inhaled, ingested, or injected and its effects depend on the dose, as well as its mode of administration. When inhaled or injected, the effects last a short period of time: about five to 15 minutes. Effects can last three hours or more when orally ingested along with an MAOI, such as the ayahuasca brew of many native Amazonian tribes. DMT can produce vivid "projections" of mystical experiences involving euphoria and dynamic hallucinations of geometric forms.

DMT is a functional analog and structural analog of other psychedelic tryptamines such as "O"-Acetylpsilocin (4-AcO-DMT), 5-MeO-DMT, bufotenin (5-HO-DMT), psilocybin (4-PO-DMT), and psilocin (4-HO-DMT). The structure of DMT occurs within some important biomolecules like serotonin and melatonin, making them structural analogs of DMT.

DMT is produced in many species of plants often in conjunction with its close chemical relatives 5-methoxy-"N","N"-dimethyltryptamine (5-MeO-DMT) and bufotenin (5-OH-DMT). DMT-containing plants are commonly used in indigenous Amazonian shamanic practices. It is usually one of the main active constituents of the drink ayahuasca; however, ayahuasca is sometimes brewed with plants that do not produce DMT. It occurs as the primary psychoactive alkaloid in several plants including "Mimosa tenuiflora", "Diplopterys cabrerana", and "Psychotria viridis". DMT is found as a minor alkaloid in snuff made from Virola bark resin in which 5-MeO-DMT is the main active alkaloid. DMT is also found as a minor alkaloid in bark, pods, and beans of "Anadenanthera peregrina" and "Anadenanthera colubrina" used to make Yopo and Vilca snuff, in which bufotenin is the main active alkaloid. Psilocin and its precursor psilocybin, an active chemical in many psilocybin mushrooms, are structurally similar to DMT.

The psychotropic effects of DMT were first studied scientifically by the Hungarian chemist and psychologist Stephen Szára, who performed research with volunteers in the mid-1950s. Szára, who later worked for the US National Institutes of Health, had turned his attention to DMT after his order for LSD from the Swiss company Sandoz Laboratories was rejected on the grounds that the powerful psychotropic could be dangerous in the hands of a communist country.

DMT is generally not active orally unless it is combined with a monoamine oxidase inhibitor (MAOI) such as a reversible inhibitor of monoamine oxidase A (RIMA), for example, harmaline. Without a MAOI, the body quickly metabolizes orally administered DMT, and it therefore has no hallucinogenic effect unless the dose exceeds monoamine oxidase's metabolic capacity. Other means of ingestion such as vaporizing, injecting, or insufflating the drug can produce powerful hallucinations for a short time (usually less than half an hour), as the DMT reaches the brain before it can be metabolized by the body's natural monoamine oxidase. Taking a MAOI prior to vaporizing or injecting DMT prolongs and potentiates the effects.

Induced DMT experiences can include profound time-dilation, visual, auditory, tactile, and proprioceptive distortions and hallucinations, and other experiences that, by most firsthand accounts, defy verbal or visual description. Examples include perceiving hyperbolic geometry or seeing Escher-like impossible objects.

Several scientific experimental studies have tried to measure subjective experiences of altered states of consciousness induced by drugs under highly controlled and safe conditions.

In the 1990s, Rick Strassman and his colleagues conducted a five-year-long DMT study at the University of New Mexico. The results provided insight about the quality of subjective psychedelic experiences. In this study participants received the DMT dosage intravenously via injection and the findings suggested that different psychedelic experiences can occur, depending on the level of dosage. Lower doses (0.01 and 0.05 mg/kg) produced somaesthetic and emotional responses, but not hallucinogenic experiences (e.g., 0.05 mg/kg had mild mood elevating and calming properties). In contrast, responses produced by higher doses (0.2 and 0.4 mg/kg) researchers labeled as "hallucinogenic" that elicited "intensely colored, rapidly moving display of visual images, formed, abstract or both". Comparing to other sensory modalities the most affected was the visual. Participants reported visual hallucinations, fewer auditory hallucinations and specific physical sensations progressing to a sense of bodily dissociation, as well as to experiences of euphoria, calm, fear, and anxiety.

Strassman also stressed the importance of the context where the drug has been taken. He claimed that DMT has no beneficial effects of itself, rather the context when and where people take it plays an important role.

It appears that DMT can induce a state or feeling to a person that he or she is able to "communicate with other intelligent-life forms" (see "Machine Elves"). High doses of DMT produce a state that involves a sense of "another intelligence" that people sometimes describe as "super-intelligent", but "emotionally detached".

In 1995 Adolf Dittrich and Daniel Lamparter did a study where they found that DMT-induced altered state of consciousness (ASC) is strongly influenced by habitual, rather than situative factors. In the study researchers used three dimensions of the APZ questionnaire to describe ASC (rating scales of ASC). First, oceanic boundlessness (OB) refers to dissolution of ego boundaries mostly associated with positive emotions. Second, anxious ego-dissolution (AED) includes disorder of thoughts, loss of autonomy and self-control and third, visionary restructuralization (VR) that includes auditory and visual illusions, as well as hallucinations. Results showed strong effects within the first and third dimensions for all conditions, especially with DMT, and suggested strong intrastability of elicited reactions independently of the condition for the OB and VR scales. Importantly, the experiment was conducted in a safe laboratory environment. This particular setting had a certain influence on found results that might be very different outside the laboratory environment.

Entities perceived during DMT inebriation have been represented in diverse forms of psychedelic art. The term "machine elf" was coined by ethnobotanist Terence McKenna for the entities he encountered in DMT "hyperspace", also using terms like "fractal elves", or "self-transforming machine elves". McKenna first encountered the "machine elves" after smoking DMT in Berkeley in 1965. His subsequent speculations regarding the hyperdimensional space in which they were encountered have inspired a great many artists and musicians, and the meaning of DMT entities has been a subject of considerable debate among participants in a networked cultural underground, enthused by McKenna's effusive accounts of DMT hyperspace. Cliff Pickover has also written about the "machine elf" experience, in the book "Sex, Drugs, Einstein, & Elves", while Rick Strassman notes many similarities between self-reports of his DMT study participants' encounters with these "entities", and mythological descriptions of figures such as Chayot Ha Kodesh in Ancient religions, including both angels and demons. Strassman also argues for a similarity in his study participants' descriptions of mechanized wheels, gears and machinery in these encounters, with those described in visions of encounters with the Living Creatures and Ophanim of the Hebrew Bible, noting they may stem from a common neuropsychopharmacological experience.

Strassman argues that the more positive of the "external entities" encountered in DMT experiences should be understood as analogous to certain forms of angels: 
However, Strassman's experimental participants also note that some other entities can subjectively resemble creatures more like insects and aliens. As a result, Strassman writes these experiences among his experimental participants "also left me feeling confused and concerned about where the spirit molecule was leading us. It was at this point that I began to wonder if I was getting in over my head with this research."

Hallucinations of strange creatures had been reported by Stephen Szara in a 1958 study in psychotic patients, in which he described how one of his subjects under the influence of DMT had experienced "strange creatures, dwarves or something" at the beginning of a DMT trip.

Other researchers of the entities seemingly encountered by DMT users describe them as "entities" or "beings" in humanoid as well as animal form, with descriptions of "little people" being common (non-human gnomes, elves, imps, etc.). Strassman and others have speculated that this form of hallucination may be the cause of alien abduction and extraterrestrial encounter experiences, which may occur through endogenously-occurring DMT.

Likening them to descriptions of rattling and chattering auditory phenomenon described in encounters with the Hayyoth in the Book of Ezekiel, Rick Strassman notes that participants in his studies, when reporting encounters with the alleged entities, have also described loud auditory hallucinations, such as one subject reporting typically "the elves laughing or talking at high volume, chattering, twittering".

According to a dose-response study in human subjects, dimethyltryptamine administered intravenously slightly elevated blood pressure, heart rate, pupil diameter, and rectal temperature, in addition to elevating blood concentrations of beta-endorphin, corticotropin, cortisol, and prolactin; growth hormone blood levels rise equally in response to all doses of DMT, and melatonin levels were unaffected."

The dependence potential of DMT and the risk of sustained psychological disturbance may be minimal when used infrequently, as in religious ceremonies; however, the physiological dependence potential of DMT and ayahuasca has not yet been documented convincingly.

In the 1950s, the endogenous production of psychoactive agents was considered to be a potential explanation for the hallucinatory symptoms of some psychiatric diseases; this is known as the transmethylation hypothesis. Several speculative and yet untested hypotheses suggest that endogenous DMT is produced in the human brain and is involved in certain psychological and neurological states. DMT is naturally occurring in small amounts in rat brain, human cerebrospinal fluid, and other tissues of humans and other mammals. In 2011, Nicholas V. Cozzi, of the University of Wisconsin School of Medicine and Public Health, concluded that INMT, an enzyme that is associated with the biosynthesis of DMT and endogenous hallucinogens, is present in the primate (rhesus macaque) pineal gland, retinal ganglion neurons, and spinal cord. Neurobiologist Andrew Gallimore (2013) suggested that while DMT might not have a modern neural function, it may have been an ancestral neuromodulator once secreted in psychedelic concentrations during REM sleep, a function now lost.

A standard dose for vaporized DMT is 20–60 milligrams. In general, this is inhaled in a few successive breaths. The effects last for a short period of time, usually 5 to 15 minutes, dependent on the dose. The onset after inhalation is very fast (less than 45 seconds) and peak effects are reached within a minute. In the 1960s, DMT was known as a "business trip" in the US because of the relatively short duration (and rapid onset) of action when inhaled. DMT can be inhaled using a bong or even an e-cigarette. A DMT-infused smoking blend is called Changa.

In a study conducted from 1990 through 1995, University of New Mexico psychiatrist Rick Strassman found that some volunteers injected with high doses of DMT reported experiences with perceived alien entities. Usually, the reported entities were experienced as the inhabitants of a perceived independent reality that the subjects reported visiting while under the influence of DMT.

DMT is broken down by the enzyme monoamine oxidase through a process called deamination, and is quickly inactivated orally unless combined with a monoamine oxidase inhibitor (MAOI). The traditional South American beverage ayahuasca, or yage, is derived by boiling the ayahuasca vine ("Banisteriopsis caapi") with leaves of one or more plants containing DMT, such as "Psychotria viridis", "Psychotria carthagenensis", or "Diplopterys cabrerana". The Ayahuasca vine contains harmala alkaloids, highly active reversible inihibitors of monoamine oxidase A (RIMAs), rendering the DMT orally active by protecting it from deamination. A variety of different recipes are used to make the brew depending on the purpose of the ayahuasca session, or local availability of ingredients. Two common sources of DMT in the western US are reed canary grass ("Phalaris arundinacea") and Harding grass ("Phalaris aquatica"). These invasive grasses contain low levels of DMT and other alkaloids but also contain gramine, which is toxic and difficult to separate. In addition, Jurema ("Mimosa tenuiflora") shows evidence of DMT content: the pink layer in the inner rootbark of this small tree contains a high concentration of "N,N"-DMT.

Taken orally with an RIMA, DMT produces a long lasting (over three hour), slow, deep metaphysical experience similar to that of psilocybin mushrooms, but more intense. RIMAs should be used with caution as they can have fatal interactions with some prescription drugs such as SSRI antidepressants, and some over-the-counter drugs known as sympathomimetics such as Ephedrine or certain cough medicines and even some herbal remedies .

DMT has been used in South America since pre-Columbian times.

DMT was first synthesized in 1931 by chemist Richard Helmuth Fredrick Manske (born 1901 in Berlin, Germany – 1977). In general, its discovery as a natural product is credited to Brazilian chemist and microbiologist Oswaldo Gonçalves de Lima (1908–1989) who, in 1946, isolated an alkaloid he named "nigerina" (nigerine) from the root bark of "jurema preta", that is, "Mimosa tenuiflora". However, in a careful review of the case Jonathan Ott shows that the empirical formula for nigerine determined by Gonçalves de Lima, which notably contains an atom of oxygen, can match only a partial, "impure" or "contaminated" form of DMT. It was only in 1959, when Gonçalves de Lima provided American chemists a sample of "Mimosa tenuiflora" roots, that DMT was unequivocally identified in this plant material. Less ambiguous is the case of isolation and formal identification of DMT in 1955 in seeds and pods of "Anadenanthera peregrina" by a team of American chemists led by Evan Horning (1916–1993). Since 1955, DMT has been found in a host of organisms: in at least fifty plant species belonging to ten families, and in at least four animal species, including one gorgonian and three mammalian species (including humans).

In terms of a scientific understanding, the hallucinogenic properties of DMT were not uncovered until 1956 by Hungarian chemist and psychiatrist Stephen Szara. In his paper “Dimethyltryptamin: Its Metabolism in Man;
the Relation of its Psychotic Effect to the
Serotonin Metabolism”, Szara employed synthetic DMT, synthesized by the method of Speeter and Anthony, which was then administered to 20 volunteers by intramuscular injection. Urine samples were collected from these volunteers for the identification of DMT metabolites. This is considered to be the converging link between the chemical structure DMT to its cultural consumption as a psychoactive and religious sacrament.

Another historical milestone is the discovery of DMT in plants frequently used by Amazonian natives as additive to the vine "Banisteriopsis caapi" to make ayahuasca decoctions. In 1957, American chemists Francis Hochstein and Anita Paradies identified DMT in an "aqueous extract" of leaves of a plant they named "Prestonia amazonicum" ["sic"] and described as "commonly mixed" with "B. caapi". The lack of a proper botanical identification of "Prestonia amazonica" in this study led American ethnobotanist Richard Evans Schultes (1915–2001) and other scientists to raise serious doubts about the claimed plant identity. The mistake likely led the writer William Burroughs to regard the DMT he experimented with in Tangier in 1961 as "Prestonia". Better evidence was produced in 1965 by French pharmacologist Jacques Poisson, who isolated DMT as a sole alkaloid from leaves, provided and used by Agaruna Indians, identified as having come from the vine "Diplopterys cabrerana" (then known as "Banisteriopsis rusbyana"). Published in 1970, the first identification of DMT in the plant "Psychotria viridis", another common additive of ayahuasca, was made by a team of American researchers led by pharmacologist Ara der Marderosian. Not only did they detect DMT in leaves of "P. viridis" obtained from Kaxinawá indigenous people, but they also were the first to identify it in a sample of an ayahuasca decoction, prepared by the same indigenous people.





In 2017 the Santo Daime Church Céu do Montréal received religious exemption to use Ayahuasca as a sacrament in their rituals.


In December 2004, the Supreme Court lifted a stay, thereby allowing the Brazil-based União do Vegetal (UDV) church to use a decoction containing DMT in their Christmas services that year. This decoction is a tea made from boiled leaves and vines, known as hoasca within the UDV, and ayahuasca in different cultures. In "Gonzales v. O Centro Espírita Beneficente União do Vegetal", the Supreme Court heard arguments on November 1, 2005, and unanimously ruled in February 2006 that the U.S. federal government must allow the UDV to import and consume the tea for religious ceremonies under the 1993 Religious Freedom Restoration Act.

In September 2008, the three Santo Daime churches filed suit in federal court to gain legal status to import DMT-containing ayahuasca tea. The case, "Church of the Holy Light of the Queen v. Mukasey", presided over by Judge Owen M. Panner, was ruled in favor of the Santo Daime church. As of March 21, 2009, a federal judge says members of the church in Ashland can import, distribute and brew ayahuasca. U.S. District Judge Owen Panner issued a permanent injunction barring the government from prohibiting or penalizing the sacramental use of "Daime tea". Panner's order said activities of The Church of the Holy Light of the Queen are legal and protected under freedom of religion. His order prohibits the federal government from interfering with and prosecuting church members who follow a list of regulations set out in his order.


Under the Misuse of Drugs act 1981 6.0 g of DMT is considered enough to determine a court of trial and 2.0 g is considered intent to sell and supply.

Between 2011 and 2012, the Australian Federal Government was considering changes to the Australian Criminal Code that would classify any plants containing any amount of DMT as "controlled plants". DMT itself was already controlled under current laws. The proposed changes included other similar blanket bans for other substances, such as a ban on any and all plants containing Mescaline or Ephedrine. The proposal was not pursued after political embarrassment on realisation that this would make the official Floral Emblem of Australia, Acacia pycnantha (Golden Wattle), illegal. The Therapeutic Goods Administration and federal authority had considered a motion to ban the same, but this was withdrawn in May 2012 (as DMT may still hold potential entheogenic value to native and/or religious people).

DMT is commonly handled and stored as a fumarate, as other DMT acid salts are extremely hygroscopic and will not readily crystallize. Its freebase form, although less stable than DMT fumarate, is favored by recreational users choosing to vaporize the chemical as it has a lower boiling point.

Dimethyltryptamine is an indole alkaloid derived from the shikimate pathway. Its biosynthesis is relatively simple and summarized in the adjacent picture. In plants, the parent amino acid L-tryptophan is produced endogenously where in animals L-tryptophan is an essential amino acid coming from diet. No matter the source of L-tryptophan, the biosynthesis begins with its decarboxylation by an aromatic amino acid decarboxylase (AADC) enzyme (step 1). The resulting decarboxylated tryptophan analog is tryptamine. Tryptamine then undergoes a transmethylation (step 2): the enzyme indolethylamine-N-methyltransferase (INMT) catalyzes the transfer of a methyl group from cofactor S-adenosyl-methionine (SAM), via nucleophilic attack, to tryptamine. This reaction transforms SAM into S-adenosylhomocysteine (SAH), and gives the intermediate product "N"-methyltryptamine (NMT). NMT is in turn transmethylated by the same process (step 3) to form the end product "N","N"-dimethyltryptamine. Tryptamine transmethylation is regulated by two products of the reaction: SAH, and DMT were shown "ex vivo" to be among the most potent inhibitors of rabbit INMT activity.

This transmethylation mechanism has been repeatedly and consistently proven by radiolabeling of SAM methyl group with carbon-14 (C-CH)SAM.

DMT can be synthesized through several possible pathways from different starting materials. The two most commonly encountered synthetic routes are through the reaction of indole with oxalyl chloride followed by reaction with dimethylamine and reduction of the carbonyl functionalities with lithium aluminum hydride to form DMT. The second commonly encountered route is through the n,n-dimethylation of tryptamine using formaldehyde followed by reduction with sodium cyanoborohydride or sodium triacetoxyborohydride. Sodium borohydride is not used as it reduces the formaldehyde to methanol before it is able to react with the primary amine of tryptamine. Bufotenine, a plant extract, can also be synthesized into DMT.

In a clandestine setting, DMT is not typically synthesized due to the lack of availability of the starting materials, namely tryptamine and oxalyl chloride. Instead, it is more often extracted from plant sources using a non-polar hydrocarbon solvent such as naphtha or heptane, and a base such as sodium hydroxide.

Alternatively, an acid-base extraction is sometimes used instead.

A variety of plants contain DMT at sufficient levels for being viable sources, but specific plants such as "Mimosa tenuiflora" and "Acacia confusa" are most often used.

The chemicals involved in the extraction are commonly available. The plant material may be illegal to procure in some countries. The end product (DMT) is illegal in most countries.

Published in "Science" in 1961, Julius Axelrod found an "N"-methyltransferase enzyme capable of mediating biotransformation of tryptamine into DMT in a rabbit's lung. This finding initiated a still ongoing scientific interest in endogenous DMT production in humans and other mammals. From then on, two major complementary lines of evidence have been investigated: localization and further characterization of the "N"-methyltransferase enzyme, and analytical studies looking for endogenously produced DMT in body fluids and tissues.

In 2013, researchers reported DMT in the pineal gland microdialysate of rodents.

A study published in 2014 reported the biosynthesis of "N,N"-dimethyltryptamine (DMT) in the human melanoma cell line SK-Mel-147 including details on its metabolism by peroxidases.

In 2014, researchers demonstrated the immunomodulatory potential of DMT and 5-MeO-DMT through the Sigma-1 receptor of human immune cells. This immunomodulatory activity may contribute to significant anti-inflammatory effects and tissue regeneration.

The first claimed detection of mammalian endogenous DMT was published in June 1965: German researchers F. Franzen and H. Gross report to have evidenced and quantified DMT, along with its structural analog bufotenin (5-HO-DMT), in human blood and urine. In an article published four months later, the method used in their study was strongly criticized, and the credibility of their results challenged.

Few of the analytical methods used prior to 2001 to measure levels of endogenously formed DMT had enough sensitivity and selectivity to produce reliable results. Gas chromatography, preferably coupled to mass spectrometry (GC-MS), is considered a minimum requirement. A study published in 2005 implements the most sensitive and selective method ever used to measure endogenous DMT: liquid chromatography-tandem mass spectrometry with electrospray ionization (LC-ESI-MS/MS) allows for reaching limits of detection (LODs) 12 to 200 fold lower than those attained by the best methods employed in the 1970s. The data summarized in the table below are from studies conforming to the abovementioned requirements (abbreviations used: CSF = cerebrospinal fluid; LOD = limit of detection; n = number of samples; ng/L and ng/kg = nanograms (10 g) per litre, and nanograms per kilogram, respectively):

A 2013 study found DMT in microdialysate obtained from a rat's pineal gland, providing evidence of endogenous DMT in the mammalian brain. In 2019 experiments showed that the rat brain is capable of synthesizing and releasing DMT. These results raise the possibility that this phenomenon may occur similarly in human brains.

DMT may be measured in blood, plasma or urine using chromatographic techniques as a diagnostic tool in clinical poisoning situations or to aid in the medicolegal investigation of suspicious deaths. In general, blood or plasma DMT levels in recreational users of the drug are in the 10–30 μg/L range during the first several hours post-ingestion. Less than 0.1% of an oral dose is eliminated unchanged in the 24-hour urine of humans.

Before techniques of molecular biology were used to localize indolethylamine "N"-methyltransferase (INMT), characterization and localization went on a par: samples of the biological material where INMT is hypothesized to be active are subject to enzyme assay. Those enzyme assays are performed either with a radiolabeled methyl donor like (C-CH)SAM to which known amounts of unlabeled substrates like tryptamine are added or with addition of a radiolabeled substrate like (C)NMT to demonstrate in vivo formation. As qualitative determination of the radioactively tagged product of the enzymatic reaction is sufficient to characterize INMT existence and activity (or lack of), analytical methods used in INMT assays are not required to be as sensitive as those needed to directly detect and quantify the minute amounts of endogenously formed DMT (see DMT subsection below). The essentially qualitative method thin layer chromatography (TLC) was thus used in a vast majority of studies. Also, robust evidence that INMT can catalyze transmethylation of tryptamine into NMT and DMT could be provided with reverse isotope dilution analysis coupled to mass spectrometry for rabbit and human lung during the early 1970s.

Selectivity rather than sensitivity proved to be an Achilles' heel for some TLC methods with the discovery in 1974–1975 that incubating rat blood cells or brain tissue with (C-CH)SAM and NMT as substrate mostly yields tetrahydro-β-carboline derivatives, and negligible amounts of DMT in brain tissue. It is indeed simultaneously realized that the TLC methods used thus far in almost all published studies on INMT and DMT biosynthesis are incapable to resolve DMT from those tetrahydro-β-carbolines. These findings are a blow for all previous claims of evidence of INMT activity and DMT biosynthesis in avian and mammalian brain, including in vivo, as they all relied upon use of the problematic TLC methods: their validity is doubted in replication studies that make use of improved TLC methods, and fail to evidence DMT-producing INMT activity in rat and human brain tissues. Published in 1978, the last study attempting to evidence in vivo INMT activity and DMT production in brain (rat) with TLC methods finds biotransformation of radiolabeled tryptamine into DMT to be real but "insignificant". Capability of the method used in this latter study to resolve DMT from tetrahydro-β-carbolines is questioned later.
To localize INMT, a qualitative leap is accomplished with use of modern techniques of molecular biology, and of immunohistochemistry. In humans, a gene encoding INMT is determined to be located on chromosome 7. Northern blot analyses reveal INMT messenger RNA (mRNA) to be highly expressed in rabbit lung, and in human thyroid, adrenal gland, and lung. Intermediate levels of expression are found in human heart, skeletal muscle, trachea, stomach, small intestine, pancreas, testis, prostate, placenta, lymph node, and spinal cord. Low to very low levels of expression are noted in rabbit brain, and human thymus, liver, spleen, kidney, colon, ovary, and bone marrow. INMT mRNA expression is absent in human peripheral blood leukocytes, whole brain, and in tissue from 7 specific brain regions (thalamus, subthalamic nucleus, caudate nucleus, hippocampus, amygdala, substantia nigra, and corpus callosum). Immunohistochemistry showed INMT to be present in large amounts in glandular epithelial cells of small and large intestines. In 2011, immunohistochemistry revealed the presence of INMT in primate nervous tissue including retina, spinal cord motor neurons, and pineal gland.

DMT peak level concentrations ("C") measured in whole blood after intramuscular (IM) injection (0.7 mg/kg, n = 11) and in plasma following intravenous (IV) administration (0.4 mg/kg, n = 10) of fully psychedelic doses are in the range of ≈14 to 154 μg/L and 32 to 204 μg/L, respectively.
The corresponding molar concentrations of DMT are therefore in the range of 0.074–0.818 μM in whole blood and 0.170–1.08 μM in plasma. However, several studies have described active transport and accumulation of DMT into rat and dog brain following peripheral administration.
Similar active transport, and accumulation processes likely occur in human brain and may concentrate DMT in brain by several-fold or more (relatively to blood), resulting in local concentrations in the micromolar or higher range. Such concentrations would be commensurate with serotonin brain tissue concentrations, which have been consistently determined to be in the 1.5-4 μM range.

Closely coextending with peak psychedelic effects, mean time to reach peak concentrations ("T") was determined to be 10–15 minutes in whole blood after IM injection, and 2 minutes in plasma after IV administration. When taken orally mixed in an ayahuasca decoction, and in freeze-dried ayahuasca gel caps, DMT "T" is considerably delayed: 107.59 ± 32.5 minutes, and 90–120 minutes, respectively.
The pharmacokinetics for vaporizing DMT have not been studied or reported.

DMT binds non-selectively with affinities < 0.6 μM to the following serotonin receptors: 5-HT, 5-HT, 5-HT, 5-HT, 5-HT, 5-HT, 5-HT, and 5-HT. An agonist action has been determined at 5-HT, 5-HT and 5-HT. Its efficacies at other serotonin receptors remain to be determined. Of special interest will be the determination of its efficacy at human 5-HT receptor as two "in vitro" assays evidenced DMT's high affinity for this receptor: 0.108 μM and 0.184 μM. This may be of importance because chronic or frequent uses of serotonergic drugs showing preferential high affinity and clear agonism at 5-HT receptor have been causally linked to valvular heart disease.

It has also been shown to possess affinity for the dopamine D, α-adrenergic, α-adrenergic, imidazoline-1, and σ receptors. Converging lines of evidence established activation of the σ receptor at concentrations of 50–100 μM. Its efficacies at the other receptor binding sites are unclear. It has also been shown "in vitro" to be a substrate for the cell-surface serotonin transporter (SERT) expressed in human platelets, and the rat vesicular monoamine transporter 2 (VMAT2), which was transiently expressed in fall armyworm Sf9 cells. DMT inhibited SERT-mediated serotonin uptake into platelets at an average concentration of 4.00 ± 0.70 μM and VMAT2-mediated serotonin uptake at an average concentration of 93 ± 6.8 μM.

As with other so-called "classical hallucinogens", a large part of DMT psychedelic effects can be attributed to a functionally selective activation of the 5-HT receptor. DMT concentrations eliciting 50% of its maximal effect (half maximal effective concentration = EC or K) at the human 5-HT receptor "in vitro" are in the 0.118–0.983 μM range. This range of values coincides well with the range of concentrations measured in blood and plasma after administration of a fully psychedelic dose (see Pharmacokinetics).

As DMT has been shown to have slightly better efficacy (EC) at human serotonin 2C receptor than at the 2A receptor, 5-HT is also likely implicated in DMT's overall effects. Other receptors, such as 5-HT σ, may also play a role.

In 2009, it was hypothesized that DMT may be an endogenous ligand for the σ receptor. The concentration of DMT needed for σ activation "in vitro" (50–100 μM) is similar to the behaviorally active concentration measured in mouse brain of approximately 106 μM This is minimally 4 orders of magnitude higher than the average concentrations measured in rat brain tissue or human plasma under basal conditions (see Endogenous DMT), so σ receptors are likely to be activated only under conditions of high local DMT concentrations. If DMT is stored in synaptic vesicles, such concentrations might occur during vesicular release. To illustrate, while the "average" concentration of serotonin in brain tissue is in the 1.5–4 μM range, the concentration of serotonin in synaptic vesicles was measured at 270 mM. Following vesicular release, the resulting concentration of serotonin in the synaptic cleft, to which serotonin receptors are exposed, is estimated to be about 300 μM. Thus, while "in vitro" receptor binding affinities, efficacies, and average concentrations in tissue or plasma are useful, they are not likely to predict DMT concentrations in the vesicles or at synaptic or intracellular receptors. Under these conditions, notions of receptor selectivity are moot, and it seems probable that most of the receptors identified as targets for DMT (see above) participate in producing its psychedelic effects.

Electronic cigarette cartridges filled with DMT started to be sold on the black market in 2018.




</doc>
<doc id="8750" url="https://en.wikipedia.org/wiki?curid=8750" title="Da capo">
Da capo

Da capo (, , ) is an Italian musical term that means "from the beginning" (literally, "from the head"). It is often abbreviated as D.C. The term is a directive to repeat the previous part of music, often used to save space, and thus is an easier way of saying to repeat the music from the beginning.

In small pieces, this might be the same thing as a repeat. But in larger works, D.C. might occur after one or more repeats of small sections, indicating a return to the very beginning. The resulting structure of the piece is generally in ternary form. Sometimes, the composer describes the part to be repeated, for example: "Menuet da capo". In opera, where an aria of this structure is called a "da capo aria", the repeated section is often adorned with grace notes.

The word "Fine" (Ital. 'end') is generally placed above the stave at the point where the movement ceases after a 'Da capo' repetition. Its place is occasionally taken by a pause (see fermata)."






</doc>
<doc id="8751" url="https://en.wikipedia.org/wiki?curid=8751" title="Dominatrix">
Dominatrix

A dominatrix (, plural dominatrices ), is a woman who takes the dominant role in BDSM activities. A dominatrix might be of any sexual orientation, but her orientation does not necessarily limit the genders of her submissive partners. The role of a dominatrix may not even involve physical pain toward the submissive; her domination can be verbal, involving humiliating tasks, or servitude. A dominatrix is typically a paid professional ("pro-domme") as the term "dominatrix" is little-used within the non-professional BDSM scene.

The term "domme" is a likely a coined pseudo-French feminine inflection of the slang "dom" (short for "dominant"). The use of "domme", "dominatrix", "dom", or "dominant" by any woman in a dominant role is chosen mostly by personal preference and the conventions of the local BDSM scene. The term mistress or dominant mistress is sometimes also used. Female dominance, female domination or femdom refer to BDSM activities in which the dominant partner is female.

As fetish culture is increasingly becoming more prevalent in Western media, depictions of dominatrices in film and television have become more common.

"Dominatrix" is the feminine form of the Latin "dominator", a ruler or lord, and was originally used in a non-sexual sense. Its use in English dates back to at least 1561. Its earliest recorded use in the prevalent modern sense, as a female dominant in S&M, dates to 1961. It was initially coined to describe a woman who provides punishment-for-pay as one of the case studies within Bruce Roger's pulp paperback "The Bizarre Lovemakers". The term was taken up shortly after by the Myron Kosloff title "Dominatrix" (with art by Eric Stanton) in 1968, and entered more popular mainstream knowledge following the 1976 film "Dominatrix Without Mercy".

Although the term "dominatrix" was not used, the classic example in literature of the female dominant-male submissive relationship is portrayed in the 1870 novella "Venus in Furs" by Austrian writer Leopold von Sacher-Masoch. The term "masochism" was later derived from the author's name by Richard von Krafft-Ebing in the latter's 1886 forensic study "Psychopathia Sexualis".

The history of the dominatrix is argued to date back to rituals of the Goddess Inanna (or Ishtar as she was known in Akkadian), in ancient Mesopotamia. Ancient cuneiform texts consisting of "Hymns to Inanna" have been cited as examples of the archetype of powerful, sexual female displaying dominating behaviors and forcing Gods and men into submission to her. Archaeologist and historian Anne O. Nomis notes that Inanna's rituals included cross-dressing of cult personnel, and rituals "imbued with pain and ecstasy, bringing about initiation and journeys of altered consciousness; punishment, moaning, ecstasy, lament and song, participants exhausting themselves with weeping and grief."

The tale of Phyllis and Aristotle, which became popular and gained numerous versions from the 12th century onwards, tells the story of a dominant woman who seduced and dominated the male intellect of the greatest philosopher. In the story, Phyllis forces Aristotle to kneel on the ground so that she rides on his back while whipping and verbally humiliating him.

The profession appears to have originated as a specialization within brothels, before becoming its own unique craft. As far back as the 1590s, flagellation within an erotic setting is recorded. The profession features in erotic prints of the era, such as the British Museum mezzotint "The Cully Flaug'd" (c. 1674–1702), and in accounts of forbidden books which record the flogging schools and the activities practised.

Within the 18th century, female "Birch Disciplinarians" advertised their services in a book masked as a collection of lectures or theatrical plays, entitled "Fashionable Lectures" (c. 1761). This included the names of 57 women, some actresses and courtesans, who catered to birch discipline fantasies, keeping a room with rods and cat o' nine tails, and charging their clients a Guinea for a "lecture".

The 19th century is characterised by what historian Anne O. Nomis characterises as the "Golden Age of the Governess". No fewer than twenty establishments were documented as having existed by the 1840s, supported entirely by flagellation practices and known as "Houses of Discipline" distinct from brothels. Amongst the well-known "dominatrix governesses" were Mrs Chalmers, Mrs Noyeau, the late Mrs Jones of Hertford Street and London Street, the late Mrs Theresa Berkley, Bessy Burgess of York Square and Mrs Pyree of Burton Cres. The most famous of these Governess "female flagellants" was Theresa Berkley, who operated her establishment on Charlotte Street in the central London district of Marylebone. She is recorded to have used implements such as whips, canes and birches, to chastise and punish her male clients, as well as the Berkley Horse, a specially designed flogging machine, and a pulley suspension system for lifting them off the floor. Such historical use of corporal punishment and suspension, in a setting of domination roleplay, connects very closely to the practices of modern-day professional dominatrices.

The "bizarre style" (as it came to be called) of leather catsuits, claws, tail whips, and latex rubber only came about in the 20th century, initially within commercial fetish photography, and taken up by dominatrices. Within the mid-20th century, dominatrices operated in a very discreet and underground manner, which has made them difficult to trace within the historical record. A few photographs still exist of the women who ran their domination businesses in London, New York, The Hague and Hamburg's Herbertstraße, predominantly in sepia and black-and-white photographs, and scans from magazine articles, copied and re-copied. Amongst these were Miss Doreen of London who was acquainted with John Sutcliffe of "AtomAge" fame, whose clients reportedly included Britain's top politicians and businessmen. In New York, the dominatrix Anne Laurence was known within the underground circle of acquaintances during the 1950s, with Monique Von Cleef arriving in the early 1960s, and hitting national headlines when her home was raided by police detectives on 22 December 1965. Von Cleef went on to set up her "House of Pain" in The Hague in the 1970s, which became one of the world capitals for dominatrices, reportedly with visiting lawyers, ambassadors, diplomats and politicians. Domenica Niehoff worked as a dominatrix in Hamburg and appeared on talk shows on German television from the 1970s onwards, campaigning for sex workers' rights. Mistress Raven, founder and manager of Pandora's Box, one of New York's best known BDSM studios, was featured in Nick Broomfield's 1996 documentary film "Fetishes".

The term "dominatrix" is mostly used to describe a female professional dominant (or "pro-domme") who is paid to engage in BDSM play with a submissive. Professional dominatrices are not prostitutes, despite the sensual and erotic interactions she has. An appointment or roleplay is referred to as a "session", and is often conducted in a dedicated professional play space which has been set up with specialist equipment, known as a "dungeon". Sessions may also be conducted remotely by letter or telephone, or in the contemporary era of technological connectivity by email or online chat. Most, but not all, clients of female professional dominants are men. Male professional dominants also exist, catering predominantly to the gay male market.

Women who engage in female domination typically promote and title themselves under the terms "dominatrix", "mistress", "lady", "madame", "herrin" or "goddess". In a study of German dominatrices, Andrew Wilson said that the trend for dominatrices choosing names aimed at creating and maintaining an atmosphere in which class, femininity and mystery are key elements of their self-constructed identity.

Some professional dominatrices set minimum age limits for their clients. Popular requests from clients are for dungeon play including bondage, spanking and cock and ball torture, or for medical play using hoods, gas masks and urethral sounding. Verbal erotic humiliation, such as small penis humiliation, is also popular. It is not unusual for a dominatrix to consider her profession different from that of an escort and not perform tie and tease or "happy endings". Typically professional dominatrices do not have sexual intercourse with their clients, do not become naked with their clients and do not allow their clients to touch them. The Canadian dominatrix Terri-Jean Bedford, who was one of three women who initiated an application in the Ontario Superior Court seeking invalidation of Canada's laws regarding brothels, sought to differentiate for clarity her occupation as a dominatrix rather than a prostitute to the media, due to frequent misunderstanding and conflation by the public of the two terms.

While dominatrices come from many different backgrounds, it has been shown that a considerable number are well-educated. Research into US dominatrices published in 2012 indicated that 39% of the sample studied had received some sort of graduate training.

A 1985 study suggested that about 30 percent of participants in BDSM subculture were female. A 1994 report indicated that around a quarter of the women who took part in BDSM subculture did so professionally. In a 1995 study of Internet discussion group messages, the preference for the dominant-initiator role was expressed by 11% of messages by heterosexual women, compared to 71% of messages by heterosexual men.

Professional dominatrices can be seen advertising their services online and in print publications which carry erotic services advertising, such as contact magazines and fetish magazines that specialise in female domination. The precise number of women actively offering professional domination services is unknown. Most professional dominatrices practice in large metropolitan cities such as New York, Los Angeles, and London, with as many as 200 women working as dominatrices in Los Angeles.

Professional dominatrices may take pride or differentiation in their psychological insight into their clients' fetishes and desires, as well as their technical ability to perform complex BDSM practices, such as Japanese shibari, head-scissoring, and other forms of bondage, suspension, torture roleplay, and corporal punishment, and other such practices which require a high degree of knowledge and competency to safely oversee. From a sociological point of view, Danielle Lindemann has stated the "embattled purity regime" in which many pro-dommes emphasise their specialist knowledge and professional skills, while distancing themselves from economic criteria for success, in a way which is comparable to avant-garde artists.

Some dominatrices practice financial domination, or findom, a fetish in which a submissive is aroused by sending money or gifts to a dominatrix at her instruction. In some cases the dominatrix is given control of the submissive's finances or a "blackmail" scenario is acted out. In the majority of cases the dominatrix and the submissive do not physically meet. The interactions are typically performed using the Internet, which is also where such services are advertised. Findom was originally a niche service that a traditional dominatrix would offer, but it has become popular with less-experienced online practitioners.

To differentiate women who identify as a dominatrix but do not offer paid services, non-professional dominants are occasionally referred to as a "lifestyle" dominatrix or Mistress. The term "lifestyle" to signify BDSM is occasionally a contention topic in the BDSM community and that some dominatrices may dislike the term. Some professional dominatrices are also "lifestyle" dominatrices - i.e., in addition to paid sessions with submissive clients they engage in unpaid recreational sessions or may incorporate power exchange within their own private lives and relationships. However, the term has fallen out of general usage with respect to women who are dominant in their private relationships, and has taken on more and more, the connotation of "professional."

Catherine Robbe-Grillet is a personal dominatrix. Born in Paris in September 24, 1930, she then became France's most famous dominatrix. She is also writer and actress, the widow of nouveau roman pioneer and sadist Alain Robbe-Grillet. She currently lives with Beverly Charpentier, a 51-year-old South African woman who is her submissive companion. Although being such a famous dominatrix, she has never accepted payment for her "ceremonies". She's quoted as saying "If someone pays, then they are in charge. I need to remain free. It is important that everyone involved knows that I do it solely for my pleasure." "Catherine is my secret garden,” Charpentier says. "I have given myself to her, body and soul. She does whatever she wants, whenever she wants, with either or both, according to her pleasure—and her pleasure is also my pleasure." Catherine has always been heavily censored in her novels for writing about S/M stories. She identifies as a “pro-sex feminist” and “the kind of feminist who supports the right of any man or women to work as a prostitute, if it is their free choice.”

The dominatrix is a female archetype which operates on a symbolic mode of representation, associated with particular attire and props that are drawn on within popular culture to signify her role—as a strong, dominant, sexualised woman—linked to but distinct from images of sexual fetish. During the twentieth century, the imagery associated with dominatrices was developed by the work of a number of artists including the costume designer and photographer Charles Guyette, the publisher and film director Irving Klaw, and the illustrators Eric Stanton and Gene Bilbrew who drew for the fetish magazine Exotique.

One of the garments commonly associated with the dominatrix is the catsuit. Historically, the black leather female catsuit entered dominant fetish culture in the 1950s with the "AtomAge" magazine and its connections to fetish fashion designer John Sutcliffe. The spill-over into mainstream culture, occurred with catsuits being worn by strong female protagonists in popular 1960s TV programs like "The Avengers", and in the comic super-heroines such as Catwoman, in which the catsuit represented the independent woman capable of "kick-ass" moves and antics, enabling complete freedom of movement. On another level, the one-piece catsuit accentuated and exaggerated the sexualized female form, providing visual access to a woman's body, while simultaneously obstructing physical penetrative access. "You can look but you can't touch" is the mechanism of this operation, which plays upon the BDSM practice known as "tease and denial".

Other common signifying footwear of the dominatrix are thigh-high boots in leather or shiny PVC, which have long held a fetishistic status and are sometimes called kinky boots, along with the very high stiletto heel. Fishnet stockings, seamed hosiery, and stockings and garter belts (suspenders) are also popular accents in the representation and attire of dominatrices, to emphasize the form and length of their legs, with erotic connotation.

Tight, leather corsets are another staple garment of the dominatrix signification. Gloves, whether long opera gloves or fingerless gloves, are often a further accessory to emphasize the feminine role. Neck corsets may also be worn.

Dominatrices often wear clothing made from fetish fashion materials. Examples include PVC clothing, latex clothing and garments drawn from the leather subculture. In some cases elements of dominatrix attire, such as leather boots and peaked cap, are drawn from Nazi chic, particularly the black SS officer's uniform which has been widely adopted and fetishized by underground gay and BDSM lifestyle groups to satisfy a uniform fetish.

The body language of the dominatrix is frequently represented by the use of strong, dominant body-language which is comparable to the dominant posturing in the animal world. The props she may brandish will strongly signify her role as dominatrix, such as bearing a flogger, whip or riding crop as illustrated in the artwork of Bruno Zach in the early 20th century, in conventional representation.

Literature on the Dominatrix has been around since the 10th century. Canoness Hroswitha, in her manuscript "Maria," uses the word "Dominatrix" for the main character. She is portrayed as an unattainable woman who is too good for any of the men who are in love with her. The theme of "the unattainable woman" has been used thoroughly in medieval literature as well, although it differs from a dominatrix. Medieval themes surrounding the unattainable woman concerned issues of social classes and structure, with chivalry being a prime part of a relationship between a man and woman. There are some exceptions to this trend during medieval times. In Cervantes’ "Don Quixote" (1605), Celadon is imprisoned by Galatea. Celadon complains that his “mistress . . . Galatea keeps me on such a short leash”. Robert Herrick published in 1648, Hesperides. In it there were three revealing poems "An Hymne to Love", "The Dream", and "To Love" which showcase masculine longing for domination, restraint, discipline. In "Ulysses" by James Joyce, the character Bloom has many fantasies of submission to a lady and to receive whippings by her. Female domination has been explored in literature tracing back as far as the 10th century.

Depictions of dominatrices in popular culture include:




Female domination is also depicted in the novels "Love Story in the Style of Femdom" (2018) and "Our Wild Sex in Malindi" (2020) by the Russian writer Andrei Gusev.





</doc>
<doc id="8752" url="https://en.wikipedia.org/wiki?curid=8752" title="Flag of Denmark">
Flag of Denmark

The flag of Denmark (, ) is red with a white Scandinavian cross that extends to the edges of the flag; the vertical part of the cross is shifted to the hoist side.

A banner with a white-on-red cross is attested as having been used by the kings of Denmark since the 14th century. An origin legend with considerable impact on Danish national historiography connects the introduction of the flag to the Battle of Lindanise of 1219. 
The elongated Nordic cross reflects the use as a maritime flag in the 18th century. The flag became popular as a national flag in the early 19th century. Its private use was outlawed in 1834, and again permitted in a regulation of 1854. The flag holds the world record of being the oldest continuously used national flag.

In 1748, a regulation defined the correct lengths of the two last fields in the flag as .
In May 1893 a new regulation to all chiefs of police, stated that the police should not intervene, if the two last fields in the flag were longer than as long as these did not exceed , and provided that this was the only rule violated.
This regulation is still in effect today and thus the legal proportions of the National flag is today 3:1:3 in width and anywhere between 3:1:4.5 and 3:1:5.25 in length.

No official definition of "Dannebrog rød" exists. The private company "Dansk Standard", regulation number 359 (2005), defines the red colour of the flag as Pantone 186c.

A tradition recorded in the 16th century traces the origin of the flag to the campaigns of Valdemar II of Denmark (r. 1202–1241). The oldest of them is in Christiern Pedersen's ""Danske Krønike"", which is a sequel to Saxo's Gesta Danorum, written 1520–23. Here, the flag falls from the sky during a Russian campaign of Valdemar's. Pedersen also states that the very same flag was taken into exile by Eric of Pomerania in 1440.

The second source is the writing of the Franciscan friar Petrus Olai (Peder Olsen) of Roskilde (died c. 1570). This record describes a battle in 1208 near Fellin during the Estonia campaign of King Valdemar II. The Danes were all but defeated when a lamb-skin banner depicting a white cross fell from the sky and miraculously led to a Danish victory. In a third account, also by Petrus Olai, in "Danmarks Tolv Herligheder" ("Twelve Splendours of Denmark"), in splendour number nine, the same story is re-told almost verbatim, with a paragraph inserted correcting the year to 1219. Now, the flag is falling from the sky in the Battle of Lindanise, also known as the Battle of Valdemar (Danish: "Volmerslaget"), near Lindanise (Tallinn) in Estonia, of 15 June 1219.

It is this third account that has been the most influential, and some historians have treated it as the primary account taken from a (lost) source dating to the first half of the 15th century.

In Olai's account, the battle was going badly, and defeat seemed imminent. However the Danish Bishop Anders Sunesen on top of a hill overlooking the battle prayed to God with his arms raised, which meant that the Danes moved closer to victory the more he prayed. When he raised his arms the Danes surged forward and when his arms grew tired and he let them fall, the Estonians turned the Danes back. Attendants rushed forward to raise his arms once again and the Danes surged forward again. At a second he was so tired in his arms that he dropped them and the Danes then lost the advantage and were moving closer to defeat. He needed two soldiers to keep his hands up and when the Danes were about to lose, 'Dannebrog' miraculously fell from the sky and the King took it, showed it to the troops and their hearts were filled with courage and the Danes won the battle.

The possible historical nucleus behind this origin legend was extensively discussed by Danish historians in the 19th to 20th centuries. Jørgensen (1875) argues that Bishop Theoderich was the original instigator of the 1218 inquiry from Bishop Albert of Buxhoeveden to King Valdemar II which led to the Danish participation in the Baltic crusades. Jørgensen speculates that Bishop Theoderich might have carried the Knight Hospitaller's banner in the 1219 battle and that "the enemy thought this was the King's symbol and mistakenly stormed Bishop Theoderich tent. He claims that the origin of the legend of the falling flag comes from this confusion in the battle."

The Danish church-historian L. P. Fabricius (1934) ascribes the origin to the 1208 Battle of Fellin, not the Battle of Lindanise in 1219, based on the earliest source available about the story. Fabricius speculated that it might have been Archbishop Andreas Sunesøn's personal ecclesiastical banner or perhaps even the flag of Archbishop Absalon, under whose initiative and supervision several smaller crusades had already been conducted in Estonia. The banner would then already be known in Estonia. Fabricius repeats Jørgensen's idea about the flag being planted in front of Bishop Theodorik's tent, which the enemy mistakenly attacks believing it to be the tent of the King.

A different theory is briefly discussed by Fabricius and elaborated more by Helge Bruhn (1949). Bruhn interprets the story in the context of the widespread tradition of the miraculous appearance of crosses in the sky in Christian legend, specifically comparing such an event attributed to a battle of 10 September 1217 near Alcazar, where it is said that a golden cross on white appeared in the sky, to bring victory to the Christians.

In Swedish national historiography of the 18th century, there is a tale paralleling the Danish legend, in which 
a golden cross appears in the blue sky during a Swedish battle in Finland in 1157.

The white-on-red cross emblem originates in the age of the Crusades. In the 12th century, it was also used as war flag by the Holy Roman Empire.

In the "Gelre Armorial", dated 1340–1370, such a banner is shown alongside the coat of arms of the king of Denmark. This is the earliest known undisputed colour rendering of the Dannebrog. At about the same time, Valdemar IV of Denmark displays a cross in his coat of arms on his "Danælog" seal ("Rettertingsseglet", dated 1356). The image from the Armorial Gelre is nearly identical to an image found in a 15th-century coats of arms book now located in the National Archives of Sweden ("Riksarkivet"). The seal of Eric of Pomerania (1398) as king of the Kalmar union displays the arms of Denmark chief dexter, three lions. In this version, the lions are holding a Dannebrog banner.

The reason why the kings of Denmark in the 14th century begin displaying the cross banner in their coats of arms is unknown. Caspar Paludan-Müller (1873) suggested that it may reflect a banner sent by the pope to the Danish king in support of the Baltic countries. Adolf Ditlev Jørgensen (1875) identifies the banner as that of the Knights Hospitaller, which order had a presence in Denmark from the later 12th century.

Several coins, seals and images exist, both foreign and domestic, from the 13th to 15th centuries and even earlier, showing heraldic designs similar to Dannebrog, alongside the royal coat of arms (three blue lions on a golden shield.)

There is a record suggesting that the Danish army had a "chief banner" ("hoffuitbanner") in the early 16th century. Such a banner is mentioned in 1570 by Niels Hemmingsøn in the context of a 1520 battle between Danes and Swedes near Uppsala as nearly captured by the Swedes but saved by the heroic actions of the banner-carrier Mogens Gyldenstierne and Peder Skram. The legend attributing the miraculous origin of the flag to the campaigns of Valdemar II of Denmark (r. 1202–1241) were recorded by Christiern Pedersen and Petrus Olai in the 1520s.

Hans Svaning's "History of King Hans" from 1558–1559 and Johan Rantzau's "History about the Last Dithmarschen War", from 1569, record the further fate of the Danish "hoffuitbanner": According to this tradition, the original flag from the Battle of Lindanise was used in the small campaign of 1500 when King Hans tried to conquer Dithmarschen (in western Holstein in north Germany). The flag was lost in a devastating defeat at the Battle of Hemmingstedt on 17 February 1500. In 1559, King Frederik II recaptured it during his own Dithmarschen campaign.

In 1576, the son of "Johan Rantzau", Henrik Rantzau, also writes about the war and the fate of the flag, noting that the flag was in a poor condition when returned. He records that the flag after its return to Denmark was placed in the cathedral in Slesvig. Slesvig historian Ulrik Petersen (1656–1735) confirms the presence of such a banner in the cathedral in the early 17th century, and records that it had crumbled away by about 1660.

Contemporary records describing the battle of Hemmingstedt make no reference to the loss of the original Dannebrog, although the capitulation state that all Danish banners lost in 1500 were to be returned. In a letter dated 22 February 1500 to Oluf Stigsøn, King John describes the battle, but does not mention the loss of an important flag. In fact, the entire letter gives the impression that the lost battle was of limited importance. In 1598, Neocorus wrote that the banner captured in 1500 was brought to the church in Wöhrden and hung there for the next 59 years, until it was returned to the Danes as part of the peace settlement in 1559.

Used as maritime flag since the 16th century, the Dannebrog was introduced as regimental flag in the Danish army in 1785, and for the militia (landeværn) in 1801. From 1842, it was used as the flag of the entire army.

In parallel to the development of Romantic nationalism in other European countries, the military flag increasingly came to be seen as representing the nation itself during the first half of the 19th century. Poems of this period invoking the "Dannebrog" were written by B.S. Ingemann, N.F.S. Grundtvig, Oehlenschläger, Chr. Winther and H.C. Andersen. By the 1830s, the military flag had become so popular as unofficial national flag, and its use by private citizens was outlawed in a circular enacted on 7 January 1834.

In the national enthusiasm sparked by the First Schleswig War during 1848–1850, the flag was still very widely displayed, and the prohibition of private use was again repealed in a regulation of 7 July 1854, for the first time allowing Danish citizens to display the Dannebrog (but not the swallow-tailed "Splitflag" variant). Special permission to use the "Splitflag" was given to individual institutions and private companies, especially after 1870. On 10 April 1915, the hoisting of any "other" flag on Danish soil was prohibited. In 1886, the war ministry introduced a regulation indicating that the flag should be flown from military buildings on thirteen specified days, including royal birthdays, the date of the signing of the Constitution of 5 June 1849 and on days of remembrance for military battles. In 1913, the naval ministry issued its own list of flag days. From 1939 until 2012, the yearbook "Hvem-Hvad-Hvor" included a list of flag days. As of 2019 flag days can be viewed at the "Ministry of Justice (Justitsministeriet)" as well as "The Denmark Society (Danmarks-Samfundet)".

The size and shape of the civil ensign (""Koffardiflaget"") for merchant ships is given in the regulation of 11 June 1748, which says: "A red flag with a white cross with no split end. The white cross must be of the flag's height. The two first fields must be square in form and the two outer fields must be lengths of those". The proportions are thus: 3:1:3 vertically and 3:1:4.5 horizontally. This definition are the absolute proportions for the Danish national flag to this day, for both the civil version of the flag (""Stutflaget""), as well as the merchant flag (""Handelsflaget""). The civil flag and the merchant flag are identical in colour and design.

A regulation passed in 1758 required Danish ships sailing in the Mediterranean to carry the royal cypher in the center of the flag in order to distinguish them from Maltese ships, due to the similarity of the flag of the Sovereign Military Order of Malta.

According to the regulation of 11 June 1748 the colour was simply red, which is common known today as "Dannebrog rød" (""Dannebrog red""). The only available red fabric dye in 1748 was made of madder root, which can be processed to produce a brilliant red dye (used historically for British soldiers' jackets). A regulation of 4 May 1927 once again states that Danish merchant ships have to fly flags according to the regulation of 1748.

The first regulation regarding the "Splitflag" dates from 27 March 1630, in which King Christian IV orders that Norwegian "Defensionskibe" (armed merchants ships) may only use the "Splitflag" if they are in Danish war service. In 1685 an order, distributed to a number of cities in Slesvig, states that all ships must carry the Danish flag, and in 1690 all merchant ships are forbidden to use the "Splitflag", with the exception of ships sailing in the East Indies, West Indies and along the coast of Africa. In 1741 it is confirmed that the regulation of 1690 is still very much in effect; that merchant ships may not use the "Splitflag". At the same time the Danish East India Company is allowed to fly the "Splitflag" when past the equator.

Some confusion must have existed regarding the "Splitflag". In 1696 the Admiralty presented the King with a proposal for a standard regulating both size and shape of the "Splitflag". In the same year a royal resolution defines the proportions of the "Splitflag", which in this resolution is called "Kongeflaget" (the King's flag), as follows: "The cross must be of the flags height. The two first fields must be square in form with the sides three times the cross width. The two outer fields are rectangular and the length of the square fields. The tails are the length of the flag".

These numbers are the basic for the "Splitflag", or "Orlogsflag", today, though the numbers have been slightly altered. The term "Orlogsflag" dates from 1806 and denotes use in the Danish Navy.

From about 1750 to the early 19th century, a number of ships and companies which the government has interests in, received approval to use the "Splitflag".

In the royal resolution of 25 October 1939 for the Danish Navy, it is stated that the "Orlogsflag" is a "Splitflag" with a deep red (""dybrød"") or madder red (""kraprød"") colour. Like the National flag, no nuance is given, but in modern days this is given as 195U. Furthermore, the size and shape is corrected in this resolution to be: "The cross must be of the flag's height. The two first fields must be square in form with the height of of the flag's height. The two outer fields are rectangular and the length of the square fields. The tails are the length of the rectangular fields". Thus, if compared to the standard of 1696, both the rectangular fields and the tails have decreased in size.

The "Splitflag" and "Orlogsflag" have similar shapes but different sizes and shades of red. Legally, they are two different flags. The "Splitflag" is a Danish flag ending in a swallow-tail, it is "Dannebrog red", and is used on land. The "Orlogsflag" is an elongated "Splitflag" with a deeper red colour and is only used on sea.

The "Orlogsflag" with no markings, may only be used by the Royal Danish Navy. There are though a few exceptions to this. A few institutions have been allowed to fly the clean "Orlogsflag". The same flag with markings has been approved for a few dozen companies and institutions over the years.

Furthermore, the "Orlogsflag" is only described as such if it has no additional markings. Any swallow-tail flag, no matter the color, is called a "Splitflag" provided it bears additional markings.

The current version of the royal standard was introduced on 16 November 1972 when the Queen adopted a new version of her personal coat of arms. The royal standard is the flag of Denmark with a swallow-tail and charged with the monarch's coat of arms set in a white square. The centre square is 32 parts in a flag with the ratio 56:107.

Greenland and the Faroe Islands are additional autonomous territories within the Kingdom of Denmark. They have their own official flags.

Some areas in Denmark have unofficial flags, listed below. The regional flags of Bornholm and Ærø are known to be in active use. The flags of Vendsyssel (Vendelbrog) and the Jutlandic flag ("Den jyske fane") are obscure. None of these flags have legal recognition in Denmark, and are officially considered to be "fantasy flags". Denmark reserves official recognition to official flags and regional flags ("områdeflag") from other jurisdictions.




</doc>
<doc id="8753" url="https://en.wikipedia.org/wiki?curid=8753" title="Dharma">
Dharma

Dharma (; , ; (Pali : "dhamma") is a key concept with multiple meanings in Indian religions, such as Hinduism, Buddhism, Jainism, Sikhism and others. There is no single-word translation for "dharma" in Western languages.

In Hinduism, dharma signifies behaviours that are considered to be in accord with "Ṛta", the order that makes life and universe possible, and includes duties, rights, laws, conduct, virtues and "right way of living". In Buddhism, dharma means "cosmic law and order", as applied to the teachings of Buddha and can be applied to mental constructs or what is cognised by the mind. In Buddhist philosophy, "dhamma/dharma" is also the term for "phenomena". Dharma in Jainism refers to the teachings of "Tirthankara" ("Jina") and the body of doctrine pertaining to the purification and moral transformation of human beings. For Sikhs, dharma means the path of righteousness and proper religious practice.

The concept of dharma was already in use in the historical Vedic religion, and its meaning and conceptual scope has evolved over several millennia. The ancient Tamil moral text of "Tirukkural" is solely based on "aṟam", the Tamil term for dharma. The antonym of dharma is "adharma".

The Classical Sanskrit noun "dharma" (धर्म) or the Prakrit Dhaṃma (𑀥𑀁𑀫) are a derivation from the root "dhṛ", which means "to hold, maintain, keep". Hence, "dharma" holds one from falling down or falling to hell. Therefore, it takes the meaning of "what is established or firm", and hence "law". It is derived from an older Vedic Sanskrit "n"-stem "dharman-", with a literal meaning of "bearer, supporter", in a religious sense conceived as an aspect of Rta.

In the Rigveda, the word appears as an "n"-stem, ', with a range of meanings encompassing "something established or firm" (in the literal sense of prods or poles). Figuratively, it means "sustainer" and "supporter" (of deities). It is semantically similar to the Greek "Themis" ("fixed decree, statute, law"). In Classical Sanskrit, the noun becomes thematic: '.

The word "dharma" derives from Proto-Indo-European root 

In Classical Sanskrit, and in the Vedic Sanskrit of the Atharvaveda, the stem is thematic: "" (Devanāgarī: धर्म). In Prakrit and Pāli, it is rendered "dhamma". In some contemporary Indian languages and dialects it alternatively occurs as "dharm".

When the Mauryan Emperor Ashoka wanted in the 3rd century BCE to translate the word "Dharma" (he used Prakrit word "Dhaṃma") into Greek and Aramaic, he used the Greek word "Eusebeia" (εὐσέβεια, piety, spiritual maturity, or godliness) in the Kandahar Bilingual Rock Inscription and the Kandahar Greek Edicts, and the Aramaic word "Qsyt" ("Truth") in the Kandahar Bilingual Rock Inscription.

Dharma is a concept of central importance in Indian philosophy and religion. It has multiple meanings in Hinduism, Buddhism, Sikhism and Jainism. It is difficult to provide a single concise definition for "dharma", as the word has a long and varied history and straddles a complex set of meanings and interpretations. There is no equivalent single-word synonym for "dharma" in western languages.

There have been numerous, conflicting attempts to translate ancient Sanskrit literature with the word "dharma" into German, English and French. The concept, claims Paul Horsch, has caused exceptional difficulties for modern commentators and translators. For example, while Grassmann's translation of Rig-veda identifies seven different meanings of dharma, Karl Friedrich Geldner in his translation of the Rig-veda employs 20 different translations for dharma, including meanings such as "law", "order", "duty", "custom", "quality", and "model", among others. However, the word "dharma" has become a widely accepted loanword in English, and is included in all modern unabridged English dictionaries.

The root of the word "dharma" is "dhri", which means "to support, hold, or bear". It is the thing that regulates the course of change by not participating in change, but that principle which remains constant. Monier-Williams, the widely cited resource for definitions and explanation of Sanskrit words and concepts of Hinduism, offers numerous definitions of the word "dharma", such as that which is established or firm, steadfast decree, statute, law, practice, custom, duty, right, justice, virtue, morality, ethics, religion, religious merit, good works, nature, character, quality, property. Yet, each of these definitions is incomplete, while the combination of these translations does not convey the total sense of the word. In common parlance, "dharma" means "right way of living" and "path of rightness".

The meaning of the word "dharma" depends on the context, and its meaning has evolved as ideas of Hinduism have developed through history. In the earliest texts and ancient myths of Hinduism, "dharma" meant cosmic law, the rules that created the universe from chaos, as well as rituals; in later Vedas, Upanishads, Puranas and the Epics, the meaning became refined, richer, and more complex, and the word was applied to diverse contexts.
In certain contexts, "dharma" designates human behaviours considered necessary for order of things in the universe, principles that prevent chaos, behaviours and action necessary to all life in nature, society, family as well as at the individual level. "Dharma" encompasses ideas such as duty, rights, character, vocation, religion, customs and all behaviour considered appropriate, correct or morally upright.

The antonym of "dharma" is "adharma" (Sanskrit: अधर्म), meaning that which is "not dharma". As with "dharma", the word "adharma" includes and implies many ideas; in common parlance, adharma means that which is against nature, immoral, unethical, wrong or unlawful.

In Buddhism, "dharma" incorporates the teachings and doctrines of the founder of Buddhism, the Buddha.

According to Pandurang Vaman Kane, author of the authoritative book "History of Dharmasastra", the word "dharma" appears at least fifty-six times in the hymns of the Rigveda, as an adjective or noun. According to Paul Horsch, the word "dharma" has its origin in the myths of Vedic Hinduism. The hymns of the Rig Veda claim Brahman created the universe from chaos, they hold (dhar-) the earth and sun and stars apart, they support (dhar-) the sky away and distinct from earth, and they stabilise (dhar-) the quaking mountains and plains. The gods, mainly Indra, then deliver and hold order from disorder, harmony from chaos, stability from instability – actions recited in the Veda with the root of word dharma. In hymns composed after the mythological verses, the word dharma takes expanded meaning as a cosmic principle and appears in verses independent of gods. It evolves into a concept, claims Paul Horsch, that has a dynamic functional sense in Atharvaveda for example, where it becomes the cosmic law that links cause and effect through a subject. Dharma, in these ancient texts, also takes a ritual meaning. The ritual is connected to the cosmic, and "dharmani" is equated to ceremonial devotion to the principles that gods used to create order from disorder, the world from chaos. Past the ritual and cosmic sense of dharma that link the current world to mythical universe, the concept extends to ethical-social sense that links human beings to each other and to other life forms. It is here that dharma as a concept of law emerges in Hinduism.

Dharma and related words are found in the oldest Vedic literature of Hinduism, in later Vedas, Upanishads, Puranas, and the Epics; the word dharma also plays a central role in the literature of other Indian religions founded later, such as Buddhism and Jainism. According to Brereton, "Dharman" occurs 63 times in Rig-veda; in addition, words related to Dharman also appear in Rig-veda, for example once as dharmakrt, 6 times as "satyadharman", and once as "dharmavant", 4 times as "dharman" and twice as "dhariman".

Indo-European parallels for "Dharma" are known, but the only Iranian equivalent is Old Persian "darmān" "remedy", the meaning of which is rather removed from Indo-Aryan "dhárman", suggesting that the word "Dharma" did not have a major role in the Indo-Iranian period, and was principally developed more recently under the Vedic tradition. However, it is thought that the "Daena" of Zoroastrianism, also meaning the "eternal Law" or "religion", is related to Sanskrit "Dharma". Ideas in parts overlapping to "Dharma" are found in other ancient cultures: such as Chinese Tao, Egyptian Maat, Sumerian Me.

In the mid-20th century, an inscription of the Indian Emperor Asoka from the year 258 BC was discovered in Afghanistan, the Kandahar Bilingual Rock Inscription. This rock inscription contains Greek and Aramaic text. According to , on the rock appears a Greek rendering for the Sanskrit word dharma: the word eusebeia. Scholars of Hellenistic Greece explain eusebeia as a complex concept. Eusebia means not only to venerate gods, but also spiritual maturity, a reverential attitude toward life, and includes the right conduct toward one's parents, siblings and children, the right conduct between husband and wife, and the conduct between biologically unrelated people. This rock inscription, concludes Paul Hacker, suggests dharma in India, about 2300 years ago, was a central concept and meant not only religious ideas, but ideas of right, of good, of one's duty toward the human community.

The evolving literature of Hinduism linked "dharma" to two other important concepts: "Ṛta" and "Māyā". Ṛta in Vedas is the truth and cosmic principle which regulates and coordinates the operation of the universe and everything within it. Māyā in Rig-veda and later literature means illusion, fraud, deception, magic that misleads and creates disorder, thus is contrary to reality, laws and rules that establish order, predictability and harmony. Paul Horsch suggests Ṛta and dharma are parallel concepts, the former being a cosmic principle, the latter being of moral social sphere; while Māyā and dharma are also correlative concepts, the former being that which corrupts law and moral life, the later being that which strengthens law and moral life.

Day proposes dharma is a manifestation of Ṛta, but suggests Ṛta may have been subsumed into a more complex concept of dharma, as the idea developed in ancient India over time in a nonlinear manner. The following verse from the Rigveda is an example where "rta" and dharma are linked:

Dharma is an organising principle in Hinduism that applies to human beings in solitude, in their interaction with human beings and nature, as well as between inanimate objects, to all of cosmos and its parts. It refers to the order and customs which make life and universe possible, and includes behaviours, rituals, rules that govern society, and ethics. Hindu dharma includes the religious duties, moral rights and duties of each individual, as well as behaviours that enable social order, right conduct, and those that are virtuous. Dharma, according to Van Buitenen, is that which all existing beings must accept and respect to sustain harmony and order in the world. It is neither the act nor the result, but the natural laws that guide the act and create the result to prevent chaos in the world. It is innate characteristic, that makes the being what it is. It is, claims Van Buitenen, the pursuit and execution of one's nature and true calling, thus playing one's role in cosmic concert. In Hinduism, it is the dharma of the bee to make honey, of cow to give milk, of sun to radiate sunshine, of river to flow. In terms of humanity, dharma is the need for, the effect of and essence of service and interconnectedness of all life.

In its true essence, dharma means for a Hindu to "expand the mind". Furthermore, it represents the direct connection between the individual and the societal phenomena that bind the society together. In the way societal phenomena affect the conscience of the individual, similarly may the actions of an individual alter the course of the society, for better or for worse. This is been subtly echoed by the credo धर्मो धारयति प्रजा: meaning dharma is that which holds and provides support to the social construct.

In Hinduism, "dharma" includes two aspects – "sanātana dharma", which is the overall, unchanging and abiding principals of dharma which are not subject to change, and "yuga dharma", which is valid for a yuga, an epoch or age as established by Hindu tradition and thus may change at the conclusion of its time.

The history section of this article discusses the development of dharma concept in Vedas. This development continued in the Upanishads and later ancient scripts of Hinduism. In Upanishads, the concept of dharma continues as universal principle of law, order, harmony, and truth. It acts as the regulatory moral principle of the Universe. It is explained as law of righteousness and equated to "satya" (Sanskrit: सत्यं, truth), in hymn 1.4.14 of Brhadaranyaka Upanishad, as follows:

The Hindu religion and philosophy, claims Daniel Ingalls, places major emphasis on individual practical morality. In the Sanskrit epics, this concern is omnipresent.

In the Second Book of Ramayana, for example, a peasant asks the King to do what dharma morally requires of him, the King agrees and does so even though his compliance with the law of dharma costs him dearly. Similarly, dharma is at the centre of all major events in the life of Rama, Sita, and Lakshman in Ramayana, claims Daniel Ingalls. Each episode of Ramayana presents life situations and ethical questions in symbolic terms. The issue is debated by the characters, finally the right prevails over wrong, the good over evil. For this reason, in Hindu Epics, the good, morally upright, law-abiding king is referred to as "dharmaraja".

In Mahabharata, the other major Indian epic, similarly, dharma is central, and it is presented with symbolism and metaphors. Near the end of the epic, the god Yama, referred to as dharma in the text, is portrayed as taking the form of a dog to test the compassion of Yudhishthira, who is told he may not enter paradise with such an animal, but refuses to abandon his companion, for which decision he is then praised by dharma. The value and appeal of the Mahabharata is not as much in its complex and rushed presentation of metaphysics in the 12th book, claims Ingalls, because Indian metaphysics is more eloquently presented in other Sanskrit scriptures; the appeal of Mahabharata, like Ramayana, is in its presentation of a series of moral problems and life situations, to which there are usually three answers given, according to Ingalls: one answer is of Bhima, which is the answer of brute force, an individual angle representing materialism, egoism, and self; the second answer is of Yudhishthira, which is always an appeal to piety and gods, of social virtue and of tradition; the third answer is of introspective Arjuna, which falls between the two extremes, and who, claims Ingalls, symbolically reveals the finest moral qualities of man. The Epics of Hinduism are a symbolic treatise about life, virtues, customs, morals, ethics, law, and other aspects of dharma. There is extensive discussion of dharma at the individual level in the Epics of Hinduism, observes Ingalls; for example, on free will versus destiny, when and why human beings believe in either, ultimately concluding that the strong and prosperous naturally uphold free will, while those facing grief or frustration naturally lean towards destiny. The Epics of Hinduism illustrate various aspects of dharma, they are a means of communicating dharma with metaphors.

According to Klaus Klostermaier, 4th century Hindu scholar Vātsyāyana explained dharma by contrasting it with adharma. Vātsyāyana suggested that dharma is not merely in one's actions, but also in words one speaks or writes, and in thought. According to Vātsyāyana:


In the "Yoga Sutras" of Patanjali the dharma is real; in the Vedanta it is unreal.
Dharma is part of yoga, suggests Patanjali; the elements of Hindu dharma are the attributes, qualities and aspects of yoga. Patanjali explained dharma in two categories: "yamas" (restraints) and "niyamas" (observances).

The five yamas, according to Patanjali, are: abstain from injury to all living creatures, abstain from falsehood (satya), abstain from unauthorised appropriation of things-of-value from another (acastrapurvaka), abstain from coveting or sexually cheating on your partner, and abstain from expecting or accepting gifts from others. The five yama apply in action, speech and mind. In explaining yama, Patanjali clarifies that certain professions and situations may require qualification in conduct. For example, a fisherman must injure a fish, but he must attempt to do this with least trauma to fish and the fisherman must try to injure no other creature as he fishes.

The five niyamas (observances) are cleanliness by eating pure food and removing impure thoughts (such as arrogance or jealousy or pride), contentment in one's means, meditation and silent reflection regardless of circumstances one faces, study and pursuit of historic knowledge, and devotion of all actions to the Supreme Teacher to achieve perfection of concentration.

Dharma is an empirical and experiential inquiry for every man and woman, according to some texts of Hinduism. For example, Apastamba Dharmasutra states:

In other texts, three sources and means to discover dharma in Hinduism are described. These, according to , are: First, learning historical knowledge such as Vedas, Upanishads, the Epics and other Sanskrit literature with the help of one's teacher. Second, observing the behaviour and example of good people. The third source applies when neither one's education nor example exemplary conduct is known. In this case, "atmatusti" is the source of dharma in Hinduism, that is the good person reflects and follows what satisfies his heart, his own inner feeling, what he feels driven to.

Some texts of Hinduism outline "dharma" for society and at the individual level. Of these, the most cited one is "Manusmriti", which describes the four "Varnas", their rights and duties. Most texts of Hinduism, however, discuss "dharma" with no mention of "Varna" (caste). Other dharma texts and Smritis differ from Manusmriti on the nature and structure of Varnas. Yet, other texts question the very existence of varna. Bhrigu, in the Epics, for example, presents the theory that dharma does not require any varnas. In practice, medieval India is widely believed to be a socially stratified society, with each social strata inheriting a profession and being endogamous. Varna was not absolute in Hindu dharma; individuals had the right to renounce and leave their Varna, as well as their asramas of life, in search of moksa. While neither Manusmriti nor succeeding Smritis of Hinduism ever use the word varnadharma (that is, the dharma of varnas), or varnasramadharma (that is, the dharma of varnas and asramas), the scholarly commentary on Manusmriti use these words, and thus associate dharma with varna system of India. In 6th century India, even Buddhist kings called themselves "protectors of varnasramadharma" – that is, dharma of varna and asramas of life.

At the individual level, some texts of Hinduism outline four āśramas, or stages of life as individual's dharma. These are: (1) brahmacārya, the life of preparation as a student, (2) gṛhastha, the life of the householder with family and other social roles, (3) vānprastha or aranyaka, the life of the forest-dweller, transitioning from worldly occupations to reflection and renunciation, and (4) sannyāsa, the life of giving away all property, becoming a recluse and devotion to moksa, spiritual matters.

The four stages of life complete the four human strivings in life, according to Hinduism. Dharma enables the individual to satisfy the striving for stability and order, a life that is lawful and harmonious, the striving to do the right thing, be good, be virtuous, earn religious merit, be helpful to others, interact successfully with society. The other three strivings are Artha – the striving for means of life such as food, shelter, power, security, material wealth, and so forth; Kama – the striving for sex, desire, pleasure, love, emotional fulfilment, and so forth; and Moksa – the striving for spiritual meaning, liberation from life-rebirth cycle, self-realisation in this life, and so forth. The four stages are neither independent nor exclusionary in Hindu dharma.

Dharma being necessary for individual and society, is dependent on poverty and prosperity in a society, according to Hindu dharma scriptures. For example, according to Adam Bowles, Shatapatha Brahmana 11.1.6.24 links social prosperity and "dharma" through water. Waters come from rains, it claims; when rains are abundant there is prosperity on the earth, and this prosperity enables people to follow Dharma – moral and lawful life. In times of distress, of drought, of poverty, everything suffers including relations between human beings and the human ability to live according to dharma.

In Rajadharmaparvan 91.34-8, the relationship between poverty and dharma reaches a full circle. A land with less moral and lawful life suffers distress, and as distress rises it causes more immoral and unlawful life, which further increases distress. Those in power must follow the raja dharma (that is, dharma of rulers), because this enables the society and the individual to follow dharma and achieve prosperity.

The notion of "dharma" as duty or propriety is found in India's ancient legal and religious texts. Common examples of such use are Pitri Dharma (meaning a person's duty as a father), Putra Dharma (a person's duty as a son), Raj Dharma (a person's duty as a king) and so forth. In Hindu philosophy, justice, social harmony, and happiness requires that people live per dharma. The Dharmashastra is a record of these guidelines and rules. The available evidence suggest India once had a large collection of dharma related literature (sutras, shastras); four of the sutras survive and these are now referred to as Dharmasutras. Along with laws of Manu in Dharmasutras, exist parallel and different compendium of laws, such as the laws of Narada and other ancient scholars. These different and conflicting law books are neither exclusive, nor do they supersede other sources of dharma in Hinduism. These Dharmasutras include instructions on education of the young, their rites of passage, customs, religious rites and rituals, marital rights and obligations, death and ancestral rites, laws and administration of justice, crimes, punishments, rules and types of evidence, duties of a king, as well as morality.

In Buddhism "dharma" means cosmic law and order, but is also applied to the teachings of the Buddha. In Buddhist philosophy, "dhamma/dharma" is also the term for "phenomena".

For practising Buddhists, references to "dharma" ("dhamma" in Pali) particularly as "the Dharma", generally means the teachings of the Buddha, commonly known throughout the East as Buddha-Dharma. It includes especially the discourses on the fundamental principles (such as the Four Noble Truths and the Noble Eightfold Path), as opposed to the parables and to the poems.

The status of Dharma is regarded variably by different Buddhist traditions. Some regard it as an ultimate truth, or as the fount of all things which lie beyond the "three realms" (Sanskrit: "tridhatu") and the "wheel of becoming" (Sanskrit: "bhavachakra"), somewhat like the pagan Greek and Christian logos: this is known as "Dharmakaya" (Sanskrit). Others, who regard the Buddha as simply an enlightened human being, see the Dharma as the essence of the "84,000 different aspects of the teaching" (Tibetan: "chos-sgo brgyad-khri bzhi strong") that the Buddha gave to various types of people, based upon their individual propensities and capabilities.

Dharma refers not only to the sayings of the Buddha, but also to the later traditions of interpretation and addition that the various schools of Buddhism have developed to help explain and to expand upon the Buddha's teachings. For others still, they see the Dharma as referring to the "truth", or the ultimate reality of "the way that things really are" (Tibetan: "Chö").

The Dharma is one of the Three Jewels of Buddhism in which practitioners of Buddhism seek refuge, or that upon which one relies for his or her lasting happiness. The Three Jewels of Buddhism are the Buddha, meaning the mind's perfection of enlightenment, the Dharma, meaning the teachings and the methods of the Buddha, and the Sangha, meaning the monastic community who provide guidance and support to followers of the Buddha.

Dharma is employed in Ch'an in a specific context in relation to transmission of authentic doctrine, understanding and bodhi; recognised in Dharma transmission.

In Theravada Buddhism obtaining ultimate realisation of the dhamma is achieved in three phases; learning, practising and realising.

In Pali 

The word Dharma in Jainism is found in all its key texts. It has a contextual meaning and refers to a number of ideas. In the broadest sense, it means the teachings of the Jinas, or teachings of any competing spiritual school, a supreme path, socio-religious duty, and that which is the highest "mangala" (holy).

The major Jain text, "Tattvartha Sutra" mentions "Das-dharma" with the meaning of "ten righteous virtues". These are forbearance, modesty, straightforwardness, purity, truthfulness, self-restraint, austerity, renunciation, non-attachment, and celibacy. Acārya Amṛtacandra, author of the Jain text, "Puruṣārthasiddhyupāya" writes:
The term "dharmastikaay" also has a specific ontological and soteriological meaning in Jainism, as a part of its theory of six dravya (substance or a reality). In the Jain tradition, existence consists of "jiva" (soul, atman) and "ajiva" (non-soul), the latter consisting of five categories: inert non-sentient atomic matter (pudgalastikaay), space (akasha), time (kala), principle of motion (dharmastikaay), and principle of rest (adharmastikaay). The use of the term "dharmastikaay" to mean motion and to refer to an ontological sub-category is peculiar to Jainism, and not found in the metaphysics of Buddhism and various schools of Hinduism.

For Sikhs, the word "dharam" () means the path of righteousness and proper religious practice. Guru Granth Sahib in hymn 1353 connotes dharma as duty. The 3HO movement in Western culture, which has incorporated certain Sikh beliefs, defines Sikh Dharma broadly as all that constitutes religion, moral duty and way of life.

The importance of "dharma" to Indian sentiments is illustrated by India's decision in 1947 to include the Ashoka Chakra, a depiction of the "dharmachakra" (the "wheel of dharma"), as the central motif on its flag.





</doc>
<doc id="8756" url="https://en.wikipedia.org/wiki?curid=8756" title="Daniel Dennett">
Daniel Dennett

Daniel Clement Dennett III (born March 28, 1942) is an American philosopher, writer, and cognitive scientist whose research centers on the philosophy of mind, philosophy of science, and philosophy of biology, particularly as those fields relate to evolutionary biology and cognitive science.

As of 2017, he is the co-director of the Center for Cognitive Studies and the Austin B. Fletcher Professor of Philosophy at Tufts University. Dennett is an atheist and secularist, a member of the Secular Coalition for America advisory board, and a member of the Committee for Skeptical Inquiry, as well as an outspoken supporter of the Brights movement. Dennett is referred to as one of the "Four Horsemen of New Atheism", along with Richard Dawkins, Sam Harris, and the late Christopher Hitchens.

Dennett is a member of the editorial board for "The Rutherford Journal".

Dennett was born on March 28, 1942, in Boston, Massachusetts, the son of Ruth Marjorie (née Leck) and Daniel Clement Dennett Jr. Dennett spent part of his childhood in Lebanon, where, during World War II, his father was a covert counter-intelligence agent with the Office of Strategic Services posing as a cultural attaché to the American Embassy in Beirut. When he was five, his mother took him back to Massachusetts after his father died in an unexplained plane crash. Dennett's sister is the investigative journalist Charlotte Dennett. Dennett says that he was first introduced to the notion of philosophy while attending summer camp at age 11, when a camp counselor said to him, "You know what you are, Daniel? You're a philosopher."

Dennett graduated from Phillips Exeter Academy in 1959, and spent one year at Wesleyan University before receiving his Bachelor of Arts in philosophy at Harvard University in 1963. At Harvard University he was a student of W. V. Quine. In 1965, he received his Doctor of Philosophy in philosophy at the University of Oxford, where he studied under Gilbert Ryle and was a member of Hertford College. His dissertation was entitled "The Mind and the Brain: Introspective Description in the Light of Neurological Findings; Intentionality".
Dennett describes himself as "an autodidact—or, more properly, the beneficiary of hundreds of hours of informal tutorials on all the fields that interest me, from some of the world's leading scientists".

He is the recipient of a Fulbright Fellowship, two Guggenheim Fellowships, and a Fellowship at the Center for Advanced Study in the Behavioral Sciences. He is a Fellow of the Committee for Skeptical Inquiry and a Humanist Laureate of the International Academy of Humanism. He was named 2004 Humanist of the Year by the American Humanist Association. In 2006, Dennett received the Golden Plate Award of the American Academy of Achievement.

In February 2010, he was named to the Freedom From Religion Foundation's Honorary Board of distinguished achievers.

In 2012, he was awarded the Erasmus Prize, an annual award for a person who has made an exceptional contribution to European culture, society or social science, "for his ability to translate the cultural significance of science and technology to a broad audience."

In 2018, he was awarded an honorary degree by Radboud University, located in Nijmegen, Netherlands, for his contributions to and influence on cross-disciplinary science.

While he is a confirmed compatibilist on free will, in "On Giving Libertarians What They Say They Want"—chapter 15 of his 1978 book "Brainstorms"—Dennett articulated the case for a two-stage model of decision making in contrast to libertarian views.

While other philosophers have developed two-stage models, including William James, Henri Poincaré, Arthur Compton, and Henry Margenau, Dennett defends this model for the following reasons:

Leading libertarian philosophers such as Robert Kane have rejected Dennett's model, specifically that random chance is directly involved in a decision, on the basis that they believe this eliminates the agent's motives and reasons, character and values, and feelings and desires. They claim that, if chance is the primary cause of decisions, then agents cannot be liable for resultant actions. Kane says:

Dennett has remarked in several places (such as "Self-portrait", in "Brainchildren") that his overall philosophical project has remained largely the same since his time at Oxford. He is primarily concerned with providing a philosophy of mind that is grounded in empirical research. In his original dissertation, "Content and Consciousness", he broke up the problem of explaining the mind into the need for a theory of content and for a theory of consciousness. His approach to this project has also stayed true to this distinction. Just as "Content and Consciousness" has a bipartite structure, he similarly divided "Brainstorms" into two sections. He would later collect several essays on content in "The Intentional Stance" and synthesize his views on consciousness into a unified theory in "Consciousness Explained". These volumes respectively form the most extensive development of his views.

In chapter 5 of "Consciousness Explained" Dennett describes his multiple drafts model of consciousness. He states that, "all varieties of perception—indeed all varieties of thought or mental activity—are accomplished in the brain by parallel, multitrack processes of interpretation and elaboration of sensory inputs. Information entering the nervous system is under continuous 'editorial revision.'" (p. 111). Later he asserts, "These yield, over the course of time, something "rather like" a narrative stream or sequence, which can be thought of as subject to continual editing by many processes distributed around the brain, ..." (p. 135, emphasis in the original).

In this work, Dennett's interest in the ability of evolution to explain some of the content-producing features of consciousness is already apparent, and this has since become an integral part of his program. He defends a theory known by some as Neural Darwinism. He also presents an argument against qualia; he argues that the concept is so confused that it cannot be put to any use or understood in any non-contradictory way, and therefore does not constitute a valid refutation of physicalism. His strategy mirrors his teacher Ryle's approach of redefining first person phenomena in third person terms, and denying the coherence of the concepts which this approach struggles with.

Dennett self-identifies with a few terms:
In "Consciousness Explained", he affirms "I am a sort of 'teleofunctionalist', of course, perhaps the original teleofunctionalist". He goes on to say, "I am ready to come out of the closet as some sort of verificationist".(page 460-461)

Much of Dennett's work since the 1990s has been concerned with fleshing out his previous ideas by addressing the same topics from an evolutionary standpoint, from what distinguishes human minds from animal minds ("Kinds of Minds"), to how free will is compatible with a naturalist view of the world ("Freedom Evolves").

Dennett sees evolution by natural selection as an algorithmic process (though he spells out that algorithms as simple as long division often incorporate a significant degree of randomness). This idea is in conflict with the evolutionary philosophy of paleontologist Stephen Jay Gould, who preferred to stress the "pluralism" of evolution (i.e., its dependence on many crucial factors, of which natural selection is only one).

Dennett's views on evolution are identified as being strongly adaptationist, in line with his theory of the intentional stance, and the evolutionary views of biologist Richard Dawkins. In "Darwin's Dangerous Idea", Dennett showed himself even more willing than Dawkins to defend adaptationism in print, devoting an entire chapter to a criticism of the ideas of Gould. This stems from Gould's long-running public debate with E. O. Wilson and other evolutionary biologists over human sociobiology and its descendant evolutionary psychology, which Gould and Richard Lewontin opposed, but which Dennett advocated, together with Dawkins and Steven Pinker. Gould argued that Dennett overstated his claims and misrepresented Gould's, to reinforce what Gould describes as Dennett's "Darwinian fundamentalism".

Dennett's theories have had a significant influence on the work of evolutionary psychologist Geoffrey Miller.

In "Darwin's Dangerous Idea", Dennett writes that evolution can account for the origin of morality. He rejects, however, the idea that morality being natural to us implies that we should take a skeptical position regarding ethics, noting that what is fallacious in the naturalistic fallacy is not to support values per se, but rather to "rush" from facts to values. 

In his 2006 book, "", Dennett attempts to account for religious belief naturalistically, explaining possible evolutionary reasons for the phenomenon of religious adherence.
In this book he declares himself to be "a bright", and defends the term.

He has been doing research into clerics who are secretly atheists and how they rationalize their works. He found what he called a "don't ask, don't tell" conspiracy because believers did not want to hear of loss of faith. That made unbelieving preachers feel isolated but they did not want to lose their jobs and sometimes their church-supplied lodgings and generally consoled themselves that they were doing good in their pastoral roles by providing comfort and required ritual. The research, with Linda LaScola, was further extended to include other denominations and non-Christian clerics. The research and stories Dennett and LaScola accumulated during this project were published in their 2013 co-authored book, "Caught in the Pulpit: Leaving Belief Behind".

He has also written about and advocated the notion of memetics as a philosophically useful tool, most recently in his "Brains, Computers, and Minds", a three-part presentation through Harvard's MBB 2009 Distinguished Lecture Series.

Dennett has been critical of postmodernism, having said: Postmodernism, the school of "thought" that proclaimed "There are no truths, only interpretations" has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for "conversations" in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster.

Dennett adopted and somewhat redefined the term "deepity", originally coined by Miriam Weizenbaum. Dennett used "deepity" for a statement that is apparently profound, but is actually trivial on one level and meaningless on another. Generally, a deepity has two (or more) meanings: one that is true but trivial, and another that sounds profound and would be important if true, but is actually false or meaningless. Examples are "Que sera sera!", "Beauty is only skin deep!", "The power of intention can transform your life." The term many times.

While approving of the increase in efficiency that humans reap by using resources such as expert systems in medicine or GPS in navigation, Dennett sees a danger in machines performing an ever-increasing proportion of basic tasks in perception, memory, and algorithmic computation because people may tend to anthropomorphize such systems and attribute intellectual powers to them that they do not possess. He believes the relevant danger from AI is that people will misunderstand the nature of basically "parasitic" AI systems, rather than employing them constructively to challenge and develop the human user's powers of comprehension.

As given in his most recent book, "From Bacteria to Bach and Back", Dennett's views are contrary to those of Nick Bostrom. Although acknowledging that it is "possible in principle" to create AI with human-like comprehension and agency, Dennett maintains that the difficulties of any such "strong AI" project would be orders of magnitude greater than those raising concerns have realized. According to Dennett, the prospect of superintelligence (AI massively exceeding the cognitive performance of humans in all domains) is at least 50 years away, and of far less pressing significance than other problems the world faces.

Dennett married Susan Bell in 1962. They live in North Andover, Massachusetts, and have a daughter, a son, and five grandchildren.

Dennett is an avid sailor.




</doc>
<doc id="8757" url="https://en.wikipedia.org/wiki?curid=8757" title="Darwin's Dangerous Idea">
Darwin's Dangerous Idea

Darwin's Dangerous Idea: Evolution and the Meanings of Life is a 1995 book by the philosopher Daniel Dennett, in which the author looks at some of the repercussions of Darwinian theory. The crux of the argument is that, whether or not Darwin's theories are overturned, there is no going back from the dangerous idea that design (purpose or what something is for) might not need a designer. Dennett makes this case on the basis that natural selection is a blind process, which is nevertheless sufficiently powerful to explain the evolution of life. Darwin's discovery was that the generation of life worked algorithmically, that processes behind it work in such a way that given these processes the results that they tend toward must be so.

Dennett says, for example, that by claiming that minds cannot be reduced to purely algorithmic processes, many of his eminent contemporaries are claiming that miracles can occur. These assertions have generated a great deal of debate and discussion in the general public. The book was a finalist for the 1995 National Book Award in non-fiction and the 1996 Pulitzer Prize for General Non-Fiction.

Dennett's previous book was "Consciousness Explained" (1991). Dennett noted discomfort with Darwinism among not only lay people but also even academics and decided it was time to write a book dealing with the subject. "Darwin's Dangerous Idea" is not meant to be a work of science, but rather an interdisciplinary book; Dennett admits that he does not understand all of the scientific details himself. He goes into a moderate level of detail, but leaves it for the reader to go into greater depth if desired, providing references to this end.

In writing the book, Dennett wanted to "get thinkers in other disciplines to take evolutionary theory seriously, to show them how they have been underestimating it, and to show them why they have been listening to the wrong sirens". To do this he tells a story; one that is mainly original but includes some material from his previous work.

Dennett taught an undergraduate seminar at Tufts University on Darwin and philosophy, which included most of the ideas in the book. He also had the help of fellow staff and other academics, some of whom read drafts of the book. It is dedicated to W. V. O. Quine, "teacher and friend".

"Starting in the Middle", Part I of "Darwin's Dangerous Idea", gets its name from a quote by Willard Van Orman Quine: "Analyze theory-building how we will, we all must start in the middle. Our conceptual firsts are middle-sized, middle-distance objects, and our introduction to them and to everything comes midway in the cultural evolution of the race."

The first chapter "Tell Me Why" is named after a song.

Before Charles Darwin, and still today, a majority of people see God as the ultimate cause of all design, or the ultimate answer to 'why?' questions. John Locke argued for the primacy of mind before matter, and David Hume, while exposing problems with Locke's view, could not see any alternative.
Darwin provided just such an alternative: evolution. Besides providing evidence of common descent, he introduced a mechanism to explain it: natural selection. According to Dennett, natural selection is a mindless, mechanical and algorithmic process—Darwin's dangerous idea. The third chapter introduces the concept of "skyhooks" and "cranes" (see below). He suggests that resistance to Darwinism is based on a desire for skyhooks, which do not really exist. According to Dennett, good reductionists explain apparent design without skyhooks; greedy reductionists try to explain it without cranes.

Chapter 4 looks at the tree of life, such as how it can be visualized and some crucial events in life's history. The next chapter concerns the possible and the actual, using the 'Library of Mendel' (the space of all logically possible genomes) as a conceptual aid.

In the last chapter of part I, Dennett treats human artifacts and culture as a branch of a unified Design Space. Descent or homology can be detected by shared design features that would be unlikely to appear independently. However, there are also "Forced Moves" or "Good Tricks" that will be discovered repeatedly, either by natural selection (see convergent evolution) or human investigation.

The first chapter of part II, "Darwinian Thinking in Biology", asserts that life originated without any skyhooks, and the orderly world we know is the result of a blind and undirected shuffle through chaos.

The eighth chapter's message is conveyed by its title, "Biology is Engineering"; biology is the study of design, function, construction and operation. However, there are some important differences between biology and engineering. Related to the engineering concept of optimization, the next chapter deals with adaptationism, which Dennett endorses, calling Gould and Lewontin's "refutation" of it an illusion. Dennett thinks adaptationism is, in fact, the best way of uncovering constraints.

The tenth chapter, entitled "Bully for Brontosaurus", is an extended critique of Stephen Jay Gould, who Dennett feels has created a distorted view of evolution with his popular writings; his "self-styled revolutions" against adaptationism, gradualism and other orthodox Darwinism all being false alarms. The final chapter of part II dismisses directed mutation, the inheritance of acquired traits and Teilhard's "Omega Point", and insists that other controversies and hypotheses (like the unit of selection and Panspermia) have no dire consequences for orthodox Darwinism.

"Mind, Meaning, Mathematics and Morality" is the name of Part III, which begins with a quote from Nietzsche. Chapter 12, "The Cranes of Culture", discusses cultural evolution. It asserts that the meme has a role to play in our understanding of culture, and that it allows humans, alone among animals, to "transcend" our selfish genes. "Losing Our Minds to Darwin" follows, a chapter about the evolution of brains, minds and language. Dennett criticizes Noam Chomsky's perceived resistance to the evolution of language, its modeling by artificial intelligence, and reverse engineering.

The evolution of meaning is then discussed, and Dennett uses a series of thought experiments to persuade the reader that meaning is the product of meaningless, algorithmic processes.
Chapter 15 asserts that Gödel's Theorem does not make certain sorts of artificial intelligence impossible. Dennett extends his criticism to Roger Penrose. The subject then moves on to the origin and evolution of morality, beginning with Thomas Hobbes (who Dennett calls "the first sociobiologist") and Friedrich Nietzsche. He concludes that only an evolutionary analysis of ethics makes sense, though he cautions against some varieties of 'greedy ethical reductionism'. Before moving to the next chapter, he discusses some sociobiology controversies.

The penultimate chapter, entitled "Redesigning Morality", begins by asking if ethics can be 'naturalized'. Dennett does not believe there is much hope of discovering an algorithm for doing the right thing, but expresses optimism in our ability to design and redesign our approach to moral problems. In "The Future of an Idea", the book's last chapter, Dennett praises biodiversity, including cultural diversity. In closing, he uses "Beauty and the Beast" as an analogy; although Darwin's idea may seem dangerous, it is actually quite beautiful.

Dennett believes there is little or no principled difference between the naturally generated products of evolution and the man-made artifacts of human creativity and culture. For this reason he indicates deliberately that the complex fruits of the tree of life are in a very meaningful sense "designed"—even though he does not believe evolution was guided by a higher intelligence.

Dennett supports using the notion of memes to better understand cultural evolution. He also believes even human creativity might operate by the Darwinian mechanism. This leads him to propose that the "space" describing biological "design" is connected with the space describing human culture and technology.

A precise mathematical definition of Design Space is not given in "Darwin's Dangerous Idea". Dennett acknowledges this and admits he is offering a philosophical idea rather than a scientific formulation.

Dennett describes natural selection as a substrate-neutral, mindless algorithm for moving through Design Space.

Dennett writes about the fantasy of a "universal acid" as a liquid that is so corrosive that it would eat through anything that it came into contact with, even a potential container. Such a powerful substance would transform everything it was applied to; leaving something very different in its wake. This is where Dennett draws parallels from the “universal acid” to Darwin's idea: 

“it eats through just about every traditional concept, and leaves in its wake a revolutionized world-view, with most of the old landmarks still recognizable, but transformed in fundamental ways.”

While there are people who would like to see Darwin's idea contained within the field of biology, Dennett asserts that this dangerous idea inevitably “leaks” out to transform other fields as well.

Dennett uses the term "skyhook" to describe a source of design complexity that does not build on lower, simpler layers—in simple terms, a miracle.

In philosophical arguments concerning the reducibility (or otherwise) of the human mind, Dennett's concept pokes fun at the idea of intelligent design emanating from on high, either originating from one or more gods, or providing its own grounds in an absurd, Munchausen-like bootstrapping manner.

Dennett also accuses various competing neo-Darwinian ideas of making use of such supposedly unscientific skyhooks in explaining evolution, coming down particularly hard on the ideas of Stephen Jay Gould.

Dennett contrasts theories of complexity that require such miracles with those based on "cranes", structures that permit the construction of entities of greater complexity but are themselves founded solidly "on the ground" of physical science.

In "The New York Review of Books", John Maynard Smith praised "Darwin's Dangerous Idea":
It is therefore a pleasure to meet a philosopher who understands what Darwinism is about, and approves of it.
Dennett goes well beyond biology. He sees Darwinism as a corrosive acid, capable of dissolving our earlier belief and forcing a reconsideration of much of sociology and philosophy. Although modestly written, this is not a modest book. Dennett argues that, if we understand "Darwin's dangerous idea", we are forced to reject or modify much of our current intellectual baggage...
Writing in the same publication, Stephen Jay Gould criticised "Darwin's Dangerous Idea" for being an "influential but misguided ultra-Darwinian manifesto":

Daniel Dennett devotes the longest chapter in "Darwin's Dangerous Idea" to an excoriating caricature of my ideas, all in order to bolster his defense of Darwinian fundamentalism. If an argued case can be discerned at all amid the slurs and sneers, it would have to be described as an effort to claim that I have, thanks to some literary skill, tried to raise a few piddling, insignificant, and basically conventional ideas to "revolutionary" status, challenging what he takes to be the true Darwinian scripture. Since Dennett shows so little understanding of evolutionary theory beyond natural selection, his critique of my work amounts to little more than sniping at false targets of his own construction. He never deals with my ideas as such, but proceeds by hint, innuendo, false attribution, and error.

Gould was also a harsh critic of Dennett's idea of the "universal acid" of natural selection and of his subscription to the idea of memetics; Dennett responded, and the exchange between Dennett, Gould, and Robert Wright was printed in the "New York Review of Books".

Biologist H. Allen Orr wrote a critical review emphasizing similar points in the "Boston Review".

The book has also provoked a negative reaction from creationists; Frederick Crews writes that "Darwin's Dangerous Idea" "rivals Richard Dawkins's "The Blind Watchmaker" as the creationists' most cordially hated text."




</doc>
<doc id="8758" url="https://en.wikipedia.org/wiki?curid=8758" title="Douglas Hofstadter">
Douglas Hofstadter

Douglas Richard Hofstadter (born February 15, 1945) is an American scholar of cognitive science, physics, and comparative literature whose research includes concepts such as the sense of self in relation to the external world, consciousness, analogy-making, artistic creation, literary translation, and discovery in mathematics and physics. His 1979 book "Gödel, Escher, Bach: An Eternal Golden Braid" won both the Pulitzer Prize for general nonfiction
and a National Book Award (at that time called The American Book Award) for Science. His 2007 book "I Am a Strange Loop" won the "Los Angeles Times" Book Prize for Science and Technology.

Hofstadter was born in New York City to Jewish parents: Nobel Prize-winning physicist Robert Hofstadter and Nancy Givan Hofstadter. He grew up on the campus of Stanford University, where his father was a professor, and attended the International School of Geneva in 1958–59. He graduated with Distinction in mathematics from Stanford University in 1965, and received his Ph.D. in physics from the University of Oregon in 1975, where his study of the energy levels of Bloch electrons in a magnetic field led to his discovery of the fractal known as Hofstadter's butterfly.

Since 1988, Hofstadter has been the College of Arts and Sciences Distinguished Professor of Cognitive Science and Comparative Literature at Indiana University in Bloomington, where he directs the Center for Research on Concepts and Cognition which consists of himself and his graduate students, forming the "Fluid Analogies Research Group" (FARG). He was initially appointed to the Indiana University's Computer Science Department faculty in 1977, and at that time he launched his research program in computer modeling of mental processes (which he called "artificial intelligence research", a label he has since dropped in favor of "cognitive science research"). In 1984, he moved to the University of Michigan in Ann Arbor, where he was hired as a professor of psychology and was also appointed to the Walgreen Chair for the Study of Human Understanding. In 1988 he returned to Bloomington as "College of Arts and Sciences Professor" in both cognitive science and computer science. He was also appointed adjunct professor of history and philosophy of science, philosophy, comparative literature, and psychology, but has said that his involvement with most of those departments is nominal. In 1988 Hofstadter received the "In Praise of Reason" award, the Committee for Skeptical Inquiry's highest honor. In April 2009 he was elected a Fellow of the American Academy of Arts and Sciences and a member of the American Philosophical Society. In 2010 he was elected a member of the Royal Society of Sciences in Uppsala, Sweden.

Hofstadter's many interests include music, visual art, the mind, creativity, consciousness, self-reference, translation and mathematics.

At the University of Michigan and Indiana University, he and Melanie Mitchell coauthored a computational model of "high-level perception"—Copycat—and several other models of analogy-making and cognition, including the Tabletop project, co-developed with Robert M. French. Hofstadter's doctoral student James Marshall subsequently extended the Copycat project under the name "Metacat". The Letter Spirit project, implemented by Gary McGraw and John Rehling, aims to model artistic creativity by designing stylistically uniform "gridfonts" (typefaces limited to a grid). Other more recent models include Phaeaco (implemented by Harry Foundalis) and SeqSee (Abhijit Mahabal), which model high-level perception and analogy-making in the microdomains of Bongard problems and number sequences, respectively, as well as George (Francisco Lara-Dammer), which models the processes of perception and discovery in triangle geometry.

The pursuit of beauty has driven Hofstadter both inside and outside his professional work. He seeks beautiful mathematical patterns, beautiful explanations, beautiful typefaces, beautiful sonic patterns in poetry, "etc". Hofstadter has said of himself, "I'm someone who has one foot in the world of humanities and arts, and the other foot in the world of science." He has had several exhibitions of his artwork in various university galleries. These shows have featured large collections of his gridfonts, his ambigrams (pieces of calligraphy created with two readings, either of which is usually obtained from the other by rotating or reflecting the ambigram, but sometimes simply by "oscillation", like the Necker Cube or the rabbit/duck figure of Joseph Jastrow), and his "Whirly Art" (music-inspired visual patterns realized using shapes based on various alphabets from India). Hofstadter invented the term "ambigram" in 1984; many ambigrammists have since taken up the concept.

Hofstadter collects and studies cognitive errors (largely, but not solely, speech errors), "bon mots" (spontaneous humorous quips), and analogies of all sorts, and his longtime observation of these diverse products of cognition, and his theories about the mechanisms that underlie them, have exerted a powerful influence on the architectures of the computational models he and FARG members have developed.

All FARG computational models share certain key principles, including:

FARG models also have an overarching philosophy that all cognition is built from the making of analogies. The computational architectures that share these precepts are called "active symbols" architectures.

Hofstadter's thesis about consciousness, first expressed in "Gödel, Escher, Bach" ("GEB") but also present in several of his later books, is that it is an emergent consequence of seething lower-level activity in the brain. In "GEB" he draws an analogy between the social organization of a colony of ants and the mind seen as a coherent "colony" of neurons. In particular, Hofstadter claims that our sense of having (or being) an "I" comes from the abstract pattern he terms a "strange loop", an abstract cousin of such concrete phenomena as audio and video feedback that Hofstadter has defined as "a level-crossing feedback loop". The prototypical example of a strange loop is the self-referential structure at the core of Gödel's incompleteness theorems. Hofstadter's 2007 book "I Am a Strange Loop" carries his vision of consciousness considerably further, including the idea that each human "I" is distributed over numerous brains, rather than being limited to one.

Hofstadter's writing is characterized by an intense interaction between form and content, as exemplified by the 20 dialogues in "GEB", many of which simultaneously discuss and imitate strict musical forms used by Bach, such as canons and fugues. Most of Hofstadter's books feature some kind of structural alternation: in "GEB" between dialogues and chapters, in "The Mind's I" between selections and reflections, in "Metamagical Themas" between Chapters and Postscripts, and so forth. In both his writing and his teaching, Hofstadter stresses the concrete, constantly using examples and analogies, and avoids the abstract. Typical of the courses he teaches is his seminar "Group Theory and Galois Theory Visualized", in which abstract mathematical ideas are rendered as concretely as possible. He puts great effort into making ideas clear and visual, and asserts that when he teaches, if his students do not understand something, it is never their fault but always his own.

Hofstadter is passionate about languages. In addition to English, his mother tongue, he speaks French and Italian fluently (the language spoken at home with his children is Italian). At various times in his life, he has studied (in descending order of level of fluency reached) German, Russian, Spanish, Swedish, Mandarin, Dutch, Polish, and Hindi. His love of sounds pushes him to strive to minimize, and ideally get rid of, any foreign accent.

"Le Ton beau de Marot: In Praise of the Music of Language" is a long book devoted to language and translation, especially poetry translation, and one of its leitmotifs is a set of 88 translations of "Ma Mignonne", a highly constrained poem by 16th-century French poet Clément Marot. In this book, Hofstadter jokingly describes himself as "pilingual" (meaning that the sum total of the varying degrees of mastery of all the languages that he's studied comes to 3.14159 ...), as well as an "oligoglot" (someone who speaks "a few" languages).

In 1999, the bicentennial year of Russian poet and writer Alexander Pushkin, Hofstadter published a verse translation of Pushkin's classic novel-in-verse "Eugene Onegin". He has translated many other poems too (always respecting their formal constraints), and two novels (in prose): "La Chamade" ("That Mad Ache") by French writer Françoise Sagan, and "La Scoperta dell'Alba" ("The Discovery of Dawn") by Walter Veltroni, the then head of the Partito Democratico in Italy. "The Discovery of Dawn" was published in 2007, and "That Mad Ache" was published in 2009, bound together with Hofstadter's essay "Translator, Trader: An Essay on the Pleasantly Pervasive Paradoxes of Translation".

Hofstadter's Law is "It always takes longer than you expect, even when you take into account Hofstadter's Law." The law is stated in "GEB".

Hofstadter's former Ph.D. students include (with dissertation title):


Hofstadter has said that he feels "uncomfortable with the nerd culture that centers on computers". He admits that "a large fraction [of his audience] seems to be those who are fascinated by technology", but when it was suggested that his work "has inspired many students to begin careers in computing and artificial intelligence" he replied that he was pleased about that, but that he himself has "no interest in computers". In that interview he also mentioned a course he has twice given at Indiana University, in which he took a "skeptical look at a number of highly-touted AI projects and overall approaches". For example, upon the defeat of Garry Kasparov by Deep Blue, he commented that "It was a watershed event, but it doesn't have to do with computers becoming intelligent". Hofstadter's disinterest in computers is analogous to an astronomer's disinterest in telescopes. In his book "Metamagical Themas", he says that "in this day and age, how can anyone fascinated by creativity and beauty fail to see in computers the ultimate tool for exploring their essence?".

Provoked by predictions of a technological singularity (a hypothetical moment in the future of humanity when a self-reinforcing, runaway development of artificial intelligence causes a radical change in technology and culture), Hofstadter has both organized and participated in several public discussions of the topic. At Indiana University in 1999 he organized such a symposium, and in April 2000, he organized a larger symposium titled "Spiritual Robots" at Stanford University, in which he moderated a panel consisting of Ray Kurzweil, Hans Moravec, Kevin Kelly, Ralph Merkle, Bill Joy, Frank Drake, John Holland and John Koza. Hofstadter was also an invited panelist at the first Singularity Summit, held at Stanford in May 2006. Hofstadter expressed doubt that the singularity will occur in the foreseeable future.

In 1988 Dutch director Piet Hoenderdos created a docudrama about Hofstadter and his ideas, "Victim of the Brain", based on "The Mind's I". It includes interviews with Hofstadter about his work.

When Martin Gardner retired from writing his "Mathematical Games" column for "Scientific American" magazine, Hofstadter succeeded him in 1981–83 with a column titled "Metamagical Themas" (an anagram of "Mathematical Games"). An idea he introduced in one of these columns was the concept of "Reviews of This Book", a book containing nothing but cross-referenced reviews of itself that has an online implementation. One of Hofstadter's columns in "Scientific American" concerned the damaging effects of sexist language, and two chapters of his book "Metamagical Themas" are devoted to that topic, one of which is a biting analogy-based satire, "A Person Paper on Purity in Language" (1985), in which the reader's presumed revulsion at racism and racist language is used as a lever to motivate an analogous revulsion at sexism and sexist language; Hofstadter published it under the pseudonym William Satire, an allusion to William Safire. Another column reported on the discoveries made by University of Michigan professor Robert Axelrod in his computer tournament pitting many iterated prisoner's dilemma strategies against each other, and a follow-up column discussed a similar tournament that Hofstadter and his graduate student Marek Lugowski organized. The "Metamagical Themas" columns ranged over many themes, including patterns in Frédéric Chopin's piano music (particularly his études), the concept of superrationality (choosing to cooperate when the other party/adversary is assumed to be equally intelligent as oneself), and the self-modifying game of Nomic, based on the way in which the legal system modifies itself, and developed by philosopher Peter Suber.

Hofstadter was married to Carol Ann Brush until her death. They met in Bloomington, and married in Ann Arbor in 1985. They had two children, Danny and Monica. Carol died in 1993 from the sudden onset of a brain tumor—glioblastoma multiforme—when their children were five and two. The Carol Ann Brush Hofstadter Memorial Scholarship for Bologna-bound Indiana University students was established in 1996 in her name. Hofstadter's book "Le Ton beau de Marot" is dedicated to their two children and its dedication reads "To M. & D., living sparks of their Mommy's soul".

In the fall of 2010, Hofstadter met Baofen Lin in a chacha class, and they married in Bloomington in September 2012.

Hofstadter has composed numerous pieces for piano, and a few for piano and voice. He created an audio CD, "DRH/JJ", which includes all these compositions performed mostly by pianist Jane Jackson, with a few performed by Brian Jones, Dafna Barenboim, Gitanjali Mathur and Hofstadter.

The dedication for "I Am A Strange Loop" is: "To my sister Laura, who can understand, and to our sister Molly, who cannot." Hofstadter explains in the preface that his younger sister Molly never developed the ability to speak or understand language.

As a consequence of his attitudes about consciousness and empathy, Hofstadter has been a vegan for roughly half his life.

In the 1982 novel "", Arthur C. Clarke's first sequel to "", HAL 9000 is described by Dr. Chandra as being caught in a "Hofstadter–Möbius loop". The movie uses the term "H. Möbius loop".

On April 3, 1995, Hofstadter's book "Fluid Concepts & Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought" was the first book ever sold by Amazon.com.

The books published by Hofstadter are (the ISBNs refer to paperback editions, where available):

Hofstadter has written, among many others, the following papers:


Hofstadter has also written over 50 papers that were published through the Center for Research on Concepts and Cognition.

Hofstadter has written forewords for or edited the following books:
Translations




</doc>
<doc id="8765" url="https://en.wikipedia.org/wiki?curid=8765" title="Dahomey">
Dahomey

The Kingdom of Dahomey () was an African kingdom (located within the area of the present-day country of Benin) that existed from about 1600 until 1904, when the last king, Béhanzin, was defeated by the French, and the country was annexed into the French colonial empire. Dahomey developed on the Abomey Plateau amongst the Fon people in the early 17th century and became a regional power in the 18th century by conquering key cities on the Atlantic coast.

For much of the 18th and 19th centuries, the Kingdom of Dahomey was a key regional state, eventually ending tributary status to the Oyo Empire. The Kingdom of Dahomey was an important regional power that had an organized domestic economy built on conquest and slave labor, significant international trade with Europeans, a centralized administration, taxation systems, and an organized military. Notable in the kingdom were significant artwork, an all-female military unit called the Dahomey Amazons by European observers, and the elaborate religious practices of Vodun with the large festival of the Annual Customs of Dahomey which involved large scale human sacrifice. They traded prisoners, which they captured during wars and raids, and exchanged them with Europeans for goods such as knives, bayonets, firearms, fabrics, and spirits.

The Kingdom of Dahomey was referred to by many different names and has been written in a variety of ways, including "Danxome", "Danhome", and "Fon". The name "Fon" relates to the dominant ethnic and language group, the Fon people, of the royal families of the kingdom and is how the kingdom first became known to Europeans. The names "Dahomey", "Danxome", and "Danhome" all have a similar origin story, which historian Edna Bay says may be a false etymology.

The story goes that Dakodonu, considered the second king in modern kings lists, was granted permission by the Gedevi chiefs, the local rulers, to settle in the Abomey plateau. Dakodonu requested additional land from a prominent chief named Dan (or Da) to which the chief responded sarcastically "Should I open up my belly and build you a house in it?" For this insult, Dakodonu killed Dan and began the construction of his palace on the spot. The name of the kingdom was derived from the incident: Dan=chief dan, xo=Belly, me=Inside of.

The Kingdom of Dahomey was established around 1600 by the Fon people who had recently settled in the area (or were possibly a result of intermarriage between the Aja people and the local Gedevi). The foundational king for Dahomey is often considered to be Houegbadja (c. 1645–1685), who built the Royal Palaces of Abomey and began raiding and taking over towns outside of the Abomey plateau.

King Agaja, Houegbadja's grandson, came to the throne in 1708 and began significant expansion of the Kingdom of Dahomey. This expansion was made possible by the superior military force of King Agaja's Dahomey. In contrast to surrounding regions, Dahomey employed a professional standing army numbering around ten thousand. What the Dahomey lacked in numbers, they made up for in discipline and superior arms. In 1724, Agaja conquered Allada, the origin for the royal family according to oral tradition, and in 1727 he conquered Whydah. This increased size of the kingdom, particularly along the Atlantic coast, and increased power made Dahomey into a regional power. The result was near constant warfare with the main regional state, the Oyo Empire, from 1728 until 1740. The warfare with the Oyo empire resulted in Dahomey assuming a tributary status to the Oyo empire.

Tegbesu, also spelled as Tegbessou, was King of Dahomey, in present-day Benin, from 1740 until 1774. Tegbesu was not the oldest son of King Agaja (1718–1740), but was selected following his father's death after winning a succession struggle with a brother. King Agaja had significantly expanded the Kingdom of Dahomey during his reign, notably conquering Whydah in 1727. This increased the size of the kingdom and increased both domestic dissent and regional opposition. Tegbessou ruled over Dahomey at a point where it needed to increase its legitimacy over those who it had recently conquered. As a result, Tegbesu is often credited with a number of administrative changes in the kingdom in order to establish the legitimacy of the kingdom. The slave trade increased significantly during Tegbessou's reign and began to provide the largest part of the income for the king. In addition, Tegbesu's rule is the one with the first significant "kpojito" or mother of the leopard with Hwanjile in that role. The "kpojito" became a prominently important person in Dahomey royalty. Hwanjile, in particular, is said to have changed dramatically the religious practices of Dahomey by creating two new deities and more closely tying worship to that of the king. According to one oral tradition, as part of the tribute owed by Dahomey to Oyo, Agaja had to give to Oyo one of his sons. The story claims that only Hwanjile, of all of Agaja's wives, was willing to allow her son to go to Oyo. This act of sacrifice, according to the oral tradition made Tegbesu, was favored by Agaja. Agaja reportedly told Tegbesu that he was the future king, but his brother Zinga was still the official heir.

The kingdom fought the First Franco-Dahomean War and Second Franco-Dahomean War with France. The kingdom was reduced and made a French protectorate in 1894.

In 1904 the area became part of a French colony, French Dahomey.

In 1958 French Dahomey became the self-governing colony called the Republic of Dahomey and gained full independence in 1960. It was renamed in 1975 the People's Republic of Benin and in 1991 the Republic of Benin. The Dahomey kingship exists as a ceremonial role to this day.

Early writings, predominantly written by European slave traders, often presented the kingdom as an absolute monarchy led by a despotic king. However, these depictions were often deployed as arguments by different sides in the slave trade debates, mainly in the United Kingdom, and as such were probably exaggerations. Recent historical work has emphasized the limits of monarchical power in the Kingdom of Dahomey. Historian John C. Yoder, with attention to the Great Council in the kingdom, argued that its activities do not "imply that Dahomey's government was democratic or even that her politics approximated those of nineteenth-century European monarchies. However, such evidence does support the thesis that governmental decisions were molded by conscious responses to internal political pressures as well as by executive fiat." The primary political divisions revolved around villages with chiefs and administrative posts appointed by the king and acting as his representatives to adjudicate disputes in the village.

The King of Dahomey ("Ahosu" in the Fon language) was the sovereign power of the kingdom. All of the kings were claimed to be part of the "Alladaxonou" dynasty, claiming descent from the royal family in Allada. Much of the succession rules and administrative structures were created early by Houegbadja, Akaba, and Agaja. Succession through the male members of the line was the norm typically going to the oldest son, but not always. The king was selected largely through discussion and decision in the meetings of the Great Council, although how this operated was not always clear. The Great Council brought together a host of different dignitaries from throughout the kingdom yearly to meet at the Annual Customs of Dahomey. Discussions would be lengthy and included members, both men and women, from throughout the kingdom. At the end of the discussions, the king would declare the consensus for the group.

Key positions in the King's court included the "migan", the "mehu", the "yovogan", the "kpojito" (or queen mother), and later the "chacha" (or viceroy) of Whydah. The "migan" (combination of mi—our and gan—chief) was a primary consul for the king, a key judicial figure, and served as the head executioner. The "mehu" was similarly a key administrative officer who managed the palaces and the affairs of the royal family, economic matters, and the areas to the south of Allada (making the position key to contact with Europeans).

The relations between Dahomey and other countries were complex and heavily impacted by the Gold trade. The Oyo empire engaged in regular warfare with the kingdom of Dahomey and Dahomey was a tributary to Oyo from 1732 until 1823. The city-state of Porto-Novo, under the protection of Oyo, and Dahomey had a long-standing rivalry largely over control of the Gold trade along the coast. The rise of Abeokuta in the 1840s created another power rivaling Dahomey, largely by creating a safe haven for people from the slave trade.

The last known slave ship that sailed to the United States of America to port in Mobile, Alabama brought a group of 110 slaves from the Dahomey Kingdom, purchased long after the abolition of the international slave trade. Thomas Jefferson signed the Act Prohibiting Importation of Slaves into law on March 2, effective January 1, 1808. The story was mentioned in the newspaper "The Tarboro Southerner" on July 14, 1860. On July 9, 1860, a schooner called "Clotilda", captained by William Foster, arrived in the bay of Mobile, Alabama carrying the last known shipment of slaves to the US from the Dahomey Kingdom. In 1858, a man known as Timothy Meaher made a wager with acquaintances that despite the law banning the slave trade, he could safely bring a load of slaves from Africa.
Describing how he came in possession of the slaves, Captain William Foster wrote in his journal in 1860, "from thence I went to see the King of Dahomey. Having agreeably transacted affairs with the Prince we went to the warehouse where they had in confinement four thousand captives in a state of nudity from which they gave me liberty to select one hundred and twenty-five as mine offering to brand them for me, from which I preemptorily ["sic"] forbid; commenced taking on cargo of negroes ["sic"], successfully securing on board one hundred and ten."

A notable descendant of a slave from this ship is Ahmir Khalib Thompson (American music artist known as Questlove). Mr. Thompson's story is depicted in the PBS Television show "Finding Your Roots" [Season 4, Episode 9].

The Kingdom of Dahomey also sent a diplomatic mission to Brazil, in 1750, while the country was still under Portuguese rule, in order to strengthen diplomatic relations, after an incident which led to the expulsion of Portuguese-Brazilian diplomatic authorities, in 1743. The interest of maintaining these relations was economic, due to slave trade. It is also important to note that Dahomey was the second country - the first being Portugal - to recognize the independence of Brazil in 1822.

The military of the Kingdom of Dahomey was divided into two units: the right and the left. The right was controlled by the "migan" and the left was controlled by the "mehu". At least by the time of Agaja, the kingdom had developed a standing army that remained encamped wherever the king was. Soldiers in the army were recruited as young as seven or eight years old, initially serving as shield carriers for regular soldiers. After years of apprenticeship and military experience, they were allowed to join the army as regular soldiers. To further incentivize the soldiers, each soldier received bonuses paid in cowry shells for each enemy they killed or captured in battle. This combination of lifelong military experience and monetary incentives resulted in a cohesive, well-disciplined military. One European said Agaja's standing army consisted of "elite troops, brave and well-disciplined, led by a prince full of valor and prudence, supported by a staff of experienced officers".

In addition to being well-trained, the Dahomey army under Agaja was also very well armed. The Dahomey army favored imported European weapons as opposed to traditional weapons. For example, they used European flintlock muskets in long-range combat and imported steel swords and cutlasses in close combat. The Dahomey army also possessed twenty-five cannons.

When going into battle, the king would take a secondary position to the field commander with the reason given that if any spirit were to punish the commander for decisions it should not be the king. Unlike other regional powers, the military of Dahomey did not have a significant cavalry (like the Oyo empire) or naval power (which prevented expansion along the coast). The Dahomey Amazons, a unit of all-female soldiers, is one of the most unusual aspects of the military of the kingdom.

The Dahomean state became widely known for its corps of female soldiers. Their origins are debated; they may have formed from a palace guard or from gbetos (female hunting teams).

They were organized around 1729 to fill out the army and make it look larger in battle, armed only with banners. The women reportedly behaved so courageously they became a permanent corps. In the beginning, the soldiers were criminals pressed into service rather than being executed. Eventually, however, the corps became respected enough that King Ghezo ordered every family to send him their daughters, with the fittest being chosen as soldiers.

The economic structure of the kingdom was highly intertwined with the political and religious systems and these developed together significantly. The main currency was cowry shells.

The domestic economy largely focused on agriculture and crafts for local consumption. Until the development of palm oil, very little agricultural or craft goods were traded outside of the kingdom. Markets served a key role in the kingdom and were organized around a rotating cycle of four days with a different market each day (the market type for the day was religiously sanctioned). Agriculture work was largely decentralized and done by most families. However, with the expansion of the kingdom, agricultural plantations began to be a common agricultural method in the kingdom. Craftwork was largely dominated by a formal guild system.

Herskovits recounts a complex tax system in the kingdom, in which officials who represented the king, the "tokpe", gathered data from each village regarding their harvest. Then the king set a tax based upon the level of production and village population. In addition, the king's own land and production were taxed. After significant road construction undertaken by the kingdom, toll booths were also established that collected yearly taxes based on the goods people carried and their occupation. Officials also sometimes imposed fines for public nuisance before allowing people to pass.

The Kingdom of Dahomey shared many religious rituals with surrounding populations; however, it also developed unique ceremonies, beliefs, and religious stories for the kingdom. These included royal ancestor worship and the specific vodun practices of the kingdom.

Early kings established clear worship of royal ancestors and centralized their ceremonies in the Annual Customs of Dahomey. The spirits of the kings had an exalted position in the land of the dead and it was necessary to get their permission for many activities on earth. Ancestor worship pre-existed the kingdom of Dahomey; however, under King Agaja, a cycle of ritual was created centered on first celebrating the ancestors of the king and then celebrating a family lineage.

The Annual Customs of Dahomey ("xwetanu" or "huetanu" in Fon) involved multiple elaborate components and some aspects may have been added in the 19th century. In general, the celebration involved distribution of gifts, human sacrifice, military parades, and political councils. Its main religious aspect was to offer thanks and gain the approval for ancestors of the royal lineage. However, the custom also included military parades, public discussions, gift giving (the distribution of money to and from the king), and human sacrifice and the spilling of blood.

Human sacrifice was an important part of the practice. During the Annual Custom, 500 prisoners would be sacrificed. In addition, when a ruler died, hundreds, to thousands of prisoners would be sacrificed. As many as 4,000 were reported killed In one of these ceremonies in 1727.

Dahomey had a unique form of West African Vodun that linked together preexisting animist traditions with vodun practices. Oral history recounted that Hwanjile, a wife of Agaja and mother of Tegbessou, brought Vodun to the kingdom and ensured its spread. The primary deity is the combined Mawu-Lisa (Mawu having female characteristics and Lisa having male characteristics) and it is claimed that this god took over the world that was created by their mother Nana-Buluku. Mawu-Lisa governs the sky and is the highest pantheon of gods, but other gods exist in the earth and in thunder. Religious practice organized different priesthoods and shrines for each different god and each different pantheon (sky, earth or thunder). Women made up a significant amount of the priest class and the chief priest was always a descendant of Dakodonou.

The arts in Dahomey were unique and distinct from the artistic traditions elsewhere in Africa. The arts were substantially supported by the king and his family, had non-religious traditions, assembled multiple different materials, and borrowed widely from other peoples in the region. Common art forms included wood and ivory carving, metalwork (including silver, iron and brass, appliqué cloth, and clay bas-reliefs).

The king was key in supporting the arts and many of them provided significant sums for artists resulting in the unique development, for the region, of a non-religious artistic tradition in the kingdom. Artists were not of a specific class but both royalty and commoners made important artistic contributions. Kings were often depicted in large zoomorphic forms with each king resembling a particular animal in multiple representations.

Suzanne Blier identifies two unique aspects of art in Dahomey: 1. Assemblage of different components and 2. Borrowing from other states. Assemblage of art, involving the combination of multiple components (often of different materials) combined together in a single piece of art, was common in all forms and was the result of the various kings promoting finished products rather than particular styles. This assembling may have been a result of the second feature, which involved the wide borrowing of styles and techniques from other cultures and states. Clothing, cloth work, architecture, and the other forms of art all resemble other artistic representation from around the region.

Much of the artwork revolved around the royalty. Each of the palaces at the Royal Palaces of Abomey contained elaborate bas-reliefs ("noundidė" in Fon) providing a record of the king's accomplishments. Each king had his own palace within the palace complex and within the outer walls of their personal palace was a series of clay reliefs designed specific to that king. These were not solely designed for royalty and chiefs, temples, and other important buildings had similar reliefs. The reliefs would present Dahomey kings often in military battles against the Oyo or Mahi tribes to the north of Dahomey with their opponents depicted in various negative depictions (the king of Oyo is depicted in one as a baboon eating a cob of corn). Historical themes dominated representation and characters were basically designed and often assembled on top of each other or in close proximity creating an ensemble effect. In addition to the royal depictions in the reliefs, royal members were depicted in power sculptures known as "bocio", which incorporated mixed materials (including metal, wood, beads, cloth, fur, feathers, and bone) onto a base forming a standing figure. The bocio are religiously designed to include different forces together to unlock powerful forces. In addition, the cloth appliqué of Dahomey depicted royalty often in similar zoomorphic representation and dealt with matters similar to the reliefs, often the kings leading during warfare.

Dahomey had a distinctive tradition of casting small brass figures of animals or people, which were worn as jewellery or displayed in the homes of the relatively well-off. These figures, which continue to be made for the tourist trade, were relatively unusual in traditional African art in having no religious aspect, being purely decorative, as well as indicative of some wealth. Also unusual, by being so early and clearly provenanced, is a carved wooden tray (not dissimilar to much more recent examples) in Ulm, Germany, which was brought to Europe before 1659, when it was described in a printed catalogue.

The Kingdom of Dahomey has been depicted in a number of different literary works of fiction or creative nonfiction. "In Dahomey" (1903) was a successful Broadway musical, the first full-length Broadway musical written entirely by African Americans, in the early 20th century. Novelist Paul Hazoumé's first novel "Doguicimi" (1938) was based on decades of research into the oral traditions of the Kingdom of Dahomey during the reign of King Ghezo. The anthropologist Judith Gleason wrote a novel, "Agõtĩme: Her Legend" (1970), centered on one of the wives of a king of Dahomey in the late 18th century, who offends her husband who sells her to slavery in Brazil; she makes a bargain with a "vodu" (deity), putting her son on the throne of Dahomey and bringing her home. Another novel tracing the background of a slave, this time in the United States, was "The Dahomean", or "The Man from Dahomey" (1971), by the African-American novelist Frank Yerby; its hero is an aristocratic warrior. In the third of George McDonald Fraser's Flashman novels, "Flash for Freedom" (1971), Flashman dabbles in the slave trade and visits Dahomey. "The Viceroy of Ouidah" (1980) by Bruce Chatwin is the story of a Brazilian who, hoping to make his fortune from slave trading, sails to Dahomey in 1812, befriending its unbalanced king and coming to a bad end. The book was later adapted into the film "Cobra Verde" (1987) directed by Werner Herzog. The main character of one of the two parallel stories in "Will Do Magic for Small Change" (2016) by Andrea Hairston is Kehinde, a Yoruba woman forced into the Dahomean army; she struggles with divided loyalty, and after the fall of Behanzin, joins a French entertainment troupe who intend to exhibit her as an Amazon at the Chicago World's Fair. The Booker prize winning novel "Girl, Woman, Other" (2019) by Bernardine Evaristo features a character named Amma who writes and directs a play titled 'The Last Amazon of Dahomey'.

Behanzin's resistance to the French attempt to end slave trading and human sacrifice has been central to a number of works. Jean Pliya's first play "Kondo le requin" (1967), winner of the Grand Prize for African History Literature, tells the story of Behanzin's struggle to maintain the old order. Maryse Condé's novel "The Last of the African Kings" (1992) similarly focuses on Behanzin's resistance and his exile to the Caribbean. The novel "Thread of Gold Beads" (2012) by Nike Campbell-Fatoki centers on a daughter of Behanzin; through her eyes, the end of his reign is observed.




</doc>
<doc id="8767" url="https://en.wikipedia.org/wiki?curid=8767" title="Dragoon">
Dragoon

Dragoons originally were a class of mounted infantry, who used horses for mobility, but dismounted to fight on foot. From the early 17th century onward, dragoons were increasingly also employed as conventional cavalry, trained for combat with swords from horseback.

Dragoon regiments were established in most European armies during the late 17th and early 18th centuries.

The name is derived from a type of firearm, called a "dragon", which was a handgun version of a blunderbuss, carried by dragoons of the French Army.

The title has been retained in modern times by a number of armoured or ceremonial mounted regiments.

The establishment of dragoons evolved from the practice of sometimes transporting infantry by horse when speed of movement was needed. In 1552 Prince Alexander of Parma mounted several companies of infantry on pack horses to achieve surprise. Another early instance was ordered by Louis of Nassau in 1572 during operations near Mons in Hainaut, when 500 infantry were transported this way. It is also suggested the first dragoons were raised by the Marshal de Brissac in 1600. According to old German literature, dragoons were invented by Count Ernst von Mansfeld, one of the greatest German military commanders, in the early 1620s. There are other instances of mounted infantry predating this. However Mansfeld, who had learned his profession in Hungary and the Netherlands, often used horses to make his foot troops more mobile, creating what was called an "armée volante" (French for "flying army").

In the 16th century Spanish civil wars in Peru conquistadors fought on horse with arquebuses, prefiguring the origin of European dragoons.

The name possibly derives from an early weapon, a short wheellock called a "dragon", because the first dragoons raised in France had their carbine's muzzle decorated with a dragon's head. The practice comes from a time when all gunpowder weapons had distinctive names, including the culverin, serpentine, falcon, falconet, etc. It is also sometimes claimed a galloping infantryman with his loose coat and the burning match resembled a dragon.

It has also been suggested that the name derives from the German "tragen" or the Dutch "dragen", both being the verb "to carry" in their respective languages. Howard Reid claims that the name and role descend from the Latin Draconarius.

Dragoon is occasionally used as a verb to mean to subjugate or persecute by the imposition of troops; and by extension to compel by any violent measures or threats. The term dates from 1689, at a time when dragoons were being used by the French monarchy to persecute Protestants, particularly by forcing Protestants to lodge a dragoon in their house to watch over them, at the householder's expense.

Early dragoons were not organized in squadrons or troops as were cavalry, but in companies like the infantry: their officers and non-commissioned officers bore infantry ranks. Dragoon regiments used drummers, not buglers, to communicate orders on the battlefield. The flexibility of mounted infantry made dragoons a useful arm, especially when employed for what would now be termed "internal security" against smugglers or civil unrest, and on line of communication security duties. During the English Civil War dragoons were used for a variety of tasks: providing outposts, holding defiles or bridges in the front or rear of the main army, lining hedges or holding enclosures, and providing dismounted musketeers to support regular cavalry.. In the closing stages of the Battle of Naseby Okey's Dragoons, who had started the action as dismounted musketeers, got on their horses and charged, possibly the first time this was done. Supplied with inferior horses and more basic equipment, the dragoon regiments were cheaper to recruit and maintain than the expensive regiments of cavalry. When in the 17th century Gustav II Adolf introduced dragoons into the Swedish Army, he provided them with a sabre, an axe and a matchlock musket, utilizing them as "labourers on horseback". Many of the European armies henceforth imitated this all-purpose set of weaponry.

A non-military use of dragoons was the 1681 Dragonnades, a policy instituted by Louis XIV to intimidate Huguenot families into either leaving France or re-converting to Catholicism by billeting ill-disciplined dragoons in Protestant households. While other categories of infantry and cavalry were also used, the mobility, flexibility and available numbers of the dragoon regiments made them particularly suitable for repressive work of this nature over a wide area.

In the Spanish Army, Pedro de la Puente organized a body of dragoons in Innsbruck in 1635. In 1640, a tercio of a thousand dragoons armed with the arquebus was created in Spain. By the end of the 17th century, the Spanish Army had three tercios of dragoons in Spain, plus three in the Netherlands and three more in Milan. In 1704, the Spanish dragoons were reorganised into regiments by Philip V, as were the rest of the tercios.

Towards the end of 1776, George Washington realized the need for a mounted branch of the American military. In January 1777 four regiments of light dragoons were raised. Short term enlistments were abandoned and the dragoons joined for three years, or "the war". They participated in most of the major engagements of the American War of Independence, including the Battles of White Plains, Trenton, Princeton, Brandywine, Germantown, Saratoga, Cowpens, and Monmouth, as well as the Yorktown campaign.

Dragoons were at a disadvantage when engaged against true cavalry, and constantly sought to improve their horsemanship, armament and social status. By the Seven Years' War the primary role of dragoons in most European armies had progressed from that of mounted infantry to that of heavy cavalry. Earlier dragoon responsibilities for scouting and picket duty had passed to hussars and similar light cavalry corps in the French, Austrian, Prussian, and other armies. In the Imperial Russian Army, due to the availability of the Cossack troops, the dragoons were retained in their original role for much longer.

An exception to the rule was the British Army. To reduce military budgets, all horse (cavalry) regiments were gradually demoted to dragoons from 1746 onward — which meant they were paid on a lower scale. When this was completed in 1788, the heavy cavalry regiments had become either Dragoon Guards or Heavy Dragoons (depending on their precedence). The designation of Dragoon Guards did not mean that these regiments (the former 2nd to 8th Horse) had become Household Troops, but simply that they had been given a more dignified title to compensate for the loss of pay and prestige. Starting in 1756, seven regiments of Light Dragoons were raised. These Light Dragoons were trained in reconnaissance, skirmishing and other work requiring endurance in accordance with contemporary standards of light cavalry performance. The success of this new class of cavalry was such that eight regular Dragoon regiments were converted to Light Dragoons between 1768 and 1783.

During the Napoleonic Wars, dragoons generally assumed a cavalry role, though remaining a lighter class of mounted troops than the armored cuirassiers. Dragoons rode larger horses than the light cavalry and wielded straight, rather than curved swords. Emperor Napoleon often formed complete divisions out of his 30 dragoon regiments and used them as battle cavalry to break the enemy's main resistance. In 1809, French dragoons scored notable successes against Spanish armies at the Battle of Ocana and the Battle of Alba de Tormes.

British heavy dragoons made devastating charges against French infantry at the Battle of Salamanca in 1812 and at the Battle of Waterloo in 1815. 31 regiments were in existence at the height of the Napoleonic Wars: seven Dragoon Guards regiments and 24 cavalry of the line regiments. The Dragoon Guards and Dragoon regiments were the heavy cavalry regiments of the British Army, although by continental standards they were not the heaviest type of cavalry since they carried no armour (unlike cuirassiers). While some of the cavalry regiments of the line were simply designated as regiments of "dragoons", the lighter cavalry regiments, which were particularly mobile, became regiments of "Light Dragoons", employing the 1796-pattern light cavalry sabres. From 1805 four regiments of Light Dragoons were designated Hussars (7th, 10th, 15th and 18th Regiments), differentiated by uniform, and the wearing of mustaches. After the end of the Napoleonic Wars (starting in 1816) some regiments became " lancers", identified by the lances that they carried.

The creation of a unified German state in 1871 brought together the dragoon regiments of Prussia, Bavaria, Saxony, Mecklenburg, Oldenburg, Baden, Hesse and Württemberg in a single numbered sequence, although historic distinctions of insignia and uniform were largely preserved. Two regiments of the Imperial Guard were designated as dragoons.

The Austrian (later Austro-Hungarian) Army of the 19th century included six regiments of dragoons in 1836, classed as heavy cavalry for shock action, but in practice used as medium troops with a variety of roles. After 1859 all but two Austrian dragoon regiments were converted to cuirassiers or disbanded. From 1868 to 1918 the Austro-Hungarian dragoons numbered 15 regiments.

During the 18th century several regiments of dragoons were created in Spain's American viceroyalties to protect the northern provinces and borders of New Spain in the present-day states of California, Nevada, Colorado, Texas, Kansas, Arizona, Montana, North Dakota and South Dakota. Some of these functioned as a police force. In 1803, the regiments of dragoons began to be called light cavalry (C"azadores") and dragoons disappeared from the Spanish Army shortly after 1815.

In New Spain, soon to be México, dragoons were important and elite units of the Royal Army. A number of dragoons became important military and political figures, among them Ignacio Allende and Juan Aldama, members of the Queen's Regiment of Dragoons who defected and then initiated the independence movement in México in 1810. Another important dragoon was Agustin de Iturbide, who would ultimately achieve Mexican independence in 1821. He was known as the greatest horseman in México and became so renowned in battle during his youth that he acquired the nickname "El Dragón de Hierro" or "The Iron Dragon" (in Spanish, "dragon" and "dragoon" both sound and are written exactly the same). He would go on to become Agustín I, after being elected Emperor of México. The political importance of dragoons during this time in the nascent country cannot be overstated.

Prior to the War of 1812 the U.S. organized the Regiment of Light Dragoons. For the war a second regiment was activated; that regiment was consolidated with the original regiment in 1814. The original regiment was consolidated with the Corps of Artillery in June 1815.

The 1st United States Dragoons explored Iowa after the Black Hawk Purchase put the area under U.S. control. In the summer of 1835, the regiment blazed a trail along the Des Moines river and established outposts from present-day Des Moines to Fort Dodge. In 1933, the State of Iowa opened the Dragoon Trail, a scenic and historic drive that follows the path of the 1st United States Dragoons on their historic march.

In 1861 the two existing U.S. Dragoon regiments were re-designated as the 1st and 2nd Cavalry. This reorganization did not affect their role or equipment, although the traditional orange uniform braiding of the dragoons was replaced by the standard yellow of the Cavalry branch. This marked the official end of dragoons in the U.S. Army, although certain modern units trace their origins back to the historic dragoon regiments.

In several stages between 1816 and 1861, the 21 existing Light Dragoon regiments in the British Army were disbanded or converted to lancers or hussars.

Between 1881 and 1907 all Russian cavalry (other than Cossacks and Imperial Guard regiments) were designated as dragoons, reflecting an emphasis on the double ability of dismounted action as well as the new cavalry tactics in their training and a growing acceptance of the impracticality of employing historical cavalry tactics against modern firepower. Upon the reinstatement of Uhlan and Hussar Regiments in 1907 their training pattern, as well as that of the Cuirassiers of the Guard, remained unchanged until the collapse of the Russian Imperial Army.

In Japan, in the late 19th century/early 20th century, dragoons were deployed in the same way as in other armies, but were dressed as hussars.

In 1914 there were still dragoon regiments in the British, French, German, Russian, Austro-Hungarian, Peruvian, Norwegian, Swedish, Danish and Spanish armies. Their uniforms varied greatly, lacking the characteristic features of hussar or lancer regiments. There were occasional reminders of the mounted infantry origins of this class of soldier. Thus the 28 dragoon regiments of the Imperial German Army wore the Pickelhaube (spiked helmet) of the same design as those of the infantry and the British dragoons wore scarlet tunics for full dress while hussars and all but one of the lancer regiments wore dark blue. In other respects however dragoons had adopted the same tactics, roles and equipment as other branches of the cavalry and the distinction had become simply one of traditional titles. Weaponry had ceased to have a historic connection, with both the French and German dragoon regiments carrying lances during the early stages of World War I.

The historic German, Russian and Austro-Hungarian dragoon regiments ceased to exist as distinct branches following the overthrow of the respective imperial regimes of these countries during 1917–18. The Spanish dragoons, which dated back to 1640, were reclassified as numbered cavalry regiments in 1931 as part of the army modernization policies of the new republic.

The Australian Light Horse were similar to 18th-century dragoon regiments in some respects, being mounted infantry which normally fought on foot, their horses' purpose being transportation. They served during the Second Boer War and World War I. The Australian 4th Light Horse Brigade became famous for the Battle of Beersheba in 1917 where they charged on horseback using rifle bayonets, since neither sabres or lances were part of their equipment. Later in the Palestine campaign Pattern 1908 Cavalry Swords were issued and used in the campaign leading to the fall of Damascus.

Probably the last use of real dragoons (infantry on horseback) in combat was made by the Portuguese Army in the war in Angola during the 1960s and 1970s. In 1966, the Portuguese created an experimental horse platoon, to operate against the guerrillas in the high grass region of Eastern Angola, in which each soldier was armed with a G3 assault rifle for combat on foot and with an automatic pistol to fire from horseback. The troops on horseback were able to operate in difficult terrain unsuited to motor vehicles and had the advantage of being able to control the area around them, with a clear view over the grass that foot troops did not have. Moreover, these unconventional troops created a psychological impact on an enemy that was not used to facing horse troops, and thus had no training or strategy to deal with them. The experimental horse platoon was so successful that its entire parent battalion was transformed from an armored reconnaissance unit to a three-squadron horse battalion known as the "Dragoons of Angola". One of the typical operations carried out by the Dragoons of Angola, in cooperation with airmobile forces, consisted of the dragoons chasing the guerrillas and pushing them in one direction, with the airmobile troops being launched from helicopter in the enemy rear, trapping the enemy between the two forces.

Until 1918 Dragoner (en: dragoon) was the designation given to the lowest ranks in the dragoon regiments of the Austro-Hungarian and Imperial German Armies. The "Dragoner" rank, together with all other private ranks of the different branch of service, did belong to the so-called gemeine rank group.

The Brazilian president's honor guard is provided (amongst other units) by a regiment of dragoons: the 1st Guards Cavalry Regiment of the Brazilian Army.

This regiment is known as the "Dragões da Independência" (Independence Dragoons). The name was given in 1927 and refers to the fact that a detachment of dragoons escorted the Prince Royal of Portugal, Pedro I, at the time when he declared Brazilian independence from Portugal, on September 7, 1822.

The Independence Dragoons wear 19th-century dress uniforms similar to those of the earlier Imperial Honor Guard, which are used as the regimental full dress uniform since 1927. The uniform was designed by Debret, in white and red, with plumed bronze helmets. The colors and pattern were influenced by the Austrian dragoons of the period, as the Brazilian Empress Consort was also an Austrian Archduchess. The color of the plumes varies according to rank. The Independence Dragoons are armed with lances and sabres, the latter only for the officers and the colour guard.

The regiment was established in 1808 by the Prince Regent and future king of Portugal, John VI, with the duty of protecting the Portuguese royal family, which had sought refuge in Brazil during the Napoleonic wars. However dragoons had existed in Portugal since at least the early 18th century and, in 1719, units of this type of cavalry were sent to Brazil, initially to escort shipments of gold and diamonds and to guard the Viceroy who resided in Rio de Janeiro (1st Cavalry Regiment – Vice-Roy Guard Squadron). Later, they were also sent to the south to serve against the Spanish during frontier clashes. After the proclamation of Brazilian independence, the title of the regiment was changed to that of the Imperial Honor Guard, with the role of protecting the Imperial Family. The Guard was later disbanded by Emperor Peter II and would be recreated only later in the republican era.

At the time of the Republic proclamation in 1889, horse #6 of the Imperial Honor Guard was ridden by the officer making the declaration of the end of Imperial rule, Second Lieutenant Eduardo José Barbosa. This is commemorated by the custom under which the horse having this number is used only by the commander of the modern regiment.

There are three dragoon regiments in the Canadian Forces: the Royal Canadian Dragoons and two reserve regiments, the British Columbia Dragoons and the Saskatchewan Dragoons.
The Royal Canadian Dragoons is the senior Armoured regiment in the Canadian Forces. The current role of The Royal Canadian Dragoons is to provide Armour Reconnaissance support to 2 Canadian Mechanized Brigade Group (2 CMBG) operations.

The Royal Canadian Mounted Police were accorded the formal status of a regiment of dragoons in 1921. The modern RCMP does not retain any military status however.

Founded as the "Dragones de la Reina" (Queen's Dragoons) in 1758 and later renamed the Dragoons of Chile in 1812, and then becoming the Carabineros de Chile in 1903. The Carabineros are the national police of Chile. The military counterpart, that of the 15th Reinforced Regiment "Dragoons" is now as of 2010 the 4th Armored Brigade "Chorrillos" based in Punta Arenas as the 6th Armored Cavalry Squadron "Dragoons", and form part of the 5th Army Division.

The Royal Danish Army includes amongst its historic regiments the Jutish Dragoon Regiment, which was raised in 1670.

The modern French Army retains three dragoon regiments from the thirty-two in existence at the beginning of World War I: the 2nd, which is a nuclear, biological and chemical protection regiment, the 5th, an experimental Combined arms regiment, and the 13th (Special Reconnaissance).

Beginning in the 17th century, the mercenary army of the Grand Duchy of Lithuania included dragoon units. In the middle of the 17th century there were 1,660 dragoons in an army totaling 8,000 men. By the 18th century there were four regiments of dragoons. 

Lithuanian cavalrymen served in dragoon regiments of both the Russian and Prussian armies, after the Partitions of the Polish–Lithuanian Commonwealth.

Between 1920–1924 and 1935–1940 the Lithuanian Army included the Third Dragoon "Iron Wolf" Regiment. The dragoons were the equivalent of the present-day Volunteer Forces.

In modern Lithuania the Grand Duke Butigeidis Battalion ("Lithuanian: didžiojo kunigaikščio Butigeidžio dragūnų batalionas") is designated as dragoons, with a motorized infantry role.

In the Norwegian Army during the early part of the 20th century, dragoons served in part as mounted troops, and in part on skis or bicycles ("hjulryttere", meaning "wheel-riders"). Dragoons fought on horses, bicycles and skis against the German invasion in 1940. After World War II the dragoon regiments were reorganized as armoured reconnaissance units. "Dragon" is the rank of a compulsory service private cavalryman while enlisted (regular) cavalrymen have the same rank as infantrymen: "Grenader".

The Presidential Escort Life Guard Dragoons Regiment "Field Marshal Domingo Nieto", named after Field Marshal Domingo Nieto, of the President of the Republic of Perú were the traditional Guard of the Government Palace of Perú until March 5, 1987 and its disbandment in that year. However, by Ministerial Resolution No 139-2012/DE/EP of February 2, 2012 the restoration of the Cavalry Regiment "Marshal Domingo Nieto" as the official escort of the President of the Republic of Peru was announced. The main mission of the reestablished regiment was to guarantee the security of the President of the Republic and of the Government Palace.

This regiment of dragoons was created in 1904 following the suggestion of a French military mission which undertook the reorganization of the Peruvian Army in 1896. The initial title of the unit was Cavalry Squadron "President's Escort". It was modelled on the French dragoons of the period. The unit was later renamed as the Cavalry Regiment "President's Escort" before receiving its current title in 1949.

The Peruvian Dragoon Guard has throughout its existence worn French-style uniforms of black tunic and red breeches in winter and white coat and red breeches in summer, with red and white plumed bronze helmets with the coat of arms of Peru and golden or red epaulettes depending on rank. They retain their original armament of lances and sabres, until the 1980s rifles were used for dismounted drill.

At 13:00 hours every day, the main esplanade in front of the Government Palace of Perú fronting Lima's Main Square serves as the stage for the changing of the guard, undertaken by members of the Presidential Life Guard Escort Dragoons, mounted or dismounted. While the dismounted changing is held on Mondays and Fridays, the mounted ceremony is held twice a month on a Sunday.

The Portuguese Army still maintains two units which are descended from former regiments of dragoons. These are the 3rd Regiment of Cavalry (the former "Olivença Dragoons") and the 6th Regiment of Cavalry (the former "Chaves Dragoons"). Both regiments are, presently, armoured units. The Portuguese Rapid Reaction Brigade' Armoured Reconnaissance Squadron – a unit from the 3rd Regiment of Cavalry – is known as the "Paratroopers Dragoons".

During the Portuguese Colonial War in the 1960s and the 1970s, the Portuguese Army created an experimental horse platoon, to combat the guerrillas in eastern Angola. This unit was soon augmented, becoming a group of three squadrons, known as the "Angola Dragoons". The Angola Dragoons operated as mounted infantry – like the original dragoons – each soldier being armed with a pistol to fire when on horseback and with an automatic rifle, to use when dismounted. A unit of the same type was being created in Mozambique when the war ended in 1974.
The Spanish Army began the training of a dragoon corps in 1635 under the direction of Pedro de la Puente at Innsbruck. In 1640 the first dragoon "tercio" was created, equipped with arquebuses and maces. The number of dragoon tercios was increased to nine by the end of the XVII century: three garrisoned in Spain, another three in the Netherlands and the remainder in Milan.

The "tercio"s were converted into a regimental system, beginning in 1704. Philip V created several additional dragoon regiments to perform the functions of a police corps in the New World. Notable amongst those units were the leather-clad "dragones de cuera".

In 1803 the dragoon regiments were renamed as ""caballería ligera"" (light cavalry). By 1815 these units had been disbanded.

Spain recreated its dragoons in the late nineteenth century. In 1930, three Spanish dragoon regiments were still in existence.

In the Swedish Army, dragoons comprise the Military Police and Military Police Rangers. They also form the 13th Battalion of the Life Guards, which is a military police unit. The 13th (Dragoons) Battalion have roots that go back as far as 1523, making it one of the world's oldest military units still in service. Today, the only mounted units still retained by the Swedish Army are the two dragoons squadrons of the King's Guards Battalion of the Life Guards. Horses are used for ceremonial purposes only, most often when the dragoons take part in the changing of the guards at The Royal Palace in Stockholm. ""Livdragon"" is the rank of a private cavalryman.

In the Swiss Army, mounted dragoons existed until the early 1970s, when they were converted into Armoured Grenadiers units. The ""Dragoner"" had to prove he was able to keep a horse at home before entering the army. At the end of basic training they had to buy a horse at a reduced price from the army and to take it home together with equipment, uniform and weapon. In the "yearly repetition course" the dragoons served with their horses, often riding from home to the meeting point.

The abolition of the dragoon units, believed to be the last non-ceremonial horse cavalry in Europe, was a contentious issue in Switzerland. On 5 December 1972 the Swiss "National Council" approved the measure by 91 votes, against 71 for retention.

In the present-day British Army regular army, four regiments are designated as dragoons: The 1st The Queen's Dragoon Guards, The Royal Scots Dragoon Guards, the Royal Dragoon Guards, and the Light Dragoons. In the Territorial Army, one of the five squadrons of the Royal Yeomanry—the Westminster Dragoons— also has the title of dragoons.

The 1st and 2nd Battalion, 48th Infantry were mechanized infantry units assigned to the 3rd Armored Division (3AD) in West Germany during the Cold War. The unit crest of the 48th Infantry designated the unit as Dragoons, purely a traditional designation.

The 1st Dragoons was reformed in the Vietnam War era as the 1st Squadron, 1st U.S. Cavalry. It served in the Iraq War and remains as the oldest cavalry unit, as well as the most decorated one, in the U.S. Army. Today's modern 1–1 Cavalry is a scout/attack unit, equipped with MRAPs, M3A3 Bradley CFVs, and Strykers.

Another modern United States Army unit, informally known as the 2nd Dragoons, is the 2nd Cavalry Regiment. This unit was originally organized as the Second Regiment of Dragoons in 1836 and was renamed the Second Cavalry Regiment in 1861, being redesignated as the 2nd Armored Cavalry Regiment in 1948. The regiment is currently equipped with the Stryker family of wheeled fighting vehicles and was redesignated as the 2nd Stryker Cavalry Regiment in 2006. In 2011 the 2nd Dragoon regiment was redesignated as the 2nd Cavalry Regiment. The 2nd Cavalry Regiment has the distinction of being the longest continuously serving regiment in the United States Army.

The 113th Army Band at Fort Knox is also officially nicknamed as "The Dragoons." This derives from its formation as the Band, First Regiment of Dragoons on July 8, 1840.

Company D, 3rd Light Armored Reconnaissance Battalion of the United States Marine Corps, is nicknamed the "Dragoons". Their combat history includes Operation Iraqi Freedom and Operation Enduring Freedom from 2002 to 2013.



Full


</doc>
<doc id="8768" url="https://en.wikipedia.org/wiki?curid=8768" title="Dulcimer">
Dulcimer

A dulcimer is a type of musical string instrument. It is a variety of zither. Among its forms are:


</doc>
<doc id="8769" url="https://en.wikipedia.org/wiki?curid=8769" title="Dutch West India Company">
Dutch West India Company

The Dutch West India Company (, ; ) was a chartered company (known as the "GWC") of Dutch merchants as well as foreign investors. Among its founders was Willem Usselincx (1567–1647) and Jessé de Forest (1576–1624). On 3 June 1621, it was granted a for a trade monopoly in the Dutch West Indies by the Republic of the Seven United Netherlands and given jurisdiction over Dutch participation in the Atlantic slave trade, Brazil, the Caribbean, and North America. The area where the company could operate consisted of West Africa (between the Tropic of Cancer and the Cape of Good Hope) and the Americas, which included the Pacific Ocean and the eastern part of New Guinea. The intended purpose of the charter was to eliminate competition, particularly Spanish or Portuguese, between the various trading posts established by the merchants. The company became instrumental in the largely ephemeral Dutch colonization of the Americas (including New Netherland) in the seventeenth century. From 1624 to 1654, in the context of the Dutch-Portuguese War, the GWC held Portuguese territory in northeast Brazil, but they were ousted from Dutch Brazil following fierce resistance.

After several reversals, GWC reorganized and a new charter was granted in 1675, largely on the strength in the Atlantic slave trade. This "New" version lasted for more than a century, until after the Fourth Anglo-Dutch War, during which it lost most of its assets.

When the Dutch East India Company (VOC) was founded in 1602, some traders in Amsterdam did not agree with its monopolistic policies. With help from Petrus Plancius, a Dutch-Flemish astronomer, cartographer and clergyman, they sought for a northeastern or northwestern access to Asia to circumvent the VOC monopoly. In 1609, English explorer Henry Hudson, in employment of the VOC, landed on the coast of New England and sailed up what is now known as the Hudson River in his quest for the Northwest Passage to Asia. However, he failed to find a passage. Consequently, in 1615 Isaac Le Maire and Samuel Blommaert, assisted by others, focused on finding a south-westerly route around South America's Tierra del Fuego archipelago in order to circumvent the monopoly of the VOC.

One of the first sailors who focused on trade with Africa was Balthazar de Moucheron. The trade with Africa offered several possibilities to set up trading posts or factories, an important starting point for negotiations. It was Blommaert, however, who stated that, in 1600, eight companies sailed on the coast of Africa, competing with each other for the supply of copper, from the Kingdom of Loango. Pieter van den Broecke was employed by one of these companies. In 1612, a Dutch fortress was built in Mouree (present day Ghana), along the Dutch Gold Coast.

Trade with the Caribbean, for salt, sugar and tobacco, was hampered by Spain and delayed because of peace negotiations. Spain offered peace on condition that the Dutch Republic would withdraw from trading with Asia and America. Spain refused to sign the peace treaty if a West Indian Company would be established. At this time, the Dutch War of Independence (1568–1648) between Spain and the Dutch Republic was occurring. Grand Pensionary Johan van Oldenbarnevelt offered to only suspend trade with the West in exchange for the Twelve Years' Truce. The result was that, during a few years, the company sailed under a foreign flag in South America. However, ten years later, Stadtholder Maurice of Orange, proposed to continue the war with Spain, but also to distract attention from Spain to the Republic. In 1619, his opponent Johan van Oldenbarnevelt was beheaded, and when two years later the truce expired, the West Indian Company was established.

The West India Company received its charter from the States-General in 1621, but its foundation had been suggested much earlier in the 17th century only to be delayed by the conclusion of the Twelve Years' Truce between Spain and the United Provinces in 1609.

The "Dutch West India Company" was organized similarly to the Dutch East India Company (VOC). Like the VOC, the GWC company had five offices, called chambers ("kamers"), in Amsterdam, Rotterdam, Hoorn, Middelburg and Groningen, of which the chambers in Amsterdam and Middelburg contributed most to the company. The board consisted of 19 members, known as the Heeren XIX (the Nineteen Gentlemen). The institutional structure of the GWC followed the federal structure, which entailed extensive discussion for any decision, with regional representation: 8 from Amsterdam; 4 from Zeeland, 2 each from the Northern Quarter (Hoorn and Enkhuizen), the Maas (Rotterdam and Dordrecht), the region of Groningen, and one representative from the States General. Each region had its own chamber and board of directors. The validity of the charter was set at 24 years.

Only in 1623 was funding arranged, after several bidders were put under pressure. The States General of the Netherlands and the VOC pledged one million guilders in the form of capital and subsidy. Although Iberian writers said that crypto-Jews or Marranos played an important role in the formation of both the VOC and the GWC, research has shown that initially they played a minor role, but expanded during the period of the Dutch in Brazil. Emigrant Calvinists from the Spanish Netherlands did make significant investments in the GWC. Investors did not rush to put their money in the company in 1621, but the States-General urged municipalities and other institutions to invest. Explanations for the slow investment by individuals were that shareholders had "no control over the directors' policy and the handling of ordinary investors' money," that it was a "racket" to provide "cushy posts for the directors and their relatives, at the expense of ordinary shareholders." The VOC directors invested money in the GWC, without consulting their shareholders, causing dissent among a number of shareholders. In order to attract foreign shareholders, the GWC offered equal standing to foreign investors with Dutch, resulting in shareholders from France, Switzerland, and Venice. A translation of the original 1621 charter appeared in English, "Orders and Articles granted by the High and Mightie Lords the States General of the United Provinces concerning the erecting of a West-Indies Companie, Anno Dom. MDCXII". by 1623, the capital for the GWC at 2.8 million florins was not as great the VOC's original capitalization of 6.5 million, but it was still a substantial sum. The GWC had 15 ships to carry trade and plied the west African coast and Brazil.

Unlike the VOC, the GWC had no right to deploy military troops. When the Twelve Years' Truce in 1621 was over, the Republic had a free hand to re-wage war with Spain. A "Groot Desseyn" ("grand design") was devised to seize the Portuguese colonies in Africa and the Americas, so as to dominate the sugar and slave trade. When this plan failed, privateering became one of the major goals within the GWC. The arming of merchant ships with guns and soldiers to defend themselves against Spanish ships was of great importance. On almost all ships in 1623, 40 to 50 soldiers were stationed, possibly to assist in the hijacking of enemy ships. It is unclear whether the first expedition was the expedition by Jacques l'Hermite to the coast of Chile, Peru and Bolivia, set up by Stadtholder Maurice with the support of the States General and the VOC.
The company was initially a dismal failure, in terms of its expensive early projects, and its directors shifted emphasis from conquest of territory to pursue plunder of shipping. The most spectacular success for the GWC was Piet Heyn's seizure of the Spanish silver fleet, which carried silver from Spanish colonies to Spain. He had also seized a consignment of sugar from Brazil and a galleon from Honduras with cacao, indigo, and other valuable goods. Privateering was its most profitable activity in the late 1620s. Despite Heyn's success at plunder, the company's directors realized that it was not a basis to build long-term profit, leading them to renew their attempts to seize Iberian territory in the Americas. They decided their target was Brazil.

There were conflicts between directors from different areas of The Netherlands, with Amsterdam less supportive of the company. Non-maritime cities, including Haarlem, Leiden, and Gouda, along with Enkhuizen and Hoorn were enthusiastic about seizing territory. They sent a fleet to Brazil, capturing Olinda and Pernambuco in 1630 in their initial foray to create a Dutch Brazil, but could not hold them due to a strong Portuguese resistance. Company ships continued privateering in the Caribbean, as well seizing vital land resources, particularly salt pans. The company's general lack of success saw their shares plummet and the Dutch and The Spanish renewed truce talks in 1633.

In 1629 the GWC gave permission to a number of investors in New Netherlands to found patroonships, enabled by the Charter of Freedoms and Exemptions which was ratified by the Dutch States-General on June 7, 1629. The patroonships were created to help populate the colony, by providing investors grants providing land for approximately 50 people and "upwards of 15 years old", per grant, mainly in the region of New Netherland. Patroon investors could expand the size of their land grants as large as 4 miles, "along the shore or along one bank of a navigable river..." Rensselaerswyck was the most successful Dutch West India Company patroonship.

The New Netherland area, which included New Amsterdam, covered parts of present-day New York, Connecticut, Delaware, and New Jersey. Other settlements were established on the Netherlands Antilles, and in South America, in Dutch Brazil, Suriname and Guyana. In Africa, posts were established on the Gold Coast (now Ghana), the Slave Coast (now Benin), and briefly in Angola. It was a neo-feudal system, where patrons were permitted considerable powers to control the overseas colony. In the Americas, fur (North America) and sugar (South America) were the most important trade goods, while African settlements traded the enslaved (mainly destined for the plantations on the Antilles and Suriname), gold, and ivory.

In North America, the settlers Albert Burgh, Samuel Blommaert, Samuel Godijn, Johannes de Laet had little success with populating the colony of New Netherland, and to defend themselves against local Amerindians. Only Kiliaen Van Rensselaer managed to maintain his settlement in the north along the Hudson. Samuel Blommaert secretly tried to secure his interests with the founding of the colony of New Sweden on behalf of Sweden on the Delaware in the south. The main focus of the GWC now went to Brazil.

Only in 1630 did the West India Company manage to conquer a part of Brazil. In 1630, the colony of New Holland (capital Mauritsstad, present-day Recife) was founded, taking over Portuguese possessions in Brazil. In the meantime, the war demanded so many of its forces that the Company had to operate under a permanent threat of bankruptcy. In fact, the GWC went bankrupt in 1636 and all attempts at rehabilitation were doomed to failure.

Because of the ongoing war in Brazil, the situation for the GWC in 1645, at the end of the charter, was very bad. An attempt to compensate the losses of the GWC with the profits of the VOC failed because the directors of the VOC did not want to. Merging the two companies was not feasible. Amsterdam was not willing to help out, because it had too much interest in peace and healthy trade relations with Portugal. This indifferent attitude of Amsterdam was the main cause of the slow, half-hearted policy, which would eventually lead to losing the colony. In 1647 the Company made a restart using 1.5 million guilders, capital of the VOC. The States General took responsibility for the warfare in Brazil.

Due to the Peace of Westphalia the seizing of Spanish ships was no longer allowed. Many merchants from Amsterdam and Zeeland decided to work with marine and merchants from Hamburg, Glückstadt (then Danish), England and other countries. In 1649, the GWC obtained a monopoly on gold and enslaved Africans in the kingdom of Accra (present-day Ghana). In 1662 there were contacts with the owners of the Asiento, which were obliged to deliver 24,000 enslaved Africans. In 1663 and 1664 the GWC sold more enslaved Africans than the Portuguese and English together.

The first West India Company suffered a long agony, and its end in 1674 was painless. The reason that the GWC could drag on for twenty years was due to its valuable West African possessions, due to its slaves.

When the GWC could not repay its debts in 1674, the company was dissolved. But because of high demand for trade with the West (mainly slave trade), and the fact that still many colonies existed, it was decided to establish the Second Chartered West India Company (also called New West India Company) in 1675. This new company had the same trade area as the first. All ships, fortresses, etc. were taken over by the new company. The number of directors was reduced from 19 to 10, and the number of governors from 74 to 50. The new GWC had a capital that was slightly more than guilders around 1679, which was largely supplied by the Amsterdam Chamber.

From 1694 until 1700, the GWC waged a long conflict against the Eguafo Kingdom along the Gold Coast, present-day Ghana. The Komenda Wars drew in significant numbers of neighbouring African kingdoms and led to replacement of the gold trade with enslaved Africans.

After the Fourth Anglo-Dutch War, it became apparent that the Dutch West India Company was no longer capable of defending its own colonies, as Sint Eustatius, Berbice, Essequibo, Demerara, and some forts on the Dutch Gold Coast were rapidly taken by the British. In 1791, the company's stock was bought by the Dutch government, and on 1 January 1792, all territories previously held by the Dutch West India Company reverted to the rule of the States General of the Dutch Republic. Around 1800 there was an attempt to create a third West Indian Company, without any success.




</doc>
<doc id="8771" url="https://en.wikipedia.org/wiki?curid=8771" title="Dyula language">
Dyula language

Jula (or Dyula, Dioula, ߖߎ߬ߟߊ߬ߞߊ߲) is a language of the Mande language family spoken in Burkina Faso, Ivory Coast and Mali. It is one of the Manding languages and is most closely related to Bambara, being mutually intelligible with Bambara as well as Malinke. It is a trade language in West Africa and is spoken by millions of people, either as a first or second language. Like the other Mande languages, it is a tonal language. It is written in the Latin script and the Arabic script, as well as in the indigenous N'Ko script.

Dioula can be heard spoken in the 2004 film "Night of Truth", directed by Fanta Régina Nacro, Burkina Faso's first female director.

Dioula orthography is regulated in Burkina Faso by the Dioula Sub-Commission of the National Commission for Languages. On 15 July 1971, the National Sub-Commission for Dioula was created and on 16 July 1971, it began a study in order to set the Dioula alphabet. An alphabet was published on 27 July 1973 and gained official status on 2 February 1979. Some letters were added later, for borrowed words, and others were replaced: by , and by .

In Burkina Faso, the Dioula alphabet is made up of 28 letters each representing a single phoneme. In the orthography, long vowels are represented by doubled letters; thus, /e/ is written and /eː/, . The nasalisation of a vowel is written followed by an n; for example, /ẽ/ is written .

The seven vowel sounds may also be either lengthened /iː eː ɛː aː ɔː oː uː/ or nasalized /ĩ ẽ ɛ̃ ã ɔ̃ õ ũ/.

The notation of tones was recommended in 1973, but in practice they are not written. The transcription guide published in 2003 does not reiterate this recommendation. Tones are noted solely in lexicographical works. However, to avoid ambiguity, tone marking is obligatory in certain cases.

For example:


The N'Ko script is an indigenous writing system for the Manding language continuum, invented in 1949 by Solomana Kanté, a Guinean educator. Today, the script has been digitised as part of Unicode, which allows it to be used easily online, but the lack of funding from governments and the official status of French means that use of this alphabet is largely happens outside of formal education and is not systematically used on street signs, etc.





</doc>
<doc id="8774" url="https://en.wikipedia.org/wiki?curid=8774" title="Desi Arnaz">
Desi Arnaz

Desiderio Alberto Arnaz y de Acha III (March 2, 1917 – December 2, 1986), better known as Desi Arnaz, was a Cuban-American actor, musician, bandleader, comedian and film and television producer. He is best known for his role as the witty Ricky Ricardo on the American television series sitcom "I Love Lucy", where he co-starred with his then wife Lucille Ball. Arnaz and Ball are generally credited as the innovators of the syndicated rerun, which they pioneered with the "I Love Lucy" series.

Arnaz and Lucille Ball co-founded and ran the television production company called Desilu Productions, originally to market "I Love Lucy" to television networks. After "I Love Lucy" ended, Arnaz went on to produce several other television series, at first with Desilu Productions, and later independently, including "The Ann Sothern Show" and "The Untouchables". He was also renowned for leading his Latin music band, the Desi Arnaz Orchestra.

Arnaz was born Desiderio Alberto Arnaz y de Acha, III, in Santiago de Cuba, Cuba, to Desiderio Alberto Arnaz y de Alberni II (March 8, 1894 – May 31, 1973) and Dolores de Acha (April 2, 1896 – October 24, 1988). His father was Santiago's youngest mayor and also served in the Cuban House of Representatives. His maternal grandfather was Alberto de Acha, an executive at rum producer Bacardi & Co.

Arnaz describes the opulent family life of his early youth in his autobiography, "A Book" (1976)—the family owned three ranches, a palatial home, and a vacation mansion on a private island in Santiago Bay, Cuba. Following the Cuban Revolution of 1933, led by Fulgencio Batista, which overthrew President Gerardo Machado, Alberto Arnaz was jailed and all of his property was confiscated. He was released after six months when his brother-in-law Alberto de Acha intervened on his behalf. The family then fled to Miami, where Desi attended St. Patrick Catholic High School. In the summer of 1934, he attended Saint Leo Prep (near Tampa) to help improve his English. His first job was working at Woolworths in Miami. He then went into the tile business with his father before turning to show business full-time.

After finishing high school, Arnaz formed a band, the Siboney Septet, and began making a name for himself in Miami. Xavier Cugat, after seeing Arnaz perform, hired him for his touring orchestra, playing the conga drum and singing. Becoming a star attraction encouraged him to start his own band, the Desi Arnaz Orchestra.

Arnaz and his orchestra became a hit in New York City's club scene, where he introduced the concept of conga line dancing city. He came to the attention of Rogers and Hart who, in 1939, cast him in their Broadway musical "Too Many Girls". The show was a hit and RKO Pictures bought the movie rights.

Arnaz went to Hollywood the next year to appear in the show's movie version at RKO, which also starred Lucille Ball. Arnaz and Ball fell in love during the film's production and eloped on November 30, 1940.

Arnaz appeared in several movies in the 1940s such as "Bataan", starring Robert Taylor (1943). Many consider his portrayal of the jive-loving California National Guardsman Felix Ramirez to be his best early role.

He received his draft notice, but before reporting, he injured his knee. He completed his recruit training, but was classified for limited service in the United States Army during World War II. He was assigned to direct United Service Organization (USO) programs at the Birmingham General Army Hospital in the San Fernando Valley. Discovering the first thing the wounded soldiers requested was a glass of cold milk, he arranged for movie starlets to meet them and pour the milk for them.

Following his discharge from the Army on November 16, 1945, he formed another orchestra, which was successful in live appearances and recordings. He sang for troops in Birmingham Hospital with John Macchia and hired his childhood friend Marco Rizo to play piano and arrange for the orchestra.

For the 1946–47 season, Arnaz was the bandleader, conducting his Desi Arnaz Orchestra, on Bob Hope's radio show ("The Pepsodent Show") on NBC.

In 1951, Arnaz was given a game show on CBS Radio, "Your Tropical Trip" in order to entice Arnaz and Ball to stay at CBS over a competing offer from NBC, and to keep Arnaz and his band employed and in Hollywood, rather than touring. The musical game show, hosted by Arnaz, had audience members competing for a Caribbean vacation and also featured Arbaz's orchestra. The program aired from January 1951 until September, shortly before the premiere of "I Love Lucy" in October.

When he became successful in television, he kept the orchestra on his payroll, and Rizo arranged and orchestrated the music for "I Love Lucy".

On October 15, 1951, Arnaz co-starred in the premiere of "I Love Lucy", in which he played a fictionalized version of himself, Cuban orchestra leader Enrique "Ricky" Ricardo. His co-star was his real-life wife, Lucille Ball, who played Ricky's wife, Lucy. Television executives had been pursuing Ball to adapt her very popular radio series "My Favorite Husband" for television. Ball insisted on Arnaz playing her on-air spouse so the two would be able to spend more time together. CBS wanted Ball's "Husband" co-star Richard Denning.

The original premise was for the couple to portray Lucy and Larry Lopez, a successful show business couple whose glamorous careers interfered with their efforts to maintain a normal marriage. Market research indicated, however, that this scenario would not be popular, so Jess Oppenheimer changed it to make Ricky Ricardo a struggling young orchestra leader and Lucy an ordinary housewife who had show business fantasies but no talent. The character name "Larry Lopez" was dropped because of a real-life bandleader named Vincent Lopez, and was replaced with "Ricky Ricardo". The name was inspired by Henry Richard, a family friend and the brother of P.C. Richard of P.C. Richard & Son. This name translates to Enrique Ricardo. Ricky often appeared at, and later owned, the Tropicana Club, which under his ownership he renamed Club Babalu.

Initially, the idea of having Ball and the distinctly Latin American Arnaz portray a married couple encountered resistance as they were told that Desi's Cuban accent and Latin style would not be agreeable to American viewers. The couple overcame these objections, however, by touring together, during the summer of 1950, in a live vaudeville act they developed with the help of Spanish clown Pepito Pérez, together with Ball's radio show writers. Much of the material from their vaudeville act, including Lucy's memorable seal routine, was used in the pilot episode of "I Love Lucy". Segments of the pilot were recreated in the sixth episode of the show's first season. During his time on the show, Arnaz and Ball became TV's most successful entrepreneurs.

With Ball, Arnaz founded Desilu Productions in 1950, initially to produce the vaudeville-style touring act that led to "I Love Lucy". At that time, most television programs were broadcast live, and as the largest markets were in New York, the rest of the country received only kinescope images. Karl Freund, Arnaz's cameraman, and even Arnaz himself have been credited with the development of the multiple-camera setup production style using adjacent sets in front of a live audience that became the standard for subsequent situation comedies. The use of film enabled every station around the country to broadcast high-quality images of the show. Arnaz was told that it would be impossible to allow an audience onto a sound stage, but he worked with Freund to design a set that would accommodate an audience, allow filming, and adhere to fire and safety codes.. Due to the expense of 35mm film, Arnaz and Ball agreed to salary cuts. In return they retained the rights to the films. This was the basis for their invention of re-runs and syndicating TV shows (a huge source of new revenue).

In addition to "I Love Lucy", he executive produced "The Ann Sothern Show" and "Those Whiting Girls" (starring Margaret Whiting and Barbara Whiting Smith), and was involved in several other series such as "The Untouchables", "Whirlybirds", and "Sheriff of Cochise" / "United States Marshal". He also produced the feature film "Forever, Darling" (1956), in which he and Ball starred. In the late 1950s, Arnaz proposed a Western television series to his then neighbor, Victor Orsatti, who formed a production company, Ror-Vic, in partnership with actor Rory Calhoun. Ror-Vic produced "The Texan", which aired on Monday evenings on CBS from 1958 to 1960. Episodes were budgeted at $40,000 each, with two black-and-white segments filmed weekly through Desilu Studios. Despite the name, the series was filmed mostly in Pearl Flats in the Mojave Desert of Southern California. The program could have been renewed for a third season had Calhoun not desired to return to films.

The original Desilu company continued long after Arnaz's divorce from Ball and her subsequent marriage to Gary Morton. Desilu produced its own programs and provided facilities to other producers. Desilu produced "The Andy Griffith Show", "The Dick Van Dyke Show", "The Lucy Show", "", and "". When Ball sold her share of Desilu to what became Paramount Television, Arnaz went on to form his own production company from his share of Desilu. With the newly formed Desi Arnaz Productions, he made "The Mothers-In-Law" (at Desilu) for United Artists Television and NBC. This sitcom ran for two seasons from 1967 to 1969. Arnaz's company was succeeded-in-interest by the company now known as Desilu, Too. Desilu, Too and Lucille Ball Productions worked hand-in-hand with MPI Home Video in the home video reissues of the Ball/Arnaz material not owned by CBS (successor-in-interest to Paramount Television, which in turn succeeded the original Desilu company). This material included "Here's Lucy" and "The Mothers-In-Law", as well as many programs and specials Ball and Arnaz made independently of each other.

Arnaz and Ball decided that the show would maintain what Arnaz termed "basic good taste" and were therefore determined to avoid ethnic jokes, as well as humor based on physical handicaps or mental disabilities. Arnaz recalled that the only exception consisted of making fun of Ricky Ricardo's accent; even these jokes worked only when Lucy, as his wife, did the mimicking.

Arnaz was deeply patriotic about the United States. In his memoirs, he wrote that he knew of no other country in the world where "a sixteen-year-old kid, broke and unable to speak the language" could achieve the successes that he had.

Arnaz and Lucille Ball were married on November 30, 1940. Their marriage was turbulent. Convinced that Arnaz was being unfaithful to her and also because he came home drunk several times, Ball filed for divorce in September 1944, but returned to him before the interlocutory decree became final. Arnaz and Ball subsequently had two children, actors Lucie Arnaz (born 1951) and Desi Arnaz Jr. (born 1953).

Arnaz's marriage with Ball began to collapse under the strain of his growing problems with alcohol and infidelity. According to his memoir, the combined pressures of managing the production company, as well as supervising its day-to-day operations, had greatly worsened as it grew much larger, and he felt compelled to seek outlets to alleviate the stress. Arnaz was also suffering from diverticulitis. Ball divorced him in 1960. When Ball returned to weekly television, she and Arnaz worked out an agreement regarding Desilu, wherein she bought him out.

Arnaz married his second wife, Edith Eyre Hirsch (née McSkimming), on March 2, 1963, and greatly reduced his show business activities. He served as executive producer of "The Mothers-in-Law", and during its two-year run, made four guest appearances as a Spanish matador, Señor Delgado. Edith died in 1985, aged 67, from cancer.

Although Arnaz and Ball both married other spouses after their divorce in 1960, they remained friends and grew closer in his final decade. ""I Love Lucy" was never just a title", wrote Arnaz in the last years of his life. Family home video later aired on television showed Ball and Arnaz playing together with their grandson Simon shortly before Arnaz's death.

In the 1970s, Arnaz co-hosted a week of shows with daytime host and producer Mike Douglas. Vivian Vance appeared as a guest. Arnaz also headlined a "Kraft Music Hall" special on NBC that featured his two children, with a brief appearance by Vance. To promote his autobiography, "A Book", on February 21, 1976, Arnaz served as a guest host on "Saturday Night Live", with his son, Desi, Jr., also appearing. The program contained spoofs of "I Love Lucy" and "The Untouchables". The spoofs of "I Love Lucy" were supposed to be earlier concepts of the show that never made it on the air, such as "I Love Louie", where Desi lived with Louis Armstrong. He read Lewis Carroll's poem "Jabberwocky" in a heavy Cuban accent (he pronounced it "Habberwocky"). Desi Jr., played the drums and, supported by the "SNL" band, Desi sang both "Babalú" and another favorite from his dance band days, "Cuban Pete"; the arrangements were similar to the ones used on "I Love Lucy". He ended the broadcast by leading the entire cast in a raucous conga line through the "SNL" studio.

Arnaz and his wife eventually moved to Del Mar, California, where he lived the rest of his life in semi-retirement. He owned a horse-breeding farm in Corona, California, and raced thoroughbreds. He contributed to charitable and nonprofit organizations, including San Diego State University. He also taught classes at San Diego State in studio production and acting for television. Arnaz made a guest appearance on the TV series "Alice", starring Linda Lavin and produced by "I Love Lucy" co-creators Madelyn Pugh (Madelyn Davis) and Bob Carroll, Jr.

Arnaz was a regular smoker for much of his life and often smoked cigarettes on the set of "I Love Lucy". He smoked Cuban cigars until he was in his sixties. Arnaz was diagnosed with lung cancer in 1986, and died several months later on December 2, 1986, at the age of 69. Arnaz was cremated and his ashes scattered. His death came just five days before Lucille Ball received the Kennedy Center Honors. He was predeceased by his second wife, Edith, who died a year earlier on March 23, 1985. His mother outlived him by almost two years.

Desi Arnaz has two stars on the Hollywood Walk of Fame: one at 6301 Hollywood Boulevard for contributions to motion pictures and one at 6250 Hollywood Boulevard for television. Unlike his co-stars, Arnaz was never nominated for an Emmy for his performance in "I Love Lucy". In 1956, he won a Golden Globe for Best Television Achievement for helping to shape the American Comedy through his contributions in front of and behind the camera of "I Love Lucy". He was inducted into the Television Academy's Hall of Fame.

The Lucille Ball-Desi Arnaz Center museum is in Jamestown, New York, and the Desi Arnaz Bandshell in the Lucille Ball Memorial Park is in Celoron, New York.

Desi Arnaz appears as a character in Oscar Hijuelos' 1989 novel "The Mambo Kings Play Songs of Love" and is portrayed by his son, Desi Arnaz, Jr. in the 1992 film adaptation, "The Mambo Kings".

Arnaz was portrayed by Oscar Nuñez in "I Love Lucy: A Funny Thing Happened on the Way to the Sitcom", a comedy about how Arnaz and Ball battled to get their sitcom on the air. It had its world premiere in Los Angeles on July 12, 2018, co-starring Sarah Drew as Lucille Ball and Seamus Dever as "I Love Lucy" creator-producer-head writer Jess Oppenheimer. The play, written by Jess Oppenheimer's son, Gregg Oppenheimer, was recorded in front of a live audience for nationwide public radio broadcast and online distribution.

On 2 March 2019, Google celebrated what would have been Arnaz's 102nd birthday with a Google doodle.

Oscar Isaac has been cast to portray Desi in the upcoming Amazon biopic, co-starring Cate Blanchett as Lucille Ball. This project was proposed in 2015 and was still in development in May 2020, without a casting for Arnaz.








</doc>
<doc id="8777" url="https://en.wikipedia.org/wiki?curid=8777" title="DNA virus">
DNA virus

A DNA virus is a virus that has DNA as its genetic material and replicates using a DNA-dependent DNA polymerase. The nucleic acid is usually double-stranded DNA (dsDNA) but may also be single-stranded DNA (ssDNA). DNA viruses belong to either "Group I" or "Group II" of the Baltimore classification system for viruses. Single-stranded DNA is usually expanded to double-stranded in infected cells. Although "Group VII" viruses such as hepatitis B contain a DNA genome, they are not considered DNA viruses according to the Baltimore classification, but rather reverse transcribing viruses because they replicate through an RNA intermediate. Notable diseases like smallpox, herpes, and the chickenpox are caused by such DNA viruses.

Genome organization within this group varies considerably. Some have circular genomes ("Baculoviridae", "Papovaviridae" and "Polydnaviridae") while others have linear genomes ("Adenoviridae", "Herpesviridae" and some phages). Some families have circularly permuted linear genomes (phage T4 and some "Iridoviridae"). Others have linear genomes with covalently closed ends ("Poxviridae" and "Phycodnaviridae").

A virus infecting archaea was first described in 1974. Several others have been described since: most have head-tail morphologies and linear double-stranded DNA genomes. Other morphologies have also been described: spindle shaped, rod shaped, filamentous, icosahedral and spherical. Additional morphological types may exist.

Orders within this group are defined on the basis of morphology rather than DNA sequence similarity. It is thought that morphology is more conserved in this group than sequence similarity or gene order which is extremely variable. Three orders and 31 families are currently recognised. A fourth order—Megavirales—for the nucleocytoplasmic large DNA viruses has been proposed. This proposal has yet to be ratified by the ICTV. Four genera are recognised that have not yet been assigned a family.

Fifteen families are enveloped. These include all three families in the order "Herpesvirales" and the following families: "Ascoviridae", "Ampullaviridae", "Asfarviridae", "Baculoviridae", "Fuselloviridae", "Globuloviridae", "Guttaviridae", "Hytrosaviridae", "Iridoviridae", "Lipothrixviridae", "Nimaviridae" and "Poxviridae".

Bacteriophages (viruses infecting bacteria) belonging to the families "Tectiviridae" and "Corticoviridae" have a lipid bilayer membrane inside the icosahedral protein capsid and the membrane surrounds the genome. The crenarchaeal virus "Sulfolobus turreted icosahedral virus" has a similar structure.

The genomes in this group vary considerably from ~10 kilobases to over 2.5 megabases in length. The largest bacteriophage known is Klebsiella Phage vB_KleM-RaK2 which has a genome of 346 kilobases.

The virophages are a group of viruses that infect other viruses.

A virus with a novel method of genome packing infecting species of the genus "Sulfolobus" has been described. As this virus does not resemble any known virus it has been classified into a new family, the "Portogloboviridae".

Another "Sulfolobus" infecting virus—Sulfolobus ellipsoid virus 1—has been described. This enveloped virus has a unique capsid and may be classified into a new taxon.

Species of the order "Caudovirales" and of the families "Corticoviridae" and "Tectiviridae" infect bacteria.

Species of the order "Ligamenvirales" and the families "Ampullaviridae", "Bicaudaviridae", "Clavaviridae", "Fuselloviridae", "Globuloviridae", "Guttaviridae" , "Tristromaviridae" and "Turriviridae" infect hyperthermophilic archaea species of the "Crenarchaeota".

Species of the order "Herpesvirales" and of the families "Adenoviridae", "Asfarviridae", "Iridoviridae", "Papillomaviridae", "Polyomaviridae" and "Poxviridae" infect vertebrates.

Species of the families "Ascovirus", "Baculovirus", "Hytrosaviridae", "Iridoviridae" and "Polydnaviruses" and of the genus "Nudivirus" infect insects.

Species of the family "Mimiviridae" and the species "Marseillevirus", "Megavirus", "Mavirus virophage" and "Sputnik virophage" infect protozoa.

Species of the family "Nimaviridae" infect crustaceans.

Species of the family "Phycodnaviridae" and the species "Organic Lake virophage" infect algae. These are the only known dsDNA viruses that infect plants.

Species of the family "Plasmaviridae" infect species of the class "Mollicutes".

Species of the family "Pandoraviridae" infect amoebae.

Species of the genus "Dinodnavirus" infect dinoflagellates. These are the only known viruses that infect dinoflagellates.

Species of the genus "Rhizidiovirus" infect stramenopiles. These are the only known dsDNA viruses that infect stramenopiles.

Species of the genus "Salterprovirus" and "Sphaerolipoviridae" infect species of the "Euryarchaeota".

The classification of DNA viruses has undergone a revision by the International Committee on the Taxonomy of Viruses in 2020.

Three realms are recognised: Duplodnaviria, Monodnaviria and Varidnaviria. The Duplodnaviria contain several types of double stranded DNA viruses. The Monodnaviria all the single stranded viruses. The Varidnaviria are a taxon of viruses united by the presence of use of vertically-folded jelly roll capsid. 

Within the Duplodnaviria one kingdom (Heunggongvirae) and two phlya (Peploviricota, Uroviricota) are recognised. The phylum Peploviricota has one order - "Herpesvirales". The phylum Uroviricota has one order - "Caudovirales". 

Within the Monodnaviria four kingdoms (Loebvirae, Sangervirae, Shotokuvirae and Trapavirae) are recognised. 

Within the Varidnaviria two kingdoms (Bamfordvirae, Helvetiavirae) are recognised. 



A group of double stranded DNA viruses have been found in fish that appear to be related to the herpesviruses.

Another group of viruses that infect fish has been described.

The asfarviruses, iridoviruses, mimiviruses, phycodnaviruses and poxviruses have been shown to belong to a single group,—the large nuclear and cytoplasmic DNA viruses. These are also abbreviated "NCLDV". This clade can be divided into two groups:


It is probable that these viruses evolved before the separation of eukaryoyes into the extant crown groups. The ancestral genome was complex with at least 41 genes including (1) the replication machinery (2) up to four RNA polymerase subunits (3) at least three transcription factors (4) capping and polyadenylation enzymes (5) the DNA packaging apparatus (6) and structural components of an icosahedral capsid and the viral membrane.

The evolution of this group of viruses appears to be complex with genes having been gained from multiple sources. It has been proposed that the ancestor of NCLDVs has evolved from large, virus-like DNA transposons of the Polinton/Maverick family. From Polinton/Maverick transposons NCLDVs might have inherited the key components required for virion morphogenesis, including the major and minor capsid proteins, maturation protease and genome packaging ATPase.

Another group of large viruses—the Pandoraviridae—has been described. Two species—Pandoravirus salinus and Pandoravirus dulcis—have been recognized. These were isolated from Chile and Australia respectively. These viruses are about one micrometer in diameter making them one of the largest viruses discovered so far. Their gene complement is larger than any other known virus to date. At present they appear to be unrelated to any other species of virus.

An even larger genus, Pithovirus, has since been discovered, measuring about 1.5 µm in length. Another virus—Cedratvirus—may be related this group.

A group known as the pleolipoviruses, although having a similar genome organisation, differ in having either single or double stranded DNA genomes. Within the double stranded forms have runs of single stranded DNA. These viruses have been placed in the family "Pleolipoviridae". This family has been divided in three genera: Alphapleolipovirus, Betapleolipovirus and Gammapleolipovirus.

These viruses are nonlytic and form virions characterized by a lipid vesicle enclosing the genome. They do not have nucleoproteins. The lipids in the viral membrane are unselectively acquired from host cell membranes. The virions contain two to three major structural proteins, which either are embedded in the membrane or form spikes distributed randomly on the external membrane surface.

This group includes the following viruses:


Although bacteriophages were first described in 1927, it was only in 1959 that Sinshemer working with phage Phi X 174 showed that they could possess single-stranded DNA genomes. Despite this discovery, until relatively recently it was believed that most DNA viruses contained double-stranded DNA. Recent work, however, has shown that single-stranded DNA viruses can be highly abundant in seawater, freshwater, sediments, terrestrial and extreme environments, as well as metazoan-associated and marine microbial mats. Many of these "environmental" viruses belong to the family "Microviridae". However, the vast majority has yet to be classified and assigned to genera and higher taxa. Because most of these viruses do not appear to be related, or are only distantly related to known viruses, additional taxa will have to be created to accommodate them.


Although ~50 archaeal viruses are known, all but two have double stranded genomes. These two viruses have been placed in the families "Pleolipoviridae" and "Spiraviridae"

The taxonomy of DNA viruses was significantly revised in 2020. All the single stranded DNA viruses have been placed in a single realm - Monodnaviria. Within this realm four kingdoms (Loebvirae, Sangervirae, Shotokuvirae and Trapavirae) are recognised.


A division of the circular single stranded viruses into four types has been proposed. This division seems likely to reflect their phylogenetic relationships.

Type I genomes are characterized by a small circular DNA genome (approximately 2-kb), with the Rep protein and the major open reading frame (ORF) in opposite orientations. This type is characteristic of the circoviruses, geminiviruses and nanoviruses.

Type II genomes have the unique feature of two separate Rep ORFs.

Type III genomes contain two major ORFs in the same orientation. This arrangement is typical of the anelloviruses.

Type IV genomes have the largest genomes of nearly 4-kb, with up to eight ORFs. This type of genome is found in the Inoviridae and the Microviridae.

Given the variety of single stranded viruses that have been described this scheme—if it is accepted by the ICTV—will need to be extended.

All known eukaryotic ssDNA viruses have icosahedral capsids. With the exception of the family "Bidnaviridae" and "Anelloviridae", all eukaryotic ssDNA viruses encode homologous rolling-circle replication initiation proteins with characteristic N-terminal endonuclease domains and C-terminal superfamily three helicase domains. A name for this group of viruses has been proposed—circular Rep-encoding single-strand (CRESS) DNA viruses. It has been proposed that CRESS-DNA viruses have evolved from bacterial plasmids, from which they inherited the Rep genes.

A group of ssDNA viruses whose Rep proteins show homology to ssDNA viruses from the families Geminiviridae, Circoviridae, and Nanoviridae, while their coat protein is related to those of ssRNA viruses from the family Tombusviridae and unclassified oomycete-infecting viruses. The name Cruciviridae has been proposed for this group.

The families "Bidnaviridae" and "Parvoviridae" have linear genomes while the other families have circular genomes. The "Bidnaviridae" have a two part genome and infect invertebrates. The "Inoviridae" and "Microviridae" infect bacteria; the "Anelloviridae" and "Circoviridae" infect animals (mammals and birds respectively); and the "Geminiviridae" and "Nanoviridae" infect plants. In both the "Geminiviridae" and "Nanoviridae" the genome is composed of more than a single chromosome. The "Bacillariodnaviridae" infect diatoms and have a unique genome: the major chromosome is circular (~6 kilobases in length): the minor chromosome is linear (~1 kilobase in length) and complementary to part of the major chromosome. Members of the "Spiraviridae" infect archaea. Members of the "Genomoviridae" infect fungi.

All viruses in this group require formation of a replicative form—a double stranded DNA intermediate—for genome replication. This is normally created from the viral DNA with the assistance of the host's own DNA polymerase.

In the 9th edition of the viral taxonomy of the ICTV (published 2011) the Bombyx mori densovirus type 2 was placed in a new family—the "Bidnaviridae" on the basis of its genome structure and replication mechanism. This is currently the only member of this family but it seems likely that other species will be allocated to this family in the near future.

A new genus—Bufavirus—was proposed on the basis of the isolation of two new viruses from human stool. Another member of this genus—megabat bufavius 1—has been reported from bats. The human viruses have since been renamed Primate protoparvovirus and been placed in the genus Protoparvovirus.

Another proposed genus is Pecovirus. These are similar in organisation to the Smacovirus but share little sequence similarity.


The most recently introduced family of ssDNA viruses is the "Genomoviridae" (the family name is an acronym derived from "ge"minivirus-like, "no mo"vement protein).

The family includes 9 genera, namely "Gemycircularvirus", "Gemyduguivirus", "Gemygorvirus", "Gemykibivirus", "Gemykolovirus", "Gemykrogvirus", "Gemykroznavirus", "Gemytondvirus" and "Gemyvongvirus".

The genus name "Gemycircularvirus" stands for Gemini-like myco-infecting circular virus. the type species of the genus "Gemycircularvirus"—Sclerotinia sclerotiorum hypovirulence associated DNA virus 1—is currently the only cultivated member of the family. The rest of genomoviruses are uncultivated and have been discovered using metagenomics techniques.

Another genus has been proposed—"Gemybolavirus".


Isolates from this group have also been isolated from the cerebrospinal fluid and brains of patients with multiple sclerosis.

An isolate from this group has also been identified in a child with encephalitis.

Viruses from this group have also been isolated from the blood of HIV+ve patients.


Ostrich faecal associated ssDNA virus has been placed in the genus Gemytondvirus. Rabbit faecal associated ssDNA virus has been placed in the genus Gemykroznavirus.

Another virus from this group has been isolated from mosquitoes.

Ten new circular viruses have been isolated from dragonfly larvae. The genomes range from 1628 to 2668 nucleotides in length. These dragonfly viruses have since been placed in the Gemycircularviridae.

Additional viruses from this group have been reported from dragonflies and damselflies.


Three viruses in this group have been isolated from plants.

A virus—Cassava associated circular DNA virus—that has some similarity to Sclerotinia sclerotiorum hypovirulence associated DNA virus 1 has been isolated. This virus has been placed in the Gemycircularviridae.

Some of this group of viruses may infect fungi.

A new family, the "Smacoviridae", has been created for a number of single-stranded DNA viruses isolated from the faeces of various mammals. Smacoviruses have circular genomes of ~2.5 kilobases and have a Rep protein and capsid protein encoded in opposite orientations. 43 species have been included in this family which includes six genera—"Bovismacovirus", "Cosmacovirus", "Dragsmacovirus", "Drosmacovirus", "Huchismacovirus" and "Porprismacovirus".

A number of additional single stranded DNA viruses have been described but are as yet unclassified.

Viruses in this group have been isolated from other cases of encephalitis, diarrhoea and sewage.

Two viruses have been isolated from human faeces—circo-like virus Brazil hs1 and hs2—with genome lengths of 2526 and 2533 nucleotides respectively. These viruses have four open reading frames. These viruses appear to be related to three viruses previously isolated from waste water, a bat and from a rodent.
This appears to belong to a novel group.

A novel species of virus—human respiratory-associated PSCV-5-like virus—has been isolated from the respiratory tract. The virus is approximately 3 kilobases in length and has two open reading frames—one encoding the coat protein and the other the DNA replicase. The significance—if any—of this virus for human disease is unknown presently.

An unrelated group of ssDNA viruses, also discovered using viral metagenomics, includes the species bovine stool associated circular virus and chimpanzee stool associated circular virus. The closest relations to this genus appear to be the "Nanoviridae" but further work will be needed to confirm this. Another isolate that appears to be related to these viruses has been isolated from pig faeces in New Zealand. This isolate also appears to be related to the pig stool-associated single-stranded DNA virus. This virus has two large open reading frames one encoding the capsid gene and the other the Rep gene. These are bidirectionally transcribed and separated by intergenic regions. Another virus of this group has been reported again from pigs. A virus from this group has been isolated from turkey faeces. Another ten viruses from this group have been isolated from pig faeces. Viruses that appear to belong to this group have been isolated from other mammals including cows, rodents, bats, badgers and foxes.

Another virus in this group has been isolated from birds.

Fur seal feces-associated circular DNA virus was isolated from the faeces of a fur seal ("Arctocephalus forsteri") in New Zealand. The genome has 2 main open reading frames and is 2925 nucleotides in length. Another virus—porcine stool associated virus 4—has been isolated. It appears to be related to the fur seal virus.
Two viruses have been described from the nesting material yellow crowned parakeet ("Cyanoramphus auriceps")—Cyanoramphus nest-associated circular X virus (2308 nt) and Cyanoramphus nest-associated circular K virus (2087 nt) Both viruses have two bidirectional open reading frames. Within these are the rolling-circle replication motifs I, II, III and the helicase motifs Walker A and Walker B. There is also a conserved nonanucleotide motif required for rolling-circle replication. CynNCKV has some similarity to the picobiliphyte nano-like virus (Picobiliphyte M5584-5) and CynNCXV has some similarity to the rodent stool associated virus (RodSCV M-45).

A virus with a circular genome—sea turtle tornovirus 1—has been isolated from a sea turtle with fibropapillomatosis. It is sufficiently unrelated to any other known virus that it may belong to a new family. The closest relations seem to be the "Gyrovirinae". The proposed genus name for this virus is Tornovirus.

Another faecal virus—feline stool-associated circular DNA virus—has been described.

Among these are the parvovirus-like viruses. These have linear single-stranded DNA genomes but unlike the parvoviruses the genome is bipartate. This group includes Hepatopancreatic parvo-like virus and Lymphoidal parvo-like virus. A new family Bidensoviridae has been proposed for this group but this proposal has not been ratified by the ICTV to date. Their closest relations appear to be the "Brevidensoviruses" (family Parvoviridae).

A virus—Acheta domesticus volvovirus—has been isolated from the house cricket ("Acheta domesticus"). The genome is circular, has four open reading frames and is 2,517 nucleotides in length. It appears to be unrelated to previously described species. The genus name Volvovirus has been proposed for these species. The genomes in this genus are ~2.5 nucleotides in length and encode 4 open reading frames.

Two new viruses have been isolated from the copepods "Acartia tonsa" and "Labidocera aestiva"—Acartia tonsa copepod circo-like virus and Labidocera aestiva copepod circo-like virus respectively.

A virus has been isolated from the mud flat snail ("Amphibola crenata"). This virus has a single stranded circular genome of 2351 nucleotides that encodes 2 open reading frames that are oriented in opposite directions. The smaller open reading frame (874 nucleotides) encodes a protein with similarities to the "Rep" (replication) proteins of circoviruses and plasmids. The larger open reading frame (955 nucleotides) has no homology to any currently known protein.

An unusual—and as yet unnamed—virus has been isolated from the flatworm "Girardia tigrina". Because of its genome organization, this virus appears to belong to an entirely new family. It is the first virus to be isolated from a flatworm.

From the hepatopancreas of the shrimp ("Farfantepenaeus duorarum") a circular single stranded DNA virus has been isolated. This virus does not appear to cause disease in the shrimp.

A circo-like virus has been isolated from the shrimp ("Penaeus monodon"). The 1,777-nucleotide genome is circular and single stranded. It has some similarity to the circoviruses and cycloviruses.

Ten viruses have been isolated from echinoderms. All appear to belong to as yet undescribed genera.

A filamentous virus—Apis mellifera filamentous virus—has been described. It appears to be unrelated to other DNA viruses.

A circular single stranded DNA virus has been isolated from a grapevine. This species may be related to the family Geminiviridae but differs from this family in a number of important respects including genome size.

Several viruses—baminivirus, nepavirus and niminivirus—related to geminvirus have also been reported.

A virus—Ancient caribou feces associated virus—has been cloned from 700-y-old caribou faeces.

A new virus with a three part single stranded genome has been reported. This seems likely to be a member of a new family of viruses.

More than 600 single-stranded DNA viral genomes were identified in ssDNA purified from seawater. These fell into 129 genetically distinct groups that had no recognizable similarity to each other or to other virus sequences, and thus many likely represent new families of viruses. Of the 129 groups, eleven were much more abundant than the others, and although their hosts have yet to be identified, they are likely to be eukaryotic phytoplankton, zooplankton and bacteria.

A virus—Boiling Springs Lake virus—appears to have evolved by a recombination event between a DNA virus (circovirus) and an RNA virus (tombusvirus). The genome is circular and encodes two proteins—a Rep protein and a capsid protein.

Further reports of viruses that appear to have evolved from recombination events between ssRNA and ssDNA viruses have been made.

A new virus has been isolated from the diatom "Chaetoceros setoensis". It has a single stranded DNA genome and does not appear to be a member of any previously described group.

A virus—FLIP (Flavobacterium-infecting, lipid-containing phage)—has been isolated from a lake. This virus has a circular ssDNA genome (9,174 nucleotides) and an internal lipid membrane enclosed in an icosahedral capsid. The capsid organisation is he capsid organization pseudo T = 21 "dextro". The major capsid protein has two β-barrels. The capsid organisation is similar to bacteriophage PM2—a double stranded bacterial virus.

Satellite viruses are small viruses with either RNA or DNA as their genomic material that require another virus to replicate. There are two types of DNA satellite viruses—the alphasatellites and the betasatellites—both of which are dependent on begomoviruses. At present satellite viruses are not classified into genera or higher taxa.

Alphasatellites are small circular single strand DNA viruses that require a begomovirus for transmission. Betasatellites are small linear single stranded DNA viruses that require a begomovirus to replicate.

Phylogenetic relationships between these families are difficult to determine. The genomes differ significantly in size and organisation. Most studies that have attempted to determine these relationships are based either on some of the more conserved proteins—DNA polymerase and others—or on common structural features. In general most of the proposed relationships are tentative and have not yet been used by the ICTV in their classification.

While determining the phylogenetic relations between the various known clades of viruses is difficult, on a number of grounds the herpesviruses and caudoviruses appear to be related.

While the three families in the order Herpesvirales are clearly related on morphological grounds, it has proven difficult to determine the dates of divergence between them because of the lack of gene conservation. On morphological grounds they appear to be related to the bacteriophages—specifically the Caudoviruses.

The branching order among the herpesviruses suggests that "Alloherpesviridae" is the basal clade and that "Herpesviridae" and "Malacoherpesviridae" are sister clades. Given the phylogenetic distances between vertebrates and molluscs this suggests that herpesviruses were initially fish viruses and that they have evolved with their hosts to infect other vertebrates.

The vertebrate herpesviruses initially evolved ~ and underwent subsequent evolution on the supercontinent Pangaea. The alphaherpesvirinae separated from the branch leading to the betaherpesvirinae and gammaherpesvirinae about to . The avian herpes viruses diverged from the branch leading to the mammalian species. The mammalian species divided into two branches—the Simplexvirus and Varicellovirus genera. This latter divergence appears to have occurred around the time of the mammalian radiation.

Several dsDNA bacteriophages and the herpesviruses encode a powerful ATP driven DNA translocating machine that encapsidates a viral genome into a preformed capsid shell or prohead. The critical components of the packaging machine are the packaging enzyme (terminase) which acts as the motor and the portal protein that forms the unique DNA entrance vertex of prohead. The terminase complex consists of a recognition subunit (small terminase) and an endonuclease/translocase subunit (large terminase) and cuts viral genome concatemers. It forms a motor complex containing five large terminase subunits. The terminase-viral DNA complex docks on the portal vertex. The pentameric motor processively translocates DNA until the head shell is full with one viral genome. The motor cuts the DNA again and dissociates from the full head, allowing head-finishing proteins to assemble on the portal, sealing the portal, and constructing a platform for tail attachment. Only a single gene encoding the putative ATPase subunit of the terminase ("UL15") is conserved among all herpesviruses. To a lesser extent this gene is also found in T4-like bacteriophages suggesting a common ancestor for these two groups of viruses. Another paper has also suggested that herpesviruses originated among the bacteriophages.

A common origin for the herpesviruses and the caudoviruses has been suggested on the basis of parallels in their capsid assembly pathways and similarities between their portal complexes, through which DNA enters the capsid. These two groups of viruses share a distinctive 12-fold arrangement of subunits in the portal complex. A second paper has suggested an evolutionary relationship between these two groups of viruses.

It seems likely that the tailed viruses infecting the archaea are also related to the tailed viruses infecting bacteria.

A study involving 600 herpes genomes and 2000 caudoviral genomes suggested that an evolutionary relationship exists between these order.

The NCLDV group ("Asfarviridae", "Iridoviridae", "Marseilleviridae", "Mimiviridae", "Phycodnaviridae" and "Poxviridae") along with three other families ("Adenoviridae", "Cortiviridae" and "Tectiviridae") and the phage Sulfolobus turreted icosahedral virus and the satellite virus Sputnik all possess double β-barrel major capsid proteins suggesting a common origin.

Several studies have suggested that the family "Ascoviridae" evolved from the "Iridoviridae". A study of the Iridoviruses suggests that the Iridoviridae, Ascoviridae and Marseilleviridae are related with Ascoviruses most closely related to Iridoviruses.

The family "Polydnaviridae" may have evolved from the "Ascoviridae". Molecular evidence suggests that the "Phycodnaviridae" may have evolved from the family "Iridoviridae". These four families ("Ascoviridae", "Iridoviridae", "Phycodnaviridae" and "Polydnaviridae") may form a clade but more work is needed to confirm this.

Some of the relations among the large viruses have been established. Mimiviruses are distantly related to Phycodnaviridae. Pandoraviruses share a common ancestor with Coccolithoviruses within the family Phycodnaviridae. Pithoviruses are related to Iridoviridae and Marseilleviridae.

Based on the genome organisation and DNA replication mechanism it seems that phylogenetic relationships may exist between the rudiviruses ("Rudiviridae") and the large eukaryal DNA viruses: the African swine fever virus ("Asfarviridae"), Chlorella viruses ("Phycodnaviridae") and poxviruses ("Poxviridae").

Based on the analysis of the DNA polymerase the genus "Dinodnavirus" may be a member of the family "Asfarviridae". Further work on this virus will required before a final assignment can be made.

It has been suggested that at least some of the giant viruses may originate from mitochondria.

Based on the analysis of the coat protein, "Sulfolobus turreted icosahedral virus" may share a common ancestry with the "Tectiviridae".

The families "Adenoviridae" and "Tectiviridae" appear to be related structurally.

Baculoviruses evolved from the nudiviruses .

The "Hytrosaviridae" are related to the baculoviruses and to a lesser extent the nudiviruses suggesting they may have evolved from the baculoviruses.

The "Nimaviridae" may be related to nudiviruses and baculoviruses.

The Nudiviruses seem to be related to the polydnaviruses.

A protein common to the families "Bicaudaviridae", "Lipotrixviridae" and "Rudiviridae" and the unclassified virus Sulfolobus turreted icosahedral virus is known suggesting a common origin.

Examination of the "pol" genes that encode the DNA dependent DNA polymerase in various groups of viruses suggests a number of possible evolutionary relationships. All know viral DNA polymerases belong to the DNA "pol" families A and B. All possess a 3'–5'-exonuclease domain with three sequence motifs Exo I, Exo II and Exo III. The families A and B are distinguishable with family A Pol sharing 9 distinct consensus sequences and only two of them are convincingly homologous to sequence motif B of family B. The putative sequence motifs A, B, and C of the polymerase domain are located near the C-terminus in family A Pol and more central in family B Pol.

Phylogenetic analysis of these genes places the adenoviruses ("Adenoviridae"), bacteriophages ("Caudovirales") and the plant and fungal linear plasmids into a single clade. A second clade includes the alpha- and delta-like viral Pol from insect ascovirus ("Ascoviridae"), mammalian herpesviruses ("Herpesviridae"), fish lymphocystis disease virus ("Iridoviridae") and chlorella virus ("Phycoviridae"). The "pol" genes of the African swine fever virus ("Asfarviridae"), baculoviruses ("Baculoviridae"), fish herpesvirus ("Herpesviridae"), T-even bacteriophages ("Myoviridae") and poxviruses ("Poxviridae") were not clearly resolved. A second study showed that poxvirus, baculovirus and the animal herpesviruses form separate and distinct clades. Their relationship to the "Asfarviridae" and the "Myoviridae" was not examined and remains unclear.

The polymerases from the archaea are similar to family B DNA Pols. The T4-like viruses infect both bacteria and archaea and their "pol" gene resembles that of eukaryotes. The DNA polymerase of mitochondria resembles that of the T odd phages ("Myoviridae").

The virophage—Mavirus—may have evolved from a recombination between a transposon of the Polinton (Maverick) family and an unknown virus.

The polyoma and papillomaviruses appear to have evolved from single-stranded DNA viruses and ultimately from plasmids.

The evolutionary history of this group is currently poorly understood. An ancient origin for the single stranded circular DNA viruses has been proposed.

Capsid proteins of most icosahedral ssRNA and ssDNA viruses display the same structural fold, the eight-stranded beta-barrel, also known as the jelly-roll fold. On the other hand, the replication proteins of icosahedral ssDNA viruses belong to the superfamily of rolling-circle replication initiation proteins that are commonly found in prokaryotic plasmids. Based on these observations, it has been proposed that small DNA viruses have originated via recombination between RNA viruses and plasmids.

Circoviruses may have evolved from a nanovirus.

Given the similarities between the "rep" proteins of the alphasatellites and the nanoviruses, it is likely that the alphasatellites evolved from the nanoviruses. Further work in this area is needed to clarify this.

The geminiviruses may have evolved from phytoplasmal plasmids. The Genomoviridae and the Geminividiae appear to be related.

Based on the three-dimensional structure of the Rep proteins the geminiviruses and parvoviruses may be related.

The ancestor of the geminiviruses probably infected dicots.

The parvoviruses have frequently invaded the germ lines of diverse animal species including mammals, fishes, birds, tunicates, arthropods and flatworms. In particular they have been associated with the human genome for ~98 million years.

Members of the family Bidnaviridae have evolved from insect parvoviruses by replacing the typical replication-initiation endonuclease with a protein-primed family B DNA polymerase acquired from large DNA transposons of the Polinton/Maverick family. Some bidnavirus genes were also horizontally acquired from reoviruses (dsRNA genomes) and baculoviruses (dsDNA genomes).

Polyomaviruses, papillomaviruses and parvoviruses may have descended from unrelated circular Rep-encoding single-stranded DNA viral ancestors.

Since 1959 ~6300 prokaryote viruses have been described morphologically, including ~6200 bacterial and ~100 archaeal viruses. Archaeal viruses belong to 15 families and infect members of 16 archaeal genera. These are nearly exclusively hyperthermophiles or extreme halophiles. Tailed archaeal viruses are found only in the Euryarchaeota, whereas most filamentous and pleomorphic archaeal viruses occur in the Crenarchaeota. Bacterial viruses belong to 10 families and infect members of 179 bacterial genera: most of these are members of the Firmicutes and γ-proteobacteria.

The vast majority (96.3%) are tailed with and only 230 (3.7%) are polyhedral, filamentous or pleomorphic. The family Siphoviridae is the largest family (>3600 descriptions: 57.3%). The tailed phages appear to be monophyletic and are the oldest known virus group. They arose repeatedly in different hosts and there are at least 11 separate lines of descent.

All of the known temperate phages employ one of only three different systems for their lysogenic cycle: lambda-like integration/excision, Mu-like transposition or the plasmid-like partitioning of phage N15.

A putative course of evolution of these phages has been proposed by Ackermann.

Tailed phages originated in the early Precambrian, long before eukaryotes and their viruses. The ancestral tailed phage had an icosahedral head of about 60 nanometers in diameter and a long non contractile tail with sixfold symmetry. The capsid contained a single molecule of double stranded DNA of about 50 kilobases. The tail was probably provided with a fixation apparatus. The head and tail were held together by a connector. The viral particle contained no lipids, was heavier than its descendant viruses and had a high DNA content proportional to its capsid size (~50%). Most of the genome coded for structural proteins. Morphopoietic genes clustered at one end of the genome, with head genes preceding tail genes. Lytic enzymes were probably coded for. Part of the phage genome was nonessential and possibly bacterial.

The virus infected its host from the outside and injected its DNA. Replication involved transcription in several waves and formation of DNA concatemers.

New phages were released by burst of the infected cell after lysis of host membranes by a peptidoglycan hydrolase. Capsids were assembled from a starting point, the connector and around a scaffold. They underwent an elaborate maturation process involving protein cleavage and capsid expansion. Heads and tails were assembled separately and joined later. The DNA was cut to size and entered preformed capsids by a headful mechanism.

Subsequently, the phages evolved contractile or short tails and elongated heads. Some viruses become temperate by acquiring an integrase–excisionase complex, plasmid parts or transposons.

A possible evolutionary pathway using vesicles rather than a protein coat has been described in the archaeal plasmid pR1SE.



</doc>
