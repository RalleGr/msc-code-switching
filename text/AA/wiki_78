<doc id="9668" url="https://en.wikipedia.org/wiki?curid=9668" title="Ernst Haeckel">
Ernst Haeckel

Ernst Heinrich Philipp August Haeckel (; 16 February 1834 – 9 August 1919) was a German zoologist, naturalist, eugenicist, philosopher, physician, professor, marine biologist, and artist who discovered, described and named thousands of new species, mapped a genealogical tree relating all life forms, and coined many terms in biology, including "ecology", "phylum", "phylogeny", and "Protista." Haeckel promoted and popularised Charles Darwin's work in Germany and developed the influential but no longer widely held recapitulation theory ("ontogeny recapitulates phylogeny") claiming that an individual organism's biological development, or ontogeny, parallels and summarises its species' evolutionary development, or phylogeny. 

The published artwork of Haeckel includes over 100 detailed, multi-colour illustrations of animals and sea creatures, collected in his "Kunstformen der Natur" ("Art Forms of Nature"). As a philosopher, Ernst Haeckel wrote "Die Welträthsel" (1895–1899; in English: "The Riddle of the Universe", 1901), the genesis for the term "world riddle" ("Welträtsel"); and "Freedom in Science and Teaching" to support teaching evolution.

Haeckel was also a promoter of scientific racism and embraced the idea of Social Darwinism.

Ernst Haeckel was born on 16 February 1834, in Potsdam (then part of the Kingdom of Prussia).
In 1852 Haeckel completed studies at the "Domgymnasium", the cathedral high-school of Merseburg. He then studied medicine in Berlin and Würzburg, particularly with Albert von Kölliker, Franz Leydig, Rudolf Virchow (with whom he later worked briefly as assistant), and with the anatomist-physiologist Johannes Peter Müller (1801–1858). Together with Hermann Steudner he attended botany lectures in Würzburg. In 1857 Haeckel attained a doctorate in medicine, and afterwards he received the license to practice medicine. The occupation of physician appeared less worthwhile to Haeckel after contact with suffering patients.

Ernst Haeckel studied under Karl Gegenbaur at the University of Jena for three years, earning a habilitation in comparative anatomy in 1861, before becoming a professor of zoology at Jena, where he remained for 47 years, from 1862 to 1909. Between 1859 and 1866 Haeckel worked on many phyla, such as radiolarians, poriferans (sponges) and annelids (segmented worms). During a trip to the Mediterranean, Haeckel named nearly 150 new species of radiolarians.

From 1866 to 1867 Haeckel made an extended journey to the Canary Islands with Hermann Fol. On 17 October 1866 he arrived in London. Over the next few days he met Charles Lyell, and visited Thomas Huxley and family at their home. On 21 October he visited Charles Darwin at Down House in Kent. In 1867 he married Agnes Huschke. Their son Walter was born in 1868, their daughters Elizabeth in 1871 and Emma in 1873. In 1869 he traveled as a researcher to Norway, in 1871 to Croatia (where he lived on the island of Hvar in a monastery), and in 1873 to Egypt, Turkey, and Greece. In 1907 he had a museum built in Jena to teach the public about evolution. Haeckel retired from teaching in 1909, and in 1910 he withdrew from the Evangelical Church of Prussia.

On the occasion of his 80th birthday celebration he was presented with a two-volume work entitled "Was wir Ernst Haeckel verdanken (What We Owe to Ernst Haeckel)", edited at the request of the German Monistenbund by Heinrich Schmidt of Jena.

Haeckel's wife, Agnes, died in 1915, and he became substantially frailer, breaking his leg and arm. He sold his "Villa Medusa" in Jena in 1918 to the Carl Zeiss foundation, which preserved his library. Haeckel died on 9 August 1919.

Haeckel became the most famous proponent of Monism in Germany.

Haeckel's affinity for the German Romantic movement, coupled with his acceptance of a form of Lamarckism, influenced his political beliefs. Rather than being a strict Darwinian, Haeckel believed that the characteristics of an organism were acquired through interactions with the environment and that ontogeny reflected phylogeny. He saw the social sciences as instances of "applied biology", and that phrase was picked up and used for Nazi propaganda.
In 1906 Haeckel founded a group called the Monist League () to promote his religious and political beliefs. This group lasted until 1933 and included such notable members as Wilhelm Ostwald, Georg von Arco (1869-1940), Helene Stöcker and Walter Arthur Berendsohn. He was the first person to use the term "first world war".

Haeckel was a zoologist, an accomplished artist and illustrator, and later a professor of comparative anatomy. Although Haeckel's ideas are important to the history of evolutionary theory, and although he was a competent invertebrate anatomist most famous for his work on radiolaria, many speculative concepts that he championed are now considered incorrect. For example, Haeckel described and named hypothetical ancestral microorganisms that have never been found.

He was one of the first to consider psychology as a branch of physiology. He also proposed the kingdom "Protista" in 1866. His chief interests lay in evolution and life development processes in general, including development of nonrandom form, which culminated in the beautifully illustrated "Kunstformen der Natur" ("Art forms of nature"). Haeckel did not support natural selection, rather believing in Lamarckism.

Haeckel advanced a version of the earlier recapitulation theory previously set out by Étienne Serres in the 1820s and supported by followers of Étienne Geoffroy Saint-Hilaire including Robert Edmond Grant. It proposed a link between ontogeny (development of form) and phylogeny (evolutionary descent), summed up by Haeckel in the phrase "ontogeny recapitulates phylogeny". His concept of recapitulation has been refuted in the form he gave it (now called "strong recapitulation"), in favour of the ideas first advanced by Karl Ernst von Baer. The strong recapitulation hypothesis views ontogeny as repeating forms of adult ancestors, while weak recapitulation means that what is repeated (and built upon) is the ancestral embryonic development process. Haeckel supported the theory with embryo drawings that have since been shown to be oversimplified and in part inaccurate, and the theory is now considered an oversimplification of quite complicated relationships, however comparison of embryos remains a powerful way to demonstrate that all animals are related. Haeckel introduced the concept of heterochrony, the change in timing of embryonic development over the course of evolution.

Haeckel was a flamboyant figure, who sometimes took great, non-scientific leaps from available evidence. For example, at the time when Darwin published "On the Origin of Species by Means of Natural Selection" (1859), Haeckel postulated that evidence of human evolution would be found in the Dutch East Indies (now Indonesia). At that time, no remains of human ancestors had yet been identified. He described these theoretical remains in great detail and even named the as-yet unfound species, "Pithecanthropus alalus", and instructed his students such as Richard and Oskar Hertwig to go and find it.

One student did find some remains: a Dutchman named Eugène Dubois searched the East Indies from 1887 to 1895, discovering the remains of Java Man in 1891, consisting of a skullcap, thighbone, and a few teeth. These remains are among the oldest hominid remains ever found. Dubois classified Java Man with Haeckel's "Pithecanthropus" label, though they were later reclassified as "Homo erectus". Some scientists of the day suggested Dubois' Java Man as a potential intermediate form between modern humans and the common ancestor we share with the other great apes. The current consensus of anthropologists is that the direct ancestors of modern humans were African populations of "Homo erectus" (possibly "Homo ergaster"), rather than the Asian populations exemplified by Java Man and Peking Man. (Ironically, a new human species, Homo floresiensis, a dwarf human type, has recently been discovered in the island of Flores).

The creationist polygenism of Samuel George Morton and Louis Agassiz, which presented human races as separately created species, was rejected by Charles Darwin, who argued for the monogenesis of the human species and the African origin of modern humans. In contrast to most of Darwin's supporters, Haeckel put forward a doctrine of evolutionary polygenism based on the ideas of the linguist August Schleicher, in which several different language groups had arisen separately from speechless prehuman "Urmenschen" (), which themselves had evolved from simian ancestors. These separate languages had completed the transition from animals to man, and under the influence of each main branch of languages, humans had evolved – in a kind of Lamarckian use-inheritance – as separate species, which could be subdivided into races. From this, Haeckel drew the implication that languages with the most potential yield the human races with the most potential, led by the Semitic and Indo-Germanic groups, with Berber, Jewish, Greco-Roman and Germanic varieties to the fore. As Haeckel stated:

Haeckel's view can be seen as a forerunner of the views of Carleton Coon, who also believed that human races evolved independently and in parallel with each other. These ideas eventually fell from favour.

Haeckel also applied the hypothesis of polygenism to the modern diversity of human groups. He became a key figure in social darwinism and leading proponent of scientific racism, stating for instance:

Haeckel divided human beings into ten races, of which the Caucasian was the highest and the primitives were doomed to extinction. Haeckel claimed that '[t]he Negro' had stronger and more freely movable toes than any other race, which, he argued, was evidence of their being less evolved, and which led him to compare them to four-handed" Apes'. Haeckel also believed 'Negroes' were savages and that Whites were the most civilised.

In his "Ontogeny and Phylogeny" Harvard paleontologist Stephen Jay Gould wrote: "[Haeckel's] evolutionary racism; his call to the German people for racial purity and unflinching devotion to a 'just' state; his belief that harsh, inexorable laws of evolution ruled human civilization and nature alike, conferring upon favored races the right to dominate others ... all contributed to the rise of Nazism."

In his introduction to the Nazi party ideologue Alfred Rosenberg's 1930 book, "The Myth of the Twentieth Century", Peter Peel affirms that Rosenberg had indeed read Haeckel.

In the same line of thought, historian Daniel Gasman states that Haeckel's ideology stimulated the birth of Fascist ideology in Italy and France.

However, Robert J. Richards notes: "Haeckel, on his travels to Ceylon and Indonesia, often formed closer and more intimate relations with natives, even members of the untouchable classes, than with the European colonials." and says the Nazis rejected Haeckel, since he opposed antisemitism, while supporting ideas they disliked (for instance atheism, feminism, internationalism, pacifism etc.).

Haeckel claimed the origin of humanity was to be found in Asia: he believed that Hindustan (Indian subcontinent) was the actual location where the first humans had evolved. Haeckel argued that humans were closely related to the primates of Southeast Asia and rejected Darwin's hypothesis of Africa.

Haeckel later claimed that the missing link was to be found on the lost continent of Lemuria located in the Indian Ocean. He believed that Lemuria was the home of the first humans and that Asia was the home of many of the earliest primates; he thus supported that Asia was the cradle of hominid evolution. Haeckel also claimed that Lemuria connected Asia and Africa, which allowed the migration of humans to the rest of the world.

In Haeckel’s book "The History of Creation" (1884) he included migration routes which he thought the first humans had used outside of Lemuria.

When Haeckel was a student in the 1850s he showed great interest in embryology, attending the rather unpopular lectures twice and in his notes sketched the visual aids: textbooks had few illustrations, and large format plates were used to show students how to see the tiny forms under a reflecting microscope, with the translucent tissues seen against a black background. Developmental series were used to show stages within a species, but inconsistent views and stages made it even more difficult to compare different species. It was agreed by all European evolutionists that all vertebrates looked very similar at an early stage, in what was thought of as a common ideal type, but there was a continuing debate from the 1820s between the Romantic recapitulation theory that human embryos developed through stages of the forms of all the major groups of adult animals, literally manifesting a sequence of organisms on a linear chain of being, and Karl Ernst von Baer's opposing view, stated in von Baer's laws of embryology, that the early general forms diverged into four major groups of specialised forms without ever resembling the adult of another species, showing affinity to an archetype but no relation to other types or any transmutation of species. By the time Haeckel was teaching he was able to use a textbook with woodcut illustrations written by his own teacher Albert von Kölliker, which purported to explain human development while also using other mammalian embryos to claim a coherent sequence. Despite the significance to ideas of transformism, this was not really polite enough for the new popular science writing, and was a matter for medical institutions and for experts who could make their own comparisons.

Darwin's "On the Origin of Species", which made a powerful impression on Haeckel when he read it in 1864, was very cautious about the possibility of ever reconstructing the history of life, but did include a section reinterpreting von Baer's embryology and revolutionising the field of study, concluding that "Embryology rises greatly in interest, when we thus look at the embryo as a picture, more or less obscured, of the common parent-form of each great class of animals." It mentioned von Baer's 1828 anecdote (misattributing it to Louis Agassiz) that at an early stage embryos were so similar that it could be impossible to tell whether an unlabelled specimen was of a mammal, a bird, or of a reptile, and Darwin's own research using embryonic stages of barnacles to show that they are crustaceans, while cautioning against the idea that one organism or embryonic stage is "higher" or "lower", or more or less evolved. Haeckel disregarded such caution, and in a year wrote his massive and ambitious "Generelle Morphologie", published in 1866, presenting a revolutionary new synthesis of Darwin's ideas with the German tradition of "Naturphilosophie" going back to Goethe and with the progressive evolutionism of Lamarck in what he called "Darwinismus". He used morphology to reconstruct the evolutionary history of life, in the absence of fossil evidence using embryology as evidence of ancestral relationships. He invented new terms, including ontogeny and phylogeny, to present his evolutionised recapitulation theory that "ontogeny recapitulated phylogeny". The two massive volumes sold poorly, and were heavy going: with his limited understanding of German, Darwin found them impossible to read. Haeckel's publisher turned down a proposal for a "strictly scholarly and objective" second edition.

Haeckel's aim was a reformed morphology with evolution as the organising principle of a cosmic synthesis unifying science, religion, and art. He was giving successful "popular lectures" on his ideas to students and townspeople in Jena, in an approach pioneered by his teacher Rudolf Virchow. To meet his publisher's need for a popular work he used a student's transcript of his lectures as the basis of his "Natürliche Schöpfungsgeschichte" of 1868, presenting a comprehensive presentation of evolution. In the Spring of that year he drew figures for the book, synthesising his views of specimens in Jena and published pictures to represent types. After publication he told a colleague that the images "are completely exact, partly copied from nature, partly assembled from all illustrations of these early stages that have hitherto become known". There were various styles of embryological drawings at that time, ranging from more schematic representations to "naturalistic" illustrations of specific specimens. Haeckel believed privately that his figures were both exact and synthetic, and in public asserted that they were schematic like most figures used in teaching. The images were reworked to match in size and orientation, and though displaying Haeckel's own views of essential features, they support von Baer's concept that vertebrate embryos begin similarly and then diverge. Relating different images on a grid conveyed a powerful evolutionary message. As a book for the general public, it followed the common practice of not citing sources.
The book sold very well, and while some anatomical experts hostile to Haeckel's evolutionary views expressed some private concerns that certain figures had been drawn rather freely, the figures showed what they already knew about similarities in embryos. The first published concerns came from Ludwig Rütimeyer, a professor of zoology and comparative anatomy at the University of Basel who had placed fossil mammals in an evolutionary lineage early in the 1860s and had been sent a complimentary copy. At the end of 1868 his review in the "Archiv für Anthropologie" wondered about the claim that the work was "popular and scholarly", doubting whether the second was true, and expressed horror about such public discussion of man's place in nature with illustrations such as the evolutionary trees being shown to non-experts. Though he made no suggestion that embryo illustrations should be directly based on specimens, to him the subject demanded the utmost "scrupulosity and conscientiousness" and an artist must "not arbitrarily model or generalise his originals for speculative purposes" which he considered proved by comparison with works by other authors. In particular, "one and the same, moreover incorrectly interpreted woodcut, is presented to the reader three times in a row and with three different captions as [the] embryo of the dog, the chick, [and] the turtle". He accused Haeckel of "playing fast and loose with the public and with science", and failing to live up to the obligation to the truth of every serious researcher. Haeckel responded with angry accusations of bowing to religious prejudice, but in the second (1870) edition changed the duplicated embryo images to a single image captioned "embryo of a mammal or bird". Duplication using galvanoplastic stereotypes (clichés) was a common technique in textbooks, but not on the same page to represent different eggs or embryos. In 1891 Haeckel made the excuse that this "extremely rash foolishness" had occurred in undue haste but was "bona fide", and since repetition of incidental details was obvious on close inspection, it is unlikely to have been intentional deception.

The revised 1870 second edition of 1,500 copies attracted more attention, being quickly followed by further revised editions with larger print runs as the book became a prominent part of the optimistic, nationalist, anticlerical "culture of progress" in Otto von Bismarck's new German Empire. The similarity of early vertebrate embryos became common knowledge, and the illustrations were praised by experts such as Michael Foster of the University of Cambridge. In the introduction to his 1871 "The Descent of Man, and Selection in Relation to Sex", Darwin gave particular praise to Haeckel, writing that if "Natürliche Schöpfungsgeschichte" "had appeared before my essay had been written, I should probably never have completed it". The first chapter included an illustration: "As some of my readers may never have seen a drawing of an embryo, I have given one of man and another of a dog, at about the same early stage of development, carefully copied from two works of undoubted accuracy" with a footnote citing the sources and noting that "Häckel has also given analogous drawings in his "Schöpfungsgeschichte."" The fifth edition of Haeckel's book appeared in 1874, with its frontispiece a heroic portrait of Haeckel himself, replacing the previous controversial image of the heads of apes and humans.

Later in 1874, Haeckel's simplified embryology textbook "Anthropogenie" made the subject into a battleground over Darwinism aligned with Bismarck's "Kulturkampf" ("culture struggle") against the Catholic Church. Haeckel took particular care over the illustrations, changing to the leading zoological publisher Wilhelm Engelmann of Leipzig and obtaining from them use of illustrations from their other textbooks as well as preparing his own drawings including a dramatic double page illustration showing "early", "somewhat later" and "still later" stages of 8 different vertebrates. Though Haeckel's views had attracted continuing controversy, there had been little dispute about the embryos and he had many expert supporters, but Wilhelm His revived the earlier criticisms and introduced new attacks on the 1874 illustrations. Others joined in: both expert anatomists and Catholic priests and supporters were politically opposed to Haeckel's views.

While it has been widely claimed that Haeckel was charged with fraud by five professors and convicted by a university court at Jena, there does not appear to be an independently verifiable source for this claim. Recent analyses (Richardson 1998, Richardson and Keuck 2002) have found that some of the criticisms of Haeckel's embryo drawings were legitimate, but others were unfounded. There were multiple versions of the embryo drawings, and Haeckel rejected the claims of fraud. It was later said that "there is evidence of sleight of hand" on both sides of the feud between Haeckel and Wilhelm His. Robert J. Richards, in a paper published in 2008, defends the case for Haeckel, shedding doubt against the fraud accusations based on the material used for comparison with what Haeckel could access at the time.

Haeckel was awarded the title of Excellency by Kaiser Wilhelm II in 1907 and the Linnean Society of London's prestigious Darwin-Wallace Medal in 1908. In the United States, "Mount Haeckel", a summit in the Eastern Sierra Nevada, overlooking the Evolution Basin, is named in his honour, as is another "Mount Haeckel", a summit in New Zealand; and the asteroid 12323 Haeckel. 

In Jena he is remembered with a monument at Herrenberg (erected in 1969), an exhibition at Ernst-Haeckel-Haus, and at the Jena Phyletic Museum, which continues to teach about evolution and share his work to this day.

Darwin's 1859 book "On the Origin of Species" had immense popular influence, but although its sales exceeded its publisher's hopes it was a technical book rather than a work of popular science: long, difficult and with few illustrations. One of Haeckel's books did a great deal to explain his version of "Darwinism" to the world. It was a bestselling, provocatively illustrated book in German, titled "Natürliche Schöpfungsgeschichte", published in Berlin in 1868, and translated into English as "The History of Creation" in 1876. It was frequently reprinted until 1926.

Haeckel argued that human evolution consisted of precisely 22 phases, the 21st – the "missing link" – being a halfway step between apes and humans. He even formally named this missing link "Pithecanthropus alalus", translated as "ape man without speech".

Haeckel's literary output was extensive, including many books, scientific papers, and illustrations.





For a fuller list of works of and about Haeckel, see his entry in the .

Some historians have seen Haeckel's social Darwinism as a forerunner to National Socialist ideology. Others have denied the relationship all together.

The evidence is in some respects ambiguous. On one hand, Haeckel was an advocate of scientific racism. He held that evolutionary biology had definitively proven that races were unequal in intelligence and ability, and that their lives were also of unequal value. As a result of the "struggle for existence", it followed that the "lower" races would eventually be exterminated. He was also a social Darwinist who believed that "survival of the fittest" was a natural law, and that struggle led to improvement of the race. As an advocate of eugenics, he also believed that about 200,000 mentally and congenitally ill should be killed by a medical control board. This idea was later put into practice by the Third Reich, as part of the Aktion T4 program. Alfred Ploetz, founder of the German Society for Racial Hygiene, praised Haeckel repeatedly, and invited him to become an honorary member. Haeckel accepted the invitation. Haeckel also believed that Germany should be governed by an authoritarian political system, and that inequalities both within and between societies were an inevitable product of evolutionary law. Haeckel was also an extreme German nationalist who believed strongly in the superiority of German culture.

On the other hand, Haeckel was not an anti-Semite. In the racial hierarchies he constructed Jews tended to appear closer to the top, rather than closer to the bottom as in National Socialist racial thought. He was also a pacifist until the First World War, when he wrote propaganda in favor of the war. The principal arguments of historians who deny a meaningful connection between Haeckel and National Socialism are that Haeckel's ideas were very common at the time, that National Socialists were much more strongly influenced by other thinkers, and that Haeckel is properly classified as a 19th century German liberal, rather than a forerunner to 20th century National Socialism. They also point to incompatibilities between evolutionary biology and National Socialist ideology.

Nazis themselves divided on the question of whether Haeckel should be counted as a pioneer of their ideology. SS captain and biologist Heinz Brücher wrote a biography of Haeckel in 1936, in which he praised Haeckel as a "pioneer in biological state thinking". This opinion was also shared by the scholarly journal, "Der Biologie", which celebrated Haeckel's 100th birthday, in 1934, with several essays acclaiming him as a pioneering thinker of National Socialism. Other Nazis kept their distance from Haeckel. Nazi propaganda guidelines issued in 1935 listed books which popularized Darwin and evolution on an "expunged list". Haeckel was included by name as a forbidden author. Gunther Hecht, a member of the Nazi Department of Race Politics, also issued a memorandum rejecting Haeckel as a forerunner of National Socialism. Kurt Hildebrandt, a Nazi political philosopher, also rejected Haeckel. Eventually Nazis rejected Haeckel because his evolutionary ideas could not be reconciled with Nazi ideology.





</doc>
<doc id="9670" url="https://en.wikipedia.org/wiki?curid=9670" title="Evolutionism">
Evolutionism

Evolutionism is a term used (often derogatorily) to denote the theory of evolution. Its exact meaning has changed over time as the study of evolution has progressed. In the 19th century, it was used to describe the belief that organisms deliberately improved themselves through progressive inherited change (orthogenesis). The teleological belief went on to include cultural evolution and social evolution. In the 1970s the term Neo-Evolutionism was used to describe the idea "that human beings sought to preserve a familiar style of life unless change was forced on them by factors that were beyond their control".

The term is most often used by creationists to describe adherence to the scientific consensus on evolution as equivalent to a secular religion. The term is very seldom used within the scientific community, since the scientific position on evolution is accepted by the overwhelming majority of scientists. Because evolutionary biology is the default scientific position, it is assumed that "scientists" or "biologists" are "evolutionists" unless specifically noted otherwise. In the creation–evolution controversy, creationists often call those who accept the validity of the modern evolutionary synthesis "evolutionists" and the theory itself "evolutionism".

Before its use to describe biological evolution, the term "evolution" was originally used to refer to any orderly sequence of events with the outcome somehow contained at the start. The first five editions of Darwin's in "Origin of Species" used the word "evolved", but the word "evolution" was only used in its sixth edition in 1872. By then, Herbert Spencer had developed the concept theory that organisms strive to evolve due to an internal "driving force" (orthogenesis) in 1862. Edward B. Tylor and Lewis H Morgan brought the term "evolution" to anthropology though they tended toward the older pre-Spencerian definition helping to form the concept of unilineal (social) evolution used during the later part of what Trigger calls the Antiquarianism-Imperial Synthesis period (c1770-c1900). The term evolutionism subsequently came to be used for the now discredited theory that evolution contained a deliberate component, rather than the selection of beneficial traits from random variation by differential survival.

The term "evolution" is widely used, but the term "evolutionism" is not used in the scientific community to refer to evolutionary biology as it is redundant and anachronistic.

However, the term has been used by creationists in discussing the creation–evolution controversy. For example, the Institute for Creation Research, in order to imply placement of evolution in the category of 'religions', including atheism, fascism, humanism and occultism, commonly uses the words "evolutionism" and "evolutionist" to describe the consensus of mainstream science and the scientists subscribing to it, thus implying through language that the issue is a matter of religious belief. The BioLogos Foundation, an organization that promotes the idea of theistic evolution, uses the term "evolutionism" to describe "the atheistic worldview that so often accompanies the acceptance of biological evolution in public discourse." It views this as a subset of scientism.




</doc>
<doc id="9672" url="https://en.wikipedia.org/wiki?curid=9672" title="Entscheidungsproblem">
Entscheidungsproblem

In mathematics and computer science, the (, German for "decision problem") is a challenge posed by David Hilbert and Wilhelm Ackermann in 1928. The problem asks for an algorithm that considers, as input, a statement and answers "Yes" or "No" according to whether the statement is "universally valid", i.e., valid in every structure satisfying the axioms.

By the completeness theorem of first-order logic, a statement is universally valid if and only if it can be deduced from the axioms, so the "" can also be viewed as asking for an algorithm to decide whether a given statement is provable from the axioms using the rules of logic.

In 1936, Alonzo Church and Alan Turing published independent papers showing that a general solution to the " is impossible, assuming that the intuitive notion of "effectively calculable" is captured by the functions computable by a Turing machine (or equivalently, by those expressible in the lambda calculus). This assumption is now known as the Church–Turing thesis.

The origin of the " goes back to Gottfried Leibniz, who in the seventeenth century, after having constructed a successful mechanical calculating machine, dreamt of building a machine that could manipulate symbols in order to determine the truth values of mathematical statements. He realized that the first step would have to be a clean formal language, and much of his subsequent work was directed toward that goal. In 1928, David Hilbert and Wilhelm Ackermann posed the question in the form outlined above.

In continuation of his "program", Hilbert posed three questions at an international conference in 1928, the third of which became known as "Hilbert's ""." In 1929, Moses Schönfinkel published one paper on special cases of the decision problem, that was prepared by Paul Bernays.

As late as 1930, Hilbert believed that there would be no such thing as an unsolvable problem.

Before the question could be answered, the notion of "algorithm" had to be formally defined. This was done by Alonzo Church in 1935 with the concept of "effective calculability" based on his λ-calculus and by Alan Turing the next year with his concept of Turing machines. Turing immediately recognized that these are equivalent models of computation.

The negative answer to the ' was then given by Alonzo Church in 1935–36 (Church's theorem) and independently shortly thereafter by Alan Turing in 1936 ("Turing's proof"). Church proved that there is no computable function which decides for two given λ-calculus expressions whether they are equivalent or not. He relied heavily on earlier work by Stephen Kleene. Turing reduced the question of the existence of a 'general method' which decides whether any given Turing Machine halts or not (the halting problem) to the question of the existence of an 'algorithm' or 'general method' able to solve the '. If 'Algorithm' is understood as being equivalent to a Turing Machine, and with the answer to the latter question negative (in general), the question about the existence of an Algorithm for the "" also must be negative (in general). In his 1936 paper, Turing says: ""Corresponding to each computing machine 'it' we construct a formula 'Un(it)' and we show that, if there is a general method for determining whether 'Un(it)' is provable, then there is a general method for determining whether 'it' ever prints 0"".

The work of both Church and Turing was heavily influenced by Kurt Gödel's earlier work on his incompleteness theorem, especially by the method of assigning numbers (a Gödel numbering) to logical formulas in order to reduce logic to arithmetic.

The "" is related to Hilbert's tenth problem, which asks for an algorithm to decide whether Diophantine equations have a solution. The non-existence of such an algorithm, established by Yuri Matiyasevich in 1970, also implies a negative answer to the Entscheidungsproblem.

Some first-order theories are algorithmically decidable; examples of this include Presburger arithmetic, real closed fields and static type systems of many programming languages. The general first-order theory of the natural numbers expressed in Peano's axioms cannot be decided with an algorithm, however.

Having practical decision procedures for classes of logical formulas is of considerable interest for program verification and circuit verification. Pure Boolean logical formulas are usually decided using SAT-solving techniques based on the DPLL algorithm. Conjunctive formulas over linear real or rational arithmetic can be decided using the simplex algorithm, formulas in linear integer arithmetic (Presburger arithmetic) can be decided using Cooper's algorithm or William Pugh's Omega test. Formulas with negations, conjunctions and disjunctions combine the difficulties of satisfiability testing with that of decision of conjunctions; they are generally decided nowadays using SMT-solving techniques, which combine SAT-solving with decision procedures for conjunctions and propagation techniques. Real polynomial arithmetic, also known as the theory of real closed fields, is decidable; this is the Tarski–Seidenberg theorem, which has been implemented in computers by using the cylindrical algebraic decomposition.




</doc>
<doc id="9674" url="https://en.wikipedia.org/wiki?curid=9674" title="Einhard">
Einhard

Einhard (also Eginhard or Einhart; ; 775 – March 14, 840) was a Frankish scholar and courtier. Einhard was a dedicated servant of Charlemagne and his son Louis the Pious; his main work is a biography of Charlemagne, the "Vita Karoli Magni", "one of the most precious literary bequests of the early Middle Ages."

Einhard was from the eastern German-speaking part of the Frankish Kingdom. Born into a family of landowners of some importance, his parents sent him to be educated by the monks of Fulda, one of the most impressive centers of learning in the Frank lands. Perhaps due to his small stature, which restricted his riding and sword-fighting ability, Einhard concentrated his energies on scholarship, especially the mastering of Latin. He was accepted into the hugely wealthy court of Charlemagne around 791 or 792. Charlemagne actively sought to amass scholarly men around him and established a royal school led by the Northumbrian scholar Alcuin. Einhard evidently was a talented builder and construction manager, because Charlemagne put him in charge of the completion of several palace complexes including Aachen and Ingelheim. Despite the fact that Einhard was on intimate terms with Charlemagne, he never achieved office in his reign. In 814, on Charlemagne's death, his son Louis the Pious made Einhard his private secretary. Einhard retired from court during the time of the disputes between Louis and his sons in the spring of 830.

He died at Seligenstadt in 840.

Einhard was married to Emma, of whom little is known. There is a possibility that their marriage bore a son, Vussin. Their marriage also appears to have been exceptionally liberal for the period, with Emma being as active as Einhard, if not more so, in the handling of their property. It is said that in the later years of their marriage Emma and Einhard abstained from sexual relations, choosing instead to focus their attentions on their many religious commitments. Though he was undoubtedly devoted to her, Einhard wrote nothing of his wife until after her death on 13 December 835, when he wrote to a friend that he was reminded of her loss in ‘every day, in every action, in every undertaking, in all the administration of the house and household, in everything needing to be decided upon and sorted out in my religious and earthly responsibilities’.
Einhard made numerous references to himself as a "sinner" according to his strong Christian faith. He erected churches at both of his estates in Michelstadt and Mulinheim. In Michelstadt, he also saw fit to build a basilica completed in 827 and then sent a servant, Ratleic, to Rome with an end to find relics for the new building. Once in Rome, Ratleic robbed a catacomb of the bones of the Martyrs Marcellinus and Peter and had them translated to Michelstadt. Once there, the relics made it known they were unhappy with their new tomb and thus had to be moved again to Mulinheim. Once established there, they proved to be miracle workers. Although unsure as to why these saints should choose such a "sinner" as their patron, Einhard nonetheless set about ensuring they continued to receive a resting place fitting of their honour. Between 831 and 834 he founded a Benedictine Monastery and, after the death of his wife, served as its Abbot until his own death in 840.

Local lore from Seligenstadt portrays Einhard as the lover of Emma, one of Charlemagne's daughters, and has the couple elope from court. Charlemagne found them at Seligenstadt (then called Obermühlheim) and forgave them. This account is used to explain the name "Seligenstadt" by folk etymology. Einhard and his wife were originally buried in one sarcophagus in the choir of the church in Seligenstadt, but in 1810 the sarcophagus was presented by the Grand Duke of Hesse to the count of Erbach, who claims descent from Einhard as the husband of Imma, the reputed daughter of Charlemagne. The count put it in the famous chapel of his castle at Erbach in the Odenwald.

The most famous of Einhard's works is his biography of Charlemagne, the "Vita Karoli Magni", "The Life of Charlemagne" (c. 817–836), which provides much direct information about Charlemagne's life and character, written sometime between 817 and 830. In composing this he relied heavily upon the Royal Frankish Annals. Einhard's literary model was the classical work of the Roman historian Suetonius, the "Lives of the Caesars", though it is important to stress that the work is very much Einhard's own, that is to say he adapts the models and sources for his own purposes. His work was written as a praise of Charlemagne, whom he regarded as a foster-father ("nutritor") and to whom he was a debtor "in life and death". The work thus contains an understandable degree of bias, Einhard taking care to exculpate Charlemagne in some matters, not mention others, and to gloss over certain issues which would be of embarrassment to Charlemagne, such as the morality of his daughters; by contrast, other issues are curiously not glossed over, like his concubines.

Einhard is also responsible for three other extant works: a collection of letters, "On the Translations and the Miracles of SS. Marcellinus and Petrus", and "On the Adoration of the Cross". The latter dates from ca. 830 and was not rediscovered until 1885, when Ernst Dümmler identified a text in a manuscript in Vienna as the missing "Libellus de adoranda cruce", which Einhard had dedicated to his pupil Lupus Servatus.





</doc>
<doc id="9675" url="https://en.wikipedia.org/wiki?curid=9675" title="Ester">
Ester

In chemistry, an ester is a chemical compound derived from an acid (organic or inorganic) in which at least one –OH (hydroxyl) group is replaced by an –O–alkyl (alkoxy) group. Usually, esters are derived from substitution reaction of a carboxylic acid and an alcohol. Glycerides, which are fatty acid esters of glycerol, are important esters in biology, being one of the main classes of lipids, and making up the bulk of animal fats and vegetable oils. Esters with low molecular weight are commonly used as fragrances and found in essential oils and pheromones. Phosphoesters form the backbone of DNA molecules. Nitrate esters, such as nitroglycerin, are known for their explosive properties, while polyesters are important plastics, with monomers linked by ester moieties. Esters usually have a sweet smell and are considered high-quality solvents for a broad array of plastics, plasticizers, resins, and lacquers. They are also one of the largest classes of synthetic lubricants on the commercial market.

The word "ester" was coined in 1848 by a German chemist Leopold Gmelin, probably as a contraction of the German "Essigäther", "acetic ether".

Ester names are derived from the parent alcohol and the parent acid, where the latter may be organic or inorganic. Esters derived from the simplest carboxylic acids are commonly named according to the more traditional, so-called "trivial names" e.g. as formate, acetate, propionate, and butyrate, as opposed to the IUPAC nomenclature methanoate, ethanoate, propanoate and butanoate. Esters derived from more complex carboxylic acids are, on the other hand, more frequently named using the systematic IUPAC name, based on the name for the acid followed by the suffix "-oate". For example, the ester hexyl octanoate, also known under the trivial name hexyl caprylate, has the formula CH(CH)CO(CH)CH.

The chemical formulas of organic esters usually take the form RCOR′, where R and R′ are the hydrocarbon parts of the carboxylic acid and the alcohol, respectively. For example, butyl acetate (systematically butyl ethanoate), derived from butanol and acetic acid (systematically ethanoic acid) would be written CHCOCH. Alternative presentations are common including BuOAc and CHCOOCH.

Cyclic esters are called lactones, regardless of whether they are derived from an organic or an inorganic acid. One example of an organic lactone is γ-valerolactone.

An uncommon class of organic esters are the orthoesters, which have the formula RC(OR′). Triethylorthoformate (HC(OCH)) is derived, in terms of its name (but not its synthesis) from orthoformic acid (HC(OH)) and ethanol.

Esters can also be derived from inorganic acids. 
Inorganic acids that exist as tautomers form diverse esters

Inorganic acids that are unstable or elusive form stable esters.

In principle, all metal and metalloid alkoxides, of which many hundreds are known, could be classified as esters of the hypothetical acids.

Esters contain a carbonyl center, which gives rise to 120° C–C–O and O–C–O angles. Unlike amides, esters are structurally flexible functional groups because rotation about the C–O–C bonds has a low barrier. Their flexibility and low polarity is manifested in their physical properties; they tend to be less rigid (lower melting point) and more volatile (lower boiling point) than the corresponding amides. The p"K" of the alpha-hydrogens on esters is around 25.

Many esters have the potential for conformational isomerism, but they tend to adopt an "s"-cis (or Z) conformation rather than the "s"-trans (or E) alternative, due to a combination of hyperconjugation and dipole minimization effects. The preference for the Z conformation is influenced by the nature of the substituents and solvent, if present. Lactones with small rings are restricted to the "s"-trans (i.e. E) conformation due to their cyclic structure.

Esters are more polar than ethers but less polar than alcohols. They participate in hydrogen bonds as hydrogen-bond acceptors, but cannot act as hydrogen-bond donors, unlike their parent alcohols. This ability to participate in hydrogen bonding confers some water-solubility. Because of their lack of hydrogen-bond-donating ability, esters do not self-associate. Consequently, esters are more volatile than carboxylic acids of similar molecular weight.

Esters are generally identified by gas chromatography, taking advantage of their volatility. IR spectra for esters feature an intense sharp band in the range 1730–1750 cm assigned to "ν". This peak changes depending on the functional groups attached to the carbonyl. For example, a benzene ring or double bond in conjugation with the carbonyl will bring the wavenumber down about 30 cm.

Esters are widespread in nature and are widely used in industry. In nature, fats are in general triesters derived from glycerol and fatty acids. Esters are responsible for the aroma of many fruits, including apples, durians, pears, bananas, pineapples, and strawberries. Several billion kilograms of polyesters are produced industrially annually, important products being polyethylene terephthalate, acrylate esters, and cellulose acetate.

Esterification is the general name for a chemical reaction in which two reactants (typically an alcohol and an acid) form an ester as the reaction product. Esters are common in organic chemistry and biological materials, and often have a pleasant characteristic, fruity odor. This leads to their extensive use in the fragrance and flavor industry. Ester bonds are also found in many polymers.

The classic synthesis is the Fischer esterification, which involves treating a carboxylic acid with an alcohol in the presence of a dehydrating agent:
The equilibrium constant for such reactions is about 5 for typical esters, e.g., ethyl acetate. The reaction is slow in the absence of a catalyst. Sulfuric acid is a typical catalyst for this reaction. Many other acids are also used such as polymeric sulfonic acids. Since esterification is highly reversible, the yield of the ester can be improved using Le Chatelier's principle:

Reagents are known that drive the dehydration of mixtures of alcohols and carboxylic acids. One example is the Steglich esterification, which is a method of forming esters under mild conditions. The method is popular in peptide synthesis, where the substrates are sensitive to harsh conditions like high heat. DCC (dicyclohexylcarbodiimide) is used to activate the carboxylic acid to further reaction. 4-Dimethylaminopyridine (DMAP) is used as an acyl-transfer catalyst.

Another method for the dehydration of mixtures of alcohols and carboxylic acids is the Mitsunobu reaction:

Carboxylic acids can be esterified using diazomethane:
Using this diazomethane, mixtures of carboxylic acids can be converted to their methyl esters in near quantitative yields, e.g., for analysis by gas chromatography. The method is useful in specialized organic synthetic operations but is considered too hazardous and expensive for large-scale applications.

Carboxylic acids are esterified by treatment with epoxides, giving β-hydroxyesters:
This reaction is employed in the production of vinyl ester resin resins from acrylic acid.

Alcohols react with acyl chlorides and acid anhydrides to give esters:

The reactions are irreversible simplifying work-up. Since acyl chlorides and acid anhydrides also react with water, anhydrous conditions are preferred. The analogous acylations of amines to give amides are less sensitive because amines are stronger nucleophiles and react more rapidly than does water. This method is employed only for laboratory-scale procedures, as it is expensive.

Although not widely employed for esterifications, salts of carboxylate anions can be alkylating agent with alkyl halides to give esters. In the case that an alkyl chloride is used, an iodide salt can catalyze the reaction (Finkelstein reaction). The carboxylate salt is often generated "in situ". In difficult cases, the silver carboxylate may be used, since the silver ion coordinates to the halide aiding its departure and improving the reaction rate. This reaction can suffer from anion availability problems and, therefore, can benefit from the addition of phase transfer catalysts or highly polar aprotic solvents such as DMF.

Transesterification, which involves changing one ester into another one, is widely practiced:
Like the hydrolysation, transesterification is catalysed by acids and bases. The reaction is widely used for degrading triglycerides, e.g. in the production of fatty acid esters and alcohols. Poly(ethylene terephthalate) is produced by the transesterification of dimethyl terephthalate and ethylene glycol: 

A subset of transesterification is the alcoholysis of diketene. This reaction affords 2-ketoesters.
Alkenes undergo "hydroesterification" in the presence of metal carbonyl catalysts. Esters of propionic acid are produced commercially by this method:
A preparaton of methyl propionate is one illustrative example. 

The carbonylation of methanol yields methyl formate, which is the main commercial source of formic acid. The reaction is catalyzed by sodium methoxide:

In the presence of palladium-based catalysts, ethylene, acetic acid, and oxygen react to give vinyl acetate:
Direct routes to this same ester are not possible because vinyl alcohol is unstable.

Carboxylic acids also add across alkynes to give the same products.

Silicotungstic acid is used to manufacture ethyl acetate by the alkylation of acetic acid by ethylene:

The Tishchenko reaction involve disproportionation of an aldehyde in the presence of an anhydrous base to give an ester. Catalysts are aluminium alkoxides or sodium alkoxides. Benzaldehyde reacts with sodium benzyloxide (generated from sodium and benzyl alcohol) to generate benzyl benzoate. The method is used in the production of ethyl acetate from acetaldehyde.


Esters react with nucleophiles at the carbonyl carbon. The carbonyl is weakly electrophilic but is attacked by strong nucleophiles (amines, alkoxides, hydride sources, organolithium compounds, etc.). The C–H bonds adjacent to the carbonyl are weakly acidic but undergo deprotonation with strong bases. This process is the one that usually initiates condensation reactions. The carbonyl oxygen in esters is weakly basic, less so than the carbonyl oxygen in amides due to resonance donation of an electron pair from nitrogen in amides, but forms adducts.

Esterification is a reversible reaction. Esters undergo hydrolysis under acid and basic conditions. Under acidic conditions, the reaction is the reverse reaction of the Fischer esterification. Under basic conditions, hydroxide acts as a nucleophile, while an alkoxide is the leaving group. This reaction, saponification, is the basis of soap making.

The alkoxide group may also be displaced by stronger nucleophiles such as ammonia or primary or secondary amines to give amides: (ammonolysis reaction)
This reaction is not usually reversible. Hydrazines and hydroxylamine can be used in place of amines. Esters can be converted to isocyanates through intermediate hydroxamic acids in the Lossen rearrangement.

Sources of carbon nucleophiles, e.g., Grignard reagents and organolithium compounds, add readily to the carbonyl.

Compared to ketones and aldehydes, esters are relatively resistant to reduction. The introduction of catalytic hydrogenation in the early part of the 20th century was a breakthrough; esters of fatty acids are hydrogenated to fatty alcohols.
A typical catalyst is copper chromite. Prior to the development of catalytic hydrogenation, esters were reduced on a large scale using the Bouveault–Blanc reduction. This method, which is largely obsolete, uses sodium in the presence of proton sources.

Especially for fine chemical syntheses, lithium aluminium hydride is used to reduce esters to two primary alcohols. The related reagent sodium borohydride is slow in this reaction. DIBAH reduces esters to aldehydes.

Direct reduction to give the corresponding ether is difficult as the intermediate hemiacetal tends to decompose to give an alcohol and an aldehyde (which is rapidly reduced to give a second alcohol). The reaction can be achieved using triethylsilane with a variety of Lewis acids.

As for aldehydes, the hydrogen atoms on the carbon adjacent ("α to") the carboxyl group in esters are sufficiently acidic to undergo deprotonation, which in turn leads to a variety of useful reactions. Deprotonation requires relatively strong bases, such as alkoxides. Deprotonation gives a nucleophilic enolate, which can further react, e.g., the Claisen condensation and its intramolecular equivalent, the Dieckmann condensation. This conversion is exploited in the malonic ester synthesis, wherein the diester of malonic acid reacts with an electrophile (e.g., alkyl halide), and is subsequently decarboxylated. Another variation is the Fráter–Seebach alkylation.


As a class, esters serve as protecting groups for carboxylic acids. Protecting a carboxylic acid is useful in peptide synthesis, to prevent self-reactions of the bifunctional amino acids. Methyl and ethyl esters are commonly available for many amino acids; the "t"-butyl ester tends to be more expensive. However, "t"-butyl esters are particularly useful because, under strongly acidic conditions, the "t"-butyl esters undergo elimination to give the carboxylic acid and isobutylene, simplifying work-up.

Many esters have distinctive fruit-like odors, and many occur naturally in the essential oils of plants. This has also led to their common use in artificial flavorings and fragrances which aim to mimic those odors.




</doc>
<doc id="9677" url="https://en.wikipedia.org/wiki?curid=9677" title="Endosymbiont">
Endosymbiont

An endosymbiont or endobiont is any organism that lives within the body or cells of another organism most often, though not always, in a mutualistic relationship.
(The term endosymbiosis is from the Greek: ἔνδον "endon" "within", σύν "syn" "together" and βίωσις "biosis" "living".) Examples are nitrogen-fixing bacteria (called rhizobia), which live in the root nodules of legumes; single-cell algae inside reef-building corals, and bacterial endosymbionts that provide essential nutrients to about 10–15% of insects.

There are two types of symbiont transmissions. In horizontal transmission, each new generation acquires free living symbionts from the environment. An example is the nitrogen-fixing bacteria in certain plant roots. Vertical transmission takes place when the symbiont is transferred directly from parent to offspring. There is also a combination of these types, where symbionts are transferred vertically for some generation before a switch of host occurs and new symbionts are horizontally acquired from the environment. In vertical transmissions, the symbionts often have a reduced genome and are no longer able to survive on their own. As a result, the symbiont depends on the host, resulting in a highly intimate co-dependent relationship. For instance, pea aphid symbionts have lost genes for essential molecules, now relying on the host to supply them with nutrients. In return, the symbionts synthesize essential amino acids for the aphid host . Other examples include "Wigglesworthia" nutritional symbionts of tse-tse flies, or in sponges. When a symbiont reaches this stage, it begins to resemble a cellular organelle, similar to mitochondria or chloroplasts.

Many instances of endosymbiosis are obligate; that is, either the endosymbiont or the host cannot survive without the other, such as the gutless marine worms of the genus "Riftia", which get nutrition from their endosymbiotic bacteria. The most common examples of obligate endosymbioses are mitochondria and chloroplasts. Some human parasites, e.g. "Wuchereria bancrofti" and "Mansonella perstans", thrive in their intermediate insect hosts because of an obligate endosymbiosis with "Wolbachia spp". They can both be eliminated from said hosts by treatments that target this bacterium. However, not all endosymbioses are obligate and some endosymbioses can be harmful to either of the organisms involved.

Two major types of organelle in eukaryotic cells, mitochondria and plastids such as chloroplasts, are considered to be bacterial endosymbionts. This process is commonly referred to as symbiogenesis.

Symbiogenesis explains the origins of eukaryotes, whose cells contain two major kinds of organelle: mitochondria and chloroplasts. The theory proposes that these organelles evolved from certain types of bacteria that eukaryotic cells engulfed through phagocytosis. These cells and the bacteria trapped inside them entered an endosymbiotic relationship, meaning that the bacteria took up residence and began living exclusively within the eukaryotic cells.

Numerous insect species have endosymbionts at different stages of symbiogenesis. A common theme of symbiogenesis involves the reduction of the genome to only essential genes for the host and symbiont collective genome. A remarkable example of this is the fractionation of the "Hodgkinia" genome of "Magicicada" cicadas. Because the cicada life cycle takes years underground, natural selection on endosymbiont populations is relaxed for many bacterial generations. This allows the symbiont genomes to diversify within the host for years with only punctuated periods of selection when the cicadas reproduce. As a result, the ancestral "Hodgkinia" genome has split into three groups of primary endosymbiont, each encoding only a fraction of the essential genes for the symbiosis. The host now requires all three sub-groups of symbiont, each with degraded genomes lacking most essential genes for bacterial viability.

The best-studied examples of endosymbiosis are known from invertebrates. These symbioses affect organisms with global impact, including "symbiodinium" of corals, or "Wolbachia" of insects. Many insect agricultural pests and human disease vectors have intimate relationships with primary endosymbionts.

Scientists classify insect endosymbionts in two broad categories, 'Primary' and 'Secondary'. Primary endosymbionts (sometimes referred to as P-endosymbionts) have been associated with their insect hosts for many millions of years (from 10 to several hundred million years in some cases). They form obligate associations (see below), and display cospeciation with their insect hosts. Secondary endosymbionts exhibit a more recently developed association, are sometimes horizontally transferred between hosts, live in the hemolymph of the insects (not specialized bacteriocytes, see below), and are not obligate.

Among primary endosymbionts of insects, the best-studied are the pea aphid ("Acyrthosiphon pisum") and its endosymbiont "Buchnera sp." APS, the tsetse fly "Glossina morsitans morsitans" and its endosymbiont "Wigglesworthia glossinidia brevipalpis" and the endosymbiotic protists in lower termites. As with endosymbiosis in other insects, the symbiosis is obligate in that neither the bacteria nor the insect is viable without the other. Scientists have been unable to cultivate the bacteria in lab conditions outside of the insect. With special nutritionally-enhanced diets, the insects can survive, but are unhealthy, and at best survive only a few generations.

In some insect groups, these endosymbionts live in specialized insect cells called bacteriocytes (also called "mycetocytes"), and are maternally-transmitted, i.e. the mother transmits her endosymbionts to her offspring. In some cases, the bacteria are transmitted in the egg, as in "Buchnera"; in others like "Wigglesworthia", they are transmitted via milk to the developing insect embryo. In termites, the endosymbionts reside within the hindguts and are transmitted through trophallaxis among colony members.

The primary endosymbionts are thought to help the host either by providing nutrients that the host cannot obtain itself or by metabolizing insect waste products into safer forms. For example, the putative primary role of "Buchnera" is to synthesize essential amino acids that the aphid cannot acquire from its natural diet of plant sap. Likewise, the primary role of "Wigglesworthia", it is presumed, is to synthesize vitamins that the tsetse fly does not get from the blood that it eats. In lower termites, the endosymbiotic protists play a major role in the digestion of lignocellulosic materials that constitute a bulk of the termites' diet.

Bacteria benefit from the reduced exposure to predators and competition from other bacterial species, the ample supply of nutrients and relative environmental stability inside the host.

Genome sequencing reveals that obligate bacterial endosymbionts of insects have among the smallest of known bacterial genomes and have lost many genes that are commonly found in closely related bacteria. Several theories have been put forth to explain the loss of genes. It is presumed that some of these genes are not needed in the environment of the host insect cell. A complementary theory suggests that the relatively small numbers of bacteria inside each insect decrease the efficiency of natural selection in 'purging' deleterious mutations and small mutations from the population, resulting in a loss of genes over many millions of years. Research in which a parallel phylogeny of bacteria and insects was inferred supports the belief that the primary endosymbionts are transferred only vertically (i.e., from the mother), and not horizontally (i.e., by escaping the host and entering a new host).

Attacking obligate bacterial endosymbionts may present a way to control their insect hosts, many of which are pests or carriers of human disease. For example, aphids are crop pests and the tsetse fly carries the organism "Trypanosoma brucei" that causes African sleeping sickness. Other motivations for their study involve understanding the origins of symbioses in general, as a proxy for understanding e.g. how chloroplasts or mitochondria came to be obligate symbionts of eukaryotes or plants.

The pea aphid ("Acyrthosiphon pisum") is known to contain at least three secondary endosymbionts, "Hamiltonella defensa", "Regiella insecticola", and "Serratia symbiotica". "Hamiltonella defensa" defends its aphid host from parasitoid wasps. This defensive symbiosis improves the survival of aphids, which have lost some elements of the insect immune response.

One of the best-understood defensive symbionts is the spiral bacteria "Spiroplasma poulsonii". "Spiroplasma sp." can be reproductive manipulators, but also defensive symbionts of "Drosophila" flies. In "Drosophila neotestacea", "S. poulsonii" has spread across North America owing to its ability to defend its fly host against nematode parasites. This defence is mediated by toxins called "ribosome-inactivating proteins" that attack the molecular machinery of invading parasites. These "Spiroplasma" toxins represent one of the first examples of a defensive symbiosis with a mechanistic understanding for defensive symbiosis between an insect endosymbiont and its host.

"Sodalis glossinidius" is a secondary endosymbiont of tsetse flies that lives inter- and intracellularly in various host tissues, including the midgut and hemolymph. Phylogenetic studies have not indicated a correlation between evolution of "Sodalis" and tsetse. Unlike tsetse's primary symbiont "Wigglesworthia", though, "Sodalis" has been cultured "in vitro".

Many other insects have secondary endosymbionts not reviewed here.

Extracellular endosymbionts are also represented in all four extant classes of Echinodermata (Crinoidea, Ophiuroidea, Echinoidea, and Holothuroidea). Little is known of the nature of the association (mode of infection, transmission, metabolic requirements, etc.) but phylogenetic analysis indicates that these symbionts belong to the alpha group of the class Proteobacteria, relating them to "Rhizobium" and "Thiobacillus". Other studies indicate that these subcuticular bacteria may be both abundant within their hosts and widely distributed among the Echinoderms in general.

Some marine oligochaeta (e.g., "Olavius algarvensis" and "Inanidrillus spp.") have obligate extracellular endosymbionts that fill the entire body of their host. These marine worms are nutritionally dependent on their symbiotic chemoautotrophic bacteria lacking any digestive or excretory system (no gut, mouth, or nephridia).

The sea slug "Elysia chlorotica" lives in endosymbiotic relationship with the algae "Vaucheria litorea", and the jellyfish "Mastigias" have a similar relationship with an algae.

Dinoflagellate endosymbionts of the genus "Symbiodinium", commonly known as zooxanthellae, are found in corals, mollusks (esp. giant clams, the "Tridacna"), sponges, and foraminifera. These endosymbionts drive the formation of coral reefs by capturing sunlight and providing their hosts with energy for carbonate deposition.

Previously thought to be a single species, molecular phylogenetic evidence over the past couple decades has shown there to be great diversity in "Symbiodinium". In some cases, there is specificity between host and "Symbiodinium" clade. More often, however, there is an ecological distribution of "Symbiodinium", the symbionts switching between hosts with apparent ease. When reefs become environmentally stressed, this distribution of symbionts is related to the observed pattern of coral bleaching and recovery. Thus, the distribution of "Symbiodinium" on coral reefs and its role in coral bleaching presents one of the most complex and interesting current problems in reef ecology.

In marine environments, bacterial endosymbionts have more recently been discovered. These endosymbiotic relationships are especially prevalent in oligotrophic or nutrient-poor regions of the ocean like that of the North Atlantic.  In these oligotrophic waters, cell growth of larger phytoplankton like that of diatoms is limited by low nitrate concentrations.  Endosymbiotic bacteria fix nitrogen for their diatom hosts and in turn receive organic carbon from photosynthesis. These symbioses play an important role in global carbon cycling in oligotrophic regions.

One known symbiosis between the diatom "Hemialus" spp. and the cyanobacterium "Richelia intracellularis" has been found in the North Atlantic, Mediterranean, and Pacific Ocean. The "Richelia" endosymbiont is found within the diatom frustule of "Hemiaulus" spp., and has a reduced genome likely losing genes related to pathways the host now provides.  Research by Foster et al. (2011) measured nitrogen fixation by the cyanobacterial host "Richelia intracellularis" well above intracellular requirements, and found the cyanobacterium was likely fixing excess nitrogen for Hemiaulus host cells.  Additionally, both host and symbiont cell growth were much greater than free-living "Richelia intracellularis" or symbiont-free "Hemiaulus" spp.  The "Hemaiulus"-"Richelia" symbiosis is not obligatory especially in areas with excess nitrogen (nitrogen replete).

"Richelia intracellularis" is also found in "Rhizosolenia" spp., a diatom found in oligotrophic oceans. Compared to the "Hemaiulus" host, the endosymbiosis with "Rhizosolenia" is much more consistent, and "Richelia intracellularis" is generally found in "Rhizosolenia". There are some asymbiotic (occurs without an endosymbiont) Rhizosolenia, however there appears to be mechanisms limiting growth of these organisms in low nutrient conditions. Cell division for both the diatom host and cyanobacterial symbiont can be uncoupled and mechanisms for passing bacterial symbionts to daughter cells during cell division are still relatively unknown.

Other endosymbiosis with nitrogen fixers in open oceans include Calothrix in Chaetocerous spp. and UNCY-A in prymnesiophyte microalga.  The Chaetocerous-Calothrix endosymbiosis is hypothesized to be more recent, as the Calothrix genome is generally intact. While other species like that of the UNCY-A symbiont and Richelia have reduced genomes.  This reduction in genome size occurs within nitrogen metabolism pathways indicating endosymbiont species are generating nitrogen for their hosts and losing the ability to use this nitrogen independently. This endosymbiont reduction in genome size, might be a step that occurred in the evolution of organelles (above).

"Mixotricha paradoxa" is a protozoan that lacks mitochondria. However, spherical bacteria live inside the cell and serve the function of the mitochondria. "Mixotricha" also has three other species of symbionts that live on the surface of the cell.

"Paramecium bursaria", a species of ciliate, has a mutualistic symbiotic relationship with green alga called Zoochlorella. The algae live inside the cell, in the cytoplasm.

"Paulinella chromatophora" is a freshwater amoeboid which has recently (evolutionarily speaking) taken on a cyanobacterium as an endosymbiont.

Many foraminifera are hosts to several types of algae, such as red algae, diatoms, dinoflagellates and chlorophyta. These endosymbionts can be transmitted vertically to the next generation via asexual reproduction of the host, but because the endosymbionts are larger than the foraminiferal gametes, they need to acquire new algae again after sexual reproduction.

Several species of radiolaria have photosynthetic symbionts. In some species the host will sometimes digest algae to keep their population at a constant level.

Hatena arenicola is a flagellate protist with a complicated feeding apparaturs that feed on other microbes. But when it engulf a green alga from the genus Nephroselmis, the feeding apparatus disappears and it becomes photosyntethic. During mitosis the algae is transferred to only one of the two cells, and the cell without the algae needs to start the cycle all over again.

In 1966, biologist Kwang W. Jeon found that a lab strain of Amoeba proteus had been infected by bacteria that lived inside the cytoplasmic vacuoles. This infection killed all the protists except from a few individuals. After the equivalent of 40 host generations, the two organisms gradually became mutually interdependent. Over many years of study, it has been confirmed that a genetic exchenge between the prokaryotes and protists has occurred.

The spotted salamander ("Ambystoma maculatum") lives in a relationship with the algae "Oophila amblystomatis", which grows in the egg cases.

Chloroplasts are primary endosymbionts of plants that provide energy to the plant by generating sugars.

Of all the plants, "Azolla" has the most intimate relationship with a symbiont, as its cyanobacterium symbiont "Anabaena" is passed on directly from one generation to the next.

The human genome project found several thousand endogenous retroviruses, endogenous viral elements in the genome that closely resemble and can be derived from retroviruses, organized into 24 families.


</doc>
<doc id="9678" url="https://en.wikipedia.org/wiki?curid=9678" title="Exponential function">
Exponential function

In mathematics, an exponential function is a function of the form

where is a positive real number not equal to 1, and the argument occurs as an exponent. For real numbers and a function of the form formula_1 is also an exponential function, since it can be rewritten as 

As functions of a real variable, exponential functions are uniquely characterized by the fact that the growth rate of such a function (that is, its derivative) is directly proportional to the value of the function. The constant of proportionality of this relationship is the natural logarithm of the base :

For , the function formula_3 is increasing (as depicted for and ), because formula_4 makes the derivative always positive; while for , the function is decreasing (as depicted for ); and for the function is constant.

The constant is the unique base for which the constant of proportionality is 1, so that the function is its own derivative:

This function, also denoted as formula_5, is called the "natural exponential function", or simply "the exponential function". Since any exponential function can be written in terms of the natural exponential as formula_6, it is computationally and conceptually convenient to reduce the study of exponential functions to this particular one. The natural exponential is hence denoted by

The former notation is commonly used for simpler exponents, while the latter is preferred when the exponent is a complicated expression. The graph of formula_7 is upward-sloping, and increases faster as increases. The graph always lies above the -axis, but becomes arbitrarily close to it for large negative ; thus, the -axis is a horizontal asymptote. The equation formula_8 means that the slope of the tangent to the graph at each point is equal to its -coordinate at that point. Its inverse function is the natural logarithm, denoted formula_9 formula_10 or formula_11 because of this, some old texts refer to the exponential function as the antilogarithm.

The exponential function satisfies the fundamental multiplicative identity (which can be extended to complex-valued exponents as well):

It can be shown that every continuous, nonzero solution of the functional equation formula_12 is an exponential function, formula_13 with formula_14 The multiplicative identity, along with the definition formula_15, shows that formula_16 for positive integers , relating the exponential function to the elementary notion of exponentiation. 

The argument of the exponential function can be any real or complex number, or even an entirely different kind of mathematical object (e.g., matrix).

The ubiquitous occurrence of the exponential function in pure and applied mathematics has led mathematician W. Rudin to opine that the exponential function is "the most important function in mathematics". In applied settings, exponential functions model a relationship in which a constant change in the independent variable gives the same proportional change (i.e., percentage increase or decrease) in the dependent variable. This occurs widely in the natural and social sciences, as in a self-reproducing population, a fund accruing compound interest, or a growing body of manufacturing expertise. Thus, the exponential function also appears in a variety of contexts within physics, chemistry, engineering, mathematical biology, and economics. 

The real exponential function formula_17 can be characterized in a variety of equivalent ways. It is commonly defined by the following power series:

Since the radius of convergence of this power series is infinite, this definition is, in fact, applicable to all complex numbers formula_19 (see for the extension of formula_20 to the complex plane). The constant can then be defined as formula_21

The term-by-term differentiation of this power series reveals that formula_22 for all real , leading to another common characterization of formula_20 as the unique solution of the differential equation

satisfying the initial condition formula_25

Based on this characterization, the chain rule shows that its inverse function, the natural logarithm, satisfies formula_26 for formula_27 or formula_28 This relationship leads to a less common definition of the real exponential function formula_20 as the solution formula_30 to the equation

By way of the binomial theorem and the power series definition, the exponential function can also be defined as the following limit:

The exponential function arises whenever a quantity grows or decays at a rate proportional to its current value. One such situation is continuously compounded interest, and in fact it was this observation that led Jacob Bernoulli in 1683 to the number

now known as . Later, in 1697, Johann Bernoulli studied the calculus of the exponential function.

If a principal amount of 1 earns interest at an annual rate of compounded monthly, then the interest earned each month is times the current value, so each month the total value is multiplied by , and the value at the end of the year is . If instead interest is compounded daily, this becomes . Letting the number of time intervals per year grow without bound leads to the limit definition of the exponential function,

first given by Leonhard Euler.
This is one of a number of characterizations of the exponential function; others involve series or differential equations.

From any of these definitions it can be shown that the exponential function obeys the basic exponentiation identity,

which justifies the notation for .

The derivative (rate of change) of the exponential function is the exponential function itself. More generally, a function with a rate of change "proportional" to the function itself (rather than equal to it) is expressible in terms of the exponential function. This function property leads to exponential growth or exponential decay.

The exponential function extends to an entire function on the complex plane. Euler's formula relates its values at purely imaginary arguments to trigonometric functions. The exponential function also has analogues for which the argument is a matrix, or even an element of a Banach algebra or a Lie algebra.

The importance of the exponential function in mathematics and the sciences stems mainly from its property as the unique function which is equal to its derivative and is equal to 1 when . That is,

Functions of the form for constant are the only functions that are equal to their derivative (by the Picard–Lindelöf theorem). Other ways of saying the same thing include:

If a variable's growth or decay rate is proportional to its size—as is the case in unlimited population growth (see Malthusian catastrophe), continuously compounded interest, or radioactive decay—then the variable can be written as a constant times an exponential function of time. Explicitly for any real constant , a function satisfies if and only if for some constant . The constant "k" is called the decay constant, disintegration constant, rate constant, or transformation constant.

Furthermore, for any differentiable function , we find, by the chain rule:

A continued fraction for can be obtained via an identity of Euler:

The following generalized continued fraction for converges more quickly:

or, by applying the substitution :

with a special case for :

This formula also converges, though more slowly, for . For example:

As in the real case, the exponential function can be defined on the complex plane in several equivalent forms. The most common definition of the complex exponential function parallels the power series definition for real arguments, where the real variable is replaced by a complex one:

Term-wise multiplication of two copies of these power series in the Cauchy sense, permitted by Mertens' theorem, shows that the defining multiplicative property of exponential functions continues to hold for all complex arguments:

The definition of the complex exponential function in turn leads to the appropriate definitions extending the trigonometric functions to complex arguments.

In particular, when formula_46 (formula_47 real), the series definition yields the expansion

In this expansion, the rearrangement of the terms into real and imaginary parts is justified by the absolute convergence of the series. The real and imaginary parts of the above expression in fact correspond to the series expansions of and , respectively. 

This correspondence provides motivation for cosine and sine for all complex arguments in terms of formula_49 and the equivalent power series:

for all formula_51

The functions , , and so defined have infinite radii of convergence by the ratio test and are therefore entire functions ("i.e.", holomorphic on formula_52). The range of the exponential function is formula_53, while the ranges of the complex sine and cosine functions are both formula_52 in its entirety, in accord with Picard's theorem, which asserts that the range of a nonconstant entire function is either all of formula_52, or formula_52 excluding one lacunary value.

These definitions for the exponential and trigonometric functions lead trivially to Euler's formula:

We could alternatively define the complex exponential function based on this relationship. If formula_59, where formula_60 and formula_30 are both real, then we could define its exponential as

where , , and on the right-hand side of the definition sign are to be interpreted as functions of a real variable, previously defined by other means.

For formula_63, the relationship formula_64 holds, so that formula_65 for real formula_47 and formula_67 maps the real line (mod formula_68) to the unit circle. Based on the relationship between formula_69 and the unit circle, it is easy to see that, restricted to real arguments, the definitions of sine and cosine given above coincide with their more elementary definitions based on geometric notions.

The complex exponential function is periodic with period formula_70 and formula_71 holds for all formula_72. 

When its domain is extended from the real line to the complex plane, the exponential function retains the following properties:
for all formula_45.

Extending the natural logarithm to complex arguments yields the complex logarithm , which is a multivalued function.

We can then define a more general exponentiation:

for all complex numbers and . This is also a multivalued function, even when is real. This distinction is problematic, as the multivalued functions and are easily confused with their single-valued equivalents when substituting a real number for . The rule about multiplying exponents for the case of positive real numbers must be modified in a multivalued context:

See failure of power and logarithm identities for more about problems with combining powers.

The exponential function maps any line in the complex plane to a logarithmic spiral in the complex plane with the center at the origin. Two special cases exist: when the original line is parallel to the real axis, the resulting spiral never closes in on itself; when the original line is parallel to the imaginary axis, the resulting spiral is a circle of some radius.

Considering the complex exponential function as a function involving four real variables:
the graph of the exponential function is a two-dimensional surface curving through four dimensions.

Starting with a color-coded portion of the formula_81 domain, the following are depictions of the graph as variously projected into two or three dimensions.

The second image shows how the domain complex plane is mapped into the range complex plane:

The third and fourth images show how the graph in the second image extends into one of the other two dimensions not shown in the second image.

The third image shows the graph extended along the real formula_60 axis. It shows the graph is a surface of revolution about the formula_60 axis of the graph of the real exponential function, producing a horn or funnel shape.

The fourth image shows the graph extended along the imaginary formula_30 axis. It shows that the graph's surface for positive and negative formula_30 values doesn't really meet along the negative real formula_83 axis, but instead forms a spiral surface about the formula_30 axis. Because its formula_30 values have been extended to ±2π, this image also better depicts the 2π periodicity in the imaginary formula_30 value.

Complex exponentiation can be defined by converting to polar coordinates and using the identity :

However, when is not an integer, this function is multivalued, because is not unique (see failure of power and logarithm identities).

The power series definition of the exponential function makes sense for square matrices (for which the function is called the matrix exponential) and more generally in any unital Banach algebra . In this setting, , and is invertible with inverse for any in . If , then , but this identity can fail for noncommuting and .

Some alternative definitions lead to the same function. For instance, can be defined as

Or can be defined as , where is the solution to the differential equation , with initial condition ; it follows that for every in 

Given a Lie group and its associated Lie algebra formula_95, the exponential map is a map formula_95 satisfying similar properties. In fact, since is the Lie algebra of the Lie group of all positive real numbers under multiplication, the ordinary exponential function for real arguments is a special case of the Lie algebra situation. Similarly, since the Lie group of invertible matrices has as Lie algebra , the space of all matrices, the exponential function for square matrices is a special case of the Lie algebra exponential map.

The identity can fail for Lie algebra elements and that do not commute; the Baker–Campbell–Hausdorff formula supplies the necessary correction terms.

The function is not in (i.e., is not the quotient of two polynomials with complex coefficients).

For distinct complex numbers }, the set } is linearly independent over .

The function is transcendental over .

When computing (an approximation of) the exponential function near the argument , the result will be close to 1, and computing the value of the difference formula_97 with floating-point arithmetic may lead to the loss of (possibly all) significant figures, producing a large calculation error, possibly even a meaningless result. 

Following a proposal by William Kahan, it may thus be useful to have a dedicated routine, often called codice_1, for computing directly, bypassing computation of . For example, if the exponential is computed by using its Taylor series
one may use the Taylor series of formula_99

This was first implemented in 1979 in the Hewlett-Packard HP-41C calculator, and provided by several calculators, operating systems (for example Berkeley UNIX 4.3BSD), computer algebra systems, and programming languages (for example C99).

In addition to base , the IEEE 754-2008 standard defines similar exponential functions near 0 for base 2 and 10: formula_101 and formula_102.

A similar approach has been used for the logarithm (see lnp1).

An identity in terms of the hyperbolic tangent,
gives a high-precision value for small values of on systems that do not implement .





</doc>
<doc id="9679" url="https://en.wikipedia.org/wiki?curid=9679" title="Prince Eugene of Savoy">
Prince Eugene of Savoy

Prince Eugene Francis of Savoy–Carignano (18 October 1663 – 21 April 1736) was a field marshal in the army of the Holy Roman Empire and of the Austrian Habsburg dynasty during the 17th and 18th centuries. He was one of the most successful military commanders of his time, and rose to the highest offices of state at the Imperial court in Vienna.

Born in Paris, Eugene grew up around the court of King Louis XIV of France. Based on his poor physique and bearing, the Prince was initially prepared for a clerical career, but by the age of 19 he had determined on a military career. Following a scandal involving his mother Olympe, he was rejected by Louis XIV for service in the French army. Eugene moved to Austria and transferred his loyalty to the Holy Roman Empire.

Spanning six decades, Eugene served three Holy Roman emperors: Leopold I, Joseph I, and Charles VI. He first saw action against the Ottoman Turks at the Siege of Vienna in 1683 and the subsequent War of the Holy League, before serving in the Nine Years' War, fighting alongside his cousin, the Duke of Savoy. However, the Prince's fame was secured with his decisive victory against the Ottomans at the Battle of Zenta in 1697, earning him Europe-wide fame. Eugene enhanced his standing during the War of the Spanish Succession, where his partnership with the Duke of Marlborough secured victories against the French on the fields of Blenheim (1704), Oudenarde (1708), and Malplaquet (1709); he gained further success in the war as Imperial commander in northern Italy, most notably at the Battle of Turin (1706). Renewed hostilities against the Ottomans in the Austro-Turkish War consolidated his reputation, with victories at the battles of Petrovaradin (1716), and the decisive encounter at Belgrade (1717).

Throughout the late 1720s, Eugene's influence and skillful diplomacy managed to secure the Emperor powerful allies in his dynastic struggles with the Bourbon powers, but physically and mentally fragile in his later years, Eugene enjoyed less success as commander-in-chief of the army during his final conflict, the War of the Polish Succession. Nevertheless, in Austria, Eugene's reputation remains unrivalled. Although opinions differ as to his character, there is no dispute over his great achievements: he helped to save the Habsburg Empire from French conquest; he broke the westward thrust of the Ottomans, liberating parts of Europe after a century and a half of Turkish occupation; and he was one of the great patrons of the arts whose building legacy can still be seen in Vienna today. Eugene died in his sleep at his home on 21 April 1736, aged 72.

Prince Eugene was born in the Hôtel de Soissons in Paris on 18 October 1663. His mother, Olympia Mancini, was one of Cardinal Mazarin's nieces whom he had brought to Paris from Rome in 1647 to further his, and, to a lesser extent, their ambitions. The Mancinis were raised at the Palais-Royal along with the young Louis XIV, with whom Olympia formed an intimate relationship. Yet to her great disappointment, her chance to become queen passed by, and in 1657, Olympia married Eugene Maurice, Count of Soissons, Count of Dreux and Prince of Savoy. Together they had had five sons (Eugene being the youngest) and three daughters, but neither parent spent much time with the children: his father, a brave, unglamorous French soldier, spent much of his time away campaigning, while Olympia's passion for court intrigue meant the children received little attention from her.

The King remained strongly attached to Olympia, so much so that many believed them to be lovers; but her scheming eventually led to her downfall. After falling out of favour at court, Olympia turned to Catherine Deshayes (known as "La Voisin"), and the arts of black magic and astrology. It was a fatal relationship. Embroiled in the "affaire des poisons", suspicions now abounded of her involvement in her husband's premature death in 1673, and even implicated her in a plot to kill the King himself. Whatever the truth, Olympia, rather than face trial, subsequently fled France for Brussels in January 1680, leaving Eugene in the care of his father's mother, Marie de Bourbon, and her daughter, Hereditary Princess of Baden, mother of Prince Louis of Baden.

From the age of ten, Eugene had been brought up for a career in the church; a personal choice of the King, basing the decision on the young Prince's poor physique and bearing. Certainly Eugene's appearance was not impressive — "He was never good-looking …" wrote the Duchess of Orléans, "It is true that his eyes are not ugly, but his nose ruins his face; he has two large teeth which are visible at all times."

In February 1683, to the surprise of his family, Eugene declared his intention of joining the army. Now 19 years old, Eugene applied directly to Louis XIV for command of a company in French service, but the King—who had shown no compassion for Olympia's children since her disgrace—refused him out of hand. "The request was modest, not so the petitioner," he remarked. "No one else ever presumed to stare me out so insolently." Whatever the case, Louis XIV's choice would cost him dearly twenty years later, for it would be precisely Eugene, in collaboration with the Duke of Marlborough, who would defeat the French army at Blenheim, a decisive battle which checked French military supremacy and political power.

Denied a military career in France, Eugene decided to seek service abroad. One of Eugene's brothers, Louis Julius, had entered Imperial service the previous year, but he had been immediately killed fighting the Ottoman Turks in 1683. When news of his death reached Paris, Eugene decided to travel to Austria in the hope of taking over his brother's command. It was not an unnatural decision: his cousin, Louis of Baden, was already a leading general in the Imperial army, as was a more distant cousin, Maximilian II Emanuel, Elector of Bavaria. On the night of July 26, 1683, Eugene left Paris and headed east.

By May 1683, the Ottoman threat to Emperor Leopold I's capital, Vienna, was very real. The Grand Vizier, Kara Mustafa Pasha—encouraged by Imre Thököly's Magyar rebellion—had invaded Hungary with between 100,000–200,000 men; within two months approximately 90,000 were beneath Vienna's walls. With the 'Turks at the gates', the Emperor fled for the safe refuge of Passau up the Danube, a more distant and secure part of his dominion. It was at Leopold I's camp that Eugene arrived in mid-August.

Although Eugene was not of Austrian extraction, he did have Habsburg antecedents. His grandfather, Thomas Francis, founder of the Carignano line of the House of Savoy, was the son of Catherine Michelle—a daughter of Philip II of Spain—and the great-grandson of the Emperor Charles V. But of more immediate consequence to Leopold I was the fact that Eugene was the second cousin of Victor Amadeus, the Duke of Savoy, a connection that the Emperor hoped might prove useful in any future confrontation with France. These ties, together with his ascetic manner and appearance (a positive advantage to him at the sombre court of Leopold I), ensured the refugee from the hated French king a warm welcome at Passau, and a position in Imperial service. Though French was his favored language, he communicated with Leopold in Italian, as the Emperor (though he knew it perfectly) disliked French. But Eugene also had a reasonable command of German, which he understood very easily, something that helped him much in the military.

Eugene was in no doubt where his new allegiance lay—"I will devote all my strength, all my courage, and if need be, my last drop of blood, to the service of your Imperial Majesty." This loyalty was immediately put to the test. By September, the Imperial forces under the Duke of Lorraine, together with a powerful Polish army under King John III Sobieski, were poised to strike the Sultan's army. On the morning of 12 September, the Christian forces drew up in line of battle on the south-eastern slopes of the Vienna Woods, looking down on the massed enemy camp. The day-long Battle of Vienna resulted in the lifting of the 60-day siege, and the Sultan's forces were routed and in retreat. Serving under Baden, Eugene distinguished himself in the battle, earning commendation from Lorraine and the Emperor; he later received the nomination for the colonelcy of the Dragoon Regiment Kufstein.

In March 1684, Leopold I formed the Holy League with Poland and Venice to counter the Ottoman threat. For the next two years, Eugene continued to perform with distinction on campaign and establish himself as a dedicated, professional soldier; by the end of 1685, still only 22 years old, he was made a Major-General. However, little is known of Eugene's life during these early campaigns. Contemporary observers make only passing comments of his actions, and his own surviving correspondence, largely to his cousin Victor Amadeus, are typically reticent about his own feelings and experiences. Nevertheless, it is clear that Baden was impressed with Eugene's qualities—"This young man will, with time, occupy the place of those whom the world regards as great leaders of armies."

In June 1686, the Duke of Lorraine besieged Buda (Budapest), the centre of the Ottoman occupation in Hungary. After resisting for 78 days, the city fell on 2 September, and Turkish resistance collapsed throughout the region as far away as Transylvania and Serbia. Further success followed in 1687, where, commanding a cavalry brigade, Eugene made an important contribution to the victory at the Battle of Mohács on 12 August. Such was the scale of their defeat that the Ottoman army mutinied—a revolt which spread to Constantinople. The Grand Vizier, Suluieman Pasha, was executed and Sultan Mehmed IV, deposed. Once again, Eugene's courage earned him recognition from his superiors, who granted him the honour of personally conveying the news of victory to the Emperor in Vienna. For his services, Eugene was promoted to Lieutenant-General in November 1687. He was also gaining wider recognition. King Charles II of Spain bestowed upon him the Order of the Golden Fleece, while his cousin, Victor Amadeus, provided him with money and two profitable abbeys in Piedmont. However, Eugene's military career suffered a temporary setback in 1688 when, on 6 September, the Prince suffered a severe wound to his knee by a musket ball during the Siege of Belgrade. It was not until January 1689 that he could return to active service.

Just as Belgrade was falling to Imperial forces under Max Emmanuel in the east, French troops in the west were crossing the Rhine into the Holy Roman Empire. Louis XIV had hoped that a show of force would lead to a quick resolution to his dynastic and territorial disputes with the princes of the Empire along his eastern border, but his intimidatory moves only strengthened German resolve, and in May 1689, Leopold I and the Dutch signed an offensive compact aimed at repelling French aggression.

The Nine Years' War was professionally and personally frustrating for the Prince. Initially fighting on the Rhine with Max Emmanuel—receiving a slight head wound at the Siege of Mainz in 1689—Eugene subsequently transferred himself to Piedmont after Victor Amadeus joined the Alliance against France in 1690. Promoted to general of cavalry, he arrived in Turin with his friend the Prince of Commercy; but it proved an inauspicious start. Against Eugene's advice, Amadeus insisted on engaging the French at Staffarda and suffered a serious defeat—only Eugene's handling of the Savoyard cavalry in retreat saved his cousin from disaster. Eugene remained unimpressed with the men and their commanders throughout the war in Italy. "The enemy would long ago have been beaten," he wrote to Vienna, "if everyone had done their duty." So contemptuous was he of the Imperial commander, Count Caraffa, he threatened to leave Imperial service.

In Vienna, Eugene's attitude was dismissed as the arrogance of a young upstart, but so impressed was the Emperor by his passion for the Imperial cause, he promoted him to Field-Marshal in 1693. When Caraffa's replacement, Count Caprara, was himself transferred in 1694, it seemed that Eugene's chance for command and decisive action had finally arrived. But Amadeus, doubtful of victory and now more fearful of Habsburg influence in Italy than he was of French, had begun secret dealings with Louis XIV aimed at extricating himself from the war. By 1696, the deal was done, and Amadeus transferred his troops and his loyalty to the enemy. Eugene was never to fully trust his cousin again; although he continued to pay due reverence to the Duke as head of his family, their relationship would forever after remain strained.

Military honours in Italy undoubtedly belonged to the French commander Marshal Catinat, but Eugene, the one Allied general determined on action and decisive results, did well to emerge from the Nine Years' War with an enhanced reputation. With the signing of the Treaty of Ryswick in September/October 1697, the desultory war in the west was finally brought to an inconclusive end, and Leopold I could once again devote all his martial energies into defeating the Ottoman Turks in the east.

The distractions of the war against Louis XIV had enabled the Turks to recapture Belgrade in 1690. In August 1691, the Austrians, under Louis of Baden, regained the advantage by heavily defeating the Turks at the Battle of Slankamen on the Danube, securing Habsburg possession of Hungary and Transylvania. However, when Baden was transferred west to fight the French in 1692, his successors, first Caprara, then from 1696, Frederick Augustus, the Elector of Saxony, proved incapable of delivering the final blow. On the advice of the President of the Imperial War Council, Rüdiger Starhemberg, Eugene was offered supreme command of Imperial forces in April 1697. This was Eugene's first truly independent command—no longer need he suffer under the excessively cautious generalship of Caprara and Caraffa, or be thwarted by the deviations of Victor Amadeus. But on joining his army, he found it in a state of 'indescribable misery'. Confident and self-assured, the Prince of Savoy (ably assisted by Commercy and Guido Starhemberg) set about restoring order and discipline.

Leopold I had warned Eugene to act cautiously, but when the Imperial commander learnt of Sultan Mustafa II's march on Transylvania, Eugene abandoned all ideas of a defensive campaign and moved to intercept the Turks as they crossed the River Tisza at Zenta on 11 September 1697. It was late in the day before the Imperial army struck. The Turkish cavalry had already crossed the river so Eugene decided to attack immediately, arranging his men in a half-moon formation. The vigour of the assault wrought terror and confusion amongst the Turks, and by nightfall, the battle was won. For the loss of some 2,000 dead and wounded, Eugene had inflicted approximately 25,000 casualties on his enemy—including the Grand Vizier, Elmas Mehmed Pasha—annihilating the Turkish army. Although the Ottomans lacked western organisation and training, the Savoyard prince had revealed his tactical skill, his capacity for bold decision, and his ability to inspire his men to excel in battle against a dangerous foe.

After a brief terror-raid into Ottoman-held Bosnia, culminating in the sack of Sarajevo, Eugene returned to Vienna in November to a triumphal reception. His victory at Zenta had turned him into a European hero, and with victory came reward. Land in Hungary, given him by the Emperor, yielded a good income, enabling the Prince to cultivate his newly acquired tastes in art and architecture (see below); but for all his new-found wealth and property, he was, nevertheless, without personal ties or family commitments. Of his four brothers, only one was still alive at this time. His fourth brother, Emmanuel, had died aged 14 in 1676; his third, Louis Julius (already mentioned) had died on active service in 1683, and his second brother, Philippe, died of smallpox in 1693. Eugene's remaining brother, Louis Thomas—ostracised for incurring the displeasure of Louis XIV—travelled Europe in search of a career, before arriving in Vienna in 1699. With Eugene's help, Louis found employment in the Imperial army, only to be killed in action against the French in 1702. Of Eugene's sisters, the youngest had died in childhood. The other two, Marie Jeanne-Baptiste and Louise Philiberte, led dissolute lives. Expelled from France, Marie joined her mother in Brussels, before eloping with a renegade priest to Geneva, living with him unhappily until her premature death in 1705. Of Louise, little is known after her early salacious life in Paris, but in due course, she lived for a time in a convent in Savoy before her death in 1726.

The Battle of Zenta proved to be the decisive victory in the long war against the Turks. With Leopold I's interests now focused on Spain and the imminent death of Charles II, the Emperor terminated the conflict with the Sultan, and signed the Treaty of Karlowitz on 26 January 1699.

With the death of the infirm and childless Charles II of Spain on 1 November 1700, the succession of the Spanish throne and subsequent control over her empire once again embroiled Europe in war—the War of the Spanish Succession. On his deathbed Charles II had bequeathed the entire Spanish inheritance to Louis XIV's grandson, Philip, Duke of Anjou. This threatened to unite the Spanish and French kingdoms under the House of Bourbon—something unacceptable to England, the Dutch Republic, and Leopold I, who had himself a claim to the Spanish throne. From the beginning, the Emperor had refused to accept the will of Charles II, and he did not wait for England and the Dutch Republic to begin hostilities. Before a new Grand Alliance could be concluded Leopold I prepared to send an expedition to seize the Spanish lands in Italy.

Eugene crossed the Alps with some 30,000 men in May/June 1701. After a series of brilliant manoeuvres the Imperial commander defeated Catinat at the Battle of Carpi on 9 July. "I have warned you that you are dealing with an enterprising young prince," wrote Louis XIV to his commander, "he does not tie himself down to the rules of war." On 1 September Eugene defeated Catinat's successor, Marshal Villeroi, at the Battle of Chiari, in a clash as destructive as any in the Italian theatre. But as so often throughout his career the Prince faced war on two fronts—the enemy in the field and the government in Vienna. Starved of supplies, money and men, Eugene was forced into unconventional means against the vastly superior enemy. During a daring raid on Cremona on the night of 31 January/1 February 1702 Eugene captured the French commander-in-chief. Yet the coup was less successful than hoped: Cremona remained in French hands, and the Duke of Vendôme, whose talents far exceeded Villeroi's, became the theatre's new commander. Villeroi's capture caused a sensation in Europe, and had a galvanising effect on English public opinion. "The surprise at Cremona," wrote the diarist John Evelyn, "… was the greate discourse of this weeke"; but appeals for succour from Vienna remained unheeded, forcing Eugene to seek battle and gain a 'lucky hitt'. The resulting Battle of Luzzara on 15 August proved inconclusive. Although Eugene's forces inflicted double the number of casualties on the French the battle settled little except to deter Vendôme trying an all-out assault on Imperial forces that year, enabling Eugene to hold on south of the Alps. With his army rotting away, and personally grieving for his long standing friend Prince Commercy who had died at Luzzara, Eugene returned to Vienna in January 1703.

Eugene's European reputation was growing (Cremona and Luzzara had been celebrated as victories throughout the Allied capitals), yet because of the condition and morale of his troops the 1702 campaign had not been a success. Austria itself was now facing the direct threat of invasion from across the border in Bavaria where the state's Elector, Maximilian Emanuel, had declared for the Bourbons in August the previous year. Meanwhile, in Hungary a small-scale revolt had broken out in May and was fast gaining momentum. With the monarchy at the point of complete financial breakdown Leopold I was at last persuaded to change the government. At the end of June 1703 Gundaker Starhemberg replaced Gotthard Salaburg as President of the Treasury, and Prince Eugene succeeded Henry Mansfeld as the new President of the Imperial War Council ("Hofkriegsratspräsident").

As head of the war council Eugene was now part of the Emperor's inner circle, and the first president since Montecuccoli to remain an active commander. Immediate steps were taken to improve efficiency within the army: encouragement and, where possible, money, was sent to the commanders in the field; promotion and honours were distributed according to service rather than influence; and discipline improved. But the Austrian monarchy faced severe peril on several fronts in 1703: by June the Duke of Villars had reinforced the Elector of Bavaria on the Danube thus posing a direct threat to Vienna, while Vendôme remained at the head of a large army in northern Italy opposing Guido Starhemberg's weak Imperial force. Of equal alarm was Francis II Rákóczi's revolt which, by the end of the year, had reached as far as Moravia and Lower Austria.

Dissension between Villars and the Elector of Bavaria had prevented an assault on Vienna in 1703, but in the Courts of Versailles and Madrid, ministers confidently anticipated the city's fall. The Imperial ambassador in London, Count Wratislaw, had pressed for Anglo-Dutch assistance on the Danube as early as February 1703, but the crisis in southern Europe seemed remote from the Court of St. James's where colonial and commercial considerations were more to the fore of men's minds. Only a handful of statesmen in England or the Dutch Republic realised the true implications of Austria's peril; foremost amongst these was the English Captain-General, the Duke of Marlborough.

By early 1704 Marlborough had resolved to march south and rescue the situation in southern Germany and on the Danube, personally requesting the presence of Eugene on campaign so as to have "a supporter of his zeal and experience". The Allied commanders met for the first time at the small village of Mundelsheim on 10 June, and immediately formed a close rapport—the two men becoming, in the words of Thomas Lediard, 'Twin constellations in glory'. This professional and personal bond ensured mutual support on the battlefield, enabling many successes during the Spanish Succession war. The first of these victories, and the most celebrated, came on 13 August 1704 at the Battle of Blenheim. Eugene commanded the right wing of the Allied army, holding the Elector of Bavaria's and Marshal Marsin's superior forces, while Marlborough broke through the Marshal Tallard's center, inflicting over 30,000 casualties. The battle proved decisive: Vienna was saved and Bavaria was knocked out of the war. Both Allied commanders were full of praise for each other's performance. Eugene's holding operation, and his pressure for action leading up to the battle, proved crucial for the Allied success.

In Europe Blenheim is regarded as much a victory for Eugene as it is for Marlborough, a sentiment echoed by Sir Winston Churchill (Marlborough's descendant and biographer), who pays tribute to "the glory of Prince Eugene, whose fire and spirit had exhorted the wonderful exertions of his troops." France now faced the real danger of invasion, but Leopold I in Vienna was still under severe strain: Rákóczi's revolt was a major threat; and Guido Starhemberg and Victor Amadeus (who had once again switched loyalties and rejoined the Grand Alliance in 1703) had been unable to halt the French under Vendôme in northern Italy. Only Amadeus' capital, Turin, held on.

Eugene returned to Italy in April 1705, but his attempts to move west towards Turin were thwarted by Vendôme's skillful manoeuvres. Lacking boats and bridging materials, and with desertion and sickness rife within his army, the outnumbered Imperial commander was helpless. Leopold I's assurances of money and men had proved illusory, but desperate appeals from Amadeus and criticism from Vienna goaded the Prince into action, resulting in the Imperialists' bloody defeat at the Battle of Cassano on 16 August. However, following Leopold I's death and the accession of Joseph I to the Imperial throne in May 1705, Eugene at last began to receive the personal backing he desired. Joseph I proved to be a strong supporter of Eugene's supremacy in military affairs; he was the most effective emperor the Prince served and the one he was happiest under. Promising support, Joseph I persuaded Eugene to return to Italy and restore Habsburg honour.

The Imperial commander arrived in theatre in mid-April 1706, just in time to organise an orderly retreat of what was left of Count Reventlow's inferior army following his defeat by Vendôme at the Battle of Calcinato on 19 April. Vendôme now prepared to defend the lines along the river Adige, determined to keep Eugene cooped to the east while the Marquis of La Feuillade threatened Turin. However, feigning attacks along the Adige, Eugene descended south across the river Po in mid-July, outmanoeuvring the French commander and gaining a favourable position from which he could at last move west towards Piedmont and relieve Savoy's capital.

Events elsewhere were now to have major consequences for the war in Italy. With Villeroi's crushing defeat by Marlborough at the Battle of Ramillies on 23 May, Louis XIV recalled Vendôme north to take command of French forces in Flanders. It was a transfer that Saint-Simon considered something of a deliverance for the French commander who was "now beginning to feel the unlikelihood of success [in Italy] … for Prince Eugene, with the reinforcements that had joined him after the Battle of Calcinato, had entirely changed the outlook in that theatre of the war." The Duke of Orléans, under the direction of Marsin, replaced Vendôme, but indecision and disorder in the French camp led to their undoing. After uniting his forces with Victor Amadeus at Villastellone in early September, Eugene attacked, overwhelmed, and decisively defeated the French forces besieging Turin on 7 September. Eugene's success broke the French hold on northern Italy, and the whole Po valley fell under Allied control. Eugene had gained a victory as signal as his colleague had at Ramillies—"It is impossible for me to express the joy it has given me;" wrote Marlborough, "for I not only esteem but I really love the prince. This glorious action must bring France so low, that if our friends could but be persuaded to carry on the war with vigour one year longer, we cannot fail, with the blessing of God, to have such a peace as will give us quiet for all our days."

The Imperial victory in Italy marked the beginning of Austrian rule in Lombardy, and earned Eugene the Governorship of Milan. But the following year was to prove a disappointment for the Prince and the Grand Alliance as a whole. The Emperor and Eugene (whose main goal after Turin was to take Naples and Sicily from Philip duc d'Anjou's supporters), reluctantly agreed to Marlborough's plan for an attack on Toulon—the seat of French naval power in the Mediterranean. However, disunion between the Allied commanders—Victor Amadeus, Eugene, and the English Admiral Shovell—doomed the Toulon enterprise to failure. Although Eugene favoured some sort of attack on France's south-eastern border it was clear he felt the expedition impractical, and had shown none of the "alacrity which he had displayed on other occasions." Substantial French reinforcements finally brought an end to the venture, and on 22 August 1707 the Imperial army began its retirement. The subsequent capture of Susa could not compensate for the total collapse of the Toulon expedition and with it any hope of an Allied war-winning blow that year.

At the beginning of 1708 Eugene successfully evaded calls for him to take charge in Spain (in the end Guido Starhemberg was sent), thus enabling him to take command of the Imperial army on the Moselle and once again unite with Marlborough in the Spanish Netherlands. Eugene (without his army) arrived at the Allied camp at Assche, west of Brussels, in early July, providing a welcome boost to morale after the early defection of Bruges and Ghent to the French. " … our affairs improved through God's support and Eugene's aid," wrote the Prussian General Natzmer, "whose timely arrival raised the spirits of the army again and consoled us." Heartened by the Prince's confidence the Allied commanders devised a bold plan to engage the French army under Vendôme and the Duke of Burgundy. On 10 July the Anglo-Dutch army made a forced march to surprise the French, reaching the river Scheldt just as the enemy were crossing to the north. The ensuing battle on 11 July—more a contact action rather than a set-piece engagement—ended in a resounding success for the Allies, aided by the dissension of the two French commanders. While Marlborough remained in overall command, Eugene had led the crucial right flank and centre. Once again the Allied commanders had co-operated remarkably well. "Prince Eugene and I," wrote the Duke, "shall never differ about our share of the laurels."

Marlborough now favoured a bold advance along the coast to bypass the major French fortresses, followed by a march on Paris. But fearful of unprotected supply-lines, the Dutch and Eugene favoured a more cautious approach. Marlborough acquiesced and resolved upon the siege of Vauban's great fortress, Lille. While the Duke commanded the covering force, Eugene oversaw the siege of the town which surrendered on 22 October; however, it was not until 10 December that the resolute Marshal Boufflers yielded the citadel. Yet for all the difficulties of the siege (Eugene was badly wounded above his left eye by a musket ball, and even survived an attempt to poison him), the campaign of 1708 had been a remarkable success. The French were driven out of almost all the Spanish Netherlands. "He who has not seen this," wrote Eugene, "has seen nothing."

The recent defeats, together with the severe winter of 1708–09, had caused extreme famine and privation in France. Louis XIV was close to accepting Allied terms, but the conditions demanded by the leading Allied negotiators, Anthonie Heinsius, Charles Townshend, Marlborough, and Eugene—principally that Louis XIV should use his own troops to force Philip V off the Spanish throne—proved unacceptable to the French. Neither Eugene nor Marlborough had objected to the Allied demands at the time, but neither wanted the war with France to continue, and would have preferred further talks to deal with the Spanish issue. But the French King offered no further proposals. Lamenting the collapse of the negotiations, and aware of the vagaries of war, Eugene wrote to the Emperor in mid-June 1709. "There can be no doubt that the next battle will be the biggest and bloodiest that has yet been fought."

After the fall of Tournai on 3 September (itself a major undertaking), the Allied generals turned their attention towards Mons. Marshal Villars, recently joined by Boufflers, moved his army south-west of the town and began to fortify his position. Marlborough and Eugene favoured an engagement before Villars could render his position impregnable; but they also agreed to wait for reinforcements from Tournai which did not arrive until the following night, thus giving the French further opportunity to prepare their defences. Notwithstanding the difficulties of the attack, however, the Allied generals did not shrink from their original determination. The subsequent Battle of Malplaquet, fought on 11 September 1709, was the bloodiest engagement of the war. On the left flank, the Prince of Orange led his Dutch infantry in desperate charges only to have it cut to pieces; on the other flank, Eugene attacked and suffered almost as severely. But sustained pressure on his extremities forced Villars to weaken his centre, thus enabling Marlborough to breakthrough and claim victory. Villars was unable to save Mons, which subsequently capitulated on 21 October, but his resolute defence at Malplaquet—inflicting up to 25% casualties on the Allies—may have saved France from destruction.

In August 1709 Eugene's chief political opponent and critic in Vienna, Prince Salm, retired as court chamberlain. Eugene and Wratislaw were now the undisputed leaders of the Austrian government: all major departments of state were in their hands or those of their political allies. However, another attempt at a negotiated settlement at Geertruidenberg in April 1710 failed, largely because the English Whigs still felt strong enough to refuse concessions, while Louis XIV saw little reason to accept what he had refused the previous year. Eugene and Marlborough could not be accused of wrecking the negotiations, but neither showed regret at the breakdown of the talks. There was no alternative but to continue the war, and in June the Allied commanders captured Douai. This success was followed by a series of minor sieges, and by the close of 1710 the Allies had cleared much of France's protective ring of fortresses. Yet there had been no final, decisive breakthrough, and this was to be the last year that Eugene and Marlborough would work together.

Following the death of Joseph I on 17 April 1711 his brother, Charles, the pretender to the Spanish throne, became emperor. In England the new Tory government (the 'peace party' who had deposed the Whigs in October 1710) declared their unwillingness to see Charles VI become Emperor as well as King of Spain, and had already begun secret negotiations with the French. In January 1712 Eugene arrived in England hoping to divert the government away from its peace policy, but despite the social success the visit was a political failure: Queen Anne and her ministers remained determined to end the war regardless of the Allies. Eugene had also arrived too late to save Marlborough who, seen by the Tories as the main obstacle to peace, had already been dismissed on charges of embezzlement. Elsewhere, however, the Austrians had made some progress—the Hungarian revolt had finally came to end. Although Eugene would have preferred to crush the rebels the Emperor had offered lenient conditions, leading to the signing of the Treaty of Szatmár on 30 April 1711.

Hoping to influence public opinion in England and force the French into making substantial concessions, Eugene prepared for a major campaign. However, on 21 May 1712—when the Tories felt they had secured favourable terms with their unilateral talks with the French—the Duke of Ormonde (Marlborough's successor) received the so-called 'restraining orders', forbidding him to take part in any military action. Eugene took the fortress of Le Quesnoy in early July, before besieging Landrecies, but Villars, taking advantage of Allied disunity, outmanoeuvred Eugene and defeated the Earl of Albermarle's Dutch garrison at Denain on 24 July. The French followed the victory by seizing the Allies' main supply magazine at Marchiennes, before reversing their earlier losses at Douai, Le Quesnoy and Bouchain. In one summer the whole forward Allied position laboriously built up over the years to act as the springboard into France had been precipitously abandoned.

With the death in December of his friend and close political ally, Count Wratislaw, Eugene became undisputed 'first minister' in Vienna. His position was built on his military successes, but his actual power was expressed through his role as president of the war council, and as "de facto" president of the conference which dealt with foreign policy. In this position of influence Eugene took the lead in pressing Charles VI towards peace. The government had come to accept that further war in the Netherlands or Spain was impossible without the aid of the Maritime Powers; yet the Emperor, still hoping that somehow he could place himself on the throne in Spain, refused to make peace at the Utrecht conference along with the other Allies. Reluctantly, Eugene prepared for another campaign, but lacking troops, finance, and supplies his prospects in 1713 were poor. Villars, with superior numbers, was able to keep Eugene guessing as to his true intent. Through successful feints and stratagems Landau fell to the French commander in August, followed in November by Freiburg. Eugene was reluctant to carry on the war, and wrote to the Emperor in June that a bad peace would be better than being 'ruined equally by friend and foe'. With Austrian finances exhausted and the German states reluctant to continue the war, Charles VI was compelled to enter into negotiations. Eugene and Villars (who had been old friends since the Turkish campaigns of the 1680s) initiated talks on 26 November. Eugene proved an astute and determined negotiator, and gained favourable terms by the Treaty of Rastatt signed on 7 March 1714 and the Treaty of Baden signed on 7 September 1714. Despite the failed campaign in 1713 the Prince was able to declare that, "in spite of the military superiority of our enemies and the defection of our Allies, the conditions of peace will be more advantageous and more glorious than those we would have obtained at Utrecht."

Eugene's main reason for desiring peace in the west was the growing danger posed by the Turks in the east. Turkish military ambitions had revived after 1711 when they had mauled Peter the Great's army on the river Pruth: in December 1714 Sultan Ahmed III's forces attacked the Venetians in the Morea. To Vienna it was clear that the Turks intended to attack Hungary and undo the whole Karlowitz settlement of 1699. After the Porte rejected an offer of mediation in April 1716, Charles VI despatched Eugene to Hungary to lead his relatively small but professional army. Of all Eugene's wars this was the one in which he exercised most direct control; it was also a war which, for the most part, Austria fought and won on her own.
Eugene left Vienna in early June 1716 with a field army of between 80,000–90,000 men. By early August 1716 the Ottoman Turks, some 200,000 men under the sultan's son-in-law, the Grand Vizier Damat Ali Pasha, were marching from Belgrade towards Eugene's position west of the fortress of Petrovaradin on the north bank of the Danube. The Grand Vizier had intended to seize the fortress; but Eugene gave him no chance to do so. After resisting calls for caution and forgoing a council of war, the Prince decided to attack immediately on the morning of 5 August with approximately 70,000 men. The Turkish janissaries had some initial success, but after an Imperial cavalry attack on their flank, Ali Pasha's forces fell into confusion. Although the Imperials lost almost 5,000 dead or wounded, the Turks, who retreated in disorder to Belgrade, seem to have lost double that amount, including the Grand Vizier himself who had entered the mêlée and subsequently died of his wounds.

Eugene proceeded to take the Banat fortress of Timișoara (Temeswar in German) in mid-October 1716 (thus ending 164 years of Turkish rule), before turning his attention to the next campaign and to what he considered the main goal of the war, Belgrade. Situated at the confluence of the Rivers Danube and Sava, Belgrade held a garrison of 30,000 men under Mustapha Pasha. Imperial troops besieged the place in mid-June 1717, and by the end of July large parts of the city had been destroyed by artillery fire. By the first days of August, however, a huge Turkish field army (150,000–200,000 strong), under the new Grand Vizier, Halil Pasha, had arrived on the plateau east of the city to relieve the garrison. News spread through Europe of Eugene's imminent destruction; but he had no intention of lifting the siege. With his men suffering from dysentery, and continuous bombardment from the plateau, Eugene, aware that a decisive victory alone could extricate his army, decided to attack the relief force. On the morning of 16 August 40,000 Imperial troops marched through the fog, caught the Turks unaware, and routed Halil Pasha's army; a week later Belgrade surrendered, effectively bringing an end to the war. The victory was the crowning point of Eugene's military career and had confirmed him as the leading European general. His ability to snatch victory at the moment of defeat had shown the Prince at his best.

The principal objectives of the war had been achieved: the task Eugene had begun at Zenta was complete, and the Karlowitz settlement secured. By the terms of the Treaty of Passarowitz, signed on 21 July 1718, the Turks surrendered the Banat of Temeswar, along with Belgrade and most of Serbia, although they regained the Morea from the Venetians. The war had dispelled the immediate Turkish threat to Hungary, and was a triumph for the Empire and for Eugene personally.

While Eugene fought the Turks in the east, unresolved issues following the Utrecht/Rastatt settlements led to hostilities between the Emperor and Philip V of Spain in the west. Charles VI had refused to recognise Philip V as King of Spain, a title which he himself claimed; in return, Philip V had refused to renounce his claims to Naples, Milan, and the Netherlands, all of which had transferred to the House of Austria following the Spanish Succession war. Philip V was roused by his influential wife, Elisabeth Farnese, daughter of the Hereditary Prince of Parma, who personally held dynastic claims in the name of her son, Don Charles, to the duchies of Tuscany, Parma and Piacenza. Representatives from a newly formed Anglo-French alliance—who were desirous of European peace for their own dynastic securities and trade opportunities—called on both parties to recognise each other's sovereignty. Yet Philip V remained intractable, and on 22 August 1717 his chief minister, Alberoni, effected the invasion of Austrian Sardinia in what seemed like the beginning of the reconquest of Spain's former Italian empire.

Eugene returned to Vienna from his recent victory at Belgrade (before the conclusion of the Turkish war) determined to prevent an escalation of the conflict, complaining that, "two wars cannot be waged with one army"; only reluctantly did the Prince release some troops from the Balkans for the Italian campaign. Rejecting all diplomatic overtures Philip V unleashed another assault in June 1718, this time against Savoyard Sicily as a preliminary to attacking the Italian mainland. Realising that only the British fleet could prevent further Spanish landings, and that pro-Spanish groups in France might push the regent, Duke of Orléans, into war against Austria, Charles VI had no option but to sign the Quadruple Alliance on 2 August 1718, and formally renounce his claim to Spain. Despite the Spanish fleet's destruction off Cape Passaro, Philip V and Elisabeth remained resolute, and rejected the treaty.

Although Eugene could have gone south after the conclusion of the Turkish war, he chose instead to conduct operations from Vienna; but Austria's military effort in Sicily proved derisory, and Eugene's chosen commanders, Zum Jungen, and later Count Mercy, performed poorly. It was only from pressure exerted by the French army advancing into the Basque provinces of northern Spain in April 1719, and the British Navy's attacks on the Spanish fleet and shipping, that compelled Philip V and Elisabeth to dismiss Alberoni and join the Quadruple Alliance on 25 January 1720. Nevertheless, the Spanish attacks had strained Charles VI's government, causing tension between the Emperor and his Spanish Council on the one hand, and the conference, headed by Eugene, on the other. Despite Charles VI's own personal ambitions in the Mediterranean it was clear to the Emperor that Eugene had put the safeguarding of his conquests in Hungary before everything else, and that military failure in Sicily also had to rest on Eugene. Consequently, the Prince's influence over the Emperor declined considerably.

Eugene had become governor of the Southern Netherlands—then the Austrian Netherlands—in June 1716, but he was an absent ruler, directing policy from Vienna through his chosen representative the Marquis of Prié. Prié proved unpopular with the local population and the guilds who, following the Barrier Treaty of 1715, were obliged to meet the financial demands of the administration and the Dutch barrier garrisons; with Eugene's backing and encouragement, civil disturbances in Antwerp and Brussels were forcibly suppressed. After displeasing the Emperor over his initial opposition to the formation of the Ostend Company, Prié also lost the support of the native nobility from within his own council of state in Brussels, particularly from the Marquis de Mérode-Westerloo. One of Eugene's former favourites, General Bonneval, also joined the noblemen in opposition to Prié, further undermining the Prince. When Prié's position became untenable, Eugene felt compelled to resign his post as governor of the Southern Netherlands on 16 November 1724. As compensation, Charles VI conferred on him the honorary position as vicar-general of Italy, worth 140,000 gulden a year, and an estate at Siebenbrunn in Lower Austria said to be worth double that amount. But his resignation distressed him, and to compound his concerns Eugene caught a severe bout of influenza that Christmas, marking the beginning of permanent bronchitis and acute infections every winter for the remaining twelve years of his life.

The 1720s saw rapidly changing alliances between the European powers and almost constant diplomatic confrontation, largely over unsolved issues regarding the Quadruple Alliance. The Emperor and the Spanish King continued to use each other's titles, and Charles VI still refused to remove the remaining legal obstacles to Don Charles' eventual succession to the duchies of Parma and Tuscany. Yet in a surprise move Spain and Austria moved closer with the signing of the Treaty of Vienna in April/May 1725. In response Britain, France, and Prussia joined together in the Alliance of Hanover to counter the danger to Europe of an Austro-Spanish hegemony. For the next three years there was the continual threat of war between the Hanover Treaty powers and the Austro-Spanish bloc.

From 1726 Eugene gradually began to regain his political influence. With his many contacts throughout Europe Eugene, backed by Gundaker Starhemberg and Count Schönborn, the Imperial vice-chancellor, managed to secure powerful allies and strengthen the Emperor's position—his skill in managing the vast secret diplomatic network over the coming years was the main reason why Charles VI once again came to depend upon him. In August 1726 Russia acceded to the Austro-Spanish alliance, and in October Frederick William of Prussia followed suit by defecting from the Allies with the signing of a mutual defensive treaty with the Emperor. Despite the conclusion of the brief Anglo-Spanish conflict, war between the European powers persisted throughout 1727–28. However, in 1729 Elisabeth Farnese abandoned the Austro-Spanish alliance. Realizing that Charles VI could not be drawn into the marriage pact she wanted, Elisabeth concluded that the best way to secure her son's succession to Parma and Tuscany now lay with Britain and France. To Eugene it was 'an event that which is seldom to be found in history'. Following the Prince's determined lead to resist all pressure, Charles VI sent troops into Italy to prevent the entry of Spanish garrisons into the contested duchies. By the beginning of 1730 Eugene, who had remained bellicose throughout the whole period, was again in control of Austrian policy.
In Britain there now emerged a new political re-alignment as the Anglo-French "entente" became increasingly defunct. Believing that a resurgent France now posed the greatest danger to their security British ministers, headed by Robert Walpole, moved to reform the Anglo-Austrian alliance, leading to the signing of the Second Treaty of Vienna on 16 March 1731. Eugene had been the Austrian minister most responsible for the alliance, believing once again it would provide security against France and Spain. The treaty compelled Charles VI to sacrifice the Ostend Company and accept, unequivocally, the accession of Don Charles to Parma and Tuscany. In return King George II as King of Great Britain and Elector of Hanover guaranteed the Pragmatic Sanction, the device to secure the rights of the Emperor's daughter, Maria Theresa, to the entire Habsburg inheritance. It was largely through Eugene's diplomacy that in January 1732 the Imperial diet also guaranteed the Pragmatic Sanction which, together with the Treaties with Britain, Russia, and Prussia, marked the culmination of the Prince's diplomacy. But the Treaty of Vienna had infuriated the court of King Louis XV: the French had been ignored and the Pragmatic Sanction guaranteed, thus increasing Habsburg influence and confirming Austria's vast territorial size. The Emperor also intended Maria Theresa to marry Francis Stephen of Lorraine which would present an unacceptable threat on France's border. By the beginning of 1733 the French army was ready for war: all that was needed was the excuse.

In 1733 the Polish King and Elector of Saxony, Augustus the Strong, died. There were two candidates for his successor: first, Stanisław Leszczyński, the father-in-law of Louis XV; second, the Elector of Saxony's son, Augustus, supported by Russia, Austria, and Prussia. The Polish succession had afforded Louis XV's chief minister, Fleury, the opportunity to attack Austria and take Lorraine from Francis Stephen. In order to gain Spanish support France backed the succession of Elisabeth Farnese's sons to further Italian lands.

Eugene entered the War of the Polish Succession as President of the Imperial War Council and commander-in-chief of the army, but he was severely handicapped by the quality of his troops and the shortage of funds; now in his seventies, the Prince was also burdened by rapidly declining physical and mental powers. France declared war on Austria on 10 October 1733, but without the funds from the Maritime Powers—who, despite the Vienna treaty, remained neutral throughout the war—Austria could not hire the necessary troops to wage an offensive campaign. "The danger to the monarchy," wrote Eugene to the Emperor in October, "cannot be exaggerated". By the end of the year Franco-Spanish forces had seized Lorraine and Milan; by early 1734 Spanish troops had taken Sicily.

Eugene took command on the Rhine in April 1734, but vastly outnumbered he was forced onto the defensive. In June Eugene set out to relieve Philippsburg, yet his former drive and energy was now gone. Accompanying Eugene was a young Frederick the Great, sent by his father to learn the art of war. Frederick gained considerable knowledge from Eugene, recalling in later life his great debt to his Austrian mentor, but the Prussian prince was aghast at Eugene's condition, writing later, "his body was still there but his soul had gone." Eugene conducted another cautious campaign in 1735, once again pursuing a sensible defensive strategy on limited resources; but his short-term memory was by now practically non-existent, and his political influence disappeared completely—Gundaker Starhemberg and Johann Christoph von Bartenstein now dominated the conference in his place. However, fortunately for Charles VI Fleury was determined to limit the scope of the war, and in October 1735 he granted generous peace preliminaries to the Emperor.

During the last 20 years of his life Eugene was particularly close to Countess Eleonora Batthyány, daughter of Count Theodor von Strattman.
Much about their acquaintance remains speculative (Eugene never mentions her in any of his surviving letters), and there is certainly no suggestion of a sexual relationship, but although they lived apart most foreign diplomats regarded Eleonora as his "official lover". Eugene and Eleonora were constant companions, meeting for dinner, receptions and card games almost every day till his death. But their surviving correspondence does not indicate any real intimacy in the relationship. Eugene's other friends such as the papal nuncio, Passionei, made up for the family he still lacked.

For his only surviving nephew, Emmanuel, the son of his brother Louis Thomas, Eugene arranged a marriage with one of the daughters of Prince Liechtenstein, but Emmanuel died of smallpox in 1729. With the death of Emmanuel's son in 1734, no close male relatives remained to succeed the Prince. His closest relative, therefore, was Louis Thomas's unmarried daughter, Princess Maria Anna Victoria of Savoy, whom Eugene had never met and, as he had heard nothing but bad of her, made no effort to do so.

Eugene returned to Vienna from the War of the Polish Succession in October 1735, weak and feeble; when Maria Theresa and Francis Stephen married in February 1736 Eugene was too ill to attend. After playing cards at Countess Batthyány's on the evening of 20 April he returned to his bed at the Stadtpalais. When his servants arrived to wake him the next morning, 21 April 1736, they found Prince Eugene dead after choking from phlegm in his throat, presumably after suffering from pneumonia. Eugene's heart was buried with those of others of his family in Turin. His remains were carried in a long procession to St. Stephen's Cathedral, where the body was interred in the "Kreuzkapelle".

Countess Batthyány expressed in a letter dated 23 December 1720, that at the "Kreuzkapelle" a solemn requiem would be held annually. She dedicated for this purpose two thousand guilders.

Despite being one of the richest and most celebrated men of his age, Eugene never married and the suggestion is that he was predominantly homosexual. History knows little of his life before 1683. In his early boyhood in Paris "he belonged to a small, effeminate set that included such unabashed perverts as the young abbé de Choisy who was invariably dressed as a girl" wrote historian Nicholas Henderson. The Duchess of Orléans, who had known Eugene from those days, would later write to her aunt, Princess Sophia of Hanover, describing Eugene's antics with lackeys and pages. He was "a vulgar whore" along with the Prince of Turenne, and "often played the woman with young people" with the nickname of 'Madame Simone' or 'Madam l'Ancienne'. He preferred a "couple of fine page boys" to any woman, and was refused an ecclesiastical benefice due to his "depravity".

Of related interest is a popular soldier's song which parodied an imaginary voyage by Eugene and the Marquis de la Moussaye on the Rhine. A storm breaks and the general fears the worst, but the Marquis consoles him: "Our lives are safe/ For we are sodomites/ Destined to perish only by fire/ We shall land." Riess suggests that a comment made by Johann Matthias von der Schulenburg in 1709, who had served under Eugene that the prince enjoyed "la petite débauche et la p[ine] au-delà de tout," is another testament to sodomy.

Eugene's rewards for his victories, his share of booty, his revenues from his abbeys in Savoy, and a steady income from his Imperial offices and governorships, enabled him to contribute to the landscape of Baroque architecture. Eugene spent most of his life in Vienna at his Winter Palace, the Stadtpalais, built by Fischer von Erlach. The palace acted as his official residence and home, but for reasons that remain speculative the Prince's association with Fischer ended before the building was complete, favouring instead Johann Lukas von Hildebrandt as his chief architect. Eugene first employed Hildebrandt to finish the Stadtpalais before commissioning him to prepare plans for a palace (Savoy Castle) on his Danubian island at Ráckeve. Begun in 1701 the single-story building took twenty years to complete; yet, probably because of the Rákóczi revolt, the Prince seems to have visited it only once—after the siege of Belgrade in 1717.

Of more importance was the grandiose complex of the two Belvedere palaces in Vienna. The single-storey Lower Belvedere, with its exotic gardens and zoo, was completed in 1716. The Upper Belvedere, completed between 1720 and 1722, is a more substantial building; with sparkling white stucco walls and copper roof it became a wonder of Europe. Eugene and Hildebrandt also converted an existing structure on his Marchfeld estate into a country seat, the Schlosshof, situated between the Rivers Danube and Morava. The building, completed in 1729, was far less elaborate than his other projects but it was strong enough to serve as a fortress in case of need. Eugene spent much of his spare time there in his last years accommodating large hunting parties.

In the years following the Peace of Rastatt Eugene became acquainted with a large number of scholarly men. Given his position and responsiveness they were keen to meet him: few could exist without patronage and this was probably the main reason for Gottfried Leibniz's association with him in 1714. Eugene also befriended the French writer Jean-Baptiste Rousseau who, by 1716, was receiving financial support from Eugene. Rousseau stayed on attached to the Prince's household, probably helping in the library, until he left for the Netherlands in 1722. Another acquaintance, Montesquieu, already famous for his "Persian Letters" when he arrived in Vienna in 1728, favourably recalled his time spent at the Prince's table. Nevertheless, Eugene had no literary pretensions of his own, and was not tempted like Maurice de Saxe or Marshal Villars to write his memoirs or books on the art of war. He did, however, become a collector on the grandest scale: his picture galleries were filled with 16th- and 17th-century Italian, Dutch and Flemish art; his library at the Stadtpalais crammed with over 15,000 books, 237 manuscripts as well as a huge collection of prints (of particular interest were books on natural history and geography). "It is hardly believable," wrote Rousseau, "that a man who carries on his shoulders the burden of almost all the affairs of Europe … should find as much time to read as though he had nothing else to do." At Eugene's death his possessions and estates, except those in Hungary which the crown reclaimed, went to his niece, Princess Maria Anna Victoria, who at once decided to sell everything. The artwork was bought by Charles Emmanuel III of Sardinia. Eugene's library, prints and drawings were purchased by the Emperor in 1737 and have since passed into Austrian national collections.

Napoleon considered Eugene one of the seven greatest commanders of history. Although later military critics have disagreed with that assessment, Eugene was undoubtedly the greatest Austrian general. He was no military innovator, but he had the ability to make an inadequate system work. He was equally adept as an organizer, strategist, and tactician, believing in the primacy of battle and his ability to seize the opportune moment to launch a successful attack. "The important thing," wrote Maurice de Saxe in his "Reveries", "is to see the opportunity and to know how to use it. Prince Eugene possessed this quality which is the greatest in the art of war and which is the test of the most elevated genius." This fluidity was key to his battlefield successes in Italy and in his wars against the Turks. Nevertheless, in the Low Countries, particularly after the battle of Oudenarde in 1708, Eugene, like his cousin Louis of Baden, tended to play safe and become bogged down in a conservative strategy of sieges and defending supply lines. After the attempt on Toulon in 1707, he also became very wary of combined land/sea operations. To historian Derek McKay, however, the main criticism of him as a general is his legacy—he left no school of officers nor an army able to function without him.

Eugene was a disciplinarian—when ordinary soldiers disobeyed orders he was prepared to shoot them himself—but he rejected blind brutality, writing "you should only be harsh when, as often happens, kindness proves useless". On the battlefield Eugene demanded courage in his subordinates, and expected his men to fight where and when he wanted; his criteria for promotion were based primarily on obedience to orders and courage on the battlefield rather than social position. On the whole his men responded because he was willing to push himself as hard as them. However, his position as President of the Imperial War Council proved less successful. Following the long period of peace after the Austro-Turkish War, the idea of creating a separate field army or providing garrison troops with effective training for them to be turned into such an army quickly was never considered by Eugene. By the time of the War of the Polish Succession, therefore, the Austrians were outclassed by a better prepared French force. For this Eugene was largely to blame—in his view (unlike the drilling and manoeuvres carried out by the Prussians which to Eugene seemed irrelevant to real warfare) the time to create actual fighting men was when war came. But although Frederick the Great had been struck by the muddle of the Austrian army and its poor organisation during the Polish Succession war, he later amended his initial harsh judgements. "If I understand anything of my trade," commented Frederick in 1758, "especially in the more difficult aspects, I owe that advantage to Prince Eugene. From him I learnt to hold grand objectives constantly in view, and direct all my resources to those ends." To historian Christopher Duffy it was this awareness of the 'grand strategy' that was Eugene's legacy to Frederick.

To his responsibilities Eugene attached his own personal values—physical courage, loyalty to his sovereign, honesty, self-control in all things—and he expected these qualities from his commanders. Eugene's approach was dictatorial, but he was willing to co-operate with someone he regarded as his equal, such as Baden or Marlborough. Yet the contrast to his co-commander of the Spanish Succession war were stark. "Marlborough," wrote Churchill, "was the model husband and father, concerned with building up a home, founding a family, and gathering a fortune to sustain it"; whereas Eugene, the bachelor, was "disdainful of money, content with his bright sword and his lifelong animosities against Louis XIV". The result was an austere figure, inspiring respect and admiration rather than affection. The huge equestrian statue in the centre of Vienna commemorates Eugene's achievements. It is inscribed on one side, 'To the wise counsellor of three Emperors', and on the other, 'To the glorious conqueror of Austria's enemies'.

Several ships have been named in Eugene's honour: , an Austrian battleship; an Austrian central battery ship with the same name; , a Royal Navy monitor; , an Italian light cruiser and the (later USS "Prinz Eugen"), a World War II heavy cruiser. 

The 7th SS Volunteer Mountain Division Prinz Eugen, a German mountain infantry division of the Waffen-SS formed in 1941 from "Volksdeutsche" volunteers and conscripts from the Banat, Independent State of Croatia, Hungary and Romania, was named after him.





 


</doc>
<doc id="9683" url="https://en.wikipedia.org/wiki?curid=9683" title="Emanuel Leutze">
Emanuel Leutze

Emanuel Gottlieb Leutze (May 24, 1816July 18, 1868) was a German American history painter best known for his 1851 painting "Washington Crossing the Delaware". He is associated with the Düsseldorf school of painting.

Leutze was born in Schwäbisch Gmünd, Württemberg, Germany, and was brought to the United States as a child in 1825. His parents settled first in Fredericksburg, Virginia, and then at Philadelphia. The first development of his artistic talent occurred while he was attending the sickbed of his father, when he attempted drawing to occupy the long hours of waiting. His father died in 1831. At 14, he was painting portraits for $5 apiece. Through such work, he supported himself after the death of his father. In 1834, he received his first instruction in art at the classes of John Rubens Smith, a portrait painter in Philadelphia. He soon became skilled, and promoted a plan for publishing, in Washington, portraits of eminent American statesmen; however, he was met with slight encouragement.

In 1840, one of his paintings attracted attention and gave him several orders, which enabled him to attend the Kunstakademie Düsseldorf in his native Germany. Due to his anti-academic attitude, he studied only one year at the academy, in the class of Director Schadow. Leutze was mostly influenced by the painter Karl Friedrich Lessing. In 1842 he went to Munich, studying the works of Cornelius and Kaulbach, and, while there, finished his "Columbus before the Queen". The following year he visited Venice and Rome, making studies from Titian and Michelangelo. His first work, "Columbus before the Council of Salamanca" (1841) was purchased by the Düsseldorf Art Union. A companion picture, "Columbus in Chains", procured him the gold medal of the Brussels Art Exhibition, and was subsequently purchased by the Art Union in New York; it was the basis of the 1893 $2 Columbian Issue stamp. In 1845, after a tour in Italy, he returned to Düsseldorf, marrying Juliane Lottner and making his home there for 14 years.

During his years in Düsseldorf, he was a resource for visiting Americans: he found them places to live and work, provided introductions, and gave them emotional and even financial support. For many years, he was the president of the Düsseldorf Artists' Association; in 1848, he was an early promoter of the "Malkasten" art association; and in 1857, he led the call for a gathering of artists which originated the founding of the Allgemeine deutsche Kunstgenossenschaft.

A strong supporter of Europe's Revolutions of 1848, Leutze decided to paint an image that would encourage Europe's liberal reformers with the example of the American Revolution. Using American tourists and art students as models and assistants, Leutze finished a first version of "Washington Crossing the Delaware" in 1850. Just after it was completed, the first version was damaged by fire in his studio, subsequently restored, and acquired by the Kunsthalle Bremen. On September 5, 1942, during World War II, it was destroyed in a bombing raid by the Allied forces. The second painting, a replica of the first, only larger, was ordered in 1850 by the Parisian art trader Adolphe Goupil for his New York branch and placed on exhibition on Broadway in October 1851. It is now owned by the Metropolitan Museum of Art in New York. In 1854, Leutze finished his depiction of the Battle of Monmouth, "Washington rallying the troops at Monmouth," commissioned by an important patron, the banker David Leavitt of New York City and Great Barrington, Massachusetts.

In 1859, Leutze returned to the United States and opened a studio in New York City. He divided his time between New York City and Washington, D.C. In 1859, he painted a portrait of Chief Justice Roger Brooke Taney, which hangs in the Harvard Law School. In a 1992 opinion, Justice Antonin Scalia described the portrait of Taney, made two years after Taney's infamous decision in "Dred Scott v. Sandford", as showing Taney "in black, sitting in a shadowed red armchair, left hand resting upon a pad of paper in his lap, right hand hanging limply, almost lifelessly, beside the inner arm of the chair. He sits facing the viewer and staring straight out. There seems to be on his face, and in his deep-set eyes, an expression of profound sadness and disillusionment."

Leutze also executed other portraits, including one of fellow painter William Morris Hunt. That portrait was owned by Hunt's brother Leavitt Hunt, a New York attorney and sometime Vermont resident, and was shown at an exhibition devoted to William Morris Hunt's work at the Museum of Fine Arts, Boston in 1878.

In 1860 Leutze was commissioned by the U.S. Congress to decorate a stairway in the Capitol Building in Washington, DC, for which he painted a large composition, "Westward the Course of Empire Takes Its Way", which is also commonly known as "Westward Ho!".

Late in life, he became a member of the National Academy of Design. He was also a member of the Union League Club of New York, which has a number of his paintings. At age 52, he died in Washington, D.C. of heat stroke. He was interred at Glenwood Cemetery. At the time of his death, a painting, "The Emancipation of the Slaves", was in preparation.

Leutze's portraits are known for their artistic quality and their patriotic romanticism. "Washington Crossing the Delaware" firmly ranks among the American national iconography.
Additional References:



</doc>
<doc id="9684" url="https://en.wikipedia.org/wiki?curid=9684" title="Erasmus Alberus">
Erasmus Alberus

Erasmus Alberus (c. 15005 May 1553) was a German humanist, Lutheran reformer, and poet.

He was born in the village of Bruchenbrücken (now part of Friedberg, Hesse) about the year 1500. Although his father Tilemann Alber was a schoolmaster, his early education was neglected.
Ultimately in 1518 he found his way to the University of Wittenberg, where he studied theology. He had the good fortune to attract the attention of Martin Luther and Philipp Melanchthon, and subsequently became one of Luther's most active helpers in the Protestant Reformation.

Not only did he fight for the Protestant cause as a preacher and theologian, but he was almost the only member of Luther's party who was able to confront the Roman Catholics with the weapon of literary satire. In 1542 he published a prose satire to which Luther wrote the preface, "Der Barfusser Monche Eulenspiegel und Alkoran," a parodic adaptation of the "Liber conformitatum" of the Franciscan Bartolommeo Rinonico of Pisa, in which the Franciscan order is held up to ridicule. This drew reactions from Catholic scholars such as Henricus Sedulius, who published the "Apologeticus aduersus Alcoranum Franciscanorum, pro Libro Conformitatum," which criticized Alberus' arguments in this satire. 

Of higher literary value is the didactic and satirical "Buch von der Tugend und Weisheit" (1550), a collection of forty-nine fables in which Alberus embodies his views on the relations of Church and State. His satire is incisive, but in a scholarly and humanistic way; it does not appeal to popular passions with the fierce directness which enabled the master of Catholic satire, Thomas Murner, to inflict such telling blows.

Several of Alberus's hymns, all of which show the influence of his master Luther, have been retained in the German Protestant hymnal.

After Luther's death, Alberus was for a time a deacon in Wittenberg; he became involved, however, in the political conflicts of the time, and was in Magdeburg in 1550–1551, while that town was besieged by Maurice, Elector of Saxony. In 1552 he was appointed Generalsuperintendent at Neubrandenburg in Mecklenburg, where he died on 5 May 1553.


Attribution:


</doc>
<doc id="9685" url="https://en.wikipedia.org/wiki?curid=9685" title="Earley parser">
Earley parser

In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in an abbreviated, more legible, form in a journal).

Earley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case formula_1, where "n" is the length of the parsed string, quadratic time for unambiguous grammars formula_2, and linear time for all deterministic context-free grammars. It performs particularly well when the rules are written left-recursively.

The following algorithm describes the Earley recogniser. The recogniser can be easily modified to create a parse tree as it recognises, and in that way can be turned into a parser.

In the following descriptions, α, β, and γ represent any string of terminals/nonterminals (including the empty string), X and Y represent single nonterminals, and "a" represents a terminal symbol.

Earley's algorithm is a top-down dynamic programming algorithm. In the following, we use Earley's dot notation: given a production X → αβ, the notation X → α • β represents a condition in which α has already been parsed and β is expected.

Input position 0 is the position prior to input. Input position "n" is the position after accepting the "n"th token. (Informally, input positions can be thought of as locations at token boundaries.) For every input position, the parser generates a "state set". Each state is a tuple (X → α • β, "i"), consisting of


The state set at input position "k" is called S("k"). The parser is seeded with S(0) consisting of only the top-level rule. The parser then repeatedly executes three operations: "prediction", "scanning", and "completion".


Duplicate states are not added to the state set, only new ones. These three operations are repeated until no new states can be added to the set. The set is generally implemented as a queue of states to process, with the operation to be performed depending on what kind of state it is.

The algorithm accepts if (X → γ •, 0) ends up in S("n"), where (X → γ) is the top level-rule and "n" the input length, otherwise it rejects.

Adapted from Speech and Language Processing by Daniel Jurafsky and James H. Martin, 
DECLARE ARRAY S;

function INIT(words)

function EARLEY-PARSE(words, grammar)

procedure PREDICTOR((A → α•Bβ, j), k, grammar)

procedure SCANNER((A → α•aβ, j), k, words)

procedure COMPLETER((B → γ•, x), k)
Consider the following simple grammar for arithmetic expressions:

<P> ::= <S> # the start rule
<S> ::= <S> "+" <M> | <M>
<M> ::= <M> "*" <T> | <T>
<T> ::= "1" | "2" | "3" | "4"

With the input:

This is the sequence of state sets:

The state (P → S •, 0) represents a completed parse. This state also appears in S(3) and S(1), which are complete sentences.

Earley's dissertation briefly describes an algorithm for constructing parse trees by adding a set of pointers from each non-terminal in an Earley item back to the items that caused it to be recognized. But Tomita noticed that this does not take into account the relations between symbols, so if we consider the grammar S → SS | b and the string bbb, it only notes that each S can match one or two b's, and thus produces spurious derivations for bb and bbbb as well as the two correct derivations for bbb.

Another method is to build the parse forest as you go, augmenting each Earley item with a pointer to a shared packed parse forest (SPPF) node labelled with a triple (s, i, j) where s is a symbol or an LR(0) item (production rule with dot), and i and j give the section of the input string derived by this node. A node's contents are either a pair of child pointers giving a single derivation, or a list of "packed" nodes each containing a pair of pointers and representing one derivation. SPPF nodes are unique (there is only one with a given label), but may contain more than one derivation for ambiguous parses. So even if an operation does not add an Earley item (because it already exists), it may still add a derivation to the item's parse forest.


SPPF nodes are never labeled with a completed LR(0) item: instead they are labelled with the symbol that is produced so that all derivations are combined under one node regardless of which alternative production they come from.















</doc>
<doc id="9686" url="https://en.wikipedia.org/wiki?curid=9686" title="Ethiopian cuisine">
Ethiopian cuisine

Ethiopian cuisine () characteristically consists of vegetable and often very spicy meat dishes. This is usually in the form of "wat," a thick stew, served atop "injera", a large sourdough flatbread, which is about in diameter and made out of fermented teff flour. Ethiopians eat most of the time with their right hands, using pieces of "injera" to pick up bites of entrées and side dishes.

The Ethiopian Orthodox Tewahedo Church prescribes a number of fasting periods ("tsom", "ṣōm") from any kind of animal products (including dairy products and eggs) on Wednesdays, Fridays, and the entire Lenten season, so Ethiopian cuisine contains many dishes that are vegan.

A typical dish consists of injera accompanied by a spicy stew, which frequently includes beef, lamb, vegetables and various types of legumes, such as lentils. The cuisines of the Southern Nations, Nationalities and People's Region and the Sidama people also make use of the false banana plant ("enset", Ge'ez: እንሰት "ïnset"), a type of ensete. The plant is pulverized and fermented to make various foods, including a bread-like food called "qocho" or "kocho" (Ge'ez: ቆጮ "ḳōč̣ō"), which is eaten with kitfo. The root of this plant may be powdered and prepared as a hot drink called "bulla" (Ge'ez: ቡላ "būlā"), which is often given to those who are tired or ill. Another typical Gurage preparation is coffee with butter ("kebbeh"). "Kita" herb bread is also baked.

Due in part to the brief Italian occupation, pasta is popular and frequently available throughout Ethiopia, including rural areas. Coffee is also a large part of Ethiopian culture and cuisine. After every meal, a coffee ceremony is enacted and coffee is served.

Ethiopian Orthodox Christians, Ethiopian Jews and Ethiopian Muslims avoid eating pork or shellfish, for religious reasons. Pork is considered unclean in Ethiopian Orthodox Christianity, Judaism and Islam. Many Ethiopians abstain from eating certain meats, and mostly eat vegetarian and vegan foods.

"Berbere", a combination of powdered chili pepper and other spices (somewhat analogous to Southwestern American chili powder), is an important ingredient used in many dishes. Also essential is "niter kibbeh", a clarified butter infused with ginger, garlic, and several spices.

"Mitmita" (, ) is a powdered seasoning mix used in Ethiopian cuisine. It is orange-red in color and contains ground birdseye chili peppers (piri piri), cardamom seed, cloves and salt. It occasionally has other spices including cinnamon, cumin and ginger.

In their adherence to strict fasting, Ethiopian cooks have developed a rich array of cooking oil sources—besides sesame and safflower—for use as a substitute for animal fats which are forbidden during fasting periods. Ethiopian cuisine also uses "nug" (also spelled "noog", also known as "niger seed").

"Wat" begins with a large amount of chopped red onion, which is simmered or sauteed in a pot. Once the onions have softened, "niter kebbeh" (or, in the case of vegan dishes, vegetable oil) is added. Following this, "berbere" is added to make a spicy "keiy wat" or "keyyih tsebhi". Turmeric is used instead of "berbere" for a milder "alicha wat" or both spices are omitted when making vegetable stews, such as atkilt wat. Meat such as beef ("ሥጋ", "səga"), chicken ("ዶሮ", "doro" or "derho"), fish ("ዓሣ", "asa"), goat or lamb ("በግ", "beg" or "beggi") is also added. Legumes such as split peas ("ክክ", "kək" or "kikki") and lentils ("ምስር", "məsər" or "birsin"); or vegetables such as potatoes ("ድንች", "Dənəch"), carrots and chard (ቆስጣ) are also used instead in vegan dishes.

Each variation is named by appending the main ingredient to the type of wat (e.g. "kek alicha wat"). However, the word "keiy" is usually not necessary, as the spicy variety is assumed when it is omitted (e.g. "doro wat"). The term "atkilt wat" is sometimes used to refer to all vegetable dishes, but a more specific name can also be used (as in "dinich'na caroht wat", which translates to "potatoes and carrots stew"; but notice the word "atkilt" is usually omitted when using the more specific term).

Meat along with vegetables are sautéed to make "tibs" (also "tebs", "t'ibs", "tibbs", etc., Ge'ez: ጥብስ "ṭïbs"). Tibs is served in a variety of manners, and can range from hot to mild or contain little to no vegetables. There are many variations of the delicacy, depending on type, size or shape of the cuts of meat used. Beef, mutton, and goat are the most common meats used in the preparation of tibs.
The mid-18th century European visitor to Ethiopia describes "tibs" as a portion of grilled meat served "to pay a particular compliment or show especial respect to someone." It may still be seen this way; today the dish is prepared to commemorate special events and holidays.

Kinche (Qinch’e) is a very common Ethiopian breakfast or supper, its equivalent of oatmeal. It is incredibly simple, inexpensive, and nutritious. It is made from cracked wheat, Ethiopian oats, barley or a mixture of those. It can be boiled in either milk or water with a little salt . The flavor of the Kinche comes from the nit'ir qibe, which is a spiced butter.


Another distinctively Ethiopian dish is "kitfo" (frequently spelled "ketfo"). It consists of raw (or rare) beef mince marinated in "mitmita" (Ge'ez: ሚጥሚጣ "mīṭmīṭā" a very spicy chili powder similar to the "berbere") and "niter kibbeh". "Gored gored" is very similar to "kitfo", but uses cubed rather than ground beef.

"Ayibe" is a cottage cheese that is mild and crumbly. It is much closer in texture to crumbled feta. Although not quite pressed, the whey has been drained and squeezed out. It is often served as a side dish to soften the effect of very spicy food. It has little to no distinct taste of its own. However, when served separately, ayibe is often mixed with a variety of mild or hot spices typical of Gurage cuisine.

"Gomen kitfo" is another typical Gurage dish. Collard greens (ጎመን "gōmen") are boiled, dried and then finely chopped and served with butter, chili and spices. It is a dish specially prepared for the occasion of Meskel, a very popular holiday marking the discovery of the True Cross. It is served along with "ayibe" or sometimes even "kitfo" in this tradition called "dengesa".

The enset plant (called "wesse" in the Sidamo language) is central to Sidama cuisine and after grinding and fermenting the root to produce "wassa", it is used in the preparation of several foods.

"Amulcho" is an enset flatbread used similarly to injera to eat wats made from beef, mushrooms, beans, gomen, and pumpkin.

"Borasaame" is a cooked mixture of wassa and butter sometimes eaten with Ethiopian mustard greens. It is traditionally eaten by hand using a false banana leaf and is served in a "shafeta", a vase-like ceramic vessel. A common variant of borasaame uses maize flour instead of wassa and is called "badela borasaame".
Borasaame is typically paired with a seasoned yogurt drink called wätät. Both are common foods for funerals and the celebration of "Fichee Chambalaalla", the Sidama new year.

Gomen ba siga (ጎመን በስጋ, Amharic: "cabbage with meat") is a stewed mixture of beef and Ethiopian mustard served under a layer of amulcho.

A commonly grown crop in Sidama, maize (badela in Sidaamu; also known as “corn” in North America) is often eaten as a snack with coffee. It can be ground into flour to make bread, roasted on the cob, or the kernels can be picked off to make "bokolo", which is served either boiled or roasted.

"Fit-fit" or "fir-fir" is a common breakfast dish. It is made from shredded "injera" or "kitcha" stir-fried with spices or wat. Another popular breakfast food is "fatira". The delicacy consists of a large fried pancake made with flour, often with a layer of egg. It is eaten with honey. "Chechebsa" (or "kita firfir") resembles a pancake covered with "berbere" and "niter kibbeh", or other spices, and may be eaten with a spoon. "Genfo" is a kind of porridge, which is another common breakfast dish. It is usually served in a large bowl with a dug-out made in the middle of the genfo and filled with spiced "niter kibbeh". A variation of "ful", a fava bean stew with condiments, served with baked rolls instead of "injera", is also common for breakfast.

Typical Ethiopian snacks are Dabo Kolo (small pieces of baked bread that are similar to pretzels) or "kolo" (roasted barley sometimes mixed with other local grains). "Kolo" made from roasted and spiced barley, safflower kernels, chickpeas and/or peanuts are often sold by kiosks and street vendors, wrapped in a paper cone. Snacking on popcorn is also common.

A "gursha" (var. "gorsha", "goorsha") is an act of friendship and love. When eating injera, a person uses his or her right hand to strip off a piece, wraps it around some "wat" or "kitfo", and then puts it into his or her mouth. During a meal with friends or family, it is a common custom to feed others in the group with one's hand by putting the rolled injera or a spoon full of other dishes into another's mouth. This is called a "gursha", and the larger the gursha, the stronger the friendship or bond (only surpassed by the brewing of Tej together). This tradition was featured in "The Food Wife," an episode of "The Simpsons" that uses Ethiopian cuisine as a plot point.

There are many different traditional alcoholic drinks which are home made and of natural ingredients.

Tella is a home-brewed beer served in "tella bet" ("tella houses") which specialize in serving only tella. Tella is the most common beverage made and served in households during holidays.

It is an alcoholic drink which is prepared from bikil (barley) as main ingredient and gesho ("Rhamnus prinoides") for fermentation purpose.

In Oromiffaa the drink is called "farso" and in Tigrinya "siwa".

Tej is a potent honey wine. It is similar to mead, and is frequently served in bars, particularly in a "tej bet" or "tej house".

It is prepared from honey and gesho. It has a sweet taste and the alcoholic content is relatively higher than tella. This drink can be stored for a long time; the longer it is stored, the higher the alcohol content, and the stronger the taste.

Areki, also known as katikala, is probably the strongest alcoholic drink of Ethiopia.

Ethiopians have diverse traditional non-alcoholic drinks which include natural and healthy ingredients.

Kenetto, also known as keribo, is a non-alcoholic traditional drink. It is mostly used as substitute for tella for those who don't drink alcohol.

Borde is a cereal-based traditional fermented beverage famous in southern Ethiopia.

Just like the rest of the world Ethiopians also enjoy several locally manufactured beers, wine and non alcoholic products like Coca-Cola and other similar products.
Ambo Mineral Water or "Ambo wuha" is a bottled carbonated mineral water, sourced from the springs in Ambo Senkele near the town of Ambo.

"Atmet" is a barley and oat-flour based drink that is cooked with water, sugar and "kibe" (Ethiopian clarified butter) until the ingredients have combined to create a consistency slightly thicker than egg-nog. Though this drink is often given to women who are nursing, the sweetness and smooth texture make it a comfort drink for anyone who enjoys its flavor.

According to some sources, drinking of coffee ("buna") is likely to have originated in Ethiopia. A key national beverage, it is an important part of local commerce.

The coffee ceremony is the traditional serving of coffee, usually after a big meal. It often involves the use of a "jebena" (ጀበና), a clay coffee pot in which the coffee is boiled. The preparer roasts the coffee beans in front of guests, then walks around wafting the smoke throughout the room so participants may sample the scent of coffee. Then the preparer grinds the coffee beans in a traditional tool called a "mokecha". The coffee is put into the jebena, boiled with water, and then served in small cups called "si'ni". Coffee is usually served with sugar, but is also served with salt in many parts of Ethiopia. In some parts of the country, "niter kibbeh" is added instead of sugar or salt.

Snacks, such as popcorn or toasted barley (or "kolo"), are often served with the coffee. In most homes, a dedicated coffee area is surrounded by fresh grass, with special furniture for the coffee maker. A complete ceremony has three rounds of coffee (abol, tona and bereka) and is accompanied by the burning of frankincense.

Tea will most likely be served if coffee is declined.



</doc>
<doc id="9688" url="https://en.wikipedia.org/wiki?curid=9688" title="Epistle of James">
Epistle of James

The Epistle of James, the Letter of James, or simply James (), is one of the 21 epistles (didactic letters) in the New Testament.

The author identifies himself as "James, a servant [or slave] of God and of the Lord Jesus Christ" who is writing to "the twelve tribes scattered abroad". The epistle is traditionally attributed to James the brother of Jesus (James the Just), and the audience is generally considered to be Jewish Christians, who were dispersed outside Israel.

Framing his letter within an overall theme of patient perseverance during trials and temptations, James writes to encourage his readers to live consistently with what they have learned in Christ. He wants his readers to mature in their faith in Christ by living what they say they believe. He condemns various sins, including pride, hypocrisy, favouritism, and slander. He encourages and implores believers to humbly live by godly, rather than worldly wisdom and to pray in all situations. 

For the most part, until the late 20th century, the epistle of James was relegated to benign disregard – though it was shunned by many early theologians and scholars due to its advocacy of Torah observance and good works. Famously, Luther disliked the epistle due to its lack of Christology and its focus on Torah observance, and sidelined it to an appendix.

The epistle aims at a wide Jewish audience. During the last decades, the epistle of James has attracted increasing scholarly interest due to a surge in the quest for the historical James, his role within the Jesus movement, his beliefs, and his relationships and views. This James revival is also associated with the growing awareness about the Jewish grounding of the epistle and of the early Jesus movement. According to Baukham, as Christian scholarship and theology have moved to a more comfortable embrace of the Jewish grounding of the early Jesus movement, and of the diversity of early belief in Jesus, interest in the earliest layers of the tradition has been on the rise.

The debate about the authorship of James is inconclusive and shadows debates about Christology, and about historical accuracy. Direct authorship by James is now a minority view. The text’s affinity to Torah observance, good deeds, and the traditions that emanated from the historical James are acknowledged by a majority of scholars. The current scholarship regarding the authorship of James: 

According to Robert J. Foster, "there is little consensus as to the genre, structure, dating, and authorship of the book of James." There are four "commonly espoused" views concerning authorship and dating of the Epistle of James:

The writer refers to himself only as "James, a servant of God and of the Lord Jesus Christ" (). Jesus had two apostles named James: James, the son of Zebedee and James, the son of Alphaeus, but it is unlikely that either of these wrote the letter. According to the Book of Acts, James, the brother of John, was killed by Herod Agrippa I (). James, the son of Alphaeus is a more viable candidate for authorship, although he is not prominent in the scriptural record, and relatively little is known about him. Hippolytus, writing in the early third century, asserted in his work "On the 12 Apostles": 
The similarity of his alleged martyrdom to the stoning of James the Just has led some scholars, such as Robert Eisenman and James Tabor, to assume that these "two Jameses" were one and the same. This identification of James of Alphaeus with James the Just (as well as James the Less) has long been asserted, as evidenced by their conflation in Jacobus de Voragine's medieval hagiography the "Golden Legend".

Some have said the authorship of this epistle points to James, the brother of Jesus, to whom Jesus evidently had made a special appearance after his resurrection described in the New Testament as this James was prominent among the disciples. James the brother of Jesus was not a follower of Jesus before Jesus died according to John 7:2-5, which states that during Jesus' life "not even his brothers believed in him".

From the middle of the 3rd century, patristic authors cited the epistle as written by James, the brother of Jesus and a leader of the Jerusalem church. Not numbered among the Twelve Apostles unless he is identified as James the Less, James was nonetheless a very important figure: Paul the Apostle described him as "the brother of the Lord" in Galatians 1:19 and as one of the three "pillars of the Church" in 2:9."There is no doubt that James became a much more important person in the early Christian movement than a casual reader of the New Testament is likely to imagine." The James believers are acquainted with emerges out of ; and Acts 12,15,21. We also have accounts about James in Josephus, Eusebius, Origen, the Gospel of Thomas, the Apocalypses of James, the Gospel of the Hebrews and the Pseudo-Clementine literature - most of whom cast him as righteous and as the undisputed leader of the Jewish camp. "His influence is central and palpable in Jerusalem and in Antioch, despite the fact that he did not minister at Antioch. Although we are dependent on sources dominated by the Pauline perspective… the role and influence of James overshadow all others at Antioch."
Pseudonymous authorship (3 above) implies that the person named "James" is respected and doubtless well known. Moreover, this James, brother of Jesus, is honored by the epistle written and distributed after the lifetime of James, the brother of Jesus.

John Calvin and others suggested that the author was the James, son of Alphaeus, who is referred to as James the Less. The Protestant reformer Martin Luther denied it was the work of an apostle and termed it an "epistle of straw".

The Holy Tradition of the Orthodox Church teaches that the Book of James was "written not by either of the apostles, but by the 'brother of the Lord' who was the first bishop of the Church in Jerusalem (see , )."

Koester H. (1965) and Kloppenborg J. (1987) are widely recognized for bringing about the pivot from the above (traditional) emphasis on James as wisdom and ethics literature, to focus on the apocalyptic and pre-Gentile (Jewish) context of James. Later studies strengthened this recent appreciation for the pre-Gentile foundations of Q, M, and James. In addition to James, traces of the Jewish followers of Jesus are to be found in the extra-canonical Jewish Gospels (Nazoraeans, Ebionites), in the Didache and in the Pseudo-Clementine literature, texts not focused on Jesus’ death and resurrection and either advocate, or seem to advocate, Torah observance.

Many scholars consider the epistle to be pseudepigrapha:

Scholars, such as Luke Timothy Johnson, suggest an early dating for the Epistle of James:
The Letter of James also, according to the majority of scholars who have carefully worked through its text in the past two centuries, is among the earliest of New Testament compositions. It contains no reference to the events in Jesus' life, but it bears striking testimony to Jesus' words. Jesus' sayings are embedded in James' exhortations in a form that is clearly not dependent on the written Gospels.

If written by James the brother of Jesus, it would have been written sometime before AD 69 (or AD 62), when he was martyred.

However, the mention of elders and the role of elders in the church suggests a late first century composition, after the first generation of disciples. The epistle also assumes that Paul's writings
were well known, even to the extent of being misused. Other factors, including its history of use in the church point to a much later date. 

The earliest extant manuscripts of James usually date to the mid-to-late 3rd century.

James is considered New Testament wisdom literature: "like Proverbs and Sirach, it consists largely of moral exhortations and precepts of a traditional and eclectic nature."

The content of James is directly parallel, in many instances, to sayings of Jesus found in the gospels of Luke and Matthew, i.e., those attributed to the hypothetical Q source. Compare, e.g., "Do not swear at all, either by heaven...or by the earth...Let your word be 'Yes, Yes' or 'No, No'; anything more than this comes from the evil one" (Matthew 5:34, 37) and "...do not swear either by heaven or by earth or by any other oath, but let your 'Yes' be yes and your 'No' be no, so that you may not fall under condemnation" (James 5:12). According to James Tabor, the epistle of James contains "no fewer than thirty direct references, echoes, and allusions to the teachings of Jesus found in the Q source."

Some view the epistle as having no overarching outline: "James may have simply grouped together small 'thematic essays' without having more linear, Greco-Roman structures in mind." That view is generally supported by those who believe that the epistle may not be a true piece of correspondence between specific parties but an example of wisdom literature, formulated as a letter for circulation. The "Catholic Encyclopedia" says, "the subjects treated of in the Epistle are many and various; moreover, St. James not infrequently, whilst elucidating a certain point, passes abruptly to another, and presently resumes once more his former argument."

Others view the letter as having only broad topical or thematic structure. They generally organize James under three (Ralph Martin) to seven (Luke Johnson) general key themes or segments.

A third group believes that James was more purposeful in structuring his letter, linking each paragraph theologically and thematically:

The third view of the structuring of James is a historical approach that is supported by scholars who are not content with leaving the book as "New Testament wisdom literature, like a small book of proverbs" or "like a loose collection of random pearls dropped in no particular order onto a piece of string."

A fourth group uses modern discourse analysis or Greco-Roman rhetorical structures to describe the structure of James.

The United Bible Societies' "Greek New Testament" divides the letter into the following sections:
A 2013 article in the "Evangelical Quarterly" explores a violent historical background behind the epistle and offers the suggestion that it was indeed written by James, the brother of Jesus, and it was written before AD 62, the year he was killed. The 50s saw the growth of turmoil and violence in Roman Judea, as Jews became more and more frustrated with corruption, injustice and poverty. It continued into the 60s, four years before James was killed. War broke out with Rome and would lead to the destruction of Jerusalem and the scattering of the people. The epistle is renowned for exhortations on fighting poverty and caring for the poor in practical ways (1:26–27; 2:1-4; 2:14-19; 5:1-6), standing up for the oppressed (2:1-4; 5:1-6) and not being "like the world" in the way one responds to evil in the world (1:26-27; 2:11; 3:13-18; 4:1-10). Worldly wisdom is rejected and people are exhorted to embrace heavenly wisdom, which includes peacemaking and pursuing righteousness and justice (3:13-18).

This approach sees the epistle as a real letter with a real immediate purpose: to encourage Christian Jews not to revert to violence in their response to injustice and poverty but to stay focused on doing good, staying holy and to embrace the wisdom of heaven, not that of the world.

The epistle contains the following famous passage concerning salvation and justification:
That passage has been cited in Christian theological debates, especially regarding the doctrine of justification. Gaius Marius Victorinus (4th century) associated James's teaching on works with the heretical Symmachian sect, followers of Symmachus the Ebionite, and openly questioned whether James' teachings were heretical. This passage has also been contrasted with the teachings of Paul the Apostle on justification. Some scholars even believe that the passage is a response to Paul. One issue in the debate is the meaning of the Greek word δικαιόω (dikaiόō) 'render righteous or such as he ought to be', with some among the participants taking the view that James is responding to a misunderstanding of Paul.

Roman Catholicism and Eastern Orthodoxy have historically argued that the passage disproves the doctrine of justification by faith alone ("sola fide"). The early (and many modern) Protestants resolve the apparent conflict between James and Paul regarding faith and works in alternate ways from the Catholics and Orthodox:

According to Ben Witherington III, differences exist between the Apostle Paul and James, but both used the law of Moses, the teachings of Jesus and other Jewish and non-Jewish sources, and "Paul was not anti-law any more than James was a legalist".

The epistle is also the chief biblical text for the Anointing of the Sick. James wrote:
G. A. Wells suggested that the passage was evidence of late authorship of the epistle, on the grounds that the healing of the sick being done through an official body of presbyters (elders) indicated a considerable development of ecclesiastical organisation "whereas in Paul's day to heal and work miracles pertained to believers indiscriminately (I Corinthians, XII:9)."

James and the M Source material in Matthew are unique in the canon in their stand against the rejection of works and deeds. According to Sanders, traditional Christian theology wrongly divested the term "works" of its ethical grounding, part of the effort to characterize Judaism as legalistic. However, for James and for all Jews, faith is alive only through Torah observance. In other words, belief demonstrates itself through practice and manifestation. For James, claims about belief are empty, unless they are alive in action, works and deeds.

James is unique in the canon by its explicit and wholehearted support of Torah-observance (the Law). According to Bibliowicz, not only is this text a unique view into the milieu of the Jewish founders - its inclusion in the canon signals that as canonization begun (fourth century onward) Torah observance among believers in Jesus was still authoritative. 
"Some have attempted while I am still alive, to transform my words by certain various interpretations, in order to teach the dissolution of the law; as though I myself were of such a mind, but did not freely proclaim it, which God forbid! For such a thing were to act in opposition to the law of God which was spoken by Moses, and was borne witness to by our Lord in respect of its eternal continuance; for thus he spoke: ‘The heavens and the earth shall pass away, but one jot or one tittle shall in no wise pass away from the law." 

James seem to propose a more radical and demanding interpretation of the law than mainstream Judaism. According to Painter, there is nothing in James to suggest any relaxation of the demands of the law. ‘No doubt James takes for granted his readers' observance of the whole law, while focusing his attention on its moral demands.’

The Epistle of James was first explicitly referred to and quoted by Origen of Alexandria, and possibly a bit earlier by Irenaeus of Lyons as well as Clement of Alexandria in a lost work according to Eusebius, although it was not mentioned by Tertullian, who was writing at the end of the Second century. It is also absent from the Muratorian fragment, the earliest known list of New Testament books.

The Epistle of James was included among the twenty-seven New Testament books first listed by Athanasius of Alexandria in his "Thirty-Ninth Festal Epistle" (AD 367) and was confirmed as a canonical epistle of the New Testament by a series of councils in the fourth century.

In the first centuries of the Church the authenticity of the Epistle was doubted by some, including Theodore of Mopsuestia in the mid-fifth century. Because of the silence of several of the western churches regarding it, Eusebius classes it among the Antilegomena or contested writings ("Historia ecclesiae", 3.25; 2.23). Jerome gives a similar appraisal but adds that with time it had been universally admitted. Gaius Marius Victorinus, in his commentary on the Epistle to the Galatians, openly questioned whether the teachings of James were heretical.

Its late recognition in the Church, especially in the West, may be explained by the fact that it was written for or by Jewish Christians, and therefore not widely circulated among the Gentile Churches. There is some indication that a few groups distrusted the book because of its doctrine. In Reformation times a few theologians, most notably Martin Luther in his early ministry, argued that this epistle should not be part of the canonical New Testament.

Martin Luther's description of the Epistle of James varies. In some cases, Luther argues that it was not written by an apostle; but in other cases, he describes James as the work of an apostle. He even cites it as authoritative teaching from God and describes James as "a good book, because it sets up no doctrines of men but "vigorously promulgates the law of God"." Lutherans hold that the Epistle is rightly part of the New Testament, citing its authority in the Book of Concord; however, it remains part of the Lutheran antilegomena.




</doc>
<doc id="9689" url="https://en.wikipedia.org/wiki?curid=9689" title="Epistle of Jude">
Epistle of Jude

The Epistle of Jude, often shortened to Jude, is the penultimate book of the New Testament and the Bible as a whole and is traditionally attributed to Jude, the servant of Jesus and the brother of James the Just.

The letter of Jude was one of the disputed books of the biblical canon. The links between the Epistle and 2 Peter, its use of the biblical apocrypha, and its brevity raised concern. It is one of the shortest books in the Bible: only 1 chapter of 25 verses long.

Some early manuscripts containing the text of this epistle are:

Jude urges his readers to defend the deposit of Christ's doctrine that had been closed by the time he wrote his epistle, and to remember the words of the apostles spoken somewhat before. Jude then asks the reader to recall how even after the Lord saved his own people out of the land of Egypt, he did not hesitate to destroy those who fell into unbelief, much as he punished the angels who fell from their original exalted status and Sodom and Gomorrah. He describes in vivid terms the apostates of his day. He exhorts believers to remember the words spoken by the Apostles, using language similar to the second epistle of Peter to answer concerns that the Lord seemed to tarry, "How that they told you there should be mockers in the last time, who should walk after their own ungodly lusts...", and to keep themselves in God's love, before delivering a doxology.

Jude quotes directly from the Book of Enoch, part of the scripture of the Ethiopian and Eritrean churches but rejected by other churches. He cites Enoch's prophecy that the Lord would come with many thousands of his saints to render judgment on the whole world. He also paraphrases (verse 9) an incident in a text that has been lost about Satan and Michael the Archangel quarreling over the body of Moses.

I. Salutation (1–3)

II. Occasion for the Letter (3–4)<br>
  A. The change of Subject (3) <br>
  B. The Reason for the Change: The Presence of Godless Apostates (4)

III. Warning against the False Teachers (5–16)<br>
  A. Historical Examples of the Judgement of Apostates (5–7) <br>
    1. Unbelieving Israel (5) <br>
    2. Angels who fell (6) <br>
    3. Sodom and Gomorrah (7) <br>
  B. Description of the Apostates of Jude's Day (8–16)<br>
    1. Their slanderous speech deplored (8–10) <br>
    2. Their character graphically portrayed (11–13) <br>
    3. Their destruction prophesied (14–16)

IV. Exhortation to Believers (17–23)

V. Concluding Doxology (24–25)

The Epistle of Jude is held as canonical in the Christian Church. Conservative scholars date it between 70 and 90. Some scholars consider the letter a pseudonymous work written between the end of the 1st century and the first quarter of the 2nd century because of its references to the apostles and to tradition and because of its competent Greek style.

"More remarkable is the evidence that by the end of the second century Jude was widely accepted as canonical." Clement of Alexandria, Tertullian, and the Muratorian canon considered the letter canonical. The first historical record of doubts as to authorship are found in the writings of Origen of Alexandria, who spoke of the doubts held by some, albeit not him. Eusebius classified it with the "disputed writings, the "antilegomena."" The letter was eventually accepted as part of the Canon by Church Fathers such as Athanasius of Alexandria and the Synods of Laodicea (c. 363) and Carthage (c. 397).

The epistle title is written as follows: "Jude, a servant of Jesus Christ and brother of James" (NRSV). "James" is generally taken to mean James the Just, a prominent leader in the early church. Not a lot is known of Jude, which would explain the apparent need to identify him by reference to his better-known brother.

As the brother of James the Just, it has traditionally meant Jude was also the brother of Jesus, since James is described as being the brother of Jesus. For instance Clement of Alexandria (c. 150–215 AD) wrote in his work ""Comments on the Epistle of Jude"" that Jude, the Epistle of Jude's author, was a son of Joseph and a brother of Jesus (without specifying whether he was a son of Joseph by a previous marriage or of Joseph and Mary).

There is also a dispute as to whether "brother" means someone who has the same father and mother, or a half-brother or cousin or more distant familial relationship. This dispute over the true meaning of "brother" grew as the doctrine of the Virgin Birth evolved.

Outside the book of Jude, a "Jude" is mentioned five times in the New Testament: three times as Jude the Apostle (Luke 6:16, Acts 1:13, John 14:22), and twice as Jude the brother of Jesus (Matthew 13:55, Mark 6:3) (aside from references to Judas Iscariot and Judah (son of Jacob)). Debate continues as to whether the author of the epistle is either, both, or neither. Some scholars have argued that since the author of the letter has not identified himself as an apostle and actually refers to the apostles as a third party, he cannot be identified with Jude the Apostle. Others have drawn the opposite conclusion, i.e., that, as an apostle, he would not have made a claim of apostleship on his own behalf.

The Epistle of Jude is a brief book of only a single chapter with 25 verses. It was composed as an "encyclical letter"—that is, one not directed to the members of one church in particular, but intended rather to be circulated and read in all churches.

The wording and syntax of this epistle in its original Greek demonstrates that the author was capable and fluent. The epistle is addressed to Christians in general, and it warns them about the doctrine of certain errant teachers to whom they were exposed.

The epistle's style is combative, impassioned, and rushed. Many examples of evildoers and warnings about their fates are given in rapid succession.

The epistle concludes with a doxology, which is considered by Peter H. Davids to be one of the highest in quality contained in the Bible.

Part of Jude is very similar to 2 Peter (mainly 2 Peter chapter 2), so much so that most scholars agree that there is a dependence between the two, i.e., that either one letter used the other directly, or they both drew on a common source. Comparing the Greek text portions of 2 Peter 2:1–3:3 (426 words) to Jude 4–18 (311 words) results in 80 words in common and 7 words of substituted synonyms.

The shared passages are:
Because this epistle is much shorter than 2 Peter, and due to various stylistic details, some writers consider Jude the source for the similar passages of 2 Peter. However, other writers, arguing that Jude 18 quotes as past tense, consider Jude to have come after 2 Peter.

Some scholars who consider Jude to predate 2 Peter note that the latter appears to quote the former but omits the reference to the non-canonical book of Enoch.

The Epistle of Jude references at least two other books, with one being non-canonical in all churches and the other non-canonical in most churches.

Verse 9 refers to a dispute between Michael the Archangel and the devil about the body of Moses. Some interpreters understand this reference to be an allusion to the events described in . The classical theologian Origen attributes this reference to the non-canonical Assumption of Moses. According to James Charlesworth, there is no evidence the surviving book of this name ever contained any such content. Others believe it to be in the lost ending of the book.

Verses 14–15 contain a direct quotation of a prophecy from 1 Enoch 1:9. The title "Enoch, the seventh from Adam" is also sourced from 1 En. 60:1. Most commentators assume that this indicates that Jude accepts the antediluvian patriarch Enoch as the author of the Book of Enoch which contains the same quotation. However, an alternative explanation is that Jude quotes the Book of Enoch aware that verses 14–15 are in fact an expansion of the words of Moses from Deuteronomy 33:2. This is supported by Jude's unusual Greek statement that "Enoch the Seventh from Adam prophesied "to" the false teachers", not "concerning" them.

The Book of Enoch is not considered canonical by most churches, although it is by the Ethiopian Orthodox church. According to Western scholars, the older sections of the Book of Enoch (mainly in the "Book of the Watchers") date from about 300 BC and the latest part ("Book of Parables") probably was composed at the end of the 1st century BC. 1 Enoch 1:9, mentioned above, is part of the pseudepigrapha and is among the Dead Sea Scrolls [4Q Enoch (4Q204[4QENAR]) COL I 16–18]. It is generally accepted by scholars that the author of the Epistle of Jude was familiar with the Book of Enoch and was influenced by it in thought and diction.



Online translations of the Epistle of Jude:
Audiobook Version:

Additional information:


</doc>
<doc id="9692" url="https://en.wikipedia.org/wiki?curid=9692" title="Eusebius Amort">
Eusebius Amort

Eusebius Amort (November 15, 1692February 5, 1775) was a German Roman Catholic theologian.

Amort was born at Bibermuhle, near Tolz, in Upper Bavaria. He studied at Munich, and at an early age joined the Canons Regular at Polling, where, shortly after his ordination in 1717, he taught theology and philosophy.
The Parnassus Boicus learned society was based on a plan started in 1720 by three Augustinian fathers: Eusebius Amort, Gelasius Hieber (1671–1731), a famous preacher in the German language and Agnellus Kandler (1692–1745), a genealogist and librarian. The initial plans fell through, but in 1722 they issued the first volume of the "Parnassus Boicus" journal, communicating interesting information from the arts and sciences.

In 1733 Amort went to Rome as theologian to Cardinal Niccolo Maria Lercari (died 1757).
He returned to Polling in 1735 and devoted the rest of his life to the revival of learning in Bavaria. He died at Polling in 1775.

Amort, who had the reputation of being the most learned man of his age, was a voluminous writer on every conceivable subject, from poetry to astronomy, from dogmatic theology to mysticism. His best known works are:


The list of his other works, including his three erudite contributions to the question of authorship of the "Imitatio Christi", will be found in C. Toussaint's scholarly article in Alfred Vacant's "Dictionnaire de theologie" (1900, cols 1115-1117).

Citations

Sources


</doc>
<doc id="9693" url="https://en.wikipedia.org/wiki?curid=9693" title="Episcopus vagans">
Episcopus vagans

The "Oxford Dictionary of the Christian Church" mentions as the main lines of succession deriving from "" in the 20th century those founded by Arnold Mathew, Joseph René Vilatte and Leon Chechemian. Others that could be added are those derived from Aftimios Ofiesh, Carlos Duarte Costa, Paolo Miraglia-Gulotti, Emmanuel Milingo, Pierre Martin Ngô Đình Thục and Marcel Lefebvre.

In Western Christianity it has traditionally been taught, since as far back as the time of the Donatist controversy of the fourth and fifth centuries, that any bishop can consecrate any other baptised man as a bishop provided the bishop observes the minimum requirements for the sacramental validity of the ceremony. This means that the consecration is considered valid even if it flouts certain ecclesiastical laws, and even if the participants are schismatics or heretics.

According to a theological view affirmed, for instance, by the International Bishops' Conference of the Old Catholic Church with regard to ordinations by Arnold Mathew, an episcopal ordination is for service within a specific Christian church, and an ordination ceremony that concerns only the individual himself does not make him truly a bishop. The Holy See has not commented on the validity of this theory, but has declared with regard to ordinations of this kind carried out, for example, by Emmanuel Milingo, that the Church "does not recognize and does not intend to recognize in the future those ordinations or any of the ordinations derived from them and therefore the canonical state of the alleged bishops remains that in which they were before the ordination conferred by Mr Milingo". Other theologians, notably those of the Eastern Orthodox Church, dispute the notion that such ordinations have effect, a notion that opens up the possibility of valid but irregular consecrations proliferating outside the structures of the "official" denominations.

A Catholic ordained to the episcopacy without a mandate from the Pope is automatically excommunicated and is thereby forbidden to celebrate the sacraments, according to canon law.

Vlassios Pheidas, on an official Church of Greece site, uses the canonical language of the Orthodox tradition, to describe the conditions in ecclesial praxis when sacraments, including Holy Orders, are real, valid, and efficacious. He notes language is itself part of the ecclesiological problem.
This applies to the validity and efficacy of the ordination of bishops and the other sacraments, not only of the Independent Catholic Churches, but also of all other Christian churches, including the Roman Catholic Church, Oriental Orthodoxy and the Assyrian Church of the East.

Anglican bishop Colin Buchanan, in the "Historical Dictionary of Anglicanism", says that the Anglican Communion has held an Augustinian view of orders, by which "the validity of Episcopal ordinations (to whichever order) is based solely upon the historic succession in which the ordaining bishop stands, irrespective of their contemporary ecclesial context".
He describes the circumstances of Archbishop Matthew Parker's consecration as one of the reasons why this theory is "generally held". Parker was chosen by Queen Elizabeth I of England to be the first Church of England Archbishop of Canterbury after the death of the previous office holder, Cardinal Reginald Pole, the last Roman Catholic Archbishop of Canterbury. Buchanan notes the Roman Catholic Church also focuses on issues of intention and not just breaks in historical succession. He does not explain whether intention has an ecclesiological role, for Anglicans, in conferring or receiving sacraments.

According to Buchanan, "the real rise of the problem" happened in the 19th century, in the "wake of the Anglo-Catholic movement", "through mischievous activities of a tiny number of independently acting bishops". They exist worldwide, he writes, "mostly without congregations", and "many in different stages of delusion and fantasy, not least in the Episcopal titles they confer on themselves"; "the distinguishing mark" to "specifically identif[y]" an "episcopus vagans" is "the lack of a true see or the lack of a real church life to oversee". Paul Halsall, on the Internet History Sourcebooks Project, did not list a single church edifice of independent bishops, in a 1996–1998 New York City building architecture survey of religious communities, which maintain bishops claiming apostolic succession and claim cathedral status but noted there "are now literally hundreds of these ", of lesser or greater spiritual probity. They seem to have a tendency to call living room sanctuaries 'cathedrals';" those buildings were not perceived as cultural symbols and did not meet the survey criteria. David V. Barrett wrote, in "A Brief Guide to Secret Religions", that "one hallmark of such bishops is that they often collect as many lineages as they can to strengthen their Episcopal legitimacy—at least in their own eyes" and their groups have more clergy than members.

Many "episcopi vagantes" claim succession from the Old Catholic See of Utrecht, or from Eastern Orthodox, Oriental Orthodox, or Eastern Catholic Churches. A few others derive their orders from Roman Catholic bishops who have consecrated their own bishops after disputes with the Holy See.

Barrett wrote that leaders "of some esoteric movements, are also priests or bishops in small non-mainstream Christian Churches"; he explains, this type of "independent or autocephalous" group has "little in common with the Church it developed from, the Old Catholic Church, and even less in common with the Roman Catholic Church" but still claims its authority from Apostolic succession.

Many, if not most, "episcopi vagantes" are associated with Independent Catholic Churches. They may be very liberal or very conservative. "Episcopi vagantes" may also include some conservative "Continuing Anglicans" who have broken with the Anglican Communion over various issues such as Prayer Book revision, the ordination of women and the ordination of unmarried, non-celibate individuals (including homosexuals).

Buchanan writes that based the criteria of having "a true see" or having "a real church life to oversee", the bishops of most forms of the Continuing Anglican movement are not necessarily classified as vagantes, but "are always in danger of becoming such".

Arnold Mathew, according to Buchanan, "lapsed into the vagaries of an """ Stephen Edmonds, in the "Oxford Dictionary of National Biography", wrote that in 1910 Mathew's wife separated from him; that same year, he declared himself and his church seceded from the Union of Utrecht. Within a few months, on 2 November 1911, he was excommunicated by the Roman Catholic Church; sued "The Times" for libel based on the words "pseudo-bishop" used to describe him in the newspaper's translation from the Latin text """"; and, lost his case in 1913. Henry R.T. Brandreth wrote, in "Episcopi Vagantes and the Anglican Church", "[o]ne of the most regrettable features of Mathew's episcopate was the founding of the Order of Corporate Reunion (OCR) in 1908. This claimed to be a revival of Frederick George Lee's movement, but was in fact unconnected with it." Brandreth thought it "seems still to exist in a shadowy underground way" in 1947, but disconnected. Colin Holden, in "Ritualist on a Tricycle", places Mathew and his into perspective, he wrote Mathew was an "", lived in a cottage provided for him, and performed his conditional acts, sometimes called according to Holden "bedroom ordinations", in his cottage. Mathew questioned the validity of Anglican ordinations and became involved with the OCR, in 1911 according to Edmonds, and he openly advertised his offer to reordain Anglican clergy who requested it. This angered the Church of England. In 1912, D. J. Scannell O'Neill wrote in "The Fortnightly Review" that London "seems to have more than her due share of bishops" and enumerates what he refers to as "these hireling shepherds". He also announces that one of them, Mathew, revived the OCR and published "The Torch", a monthly review, advocating the reconstruction of Western Christianity and reunion with Eastern Christianity. "The Torch" stated "that the ordinations of the Church of England are not recognized by any church claiming to be Catholic" so the promoters involved Mathew to conditionally ordain group members who are "clergy of the Established Church" and "sign a profession of the Catholic Faith". It stipulated Mathew's services were not a system of simony and given without simoniac expectations. The group sought to enroll "earnest-minded Catholics who sincerely desire to help forward the work of [c]orporate [r]eunion with the Holy See". Nigel Yates, in "Anglican Ritualism in Victorian Britain, 1830-1910", described it as "an even more bizarre scheme to promote a Catholic Uniate Church in Britain" than Lee and Ambrose Lisle March Phillipps de Lisle's Association for the Promotion of the Unity of Christendom. It was editorialized by O'Neill that the "most charitable construction to be placed on this latest move of Mathew is that he is not mentally sound. Being an Irishman, it is strange that he has not sufficient humor to see the absurdity of falling away from the Catholic Church in order to assist others to unite with the Holy See." Edmonds reports that "anything between 4 and 265 was suggested" as to how many took up his offer of reordination.

When it declared devoid of canonical effect the consecration ceremony conducted by Archbishop Pierre Martin Ngô Đình Thục for the Carmelite Order of the Holy Face group at midnight of 31 December 1975, the Holy See refrained from pronouncing on its validity. It also made the same statement with regard to later ordinations by those bishops, saying that, "as for those who have already thus unlawfully received ordination or any who may yet accept ordination from these, whatever may be the validity of the orders ("quidquid sit de ordinum validitate"), the Church does not and will not recognise their ordination ("ipsorum ordinationem"), and will consider them, for all legal effects, as still in the state in which they were before, except that the ... penalties remain until they repent".

A similar declaration was issued with regard to Archbishop Emmanuel Milingo's conferring of episcopal ordination on four men - all of whom, by virtue of previous Independent Catholic consecrations, claimed already to be bishops - on 24 September 2006: the Holy See, as well as stating that, in accordance with Canon 1382 of the Code of Canon Law, all five men involved incurred automatic (""latae sententiae"") excommunication through their actions, declared that "the Church does not recognise and does not intend in the future to recognise these ordinations or any ordinations derived from them, and she holds that the canonical state of the four alleged bishops is the same as it was prior to the ordination."

In contrast, the Holy See has not questioned the validity of the consecrations that the late Archbishop Marcel Lefebvre performed in 1988 for the service of the relatively numerous followers of the Traditionalist Catholic Society of St. Pius X that he had founded, and of the bishops who, under pressure from the Chinese Catholic Patriotic Association, "have been ordained without the Pontifical mandate and who have not asked for, or have not yet obtained, the necessary legitimation", and who consequently, Pope Benedict XVI declared, "are to be considered illegitimate, but validly ordained".

Victor LaValle, in the novel "Big Machine" (2009), included three "" as part of his character's childhood involvement with an independent church:
Calvin Baker, in his novel "Dominion" (2006), includes an "" as one of his characters:
Intertextuality of ' language. Jim Higgins saw, in "More Years for the Locust", "similarities between Marxist obscurantism and an addiction to Christian arcana" and used ' pejoratively as his example of "the ever-growing proliferation of sects, sectlets and insects claiming direct descent from the master" with "fissiparous tendencies". He saw humor in the ludicrous characters and farce in their titles.



</doc>
<doc id="9695" url="https://en.wikipedia.org/wiki?curid=9695" title="Elizabeth Garrett Anderson">
Elizabeth Garrett Anderson

Elizabeth Garrett Anderson (9 June 1836 – 17 December 1917) was an English physician and suffragist. She was the first woman to qualify in Britain as a physician and surgeon. She was the co-founder of the first hospital staffed by women, the first dean of a British medical school, the first woman in Britain to be elected to a school board and, as Mayor of Aldeburgh, the first female mayor in Britain.

Garrett was born in Whitechapel, London, the second of eleven children of Newson Garrett (1812–1893), from Leiston, Suffolk, and his wife, Louisa (born Dunnell; 1813–1903), from London.

The Garrett ancestors had been ironworkers in East Suffolk since the early seventeenth century. Newson was the youngest of three sons and not academically inclined, although he possessed the family's entrepreneurial spirit. When he finished school, the town of Leiston offered little to Newson, so he left for London to make his fortune. There, he fell in love with his brother's sister-in-law, Louisa Dunnell, the daughter of an innkeeper of Suffolk origin. After their wedding, the couple went to live in a pawnbroker's shop at 1 Commercial Road, Whitechapel. The Garretts had their first three children in quick succession: Louie, Elizabeth and their brother (Dunnell Newson) who died at the age of six months. While Louisa grieved the loss of her third child, it was not easy to raise their two daughters in the city of London at that time. When Garrett was three years old, the family moved to 142 Long Acre, where they lived for two years, whilst one more child was born and her father moved up in the world, becoming not only the manager of a larger pawnbroker's shop, but also a silversmith. Garrett's grandfather, owner of the family engineering works, Richard Garrett & Sons, had died in 1837, leaving the business to his eldest son, Garrett's uncle. Despite his lack of capital, Newson was determined to be successful and in 1841, at the age of 29, he moved his family to Suffolk, where he bought a barley and coal merchants business in Snape, constructing Snape Maltings, a fine range of buildings for malting barley. 

The Garretts lived in a square Georgian house opposite the church in Aldeburgh until 1852. Newson's malting business expanded and more children were born, Edmund (1840), Alice (1842), Agnes (1845), Millicent (1847), who was to become a leader in the constitutional campaign for women's suffrage, Sam (1850), Josephine (1853) and George (1854). By 1850, Newson was a prosperous businessman and was able to build Alde House, a mansion on a hill behind Aldeburgh. A "by-product of the industrial revolution", Garrett grew up in an atmosphere of "triumphant economic pioneering" and the Garrett children were to grow up to become achievers in the professional classes of late-Victorian England. Garrett was encouraged to take an interest in local politics and, contrary to practices at the time, was allowed the freedom to explore the town with its nearby salt-marshes, beach and the small port of Slaughden with its boatbuilders' yards and sailmakers' lofts. 

There was no school in Aldeburgh so Garrett learned the three Rs from her mother. When she was 10 years old, a governess, Miss Edgeworth, a poor gentlewoman, was employed to educate Garrett and her sister. Mornings were spent in the schoolroom; there were regimented afternoon walks; educating the young ladies continued at mealtimes when Edgeworth ate with the family; at night, the governess slept in a curtained off area in the girls' bedroom. Garrett despised her governess and sought to outwit the teacher in the classroom. When Garrett was 13 and her sister 15, they were sent to a private school, the Boarding School for Ladies in Blackheath, London, which was run by the step aunts of the poet Robert Browning. There, English literature, French, Italian and German as well as deportment, were taught.
Later in life, Garrett recalled the stupidity of her teachers there, though her schooling there did help establish a love of reading. Her main complaint about the school was the lack of science and mathematics instruction. Her reading matter included Tennyson, Wordsworth, Milton, Coleridge, Trollope, Thackeray and George Eliot. Elizabeth and Louie were known as "the bathing Garretts", as their father had insisted they be allowed a hot bath once a week. However, they made what were to be lifelong friends there. When they finished in 1851, they were sent on a short tour abroad, ending with a memorable visit to the Great Exhibition in Hyde Park, London.

After this formal education, Garrett spent the next nine years tending to domestic duties, but she continued to study Latin and arithmetic in the mornings and also read widely. Her sister Millicent recalled Garrett's weekly lectures, "Talks on Things in General", when her younger siblings would gather while she discussed politics and current affairs from Garibaldi to Macaulay's "History of England". In 1854, when she was eighteen, Garrett and her sister went on a long visit to their school friends, Jane and Anne Crow, in Gateshead where she met Emily Davies, the early feminist and future co-founder of Girton College, Cambridge. Davies was to be a lifelong friend and confidante, always ready to give sound advice during the important decisions of Garrett's career. It may have been in the "English Woman's Journal", first issued in 1858, that Garrett first read of Elizabeth Blackwell, who had become the first female doctor in the United States in 1849. When Blackwell visited London in 1859, Garrett travelled to the capital. By then, her sister Louie was married and living in London. Garrett joined the Society for Promoting the Employment of Women, which organised Blackwell's lectures on "Medicine as a Profession for Ladies" and set up a private meeting between Garrett and the doctor. It is said that during a visit to Alde House around 1860, one evening while sitting by the fireside, Garrett and Davies selected careers for advancing the frontiers of women's rights; Garrett was to open the medical profession to women, Davies the doors to a university education for women, while 13-year-old Millicent was allocated politics and votes for women. At first Newson was opposed to the radical idea of his daughter becoming a physician but came round and agreed to do all in his power, both financially and otherwise, to support Garrett.

After an initial unsuccessful visit to leading doctors in Harley Street, Garrett decided to first spend six months as a surgery nurse at Middlesex Hospital, London in August 1860. On proving to be a good nurse, she was allowed to attend an outpatients' clinic, then her first operation. She unsuccessfully attempted to enroll in the hospital's Medical School but was allowed to attend private tuition in Latin, Greek and "materia medica" with the hospital's apothecary, while continuing her work as a nurse. She also employed a tutor to study anatomy and physiology three evenings a week. Eventually she was allowed into the dissecting room and the chemistry lectures. Gradually, Garrett became an unwelcome presence among the male students, who in 1861 presented a memorial to the school against her admittance as a fellow student, despite the support she enjoyed from the administration. She was obliged to leave the Middlesex Hospital but she did so with an honours certificate in chemistry and "materia medica". Garrett then applied to several medical schools, including Oxford, Cambridge, Glasgow, Edinburgh, St Andrews and the Royal College of Surgeons, all of which refused her admittance.

A companion to her in this struggle was the lesser known Dr. Sophia Jex-Blake. While both are considered "outstanding" medical figures of the late 19th century, Garrett was able to obtain her credentials by way of a "side door" through a loophole in admissions at the Worshipful Society of Apothecaries. Having privately obtained a certificate in anatomy and physiology, she was admitted in 1862 by the Society of Apothecaries who, as a condition of their charter, could not legally exclude her on account of her sex. She continued her battle to qualify by studying privately with various professors, including some at the University of St Andrews, the Edinburgh Royal Maternity and the London Hospital Medical School.

In 1865, she finally took her exam and obtained a licence (LSA) from the Society of Apothecaries to practise medicine, the first woman qualified in Britain to do so openly (previously there was Dr James Barry who was born and raised female but presented as male from the age of 20, and lived his adult life as a man). On the day, three out of seven candidates passed the exam, Garrett with the highest marks. The Society of Apothecaries immediately amended its regulations to prevent other women obtaining a licence meaning that Jex-Blake however could not follow this same path; the new rule disallowed privately educated women to be eligible for examination. It was not until 1876 that the new Medical Act (39 and 40 Vict, Ch. 41) passed which allowed British medical authorities to license all qualified applicants whatever their gender.

Though she was now a licentiate of the Society of Apothecaries, as a woman, Garrett could not take up a medical post in any hospital. So in late 1865, Garrett opened her own practice at 20 Upper Berkeley Street, London. At first patients were scarce, but the practice gradually grew. After six months in practice, she wished to open an outpatients dispensary, to enable poor women to obtain medical help from a qualified practitioner of their own gender. In 1865, there was an outbreak of cholera in Britain, affecting both rich and poor, and in their panic, some people forgot any prejudices they had in relation to a female physician. The first death due to cholera occurred in 1866, but by then Garrett had already opened St Mary's Dispensary for Women and Children, at 69 Seymour Place. In the first year, she tended to 3,000 new patients, who made 9,300 outpatient visits to the dispensary. On hearing that the Dean of the faculty of medicine at the University of Sorbonne, Paris was in favour of admitting women as medical students, Garrett studied French so that she could apply for a medical degree, which she obtained in 1870 after some difficulty.
The same year she was elected to the first London School Board, an office newly opened to women; Garrett's was the highest vote among all the candidates. Also in that year, she was made one of the visiting physicians of the East London Hospital for Children (later the Queen Elizabeth Hospital for Children), becoming the first woman in Britain to be appointed to a medical post, but she found the duties of these two positions to be incompatible with her principal work in her private practice and the dispensary, as well as her role as a new mother, so she resigned from these posts by 1873. In 1872, the dispensary became the New Hospital for Women and Children, treating women from all over London for gynaecological conditions; the hospital moved to new premises in Marylebone Street in 1874. Around this time, Garrett also entered into discussion with male medical views regarding women. In 1874, Henry Maudsley’s article on Sex and Mind in Education appeared, which argued that education for women caused over-exertion and thus reduced their reproductive capacity, sometimes causing "nervous and even mental disorders". Garrett's counter-argument was that the real danger for women was not education but boredom and that fresh air and exercise were preferable to sitting by the fire with a novel. In the same year, she co-founded London School of Medicine for Women with Sophia Jex-Blake and became a lecturer in what was the only teaching hospital in Britain to offer courses for women. She continued to work there for the rest of her career and was dean of the school from 1883 to 1902. This school was later called the Royal Free Hospital of Medicine, which later became part of what is now the medical school of University College London.

In 1873 she gained membership of the British Medical Association (BMA). In 1878 a motion was proposed to exclude women following the election of Garrett Anderson and Frances Hoggan. The motion was opposed by Dr Norman Kerr who maintained the equal rights of members. This was "one of several instances where Garrett, uniquely, was able to enter a hitherto all male medical institution which subsequently moved formally to exclude any women who might seek to follow her." In 1892, women were again admitted to the British Medical Association. In 1897, Garrett Anderson was elected president of the East Anglian branch of the BMA.
Garrett Anderson worked steadily at the development of the New Hospital for Women, and (from 1874) at the creation of the London School of Medicine for Women, where she served as its dean. Both institutions were handsomely and suitably housed and equipped. The New Hospital for Women was able to commission a building in the Euston Road; the architect was J. M. Brydon, who took into his employment at this time Anderson's sister Agnes Garrett and her cousin Rhoda Garrett, who contributed to its design. The hospital was for many years worked entirely by medical women. The schools (in Hunter Street, WC1) had over 200 students, most of them preparing for the medical degree of London University (the present-day University College London), which was opened to women in 1877.

Garrett Anderson was also active in the women's suffrage movement. In 1866, Garrett Anderson and Davies presented petitions signed by more than 1,500 asking that female heads of household be given the vote. That year, Garrett Anderson joined the first British Women's Suffrage Committee. She was not as active as her sister, Millicent Garrett Fawcett, though Garrett Anderson became a member of the Central Committee of the National Society for Women's Suffrage in 1889. After her husband's death in 1907, she became more active. As mayor of Aldeburgh, she gave speeches for suffrage, before the increasing militant activity in the movement led to her withdrawal in 1911. Her daughter Louisa, also a physician, was more active and more militant, spending time in prison in 1912 for her suffrage activities.

Elizabeth Garrett Anderson once remarked that "a doctor leads two lives, the professional and the private, and the boundaries between the two are never traversed". In 1871, she married James George Skelton Anderson (died 1907) of the Orient Steamship Company co-owned by his uncle Arthur Anderson, but she did not give up her medical practice. She had three children, Louisa (1873–1943), Margaret (1874–1875), who died of meningitis, and Alan (1877–1952). Louisa also became a pioneering doctor of medicine and feminist activist.

They retired to Aldeburgh in 1902, moving to Alde House in 1903, after the death of Elizabeth's mother. Skelton died of a stroke in 1907. She enjoyed a happy marriage and in later life, devoted time to Alde House, gardening, and travelling with younger members of the extended family.

On 9 November 1908, she was elected mayor of Aldeburgh, the first female mayor in England. Her father had been mayor in 1889.

She died in 1917 and is buried in the churchyard of St Peter and St Paul's Church, Aldeburgh.

The New Hospital for Women was renamed the Elizabeth Garrett Anderson Hospital in 1918 and amalgamated with the Obstetric Hospital in 2001 to form the Elizabeth Garrett Anderson and Obstetric Hospital before relocating to become the University College Hospital Elizabeth Garrett Anderson Wing at UCH.

The former Elizabeth Garrett Anderson Hospital buildings are incorporated into the new National Headquarters for the public service trade union UNISON. The Elizabeth Garrett Anderson Gallery, a permanent installation set within the restored hospital building, uses a variety of media to set the story of Garrett Anderson, her hospital, and women's struggle to achieve equality in the field of medicine within the wider framework of 19th and 20th century social history.

The critical care centre at Ipswich Hospital was named the Garrett Anderson Centre in her honour, in recognition of her connection to the county of Suffolk.

Elizabeth Garrett Anderson School, a secondary school for girls in Islington, London, is named after her.

The archives of Elizabeth Garrett Anderson are held at the Women's Library at the London School of Economics. The archives of the Elizabeth Garrett Anderson Hospital (formerly the New Hospital for Women) are held at the London Metropolitan Archives.

On 9 June 2016, Google Doodle commemorated her 180th birthday.

The Elizabeth Garrett Anderson programme of the NHS Leadership Academy is a master's degree in leadership and management.




</doc>
<doc id="9696" url="https://en.wikipedia.org/wiki?curid=9696" title="Erosion">
Erosion

In earth science, erosion is the action of surface processes (such as water flow or wind) that removes soil, rock, or dissolved material from one location on the Earth's crust, and then transports it to another location (not to be confused with weathering which involves no movement). This natural process is caused by the dynamic activity of erosive agents, that is, water, ice (glaciers), snow, air (wind), plants, animals, and humans. In accordance with these agents, erosion is sometimes divided into water erosion, glacial erosion, snow erosion, wind (aeolic) erosion, zoogenic erosion, and anthropogenic erosion. The particulate breakdown of rock or soil into clastic sediment is referred to as "physical" or "mechanical" erosion; this contrasts with "chemical" erosion, where soil or rock material is removed from an area by its dissolving into a solvent (typically water), followed by the flow away of that solution. Eroded sediment or solutes may be transported just a few millimetres, or for thousands of kilometres.

Natural rates of erosion are controlled by the action of geological weathering geomorphic drivers, such as rainfall; bedrock wear in rivers; coastal erosion by the sea and waves; glacial plucking, abrasion, and scour; areal flooding; wind abrasion; groundwater processes; and mass movement processes in steep landscapes like landslides and debris flows. The rates at which such processes act control how fast a surface is eroded. Typically, physical erosion proceeds fastest on steeply sloping surfaces, and rates may also be sensitive to some climatically-controlled properties including amounts of water supplied (e.g., by rain), storminess, wind speed, wave fetch, or atmospheric temperature (especially for some ice-related processes). Feedbacks are also possible between rates of erosion and the amount of eroded material that is already carried by, for example, a river or glacier. Processes of erosion that produce sediment or solutes from a place contrast with those of deposition, which control the arrival and emplacement of material at a new location.

While erosion is a natural process, human activities have increased by 10-40 times the rate at which erosion is occurring globally. At well-known agriculture sites such as the Appalachian Mountains, intensive farming practices have caused erosion up to 100x the speed of the natural rate of erosion in the region. Excessive (or accelerated) erosion causes both "on-site" and "off-site" problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes) ecological collapse, both because of loss of the nutrient-rich upper soil layers. In some cases, the eventual end result is desertification. Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems worldwide.

Intensive agriculture, deforestation, roads, anthropogenic climate change and urban sprawl are amongst the most significant human activities in regard to their effect on stimulating erosion. However, there are many prevention and remediation practices that can curtail or limit erosion of vulnerable soils.

Rainfall, and the surface runoff which may result from rainfall, produces four main types of soil erosion: "splash erosion", "sheet erosion", "rill erosion", and "gully erosion". Splash erosion is generally seen as the first and least severe stage in the soil erosion process, which is followed by sheet erosion, then rill erosion and finally gully erosion (the most severe of the four).

In "splash erosion", the impact of a falling raindrop creates a small crater in the soil, ejecting soil particles. The distance these soil particles travel can be as much as 0.6 m (two feet) vertically and 1.5 m (five feet) horizontally on level ground.

If the soil is saturated, or if the rainfall rate is greater than the rate at which water can infiltrate into the soil, surface runoff occurs. If the runoff has sufficient flow energy, it will transport loosened soil particles (sediment) down the slope. "Sheet erosion" is the transport of loosened soil particles by overland flow.

"Rill erosion" refers to the development of small, ephemeral concentrated flow paths which function as both sediment source and sediment delivery systems for erosion on hillslopes. Generally, where water erosion rates on disturbed upland areas are greatest, rills are active. Flow depths in rills are typically of the order of a few centimetres (about an inch) or less and along-channel slopes may be quite steep. This means that rills exhibit hydraulic physics very different from water flowing through the deeper, wider channels of streams and rivers.
"Gully erosion" occurs when runoff water accumulates and rapidly flows in narrow channels during or immediately after heavy rains or melting snow, removing soil to a considerable depth.

"Valley" or "stream erosion" occurs with continued water flow along a linear feature. The erosion is both downward, deepening the valley, and headward, extending the valley into the hillside, creating head cuts and steep banks. In the earliest stage of stream erosion, the erosive activity is dominantly vertical, the valleys have a typical V cross-section and the stream gradient is relatively steep. When some base level is reached, the erosive activity switches to lateral erosion, which widens the valley floor and creates a narrow floodplain. The stream gradient becomes nearly flat, and lateral deposition of sediments becomes important as the stream meanders across the valley floor. In all stages of stream erosion, by far the most erosion occurs during times of flood when more and faster-moving water is available to carry a larger sediment load. In such processes, it is not the water alone that erodes: suspended abrasive particles, pebbles, and boulders can also act erosively as they traverse a surface, in a process known as "traction".

"Bank erosion" is the wearing away of the banks of a stream or river. This is distinguished from changes on the bed of the watercourse, which is referred to as "scour". Erosion and changes in the form of river banks may be measured by inserting metal rods into the bank and marking the position of the bank surface along the rods at different times.

"Thermal erosion" is the result of melting and weakening permafrost due to moving water. It can occur both along rivers and at the coast. Rapid river channel migration observed in the Lena River of Siberia is due to thermal erosion, as these portions of the banks are composed of permafrost-cemented non-cohesive materials. Much of this erosion occurs as the weakened banks fail in large slumps. Thermal erosion also affects the Arctic coast, where wave action and near-shore temperatures combine to undercut permafrost bluffs along the shoreline and cause them to fail. Annual erosion rates along a segment of the Beaufort Sea shoreline averaged per year from 1955 to 2002.

Most river erosion happens nearer to the mouth of a river. On a river bend, the longest least sharp side has slower moving water. Here deposits build up. On the narrowest sharpest side of the bend, there is faster moving water so this side tends to erode away mostly.

Shoreline erosion, which occurs on both exposed and sheltered coasts, primarily occurs through the action of currents and waves but sea level (tidal) change can also play a role.
"Hydraulic action" takes place when the air in a joint is suddenly compressed by a wave closing the entrance of the joint. This then cracks it. "Wave pounding" is when the sheer energy of the wave hitting the cliff or rock breaks pieces off. "Abrasion" or "corrasion" is caused by waves launching sea load at the cliff. It is the most effective and rapid form of shoreline erosion (not to be confused with "corrosion"). "Corrosion" is the dissolving of rock by carbonic acid in sea water. Limestone cliffs are particularly vulnerable to this kind of erosion. "Attrition" is where particles/sea load carried by the waves are worn down as they hit each other and the cliffs. This then makes the material easier to wash away. The material ends up as shingle and sand. Another significant source of erosion, particularly on carbonate coastlines, is boring, scraping and grinding of organisms, a process termed "bioerosion".

Sediment is transported along the coast in the direction of the prevailing current (longshore drift). When the upcurrent amount of sediment is less than the amount being carried away, erosion occurs. When the upcurrent amount of sediment is greater, sand or gravel banks will tend to form as a result of deposition. These banks may slowly migrate along the coast in the direction of the longshore drift, alternately protecting and exposing parts of the coastline. Where there is a bend in the coastline, quite often a buildup of eroded material occurs forming a long narrow bank (a spit). Armoured beaches and submerged offshore sandbanks may also protect parts of a coastline from erosion. Over the years, as the shoals gradually shift, the erosion may be redirected to attack different parts of the shore.

Chemical erosion is the loss of matter in a landscape in the form of solutes. Chemical erosion is usually calculated from the solutes found in streams. Anders Rapp pioneered the study of chemical erosion in his work about Kärkevagge published in 1960.

Glaciers erode predominantly by three different processes: abrasion/scouring, plucking, and ice thrusting. In an abrasion process, debris in the basal ice scrapes along the bed, polishing and gouging the underlying rocks, similar to sandpaper on wood. Scientists have shown that, in addition to the role of temperature played in valley-deepening, other glaciological processes, such as erosion also control cross-valley variations. In a homogeneous bedrock erosion pattern, curved channel cross-section beneath the ice is created. Though the glacier continues to incise vertically, the shape of the channel beneath the ice eventually remain constant, reaching a U-shaped parabolic steady-state shape as we now see in glaciated valleys. Scientists also provide a numerical estimate of the time required for the ultimate formation of a steady-shaped U-shaped valley—approximately 100,000 years. In a weak bedrock (containing material more erodible than the surrounding rocks) erosion pattern, on the contrary, the amount of over deepening is limited because ice velocities and erosion rates are reduced.

Glaciers can also cause pieces of bedrock to crack off in the process of plucking. In ice thrusting, the glacier freezes to its bed, then as it surges forward, it moves large sheets of frozen sediment at the base along with the glacier. This method produced some of the many thousands of lake basins that dot the edge of the Canadian Shield. Differences in the height of mountain ranges are not only being the result tectonic forces, such as rock uplift, but also local climate variations. Scientists use global analysis of topography to show that glacial erosion controls the maximum height of mountains, as the relief between mountain peaks and the snow line are generally confined to altitudes less than 1500 m. The erosion caused by glaciers worldwide erodes mountains so effectively that the term "glacial buzzsaw" has become widely used, which describes the limiting effect of glaciers on the height of mountain ranges. As mountains grow higher, they generally allow for more glacial activity (especially in the accumulation zone above the glacial equilibrium line altitude), which causes increased rates of erosion of the mountain, decreasing mass faster than isostatic rebound can add to the mountain. This provides a good example of a negative feedback loop. Ongoing research is showing that while glaciers tend to decrease mountain size, in some areas, glaciers can actually reduce the rate of erosion, acting as a "glacial armor". Ice can not only erode mountains but also protect them from erosion. Depending on glacier regime, even steep alpine lands can be preserved through time with the help of ice. Scientists have proved this theory by sampling eight summits of northwestern Svalbard using Be10 and Al26, showing that northwestern Svalbard transformed from a glacier-erosion state under relatively mild glacial maxima temperature, to a glacier-armor state occupied by cold-based, protective ice during much colder glacial maxima temperatures as the Quaternary ice age progressed.

These processes, combined with erosion and transport by the water network beneath the glacier, leave behind glacial landforms such as moraines, drumlins, ground moraine (till), kames, kame deltas, moulins, and glacial erratics in their wake, typically at the terminus or during glacier retreat.

The best-developed glacial valley morphology appears to be restricted to landscapes with low rock uplift rates (less than or equal to 2 mm per year) and high relief, leading to long-turnover times. Where rock uplift rates exceed 2 mm per year, glacial valley morphology has generally been significantly modified in postglacial time. Interplay of glacial erosion and tectonic forcing governs the morphologic impact of glaciations on active orogens, by both influencing their height, and by altering the patterns of erosion during subsequent glacial periods via a link between rock uplift and valley cross-sectional shape.

At extremely high flows, kolks, or vortices are formed by large volumes of rapidly rushing water. Kolks cause extreme local erosion, plucking bedrock and creating pothole-type geographical features called rock-cut basins. Examples can be seen in the flood regions result from glacial Lake Missoula, which created the channeled scablands in the Columbia Basin region of eastern Washington.

Wind erosion is a major geomorphological force, especially in arid and semi-arid regions. It is also a major source of land degradation, evaporation, desertification, harmful airborne dust, and crop damage—especially after being increased far above natural rates by human activities such as deforestation, urbanization, and agriculture.

Wind erosion is of two primary varieties: "deflation", where the wind picks up and carries away loose particles; and "abrasion", where surfaces are worn down as they are struck by airborne particles carried by wind. Deflation is divided into three categories: (1) "surface creep", where larger, heavier particles slide or roll along the ground; (2) "saltation", where particles are lifted a short height into the air, and bounce and saltate across the surface of the soil; and (3) "suspension", where very small and light particles are lifted into the air by the wind, and are often carried for long distances. Saltation is responsible for the majority (50-70%) of wind erosion, followed by suspension (30-40%), and then surface creep (5-25%).

Wind erosion is much more severe in arid areas and during times of drought. For example, in the Great Plains, it is estimated that soil loss due to wind erosion can be as much as 6100 times greater in drought years than in wet years.

"Mass movement" is the downward and outward movement of rock and sediments on a sloped surface, mainly due to the force of gravity.

Mass movement is an important part of the erosional process and is often the first stage in the breakdown and transport of weathered materials in mountainous areas. It moves material from higher elevations to lower elevations where other eroding agents such as streams and glaciers can then pick up the material and move it to even lower elevations. Mass-movement processes are always occurring continuously on all slopes; some mass-movement processes act very slowly; others occur very suddenly, often with disastrous results. Any perceptible down-slope movement of rock or sediment is often referred to in general terms as a landslide. However, landslides can be classified in a much more detailed way that reflects the mechanisms responsible for the movement and the velocity at which the movement occurs. One of the visible topographical manifestations of a very slow form of such activity is a scree slope.

"Slumping" happens on steep hillsides, occurring along distinct fracture zones, often within materials like clay that, once released, may move quite rapidly downhill. They will often show a spoon-shaped isostatic depression, in which the material has begun to slide downhill. In some cases, the slump is caused by water beneath the slope weakening it. In many cases it is simply the result of poor engineering along highways where it is a regular occurrence.

"Surface creep" is the slow movement of soil and rock debris by gravity which is usually not perceptible except through extended observation. However, the term can also describe the rolling of dislodged soil particles in diameter by wind along the soil surface.

The amount and intensity of precipitation is the main climatic factor governing soil erosion by water. The relationship is particularly strong if heavy rainfall occurs at times when, or in locations where, the soil's surface is not well protected by vegetation. This might be during periods when agricultural activities leave the soil bare, or in semi-arid regions where vegetation is naturally sparse. Wind erosion requires strong winds, particularly during times of drought when vegetation is sparse and soil is dry (and so is more erodible). Other climatic factors such as average temperature and temperature range may also affect erosion, via their effects on vegetation and soil properties. In general, given similar vegetation and ecosystems, areas with more precipitation (especially high-intensity rainfall), more wind, or more storms are expected to have more erosion.

In some areas of the world (e.g. the mid-western USA), rainfall intensity is the primary determinant of erosivity (for a definition of "erosivity" check,) with higher intensity rainfall generally resulting in more soil erosion by water. The size and velocity of rain drops is also an important factor. Larger and higher-velocity rain drops have greater kinetic energy, and thus their impact will displace soil particles by larger distances than smaller, slower-moving rain drops.

In other regions of the world (e.g. western Europe), runoff and erosion result from relatively low intensities of stratiform rainfall falling onto the previously saturated soil. In such situations, rainfall amount rather than intensity is the main factor determining the severity of soil erosion by water.

In Taiwan, where typhoon frequency increased significantly in the 21st century, a strong link has been drawn between the increase in storm frequency with an increase in sediment load in rivers and reservoirs, highlighting the impacts climate change can have on erosion.

Vegetation acts as an interface between the atmosphere and the soil. It increases the permeability of the soil to rainwater, thus decreasing runoff. It shelters the soil from winds, which results in decreased wind erosion, as well as advantageous changes in microclimate. The roots of the plants bind the soil together, and interweave with other roots, forming a more solid mass that is less susceptible to both water and wind erosion. The removal of vegetation increases the rate of surface erosion.

The topography of the land determines the velocity at which surface runoff will flow, which in turn determines the erosivity of the runoff. Longer, steeper slopes (especially those without adequate vegetative cover) are more susceptible to very high rates of erosion during heavy rains than shorter, less steep slopes. Steeper terrain is also more prone to mudslides, landslides, and other forms of gravitational erosion processes.

Tectonic processes control rates and distributions of erosion at the Earth's surface. If the tectonic action causes part of the Earth's surface (e.g., a mountain range) to be raised or lowered relative to surrounding areas, this must necessarily change the gradient of the land surface. Because erosion rates are almost always sensitive to the local slope (see above), this will change the rates of erosion in the uplifted area. Active tectonics also brings fresh, unweathered rock towards the surface, where it is exposed to the action of erosion.

However, erosion can also affect tectonic processes. The removal by erosion of large amounts of rock from a particular region, and its deposition elsewhere, can result in a lightening of the load on the lower crust and mantle. Because tectonic processes are driven by gradients in the stress field developed in the crust, this unloading can in turn cause tectonic or isostatic uplift in the region. In some cases, it has been hypothesised that these twin feedbacks can act to localize and enhance zones of very rapid exhumation of deep crustal rocks beneath places on the Earth's surface with extremely high erosion rates, for example, beneath the extremely steep terrain of Nanga Parbat in the western Himalayas. Such a place has been called a "tectonic aneurysm".

Human land development, in forms including agricultural and urban development, is considered a significant factor in erosion and sediment transport. In Taiwan, increases in sediment load in the northern, central, and southern regions of the island can be tracked with the timeline of development for each region throughout the 20th century.

Mountain ranges are known to take many millions of years to erode to the degree they effectively cease to exist. Scholars Pitman and Golovchenko estimate that it takes probably more than 450 million years to erode a mountain mass similar to the Himalaya into an almost-flat peneplain if there are no major sea-level changes. Erosion of mountains massifs can create a pattern of equally high summits called summit accordance. It has been argued that extension during post-orogenic collapse is a more effective mechanism of lowering the height of orogenic mountains than erosion.

Examples of heavily eroded mountain ranges include the Timanides of Northern Russia. Erosion of this orogen has produced sediments that are now found in the East European Platform, including the Cambrian Sablya Formation near Lake Ladoga. Studies of these sediments indicate that it is likely that the erosion of the orogen began in the Cambrian and then intensified in the Ordovician.

If the rate of erosion is higher than the rate of soil formation the soils are being destroyed by erosion. Where soil is not destroyed by erosion, erosion can in some cases prevent the formation of soil features that form slowly. Inceptisols are common soils that form in areas of fast erosion.

While erosion of soils is a natural process, human activities have increased by 10-40 times the rate at which erosion is occurring globally. Excessive (or accelerated) erosion causes both "on-site" and "off-site" problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes) ecological collapse, both because of loss of the nutrient-rich upper soil layers. In some cases, the eventual end result is desertification. Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems.




</doc>
<doc id="9697" url="https://en.wikipedia.org/wiki?curid=9697" title="Euclidean space">
Euclidean space

Euclidean space is the fundamental space of classical geometry. Originally it was the three-dimensional space of Euclidean geometry, but in modern mathematics there are Euclidean spaces of any nonnegative integer dimension, including the three-dimensional space and the "Euclidean plane" (dimension two). It was introduced by the Ancient Greek mathematician Euclid of Alexandria, and the qualifier "Euclidean" is used to distinguish it from other spaces that were later discovered in physics and modern mathematics. 

Ancient Greek geometers introduced Euclidean space for modeling the physical universe. Their great innovation was to "prove" all properties of the space as theorems by starting from a few fundamental properties, called "postulates", which either were considered as evident (for example, there is exactly one straight line passing through two points), or seemed impossible to prove (parallel postulate).

After the introduction at the end of 19th century of non-Euclidean geometries, the old postulates were re-formalized to define Euclidean spaces through axiomatic theory. Another definition of Euclidean spaces by means of vector spaces and linear algebra has been shown to be equivalent to the axiomatic definition. It is this definition that is more commonly used in modern mathematics, and detailed in this article.

In all definitions, Euclidean spaces consist of points, which are defined only by the properties that they must have for forming a Euclidean space.

There is essentially only one Euclidean space of each dimension; that is, all Euclidean spaces of a given dimension are isomorphic. Therefore, in many cases, it is possible to work with a specific Euclidean space, which is generally the real -space formula_1 equipped with the dot product. An isomorphism from a Euclidean space to formula_2 associates with each point an -tuple of real numbers which locate that point in the Euclidean space and are called the "Cartesian coordinates" of that point.

Euclidean space was introduced by ancient Greeks as an abstraction of our physical space. Their great innovation, appearing in Euclid's "Elements" was to build and "prove" all geometry by starting from a few very basic properties, which are abstracted from the physical world, and cannot be mathematically proved because of the lack of more basic tools. These properties are called postulates, or axioms in modern language. This way of defining Euclidean space is still in use under the name of synthetic geometry.

In 1637, René Descartes introduced Cartesian coordinates and showed that this allows reducing geometric problems to algebraic computations with numbers. This reduction of geometry to algebra was a major change of point of view, as, until then, the real numbers—that is, rational numbers and non-rational numbers together–were defined in terms of geometry, as lengths and distance. 

Euclidean geometry was not applied in spaces of more than three dimensions until the 19th century. Ludwig Schläfli generalized Euclidean geometry to spaces of "n" dimensions using both synthetic and algebraic methods, and discovered all of the regular polytopes (higher-dimensional analogues of the Platonic solids) that exist in Euclidean spaces of any number of dimensions.

Despite the wide use of Descartes' approach, which was called analytic geometry, the definition of Euclidean space remained unchanged until the end of 19th century. The introduction of abstract vector spaces allowed their use in defining Euclidean spaces with a purely algebraic definition. This new definition has been shown to be equivalent to the classical definition in terms of geometric axioms. It is this algebraic definition that is now most often used for introducing Euclidean spaces.

One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angles. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation around a fixed point in the plane, in which all points in the plane turn around that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).

In order to make all of this mathematically precise, the theory must clearly define what is a Euclidean space, and the related notions of distance, angle, translation, and rotation. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a "mathematical" space is a number, not something expressed in inches or metres. 

The standard way to mathematically define a Euclidean space, as carried out in the remainder of this article, is to define a Euclidean space as a set of points on which acts a real vector space, the "space of translations" which is equipped with an inner product. The action of translations makes the space an affine space, and this allows defining lines, planes, subspaces, dimension, and parallelism. The inner product allows defining distance and angles.

The set formula_2 of -tuples of real numbers equipped with the dot product is a Euclidean space of dimension . Conversely, the choice of a point called the "origin" and an orthonormal basis of the space of translations is equivalent with defining an isomorphism between a Euclidean space of dimension and formula_2 viewed as a Euclidean space.

It follows that everything that can be said about a Euclidean space can also be said about formula_5 Therefore, many authors, specially at elementary level, call formula_2 the "standard Euclidean space" of dimension , or simply "the" Euclidean space of dimension . 

A reason for introducing such an abstract definition of Euclidean spaces, and for working with it instead of formula_2 is that it is often preferable to work in a "coordinate-free" and "origin-free" manner (that is, without choosing a preferred basis and a preferred origin). Another reason is that there is no origin nor any basis in the physical world.

A is a finite-dimensional inner product space over the real numbers.

A Euclidean space is an affine space over the reals such that the associated vector space is a Euclidean vector space. Euclidean spaces are sometimes called "Euclidean affine spaces" for distinguishing them from Euclidean vector spaces.

If is a Euclidean space, its associated vector space is often denoted formula_8 The "dimension" of a Euclidean space is the dimension of its associated vector space.

The elements of are called "points" and are commonly denoted by capital letters. The elements of formula_9 are called "Euclidean vectors" or "free vectors". They are also called "translations", although, properly speaking, a translation is the geometric transformation resulting of the action of a Euclidean vector on the Euclidean space.

The action of a translation on a point provides a point that is denoted . This action satisfies 

The fact that the action is free and transitive means that for every pair of points there is exactly one vector such that . This vector is denoted or formula_11

As previously explained, some of the basic properties of Euclidean spaces result of the structure of affine space. They are described in and its subsections. The properties resulting from the inner product are explained in and its subsections.

For any vector space, the addition acts freely and transitively on the vector space itself. Thus a Euclidean vector space can be viewed as a Euclidean space that has itself as associated vector space.

A typical case of Euclidean vector space is formula_2 viewed as a vector space equipped with the dot product as an inner product. The importance of this particular example of Euclidean space lies in the fact that every Euclidean space is isomorphic to it. More precisely, given a Euclidean space of dimension , the choice of a point, called an "origin" and an orthonormal basis of formula_9 defines an isomorphism of Euclidean spaces from to formula_5

As every Euclidean space of dimension is isomorphic to it, the Euclidean space formula_2 is sometimes called the "standard Euclidean space" of dimension . 

Some basic properties of Euclidean spaces depend only of the fact that a Euclidean space is an affine space. They are called affine properties and include the concepts of lines, subspaces, and parallelism. which are detailed in next subsections.

Let be a Euclidean space and formula_9 its associated vector space. 

A "flat", "Euclidean subspace" or "affine subspace" of is a subset of such that

is a linear subspace of formula_8 A Euclidean subspace is a Euclidean space with formula_19 as associated vector space. This linear subspace formula_19 is called the "direction" of .

If is a point of then 

Conversely, if is a point of and is a linear subspace of formula_22 then

is a Euclidean subspace of direction .

A Euclidean vector space (that is, a Euclidean space such that formula_24) has two sorts of subspaces: its Euclidean subspaces and its linear subspaces. Linear subspaces are Euclidean subspaces and a Euclidean subspace is a linear subspace if and only if it contains the zero vector.

In a Euclidean space, a "line" is a Euclidean subspace of dimension one. Since a vector space of dimension one is spanned by any nonzero vector a line is a set of the form
where and are two distinct points.

It follows that "there is exactly one line that passes through (contains) two distinct points." This implies that two distinct lines intersect in at most one point.

A more symmetric representation of the line passing through and is 
where is an arbitrary point (not necessary on the line).

In a Euclidean vector space, the zero vector is usually chosen for ; this allows simplifying the preceding formula into
A standard convention allows using this formula in every Euclidean space, see .

The "line segment", or simply "segment", joining the points and is the subset of the points such that in the preceding formulas. It is denoted or ; that is

Two subspaces and of the same dimension in a Euclidean space are "parallel" if they have the same direction. Equivalently, they are parallel, if there is a translation vector that maps one to the other:

Given a point and a subspace , there exists exactly one subspace that contains and is parallel to , which is formula_30 In the case where is a line (subspace of dimension one), this property is Playfair's axiom.

It follows that in a Euclidean plane, two lines either meet in one point or are parallel.

The concept of parallel subspaces has been extended to subspaces of different dimensions: two subspaces are parallel if the direction of one of them is contained in the direction to the other.
The vector space formula_9 associated to a Euclidean space is an inner product space. This implies a symmetric bilinear form 
that is positive definite (that is formula_33 is always positive for ). 

The inner product of a Euclidean space is often called "dot product" and denoted . This is specially the case when a Cartesian coordinate system has been chosen, as, in this case, the inner product of two vectors is the dot product of their coordinate vectors. For this reason, and for historical reasons, the dot notation is more commonly used than the bracket notation for the inner product of Euclidean spaces. This article will follow this usage; that is formula_34 will be denoted in the remainder of this article.

The Euclidean norm of a vector is 

The inner product and the norm allows expressing and proving all metric and topological properties of Euclidean geometry. The next subsection describe the most fundamental ones. "In these subsections," "denotes an arbitrary Euclidean space, and formula_9 denotes its vector space of translations."

The "distance" (more precisely the "Euclidean distance") between two points of a Euclidean space is the norm of the translation vector that maps one point to the other; that is
The "length" of a segment is the distance between its endpoints. It is often denoted .

The distance is a metric, as it satisfies the triangular inequality
Moreover, the equality is true if and only if belongs to the segment . 
This inequality means that the length of any edge of a triangle is smaller than the sum of the lengths of the other edges. This is the origin of the term "triangular inequality".

With the Euclidean distance, every Euclidean space is a complete metric space.

Two nonzero vectors and of formula_9 are "perpendicular" or "orthogonal" if their inner product is zero:

Two linear subspaces of formula_9 are orthogonal if every nonzero vector of the first one is perpendicular to every nonzero vector of the second one. This implies that the intersection of the linear subspace is reduced to the zero vector.

Two lines, and more generally two Euclidean subspaces are orthogonal if their direction are orthogonal. Two orthogonal lines that intersect are said "perpendicular".

Two segments and that share a common endpoint are "perpendicular" or "form a right angle" if the vectors formula_42 and formula_43 are orthogonal.

If and form a right angle, one has
This is the Pythagorean theorem. Its proof is easy in this context, as, expressing this in terms of the inner product, one has, using bilinearity and symmetry of the inner product:

The (non-oriented) "angle" between two nonzero vectors and in formula_9 is 
where is the principal value of the arccosine function. By Cauchy–Schwarz inequality, the argument of the arcsine is in the interval . Therefore is real, and (or } if angles are measured in degrees).

Angles are not useful in a Euclidean line, as they can be only 0 or "". 

In an oriented Euclidean plane, one can define the "oriented angle" of two vectors. The oriented angle of two vectors and is then the opposite of the oriented angle of and . In this case, the angle of two vectors can have any value modulo an integer multiple of . In particular, a reflex angle equals the negative angle .

The angle of two vectors does not change if they are multiplied by positive numbers. More precisely, if and are two vectors, and and are real numbers, then

If and are three points in a Euclidean space, the angle of the segments and is the angle of the vectors formula_42 and formula_50 As the multiplication of vectors by positive numbers do not change the angle, the angle of two half-lines with initial point can be defined: it is the angle of the segments and , where and are arbitrary points, one on each half-line. Although this is less used, one can define similarly the angle of segments or half-lines that do not share an initial points.

The angle of two lines is defined as follows. If is the angle of two segments, one on each line, the angle of any two other segments, one on each line, is either or . One of these angles is in the interval , and the other being in . The "non-oriented angle" of the two lines is the one in the interval . In an oriented Euclidean plane, the "oriented angle" of two lines belongs to the interval .

Every Euclidean vector space has an orthonormal basis (in fact, infinitely many in dimension higher than one, and two in dimension one), that is a basis formula_51 of unit vectors (formula_52) that are pairwise orthogonal (formula_53 for ). More precisely, given any basis formula_54 the Gram–Schmidt process computes an orthonormal basis such that, for every , the linear spans of formula_55 and formula_56 are equal.

Given a Euclidean space , a "Cartesian frame" is a set of data consisting of an orthonormal basis of formula_22 and a point of , called the "origin" and often denoted . A Cartesian frame formula_58 allows defining Cartesian coordinates for both and formula_9 in the following way.

The Cartesian coordinates of a vector are the coefficients of on the basis formula_60 As the basis is orthonormal, the th coefficient is the dot product formula_61

The Cartesian coordinates of a point of are the Cartesian coordinates of the vector formula_62

As a Euclidean space is an affine space, one can consider an affine frame on it, which is the same as a Euclidean frame, except that the basis is not required to be orthonormal. This define affine coordinates, sometimes called "skew coordinates" for emphasizing that the basis vectors are not pairwise orthogonal.

An affine basis of a Euclidean space of dimension is a set of points that are not contained in a hyperplan. An affine basis define barycentric coordinates for every point. 

Many other coordinates systems can be defined on a Euclidean space of dimension , in the following way. Let be a homeomorphism (or, more often, a diffeomorphism) from a dense open subset of to an open subset of formula_5 The "coordinates" of a point of are the components of . The polar coordinate system (dimension 2) and the spherical and cylindrical coordinate systems (dimension 3) are defined this way.

For points that are outside the domain of , coordinates may sometimes be defined as the limit of coordinates of neighbour points, but these coordinates may be not uniquely defined, and may be not continuous in the neighborhood of the point. For example, for the spherical coordinate system, the longitude is not defined at the pole, and on the antimeridian, the longitude passes discontinuously from –180° to +180°.

This way of defining coordinates extends easily to other mathematical structures, and in particular to manifolds.

An isometry between two metric spaces is a bijection preserving the distance, that is 

In the case of a Euclidean vector space, an isometry that maps the origin to the origin preserves the norm
since the norm of a vector is its distance from the zero vector. It preserves also the inner product
since 

An isometry Euclidean vector spaces is a linear isomorphism.

An isometry formula_68 of Euclidean spaces defines an isometry 
formula_69 of the associated Euclidean vector spaces. This implies that two isometric Euclidean spaces have the same dimension. Conversely, if and are Euclidean spaces, , , and formula_69 is an isometry, then the map formula_68 defined by 
is an isometry of Euclidean spaces.

It follows from the preceding results that an isometry of Euclidean spaces maps lines to lines, and, more generally Euclidean subspaces to Euclidean subspaces of the same dimension, and that the restriction of the isometry on these subspaces are isometries of these subspaces.

If is a Euclidean space, its associated vector space formula_9 can be considered as a Euclidean space. Every point defines an isometry of Euclidean spaces
which maps to the zero vector and has the identity as associated linear map. The inverse isometry is the map 

A Euclidean frame allows defining the map
which is an isometry of Euclidean spaces. The inverse isometry is 

"This means that, up to an isomorphism, there is exactly one Euclidean space of a given dimension." 

This justifies that many authors talk of formula_2 as "the" Euclidean space of dimension .

An isometry from a Euclidean space onto itself is called "Euclidean isometry", "Euclidean transformation" or "rigid transformation". The rigid transformations of a Euclidean space form a group (under composition), called the "Euclidean group" and often denoted of .

The simplest Euclidean transformations are translations 
They are in bijective correspondence with vectors. This is a reason for calling "space of translations" the vector space associated to a Euclidean space. The translations form a normal subgroup of the Euclidean group.

A Euclidean isometry of a Euclidean space defines a linear isometry formula_80 of the associated vector space (by "linear isometry", it is meant an isometry that is also a linear map) in the following way: denoting by the vector formula_81, if is an arbitrary point of , one has 
It is straightforward to prove that this is a linear map that does not depend from the choice of 

The map formula_83 is a group homomorphism from the Euclidean group onto the group of linear isometries, called the orthogonal group. The kernel of this homomorphism is the translation group, showing that it is a normal subgroup of the Euclidean group.

The isometries that fix a given point form the stabilizer subgroup of the Euclidean group with respect to . The restriction to this stabilizer of above group homomorphism is an isomorphism. So the isometries that fix a given point form a group isomorphic to the orthogonal group.

Let be a point, an isometry, and the translation that maps to . The isometry formula_84 fixes . So formula_85 and "the Euclidean group is the semidirect product of the translation group and the orthogonal group."

The special orthogonal group is the normal subgroup of the orthogonal group that preserves handedness. It is a subgroup of index two of the orthogonal group. Its inverse image by the group homomorphism formula_83 is a normal subgroup of index two of the Euclidean group, which is called the "special Euclidean group" or the "displacement group". Its elements are called "rigid motions" or "displacements".

Rigid motions include the identity, translations, rotations (the rigid motions that fix at least a point), and also screw motions.

Typical examples of rigid transformations that are not rigid motions are reflections, which are rigid transformations that fix a hyperplane and are not the identity. They are also the transformations consisting in changing the sign of one coordinate over some Euclidean frame.

As the special Euclidean group is a subgroup of index two of the Euclidean group, given a reflection , every rigid transformation that is not a rigid motion is the product of and a rigid motion. A glide reflection is an example of a rigid transformation that is not a rigid motion or a reflection.

All groups that have been considered in this section are Lie groups and algebraic groups.

The Euclidean distance makes a Euclidean space a metric space, and thus a topological space. This topology is called the Euclidean topology. In the case of formula_1 this topology is also the product topology. 

The open sets are the subsets that contains an open ball around each of their points. In other words, open balls form a base of the topology.

The topological dimension of a Euclidean space equals its dimension. This implies that Euclidean spaces of different dimensions are not homeomorphic. Moreover, the theorem of invariance of domain asserts that a subset of a Euclidean space is open (for the subspace topology) if and only if it is homeomorphic to an open subset of a Euclidean space of the same dimension.

Euclidean spaces are complete and locally compact. That is, a closed subset of a Euclidean space is compact if it is bounded (that is, contained in a ball). In particular, closed balls are compact.

The definition of Euclidean spaces that has been described in this article differs fundamentally of Euclid's one. In reality, Euclid did not define formally the space, because it was thought as a description of the physical world that exists independently of human mind. The need of a formal definition appeared only at the end of 19th century, with the introduction of non-Euclidean geometries. 

Two different approaches have been used. Felix Klein suggested to define geometries through their symmetries. The presentation of Euclidean spaces given in this article, is essentially issued from his Erlangen program, with the emphasis given on the groups of translations and isometries.

On the other hand, David Hilbert proposed a set of axioms, inspired by Euclid's postulates. They belong to synthetic geometry, as they do not involve any definition of real numbers. Later G. D. Birkhoff and Alfred Tarski proposed simpler sets of axioms, which use real numbers (see Birkhoff's axioms and Tarski's axioms).

In "Geometric Algebra", Emil Artin has proved that all these definitions of a Euclidean space are equivalent. It is rather easy to prove that all definitions of Euclidean spaces satisfy Hilbert's axioms, and that those involving real numbers (including the above given definition) are equivalent. The difficult part of Artin's proof is the following. In Hilbert's axioms, congruence is an equivalence relation on segments. One can thus define the "length" of a segment as its equivalence class. One must thus prove that this length satisfies properties that characterize nonnegative real numbers. It is what did Artin, with axioms that are not Hilbert's ones, but are equivalent.

Since ancient Greeks, Euclidean space is used for modeling shapes in the physical world. It is thus used in many sciences such as physics, mechanics, and astronomy. It is also widely used in all technical areas that are concerned with shapes, figure, location and position, such as architecture, geodesy, topography, navigation, industrial design, or technical drawing.

Space of dimensions higher than three occurs in several modern theories of physics; see Higher dimension. They occur also in configuration spaces of physical systems.

Beside Euclidean geometry, Euclidean spaces are also widely used in other areas of mathematics. Tangent spaces of differentiable manifolds are Euclidean vector spaces. More generally, a manifold is a space that is locally approximated by Euclidean spaces. Most non-Euclidean geometries can be modeled by a manifold, and embedded in a Euclidean space of higher dimension. For example, an elliptic space can be modeled by an ellipsoid. It is common to represent in a Euclidean space mathematics objects that are "a priori" not of a geometrical nature. An example among many is the usual representation of graphs.

Since the introduction, at the end of 19th century, of Non-Euclidean geometries, many sorts of spaces have been considered, about which one can do geometric reasoning in the same way as with Euclidean spaces. In general, they share some properties with Euclidean spaces, but may also have properties that could appear as rather strange. Some of these spaces use Euclidean geometry for their definition, or can be modeled as subspaces of a Euclidean space of higher dimension. When such a space is defined by geometrical axioms, embedding the space in a Euclidean space is a standard way for proving consistency of its definition, or, more precisely for proving that its theory is consistent, if Euclidean geometry is consistent (which cannot be proved).

A Euclidean space is an affine space equipped with a metric. Affine spaces have many other uses in mathematics. In particular, as they are defined over any field, they allow doing geometry in other contexts.

As soon as non-linear questions are considered, it is generally useful to consider affine spaces over the complex numbers as an extension of Euclidean spaces. For example, a circle and a line have always two intersection points (possibly not distinct) in the complex affine space. Therefore, most of algebraic geometry is built in complex affine spaces and affine spaces over algebraically closed fields. The shapes that are studied in algebraic geometry in these affine spaces are therefore called affine algebraic varieties.

Affine spaces over the rational numbers and more generally over algebraic number fields provide a link between (algebraic) geometry and number theory. For example, the Fermat's Last Theorem can be stated "a Fermat curve of degree higher than two has no point in the affine plane over the rationals."

Geometry in affine spaces over a finite fields has also been widely studied. For example, elliptic curves over finite fields are widely used in cryptography.

Originally, projective spaces have been introduced by adding "points at infinity" to Euclidean spaces, and, more generally to affine spaces, in order to make true the assertion "two coplanar lines meet in exactly one point". Projective space share with Euclidean and affine spaces the property of being isotropic, that is, there is no property of the space that allows distinguishing between two points or two lines. Therefore, a more isotropic definition is commonly used, which consists as defining a projective space as the set of the vector lines in a vector space of dimension one more.

As for affine spaces, projective spaces are defined over any field, and are fundamental spaces of algebraic geometry.

"Non-Euclidean geometry" refers usually to geometrical spaces where the parallel postulate is false. They include elliptic geometry, where the sum of the angles of a triangle is more than 180°, and hyperbolic geometry, where this sum is less than 180°. Their introduction in the second half of 19th century, and the proof that their theory is consistent (if Euclidean geometry is not contradictory) is one of the paradoxes that are at the origin of the foundational crisis in mathematics of the beginning of 20th century, and motivated the systematization of axiomatic theories in mathematics.

A manifold is a space that in the neighborhood of each point resembles a Euclidean space. In technical terms, a manifold is a topological space, such that each point has a neighborhood that is homeomorphic to an open subset of a Euclidean space. Manifold can be classified by increasing degree of this "resemblance" into topological manifolds, differentiable manifolds, smooth manifolds, and analytic manifolds. However, none of these types of "resemblance" respect distances and angles, even approximately. 

Distances and angles can be defined on a smooth manifold by providing a smoothly varying Euclidean metric on the tangent spaces at the points of the manifold (these tangent are thus Euclidean vector spaces). This results in a Riemannian manifold. Generally, straight lines do not exist in a Riemannian manifold, but their role is played by geodesics, which are the "shortest paths" between two points. This allows defining distances, which are measured along geodesics, and angles between geodesics, which are the angle of their tangents in the tangent space at their intersection. So, Riemannian manifolds behave locally like a Euclidean that has been bended.

Euclidean spaces are trivially Riemannian manifolds. An example illustrating this well is the surface of a sphere. In this case, geodesics are arcs of great circle, which are called orthodromes in the context of navigation. More generally, the spaces of non-Euclidean geometries can be realized as Riemannian manifolds.

The inner product that is defined to define Euclidean spaces is a positive definite bilinear form. If it is replaced by an indefinite quadratic form which is non-degenerate, one gets a pseudo-Euclidean space. 

A fundamental example of such a space is the Minkowski space, which is the space-time of Einstein's special relativity. It is a four-dimensional space, where the metric is defined by the quadratic form

where the last coordinate ("t") is temporal, and the other three ("x", "y", "z") are spatial.

To take the gravity into account, general relativity uses a pseudo-Riemannian manifold that has Minkowski spaces as tangent spaces. The curvature of this manifold at a point is a function of the value of the gravitational field at this point.




</doc>
<doc id="9700" url="https://en.wikipedia.org/wiki?curid=9700" title="Edwin Austin Abbey">
Edwin Austin Abbey

Edwin Austin Abbey (April 1, 1852August 1, 1911) was an American muralist, illustrator, and painter. He flourished at the beginning of what is now referred to as the "golden age" of illustration, and is best known for his drawings and paintings of Shakespearean and Victorian subjects, as well as for his painting of Edward VII's coronation. His most famous set of murals, "The Quest and Achievement of the Holy Grail", adorns the Boston Public Library.

Abbey was born in Philadelphia in 1852. He studied art at the Pennsylvania Academy of the Fine Arts under Christian Schuessele. Abbey began as an illustrator, producing numerous illustrations and sketches for such magazines as Harper's Weekly (1871–1874) and Scribner's Magazine. His illustrations began appearing in Harper's Weekly at an early age: before Abbey was twenty years old. He moved to New York City in 1871. His illustrations were strongly influenced by French and German black and white art. He also illustrated several best-selling books, including "Christmas Stories" by Charles Dickens (1875), "Selections from the Poetry of Robert Herrick" (1882), and "She Stoops to Conquer" by Oliver Goldsmith (1887). Abbey also illustrated a four-volume set of "The Comedies of Shakespeare" for Harper & Brothers in 1896.

He moved to England in 1878, at the request of his employers, to gather material for illustrations of the poems of Robert Herrick, published in 1882, and he settled permanently there in 1883. In 1883, he was elected to the Royal Institute of Painters in Water-Colours. About this time, he was appraised critically by the American writer, S.G.W. Benjamin:
He also created illustrations for Goldsmith's "She Stoops to Conquer" (1887), for a volume of "Old Songs" (1889), and for the comedies (and a few of the tragedies) of Shakespeare. Among his water-colours are "The Evil Eye" (1877), "The Rose in October" (1879), "An Old Song" (1886), "The Visitors" (1890), and "The Jongleur" (1892). Possibly his best known pastels are "Beatrice," "Phyllis," and "Two Noble Kinsmen."
In 1890 he made his first appearance with an oil painting, "A May Day Morn," at the Royal Academy in London. He exhibited "Richard duke of Gloucester and the Lady Anne" there in 1896, and in that year was elected A.R.A., becoming a full member in 1898. In 1902 he was chosen to paint the coronation of King Edward VII. It was the official painting of the occasion and, hence, resides at Buckingham Palace. He did receive a knighthood, although some say he refused it in 1907. Friendly with other expatriate American artists, he summered at Broadway, Worcestershire, England, where he painted and vacationed alongside John Singer Sargent at the home of Francis Davis Millet.

He completed murals for the Boston Public Library in the 1890s. The frieze for the Library was titled "The Quest and Achievement of the Holy Grail". It took Abbey eleven years to complete this series of murals in his England studio.

In 1904 he painted a mural for the Royal Exchange, London "Reconciliation of the Skinners & Merchant Taylors' Companies by Lord Mayor Billesden, 1484".

In 1908–09, Abbey began an ambitious program of murals and other artworks for the newly completed Pennsylvania State Capitol in Harrisburg, Pennsylvania. These included allegorical medallion murals representing "Science", "Art", "Justice", and "Religion" for the dome of the Rotunda, four large lunette murals beneath the dome, and multiple works for the House and Senate Chambers. For the Senate chamber he finished only one painting, "Von Steuben Training the American Soldiers at Valley Forge", and he was working on the "Reading of the Declaration of Independence" mural in early 1911, when his health began to fail. He was diagnosed with cancer. Studio assistant William Simmonds continued work on the mural with little supervision from Abbey, and with small contributions by John Singer Sergeant.

Abbey died in August 1911. William Simmonds travelled from England to install the completed murals with Abbey's widow Gertrude. The remaining two rooms, which Abbey had been unable to finish, were given to Violet Oakley, who completed the commission using her own designs.
Abbey was elected to the National Academy of Design, in 1902, and The American Academy of Arts and Letters. He was honorary member of the Royal Bavarian Society and the Société Nationale des Beaux-Arts, and was made a chevalier of the French Legion of Honour. He was a prolific illustrator, and attention to detail, including historical accuracy, influenced successive generations of illustrators.

In 1890, Edwin married Gertrude Mead, the daughter of a wealthy New York merchant. Mrs Abbey encouraged her husband to secure more ambitious commissions, although with their marriage commencing when both were in their forties, the couple remained childless. After her husband's death, Gertrude was active in preserving her husband's legacy, writing about his work and giving her substantial collection and archive to Yale. Edwin had been a keen supporter of the newly founded British School at Rome (BSR), so, in his memory, she donated £6000 to assist in building the artists' studio block and, in 1926, founded the Incorporated Edwin Austin Abbey Memorial Scholarships. The scholarships were established to enable British and American painters to pursue their practice. Recipients of Abbey funding – Scholars and, more recently, Fellows – devote their scholarship to working in the studios at the BSR, where there has, ever since, been at least one Abbey-funded artist in residence. Previous award holders include Stephen Farthing, Chantal Joffe and Spartacus Chetwynd. The Abbey Fellowships (formerly 'Awards') were established in their present form in 1990, and the Abbey studios also host the BSR's other fine art residencies, such as the Derek Hill Foundation Scholarship and the Sainsbury Scholarship in Painting and Drawing. A bust of Edwin Abbey, by Sir Thomas Brock, stands in the courtyard of the BSR. Edwin also left bequests of his works to the Metropolitan Museum of Art in New York, to the Museum of Fine Arts, Boston and to the National Gallery in London.

Abbey is buried in the churchyard of Old St Andrew's Church in Kingsbury, London. His grave is Grade II listed.





</doc>
<doc id="9703" url="https://en.wikipedia.org/wiki?curid=9703" title="Evolutionary psychology">
Evolutionary psychology

Evolutionary psychology is a theoretical approach in the social and natural sciences that examines psychological structure from a modern evolutionary perspective. It seeks to identify which human psychological traits are evolved adaptations – that is, the functional products of natural selection or sexual selection in human evolution. Adaptationist thinking about physiological mechanisms, such as the heart, lungs, and immune system, is common in evolutionary biology. Some evolutionary psychologists apply the same thinking to psychology, arguing that the modularity of mind is similar to that of the body and with different modular adaptations serving different functions. These evolutionary psychologists argue that much of human behavior is the output of psychological adaptations that evolved to solve recurrent problems in human ancestral environments.

Evolutionary psychology is not simply a subdiscipline of psychology but its evolutionary theory can provide a foundational, metatheoretical framework that integrates the entire field of psychology in the same way evolutionary biology has for biology.

Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations including the abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, and cooperate with others. There have been studies of human social behaviour related to infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price, and parental investment, with impressive findings. The theories and findings of evolutionary psychology have applications in many fields, including economics, environment, health, law, management, psychiatry, politics, and literature.

Criticism of evolutionary psychology involves questions of testability, cognitive and evolutionary assumptions (such as modular functioning of the brain, and large uncertainty about the ancestral environment), importance of non-genetic and non-adaptive explanations, as well as political and ethical issues due to interpretations of research results.

Evolutionary psychology is an approach that views human nature as the product of a universal set of evolved psychological adaptations to recurring problems in the ancestral environment. Proponents suggest that it seeks to integrate psychology into the other natural sciences, rooting it in the organizing theory of biology (evolutionary theory), and thus understanding psychology as a branch of biology. Anthropologist John Tooby and psychologist Leda Cosmides note:

Evolutionary psychology is the long-forestalled scientific attempt to assemble out of the disjointed, fragmentary, and mutually contradictory human disciplines a single, logically integrated research framework for the psychological, social, and behavioral sciences – a framework
that not only incorporates the evolutionary sciences on a full and equal basis, but that systematically works out all of the revisions in existing belief and research practice that such a synthesis requires.

Just as human physiology and evolutionary physiology have worked to identify physical adaptations of the body that represent "human physiological nature," the purpose of evolutionary psychology is to identify evolved emotional and cognitive adaptations that represent "human psychological nature." According to Steven Pinker, it is "not a single theory but a large set of hypotheses" and a term that "has also come to refer to a particular way of applying evolutionary theory to the mind, with an emphasis on adaptation, gene-level selection, and modularity." Evolutionary psychology adopts an understanding of the mind that is based on the computational theory of mind. It describes mental processes as computational operations, so that, for example, a fear response is described as arising from a neurological computation that inputs the perceptional data, e.g. a visual image of a spider, and outputs the appropriate reaction, e.g. fear of possibly dangerous animals. Under this view, any Domain-general learning is impossible because of the Combinatorial explosion. This implies Domain-specific learning. Evolutionary Psychology specifies the domain as the problems of survival and reproduction.

While philosophers have generally considered the human mind to include broad faculties, such as reason and lust, evolutionary psychologists describe evolved psychological mechanisms as narrowly focused to deal with specific issues, such as catching cheaters or choosing mates. The discipline views the human brain as comprising many functional mechanisms, called "psychological adaptations" or evolved cognitive mechanisms or "cognitive modules", designed by the process of natural selection. Examples include language-acquisition modules, incest-avoidance mechanisms, cheater-detection mechanisms, intelligence and sex-specific mating preferences, foraging mechanisms, alliance-tracking mechanisms, agent-detection mechanisms, and others. Some mechanisms, termed "domain-specific", deal with recurrent adaptive problems over the course of human evolutionary history. "Domain-general" mechanisms, on the other hand, are proposed to deal with evolutionary novelty.

Evolutionary psychology has roots in cognitive psychology and evolutionary biology but also draws on behavioral ecology, artificial intelligence, genetics, ethology, anthropology, archaeology, biology, and zoology. It is closely linked to sociobiology, but there are key differences between them including the emphasis on "domain-specific" rather than "domain-general" mechanisms, the relevance of measures of current fitness, the importance of mismatch theory, and psychology rather than behavior. Most of what is now labeled as sociobiological research is now confined to the field of behavioral ecology.

Nikolaas Tinbergen's four categories of questions can help to clarify the distinctions between several different, but complementary, types of explanations. Evolutionary psychology focuses primarily on the "why?" questions, while traditional psychology focuses on the "how?" questions.

Evolutionary psychology is founded on several core premises.


Evolutionary psychology has its historical roots in Charles Darwin's theory of natural selection. In "The Origin of Species", Darwin predicted that psychology would develop an evolutionary basis:

Two of his later books were devoted to the study of animal emotions and psychology; "The Descent of Man, and Selection in Relation to Sex" in 1871 and "The Expression of the Emotions in Man and Animals" in 1872. Darwin's work inspired William James's functionalist approach to psychology. Darwin's theories of evolution, adaptation, and natural selection have provided insight into why brains function the way they do.

The content of evolutionary psychology has derived from, on the one hand, the biological sciences (especially evolutionary theory as it relates to ancient human environments, the study of paleoanthropology and animal behavior) and, on the other, the human sciences, especially psychology.

Evolutionary biology as an academic discipline emerged with the modern synthesis in the 1930s and 1940s. In the 1930s the study of animal behavior (ethology) emerged with the work of the Dutch biologist Nikolaas Tinbergen and the Austrian biologists Konrad Lorenz and Karl von Frisch.

W.D. Hamilton's (1964) papers on inclusive fitness and Robert Trivers's (1972) theories on reciprocity and parental investment helped to establish evolutionary thinking in psychology and the other social sciences. In 1975, Edward O. Wilson combined evolutionary theory with studies of animal and social behavior, building on the works of Lorenz and Tinbergen, in his book "".

In the 1970s, two major branches developed from ethology. Firstly, the study of animal "social" behavior (including humans) generated sociobiology, defined by its pre-eminent proponent Edward O. Wilson in 1975 as "the systematic study of the biological basis of all social behavior" and in 1978 as "the extension of population biology and evolutionary theory to social organization." Secondly, there was behavioral ecology which placed less emphasis on "social" behavior; it focused on the ecological and evolutionary basis of animal and human behavior.

In the 1970s and 1980s university departments began to include the term "evolutionary biology" in their titles. The modern era of evolutionary psychology was ushered in, in particular, by Donald Symons' 1979 book "The Evolution of Human Sexuality" and Leda Cosmides and John Tooby's 1992 book "The Adapted Mind". David Buller observed that the term "evolutionary psychology" is sometimes seen as denoting research based on the specific methodological and theoretical commitments of certain researchers from the Santa Barbara school (University of California), thus some evolutionary psychologists prefer to term their work "human ecology", "human behavioural ecology" or "evolutionary anthropology" instead.

From psychology there are the primary streams of developmental, social and cognitive psychology. Establishing some measure of the relative influence of genetics and environment on behavior has been at the core of behavioral genetics and its variants, notably studies at the molecular level that examine the relationship between genes, neurotransmitters and behavior. Dual inheritance theory (DIT), developed in the late 1970s and early 1980s, has a slightly different perspective by trying to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. DIT is seen by some as a "middle-ground" between views that emphasize human universals versus those that emphasize cultural variation.

The theories on which evolutionary psychology is based originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.

Evolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively high maximal potential reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively low maximal potential reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, and he relied on group selection to explain the evolution of altruistic (self-sacrificing) behavior. But group selection was considered a weak explanation, because in any group the less altruistic individuals will be more likely to survive, and the group will become less self-sacrificing as a whole.

In 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a gene-centered view of evolution. Hamilton noted that genes can increase the replication of copies of themselves into the next generation by influencing the organism's social traits in such a way that (statistically) results in helping the survival and reproduction of other copies of the same genes (most simply, identical copies in the organism's close relatives). According to Hamilton's rule, self-sacrificing behaviors (and the genes influencing them) can evolve if they typically help the organism's close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how altruism can evolve. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories help to explain the development of altruistic behavior, and account for hostility toward cheaters (individuals that take advantage of others' altruism).

Several mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring, while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers–Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture, and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).

Evolutionary psychology is based on the hypothesis that, just like hearts, lungs, livers, kidneys, and immune systems, cognition has functional structure that has a genetic basis, and therefore has evolved by natural selection. Like other organs and tissues, this functional structure should be universally shared amongst a species, and should solve important problems of survival and reproduction.

Evolutionary psychologists seek to understand psychological mechanisms by understanding the survival and reproductive functions they might have served over the course of evolutionary history. These might include abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, cooperate with others and follow leaders. Consistent with the theory of natural selection, evolutionary psychology sees humans as often in conflict with others, including mates and relatives. For instance, a mother may wish to wean her offspring from breastfeeding earlier than does her infant, which frees up the mother to invest in additional offspring. Evolutionary psychology also recognizes the role of kin selection and reciprocity in evolving prosocial traits such as altruism. Like chimpanzees and bonobos, humans have subtle and flexible social instincts, allowing them to form extended families, lifelong friendships, and political alliances. In studies testing theoretical predictions, evolutionary psychologists have made modest findings on topics such as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price and parental investment.

Proponents of evolutionary psychology in the 1990s made some explorations in historical events, but the response from historical experts was highly negative and there has been little effort to continue that line of research. Historian Lynn Hunt says that the historians complained that the researchers:

Hunt states that, "the few attempts to build up a subfield of psychohistory collapsed under the weight of its presuppositions." She concludes that as of 2014 the "'iron curtain' between historians and psychology...remains standing."

Not all traits of organisms are evolutionary adaptations. As noted in the table below, traits may also be exaptations, byproducts of adaptations (sometimes called "spandrels"), or random variation between individuals.

Psychological adaptations are hypothesized to be innate or relatively easy to learn, and to manifest in cultures worldwide. For example, the ability of toddlers to learn a language with virtually no training is likely to be a psychological adaptation. On the other hand, ancestral humans did not read or write, thus today, learning to read and write require extensive training, and presumably involves the repurposing of cognitive capacities that evolved in response to selection pressures unrelated to written language. However, variations in manifest behavior can result from universal mechanisms interacting with different local environments. For example, Caucasians who move from a northern climate to the equator will have darker skin. The mechanisms regulating their pigmentation do not change; rather the input to those mechanisms change, resulting in different output.

One of the tasks of evolutionary psychology is to identify which psychological traits are likely to be adaptations, byproducts or random variation. George C. Williams suggested that an "adaptation is a special and onerous concept that should only be used where it is really necessary." As noted by Williams and others, adaptations can be identified by their improbable complexity, species universality, and adaptive functionality.

A question that may be asked about an adaptation is whether it is generally obligate (relatively robust in the face of typical environmental variation) or facultative (sensitive to typical environmental variation). The sweet taste of sugar and the pain of hitting one's knee against concrete are the result of fairly obligate psychological adaptations; typical environmental variability during development does not much affect their operation. By contrast, facultative adaptations are somewhat like "if-then" statements. For example, adult attachment style seems particularly sensitive to early childhood experiences. As adults, the propensity to develop close, trusting bonds with others is dependent on whether early childhood caregivers could be trusted to provide reliable assistance and attention. The adaptation for skin to tan is conditional to exposure to sunlight; this is an example of another facultative adaptation. When a psychological adaptation is facultative, evolutionary psychologists concern themselves with how developmental and environmental inputs influence the expression of the adaptation.

Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Cultural universals include behaviors related to language, cognition, social roles, gender roles, and technology. Evolved psychological adaptations (such as the ability to learn a language) interact with cultural inputs to produce specific behaviors (e.g., the specific language learned).

Basic gender differences, such as greater eagerness for sex among men and greater coyness among women, are explained as sexually dimorphic psychological adaptations that reflect the different reproductive strategies of males and females. Research has also shown participants were able to recognize the facial expression of fear significantly better on a male face than on a female face. Females also recognized fear generally better than males.

Evolutionary psychologists contrast their approach to what they term the "standard social science model," according to which the mind is a general-purpose cognition device shaped almost entirely by culture.

Evolutionary psychology argues that to properly understand the functions of the brain, one must understand the properties of the environment in which the brain evolved. That environment is often referred to as the "environment of evolutionary adaptedness".

The idea of an "environment of evolutionary adaptedness" was first explored as a part of attachment theory by John Bowlby. This is the environment to which a particular evolved mechanism is adapted. More specifically, the environment of evolutionary adaptedness is defined as the set of historically recurring selection pressures that formed a given adaptation, as well as those aspects of the environment that were necessary for the proper development and functioning of the adaptation.

Humans, comprising the genus "Homo", appeared between 1.5 and 2.5 million years ago, a time that roughly coincides with the start of the Pleistocene 2.6 million years ago. Because the Pleistocene ended a mere 12,000 years ago, most human adaptations either newly evolved during the Pleistocene, or were maintained by stabilizing selection during the Pleistocene. Evolutionary psychology therefore proposes that the majority of human psychological mechanisms are adapted to reproductive problems frequently encountered in Pleistocene environments. In broad terms, these problems include those of growth, development, differentiation, maintenance, mating, parenting, and social relationships.

The environment of evolutionary adaptedness is significantly different from modern society. The ancestors of modern humans lived in smaller groups, had more cohesive cultures, and had more stable and rich contexts for identity and meaning. Researchers look to existing hunter-gatherer societies for clues as to how hunter-gatherers lived in the environment of evolutionary adaptedness. Unfortunately, the few surviving hunter-gatherer societies are different from each other, and they have been pushed out of the best land and into harsh environments, so it is not clear how closely they reflect ancestral culture. However, all around the world small-band hunter-gatherers offer a similar developmental system for the young ("hunter-gatherer childhood model," Konner, 2005; 
"evolved developmental niche" or "evolved nest;" Narvaez et al., 2013). The characteristics of the niche are largely the same as for social mammals, who evolved over 30 million years ago: soothing perinatal experience, several years of on-request breastfeeding, nearly constant affection or physical proximity, responsiveness to need (mitigating offspring distress), self-directed play, and for humans, multiple responsive caregivers. Initial studies show the importance of these components in early life for positive child outcomes.

Evolutionary psychologists sometimes look to chimpanzees, bonobos, and other great apes for insight into human ancestral behavior.

Since an organism's adaptations were suited to its ancestral environment, a new and different environment can create a mismatch. Because humans are mostly adapted to Pleistocene environments, psychological mechanisms sometimes exhibit "mismatches" to the modern environment. One example is the fact that although about 10,000 people are killed with guns in the US annually, whereas spiders and snakes kill only a handful, people nonetheless learn to fear spiders and snakes about as easily as they do a pointed gun, and more easily than an unpointed gun, rabbits or flowers. A potential explanation is that spiders and snakes were a threat to human ancestors throughout the Pleistocene, whereas guns (and rabbits and flowers) were not. There is thus a mismatch between humans' evolved fear-learning psychology and the modern environment.

This mismatch also shows up in the phenomena of the supernormal stimulus, a stimulus that elicits a response more strongly than the stimulus for which the response evolved. The term was coined by Niko Tinbergen to refer to non-human animal behavior, but psychologist Deirdre Barrett said that supernormal stimulation governs the behavior of humans as powerfully as that of other animals. She explained junk food as an exaggerated stimulus to cravings for salt, sugar, and fats, and she says that television is an exaggeration of social cues of laughter, smiling faces and attention-grabbing action. Magazine centerfolds and double cheeseburgers pull instincts intended for an environment of evolutionary adaptedness where breast development was a sign of health, youth and fertility in a prospective mate, and fat was a rare and vital nutrient. The psychologist Mark van Vugt recently argued that modern organizational leadership is a mismatch. His argument is that humans are not adapted to work in large, anonymous bureaucratic structures with formal hierarchies. The human mind still responds to personalized, charismatic leadership primarily in the context of informal, egalitarian settings. Hence the dissatisfaction and alienation that many employees experience. Salaries, bonuses and other privileges exploit instincts for relative status, which attract particularly males to senior executive positions.

Evolutionary theory is heuristic in that it may generate hypotheses that might not be developed from other theoretical approaches. One of the major goals of adaptationist research is to identify which organismic traits are likely to be adaptations, and which are byproducts or random variations. As noted earlier, adaptations are expected to show evidence of complexity, functionality, and species universality, while byproducts or random variation will not. In addition, adaptations are expected to manifest as proximate mechanisms that interact with the environment in either a generally obligate or facultative fashion (see above). Evolutionary psychologists are also interested in identifying these proximate mechanisms (sometimes termed "mental mechanisms" or "psychological adaptations") and what type of information they take as input, how they process that information, and their outputs. Evolutionary developmental psychology, or "evo-devo," focuses on how adaptations may be activated at certain developmental times (e.g., losing baby teeth, adolescence, etc.) or how events during the development of an individual may alter life history trajectories.

Evolutionary psychologists use several strategies to develop and test hypotheses about whether a psychological trait is likely to be an evolved adaptation. Buss (2011) notes that these methods include:

<poem>"Cross-cultural Consistency." Characteristics that have been demonstrated to be cross cultural human universals such as smiling, crying, facial expressions are presumed to be evolved psychological adaptations. Several evolutionary psychologists have collected massive datasets from cultures around the world to assess cross-cultural universality.

"Function to Form (or "problem to solution")." The fact that males, but not females, risk potential misidentification of genetic offspring (referred to as "paternity insecurity") led evolutionary psychologists to hypothesize that, compared to females, male jealousy would be more focused on sexual, rather than emotional, infidelity.

"Form to Function (reverse-engineering – or "solution to problem")." Morning sickness, and associated aversions to certain types of food, during pregnancy seemed to have the characteristics of an evolved adaptation (complexity and universality). Margie Profet hypothesized that the function was to avoid the ingestion of toxins during early pregnancy that could damage fetus (but which are otherwise likely to be harmless to healthy non-pregnant women).

"Corresponding Neurological Modules." Evolutionary psychology and cognitive neuropsychology are mutually compatible – evolutionary psychology helps to identify psychological adaptations and their ultimate, evolutionary functions, while neuropsychology helps to identify the proximate manifestations of these adaptations.

"Current evolutionary Adaptiveness." In addition to evolutionary models that suggest evolution occurs across large spans of time, recent research has demonstrated that some evolutionary shifts can be fast and dramatic. Consequently, some evolutionary psychologists have focused on the impact of psychological traits in the current environment. Such research can be used to inform estimates of the prevalence of traits over time. Such work has been informative in studying evolutionary psychopathology.</poem>

Evolutionary psychologists also use various sources of data for testing, including experiments, archaeological records, data from hunter-gatherer societies, observational studies, neuroscience data, self-reports and surveys, public records, and human products.
Recently, additional methods and tools have been introduced based on fictional scenarios, mathematical models, and multi-agent computer simulations.

Foundational areas of research in evolutionary psychology can be divided into broad categories of adaptive problems that arise from the theory of evolution itself: survival, mating, parenting, family and kinship, interactions with non-kin, and cultural evolution.

Problems of survival are clear targets for the evolution of physical and psychological adaptations. Major problems the ancestors of present-day humans faced included food selection and acquisition; territory selection and physical shelter; and avoiding predators and other environmental threats.

Consciousness meets George Williams' criteria of species universality, complexity, and functionality, and it is a trait that apparently increases fitness.

In his paper "Evolution of consciousness," John Eccles argues that special anatomical and physical adaptations of the mammalian cerebral cortex gave rise to consciousness. In contrast, others have argued that the recursive circuitry underwriting consciousness is much more primitive, having evolved initially in pre-mammalian species because it improves the capacity for interaction with both social "and" natural environments by providing an energy-saving "neutral" gear in an otherwise energy-expensive motor output machine. Once in place, this recursive circuitry may well have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms, as outlined by Bernard J. Baars. Richard Dawkins suggested that humans evolved consciousness in order to make themselves the subjects of thought. Daniel Povinelli suggests that large, tree-climbing apes evolved consciousness to take into account one's own mass when moving safely among tree branches. Consistent with this hypothesis, Gordon Gallup found that chimps and orangutans, but not little monkeys or terrestrial gorillas, demonstrated self-awareness in mirror tests.

The concept of consciousness can refer to voluntary action, awareness, or wakefulness. However, even voluntary behavior involves unconscious mechanisms. Many cognitive processes take place in the cognitive unconscious, unavailable to conscious awareness. Some behaviors are conscious when learned but then become unconscious, seemingly automatic. Learning, especially implicitly learning a skill, can take place outside of consciousness. For example, plenty of people know how to turn right when they ride a bike, but very few can accurately explain how they actually do so. Evolutionary psychology approaches self-deception as an adaptation that can improve one's results in social exchanges.

Sleep may have evolved to conserve energy when activity would be less fruitful or more dangerous, such as at night, and especially during the winter season.

Many experts, such as Jerry Fodor, write that the purpose of perception is knowledge, but evolutionary psychologists hold that its primary purpose is to guide action. For example, they say, depth perception seems to have evolved not to help us know the distances to other objects but rather to help us move around in space. Evolutionary psychologists say that animals from fiddler crabs to humans use eyesight for collision avoidance, suggesting that vision is basically for directing action, not providing knowledge.

Building and maintaining sense organs is metabolically expensive, so these organs evolve only when they improve an organism's fitness. More than half the brain is devoted to processing sensory information, and the brain itself consumes roughly one-fourth of one's metabolic resources, so the senses must provide exceptional benefits to fitness. Perception accurately mirrors the world; animals get useful, accurate information through their senses.

Scientists who study perception and sensation have long understood the human senses as adaptations to their surrounding worlds. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves go around corners and interact with obstacles, creating a complex pattern that includes useful information about the sources of and distances to objects. Larger animals naturally make lower-pitched sounds as a consequence of their size. The range over which an animal hears, on the other hand, is determined by adaptation. Homing pigeons, for example, can hear very low-pitched sound (infrasound) that carries great distances, even though most smaller animals detect higher-pitched sounds. Taste and smell respond to chemicals in the environment that are thought to have been significant for fitness in the environment of evolutionary adaptedness. For example, salt and sugar were apparently both valuable to the human or pre-human inhabitants of the environment of evolutionary adaptedness, so present day humans have an intrinsic hunger for salty and sweet tastes. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.

Evolutionary psychologists contend that perception demonstrates the principle of modularity, with specialized mechanisms handling particular perception tasks. For example, people with damage to a particular part of the brain suffer from the specific defect of not being able to recognize faces (prosopagnosia). Evolutionary psychology suggests that this indicates a so-called face-reading module.

In evolutionary psychology, learning is said to be accomplished through evolved capacities, specifically facultative adaptations. Facultative adaptations express themselves differently depending on input from the environment. Sometimes the input comes during development and helps shape that development. For example, migrating birds learn to orient themselves by the stars during a critical period in their maturation. Evolutionary psychologists believe that humans also learn language along an evolved program, also with critical periods. The input can also come during daily tasks, helping the organism cope with changing environmental conditions. For example, animals evolved Pavlovian conditioning in order to solve problems about causal relationships. Animals accomplish learning tasks most easily when those tasks resemble problems that they faced in their evolutionary past, such as a rat learning where to find food or water. Learning capacities sometimes demonstrate differences between the sexes. In many animal species, for example, males can solve spatial problem faster and more accurately than females, due to the effects of male hormones during development. The same might be true of humans.

Motivations direct and energize behavior, while emotions provide the affective component to motivation, positive or negative. In the early 1970s, Paul Ekman and colleagues began a line of research which suggests that many emotions are universal. He found evidence that humans share at least five basic emotions: fear, sadness, happiness, anger, and disgust. Social emotions evidently evolved to motivate social behaviors that were adaptive in the environment of evolutionary adaptedness. For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared. Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.
Motivation has a neurobiological basis in the reward system of the brain. Recently, it has been suggested that reward systems may evolve in such a way that there may be an inherent or unavoidable trade-off in the motivational system for activities of short versus long duration.

Cognition refers to internal representations of the world and internal information processing. From an evolutionary psychology perspective, cognition is not "general purpose," but uses heuristics, or strategies, that generally increase the likelihood of solving problems that the ancestors of present-day humans routinely faced. For example, present day humans are far more likely to solve logic problems that involve detecting cheating (a common problem given humans' social nature) than the same logic problem put in purely abstract terms. Since the ancestors of present-day humans did not encounter truly random events, present day humans may be cognitively predisposed to incorrectly identify patterns in random sequences. "Gamblers' Fallacy" is one example of this. Gamblers may falsely believe that they have hit a "lucky streak" even when each outcome is actually random and independent of previous trials. Most people believe that if a fair coin has been flipped 9 times and Heads appears each time, that on the tenth flip, there is a greater than 50% chance of getting Tails. Humans find it far easier to make diagnoses or predictions using frequency data than when the same information is presented as probabilities or percentages, presumably because the ancestors of present-day humans lived in relatively small tribes (usually with fewer than 150 people) where frequency information was more readily available.

Evolutionary psychology is primarily interested in finding commonalities between people, or basic human psychological nature. From an evolutionary perspective, the fact that people have fundamental differences in personality traits initially presents something of a puzzle. (Note: The field of behavioral genetics is concerned with statistically partitioning differences between people into genetic and environmental sources of variance. However, understanding the concept of heritability can be tricky – heritability refers only to the differences between people, never the degree to which the traits of an individual are due to environmental or genetic factors, since traits are always a complex interweaving of both.)

Personality traits are conceptualized by evolutionary psychologists as due to normal variation around an optimum, due to frequency-dependent selection (behavioral polymorphisms), or as facultative adaptations. Like variability in height, some personality traits may simply reflect inter-individual variability around a general optimum. Or, personality traits may represent different genetically predisposed "behavioral morphs" – alternate behavioral strategies that depend on the frequency of competing behavioral strategies in the population. For example, if most of the population is generally trusting and gullible, the behavioral morph of being a "cheater" (or, in the extreme case, a sociopath) may be advantageous. Finally, like many other psychological adaptations, personality traits may be facultative – sensitive to typical variations in the social environment, especially during early development. For example, later born children are more likely than first borns to be rebellious, less conscientious and more open to new experiences, which may be advantageous to them given their particular niche in family structure. It is important to note that shared environmental influences do play a role in personality and are not always of less importance than genetic factors. However, shared environmental influences often decrease to near zero after adolescence but do not completely disappear.

According to Steven Pinker, who builds on the work by Noam Chomsky, the universal human ability to learn to talk between the ages of 1 – 4, basically without training, suggests that language acquisition is a distinctly human psychological adaptation (see, in particular, Pinker's "The Language Instinct"). Pinker and Bloom (1990) argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop.

Pinker follows Chomsky in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction. Chomsky himself does not believe language to have evolved as an adaptation, but suggests that it likely evolved as a byproduct of some other adaptation, a so-called spandrel. But Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.

Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language. In the 1980s, psycholinguist Myrna Gopnik identified a dominant gene that causes language impairment in the KE family of Britain. This gene turned out to be a mutation of the FOXP2 gene. Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history. This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans. However, the once-popular idea that FOXP2 is a 'grammar gene' or that it triggered the emergence of language in "Homo sapiens" is now widely discredited.

Currently several competing theories about the evolutionary origin of language coexist, none of them having achieved a general consensus. Researchers of language acquisition in primates and humans such as Michael Tomasello and Talmy Givón, argue that the innatist framework has understated the role of imitation in learning and that it is not at all necessary to posit the existence of an innate grammar module to explain human language acquisition. Tomasello argues that studies of how children and primates actually acquire communicative skills suggests that humans learn complex behavior through experience, so that instead of a module specifically dedicated to language acquisition, language is acquired by the same cognitive mechanisms that are used to acquire all other kinds of socially transmitted behavior.

On the issue of whether language is best seen as having evolved as an adaptation or as a spandrel, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptionist view of evolution, and dismisses Pinker and Bloom's question of whether "Language has evolved as an adaptation" as being misleading. He argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system. A similar argument is made by Terrence Deacon who in "The Symbolic Species" argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.

If the theory that language could have evolved as a single adaptation is accepted, the question becomes which of its many functions has been the basis of adaptation. Several evolutionary hypotheses have been posited: that language evolved for the purpose of social grooming, that it evolved as a way to show mating potential or that it evolved to form social contracts. Evolutionary psychologists recognize that these theories are all speculative and that much more evidence is required to understand how language might have been selectively adapted.

Given that sexual reproduction is the means by which genes are propagated into future generations, sexual selection plays a large role in human evolution. Human mating, then, is of interest to evolutionary psychologists who aim to investigate evolved mechanisms to attract and secure mates. Several lines of research have stemmed from this interest, such as studies of mate selection mate poaching, mate retention, mating preferences and conflict between the sexes.

In 1972 Robert Trivers published an influential paper on sex differences that is now referred to as parental investment theory. The size differences of gametes (anisogamy) is the fundamental, defining difference between males (small gametes – sperm) and females (large gametes – ova). Trivers noted that anisogamy typically results in different levels of parental investment between the sexes, with females initially investing more. Trivers proposed that this difference in parental investment leads to the sexual selection of different reproductive strategies between the sexes and to sexual conflict. For example, he suggested that the sex that invests less in offspring will generally compete for access to the higher-investing sex to increase their inclusive fitness (also see Bateman's principle<ref name="doi10.1038/hdy.1948.21"></ref>). Trivers posited that differential parental investment led to the evolution of sexual dimorphisms in mate choice, intra- and inter- sexual reproductive competition, and courtship displays. In mammals, including humans, females make a much larger parental investment than males (i.e. gestation followed by childbirth and lactation). Parental investment theory is a branch of life history theory.

Buss and Schmitt's (1993) "Sexual Strategies Theory" proposed that, due to differential parental investment, humans have evolved sexually dimorphic adaptations related to "sexual accessibility, fertility assessment, commitment seeking and avoidance, immediate and enduring resource procurement, paternity certainty, assessment of mate value, and parental investment." Their "Strategic Interference Theory" suggested that conflict between the sexes occurs when the preferred reproductive strategies of one sex interfere with those of the other sex, resulting in the activation of emotional responses such as anger or jealousy.

Women are generally more selective when choosing mates, especially under long term mating conditions. However, under some circumstances, short term mating can provide benefits to women as well, such as fertility insurance, trading up to better genes, reducing risk of inbreeding, and insurance protection of her offspring.

Due to male paternity insecurity, sex differences have been found in the domains of sexual jealousy. Females generally react more adversely to emotional infidelity and males will react more to sexual infidelity. This particular pattern is predicted because the costs involved in mating for each sex are distinct. Women, on average, should prefer a mate who can offer resources (e.g., financial, commitment), thus, a woman risks losing such resources with a mate who commits emotional infidelity. Men, on the other hand, are never certain of the genetic paternity of their children because they do not bear the offspring themselves ("paternity insecurity"). This suggests that for men sexual infidelity would generally be more aversive than emotional infidelity because investing resources in another man's offspring does not lead to propagation of their own genes.

Another interesting line of research is that which examines women's mate preferences across the ovulatory cycle. The theoretical underpinning of this research is that ancestral women would have evolved mechanisms to select mates with certain traits depending on their hormonal status. Known as the ovulatory shift hypothesis, the theory posits that, during the ovulatory phase of a woman's cycle (approximately days 10–15 of a woman's cycle), a woman who mated with a male with high genetic quality would have been more likely, on average, to produce and bear a healthy offspring than a woman who mated with a male with low genetic quality. These putative preferences are predicted to be especially apparent for short-term mating domains because a potential male mate would only be offering genes to a potential offspring. This hypothesis allows researchers to examine whether women select mates who have characteristics that indicate high genetic quality during the high fertility phase of their ovulatory cycles. Indeed, studies have shown that women's preferences vary across the ovulatory cycle. In particular, Haselton and Miller (2006) showed that highly fertile women prefer creative but poor men as short-term mates. Creativity may be a proxy for good genes. Research by Gangestad et al. (2004) indicates that highly fertile women prefer men who display social presence and intrasexual competition; these traits may act as cues that would help women predict which men may have, or would be able to acquire, resources.

Reproduction is always costly for women, and can also be for men. Individuals are limited in the degree to which they can devote time and resources to producing and raising their young, and such expenditure may also be detrimental to their future condition, survival and further reproductive output.
Parental investment is any parental expenditure (time, energy etc.) that benefits one offspring at a cost to parents' ability to invest in other components of fitness (Clutton-Brock 1991: 9; Trivers 1972). Components of fitness (Beatty 1992) include the well-being of existing offspring, parents' future reproduction, and inclusive fitness through aid to kin (Hamilton, 1964). Parental investment theory is a branch of life history theory.

Robert Trivers' theory of parental investment predicts that the sex making the largest investment in lactation, nurturing and protecting offspring will be more discriminating in mating and that the sex that invests less in offspring will compete for access to the higher investing sex (see Bateman's principle). Sex differences in parental effort are important in determining the strength of sexual selection.

The benefits of parental investment to the offspring are large and are associated with the effects on condition, growth, survival and ultimately, on reproductive success of the offspring. However, these benefits can come at the cost of parent's ability to reproduce in the future e.g. through the increased risk of injury when defending offspring against predators, the loss of mating opportunities whilst rearing offspring and an increase in the time to the next reproduction. Overall, parents are selected to maximize the difference between the benefits and the costs, and parental care will likely evolve when the benefits exceed the costs.

The Cinderella effect is an alleged high incidence of stepchildren being physically, emotionally or sexually abused, neglected, murdered, or otherwise mistreated at the hands of their stepparents at significantly higher rates than their genetic counterparts. It takes its name from the fairy tale character Cinderella, who in the story was cruelly mistreated by her stepmother and stepsisters. Daly and Wilson (1996) noted: "Evolutionary thinking led to the discovery of the most important risk factor for child homicide – the presence of a stepparent. Parental efforts and investments are valuable resources, and selection favors those parental psyches that allocate effort effectively to promote fitness. The adaptive problems that challenge parental decision making include both the accurate identification of one's offspring and the allocation of one's resources among them with sensitivity to their needs and abilities to convert parental investment into fitness increments…. Stepchildren were seldom or never so valuable to one's expected fitness as one's own offspring would be, and those parental psyches that were easily parasitized by just any appealing youngster must always have incurred a selective disadvantage"(Daly & Wilson, 1996, pp. 64–65). However, they note that not all stepparents will "want" to abuse their partner's children, or that genetic parenthood is any insurance against abuse. They see step parental care as primarily "mating effort" towards the genetic parent.

Inclusive fitness is the sum of an organism's classical fitness (how many of its own offspring it produces and supports) and the number of equivalents of its own offspring it can add to the population by supporting others. The first component is called classical fitness by Hamilton (1964).

From the gene's point of view, evolutionary success ultimately depends on leaving behind the maximum number of copies of itself in the population. Until 1964, it was generally believed that genes only achieved this by causing the individual to leave the maximum number of viable offspring. However, in 1964 W. D. Hamilton proved mathematically that, because close relatives of an organism share some identical genes, a gene can also increase its evolutionary success by promoting the reproduction and survival of these related or otherwise similar individuals. Hamilton concluded that this leads natural selection to favor organisms that would behave in ways that maximize their inclusive fitness. It is also true that natural selection favors behavior that maximizes personal fitness.

Hamilton's rule describes mathematically whether or not a gene for altruistic behavior will spread in a population:
where

The concept serves to explain how natural selection can perpetuate altruism. If there is an "altruism gene" (or complex of genes) that influences an organism's behavior to be helpful and protective of relatives and their offspring, this behavior also increases the proportion of the altruism gene in the population, because relatives are likely to share genes with the altruist due to common descent. Altruists may also have some way to recognize altruistic behavior in unrelated individuals and be inclined to support them. As Dawkins points out in "The Selfish Gene" (Chapter 6) and "The Extended Phenotype", this must be distinguished from the green-beard effect.

Although it is generally true that humans tend to be more altruistic toward their kin than toward non-kin, the relevant proximate mechanisms that mediate this cooperation have been debated (see kin recognition), with some arguing that kin status is determined primarily via social and cultural factors (such as co-residence, maternal association of sibs, etc.), while others have argued that kin recognition can also be mediated by biological factors such as facial resemblance and immunogenetic similarity of the major histocompatibility complex (MHC). For a discussion of the interaction of these social and biological kin recognition factors see Lieberman, Tooby, and Cosmides (2007) (PDF).

Whatever the proximate mechanisms of kin recognition there is substantial evidence that humans act generally more altruistically to close genetic kin compared to genetic non-kin.

Although interactions with non-kin are generally less altruistic compared to those with kin, cooperation can be maintained with non-kin via mutually beneficial reciprocity as was proposed by Robert Trivers. If there are repeated encounters between the same two players in an evolutionary game in which each of them can choose either to "cooperate" or "defect," then a strategy of mutual cooperation may be favored even if it pays each player, in the short term, to defect when the other cooperates. Direct reciprocity can lead to the evolution of cooperation only if the probability, w, of another encounter between the same two individuals exceeds the cost-to-benefit ratio of the altruistic act:

Reciprocity can also be indirect if information about previous interactions is shared. Reputation allows evolution of cooperation by indirect reciprocity. Natural selection favors strategies that base the decision to help on the reputation of the recipient: studies show that people who are more helpful are more likely to receive help. The calculations of indirect reciprocity are complicated and only a tiny fraction of this universe has been uncovered, but again a simple rule has emerged. Indirect reciprocity can only promote cooperation if the probability, q, of knowing someone's reputation exceeds the cost-to-benefit ratio of the altruistic act:

One important problem with this explanation is that individuals may be able to evolve the capacity to obscure their reputation, reducing the probability, q, that it will be known.

Trivers argues that friendship and various social emotions evolved in order to manage reciprocity. Liking and disliking, he says, evolved to help present day humans' ancestors form coalitions with others who reciprocated and to exclude those who did not reciprocate. Moral indignation may have evolved to prevent one's altruism from being exploited by cheaters, and gratitude may have motivated present day humans' ancestors to reciprocate appropriately after benefiting from others' altruism. Likewise, present day humans feel guilty when they fail to reciprocate. These social motivations match what evolutionary psychologists expect to see in adaptations that evolved to maximize the benefits and minimize the drawbacks of reciprocity.

Evolutionary psychologists say that humans have psychological adaptations that evolved specifically to help us identify nonreciprocators, commonly referred to as "cheaters." In 1993, Robert Frank and his associates found that participants in a prisoner's dilemma scenario were often able to predict whether their partners would "cheat," based on a half-hour of unstructured social interaction. In a 1996 experiment, for example, Linda Mealey and her colleagues found that people were better at remembering the faces of people when those faces were associated with stories about those individuals cheating (such as embezzling money from a church).

Humans may have an evolved set of psychological adaptations that predispose them to be more cooperative than otherwise would be expected with members of their tribal in-group, and, more nasty to members of tribal out groups. These adaptations may have been a consequence of tribal warfare. Humans may also have predispositions for "altruistic punishment" – to punish in-group members who violate in-group rules, even when this altruistic behavior cannot be justified in terms of helping those you are related to (kin selection), cooperating with those who you will interact with again (direct reciprocity), or cooperating to better your reputation with others (indirect reciprocity).

Though evolutionary psychology has traditionally focused on individual-level behaviors, determined by species-typical psychological adaptations, considerable work has been done on how these adaptations shape and, ultimately govern, culture (Tooby and Cosmides, 1989). Tooby and Cosmides (1989) argued that the mind consists of many domain-specific psychological adaptations, some of which may constrain what cultural material is learned or taught. As opposed to a domain-general cultural acquisition program, where an individual passively receives culturally-transmitted material from the group, Tooby and Cosmides (1989), among others, argue that: "the psyche evolved to generate adaptive rather than repetitive behavior, and hence critically analyzes the behavior of those surrounding it in highly structured and patterned ways, to be used as a rich (but by no means the only) source of information out of which to construct a 'private culture' or individually tailored adaptive system; in consequence, this system may or may not mirror the behavior of others in any given respect." (Tooby and Cosmides 1989).

According to Paul Baltes, the benefits granted by evolutionary selection decrease with age. Natural selection has not eliminated many harmful conditions and nonadaptive characteristics that appear among older adults, such as Alzheimer disease. If it were a disease that killed 20-year-olds instead of 70-year-olds this may have been a disease that natural selection could have eliminated ages ago. Thus, unaided by evolutionary pressures against nonadaptive conditions, modern humans suffer the aches, pains, and infirmities of aging and as the benefits of evolutionary selection decrease with age, the need for modern technological mediums against non-adaptive conditions increases.

As humans are a highly social species, there are many adaptive problems associated with navigating the social world (e.g., maintaining allies, managing status hierarchies, interacting with outgroup members, coordinating social activities, collective decision-making). Researchers in the emerging field of evolutionary social psychology have made many discoveries pertaining to topics traditionally studied by social psychologists, including person perception, social cognition, attitudes, altruism, emotions, group dynamics, leadership, motivation, prejudice, intergroup relations, and cross-cultural differences.

When endeavouring to solve a problem humans at an early age show determination while chimpanzees have no comparable facial expression. Researchers suspect the human determined expression evolved because when a human is determinedly working on a problem other people will frequently help.

Adaptationist hypotheses regarding the etiology of psychological disorders are often based on analogies between physiological and psychological dysfunctions, as noted in the table below. Prominent theorists and evolutionary psychiatrists include Michael T. McGuire, Anthony Stevens, and Randolph M. Nesse. They, and others, suggest that mental disorders are due to the interactive effects of both nature and nurture, and often have multiple contributing causes.

Evolutionary psychologists have suggested that schizophrenia and bipolar disorder may reflect a side-effect of genes with fitness benefits, such as increased creativity. (Some individuals with bipolar disorder are especially creative during their manic phases and the close relatives of people with schizophrenia have been found to be more likely to have creative professions.) A 1994 report by the American Psychiatry Association found that people suffered from schizophrenia at roughly the same rate in Western and non-Western cultures, and in industrialized and pastoral societies, suggesting that schizophrenia is not a disease of civilization nor an arbitrary social invention. Sociopathy may represent an evolutionarily stable strategy, by which a small number of people who cheat on social contracts benefit in a society consisting mostly of non-sociopaths. Mild depression may be an adaptive response to withdraw from, and re-evaluate, situations that have led to disadvantageous outcomes (the "analytical rumination hypothesis") (see Evolutionary approaches to depression).

Some of these speculations have yet to be developed into fully testable hypotheses, and a great deal of research is required to confirm their validity.

Evolutionary psychology has been applied to explain criminal or otherwise immoral behavior as being adaptive or related to adaptive behaviors. Males are generally more aggressive than females, who are more selective of their partners because of the far greater effort they have to contribute to pregnancy and child-rearing. Males being more aggressive is hypothesized to stem from the more intense reproductive competition faced by them. Males of low status may be especially vulnerable to being childless. It may have been evolutionary advantageous to engage in highly risky and violently aggressive behavior to increase their status and therefore reproductive success. This may explain why males are generally involved in more crimes, and why low status and being unmarried is associated with criminality. Furthermore, competition over females is argued to have been particularly intensive in late adolescence and young adulthood, which is theorized to explain why crime rates are particularly high during this period. Some sociologists have underlined differential exposure to androgens as the cause of these behaviors, notably Lee Ellis in his evolutionary neuroandrogenic (ENA) theory.

Many conflicts that result in harm and death involve status, reputation, and seemingly trivial insults. Steven Pinker in his book "The Blank Slate" argues that in non-state societies without a police it was very important to have a credible deterrence against aggression. Therefore, it was important to be perceived as having a credible reputation for retaliation, resulting in humans to develop instincts for revenge as well as for protecting reputation ("honor"). Pinker argues that the development of the state and the police have dramatically reduced the level of violence compared to the ancestral environment. Whenever the state breaks down, which can be very locally such as in poor areas of a city, humans again organize in groups for protection and aggression and concepts such as violent revenge and protecting honor again become extremely important.

Rape is theorized to be a reproductive strategy that facilitates the propagation of the rapist's progeny. Such a strategy may be adopted by men who otherwise are unlikely to be appealing to women and therefore cannot form legitimate relationships, or by high status men on socially vulnerable women who are unlikely to retaliate to increase their reproductive success even further. The sociobiological theories of rape are highly controversial, as traditional theories typically do not consider rape to be a behavioral adaptation, and objections to this theory are made on ethical, religious, political, as well as scientific grounds.

Adaptationist perspectives on religious belief suggest that, like all behavior, religious behaviors are a product of the human brain. As with all other organ functions, cognition's functional structure has been argued to have a genetic foundation, and is therefore subject to the effects of natural selection and sexual selection. Like other organs and tissues, this functional structure should be universally shared amongst humans and should have solved important problems of survival and reproduction in ancestral environments. However, evolutionary psychologists remain divided on whether religious belief is more likely a consequence of evolved psychological adaptations, or a byproduct of other cognitive adaptations.

Coalitional psychology is an approach to explain political behaviors between different coalitions and the conditionality of these behaviors in evolutionary psychological perspective. This approach assumes that since human beings appeared on the earth, they have evolved to live in groups instead of living as individuals to achieve benefits such as more mating opportunities and increased status. Human beings thus naturally think and act in a way that manages and negotiates group dynamics.

Coalitional psychology offers falsifiable ex ante prediction by positing five hypotheses on how these psychological adaptations operate:

Critics of evolutionary psychology accuse it of promoting genetic determinism, panadaptionism (the idea that all behaviors and anatomical features are adaptations), unfalsifiable hypotheses, distal or ultimate explanations of behavior when proximate explanations are superior, and malevolent political or moral ideas.

Critics have argued that evolutionary psychology might be used to justify existing social hierarchies and reactionary policies. It has also been suggested by critics that evolutionary psychologists' theories and interpretations of empirical data rely heavily on ideological assumptions about race and gender.

In response to such criticism, evolutionary psychologists often caution against committing the naturalistic fallacy – the assumption that "what is natural" is necessarily a moral good. However, their caution against committing the naturalistic fallacy has been criticized as means to stifle legitimate ethical discussions.

Some criticisms of evolutionary psychology point at contradictions between different aspects of adaptive scenarios posited by evolutionary psychology. One example is the evolutionary psychology model of extended social groups selecting for modern human brains, a contradiction being that the synaptic function of modern human brains require high amounts of many specific essential nutrients so that such a transition to higher requirements of the same essential nutrients being shared by all individuals in a population would decrease the possibility of forming large groups due to bottleneck foods with rare essential nutrients capping group sizes. It is mentioned that some insects have societies with different ranks for each individual and that monkeys remain socially functioning after removal of most of the brain as additional arguments against big brains promoting social networking. The model of males as both providers and protectors is criticized for the impossibility of being in two places at once, the male cannot both protect his family at home and be out hunting at the same time. In the case of the claim that a provider male could buy protection service for his family from other males by bartering food that he had hunted, critics point at the fact that the most valuable food (the food that contained the rarest essential nutrients) would be different in different ecologies and as such vegetable in some geographical areas and animal in others, making it impossible for hunting styles relying on physical strength or risk taking to be universally of similar value in bartered food and instead making it inevitable that in some parts of Africa, food gathered with no need for major physical strength would be the most valuable to barter for protection. A contradiction between evolutionary psychology's claim of men needing to be more sexually visual than women for fast speed of assessing women's fertility than women needed to be able to assess the male's genes and its claim of male sexual jealousy guarding against infidelity is also pointed at, as it would be pointless for a male to be fast to assess female fertility if he needed to assess the risk of there being a jealous male mate and in that case his chances of defeating him before mating anyway (pointlessness of assessing one necessary condition faster than another necessary condition can possibly be assessed).

Evolutionary psychology has been entangled in the larger philosophical and social science controversies related to the debate on nature versus nurture. Evolutionary psychologists typically contrast evolutionary psychology with what they call the standard social science model (SSSM). They characterize the SSSM as the "blank slate", "relativist", "social constructionist", and "cultural determinist" perspective that they say dominated the social sciences throughout the 20th century and assumed that the mind was shaped almost entirely by culture.

Critics have argued that evolutionary psychologists created a false dichotomy between their own view and the caricature of the SSSM. Other critics regard the SSSM as a rhetorical device or a straw man and suggest that the scientists whom evolutionary psychologists associate with the SSSM did not believe that the mind was a blank state devoid of any natural predispositions.

Some critics view evolutionary psychology as a form of genetic reductionism and genetic determinism, a common critique being that evolutionary psychology does not address the complexity of individual development and experience and fails to explain the influence of genes on behavior in individual cases. Evolutionary psychologists respond that they are working within a nature-nurture interactionist framework that acknowledges that many psychological adaptations are facultative (sensitive to environmental variations during individual development). The discipline is generally not focused on proximate analyses of behavior, but rather its focus is on the study of distal/ultimate causality (the evolution of psychological adaptations). The field of behavioral genetics is focused on the study of the proximate influence of genes on behavior.

A frequent critique of the discipline is that the hypotheses of evolutionary psychology are frequently arbitrary and difficult or impossible to adequately test, thus questioning its status as an actual scientific discipline, for example because many current traits probably evolved to serve different functions than they do now. Thus because there are a potentially infinite number of alternative explanations for why a trait evolved, critics contend that it is impossible to determine the exact explanation. While evolutionary psychology hypotheses are difficult to test, evolutionary psychologists assert that it is not impossible. Part of the critique of the scientific base of evolutionary psychology includes a critique of the concept of the Environment of Evolutionary Adaptation (EEA). Some critics have argued that researchers know so little about the environment in which "Homo sapiens" evolved that explaining specific traits as an adaption to that environment becomes highly speculative. Evolutionary psychologists respond that they do know many things about this environment, including the facts that present day humans' ancestors were hunter-gatherers, that they generally lived in small tribes, etc. Edward Hagen argues that the human past environments were not radically different in the same sense as the Carboniferous or Jurassic periods and that the animal and plant taxa of the era were similar to those of the modern world, as was the geology and ecology. Hagen argues that few would deny that other organs evolved in the EEA (for example, lungs evolving in an oxygen rich atmopshere) yet critics question whether or not the brain's EEA is truly knowable, which he argues constitutes selective scepticism. Hagen also argues that most evolutionary psychology research is based on the fact that females can get pregnant and males cannot, which Hagen observes was also true in the EEA.

John Alcock describes this as the "No Time Machine Argument", as critics are arguing that since it is not possible to travel back in time to the EEA, then it cannot be determined what was going on there and thus what was adaptive. Alcock argues that present day evidence allows researchers to be reasonably confident about the conditions of the EEA and that the fact that so many human behaviours are adaptive in the "current" environment is evidence that the ancestral environment of humans had much in common with the present one, as these behaviours would have evolved in the ancestral environment. Thus Alcock concludes that researchers can make predictions on the adaptive value of traits. Similarly, Dominic Murphy argues that alternative explanations cannot just be forwarded but instead need their own evidence and predictions - if one explanation makes predictions that the others cannot, it is reasonable to have confidence in that explanation. In addition, Murphy argues that other historical sciences also make predictions about modern phenomena to come up with explanations about past phenomena, for example cosmologists look for evidence for what we would expect to see in the modern day if the Big Bang was true, while geologists make predictions about modern phenomena to determine if an asteroid wiped out the dinosaurs. Murphy argues that if other historical disciplines can conduct tests without a time machine, then the onus is on the critics to show why evolutionary psychology is untestable if other historical disciplines are not, as "methods should be judged across the board, not singled out for ridicule in one context."

Evolutionary psychologists generally presume that, like the body, the mind is made up of many evolved modular adaptations, although there is some disagreement within the discipline regarding the degree of general plasticity, or "generality," of some modules. It has been suggested that modularity evolves because, compared to non-modular networks, it would have conferred an advantage in terms of fitness and because connection costs are lower.

In contrast, some academics argue that it is unnecessary to posit the existence of highly domain specific modules, and, suggest that the neural anatomy of the brain supports a model based on more domain general faculties and processes. Moreover, empirical support for the domain-specific theory stems almost entirely from performance on variations of the Wason selection task which is extremely limited in scope as it only tests one subtype of deductive reasoning.

Cecilia Heyes has argued that the picture presented by some evolutionary psychology of the human mind as a collection of cognitive instinctsorgans of thought shaped by genetic evolution over very long time periodsdoes not fit research results. She posits instead that humans have cognitive gadgets"special-purpose organs of thought" built in the course of development through social interaction.

Evolutionary psychologists have addressed many of their critics (see, for example, books by Segerstråle (2000), "Defenders of the Truth: The Battle for Science in the Sociobiology Debate and Beyond," Barkow (2005), "Missing the Revolution: Darwinism for Social Scientists," and Alcock (2001), "The Triumph of Sociobiology"). Among their rebuttals are that some criticisms are straw men, are based on an incorrect nature versus nurture dichotomy, are based on misunderstandings of the discipline, etc. Robert Kurzban suggested that "...critics of the field, when they err, are not slightly missing the mark. Their confusion is deep and profound. It’s not like they are marksmen who can’t quite hit the center of the target; they’re holding the gun backwards."






</doc>
<doc id="9705" url="https://en.wikipedia.org/wiki?curid=9705" title="Languages of Europe">
Languages of Europe

Most languages of Europe belong to the Indo-European language family. 
Out of a total European population of 744 million as of 2018, some 94% are native speakers of an Indo-European language; within Indo-European, the three largest phyla are "Romance", "Slavic", and "Germanic" with more than 200 million speakers each, between them accounting for close to 90% of Europeans. Smaller phyla of Indo-European found in Europe include Hellenic (Greek, 13 million), Baltic ( 7 million), Albanian ( 5 million), Indo-Aryan (Romani, 1.5 million), and Celtic ( 1 million).

Of the approximately 45 million Europeans speaking non-Indo-European languages, most speak languages within either the "Uralic" or "Turkic" families. Still smaller groups (such as Basque and various languages of the Caucasus) account for less than 1% of the European population between them. Immigration has added sizeable communities of speakers of African and Asian languages, amounting to about 4% of the population, with Arabic being the most widely spoken of them.

Five languages have more than 50 million native speakers in Europe: French, Italian, German, English, and Russian. While Russian has the largest number of native speakers (more than 100 million in Europe), English has the largest number of speakers in total, including some 200 million speakers of English as a second or foreign language. ("See English language in Europe".)

The Indo-European language family is descended from Proto-Indo-European, which is believed to have been spoken thousands of years ago. Early speakers of Indo-European daughter languages most likely expanded into Europe with the incipient Bronze Age, around 4,000 years ago (Bell-Beaker culture).

Roughly 215 million Europeans (primarily in Western and Southern Europe) are native speakers of Romance languages, the largest groups including
French ( 72 million),
Italian ( 65 million),
Spanish ( 40 million), 
Romanian ( 24 million),
Portuguese ( 10 million),
Catalan ( 9 million),
Sicilian ( 5 million, also subsumed under Italian), 
Venetian language ( 4 million),
Galician ( 2 million),
Sardinian ( 1 million),
Occitan ( 500,000), besides numerous smaller communities.

The Romance languages are descended from varieties of Vulgar Latin spoken in the various parts of the Roman Empire in Late Antiquity. Latin was itself part of the (otherwise extinct) Italic branch of Indo-European.
Romance languages are divided phylogenetically into "Italo-Western", "Eastern Romance" (including Romanian) and "Sardinian". The Romance-speaking area of Europe is occasionally referred to as "Latin Europe".

We can further break down Italo-Western into the "Italo-Dalmatian languages" (sometimes grouped with Eastern Romance), including the Tuscan-derived Italian and numerous local Romance lects in Italy as well as Dalmatian, and the "Western Romance languages". The Western Romance languages in turn separate into the Gallo-Romance languages, including French and its varieties (Langues d'oïl), the Rhaeto-Romance languages and the Gallo-Italic languages; the Occitano-Romance languages, grouped with either Gallo-Romance or East Iberian, including Occitan, Catalan and Aragonese; and finally the West Iberian languages (Spanish-Portuguese), including the Astur-Leonese languages, Galician-Portuguese, and Castilian.

The Germanic languages make up the predominant language family in Western, Northern and Central Europe. An estimated 210 million Europeans are native speakers of Germanic languages, the largest groups being German ( 95 million), English ( 70 million), Dutch ( 24 million), Swedish ( 10 million), Danish ( 6 million), and Norwegian ( 5 million).

There are two extant major sub-divisions: "West Germanic" and "North Germanic". A third group, East Germanic, is now extinct; the only known surviving East Germanic texts are written in the Gothic language. West Germanic is divided into Anglo-Frisian (including English), Low German, and Low Franconian (including Dutch) and High German (including Standard German).

German is spoken throughout Germany, Austria, Liechtenstein, much of Switzerland (including the northeast areas bordering on Germany and Austria), northern Italy (South Tyrol), Luxembourg, and the East Cantons of Belgium.

There are several groups of German dialects:

Low German (including Low Saxon) is spoken in various regions throughout Northern Germany and the northern and eastern parts of the Netherlands. It is an official language in Germany. It may be separated into Low Saxon (West Low German) and East Low German.

Dutch is spoken throughout the Netherlands, the northern half of Belgium, as well as the Nord-Pas de Calais region of France, and around Düsseldorf in Germany. In Belgian and French contexts, Dutch is sometimes referred to as Flemish. Dutch dialects are varied and cut across national borders.

The Anglo-Frisian language family is now mostly represented by English (Anglic), descended from the Old English language spoken by the Anglo-Saxons:
The Frisian languages are spoken by about 500,000 Frisians, who live on the southern coast of the North Sea in the Netherlands and Germany. These languages include West Frisian, Saterlandic, and North Frisian.

The "North Germanic languages" are spoken in Scandinavian countries and include Danish (Denmark), Norwegian (Norway), Swedish (Sweden and parts of Finland), or Elfdalian (in a small part of central Sweden), Faroese (Faroe Islands), and Icelandic (Iceland).

English has a long history of contact with Scandinavian languages, given the immigration of Scandinavians early in the history of Britain, and shares various features with the Scandinavian languages.

Slavic languages are spoken in large areas of Southern, Central and Eastern Europe. An estimated 250 million Europeans are native speakers of Slavic languages, the largest groups being 
Russian ( 110 million in European Russia and adjacent parts of Eastern Europe, Russian forming the largest linguistic community in Europe), 
Polish ( 55 million),
Ukrainian ( 40 million), 
Serbo-Croatian ( 21 million),
Czech ( 11 million),
Bulgarian ( 9 million), 
Slovak ( 5 million)
Belarusian and Slovene ( 3 million each)
and Macedonian ( 2 million).

Phylogenetically, Slavic is divided into three subgroups:



Uralic is native to northern Eurasia. Finno-Ugric groups the Uralic languages other than Samoyedic. 
Finnic languages include Finnish ( 5 million) and Estonian ( 1 million). The Sami languages ( 30,000) are closely related to Finnic.

The Ugric languages are represented in Europe with the Hungarian language ( 13 million), historically introduced with the Hungarian conquest of the Carpathian Basin of the 9th century.

The Samoyedic Nenets language is spoken in Nenets Autonomous Okrug of Russia, located in the far northeastern corner of Europe (as delimited by the Ural Mountains).



In the Middle Ages the two most important defining elements of Europe were "Christianitas" and "Latinitas".

The earliest dictionaries were glossaries: more or less structured lists of lexical pairs (in alphabetical order or according to conceptual fields). The Latin-German (Latin-Bavarian) "Abrogans" was among the first. A new wave of lexicography can be seen from the late 15th century onwards (after the introduction of the printing press, with the growing interest in standardisation of languages).

The concept of the nation state began to emerge in the early modern period. Nations adopted particular dialects as their national language. This, together with improved communications, led to official efforts to standardise the national language, and a number of language academies were established: 1582 "Accademia della Crusca" in Florence, 1617 "Fruchtbringende Gesellschaft" in Weimar, 1635 "Académie française" in Paris, 1713 "Real Academia Española" in Madrid. Language became increasingly linked to nation as opposed to culture, and was also used to promote religious and ethnic identity: e.g. different Bible translations in the same language for Catholics and Protestants.

The first languages whose standardisation was promoted included Italian ("questione della lingua": Modern Tuscan/Florentine vs. Old Tuscan/Florentine vs. Venetian → Modern Florentine + archaic Tuscan + Upper Italian), French (the standard is based on Parisian), English (the standard is based on the London dialect) and (High) German (based on the dialects of the chancellery of Meissen in Saxony, Middle German, and the chancellery of Prague in Bohemia ("Common German")). But several other nations also began to develop a standard variety in the 16th century.

Europe has had a number of languages that were considered linguae francae over some ranges for some periods according to some historians. Typically in the rise of a national language the new language becomes a lingua franca to peoples in the range of the future nation until the consolidation and unification phases. If the nation becomes internationally influential, its language may become a lingua franca among nations that speak their own national languages. Europe has had no lingua franca ranging over its entire territory spoken by all or most of its populations during any historical period. Some linguae francae of past and present over some of its regions for some of its populations are:


Historical attitudes towards linguistic diversity are illustrated by two French laws: the Ordonnance de Villers-Cotterêts (1539), which said that every document in France should be written in French (neither in Latin nor in Occitan) and the Loi Toubon (1994), which aimed to eliminate anglicisms from official documents. States and populations within a state have often resorted to war to settle their differences. There have been attempts to prevent such hostilities: two such initiatives were promoted by the Council of Europe, founded in 1949, which affirms the right of minority language speakers to use their language fully and freely. The Council of Europe is committed to protecting linguistic diversity. Currently all European countries except France, Andorra and Turkey have signed the Framework Convention for the Protection of National Minorities, while Greece, Iceland and Luxembourg have signed it, but have not ratified it; this framework entered into force in 1998. Another European treaty, the European Charter for Regional or Minority Languages, was adopted in 1992 under the auspices of the Council of Europe: it entered into force in 1998, and while it is legally binding for 24 countries, France, Iceland, Italy, North Macedonia, Moldova and Russia have chosen to sign without ratifying the convention.

 
The main scripts used in Europe today are the Latin and Cyrillic.

The Greek alphabet was derived from the Phoenician alphabet, and Latin was derived from the Greek via the Old Italic alphabet. In the Early Middle Ages, Ogham was used in Ireland and runes (derived the Old Italic script) in Scandinavia. Both were replaced in general use by the Latin alphabet by the Late Middle Ages. The Cyrillic script was derived from the Greek with the first texts appearing around 940 AD.
Around 1900 there were mainly two typeface variants of the Latin alphabet used in Europe: Antiqua and Fraktur. Fraktur was used most for German, Estonian, Latvian, Norwegian and Danish whereas Antiqua was used for Italian, Spanish, French, Polish, Portuguese, English, Romanian, Swedish and Finnish. The Fraktur variant was banned by Hitler in 1941, having been described as "Schwabacher Jewish letters". Other scripts have historically been in use in Europe, including Phoenician, from which modern Latin letters descend, Ancient Egyptian hieroglyphs on Egyptian artefacts traded during Antiquity various runic systems used in Northern Europe preceding Christianisation, and Arabic during the era of the Ottoman Empire.

Hungarian rovás was used by the Hungarian people in the early Middle Ages, but it was gradually replaced with the Latin-based Hungarian alphabet when Hungary became a kingdom, though it was revived in the 20th century and has certain marginal, but growing area of usage since then.

The European Union (as of 2016) had 28 member states accounting for a population of 510 million, or about 69% of the population of Europe.

The European Union has designated by agreement with the member states 24 languages as "official and working": Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish and Swedish. This designation provides member states with two "entitlements": the member state may communicate with the EU in any of the designated languages, and view "EU regulations and other legislative documents" in that language.

The European Union and the Council of Europe have been collaborating in education of member populations in languages for "the promotion of plurilingualism" among EU member states. The joint document, "Common European Framework of Reference for Languages: Learning, Teaching, Assessment (CEFR)", is an educational standard defining "the competencies necessary for communication" and related knowledge for the benefit of educators in setting up educational programs. 
In a 2005 independent survey requested by the EU's Directorate-General for Education and Culture regarding the extent to which major European languages were spoken in member states. The results were published in a 2006 document, "Europeans and Their Languages", or "Eurobarometer 243". In this study, statistically relevant samples of the population in each country were asked to fill out a survey form concerning the languages that they spoke with sufficient competency "to be able to have a conversation".

The following is a table of European languages. The number of speakers as a first or second language (L1 and L2 speakers) listed are speakers in Europe only; see list of languages by number of native speakers and list of languages by total number of speakers for global estimates on numbers of speakers.

The list is intended to include any language variety with an ISO 639 code. However, it omits sign languages. Because the ISO-639-2 and ISO-639-3 codes have different definitions, this means that some communities of speakers may be listed more than once. For instance, speakers of Austro-Bavarian are listed both under "Bavarian" (ISO-639-3 code "bar") as well as under "German" (ISO-639-2 code "de").

There are various definitions of Europe, which may or may not include all or parts of Turkey, Cyprus, Armenia, Azerbaijan, and Georgia. For convenience, the languages and associated statistics for all five of these countries are grouped together on this page, as they are usually presented at a national, rather than subnational, level.

Recent (post–1945) immigration to Europe introduced substantial communities of speakers of non-European languages.

The largest such communities include Arabic speakers (see Arabs in Europe)
and Turkish speakers (beyond European Turkey and the historical sphere of influence of the Ottoman Empire, see Turks in Europe).
Armenians, Berbers, and Kurds have diaspora communities of 1–2 million each. The various languages of Africa and languages of India form numerous smaller diaspora communities.


Various sign languages are also used in Europe. The most widespread sign language family in Europe is the French Sign Language family, but others include the BANZSL family, the Danish Sign Language family and the Swedish Sign Language family. There are also language isolates, most notably Spanish Sign Language.

The three most used sign languages in Europe, according to Ethnologue, are French Sign Language (spoken in France and Switzerland, and in countries outside Europe), British Sign Language (in the United Kingdom) and German Sign Language (in Germany).

The EU and several other European countries afford legal recognition for various sign languages.




</doc>
<doc id="9706" url="https://en.wikipedia.org/wiki?curid=9706" title="Eindhoven University of Technology">
Eindhoven University of Technology

The Eindhoven University of Technology (), abbr. TU/e, is a technical university in the Netherlands, operating in English.

The University has been placed in the top 200 universities in the world by five major ranking tables. The 2019 QS World University Rankings place Eindhoven 99th in the world, 34th in Europe, and 3rd in the Netherlands - TU/e has moved up 59 places in this world ranking since 2012 (in two other main world rankings it is 167th and 51-75th).

TU/e is the Dutch member of the EuroTech Universities Alliance, a strategic partnership of universities of science & technology in Europe: Technical University of Denmark (DTU), École Polytechnique Fédérale de Lausanne (EPFL), École Polytechnique (L’X), The Technion, Eindhoven University of Technology (TU/e), and Technical University of Munich (TUM).

The Eindhoven University of Technology was founded as the "Technische Hogeschool Eindhoven" (THE) on 23 June 1956 by the Dutch government. It was the second institute of its kind in the Netherlands, preceded only by the Delft University of Technology.

Undergraduate education was given in four- or five-year programs until 2002, styled along the lines of the German system of education; graduates of these programs were granted an engineering title and allowed to prefix their name with the title "ir." (an abbreviation of ingenieur; not to be confused with graduates of technical "hogescholen", who were engineers abbreviated "ing."). Starting in 2002, following the entry into force of the Bologna Accords, the university switched to the bachelor/master structure (students graduating in 2002 were given both an old-style engineering title and a new master's title). The undergraduate programs are now split into two parts, a three-year bachelor program and a two-year master program.

On 3 January 2011, the university's strategic vision document for the period up to 2020, the "Strategic Plan 2020", was presented. Despite the economic crisis and the budget cutbacks announced by the Dutch government for the period up to 2014, the university set itself an ambitious strategic vision for the period up to 2020. This vision included establishing a University College to foster both depth, breadth, and societal relevance in engineering education; establishing a combined Graduate School to manage the graduate programs; an increase of the student body by 50 percent; a 50 percent increase in the number of annual PhDs awarded; an increase of knowledge "valorisation" (exploitation by industry and society) to a campus-wide score of 4.2; increasing the international position of the university to within the top-100 universities; and increasing the embedding of the university within the city and the Brainport region by transforming the campus into a high-grade science park with laboratories, housing facilities for 700 students and researchers and supporting facilities. The science park was one of the more costly elements of the plan.

The Eindhoven University of Technology is a public university of the Netherlands. As such its general structure and management is determined by the "Wet op het Hoger Onderwijs en Wetenschappelijk Onderzoek" (English: "Law on Higher Education and Scientific Research"). Between that law and the statutes of the university itself, the management of the university is organized according to the following chart:

The day-to-day running of the university is in the hands of the Executive College (Dutch: "College van Bestuur"). The College provides oversight for the departments, the service organizations and the Innovation Lab, plus the local activities of the Stan Ackermans Institute. The College consists of three people, plus a secretary:


There are two bodies that provide oversight over the Executive College:


Most of the work at the university is done in the departments and the service organizations.


Both for the departments and the service organizations, the staff (and students) are involved with the running of the body. For that reason both types of bodies have advisory councils which have advisory and co-decision authorities.

Over the past two decades, the TU/e has increasingly developed commercial interests and off-campus ties. These include commercial agreements and contracts directly between the university and external companies, but also interests in spinoff companies. In order to manage these kinds of contractual obligations the university started the TU/e Holding B.V. in 1997. The Holding is a limited company, dedicated to the commercial exploitation of scientific knowledge.

The scientific departments (or faculties; Dutch: "faculteiten") are the primary vehicles for teaching and research in the university. They employ the majority of the academic staff, are responsible for teaching and sponsor the research schools and institutions.

The departments also offer Ph.D programs (Dutch: "promotiefase") whereby a qualified master may earn a Ph.D. Unlike in anglo-saxon countries these are not educational programs, however; rather, a person working towards obtaining the Ph.D is a research employee of the university.

The TU/e has nine departments:

The university offers honors programs aimed at both bachelor and master students. At the bachelor level it consists of intensive study within eight possible areas or tracks. At the master level it consists of personal leadership and professional development components, over and above the normal masters study.

In 1986, the university started a number of programs for a postgraduate doctorate of engineering (PDEng) together with two other Dutch technological universities (TU Delft and University of Twente). These programs are managed by the Stan Ackermans Institute on behalf of the 4TU Federation. Each program is two years in length. Ten programs are available at the TU/e.

Nationally, more than 3,500 students have earned the postgraduate PDEng degree through this program. On February 13, Ravi Thakkar was awarded 3000th PDEng diploma at TU/e

The university hosts a number of other educational programs that are in some way related to the main educational programs. These include the teacher's program and an MBA program.


The TU/e participates in a large number of research institutes which balance in different ways between pure science and applied science research. Some of these institutes are bound strictly to the university, others combine research across different universities.

The TU/e is among the world’s ten best-performing research universities in terms of research cooperation with industry in 2011 (Number 1 in 2009). Ten to 20 percent of the scientific publications of these ten universities in the period 2006–2008 were the result of partnerships with researchers in industry. As well as TU/e and Delft University of Technology, the top 10 also includes two universities in Japan (Tokyo Institute of Technology and Keio University in Tokyo), two in Sweden (CTH Chalmers University of Technology and KTH Royal Institute of Technology in Stockholm), and one each in Denmark (DTU Technical University of Denmark in Lyngby), Finland (University of Helsinki), Norway (Norwegian University of Science and Technology in Trondheim) and the USA (Rensselaer Polytechnic Institute in Troy, New York).

The TU/e plays a central role in the academic, economic and social life of Eindhoven and the surrounding region. In addition the university maintains relations with institutions far beyond that region as well and participates in national and international events (sometimes through the student body).

The TU/e is enormously important to the economy of the Eindhoven region, as well as the wider areas of BrabantStad and the Samenwerkingsverband Regio Eindhoven. It provides highly skilled labor for the local knowledge economy and is a knowledge and research partner for technology companies in the area.

The historic basis for the university's role as an economy and research motor was the interaction with Philips. The university was founded primarily to address the need of Philips for local personnel with academic levels of education in electronics, physics, chemistry and later computer science. Later that interest spread to DAF and Royal Dutch Shell (which became the primary employer for graduates of the chemistry department). There was also a synergy with these companies in that senior personnel were hired from them to form the academic staff of the university (which led to the Eindhoven joke that the university trains the engineers and Philips trains the professors).

Changing economic times and business strategies changed the relationship during the 1980s and 1990s. As Philips started moving away from the region, its importance to the region and the university decreased. A struggle for economic survival forced the university to seek closer ties with the city and region of Eindhoven in the 1989–1995 period, resulting in the creation of the Brainport initiative to draw high tech business and industry to the region. The university started expending more effort in knowledge valorisation, in incubating technology startups, in providing direct knowledge support for local technology companies. Also the academic interests of the research shifted with the times, with more effort going into energy efficiency research, green technologies, and other areas of interest driven by social relevance (the call for better technology in the medical field, for example, led to cooperation with the Catharina Hospital and the University of Maastricht medical department and finally the creation of the Biomedical Technology department).

The TU/e is host (and in some cases also commissioner) of a number of highly successful research schools, including the ESI and the DPI. These research institutes are a source of high-tech knowledge for high-tech companies in the area, such as ASML, NXP and FEI. The university also plays a large role as knowledge and personnel supplier to other companies in the High Tech Campus Eindhoven and helps incubate startups through the Eindhoven Twinning Center. It is also a knowledge supporter of the automotive industry in the Helmond region.

In the extended region, the TU/e is part of the backbone of the Eindhoven-Leuven-Aachen triangle. This economic cooperation agreement between three cities in three countries has created one of the most innovative regions in the European Union (measured in terms of money invested in technology and knowledge economy); the agreement is based on the cooperative triangle that connects the three technical universities in those cities.

As of the summer of 2010, the TU/e is host to the Eindhoven Energy Institute (EEI). The EEI is a virtual research institute (meaning that it doesn't have any actual offices or facilities), which manages and coordinates the activities of a large number of groups and subinstitutes in the general area of sustainable and alternative energy technologies.

The scientific director of the institute is prof.dr.ir. David Smeulders. He is pro forma head of the research department, which is split into four key areas: "Built Environment" (energy usage and patterns in building, headed by prof.dr.ir. Jan Hensen from the Department of the Built Environment), "Future Fuels" (headed by prof.dr. Philip de Goey of Mechanical Engineering), "Energy Conversion" (headed by prof.dr.ir. René Janssen from Chemical Engineering) and "Fusion and Plasma" (headed by prof.dr. Niek Lopes Cardozo from Physics). The EEI also incorporates the Graduate School on Sustainable Energy, which the TU/e had already established together with the TU Munich and DTU Lyngby. Secretarial services will be provided by the Center Technology for Sustainable Development (TDO) which also already existed at the TU/e (since 1994).

Energy research at the TU/e is among the best in academic Europe (a February 2010 study by Reed Elsevier puts it second only to Imperial College London). This fact, as well as the unique attention to energy in the built-up environment, drew the attention of the European Institute of Innovation and Technology. The EEI is now a full co-location of EIT's KIC on Sustainable Energy (InnoEnergy).

The TU/e maintains active academic cooperation with sister institutions in many different countries, for example:


The TU/e also provides education to an increasing number of foreign students and graduates. According to the 2009 annual report in the academic year 2008–2009 there were 490 exchange students, 103 foreign nationals registered in a bachelor program, 430 in a master program, 158 in a professional doctorate program (79% of the total). In 2009 the university employed 37 foreign professors (15.9% of the total) and 16 foreign associate professors (12.8%). Overall, 29.5% of the university staff was non-Dutch.

In 2011/2012, the TU/e has Erasmus bilateral agreements with many universities in 30 countries across Europe in a diverse range of subjects for student exchange.

In addition to the "regular" types of sports practiced among the student body and by the staff, the TU/e collaborates with the student body in a number of "technology sporting efforts". These usually take the form of cross-department projects, which makes them multidisciplinary efforts. Some examples include:


There university is more than just the departments, research bodies and the students. There are several ancillary activities necessary to the running of the university, activities that cross the boundaries and interests of the different departments. These activities are carried out by the universities' service organizations.

The university has the following service organizations:




Eindhoven is currently (2018) ranked between 51 and 141 in the world (the university itself provides a survey), and a top ten technical university in Europe.

In a 2003 European Commission report, TU/e was ranked as third among European research universities (after Cambridge and Oxford, at equality with TU Munich and thus making it the highest ranked Technical University in Europe), based on the impact of its scientific research.
In 2011 Academic Ranking of World Universities (ARWU) rankings, TU/e was placed at the 52-75 bucket internationally in Engineering/Technology and Computer Science ( ENG ) category and at 34th place internationally in the Computer Science subject field.




</doc>
<doc id="9707" url="https://en.wikipedia.org/wiki?curid=9707" title="Electronegativity">
Electronegativity

Electronegativity, symbol "χ", measures the tendency of an atom to attract a shared pair of electrons (or electron density). An atom's electronegativity is affected by both its atomic number and the distance at which its valence electrons reside from the charged nucleus. The higher the associated electronegativity, the more an atom or a substituent group attracts electrons.

On the most basic level, electronegativity is determined by factors like the nuclear charge (the more protons an atom has, the more "pull" it will have on electrons) and the number and location of other electrons in the atomic shells (the more electrons an atom has, the farther from the nucleus the valence electrons will be, and as a result the less positive charge they will experience—both because of their increased distance from the nucleus, and because the other electrons in the lower energy core orbitals will act to shield the valence electrons from the positively charged nucleus).

The opposite of electronegativity is electropositivity: a measure of an element's ability to donate electrons.

The term "electronegativity" was introduced by Jöns Jacob Berzelius in 1811,
though the concept was known before that and was studied by many chemists including Avogadro.
In spite of its long history, an accurate scale of electronegativity was not developed until 1932, when Linus Pauling proposed an electronegativity scale which depends on bond energies, as a development of valence bond theory. It has been shown to correlate with a number of other chemical properties. Electronegativity cannot be directly measured and must be calculated from other atomic or molecular properties. Several methods of calculation have been proposed, and although there may be small differences in the numerical values of the electronegativity, all methods show the same periodic trends between elements.

The most commonly used method of calculation is that originally proposed by Linus Pauling. This gives a dimensionless quantity, commonly referred to as the Pauling scale ("χ"), on a relative scale running from 0.79 to 3.98 (hydrogen = 2.20). When other methods of calculation are used, it is conventional (although not obligatory) to quote the results on a scale that covers the same range of numerical values: this is known as an electronegativity in "Pauling units".

As it is usually calculated, electronegativity is not a property of an atom alone, but rather a property of an atom in a molecule. Properties of a free atom include ionization energy and electron affinity. It is to be expected that the electronegativity of an element will vary with its chemical environment, but it is usually considered to be a transferable property, that is to say that similar values will be valid in a variety of situations.

Caesium is the least electronegative element (0.79); fluorine is the most (3.98). Francium and caesium were originally both assigned 0.7; caesium's value was later refined to 0.79, but no experimental data allows a similar refinement for francium. However, francium's ionization energy is known to be slightly higher than caesium's, in accordance with the relativistic stabilization of the 7s orbital, and this in turn implies that francium is in fact more electronegative than caesium.

Pauling first proposed the concept of electronegativity in 1932 to explain why the covalent bond between two different atoms (A–B) is stronger than the average of the A–A and the B–B bonds. According to valence bond theory, of which Pauling was a notable proponent, this "additional stabilization" of the heteronuclear bond is due to the contribution of ionic canonical forms to the bonding.

The difference in electronegativity between atoms A and B is given by:
where the dissociation energies, "E", of the A–B, A–A and B–B bonds are expressed in electronvolts, the factor (eV) being included to ensure a dimensionless result. Hence, the difference in Pauling electronegativity between hydrogen and bromine is 0.73 (dissociation energies: H–Br, 3.79 eV; H–H, 4.52 eV; Br–Br 2.00 eV)

As only differences in electronegativity are defined, it is necessary to choose an arbitrary reference point in order to construct a scale. Hydrogen was chosen as the reference, as it forms covalent bonds with a large variety of elements: its electronegativity was fixed first at 2.1, later revised to 2.20. It is also necessary to decide which of the two elements is the more electronegative (equivalent to choosing one of the two possible signs for the square root). This is usually done using "chemical intuition": in the above example, hydrogen bromide dissolves in water to form H and Br ions, so it may be assumed that bromine is more electronegative than hydrogen. However, in principle, since the same electronegativities should be obtained for any two bonding compounds, the data are in fact overdetermined, and the signs are unique once a reference point is fixed (usually, for H or F).

To calculate Pauling electronegativity for an element, it is necessary to have data on the dissociation energies of at least two types of covalent bond formed by that element. A. L. Allred updated Pauling's original values in 1961 to take account of the greater availability of thermodynamic data, and it is these "revised Pauling" values of the electronegativity that are most often used.

The essential point of Pauling electronegativity is that there is an underlying, quite accurate, semi-empirical formula for dissociation energies, namely:

or sometimes, a more accurate fit

This is an approximate equation, but holds with good accuracy. Pauling obtained it by noting that a bond can be approximately represented as a quantum mechanical superposition of a covalent bond and two ionic bond-states. The covalent energy of a bond is approximately, by quantum mechanical calculations, the geometric mean of the two energies of covalent bonds of the same molecules, and there is an additional energy that comes from ionic factors, i.e. polar character of the bond.

The geometric mean is approximately equal to the arithmetic mean - which is applied in the first formula above - when the energies are of the similar value, e.g., except for the highly electropositive elements, where there is a larger difference of two dissociation energies; the geometric mean is more accurate and almost always gives a positive excess energy, due to ionic bonding. The square root of this excess energy, Pauling notes, is approximately additive, and hence one can introduce the electronegativity. Thus, it is this semi-empirical formula for bond energy that underlies Pauling electronegativity concept.

The formulas are approximate, but this rough approximation is in fact relatively good and gives the right intuition, with the notion of polarity of the bond and some theoretical grounding in quantum mechanics. The electronegativities are then determined to best fit the data.

In more complex compounds, there is additional error since electronegativity depends on the molecular environment of an atom. Also, the energy estimate can be only used for single, not for multiple bonds. The energy of formation of a molecule containing only single bonds then can be approximated from an electronegativity table, and depends on the constituents and sum of squares of differences of electronegativities of all pairs of bonded atoms. Such a formula for estimating energy typically has relative error of order of 10%, but can be used to get a rough qualitative idea and understanding of a molecule.

Robert S. Mulliken proposed that the arithmetic mean of the first ionization energy (E) and the electron affinity (E) should be a measure of the tendency of an atom to attract electrons. As this definition is not dependent on an arbitrary relative scale, it has also been termed absolute electronegativity, with the units of kilojoules per mole or electronvolts.

However, it is more usual to use a linear transformation to transform these absolute values into values that resemble the more familiar Pauling values. For ionization energies and electron affinities in electronvolts,
and for energies in kilojoules per mole,

The Mulliken electronegativity can only be calculated for an element for which the electron affinity is known, fifty-seven elements as of 2006.
The Mulliken electronegativity of an atom is sometimes said to be the negative of the chemical potential. By inserting the energetic definitions of the ionization potential and electron affinity into the Mulliken electronegativity, it is possible to show that the Mulliken chemical potential is a finite difference approximation of the electronic energy with respect to the number of electrons., i.e.,

A. Louis Allred and Eugene G. Rochow considered that electronegativity should be related to the charge experienced by an electron on the "surface" of an atom: The higher the charge per unit area of atomic surface the greater the tendency of that atom to attract electrons. The effective nuclear charge, "Z", experienced by valence electrons can be estimated using Slater's rules, while the surface area of an atom in a molecule can be taken to be proportional to the square of the covalent radius, "r". When "r" is expressed in picometres,

R.T. Sanderson has also noted the relationship between Mulliken electronegativity and atomic size, and has proposed a method of calculation based on the reciprocal of the atomic volume. With a knowledge of bond lengths, Sanderson's model allows the estimation of bond energies in a wide range of compounds. Sanderson's model has also been used to calculate molecular geometry, "s"-electrons energy, NMR spin-spin constants and other parameters for organic compounds. This work underlies the concept of electronegativity equalization, which suggests that electrons distribute themselves around a molecule to minimize or to equalize the Mulliken electronegativity. This behavior is analogous to the equalization of chemical potential in macroscopic thermodynamics.

Perhaps the simplest definition of electronegativity is that of Leland C. Allen, who has proposed that it is related to the average energy of the valence electrons in a free atom,

where "ε" are the one-electron energies of s- and p-electrons in the free atom and "n" are the number of s- and p-electrons in the valence shell. It is usual to apply a scaling factor, 1.75×10 for energies expressed in kilojoules per mole or 0.169 for energies measured in electronvolts, to give values that are numerically similar to Pauling electronegativities.

The one-electron energies can be determined directly from spectroscopic data, and so electronegativities calculated by this method are sometimes referred to as spectroscopic electronegativities. The necessary data are available for almost all elements, and this method allows the estimation of electronegativities for elements that cannot be treated by the other methods, e.g. francium, which has an Allen electronegativity of 0.67. However, it is not clear what should be considered to be valence electrons for the d- and f-block elements, which leads to an ambiguity for their electronegativities calculated by the Allen method.

In this scale neon has the highest electronegativity of all elements, followed by fluorine, helium, and oxygen.

The wide variety of methods of calculation of electronegativities, which all give results that correlate well with one another, is one indication of the number of chemical properties which might be affected by electronegativity. The most obvious application of electronegativities is in the discussion of bond polarity, for which the concept was introduced by Pauling. In general, the greater the difference in electronegativity between two atoms the more polar the bond that will be formed between them, with the atom having the higher electronegativity being at the negative end of the dipole. Pauling proposed an equation to relate "ionic character" of a bond to the difference in electronegativity of the two atoms, although this has fallen somewhat into disuse.

Several correlations have been shown between infrared stretching frequencies of certain bonds and the electronegativities of the atoms involved: however, this is not surprising as such stretching frequencies depend in part on bond strength, which enters into the calculation of Pauling electronegativities. More convincing are the correlations between electronegativity and chemical shifts in NMR spectroscopy or isomer shifts in Mössbauer spectroscopy (see figure). Both these measurements depend on the s-electron density at the nucleus, and so are a good indication that the different measures of electronegativity really are describing "the ability of an atom in a molecule to attract electrons to itself".

In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements (not counting noble gases), whereas caesium is the least electronegative, at least of those elements for which substantial data is available. This would lead one to believe that caesium fluoride is the compound whose bonding features the most ionic character.

There are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon, respectively, because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity (see Allred-Rochow electronegativity, Sanderson electronegativity above). The anomalously high electronegativity of lead, in particular when compared to thallium and bismuth, is an artifact of electronegativity varying with oxidation state: its electronegativity conforms better to trends if it is quoted for the +2 state instead of the +4 state.

In inorganic chemistry it is common to consider a single value of the electronegativity to be valid for most "normal" situations. While this approach has the advantage of simplicity, it is clear that the electronegativity of an element is "not" an invariable atomic property and, in particular, increases with the oxidation state of the element.

Allred used the Pauling method to calculate separate electronegativities for different oxidation states of the handful of elements (including tin and lead) for which sufficient data was available. However, for most elements, there are not enough different covalent compounds for which bond dissociation energies are known to make this approach feasible. This is particularly true of the transition elements, where quoted electronegativity values are usually, of necessity, averages over several different oxidation states and where trends in electronegativity are harder to see as a result.

The chemical effects of this increase in electronegativity can be seen both in the structures of oxides and halides and in the acidity of oxides and oxoacids. Hence CrO and MnO are acidic oxides with low melting points, while CrO is amphoteric and MnO is a completely basic oxide.

The effect can also be clearly seen in the dissociation constants of the oxoacids of chlorine. The effect is much larger than could be explained by the negative charge being shared among a larger number of oxygen atoms, which would lead to a difference in p"K" of log() = –0.6 between hypochlorous acid and perchloric acid. As the oxidation state of the central chlorine atom increases, more electron density is drawn from the oxygen atoms onto the chlorine, diminishing the partial negative charge of individual oxygen atoms. At the same time, the positive partial charge on the hydrogen increases with higher oxidation state. This explains the observed increased acidity with increasing oxidation state in the oxoacids of chlorine.

The electronegativity of an atom changes depending on the hybridization of the orbital employed in bonding. Electrons in s orbitals are held more tightly than electrons in p orbitals. Hence, a bond to an atom that employs an sp"" hybrid orbital for bonding will be more heavily polarized to that atom when the hybrid orbital has more s character. That is, when electronegativities are compared for different hybridization schemes of a given element, the order χ(sp) < χ(sp) < χ(sp) holds (the trend should apply to non-integer hybridization indices as well). While this holds true in principle for any main-group element, values for the hybridization-specific electronegativity are most frequently cited for carbon. In organic chemistry, these electronegativities are frequently invoked to predict or rationalize bond polarities in organic compounds containing double and triple bonds to carbon.
In organic chemistry, electronegativity is associated more with different functional groups than with individual atoms. The terms group electronegativity and substituent electronegativity are used synonymously. However, it is common to distinguish between the inductive effect and the resonance effect, which might be described as σ- and π-electronegativities, respectively. There are a number of linear free-energy relationships that have been used to quantify these effects, of which the Hammett equation is the best known. Kabachnik parameters are group electronegativities for use in organophosphorus chemistry.

Electropositivity is a measure of an element's ability to donate electrons, and therefore form positive ions; thus, it is opposed to electronegativity.

Mainly, this is an attribute of metals, meaning that, in general, the greater the metallic character of an element the greater the electropositivity. Therefore, the alkali metals are most electropositive of all. This is because they have a single electron in their outer shell and, as this is relatively far from the nucleus of the atom, it is easily lost; in other words, these metals have low ionization energies.

While electronegativity increases along periods in the periodic table, and decreases down groups, electropositivity "decreases" along periods (from left to right) and "increases" down groups. This means that elements in the upper right of the periodic table of elements (oxygen, sulfur, chlorine, etc.) will have the greatest electronegativity, and those in the lower left (rubidium, cesium, and francium) the greatest electropositivity.





</doc>
<doc id="9708" url="https://en.wikipedia.org/wiki?curid=9708" title="European Charter for Regional or Minority Languages">
European Charter for Regional or Minority Languages

The European Charter for Regional or Minority Languages (ECRML) is a European treaty (CETS 148) adopted in 1992 under the auspices of the Council of Europe to protect and promote historical regional and minority languages in Europe. The preparation for the charter was undertaken by the predecessor to the current Congress of Local and Regional Authorities, the Standing Conference of Local and Regional Authorities of Europe because involvement of local and regional government was essential. The actual charter was written in the Parliamentary Assembly based on the Congress' Recommendations. It only applies to languages traditionally used by the nationals of the State Parties (thus excluding languages used by recent immigrants from other states, see immigrant languages), which significantly differ from the majority or official language (thus excluding what the state party wishes to consider as mere local dialects of the official or majority language) and that either have a territorial basis (and are therefore traditionally spoken by populations of regions or areas within the State) or are used by linguistic minorities within the State as a whole (thereby including such languages as Yiddish, Romani and Lemko, which are used over a wide geographic area).

Some states, such as Ukraine and Sweden, have tied the status of minority language to the recognized national minorities, which are defined by ethnic, cultural and/or religious criteria, thereby circumventing the Charter's notion of linguistic minority. 

Languages that are official within regions, provinces or federal units within a State (for example Catalan in Spain) are not classified as official languages of the State and may therefore benefit from the Charter. On the other hand, Ireland has not been able to sign the Charter on behalf of the Irish language (although a minority language) as it is defined as the first official language of the state. The United Kingdom has ratified the Charter in respect to (among other languages) Welsh in Wales, Scots and Gaelic in Scotland, and Irish in Northern Ireland. France, although a signatory, has been constitutionally blocked from ratifying the Charter in respect to the languages of France.

The charter provides many actions state parties can take to protect and promote historical regional and minority languages. There are two levels of protection—all signatories must apply the lower level of protection to qualifying languages. Signatories may further declare that a qualifying language or languages will benefit from the higher level of protection, which lists a range of actions from which states must agree to undertake at least 35.

Countries can ratify the charter in respect of its minority languages based on Part II or Part III of the charter, which contain varying principles. Countries can treat languages differently under the charter, for example, in the United Kingdom, the Welsh language is ratified under the general Part II principles as well as the more specific Part III commitments, while the Cornish language is ratified only under Part II.

Part II of the Charter details eight main principles and objectives upon which States must base their policies and legislation. They are seen as a framework for the preservation of the languages concerned.

Part III details comprehensive rules, across a number of sectors, by which states agree to abide. Each language to which Part III of the Charter is applied must be named specifically by the government. States must select at least thirty-five of the undertakings in respect to each language. Many provisions contain several options, of varying degrees of stringency, one of which has to be chosen “according to the situation of each language”. The areas from which these specific undertakings must be chosen are as follows:

Countries that have ratified the Charter, and languages for which the ratification was made:




</doc>
<doc id="9709" url="https://en.wikipedia.org/wiki?curid=9709" title="English Civil War">
English Civil War

The English Civil War (1642–1651) was a series of civil wars and political machinations between Parliamentarians ("Roundheads") and Royalists ("Cavaliers") principally over the manner of England's governance and part of the wider Wars of the Three Kingdoms. The first (1642–1646) and second (1648–1649) wars pitted the supporters of King Charles I against the supporters of the Long Parliament, while the third (1649–1651) saw fighting between supporters of King Charles II and supporters of the Rump Parliament. The war ended with Parliamentarian victory at the Battle of Worcester on 3 September 1651.

Unlike other civil wars in England, which were mainly fought over "who" should rule, these conflicts were also concerned with "how" the three kingdoms of England, Scotland, and Ireland were to be governed. The outcome was threefold: the trial and execution of Charles I (1649); the exile of his son, Charles II (1651); and the replacement of English monarchy with, at first, the Commonwealth of England (1649–1653) and then the Protectorate, which as the Commonwealth of England, Scotland, and Ireland unified the British Isles under the personal rule of Oliver Cromwell (1653–1658) and briefly his son Richard (1658–1659). The execution of Charles I was particularly notable given that an English king had never been executed before. In England, the monopoly of the Church of England on Christian worship was ended, while in Ireland the victors consolidated the established Protestant Ascendancy. Constitutionally, the wars established the precedent that an English monarch cannot govern without Parliament's consent, although the idea of Parliamentary sovereignty was only legally established as part of the Glorious Revolution in 1688.

The term "English Civil War" appears most often in the singular, although historians often divide the conflict into two or three separate wars. These were not restricted to England, as Wales was part of the Kingdom of England and affected accordingly. The conflicts also involved wars with Scotland and Ireland, and civil wars within them.

The wars spanning all four countries are known as the Wars of the Three Kingdoms. In the early 19th century, Sir Walter Scott referred to it as "the Great Civil War".

The 1911 "Encyclopædia Britannica" called the series of conflicts the "Great Rebellion", while some historians – notably Marxists such as Christopher Hill (1912–2003) – long favoured the term "English Revolution".

Each side had a geographical stronghold, such that minority elements were silenced or fled. The Royalist areas included the countryside, the shires, the cathedral city of Oxford, and the less economically developed areas of northern and western England. Parliament's strengths spanned the industrial centres, ports, and economically advanced regions of southern and eastern England, including the remaining cathedral cities (except York, Chester, Worcester). Lacey Baldwin Smith says, "the words "populous, rich, and rebellious" seemed to go hand in hand".

Many officers and veteran soldiers had fought in European wars, notably the Eighty Years' War between the Spanish and the Dutch, which began in 1568.

The main battle tactic came to be known as pike and shot infantry. The two sides would line up opposite one another, with infantry brigades of musketeers in the centre. These carried matchlock muskets, an inaccurate weapon which nevertheless could be lethal at a range of up to 300 yards. Musketeers would assemble three rows deep, the first kneeling, second crouching, and third standing, allowing all to fire a volley simultaneously. At times, troops divided into two groups, allowing one to reload while the other fired. Among the musketeers were pike men, carrying pikes of to long, whose main purpose was to protect the musketeers from cavalry charges. Positioned on each side of the infantry were cavalry, with a right wing led by the lieutenant-general and left by the commissary general. Its main aim was to rout the opponents' cavalry, then turn and overpower their infantry.

The Royalist cavaliers' skill and speed on horseback led to many early victories. Prince Rupert, commanding the king's cavalry, used a tactic learned while fighting in the Dutch army, where cavalry would charge at full speed into the opponent's infantry, firing their pistols just before impact.

However, with Oliver Cromwell and the introduction of the more disciplined New Model Army, a group of disciplined pike men would stand its ground, which could have a devastating effect.

The Royalist cavalry had a tendency to chase down individual targets after the initial charge, leaving their forces scattered and tired, whereas Cromwell's cavalry was slower but better disciplined. Trained to operate as a single unit, it went on to win many decisive victories.

The English Civil War broke out in 1642, less than 40 years after the death of Queen Elizabeth I. Elizabeth had been succeeded by her first cousin twice-removed, King James VI of Scotland, as James I of England, creating the first personal union of the Scottish and English kingdoms. As King of Scots, James had become accustomed to Scotland's weak parliamentary tradition since assuming control of the Scottish government in 1583, so that upon assuming power south of the border, the new King of England was affronted by the constraints the English Parliament attempted to place on him in exchange for money. In spite of this, James's personal extravagance meant he was perennially short of money and had to resort to extra-parliamentary sources of income.

This extravagance was tempered by James's peaceful disposition, so that by the succession of his son Charles I in 1625 the two kingdoms had both experienced relative peace, internally and in their relations with each other. Charles followed his father's dream in hoping to unite the kingdoms of England, Scotland and Ireland into a single kingdom. Many English Parliamentarians were suspicious of such a move, fearing that such a new kingdom might destroy old English traditions that had bound the English monarchy. As Charles shared his father's position on the power of the crown (James had described kings as "little gods on Earth", chosen by God to rule in accordance with the doctrine of the "Divine Right of Kings"), the suspicions of the Parliamentarians had some justification.

At the time, the Parliament of England did not have a large permanent role in the English system of government. Instead, it functioned as a temporary advisory committee and was summoned only if and when the monarch saw fit. Once summoned, a Parliament's continued existence was at the king's pleasure since it was subject to dissolution by him at any time.

Yet in spite of this limited role, Parliament had acquired over the centuries "de facto" powers of enough significance that monarchs could not simply ignore them indefinitely. For a monarch, Parliament's most indispensable power was its ability to raise tax revenues far in excess of all other sources of revenue at the Crown's disposal. By the 17th century, Parliament's tax-raising powers had come to be derived from the fact that the gentry was the only stratum of society with the ability and authority to collect and remit the most meaningful forms of taxation then available at the local level. So if the king wanted to ensure smooth revenue collection, he needed gentry co-operation. For all of the Crown's legal authority, its resources were limited by any modern standard to an extent that if the gentry refused to collect the king's taxes on a national scale, the Crown lacked a practical means of compelling them.

From the thirteenth century, monarchs ordered the election of representatives to sit in the House of Commons, with most voters being the owners of property, although in some potwalloper boroughs every male householder could vote. When assembled along with the House of Lords, these elected representatives formed a Parliament. So the concept of Parliaments allowed representatives of the property-owning class to meet, primarily, at least from the point of view of the monarch, to sanction whatever taxes the monarch wished to collect. In the process, the representatives could debate and enact statutes, or acts. However, Parliament lacked the power to force its will upon the monarch; its only leverage was the threat of withholding the financial means required to implement his plans.

Many concerns were raised over Charles's marriage in 1625 to a Roman Catholic French princess: Henrietta Maria. Parliament refused to assign him the traditional right to collect customs duties for his entire reign, deciding instead to grant it only on a provisional basis and negotiate with him.

Charles, meanwhile, decided to send an expeditionary force to relieve the French Huguenots, whom French royal troops held besieged in La Rochelle. Such military support for Protestants on the Continent potentially alleviated concerns about the King's marriage to a Catholic. However, Charles's insistence on giving command of the English force to his unpopular royal favourite George Villiers, the Duke of Buckingham, undermined that support. Unfortunately for Charles and Buckingham, the relief expedition proved a fiasco (1627), and Parliament, already hostile to Buckingham for his monopoly on royal patronage, opened impeachment proceedings against him. Charles responded by dissolving Parliament. This saved Buckingham but confirmed the impression that Charles wanted to avoid Parliamentary scrutiny of his ministers.

Having dissolved Parliament and unable to raise money without it, the king assembled a new one in 1628. (The elected members included Oliver Cromwell, John Hampden, and Edward Coke.) The new Parliament drew up a Petition of Right, which Charles accepted as a concession to obtain his subsidy. The Petition made reference to Magna Carta, but did not grant him the right of tonnage and poundage, which Charles had been collecting without Parliamentary authorisation since 1625. Several more active members of the opposition were imprisoned, which caused outrage; one, John Eliot, subsequently died in prison and came to be seen as a martyr for the rights of Parliament.

Charles avoided calling a Parliament for the next decade, a period known as the "personal rule of Charles I", or the "Eleven Years' Tyranny". During this period, Charles's policies were determined by his lack of money. First and foremost, to avoid Parliament, the King needed to avoid war. Charles made peace with France and Spain, effectively ending England's involvement in the Thirty Years' War. However, that in itself was far from enough to balance the Crown's finances.

Unable to raise revenue without Parliament and unwilling to convene it, Charles resorted to other means. One was to revive conventions, often outdated. For example, a failure to attend and receive knighthood at Charles's coronation became a finable offence with the fine paid to the Crown. The King also tried to raise revenue through ship money, demanding in 1634–1636 that the inland English counties pay a tax for the Royal Navy to counter the threat of privateers and pirates in the English Channel. Established law supported the policy of coastal counties and inland ports such as London paying ship money in times of need, but it had not been applied to inland counties before. Authorities had ignored it for centuries, and many saw it as yet another extra-Parliamentary, illegal tax, which prompted some prominent men to refuse to pay it. Charles issued a writ against John Hampden for his failure to pay, and although five judges including Sir George Croke supported Hampden, seven judges found in favour of the King in 1638. The fines imposed on people who refused to pay ship money and standing out against its illegality aroused widespread indignation.

During his "Personal Rule", Charles aroused most antagonism through his religious measures. He believed in High Anglicanism, a sacramental version of the Church of England, theologically based upon Arminianism, a creed shared with his main political adviser, Archbishop William Laud. In 1633, Charles appointed Laud Archbishop of Canterbury and started making the Church more ceremonial, replacing the wooden communion tables with stone altars. Puritans accused Laud of reintroducing Catholicism, and when they complained he had them arrested. In 1637, John Bastwick, Henry Burton, and William Prynne had their ears cut off for writing pamphlets attacking Laud's views — a rare penalty for gentlemen, and one that aroused anger. Moreover, the Church authorities revived statutes from the time of Elizabeth I about church attendance and fined Puritans for not attending Anglican services.

The end of Charles's independent governance came when he attempted to apply the same religious policies in Scotland. The Church of Scotland, reluctantly episcopal in structure, had independent traditions. Charles wanted one uniform Church throughout Britain and introduced a new, High Anglican version of the English Book of Common Prayer to Scotland in the middle of 1637. This was violently resisted. A riot broke out in Edinburgh, which may have been started in St Giles' Cathedral, according to legend, by Jenny Geddes. In February 1638, the Scots formulated their objections to royal policy in the National Covenant. This document took the form of a "loyal protest", rejecting all innovations not first tested by free Parliaments and General Assemblies of the Church.

In the spring of 1639, King Charles I accompanied his forces to the Scottish border to end the rebellion known as the Bishops' War, but after an inconclusive campaign, he accepted the offered Scottish truce: the Pacification of Berwick. This truce proved temporary, and a second war followed in mid-1640. A Scots army defeated Charles's forces in the north, then captured Newcastle. Charles eventually agreed not to interfere in Scotland's religion and paid the Scots' war expenses.

Charles needed to suppress the rebellion in Scotland, but had insufficient funds to do so. He needed to seek money from a newly elected English Parliament in 1640. Its majority faction, led by John Pym, used this appeal for money as a chance to discuss grievances against the Crown and oppose the idea of an English invasion of Scotland. Charles took exception to this "lèse-majesté" (offense against the ruler) and dissolved the Parliament after only a few weeks; hence its name, "the Short Parliament".

Without Parliament's support, Charles attacked Scotland again, breaking the truce at Berwick, and suffered comprehensive defeat. The Scots went on to invade England, occupying Northumberland and Durham. Meanwhile, another of Charles's chief advisers, Thomas Wentworth, 1st Viscount Wentworth, had risen to the role of Lord Deputy of Ireland in 1632, and brought in much-needed revenue for Charles by persuading the Irish Catholic gentry to pay new taxes in return for promised religious concessions.

In 1639, Charles had recalled Wentworth to England and in 1640 made him Earl of Strafford, attempting to have him achieve similar results in Scotland. This time he proved less successful and the English forces fled the field at their second encounter with the Scots in 1640. Almost the whole of Northern England was occupied and Charles forced to pay £850 per day to keep the Scots from advancing. Had he not done so they would have pillaged and burnt the cities and towns of Northern England.

All this put Charles in a desperate financial state. As King of Scots, he had to find money to pay the Scottish army in England; as King of England, he had to find money to pay and equip an English army to defend England. His means of raising English revenue without an English Parliament fell critically short of achieving this. Against this backdrop, and according to advice from the Magnum Concilium (the House of Lords, but without the Commons, so not a Parliament), Charles finally bowed to pressure and summoned another English Parliament in November 1640.

The new Parliament proved even more hostile to Charles than its predecessor. It immediately began to discuss grievances against him and his government, with Pym and Hampden (of ship money fame) in the lead. They took the opportunity presented by the King's troubles to force various reforming measures — including many with strong "anti-Papist" themes — upon him. The members passed a law stating that a new Parliament would convene at least once every three years — without the King's summons if need be. Other laws passed making it illegal for the king to impose taxes without Parliamentary consent and later gave Parliament control over the king's ministers. Finally, the Parliament passed a law forbidding the King to dissolve it without its consent, even if the three years were up. Ever since this Parliament has been known as the Long Parliament. However, Parliament did attempt to avert conflict by requiring all adults to sign The Protestation, an oath of allegiance to Charles.

Early in the Long Parliament, the house overwhelmingly accused Thomas Wentworth, Earl of Strafford of high treason and other crimes and misdemeanors.

Henry Vane the Younger supplied evidence of Strafford's claimed improper use of the army in Ireland, alleging that he had encouraged the King to use his Ireland-raised forces to threaten England into compliance. This evidence was obtained from Vane's father, Henry Vane the Elder, a member of the King's Privy council, who refused to confirm it in Parliament out of loyalty to Charles. On 10 April 1641, Pym's case collapsed, but Pym made a direct appeal to Henry Vane the Younger to produce a copy of the notes from the King's Privy council, discovered by the younger Vane and secretly turned over to Pym, to the great anguish of the Elder Vane. These notes contained evidence that Strafford had told the King, "Sir, you have done your duty, and your subjects have failed in theirs; and therefore you are absolved from the rules of government, and may supply yourself by extraordinary ways; you have an army in Ireland, with which you may reduce the kingdom."

Pym immediately launched a Bill of Attainder stating Strafford's guilt and demanding that he be put to death. Unlike a guilty verdict in a court case, attainder did not require a legal burden of proof, but it did require the king's approval. Charles, however, guaranteed Strafford that he would not sign the attainder, without which the bill could not be passed. Furthermore, the Lords opposed the severity of a death sentence on Strafford. Yet increased tensions and a plot in the army to support Strafford began to sway the issue. On 21 April, the Commons passed the Bill (204 in favour, 59 opposed, and 250 abstained), and the Lords acquiesced. Charles, still incensed over the Commons' handling of Buckingham, refused his assent. Strafford himself, hoping to head off the war he saw looming, wrote to the king and asked him to reconsider. Charles, fearing for the safety of his family, signed on 10 May. Strafford was beheaded two days later. In the meantime both Parliament and the King agreed to an independent investigation into the king's involvement in Strafford's plot.

The Long Parliament then passed the Triennial Act, also known as the Dissolution Act in May 1641, to which the Royal Assent was readily granted. The Triennial Act required Parliament to be summoned at least once in three years. When the King failed to issue a proper summons, the members could assemble on their own. This act also forbade ship money without Parliament's consent, fines in distraint of knighthood, and forced loans. Monopolies were cut back sharply, the Courts of the Star Chamber and High Commission abolished by the Habeas Corpus Act 1640, and the Triennial Act respectively. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. On 3 May, Parliament decreed The Protestation, attacking the 'wicked counsels' of Charles's government, whereby those who signed the petition undertook to defend 'the true reformed religion', Parliament, and the king's person, honour and estate. Throughout May, the House of Commons launched several bills attacking bishops and Episcopalianism in general, each time defeated in the Lords.

Charles and his Parliament hoped that the execution of Strafford and the Protestation would end the drift towards war, but in fact, they encouraged it. Charles and his supporters continued to resent Parliament's demands, and Parliamentarians continued to suspect Charles of wanting to impose episcopalianism and unfettered royal rule by military force. Within months, the Irish Catholics, fearing a resurgence of Protestant power, struck first, and all Ireland soon descended into chaos. Rumors circulated that the King supported the Irish, and Puritan members of the Commons soon started murmuring that this exemplified the fate that Charles had in store for them all.

In early January 1642, Charles, accompanied by 400 soldiers, attempted to arrest five members of the House of Commons on a charge of treason. This attempt failed. When the troops marched into Parliament, Charles enquired of William Lenthall, the Speaker, as to the whereabouts of the five. Lenthall replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." So the Speaker proclaimed himself a servant of Parliament, rather than the King.

In the summer of 1642, these national troubles helped to polarise opinion, ending indecision about which side to support or what action to take. Opposition to Charles also arose from many local grievances. For example, imposed drainage schemes in The Fens disrupted the livelihood of thousands after the King awarded a number of drainage contracts. Many saw the King as indifferent to public welfare, and this played a role in bringing much of eastern England into the Parliamentarian camp. This sentiment brought with it such people as the Earl of Manchester and Oliver Cromwell, each a notable wartime adversary of the King. Conversely, one of the leading drainage contractors, the Earl of Lindsey, was to die fighting for the King at the Battle of Edgehill.

In early January 1642, a few days after failing to capture five members of the House of Commons, Charles feared for the safety of his family and retinue and left the London area for the north country. Further frequent negotiations by letter between the King and the Long Parliament, through to early summer, proved fruitless. As the summer progressed, cities and towns declared their sympathies for one faction or the other: for example, the garrison of Portsmouth commanded by Sir George Goring declared for the King, but when Charles tried to acquire arms from Kingston upon Hull, the weaponry depository used in the previous Scottish campaigns, Sir John Hotham, the military governor appointed by Parliament in January, refused to let Charles enter the town, and when Charles returned with more men later, Hotham drove them off. Charles issued a warrant for Hotham's arrest as a traitor but was powerless to enforce it. Throughout the summer, tensions rose and there was brawling in several places, the first death from the conflict taking place in Manchester.

At the outset of the conflict, much of the country remained neutral, though the Royal Navy and most English cities favoured Parliament, while the King found marked support in rural communities. Historians estimate that both sides had only about 15,000 men between them, but the war quickly spread and eventually involved every level of society. Many areas attempted to remain neutral. Some formed bands of Clubmen to protect their localities from the worst excesses of the armies of both sides, but most found it impossible to withstand both King and Parliament. On one side, the King and his supporters fought for traditional government in church and state, while on the other, most Parliamentarians initially took up arms to defend what they saw as a traditional balance of government in church and state, which the bad advice the King received from his advisers had undermined before and during the "Eleven Years' Tyranny". The views of the members of Parliament ranged from unquestioning support of the King — at one point during the First Civil War, more members of the Commons and Lords gathered in the King's Oxford Parliament than at Westminster — through to radicals who sought major reforms in religious independence and redistribution of power at a national level. However, even the most radical Parliamentarian supporters still favoured keeping Charles on the throne.

After the debacle at Hull, Charles moved on to Nottingham, raising the royal standard there on 22 August 1642. At the time, Charles had with him about 2,000 cavalries and a small number of Yorkshire infantrymen, and using the archaic system of a Commission of Array, his supporters started to build a larger army around the standard. Charles moved in a westerly direction, first to Stafford, then on to Shrewsbury, as support for his cause seemed particularly strong in the Severn valley area and in North Wales. While passing through Wellington, he declared in what became known as the "Wellington Declaration" that he would uphold the "Protestant religion, the laws of England, and the liberty of Parliament".

The Parliamentarians who opposed the King did not remain passive in this pre-war period. As in Hull, they took measures to secure strategic towns and cities by appointing to office men sympathetic to their cause. On 9 June they voted to raise an army of 10,000 volunteers and appointed Robert Devereux, 3rd Earl of Essex its commander three days later. He received orders "to rescue His Majesty's person, and the persons of the Prince [of Wales] and the Duke of York [James II] out of the hands of those desperate persons who were about them." The Lords Lieutenant whom Parliament appointed used the Militia Ordinance to order the militia to join Essex's army.

Two weeks after the King had raised his standard at Nottingham, Essex led his army north towards Northampton, picking up support along the way (including a detachment of Huntingdonshire cavalry raised and commanded by Oliver Cromwell). By mid-September Essex's forces had grown to 21,000 infantry and 4,200 cavalries and dragoons. On 14 September he moved his army to Coventry and then to the north of the Cotswolds, a strategy that placed it between the Royalists and London. With the size of both armies now in the tens of thousands and only Worcestershire between them, it was inevitable that cavalry reconnaissance units would meet sooner or later. This happened in the first major skirmish of the Civil War, when a troop of about 1,000 Royalist cavalry under Prince Rupert, a German nephew of the King and one of the outstanding cavalry commanders of the war, defeated a Parliamentary cavalry detachment under Colonel John Brown at the Battle of Powick Bridge, which crossed the River Teme close to Worcester.
Rupert withdrew to Shrewsbury, where a council-of-war discussed two courses of action: whether to advance towards Essex's new position near Worcester, or march down the now open road towards London. The Council decided on the London route, but not to avoid a battle, for the Royalist generals wanted to fight Essex before he grew too strong, and the temper of both sides made it impossible to postpone the decision. In the Earl of Clarendon's words, "it was considered more counsellable to march towards London, it being morally sure that the earl of Essex would put himself in their way." So the army left Shrewsbury on 12 October, gaining two days' start on the enemy, and moved south-east. This had the desired effect of forcing Essex to move to intercept them.

The first pitched battle of the war, at Edgehill on 23 October 1642, proved inconclusive, both Royalists and Parliamentarians claiming victory. The second field action, the stand-off at Turnham Green, saw Charles forced to withdraw to Oxford, which would serve as his base for the rest of the war.

In 1643, Royalist forces won at Adwalton Moor, gaining control of most of Yorkshire. In the Midlands, a Parliamentary force under Sir John Gell besieged and captured the cathedral city of Lichfield, after the death of the original commander, Lord Brooke. This group then joined forces with Sir John Brereton at the inconclusive Battle of Hopton Heath (19 March 1643), where the Royalist commander, the Earl of Northampton, was killed. John Hampden died after being wounded in the Battle of Chalgrove Field (18 June 1643). Subsequent battles in the west of England at Lansdowne and Roundway Down also went to the Royalists. Prince Rupert could then take Bristol. In the same year, however, Cromwell formed his troop of "Ironsides", a disciplined unit that demonstrated his military leadership ability. With their assistance he won a victory at the Battle of Gainsborough in July.

At this stage, from 7 to 9 August 1643, there were some popular demonstrations in London — both for and against war. They were protesting at Westminster. A peace demonstration by London women, which turned violent, was suppressed by William Waller's regiment of horse. Some women were beaten and even killed, and many arrested.

After these August events, the representative of Venice in England reported to the doge that the London government took considerable measures to stifle dissent.

In general, the early part of the war went well for the Royalists. The turning point came in the late summer and early autumn of 1643, when the Earl of Essex's army forced the king to raise the Siege of Gloucester and then brushed the Royalists aside at the First Battle of Newbury (20 September 1643), to return triumphantly to London. Parliamentarian forces led by the Earl of Manchester besieged the port of King's Lynn, Norfolk, which under Sir Hamon L'Estrange held out until September. Other forces won the Battle of Winceby, giving them control of Lincoln. Political manœuvring to gain an advantage in numbers led Charles to negotiate a ceasefire in Ireland, freeing up English troops to fight on the Royalist side in England, while Parliament offered concessions to the Scots in return for aid and assistance.
Helped by the Scots, Parliament won at Marston Moor (2 July 1644), gaining York and the north of England. Cromwell's conduct in the battle proved decisive, and showed his potential as a political and as an important military leader. The defeat at the Battle of Lostwithiel in Cornwall, however, marked a serious reverse for Parliament in the south-west of England. Subsequent fighting around Newbury (27 October 1644), though tactically indecisive, strategically gave another check to Parliament.

In 1645, Parliament reaffirmed its determination to fight the war to a finish. It passed the Self-denying Ordinance, by which all members of either House of Parliament laid down their commands and re-organized its main forces into the New Model Army, under the command of Sir Thomas Fairfax, with Cromwell as his second-in-command and Lieutenant-General of Horse. In two decisive engagements — the Battle of Naseby on 14 June and the Battle of Langport on 10 July — the Parliamentarians effectively destroyed Charles's armies.

In the remains of his English realm, Charles tried to recover a stable base of support by consolidating the Midlands. He began to form an axis between Oxford and Newark-on-Trent in Nottinghamshire. These towns had become fortresses and showed more reliable loyalty to him than others. He took Leicester, which lies between them, but found his resources exhausted. Having little opportunity to replenish them, in May 1646 he sought shelter with a Presbyterian Scottish army at Southwell in Nottinghamshire. Charles was eventually handed over to the English Parliament by the Scots and imprisoned. This marked the end of the First English Civil War.

The end of the First Civil War, in 1646, left a partial power vacuum in which any combination of the three English factions, Royalists, Independents of the New Model Army ("the Army"), and Presbyterians of the English Parliament, as well as the Scottish Parliament allied with the Scottish Presbyterians (the "Kirk"), could prove strong enough to dominate the rest. Armed political Royalism was at an end, but despite being a prisoner, Charles I was considered by himself and his opponents (almost to the last) as necessary to ensure the success of whichever group could come to terms with him. Thus he passed successively into the hands of the Scots, the Parliament and the Army. The King attempted to reverse the verdict of arms by "coquetting" with each in turn. On 3 June 1647, Cornet George Joyce of Thomas Fairfax's horse seized the King for the Army, after which the English Presbyterians and the Scots began to prepare for a fresh civil war, less than two years after the conclusion of the first, this time against "Independency", as embodied in the Army. After making use of the Army's sword, its opponents attempted to disband it, to send it on foreign service and to cut off its arrears of pay. The result was that the Army leadership was exasperated beyond control, and, remembering not merely their grievances but also the principle for which the Army had fought, it soon became the most powerful political force in the realm. From 1646 to 1648 the breach between Army and Parliament widened day by day until finally the Presbyterian party, combined with the Scots and the remaining Royalists, felt itself strong enough to begin a Second Civil War.

Charles I took advantage of the deflection of attention away from himself to negotiate on 28 December 1647 a secret treaty with the Scots, again promising church reform. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles's behalf and restore him to the throne on condition of the establishment of Presbyterianism within three years.

A series of Royalist uprisings throughout England and a Scottish invasion occurred in the summer of 1648. Forces loyal to Parliament put down most of those in England after little more than a skirmish, but uprisings in Kent, Essex and Cumberland, the rebellion in Wales, and the Scottish invasion involved pitched battles and prolonged sieges.

In the spring of 1648, unpaid Parliamentarian troops in Wales changed sides. Colonel Thomas Horton defeated the Royalist rebels at the Battle of St Fagans (8 May) and the rebel leaders surrendered to Cromwell on 11 July after a protracted two-month siege of Pembroke. Sir Thomas Fairfax defeated a Royalist uprising in Kent at the Battle of Maidstone on 1 June. Fairfax, after his success at Maidstone and the pacification of Kent, turned north to reduce Essex, where, under an ardent, experienced and popular leader, Sir Charles Lucas, the Royalists had taken up arms in great numbers. Fairfax soon drove the enemy into Colchester, but his first attack on the town met with a repulse and he had to settle down to a long siege.

In the North of England, Major-General John Lambert fought a successful campaign against several Royalist uprisings, the largest being that of Sir Marmaduke Langdale in Cumberland. Thanks to Lambert's successes, the Scottish commander, the Duke of Hamilton, had to take a western route through Carlisle in his pro-Royalist Scottish invasion of England. The Parliamentarians under Cromwell engaged the Scots at the Battle of Preston (17–19 August). The battle took place largely at Walton-le-Dale near Preston, Lancashire, and resulted in a victory for Cromwell's troops over the Royalists and Scots commanded by Hamilton. This victory marked the end of the Second English Civil War.

Nearly all the Royalists who had fought in the First Civil War had given their word not to bear arms against Parliament, and many, like Lord Astley, were therefore bound by oath not to take any part in the second conflict. So the victors in the Second Civil War showed little mercy to those who had brought war into the land again. On the evening of the surrender of Colchester, Parliamentarians had Sir Charles Lucas and Sir George Lisle shot. Parliamentary authorities sentenced the leaders of the Welsh rebels, Major-General Rowland Laugharne, Colonel John Poyer and Colonel Rice Powel to death, but executed only Poyer (25 April 1649), having selected him by lot. Of five prominent Royalist peers who had fallen into Parliamentary hands, three – the Duke of Hamilton, the Earl of Holland, and Lord Capel, one of the Colchester prisoners and a man of high character – were beheaded at Westminster on 9 March.

Charles's secret pacts and encouragement of supporters to break their parole caused Parliament to debate whether to return the King to power at all. Those who still supported Charles's place on the throne, such as the army leader and moderate Fairfax, tried again to negotiate with him. The Army, furious that Parliament continued to countenance Charles as a ruler, then marched on Parliament and conducted "Pride's Purge" (named after the commanding officer of the operation, Thomas Pride) in December 1648. Troops arrested 45 members and kept 146 out of the chamber. They allowed only 75 members in, and then only at the Army's bidding. This Rump Parliament received orders to set up, in the name of the people of England, a High Court of Justice for the trial of Charles I for treason. Fairfax, a constitutional monarchist and moderate, declined to have anything to do with the trial. He resigned as head of the army, so clearing Cromwell's road to power.

At the end of the trial the 59 Commissioners (judges) found Charles I guilty of high treason as a "tyrant, traitor, murderer and public enemy". His beheading took place on a scaffold in front of the Banqueting House of the Palace of Whitehall on 30 January 1649. After the Restoration in 1660, nine of the surviving regicides not living in exile were executed and most others sentenced to life imprisonment.

After the regicide, Charles as the eldest son was publicly proclaimed King Charles II in the Royal Square of St. Helier, Jersey, on 17 February 1649 (after a first such proclamation in Edinburgh on 5 February 1649).

Ireland had undergone continual war since the rebellion of 1641, with most of the island controlled by the Irish Confederates. Increasingly threatened by the armies of the English Parliament after Charles I's arrest in 1648, the Confederates signed a treaty of alliance with the English Royalists. The joint Royalist and Confederate forces under the Duke of Ormonde tried to eliminate the Parliamentary army holding Dublin by laying siege, but their opponents routed them at the Battle of Rathmines (2 August 1649). As the former Member of Parliament Admiral Robert Blake blockaded Prince Rupert's fleet in Kinsale, Cromwell could land at Dublin on 15 August 1649 with an army to quell the Royalist alliance.

Cromwell's suppression of the Royalists in Ireland in 1649 is still remembered by many Irish people. After the Siege of Drogheda, the massacre of nearly 3,500 people — around 2,700 Royalist soldiers and 700 others, including civilians, prisoners and Catholic priests (Cromwell claimed all had carried arms) — became one of the historical memories that has driven Irish-English and Catholic-Protestant strife during the last three centuries. The Parliamentarian conquest of Ireland ground on for another four years until 1653, when the last Irish Confederate and Royalist troops surrendered. In the wake of the conquest, the victors confiscated almost all Irish Catholic-owned land and distributed it to Parliament's creditors, to Parliamentary soldiers who served in Ireland, and to English who had settled there before the war.

The execution of Charles I altered the dynamics of the Civil War in Scotland, which had raged between Royalists and Covenanters since 1644. By 1649, the struggle had left the Royalists there in disarray and their erstwhile leader, the Marquess of Montrose, had gone into exile. At first, Charles II encouraged Montrose to raise a Highland army to fight on the Royalist side. However, when the Scottish Covenanters (who did not agree with the execution of Charles I and who feared for the future of Presbyterianism under the new Commonwealth) offered him the crown of Scotland, Charles abandoned Montrose to his enemies. However, Montrose, who had raised a mercenary force in Norway, had already landed and could not abandon the fight. He did not succeed in raising many Highland clans and the Covenanters defeated his army at the Battle of Carbisdale in Ross-shire on 27 April 1650. The victors captured Montrose shortly afterwards and took him to Edinburgh. On 20 May the Scottish Parliament sentenced him to death and had him hanged the next day.
Charles II landed in Scotland at Garmouth in Morayshire on 23 June 1650 and signed the 1638 National Covenant and the 1643 Solemn League and Covenant shortly after coming ashore. With his original Scottish Royalist followers and his new Covenanter allies, Charles II became the greatest threat facing the new English republic. In response to the threat, Cromwell left some of his lieutenants in Ireland to continue the suppression of the Irish Royalists and returned to England.

He arrived in Scotland on 22 July 1650 and proceeded to lay siege to Edinburgh. By the end of August, disease and a shortage of supplies had reduced his army, and he had to order a retreat towards his base at Dunbar. A Scottish army under the command of David Leslie tried to block the retreat, but Cromwell defeated them at the Battle of Dunbar on 3 September. Cromwell's army then took Edinburgh, and by the end of the year his army had occupied much of southern Scotland.

In July 1651, Cromwell's forces crossed the Firth of Forth into Fife and defeated the Scots at the Battle of Inverkeithing (20 July 1651). The New Model Army advanced towards Perth, which allowed Charles, at the head of the Scottish army, to move south into England. Cromwell followed Charles into England, leaving George Monck to finish the campaign in Scotland. Monck took Stirling on 14 August and Dundee on 1 September. The next year, 1652, saw a mopping up of the remnants of Royalist resistance, and under the terms of the "Tender of Union", the Scots received 30 seats in a united Parliament in London, with General Monck as the military governor of Scotland.

Although Cromwell's New Model Army had defeated a Scottish army at Dunbar, Cromwell could not prevent Charles II from marching from Scotland deep into England at the head of another Royalist army. They marched to the west of England where English Royalist sympathies were strongest, but although some English Royalists joined the army, they were far fewer in number than Charles and his Scottish supporters had hoped. Cromwell finally engaged and defeated the new Scottish king at Worcester on 3 September 1651.

After the Royalist defeat at Worcester, Charles II escaped via safe houses and a famous oak tree to France, and Parliament was left in "de facto" control of England. Resistance continued for a time in the Channel Islands, Ireland and Scotland, but with the pacification of England, resistance elsewhere did not threaten the military supremacy of the New Model Army and its Parliamentary paymasters.

During the Wars, the Parliamentarians established a number of successive committees to oversee the war effort. The first, the Committee of Safety set up in July 1642, comprised 15 members of Parliament. After the Anglo-Scottish alliance against the Royalists, the Committee of Both Kingdoms replaced the Committee of Safety between 1644 and 1648. Parliament dissolved the Committee of Both Kingdoms when the alliance ended, but its English members continued to meet as the Derby House Committee. A second Committee of Safety then replaced it.

During the English Civil War, the role of bishops as wielders of political power and upholders of the established church became a matter of heated political controversy. John Calvin formulated a doctrine of Presbyterianism, which held that the offices of "presbyter" and "episkopos" in the New Testament were identical; he rejected the doctrine of apostolic succession. Calvin's follower John Knox brought Presbyterianism to Scotland when the Scottish church was reformed in 1560. In practice, Presbyterianism meant that committees of lay elders had a substantial voice in church government, as opposed to merely being subjects to a ruling hierarchy.

This vision of at least partial democracy in ecclesiology paralleled the struggles between Parliament and the King. A body within the Puritan movement in the Church of England sought to abolish the office of bishop and remake the Church of England along Presbyterian lines. The Martin Marprelate tracts (1588–1589), applying the pejorative name of "prelacy" to the church hierarchy, attacked the office of bishop with satire that deeply offended Elizabeth I and her Archbishop of Canterbury John Whitgift. The vestments controversy also related to this movement, seeking further reductions in church ceremony, and labelling the use of elaborate vestments as "unedifying" and even idolatrous.

King James I, reacting against the perceived contumacy of his Presbyterian Scottish subjects, adopted "No Bishop, no King" as a slogan; he tied the hierarchical authority of the bishop to the absolute authority he sought as King, and viewed attacks on the authority of the bishops as attacks on his authority. Matters came to a head when Charles I appointed William Laud as Archbishop of Canterbury; Laud aggressively attacked the Presbyterian movement and sought to impose the full Book of Common Prayer. The controversy eventually led to Laud's impeachment for treason by a bill of attainder in 1645 and subsequent execution. Charles also attempted to impose episcopacy on Scotland; the Scots' violent rejection of bishops and liturgical worship sparked the Bishops' Wars in 1639–1640.

During the height of Puritan power under the Commonwealth and the Protectorate, episcopacy was formally abolished in the Church of England on 9 October 1646. The Church of England remained Presbyterian until the Restoration of the monarchy under Charles II in 1660.

During the English Civil War, the English overseas possessions became highly involved. In the Channel Islands, the island of Jersey and Castle Cornet in Guernsey supported the King until a surrender with honour in December 1651.

Although the newer, Puritan settlements in North America, notably Massachusetts, were dominated by Parliamentarians, the older colonies sided with the Crown. Friction between Royalists and Puritans in Maryland came to a head in the Battle of the Severn. The Virginia Company's settlements, Bermuda and Virginia, as well as Antigua and Barbados, were conspicuous in their loyalty to the Crown. Bermuda's Independent Puritans were expelled, settling the Bahamas under William Sayle as the Eleutheran Adventurers. Parliament passed An Act for prohibiting Trade with the Barbadoes, Virginia, Bermuda and Antego in October, 1650, which stated that
The Act also authorised Parliamentary privateers to act against English vessels trading with the rebellious colonies:
The Parliament began assembling a fleet to invade the Royalist colonies, but many of the English islands in the Caribbean were captured by the Dutch and French in 1651 during the Second Anglo-Dutch War. Far to the North, Bermuda's regiment of Militia and its coastal batteries prepared to resist an invasion that never came. Built-up inside the natural defence of a nearly impassable barrier reef, to fend off the might of Spain, these defences were too powerful for the Parliamentary fleet sent in 1651 under the command of Admiral Sir George Ayscue, which was forced instead to blockade Bermuda for several months 'til the Bermudians negotiated a separate peace that respected the internal status quo. The Parliament of Bermuda avoided the Parliament of England's fate during The Protectorate, becoming one of the oldest continuous legislatures in the world.

Virginia's population swelled with Cavaliers during and after the English Civil War. Even so, Virginia Puritan Richard Bennett was made Governor answering to Cromwell in 1652, followed by two more nominal "Commonwealth Governors". The loyalty of Virginia's Cavaliers to the Crown was rewarded after the 1660 Restoration of the Monarchy when Charles II dubbed it the "Old Dominion".

Figures for casualties during this period are unreliable, but some attempt has been made to provide rough estimates.

In England, a conservative estimate is that roughly 100,000 people died from war-related disease during the three civil wars. Historical records count 84,830 dead from the wars themselves. Counting in accidents and the two Bishops' wars, an estimate of 190,000 dead is achieved, out of a total population of about five million.

Figures for Scotland are less reliable and should be treated with caution. Casualties include the deaths of prisoners-of-war in conditions that accelerated their deaths, with estimates of 10,000 prisoners not surviving or not returning home (8,000 captured during and immediately after the Battle of Worcester were deported to New England, Bermuda and the West Indies to work for landowners as indentured labourers). There are no figures to calculate how many died from war-related diseases, but if the same ratio of disease to battle deaths from English figures is applied to the Scottish figures, a not unreasonable estimate of 60,000 people is achieved, from a population of about one million.

Figures for Ireland are described as "miracles of conjecture". Certainly the devastation inflicted on Ireland was massive, with the best estimate provided by Sir William Petty, the father of English demography. Petty estimated that 112,000 Protestants and 504,000 Catholics were killed through plague, war and famine, giving an estimated total of 616,000 dead, out of a pre-war population of about one and a half million. Although Petty's figures are the best available, they are still acknowledged as tentative; they do not include an estimated 40,000 driven into exile, some of whom served as soldiers in European continental armies, while others were sold as indentured servants to New England and the West Indies. Many of those sold to landowners in New England eventually prospered, but many sold to landowners in the West Indies were worked to death.

These estimates indicate that England suffered a 4 percent loss of population, Scotland a loss of 6 percent, while Ireland suffered a loss of 41 percent of its population. Putting these numbers into the context of other catastrophes helps to understand the devastation of Ireland in particular. The Great Hunger of 1845–1852 resulted in a loss of 16 percent of the population, while during the Second World War the population of the Soviet Union fell by 16 percent.

Ordinary people took advantage of the dislocation of civil society in the 1640s to gain personal advantages. The contemporary guild democracy movement won its greatest successes among London's transport workers, notably the Thames watermen. Rural communities seized timber and other resources on the sequestrated estates of Royalists and Catholics, and on the estates of the royal family and church hierarchy. Some communities improved their conditions of tenure on such estates. The old "status quo" began a retrenchment after the end of the First Civil War in 1646, and more especially after the Restoration in 1660, but some gains were long-term. The democratic element introduced into the watermen's company in 1642, for example, survived with vicissitudes until 1827.

The wars left England, Scotland, and Ireland among the few countries in Europe without a monarch. In the wake of victory, many of the ideals (and many idealists) became sidelined. The republican government of the Commonwealth of England ruled England (and later all of Scotland and Ireland) from 1649 to 1653 and from 1659 to 1660. Between the two periods, and due to in-fighting among various factions in Parliament, Oliver Cromwell ruled over the Protectorate as Lord Protector (effectively a military dictator) until his death in 1658.

On Oliver Cromwell's death, his son Richard became Lord Protector, but the Army had little confidence in him. After seven months the Army removed Richard, and in May 1659 it re-installed the Rump. However, military force shortly afterward dissolved this as well. After the second dissolution of the Rump, in October 1659, the prospect of a total descent into anarchy loomed as the Army's pretense of unity finally dissolved into factions.
Into this atmosphere General George Monck, Governor of Scotland under the Cromwells, marched south with his army from Scotland. On 4 April 1660, in the Declaration of Breda, Charles II made known the conditions of his acceptance of the Crown of England. Monck organised the Convention Parliament, which met for the first time on 25 April 1660. On 8 May 1660, it declared that Charles II had reigned as the lawful monarch since the execution of Charles I in January 1649. Charles returned from exile on 23 May 1660. On 29 May 1660, the populace in London acclaimed him as king. His coronation took place at Westminster Abbey on 23 April 1661. These events became known as the "Restoration".

Although the monarchy was restored, it was still only with the consent of Parliament. So the civil wars effectively set England and Scotland on course towards a parliamentary monarchy form of government. The outcome of this system was that the future Kingdom of Great Britain, formed in 1707 under the Acts of Union, managed to forestall the kind of revolution typical of European republican movements which generally resulted in total abolition of monarchy. Thus the United Kingdom was spared the wave of revolutions that occurred in Europe in the 1840s. Specifically, future monarchs became wary of pushing Parliament too hard, and Parliament effectively chose the line of royal succession in 1688 with the Glorious Revolution and in the 1701 Act of Settlement.

In the early decades of the 20th century, the Whig school was the dominant theoretical view. It explained the Civil War as resulting from centuries of struggle between Parliament (notably the House of Commons) and the Monarchy, with Parliament defending the traditional rights of Englishmen, while the Stuart monarchy continually attempted to expand its right to dictate law arbitrarily. The major Whig historian, S. R. Gardiner, popularised the idea that the English Civil War was a "Puritan Revolution", which challenged the repressive Stuart Church and prepared the way for religious toleration. So Puritanism was seen as the natural ally of a people preserving their traditional rights against arbitrary monarchical power.

The Whig view was challenged and largely superseded by the Marxist school, which became popular in the 1940s, and saw the English Civil War as a bourgeois revolution. According to Marxist historian Christopher Hill:

In the 1970s, revisionist historians challenged both the Whig and the Marxist theories, notably in the 1973 anthology "The Origins of the English Civil War" (Conrad Russell ed.). These historians focused on the minutiae of the years immediately before the civil war, returning to the contingency-based historiography of Clarendon's famous "History of the Rebellion and Civil Wars in England". This, it was claimed, demonstrated that patterns of war allegiance did not fit either Whig or Marxist theories. Parliament was not inherently progressive, nor the events of 1640 a precursor for the Glorious Revolution. Furthermore, Puritans did not necessarily ally themselves with Parliamentarians. Many members of the bourgeoisie fought for the King, while many landed aristocrats supported Parliament.

From the 1990s, a number of historians replaced the historical title "English Civil War" with "Wars of the Three Kingdoms" and "British Civil Wars", positing that the civil war in England cannot be understood apart from events in other parts of Britain and Ireland. King Charles I remains crucial, not just as King of England, but through his relationship with the peoples of his other realms. For example, the wars began when Charles forced an Anglican Prayer Book upon Scotland, and when this was met with resistance from the Covenanters, he needed an army to impose his will. However, this need of military funds forced Charles I to call an English Parliament, which was not willing to grant the needed revenue unless he addressed their grievances. By the early 1640s, Charles was left in a state of near-permanent crisis management, confounded by the demands of the various factions. For example, Charles finally made terms with the Covenanters in August 1641, but although this might have weakened the position of the English Parliament, the Irish Rebellion of 1641 broke out in October 1641, largely negating the political advantage he had obtained by relieving himself of the cost of the Scottish invasion.

Thomas Hobbes gave a much earlier historical account of the English Civil War in his "Behemoth", written in 1668 and published in 1681. He assessed the causes of the war to be the conflicting political doctrines of the time. "Behemoth" offered a uniquely historical and philosophical approach to naming the catalysts for the war. It also attempted to explain why Charles I could not hold his throne and maintain peace in his kingdom.
Hobbes analysed in turn the following aspects of English thought during the war: the opinions of divinity and politics that spurred rebellion; rhetoric and doctrine used by the rebels against the king; and how opinions about "taxation, the conscription of soldiers, and military strategy" affected the outcomes of battles and shifts of sovereignty.

Hobbes attributed the war to the novel theories of intellectuals and divines spread for their own pride of reputation. He held that clerical pretensions had contributed significantly to the troubles — "whether those of puritan fundamentalists, papal supremacists or divine right Episcopalians". Hobbes wanted to abolish the independence of the clergy and bring it under the control of the civil state.

Some scholars suggest that "Behemoth" has not received its due as an academic work, being comparatively overlooked and under-rated in the shadow of Hobbes' "Leviathan". Its scholarly reputation may have suffered because it takes the form of a dialogue, which, while common in philosophy, is rarely adopted by historians. Other factors that hindered its success include Charles II's refusing its publication and Hobbes' lack of empathy with views different from his own.

Two large historical societies exist, The Sealed Knot and The English Civil War Society, which regularly re-enact events and battles of the Civil War in full period costume.


Attribution:




</doc>
<doc id="9710" url="https://en.wikipedia.org/wiki?curid=9710" title="Elementary algebra">
Elementary algebra

Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.

The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.

Algebraic notation describes the rules and conventions for writing mathematical expressions, as well as the terminology used for talking about parts of expressions. For example, the expression formula_1 has the following components:

A "coefficient" is a numerical value, or letter representing a numerical constant, that multiplies a variable (the operator is omitted). A "term" is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators. Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. formula_2) are typically used to represent constants, and those toward the end of the alphabet (e.g. formula_3 and ) are used to represent variables. They are usually written in italics.

Algebraic operations work in the same way as arithmetic operations, such as addition, subtraction, multiplication, division and exponentiation. and are applied to algebraic variables and terms. Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example, formula_4 is written as formula_5, and formula_6 may be written formula_7.

Usually terms with the highest power (exponent), are written on the left, for example, formula_8 is written to the left of . When a coefficient is one, it is usually omitted (e.g. formula_9 is written formula_8). Likewise when the exponent (power) is one, (e.g. formula_11 is written formula_12). When the exponent is zero, the result is always 1 (e.g. formula_13 is always rewritten to ). However formula_14, being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.

Other types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. As an illustration of this, while exponents are usually formatted using superscripts, e.g., formula_8, in plain text, and in the TeX mark-up language, the caret symbol "^" represents exponentiation, so formula_8 is written as "x^2"., as well as some programming languages such as Lua. In programming languages such as Ada, Fortran, Perl, Python and Ruby, a double asterisk is used, so formula_8 is written as "x**2". Many programming languages and calculators use a single asterisk to represent the multiplication symbol, and it must be explicitly used, for example, formula_12 is written "3*x".

Elementary algebra builds on and extends arithmetic by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.


Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,

An equation states that two expressions are equal using the symbol for equality, (the equals sign). One of the best-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:

This equation states that formula_39, representing the square of the length of the side that is the hypotenuse, the side opposite the right angle, is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by and .

An equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as formula_40); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g. formula_41 is true only for formula_42 and formula_43. The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.

Another type of equation is inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are: formula_44 where formula_45 represents 'greater than', and formula_46 where formula_47 represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.

By definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. formula_48), (b) symmetric (i.e. if formula_49 then formula_50) (c) transitive (i.e. if formula_49 and formula_52 then formula_53). It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:


The relations "less than" formula_47 and greater than formula_45 have the property of transitivity:
By reversing the inequation, formula_47 and formula_45 can be swapped, for example:

Substitution is replacing the terms in an expression to create a new expression. Substituting 3 for in the expression makes a new expression with meaning . Substituting the terms of a statement makes a new statement. When the original statement is true independently of the values of the terms, the statement created by substitutions is also true. Hence, definitions can be made in symbolic terms and interpreted through substitution: if formula_81 is meant as the definition of formula_82 as the product of with itself, substituting for informs the reader of this statement that formula_83 means . Often it's not known whether the statement is true independently of the values of the terms. And, substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement , if is substituted with , this implies , which is false, which implies that if then cannot be .

If and are integers, rationals, or real numbers, then implies or . Consider . Then, substituting for and for , we learn or . Then we can substitute again, letting and , to show that if then or . Therefore, if , then or ( or ), so implies or or .

If the original fact were stated as " implies or ", then when saying "consider ," we would have a conflict of terms when substituting. Yet the above logic is still valid to show that if then or or if, instead of letting and , one substitutes for and for (and with , substituting for and for ). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression into the term of the original equation, the substituted does not refer to the in the statement " implies or ."

The following sections lay out examples of some of the types of algebraic equations that may be encountered.

Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:

To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:
In words: the child is 4 years old.

The general form of a linear equation with one variable, can be written as: formula_85

Following the same procedure (i.e. subtract from both sides, and then divide by ), the general solution is given by formula_86

A linear equation with two variables has many (i.e. an infinite number of) solutions. For example:

That cannot be worked out by itself. If the son's age was made known, then there would no longer be two unknowns (variables). The problem then becomes a linear equation with just one variable, that can be solved as described above.

To solve a linear equation with two variables (unknowns), requires two related equations. For example, if it was also revealed that:

Now there are two related linear equations, each with two unknowns, which enables the production of a linear equation with just one variable, by subtracting one from the other (called the elimination method):

In other words, the son is aged 12, and since the father 22 years older, he must be 34. In 10 years, the son will be 22, and the father will be twice his age, 44. This problem is illustrated on the associated plot of the equations.

For other ways to solve this kind of equations, see below, System of linear equations.

A quadratic equation is one which includes a term with an exponent of 2, for example, formula_30, and no term with higher exponent. The name derives from the Latin "quadrus", meaning square. In general, a quadratic equation can be expressed in the form formula_92, where is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term formula_93, which is known as the quadratic term. Hence formula_94, and so we may divide by and rearrange the equation into the standard form

where formula_96 and formula_97. Solving this, by a process known as completing the square, leads to the quadratic formula

where the symbol "±" indicates that both

are solutions of the quadratic equation.

Quadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:

which is the same thing as

It follows from the zero-product property that either formula_102 or formula_103 are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,

has no real number solution since no real number squared equals −1.
Sometimes a quadratic equation has a root of multiplicity 2, such as:

For this equation, −1 is a root of multiplicity 2. This means −1 appears twice, since the equation can be rewritten in factored form as

All quadratic equations have exactly two solutions in complex numbers (but they may be equal to each other), a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equation

has solutions

Since formula_109 is not any real number, both of these solutions for "x" are complex numbers.

An exponential equation is one which has the form formula_110 for formula_111, which has solution

when formula_113. Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if

then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain

whence

or

A logarithmic equation is an equation of the form formula_118 for formula_111, which has solution

For example, if

then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get

whence

from which we obtain

A radical equation is one that includes a radical sign, which includes square roots, formula_125 cube roots, formula_126, and "n"th roots, formula_127. Recall that an "n"th root can be rewritten in exponential format, so that formula_127 is equivalent to formula_129. Combined with regular exponents (powers), then formula_130 (the square root of cubed), can be rewritten as formula_131. So a common form of a radical equation is formula_132 (equivalent to formula_133) where and are integers. It has real solution(s):
For example, if:

then

and thus 

There are different methods to solve a system of linear equations with two variables.

An example of solving a system of linear equations is by using the elimination method:

Multiplying the terms in the second equation by 2:

Adding the two equations together to get:

which simplifies to

Since the fact that formula_102 is known, it is then possible to deduce that formula_143 by either of the original two equations (by using "2" instead of ) The full solution to this problem is then

This is not the only way to solve this specific system; could have been resolved before .

Another way of solving the same system of linear equations is by substitution.

An equivalent for can be deduced by using one of the two equations. Using the second equation:

Subtracting formula_147 from each side of the equation:

and multiplying by −1:

Using this value in the first equation in the original system:

Adding "2" on each side of the equation:

which simplifies to

Using this value in one of the equations, the same solution as in the previous method is obtained.

This is not the only way to solve this specific system; in this case as well, could have been solved before .

In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is

As 0≠2, the second equation in the system has no solution. Therefore, the system has no solution.
However, not all inconsistent systems are recognized at first sight. As an example, consider the system 

Multiplying by 2 both sides of the second equation, and adding it to the first one results in
which clearly has no solution.

There are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for and ) For example:

Isolating in the second equation:

And using this value in the first equation in the system:

The equality is true, but it does not provide a value for . Indeed, one can easily verify (by just filling in some values of ) that for any there is a solution as long as formula_160. There is an infinite number of solutions for this system.

Systems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system is

When trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express "all" solutions numerically because there are an infinite number of them if there are any.

A system with a higher number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.



</doc>
<doc id="9712" url="https://en.wikipedia.org/wiki?curid=9712" title="ERP">
ERP

ERP or Erp may refer to:







</doc>
<doc id="9713" url="https://en.wikipedia.org/wiki?curid=9713" title="Ernest Thayer">
Ernest Thayer

Ernest Lawrence Thayer (; August 14, 1863 – August 21, 1940) was an American writer and poet who wrote the poem "Casey" (or "Casey at the Bat"), which is "the single most famous baseball poem ever written" according to the Baseball Almanac, and "the nation’s best-known piece of comic verse—a ballad that began a native legend as colorful and permanent as that of Johnny Appleseed or Paul Bunyan."

Thayer was born in Lawrence, Massachusetts, and raised in nearby Worcester. He graduated "magna cum laude" in philosophy from Harvard University in 1885, where he had been editor of the "Harvard Lampoon" and a member of the theatrical society Hasty Pudding. William Randolph Hearst, a friend from both activities, hired Thayer as humor columnist for "The San Francisco Examiner" 1886–88.

Thayer's last piece, dated June 3, 1888, was a ballad entitled "Casey" ("Casey at the Bat") which made him "a prize specimen of the one-poem poet" according to "American Heritage".

It was not until several months after the publication of the poem that Thayer became famous for it, since he was hardly the boastful type and had signed the June 24 poem with the nickname "Phin" which he had used since his time as a writer for the "Harvard Lampoon". Two mysteries remain about the poem: whether Casey and Mudville were based on a real person or place, and, if so, their actual identities. On March 31, 2007, Katie Zezima of "The New York Times" wrote an article called "In 'Casey' Rhubarb, 2 Cities Cry 'Foul!'" on the competing claims of two towns to such renown: Stockton, California, and Holliston, Massachusetts.
On the possible model for Casey, Thayer dismissed the notion that any single living baseball player was an influence. However, late 1880s Boston star Mike "King" Kelly is likely as a model for Casey's baseball situations. Besides being a native of a town close to Boston, Thayer, as a "San Francisco Examiner" baseball reporter in the off-season of 1887–88, covered exhibition games featuring Kelly. During November 1887, some of his reportage about a Kelly at-bat has the same ring as Casey's famous at-bat in the poem. A 2004 book by Howard W. Rosenberg, "Cap Anson 2: The Theatrical and Kingly Mike Kelly: U.S. Team Sport's First Media Sensation and Baseball's Original Casey at the Bat," reprints a 1905 Thayer letter to a Baltimore scribe who was asking about the poem's roots. In the letter, Thayer named Kelly (d. 1894), as having shown "impudence" in claiming to have inspired it. Rosenberg argues that if Thayer still felt offended, Thayer may have later denied Kelly as an influence. Kelly had also performed as a vaudeville actor, and recited the poem dozens of times.

The first public performance of the poem was on August 14, 1888, by actor De Wolf Hopper, on Thayer's 25th birthday. Thayer's recitation of the poem at a Harvard class reunion in 1895 helps solve the mystery, which lingered into the 20th century, of who had written it.

During the mid-1890s, Thayer contributed several other comic poems for Hearst's newspaper "New York Journal" and then began overseeing his family's mills in Worcester full-time. Thayer relocated to Santa Barbara in 1912, where he married Rosalind Buel Hammett and retired. He died in 1940, seven days after his 77th birthday.

"The New York Times"' obituary of Thayer on August 22, 1940, p. 19 quotes comedian DeWolf Hopper, who helped make the poem famous: 



</doc>
<doc id="9714" url="https://en.wikipedia.org/wiki?curid=9714" title="List of English-language poets">
List of English-language poets

This is a list of English-language poets, who have written much of their poetry in English. Main country of residence as a poet (not place of birth): A = Australia, B = Barbados, Bo = Bosnia, C = Canada, Ch = Chile, Cu = Cuba, D = Dominica, De = Denmark, E = England, F = France, G = Germany, Ga = Gambia, Gd = Grenada, Gh = Ghana/Gold Coast, Gr = Greece, Gu = Guyana/British Guiana, HK = Hong Kong, In = India, IoM = Isle of Man, Is = Israel, Ir = Ireland, It = Italy, J = Jamaica, Je = Jersey, Jp = Japan, K = Kenya, L = Lebanon, M = Malta, Me = Mexico, Mo = Montserrat, Ne = Nepal, Nf = Newfoundland (colony), Ni = Nigeria, NI = Northern Ireland, Nt = Netherlands, NZ = New Zealand, P = Pakistan, Pa = Palestine, Ph = Philippines, PI = Pitcairn Islands, RE = Russian Empire, S = Scotland, SA = South Africa, Se = Serbia, SL = Saint Lucia, SLe = Sierra Leone, SLk = Sri Lanka, So = Somalia, Sw = Sweden, T = Trinidad and Tobago, US = United States/preceding colonies, W = Wales, Z = Zimbabwe/Rhodesia



















</doc>
<doc id="9717" url="https://en.wikipedia.org/wiki?curid=9717" title="Excalibur">
Excalibur

Excalibur () is the legendary sword of King Arthur, sometimes also attributed with magical powers or associated with the rightful sovereignty of Britain. Excalibur and the Sword in the Stone (the proof of Arthur's lineage) are sometimes said to be the same weapon, but in most versions they are considered separate. Excalibur was associated with the Arthurian legend very early on. In Welsh, it is called "Caledfwlch"; in Cornish, "Calesvol" (in Modern Cornish: "Kalesvolgh"); in Breton, "Kaledvoulc'h"; and in Latin, "Caliburnus".

The name "Excalibur" ultimately derives from the Welsh "Caledfwlch" (and Breton "Kaledvoulc'h", Middle Cornish "Calesvol"), which is a compound of ' "hard" and ' "breach, cleft". Caledfwlch appears in several early Welsh works, including the prose tale "Culhwch and Olwen". The name was later used in Welsh adaptations of foreign material such as the "Brut"s (chronicles), which were based on Geoffrey of Monmouth. It is often considered to be related to the phonetically similar "Caladbolg", a sword borne by several figures from Irish mythology, although a borrowing of "Caledfwlch" from Irish "Caladbolg" has been considered unlikely by Rachel Bromwich and D. Simon Evans. They suggest instead that both names "may have similarly arisen at a very early date as generic names for a sword". This sword then became exclusively the property of Arthur in the British tradition.

Geoffrey of Monmouth, in his "Historia Regum Britanniae" ("The History of the Kings of Britain", c. 1136), Latinised the name of Arthur's sword as "Caliburnus" (potentially influenced by the Medieval Latin spelling "calibs" of Classical Latin "chalybs", from Greek "chályps" ["χάλυψ"] "steel"). Most Celticists consider Geoffrey's "Caliburnus" to be derivative of a lost Old Welsh text in which "bwlch" (Old Welsh "bulc[h]") had not yet been lenited to "fwlch" (Middle Welsh "vwlch" or "uwlch"). In the late 15th/early 16th-century Middle Cornish play "Beunans Ke", Arthur's sword is called "Calesvol", which is etymologically an exact Middle Cornish cognate of the Welsh "Caledfwlch". It is unclear if the name was borrowed from the Welsh (if so, it must have been an early loan, for phonological reasons), or represents an early, pan-Brittonic traditional name for Arthur's sword.

In Old French sources this then became "Escalibor", "Excalibor", and finally the familiar "Excalibur". Geoffrey Gaimar, in his Old French "L'Estoire des Engleis" (1134-1140), mentions Arthur and his sword: "this Constantine was the nephew of Arthur, who had the sword Caliburc" (""Cil Costentin, li niès Artur, Ki out l'espée Caliburc""). In Wace's "Roman de Brut" (c. 1150-1155), an Old French translation and versification of Geoffrey's "Historia", the sword is called "Calabrum", "Callibourc", "Chalabrun", and "Calabrun" (with variant spellings such as "Chalabrum", "Calibore", "Callibor", "Caliborne", "Calliborc", and "Escaliborc", found in various manuscripts of the "Brut").

In Chrétien de Troyes' late 12th-century Old French "Perceval", Arthur's knight Gawain carries the sword "Escalibor" and it is stated, "for at his belt hung Escalibor, the finest sword that there was, which sliced through iron as through wood" (""Qu'il avoit cainte Escalibor, la meillor espee qui fust, qu'ele trenche fer come fust""). This statement was probably picked up by the author of the "Estoire Merlin", or Vulgate Merlin, where the author (who was fond of fanciful folk etymologies) asserts that Escalibor "is a Hebrew name which means in French 'cuts iron, steel, and wood'" (""c'est non Ebrieu qui dist en franchois trenche fer & achier et fust""; note that the word for "steel" here, achier, also means "blade" or "sword" and comes from medieval Latin "aciarium", a derivative of "acies" "sharp", so there is no direct connection with Latin "chalybs" in this etymology). It is from this fanciful etymological musing that Thomas Malory got the notion that Excalibur meant "cut steel" (""the name of it,' said the lady, 'is Excalibur, that is as moche to say, as Cut stele"").

In Arthurian romance, a number of explanations are given for Arthur's possession of Excalibur. In Robert de Boron's "Merlin", the first tale to mention the "sword in the stone" motif c. 1200, Arthur obtained the British throne by pulling a sword from an anvil sitting atop a stone that appeared in a churchyard on Christmas Eve. In this account, as foretold by Merlin, the act could not be performed except by "the true king," meaning the divinely appointed king or true heir of Uther Pendragon. As Malory related in his most famous English-language version of the Arthurian tales, the 15th-century "Le Morte d'Arthur": ""Whoso pulleth out this sword of this stone and anvil, is rightwise king born."" After many of the gathered nobles try and fail to complete Merlin's challenge, the teenage Arthur (who up to this point had believed himself to be son of Sir Ector, not Uther's son, and went there as Sir Kay's squire) does this feat effortlessly by accident and then repeats it publicly.

The identity of this sword as Excalibur is made explicit in the Prose "Merlin", part of the Lancelot-Grail cycle of French romances (the Vulgate Cycle). In the Vulgate "Mort Artu", when Arthur at the brink of death he orders Griflet to throw the sword into the enchanted lake; after two failed attempts (as he felt such a great sword should not be thrown away), Griflet finally complies with the wounded king's request and a hand emerges from the lake to catch it. This tale becomes attached to Bedivere instead of Griflet in Malory and the English tradition. However, in the Post-Vulgate Cycle and consequently Malory, early in his reign Arthur breaks the Sword from the Stone while in combat against King Pellinore, and then is given Excalibur by a Lady of the Lake in exchange for a later boon for her (some time later, she arrives at Arthur's court to demand the head of Balin). Malory records both versions of the legend in his "Le Morte d'Arthur", naming both swords as Excalibur.

In Welsh legends, Arthur's sword is known as "Caledfwlch". In "Culhwch and Olwen", it is one of Arthur's most valuable possessions and is used by Arthur's warrior Llenlleawg the Irishman to kill the Irish king Diwrnach while stealing his magical cauldron. Irish mythology mentions a weapon "Caladbolg", the sword of Fergus mac Róich, which was also known for its incredible power and was carried by some of Ireland's greatest heroes. The name, which can also mean "hard cleft" in Irish, appears in the plural, "caladbuilc", as a generic term for "great swords" in "Togail Troi" ("The Destruction of Troy"), a 10th-century Irish translation of the classical tale.

Though not named as Caledfwlch, Arthur's sword is described vividly in "The Dream of Rhonabwy", one of the tales associated with the "Mabinogion" (as translated by Jeffrey Gantz): "Then they heard Cadwr Earl of Cornwall being summoned, and saw him rise with Arthur's sword in his hand, with a design of two chimeras on the golden hilt; when the sword was unsheathed what was seen from the mouths of the two chimeras was like two flames of fire, so dreadful that it was not easy for anyone to look."

Geoffrey's "Historia" is the first non-Welsh source to speak of the sword. Geoffrey says the sword was forged in Avalon and Latinises the name "Caledfwlch" as "Caliburnus". When his influential pseudo-history made it to Continental Europe, writers altered the name further until it finally took on the popular form "Excalibur" (various spellings in the medieval Arthurian romance and chronicle tradition include: "Calabrun", "Calabrum", "Calibourne", "Callibourc", "Calliborc", "Calibourch", "Escaliborc", and "Escalibor").

The legend was expanded upon in the Vulgate Cycle and in the Post-Vulgate Cycle which emerged in its wake. Both included the Prose "Merlin", but the Post-Vulgate authors left out the "Merlin" continuation from the earlier cycle, choosing to add an original account of Arthur's early days including a new origin for Excalibur. In many versions, Excalibur's blade was engraved with phrases on opposite sides: "Take me up" and "Cast me away" (or similar). In addition, when Excalibur was first drawn, in the first battle testing Arthur's sovereignty, its blade blinded his enemies. In several French works, such as Chrétien's "Perceval" and the Vulgate "Lancelot", Excalibur is used by Gawain, Arthur's nephew and one of his best knights. This is in contrast to later versions, where Excalibur belongs solely to Arthur.

In some tellings, Excalibur's scabbard was also said to have powers of its own, as any wounds received while wearing the scabbard would not bleed at all, thus preventing the death of the wearer. For this reason, Merlin chides Arthur for preferring the sword over the scabbard, saying that the latter was the greater treasure. In the later romance tradition, including "Le Morte d'Arthur", the scabbard is stolen from Arthur by his half-sister Morgan le Fay in revenge for the death of her beloved Accolon during the Fake Excalibur plot and thrown into a lake, never to be found again. This act later enables the death of Arthur, deprived of magical protection, many years later in his final battle.

Historically, a sword identified as Excalibur (Caliburn) was supposedly discovered during the purported exhumation of Arthur's grave at Glastonbury Abbey in 1191. On 6 March 1191, after the Treaty of Messina, either this or another claimed Excalibur was given as a gift of goodwill by Richard I of England (Richard the Lionheart) to his ally Tancred, King of Sicily.

The challenge of drawing a sword from a stone also appears in the later Arthurian stories of Galahad, whose achievement of the task indicates that he is destined to find the Holy Grail as foretold by Merlin. As told by Malory, this weapon (known as the Adventurous Sword among other names) had also come from Avalon; it had been originally wielded by Balin and eventually was used by Lancelot to give Gawain mortal wound in their duel. In "Perlesvaus", Lancelot pulls other weapons from stone on two occasions.

In Welsh mythology, the "Dyrnwyn" ("White-Hilt"), one of the Thirteen Treasures of the Island of Britain, is said to be a powerful sword belonging to Rhydderch Hael, one of the Three Generous Men of Britain mentioned in the Welsh Triads. When drawn by a worthy or well-born man, the entire blade would blaze with fire. Rhydderch was never reluctant to hand the weapon to anyone, hence his nickname Hael "the Generous", but the recipients, as soon as they had learned of its peculiar properties, always rejected the sword.

There are other similar weapons described in other mythologies. In particular, Claíomh Solais, which is an Irish term meaning "Sword of Light", or "Shining Sword", appears in a number of orally transmitted Irish folk-tales. The Sword in the Stone has an analogue in some versions of the story of Sigurd, whose father, Sigmund, draws the sword Gram out of the tree Barnstokkr where it is embedded by the Norse god Odin. A sword in the stone legend is also associated with the 12th-century Italian Saint Galgano. 

Other weapons have been associated with Arthur. Welsh tradition also knew of a dagger named Carnwennan and a spear named Rhongomyniad that belonged to him. Carnwennan ("little white-hilt") first appears in "Culhwch and Olwen", where Arthur uses it to slice the witch Orddu in half. Rhongomyniad ("spear" + "striker, slayer") is also mentioned in "Culhwch", although only in passing; it appears as simply "Ron" ("spear") in Geoffrey's "Historia". Geoffrey also names Arthur's shield as "Pridwen", but in "Culhwch", "Prydwen" ("fair face") is the name of Arthur's ship while his shield is named "Wynebgwrthucher" ("face of evening").

The Alliterative "Morte Arthure", a Middle English poem, mentions Clarent, a sword of peace meant for knighting and ceremonies as opposed to battle, which Mordred stole and then used to kill Arthur at Camlann. The Prose "Lancelot" of the Vulgate Cycle mentions a sword called Seure (Sequence), or Secace in some manuscripts, which belonged to Arthur but was borrowed by Lancelot. 





</doc>
<doc id="9719" url="https://en.wikipedia.org/wiki?curid=9719" title="Eight-bar blues">
Eight-bar blues

In music, an eight-bar blues is a common blues chord progression. Music writers have described it as "the second most common blues form" being "common to folk, rock, and jazz forms of the blues". It is often notated in or time with eight bars to the verse.

Early examples of eight-bar blues standards include:
One variant using this progression is to couple one eight-bar blues melody with a different eight-bar blues bridge to create a blues variant of the standard 32-bar song: "I Want a Little Girl" (T-Bone Walker) and "Great Balls of Fire" (Jerry Lee Lewis)(

Eight bar blues progressions have more variations than the more rigidly defined twelve bar format. The move to the IV chord usually happens at bar 3 (as opposed to 5 in twelve bar); however, "the I chord moving to the V chord right away, in the second measure, is a characteristic of the eight-bar blues."

In the following examples each box represents a 'bar' of music (the specific time signature is not relevant). The chord in the box is played for the full bar. If two chords are in the box they are each played for half a bar, etc. The chords are represented as scale degrees in Roman numeral analysis. Roman numerals are used so the musician may understand the progression of the chords regardless of the key it is played in.

"Worried Life Blues" (probably the most common eight bar blues progression):

"Heartbreak Hotel" (variation with the I on the first half):

J. B. Lenoir's "Slow Down" and "Key to the Highway" (variation with the V at bar 2):

"Get a Haircut" by George Thorogood (simple progression):

Jimmy Rogers' "Walkin' By Myself" (somewhat unorthodox example of the form):

Howlin Wolf's version of "Sitting on Top of the World" is actually a 9 bar blues that adds an extra "V" chord at the end of the progression. The song uses movement between major and dominant 7th and major and minor fourth:

The first four bar progression used by Wolf is also used in Nina Simone's 1965 version of "Trouble in Mind", but with a more uptempo beat than "Sitting on Top of the World":

The progression may be created by dropping the first four bars from the twelve-bar blues, as in the solo section of Bonnie Raitt's "Love Me Like a Man" and Buddy Guy's "Mary Had a Little Lamb":

There are at least a few very successful songs using somewhat unusual chord progressions as well. For example, the song "Ain't Nobody's Business" as performed by Freddie King at least, uses a I–III–IV–iv progression in each of the first four bars. The same four bar progression is used by the band Radiohead to make up the bulk of the song "Creep".

The same chord progression can also be called a sixteen-bar blues, if each symbol above is taken to be a half note in or time. Examples are "Nine Pound Hammer" and Ray Charles's original instrumental "Sweet Sixteen Bars".



</doc>
<doc id="9720" url="https://en.wikipedia.org/wiki?curid=9720" title="Echidna (disambiguation)">
Echidna (disambiguation)

Echidnas are Australian egg-laying mammals also known as spiny anteaters.

Echidna may also refer to:





</doc>
<doc id="9723" url="https://en.wikipedia.org/wiki?curid=9723" title="Edward Waring">
Edward Waring

Edward Waring (15 August 1798) was a British mathematician. He entered Magdalene College, Cambridge as a sizar and became Senior wrangler in 1757. He was elected a Fellow of Magdalene and in 1760 Lucasian Professor of Mathematics, holding the chair until his death. He made the assertion known as Waring's problem without proof in his writings "Meditationes Algebraicae". Waring was elected a Fellow of the Royal Society in 1763 and awarded the Copley Medal in 1784.

Waring was the eldest son of John and Elizabeth Waring, a prosperous farming couple. He received his early education in Shrewsbury School under a Mr Hotchkin and was admitted as a sizar at Magdalene College, Cambridge, on 24 March 1753, being also Millington exhibitioner. His extraordinary talent for mathematics was recognised from his early years in Cambridge. In 1757 he graduated BA as senior wrangler and on 24 April 1758 was elected to a fellowship at Magdalene. He belonged to the Hyson Club, whose members included William Paley.

At the end of 1759 Waring published the first chapter of "Miscellanea Analytica". On 28 January the next year he was appointed Lucasian professor of mathematics, one of the highest positions in Cambridge. William Samuel Powell, then tutor in St John's College, Cambridge opposed Waring's election and instead supported the candidacy of William Ludlam. In the polemic with Powell, Waring was backed by John Wilson. In fact Waring was very young and did not hold the MA, necessary for qualifying for the Lucasian chair, but this was granted him in 1760 by royal mandate. In 1762 he published the full "Miscellanea Analytica", mainly devoted to the theory of numbers and algebraic equations. In 1763 he was elected to the Royal Society. He was awarded its Copley Medal in 1784 but withdrew from the society in 1795, after he had reached sixty, 'on account of [his] age'. Waring was also a member of the academies of sciences of Göttingen and Bologna. In 1767 he took an MD degree, but his activity in medicine was quite limited. He carried out dissections with Richard Watson, professor of chemistry and later bishop of Llandaff. From about 1770 he was physician at Addenbrooke's Hospital at Cambridge, and he also practised at St Ives, Huntingdonshire, where he lived for some years after 1767. His career as a physician was not very successful since he was seriously short-sighted and a very shy man.

Waring had a younger brother, Humphrey, who obtained a fellowship at Magdalene in 1775. In 1776 Waring married Mary Oswell, sister of a draper in Shrewsbury; they moved to Shrewsbury and then retired to Plealey, 8 miles out of the town, where Waring owned an estate of 215 acres in 1797

Waring wrote a number of papers in the "Philosophical Transactions of the Royal Society", dealing with the resolution of algebraic equations, number theory, series, approximation of roots, interpolation, the geometry of conic sections, and dynamics. The "Meditationes Algebraicae" (1770), where many of the results published in "Miscellanea Analytica" were reworked and expanded, was described by Joseph-Louis Lagrange as 'a work full of excellent researches'. In this work Waring published many theorems concerning the solution of algebraic equations which attracted the attention of continental mathematicians, but his best results are in number theory. Included in this work was the so-called Goldbach conjecture (every even integer is the sum of two primes), and also the following conjecture: every odd integer is a prime or the sum of three primes. Lagrange had proved that every positive integer is the sum of not more than four squares; Waring suggested that every positive integer is either a cube or the sum of not more than nine cubes. He also advanced the hypothesis that every positive integer is either a biquadrate (fourth power) or the sum of not more than nineteen biquadrates. These hypotheses form what is known as Waring's problem. He also published a theorem, due to his friend John Wilson, concerning prime numbers; it was later proven rigorously by Lagrange.

In "Proprietates Algebraicarum Curvarum" (1772) Waring reissued in a much revised form the first four chapters of the second part of "Miscellanea Analytica". He devoted himself to the classification of higher plane curves, improving results obtained by Isaac Newton, James Stirling, Leonhard Euler, and Gabriel Cramer. In 1794 he published a few copies of a philosophical work entitled "An Essay on the Principles of Human Knowledge", which were circulated among his friends.

Waring's mathematical style is highly analytical. In fact he criticised those British mathematicians who adhered too strictly to geometry. It is indicative that he was one of the subscribers of John Landen's "Residual Analysis" (1764), one of the works in which the tradition of the Newtonian fluxional calculus was more severely criticised. In the preface of "Meditationes Analyticae" Waring showed a good knowledge of continental mathematicians such as Alexis Clairaut, Jean le Rond d'Alembert, and Euler. He lamented the fact that in Great Britain mathematics was cultivated with less interest than on the continent, and clearly desired to be considered as highly as the great names in continental mathematics—there is no doubt that he was reading their work at a level never reached by any other eighteenth-century British mathematician. Most notably, at the end of chapter three of "Meditationes Analyticae" Waring presents some partial fluxional equations (partial differential equations in Leibnizian terminology); such equations are a mathematical instrument of great importance in the study of continuous bodies which was almost completely neglected in Britain before Waring's researches. One of the most interesting results in "Meditationes Analyticae" is a test for the convergence of series generally attributed to d'Alembert (the 'ratio test'). The theory of convergence of series (the object of which is to establish when the summation of an infinite number of terms can be said to have a finite 'sum') was not much advanced in the eighteenth century.

Waring's work was known both in Britain and on the continent, but it is difficult to evaluate his impact on the development of mathematics. His work on algebraic equations contained in "Miscellanea Analytica" was translated into Italian by Vincenzo Riccati in 1770. Waring's style is not systematic and his exposition is often obscure. It seems that he never lectured and did not habitually correspond with other mathematicians. After Jérôme Lalande in 1796 observed, in "Notice sur la vie de Condorcet", that in 1764 there was not a single first-rate analyst in England, Waring's reply, published after his death as 'Original letter of Dr Waring' in the "Monthly Magazine", stated that he had given 'somewhere between three and four hundred new propositions of one kind or another'.

During his last years he sank into a deep religious melancholy, and a violent cold caused his death, in Plealey, on 15 August 1798. He was buried in the churchyard at Fitz, Shropshire.



</doc>
<doc id="9724" url="https://en.wikipedia.org/wiki?curid=9724" title="Eden Phillpotts">
Eden Phillpotts

Eden Phillpotts (4 November 1862 – 29 December 1960) was an English author, poet and dramatist. He was born in Mount Abu, India, was educated in Plymouth, Devon, and worked as an insurance officer for 10 years before studying for the stage and eventually becoming a writer.

Eden Phillpotts was a great-nephew of Henry Phillpotts, Bishop of Exeter. His father Henry Phillpotts was a son of the bishop’s younger brother Thomas Phillpotts. James Surtees Phillpotts the reforming headmaster of Bedford School was his second cousin.

Eden Phillpotts was born on 4 November 1862 at Mount Abu in Rajasthan. His father Henry was an officer in the Indian Army, while his mother Adelaide was the daughter of an Indian Civil Service officer posted in Madras, George Jenkins Waters.

Henry Phillpotts died in 1865, leaving Adelaide a widow at the age of 21. With her three small sons, of whom Eden was the eldest, she returned to England and settled in Plymouth.

Phillpotts was educated at Mannamead School in Plymouth. At school he showed no signs of a literary bent. In 1879, aged 17, he left home and went to London to earn his living. He found a job as a clerk with the Sun Fire Office.

Phillpotts' ambition was to be an actor and he attended evening classes at a drama school for two years. He came to the conclusion that he would never make a name as an actor but might have success as a writer. In his spare time out of office hours he proceeded to create a stream of small works which he was able to sell. In due course he left the insurance company to concentrate on his writing, while also working part-time as assistant editor for the weekly Black and White Magazine.

Eden Phillpotts maintained a steady output of three or four books a year for the next half century. He produced poetry, short stories, novels, plays and mystery tales. Many of his novels were about rural Devon life and some of his plays were distinguished by their effective use of regional dialect.

Eden Phillpotts died at his home in Broadclyst near Exeter, Devon, on 29 December 1960.

Phillpotts was for many years the President of the Dartmoor Preservation Association and cared passionately about the conservation of Dartmoor. He was an agnostic and a supporter of the Rationalist Press Association.

Phillpotts was a friend of Agatha Christie, who was an admirer of his work and a regular visitor to his home. In her autobiography she expressed gratitude for his early advice on fiction writing and quoted some of it. Jorge Luis Borges was another Phillpotts admirer. Borges mentioned him numerous times, wrote at least two reviews of his novels, and included him in his "Personal Library", a collection of works selected to reflect his personal literary preferences.

Philpotts allegedly sexually abused his daughter Adelaide. In a 1976 interview for a book about her father, Adelaide described an incestuous "relationship" with him that she says lasted from the age of five or six until her early thirties, when he remarried. When she herself finally married at the age of 55 her father never forgave her, and never communicated with her again.

Phillpotts wrote a great many books with a Dartmoor setting. One of his novels, "Widecombe Fair", inspired by an annual fair at the village of Widecombe-in-the-Moor, provided the scenario for his comic play "The Farmer's Wife" (1916). It went on to become a 1928 silent film of the same name, directed by Alfred Hitchcock. It was followed by a 1941 remake, directed by Norman Lee and Leslie Arliss. It became a BBC TV drama in 1955, directed by 
Owen Reed. Jan Stewer played Churdles Ash. The BBC had broadcast the play in 1934.

He co-wrote several plays with his daughter Adelaide Phillpotts, "The Farmer's Wife" and "Yellow Sands" (1926); she later claimed their relationship was incestuous. Eden is best known as the author of many novels, plays and poems about Dartmoor. His Dartmoor cycle of 18 novels and two volumes of short stories still has many avid readers despite the fact that many titles are out of print.

Philpotts also wrote a series of novels, each set against the background of a different trade or industry. Titles include: "Brunel's Tower" (a pottery) and "Storm in a Teacup" (hand-papermaking). Among his other works is "The Grey Room", the plot of which is centered on a haunted room in an English manor house. He also wrote a number of other mystery novels, both under his own name and the pseudonym Harrington Hext. These include: "The Thing at Their Heels", "The Red Redmaynes", "The Monster", "The Clue from the Stars", and "The Captain's Curio". "The Human Boy" was a collection of schoolboy stories in the same genre as Rudyard Kipling's Stalky & Co., though different in mood and style. Late in his long writing career he wrote a few books of interest to science fiction and fantasy readers, the most noteworthy being "Saurus", which involves an alien reptilian observing human life.

Eric Partridge praised the immediacy and impact of his dialect writing.

Novels

Short Fiction Books

Poetry

Plays

Nonfiction




</doc>
<doc id="9725" url="https://en.wikipedia.org/wiki?curid=9725" title="Ecuador–United States relations">
Ecuador–United States relations

Ecuador and the United States maintained close ties based on mutual interests in maintaining democratic institutions; combating cannabis and cocaine; building trade, investment, and financial ties; cooperating in fostering Ecuador's economic development; and participating in inter-American organizations. Ties are further strengthened by the presence of an estimated 150,000-200,000 Ecuadorians living in the United States and by 24,000 U.S. citizens visiting Ecuador annually, and by approximately 15,000 U.S. citizens residing in Ecuador. The United States assists Ecuador's economic development directly through the Agency for International Development (USAID) program in Ecuador and through multilateral organizations such as the Inter-American Development Bank and the World Bank. In addition, the U.S. Peace Corps operates a sizable program in Ecuador. More than 100 U.S. companies are doing business in Ecuador.
Relations between the two nations have been strained following Julian Assange's bid to seek political asylum in the Ecuadorian embassy in London following repeated claims that the US government was pursuing his extradition due to his work with Wikileaks.

Both nations are signatories of the Inter-American Treaty of Reciprocal Assistance (the "Rio Treaty") of 1947, the Western Hemisphere's regional mutual security treaty. Ecuador shares U.S. concern over increasing narcotrafficking and international terrorism and has energetically condemned terrorist actions, whether directed against government officials or private citizens. The government has maintained Ecuador virtually free of coca production since the mid-1980s and is working to combat money laundering and the transshipment of drugs and chemicals essential to the processing of cocaine.

Ecuador and the U.S. agreed in 1999 to a 10-year arrangement whereby U.S. military surveillance aircraft could use the airbase at Manta, Ecuador, as a "Forward Operating Location" to detect drug trafficking flights through the region. The arrangement expired in 2009; former president Rafael Correa vowed not to renew it, and since then the Ecuador has not had any foreign military facilities in the country.

In fisheries issues, the United States claims jurisdiction for the management of coastal fisheries up to 200 mile (370 km) from its coast, but excludes highly migratory species; Ecuador, on the other hand, claims a 200-mile (370-km) territorial sea, and imposes license fees and fines on foreign fishing vessels in the area, making no exceptions for catches of migratory species. In the early 1970s, Ecuador seized about 100 foreign-flag vessels (many of them U.S.) and collected fees and fines of more than $6 million. After a drop-off in such seizures for some years, several U.S. tuna boats were again detained and seized in 1980 and 1981.

The U.S. Magnuson Fishery Conservation and Management Act then triggered an automatic prohibition of U.S. imports of tuna products from Ecuador. The prohibition was lifted in 1983, and although fundamental differences between U.S. and Ecuadorian legislation still exist, there is no current conflict. During the period that has elapsed since seizures which triggered the tuna import ban, successive Ecuadorian governments have declared their willingness to explore possible solutions to this problem with mutual respect for longstanding positions and principles of both sides. The election of Rafael Correa in October 2006, has strained relations between the two countries and relations have since been fraught with tension. Rafael Correa is quite critical of U.S. foreign policy.

In April 2011, relations between Ecuador and the United States soured particularly after Ecuador expelled the U.S. ambassador after a leaked diplomatic cable was shown accusing president Correa of knowingly ignoring police corruption. In reciprocation, the Ecuadorian ambassador Luis Gallegos was expelled from the United States.

In 2013, when Ecuador unilaterally pulled out of a preferential trade pact with the United States over claiming the U.S. used it as blackmail in regards to the asylum request of Edward Snowden, relations between Ecuador and the United States reached an all-time low. The pact offered Ecuador 23 million USD, which it offered to the U.S. for human rights training. Tariff free imports had been offered to Ecuador in exchange for drug elimination efforts.

Julian Assange applied for Ecuadorian citizenship on 16 September 2017, which Ecuador granted on 12 December 2017. However, this development was not announced until 25 January 2018.

In April 2019, Assange was arrested by the Metropolitan Police on orders from the embassy staff. President of Ecuador, Lenin Moreno, stated that he had 'violated the terms of his asylum'. British Foreign Secretary, Jeremy Hunt stated that the British and Ecuadorian governments had been co-operating since Moreno's inauguration and aimed to resolve the situation. Assange is currently pending trial for extradition to the United States.

American schools in Ecuador:





</doc>
<doc id="9727" url="https://en.wikipedia.org/wiki?curid=9727" title="Eight-ball">
Eight-ball

Eight-ball (also spelled 8-ball or eightball, and sometimes called solids and stripes, spots and stripes of highs and lows) is a pool billiards played on a billiard table with six pockets, cue sticks, and sixteen billiard balls: a and fifteen s. The object balls include seven solid-colored balls numbered 1 through 7, seven striped balls numbered 9 through 15, and the black 8 ball. After the balls are scattered with a , a player is assigned either the group of solid or striped balls once they have legally pocketed a ball from that group. The object of the game is to legally pocket the 8 ball in a "called" pocket, which can only be done after all of the balls from a player's assigned group have been cleared from the table.

The game is the most frequently played discipline of pool, and is often thought of as synonymous with "pool". The game has numerous variations, mostly regional. It is the second most played professional pool game, after nine-ball, and for the last several decades ahead of straight pool.

The game of eight-ball arose around 1900 in the United States as a development of pyramid pool, which allows any eight of the fifteen object balls to be pocketed to win. The game arose from two changes made, namely that the 8 ball must be pocketed last to win, and that each player may only pocket half of the other object balls. By 1925 the game was popular enough for the Brunswick-Balke-Collender Company to introduce purpose-made ball sets with seven , seven , one , and the cue ball, which allowed spectators to more easily see which suit each ball belonged to. (Such colors became standard in the later British-originating variant, blackball). The rules, as officially codified in the Billiard Congress of America's rule book, were periodically revised in the years following.

American-style eight-ball rules are played around the world by professionals, and in many amateur leagues. Nevertheless, the rules for eight-ball may be the most inconsistent of any billiard game as there are several competing sets of "official" rules. 

The non-profit World Pool-Billiard Association (WPA), which has continental and national affiliates around the world (some of which long pre-date the WPA, such as the Billiard Congress of America) promulgates standardized rules as "Pool Billiards – The Rules of Play". These are used for amateur and professional play.

Meanwhile, many amateur leagues – such as the American Poolplayers Association (APA) and its affiliate the Canadian Poolplayers Association (CPA), the Valley National Eight-ball Association (VNEA) and the BCA Pool League (BCAPL) – use their own rulesets which have slight differences from WPA rules and from each other. Millions of individuals play informally, using informal "house rules" which vary not only from area to area but even from venue to venue.

The regulation size of the table's playing surface is , though exact dimensions may vary slightly by manufacturer. Some leagues and tournaments using the World Standardized Rules may allow smaller sizes, down to . Early 20th-century models are occasionally also still used. WPA professional competition generally employs regulation tables, while the amateur league championships of various leagues, including ACS, BCAPL, VNEA, and APA, use the seven-foot tables in order to fit more of them into the hosting venue.

There are seven numbered 1 through 7, seven numbered 9 through 15, an , and a . The balls are usually colored as follows:

Special sets designed to be more easily discernible on television substitute pink for the dark purple of the 4 and 12 and light tan for the darker maroon of the 7 and 15 balls, and these alternative-color sets are now also available to consumers.

To start the game, the s are placed in a triangular rack. The base of the rack is parallel to the (the short end of the pool table) and positioned so the apex ball of the rack is located on the . The balls in the rack are ideally placed so that they are all in contact with one another; this is accomplished by pressing the balls together toward the apex ball. The order of the balls should be random, with the exceptions of the 8 ball, which must be placed in the center of the rack (i.e., the middle of the third row), and the two back corner balls, one of which must be a stripe and the other a solid. The cue ball is placed anywhere the breaker desires behind the . 

One person is chosen by some predetermined method (e.g., coin toss, , or win or loss of previous game or match) to shoot first, using the cue ball to the object-ball rack apart. In most leagues it is the breaker's opponent who racks the balls, but in some, players break their own racks. If the breaker fails to make a successful break—usually defined as at least four balls hitting cushions or an object ball being pocketed—then the opponent can opt either to play from the current position or to call for a and either re-break or have the original breaker repeat the break.
If the 8 ball is pocketed on the break, then the breaker can choose either to the 8 ball and play from the current position or to re-rack and re-break; but if the cue ball is also pocketed on the break then the opponent is the one who has the choice: either to re-spot the 8 ball and shoot with behind the , accepting the current position, or to re-break or have the breaker re-break.

A player (or team) continues to shoot until committing a or failing to legally pocket an object ball (whether or not); thereupon it is the turn of the opposing players. Play alternates in this manner for the remainder of the game. Following a foul, the incoming player has anywhere on the table, unless the foul occurred on the break shot, as noted previously.

The table is "open" at the start of the game, meaning that either player may shoot at any ball. It remains open until one player legally pockets one or more object balls (excluding the 8) after the break. That player is assigned the "group", or "suit", of the pocketed ball – 1–7 (solids), or 9–15 (stripes) – and the other suit is assigned to the opponent. Balls pocketed on the break, or as the result of a foul while the table is still open, are not used to assign the suits. If a player pockets balls from both suits on an open table, they may claim either suit as their own.

Once the suits are assigned, they remain fixed throughout the game. If any balls from a player's suit are on the table, the player must hit one of them first on every shot; otherwise a foul is called and the turn ends. After all balls from the suit have been pocketed, the player's target becomes the 8 for the remainder of the game.

Once all of a player's (or team's) group of object balls are pocketed, the player attempts to sink the 8 ball. In order to win the game the player first designates which pocket the 8 ball will be pocketed into and then successfully pockets the 8 ball into that pocket. If the player knocks the 8 ball off the table then the player loses the game. If the player pockets the 8 ball and commits a foul or pockets it into another pocket than the one designated, then the player loses the game. Otherwise (i.e., if the 8 ball is neither pocketed nor knocked off the table) the shooter's turn is simply over, even if a foul occurs. In short, a world-standardized rules game of eight-ball, like a game of nine-ball, is not over until the "" is no longer on the table. The rule has been increasingly adopted by amateur leagues.

Any of the following results in a player winning the game:
Because of these rules, it's actually possible for a game to end with only one of the players having shot.

If these fouls are made, the ball can be placed anywhere on the table to prevent a player from making a purposeful foul to disadvantage the other player.

The British version of eight-ball, known internationally as blackball, has evolved into a separate game, retaining significant elements of earlier pub versions of the game, with additional influences from English billiards and snooker. It is popular in amateur competition in Britain, Ireland, Australia, and some other countries.

The game uses unnumbered, solid-colored object balls, typically red and yellow, with one black 8 ball. They are usually or in diameter, the latter being the same size as the balls used in snooker and English billiards. Tables for blackball pool are long, and feature pockets with rounded cushion openings, like snooker tables. 

The rules of blackball differ from standard eight-ball in numerous ways, including the handling of fouls, which may give the opponent two shots, racking (the 8 ball, not the apex ball, goes on the spot), selection of which group of balls will be shot by which player, handling of balls and s, and many other details.

Internationally, the World Pool-Billiard Association and the World Eightball Pool Federation both publish rules and promote events. The two rule sets differ in some details regarding the penalties for fouls.

The hybrid game eight-ball rotation is a combination of eight-ball and rotation, in which the players must pocket their balls (other than the 8, which remains last) in numerical order. Specifically, the solids player starts by pocketing the 1 ball and ascends to the 7 ball, and the stripes player starts by pocketing the 15 ball and descends to the 9 ball.




</doc>
<doc id="9728" url="https://en.wikipedia.org/wiki?curid=9728" title="Earned value management">
Earned value management

Earned value management (EVM), earned value project management, or earned value performance management (EVPM) is a project management technique for measuring project performance and progress in an objective manner.

Earned value management is a project management technique for measuring project performance and progress. It has the ability to combine measurements of the project management triangle: scope, time, and costs.

In a single integrated system, earned value management is able to provide accurate forecasts of project performance problems, which is an important contribution for project management.

Early EVM research showed that the areas of planning and control are significantly impacted by its use; and similarly, using the methodology improves both scope definition as well as the analysis of overall project performance. More recent research studies have shown that the principles of EVM are positive predictors of project success. Popularity of EVM has grown in recent years beyond government contracting, a sector in which its importance continues to rise (e.g. recent new DFARS rules), in part because EVM can also surface in and help substantiate contract disputes.

Essential features of any EVM implementation include:
EVM implementations for large or complex projects include many more features, such as indicators and forecasts of cost performance (over budget or under budget) and schedule performance (behind schedule or ahead of schedule). However, the most basic requirement of an EVM system is that it quantifies progress using PV and EV.

Project A has been approved for a duration of one year and with the budget of X. It was also planned that the project spends 50% of the approved budget and expects 50% of the work to be complete in the first six months. If now, six months after the start of the project, a project manager would report that he has spent 50% of the budget, one can initially think, that the project is perfectly on plan. However, in reality the provided information is not sufficient to come to such a conclusion. The project can spend 50% of the budget, whilst finishing only 25% of the work, which would mean the project is not doing well; or the project can spend 50% of the budget, whilst completing 75% of the work, which would mean that project is doing better than planned. EVM is meant to address such and similar issues.

EVM emerged as a financial analysis specialty in United States Government programs in the 1960s, but it has since become a significant branch of project management and cost engineering. Project management research investigating the contribution of EVM to project success suggests a moderately strong positive relationship.
Implementations of EVM can be scaled to fit projects of all sizes and complexities.

The genesis of EVM occurred in industrial manufacturing at the turn of the 20th century, based largely on the principle of "earned time" popularized by Frank and Lillian Gilbreth, but the concept took root in the United States Department of Defense in the 1960s. The original concept was called PERT/COST, but it was considered overly burdensome (not very adaptable) by contractors whom were mandated to use it, and many variations of it began to proliferate among various procurement programs. In 1967, the DoD established a criterion-based approach, using a set of 35 criteria, called the Cost/Schedule Control Systems Criteria (C/SCSC). In the 1970s and early 1980s, a subculture of C/SCSC analysis grew, but the technique was often ignored or even actively resisted by project managers in both government and industry. C/SCSC was often considered a financial control tool that could be delegated to analytical specialists.

In 1979, EVM was introduced to the architecture and engineering industry in a "Public Works Magazine" article by David Burstein, a project manager with a national engineering firm. This technique has been taught ever since as part of the project management training program presented by PSMJ Resources, an international training and consulting firm that specializes in the engineering and architecture industry.

In the late 1980s and early 1990s, EVM emerged as a project management methodology to be understood and used by managers and executives, not just EVM specialists. In 1989, EVM leadership was elevated to the Undersecretary of Defense for Acquisition, thus making EVM an element of program management and procurement. In 1991, Secretary of Defense Dick Cheney canceled the Navy A-12 Avenger II Program because of performance problems detected by EVM. This demonstrated conclusively that EVM mattered to secretary-level leadership. In the 1990s, many U.S. Government regulations were eliminated or streamlined. However, EVM not only survived the acquisition reform movement, but became strongly associated with the acquisition reform movement itself. Most notably, from 1995 to 1998, ownership of EVM criteria (reduced to 32) was transferred to industry by adoption of ANSI EIA 748-A standard.

The use of EVM expanded beyond the U.S. Department of Defense. It was adopted by the National Aeronautics and Space Administration, United States Department of Energy and other technology-related agencies. Many industrialized nations also began to utilize EVM in their own procurement programs.

An overview of EVM was included in the Project Management Institute's first PMBOK Guide in 1987 and was expanded in subsequent editions. In the most recent edition of the PMBOK guide, EVM is listed among the general tools and techniques for processes to control project costs.

The construction industry was an early commercial adopter of EVM. Closer integration of EVM with the practice of project management accelerated in the 1990s. In 1999, the Performance Management Association merged with the Project Management Institute (PMI) to become PMI's first college, the College of Performance Management. The United States Office of Management and Budget began to mandate the use of EVM across all government agencies, and, for the first time, for certain internally managed projects (not just for contractors). EVM also received greater attention by publicly traded companies in response to the Sarbanes-Oxley Act of 2002.

In Australia EVM has been codified as standards AS 4817-2003 and AS 4817-2006.

It is helpful to see an example of project tracking that does not include earned value performance management. Consider a project that has been planned in detail, including a time-phased spend plan for all elements of work. Figure 1 shows the cumulative budget (cost) for this project as a function of time (the blue line, labeled PV). It also shows the cumulative actual cost of the project (red line, labeled AC) through week 8. To those unfamiliar with EVM, it might appear that this project was over budget through week 4 and then under budget from week 6 through week 8. However, what is missing from this chart is any understanding of how much work has been accomplished during the project. If the project was actually completed at week 8, then the project would actually be well under budget and well ahead of schedule. If, on the other hand, the project is only 10% complete at week 8, the project is significantly over budget and behind schedule. A method is needed to measure technical performance objectively and quantitatively, and that is what EVM accomplishes.

Consider the same project, except this time the project plan includes pre-defined methods of quantifying the accomplishment of work. At the end of each week, the project manager identifies every detailed element of work that has been completed, and sums the EV for each of these completed elements. Earned value may be accumulated monthly, weekly, or as progress is made.

formula_1

EV is calculated by multiplying %complete of each task (completed or in progress) by its planned value

Figure 2 shows the EV curve (in green) along with the PV curve from Figure 1. The chart indicates that technical performance (i.e. progress) started more rapidly than planned, but slowed significantly and fell behind schedule at week 7 and 8. This chart illustrates the schedule performance aspect of EVM. It is complementary to critical path or critical chain schedule management.

Figure 3 shows the same EV curve (green) with the actual cost data from Figure 1 (in red). It can be seen that the project was actually under budget, relative to the amount of work accomplished, since the start of the project. This is a much better conclusion than might be derived from Figure 1.

Figure 4 shows all three curves together – which is a typical EVM line chart. The best way to read these three-line charts is to identify the EV curve first, then compare it to PV (for schedule performance) and AC (for cost performance). It can be seen from this illustration that a true understanding of cost performance and schedule performance "relies first on measuring technical performance objectively." This is the "foundational principle" of EVM.

The "foundational principle" of EVM, mentioned above, does not depend on the size or complexity of the project. However, the "implementations" of EVM can vary significantly depending on the circumstances. In many cases, organizations establish an all-or-nothing threshold; projects above the threshold require a full-featured (complex) EVM system and projects below the threshold are exempted. Another approach that is gaining favor is to scale EVM implementation according to the project at hand and skill level of the project team.

There are many more small and simple projects than there are large and complex ones, yet historically only the largest and most complex have enjoyed the benefits of EVM. Still, lightweight implementations of EVM are achievable by any person who has basic spreadsheet skills. In fact, spreadsheet implementations are an excellent way to learn basic EVM skills.

The "first step" is to define the work. This is typically done in a hierarchical arrangement called a work breakdown structure (WBS) although the simplest projects may use a simple list of tasks. In either case, it is important that the WBS or list be comprehensive. It is also important that the elements be mutually exclusive, so that work is easily categorized in one and only one element of work. The most detailed elements of a WBS hierarchy (or the items in a list) are called activities (or tasks).

The "second step" is to assign a value, called planned value (PV), to each activity. For large projects, PV is almost always an allocation of the total project budget, and may be in units of currency (e.g. dollar, euro or naira) or in labor hours, or both. However, in very simple projects, each activity may be assigned a weighted “point value" which might not be a budget number. Assigning weighted values and achieving consensus on all PV quantities yields an important benefit of EVM, because it exposes misunderstandings and miscommunications about the scope of the project, and resolving these differences should always occur as early as possible. Some terminal elements can not be known (planned) in great detail in advance, and that is expected, because they can be further refined at a later time.

The "third step" is to define "earning rules" for each activity. The simplest method is to apply just one earning rule, such as the 0/100 rule, to all activities. Using the 0/100 rule, no credit is earned for an element of work until it is finished. A related rule is called the 50/50 rule, which means 50% credit is earned when an element of work is started, and the remaining 50% is earned upon completion. Other fixed earning rules such as a 25/75 rule or 20/80 rule are gaining favor, because they assign more weight to finishing work than for starting it, but they also motivate the project team to identify when an element of work is started, which can improve awareness of work-in-progress. These simple earning rules work well for small or simple projects because generally each activity tends to be fairly short in duration.

These initial three steps define the minimal amount of planning for simplified EVM. The "final step" is to execute the project according to the plan and measure progress. When activities are started or finished, EV is accumulated according to the earning rule. This is typically done at regular intervals (e.g. weekly or monthly), but there is no reason why EV cannot be accumulated in near real-time, when work elements are started/completed. In fact, waiting to update EV only once per month (simply because that is when cost data are available) only detracts from a primary benefit of using EVM, which is to create a technical performance scoreboard for the project team.

In a lightweight implementation such as described here, the project manager has not accumulated cost nor defined a detailed project schedule network (i.e. using a critical path or critical chain methodology). While such omissions are inappropriate for managing large projects, they are a common and reasonable occurrence in many very small or simple projects. Any project can benefit from using EV alone as a real-time score of progress. One useful result of this very simple approach (without schedule models and actual cost accumulation) is to compare EV curves of similar projects, as illustrated in Figure 5. In this example, the progress of three residential construction projects are compared by aligning the starting dates. If these three home construction projects were measured with the same PV valuations, the "relative" schedule performance of the projects can be easily compared.

The actual critical path is ultimately the determining factor of every project's duration. Because earned value schedule metrics take no account of critical path data, big budget activities that are not on the critical path have the potential to dwarf the impact of performing small budget critical path activities. This can lead to "gaming" the SV and Schedule Performance Index or SPI metrics by ignoring critical path activities in favor of big-budget activities that may have much float. This can sometimes even lead to performing activities out-of-sequence just to improve the schedule tracking metrics, which can cause major problems with quality.

A simple two-step process has been suggested to fix this:

In this way, the distorting aspect of float would be eliminated. There would be no benefit to performing a non-critical activity with much float until it is due in proper sequence. Also, an activity would not generate a negative schedule variance until it had used up its float. Under this method, one way of gaming the schedule metrics would be eliminated. The only way of generating a positive schedule variance (or SPI over 1.0) would be by completing work on the current critical path ahead of schedule, which is in fact the only way for a project to get ahead of schedule.

In addition to managing technical and schedule performance, large and complex projects require that cost performance be monitored and reviewed at regular intervals. To measure cost performance, planned value (or BCWS - Budgeted Cost of Work Scheduled) and earned value (or BCWP - Budgeted Cost of Work Performed) must be in units of currency (the same units that actual costs are measured.).

In large implementations, the planned value curve is commonly called a Performance Measurement Baseline (PMB) and may be arranged in control accounts, summary-level planning packages, planning packages and work packages.

In large projects, establishing control accounts is the primary method of delegating responsibility and authority to various parts of the performing organization. Control accounts are cells of a responsibility assignment (RACI) matrix, which is the intersection of the project WBS and the organizational breakdown structure (OBS). Control accounts are assigned to Control Account Managers (CAMs).

Large projects require more elaborate processes for controlling baseline revisions, more thorough integration with subcontractor EVM systems, and more elaborate management of procured materials.

In the United States, the primary standard for full-featured EVM systems is the ANSI/EIA-748A standard, published in May 1998 and reaffirmed in August 2002. The standard defines 32 criteria for full-featured EVM system compliance. As of the year 2007, a draft of ANSI/EIA-748B, a revision to the original is available from ANSI. Other countries have established similar standards.

In addition to using BCWS and BCWP, prior to 1998 implementations often use the term actual cost of work performed (ACWP) instead of AC. Additional acronyms and formulas include:


Proponents of EVM note a number of issues with implementing it, and further limitations may be inherent to the concept itself.

Because EVM requires quantification of a project plan, it is often perceived to be inapplicable to discovery-driven or Agile software development projects. For example, it may be impossible to plan certain research projects far in advance, because research itself uncovers some opportunities (research paths) and actively eliminates others. However, another school of thought holds that all work can be planned, even if in weekly timeboxes or other short increments.

Traditional EVM is not intended for non-discrete (continuous) effort. In traditional EVM standards, non-discrete effort is called "level of effort" (LOE). If a project plan contains a significant portion of LOE, and the LOE is intermixed with discrete effort, EVM results will be contaminated. This is another area of EVM research.

Traditional definitions of EVM typically assume that project accounting and project network schedule management are prerequisites to achieving any benefit from EVM. Many small projects don't satisfy either of these prerequisites, but they too can benefit from EVM, as described for simple implementations, above. Other projects can be planned with a project network, but do not have access to true and timely actual cost data. In practice, the collection of true and timely actual cost data can be the most difficult aspect of EVM. Such projects can benefit from EVM, as described for intermediate implementations, above, and Earned Schedule.

As a means of overcoming objections to EVM's lack of connection to qualitative performance issues, the Naval Air Systems Command (NAVAIR) PEO(A) organization initiated a project in the late 1990s to integrate true technical achievement into EVM projections by utilizing risk profiles. These risk profiles anticipate opportunities that may be revealed and possibly be exploited as development and testing proceeds. The published research resulted in a Technical Performance Management (TPM) methodology and software application that is still used by many DoD agencies in informing EVM estimates with technical achievement.
The research was peer-reviewed and was the recipient of the Defense Acquisition University Acquisition Research Symposium 1997 Acker Award for excellence in the exchange of information in the field of acquisition research.

There is the difficulty inherent for any periodic monitoring of synchronizing data timing: actual deliveries, actual invoicing, and the date the EVM analysis is done are all independent, so that some items have arrived but their invoicing has not and by the time analysis is delivered the data will likely be weeks behind events. This may limit EVM to a less tactical or less definitive role where use is combined with other forms to explain why or add recent news and manage future expectations.

There is a measurement limitation for how precisely EVM can be used, stemming from classic conflict between accuracy and precision, as the mathematics can calculate deceptively far beyond the precision of the measurements of data and the approximation that is the plan estimation. The limitation on estimation is commonly understood (such as the ninety-ninety rule in software) but is not visible in any margin of error. The limitations on measurement are largely a form of digitization error as EVM measurements ultimately can be no finer than by item, which may be the Work Breakdown Structure terminal element size, to the scale of reporting period, typically end summary of a month, and by the means of delivery measure. (The delivery measure may be actual deliveries, may include estimates of partial work done at the end of month subject to estimation limits, and typically does not include QC check or risk offsets.)

As traditionally implemented, earned value management deals with, and is based in, budget and cost. It has no relationship to the investment value or benefit for which the project has been funded and undertaken. Yet due to the use of the word “value” in the name, this fact is often misunderstood. However, earned value metrics can be used to compute the cost and schedule inputs to Devaux's Index of Project Performance (the DIPP), which integrates schedule and cost performance with the planned investment value of the project's scope across the project management triangle. 





</doc>
<doc id="9730" url="https://en.wikipedia.org/wiki?curid=9730" title="Electron microscope">
Electron microscope

An electron microscope is a microscope that uses a beam of accelerated electrons as a source of illumination. As the wavelength of an electron can be up to 100,000 times shorter than that of visible light photons, electron microscopes have a higher resolving power than light microscopes and can reveal the structure of smaller objects. A scanning transmission electron microscope has achieved better than 50 pm resolution in annular dark-field imaging mode and magnifications of up to about 10,000,000× whereas most light microscopes are limited by diffraction to about 200 nm resolution and useful magnifications below 2000×.

Electron microscopes use shaped magnetic fields to form electron optical lens systems that are analogous to the glass lenses of an optical light microscope.

Electron microscopes are used to investigate the ultrastructure of a wide range of biological and inorganic specimens including microorganisms, cells, large molecules, biopsy samples, metals, and crystals. Industrially, electron microscopes are often used for quality control and failure analysis. Modern electron microscopes produce electron micrographs using specialized digital cameras and frame grabbers to capture the images.

In 1926 Hans Busch developed the electromagnetic lens.

According to Dennis Gabor, the physicist Leó Szilárd tried in 1928 to convince him to build an electron microscope, for which he had filed a patent. The first prototype electron microscope, capable of four-hundred-power magnification, was developed in 1931 by the physicist Ernst Ruska and the electrical engineer Max Knoll. The apparatus was the first practical demonstration of the principles of electron microscopy. In May of the same year, Reinhold Rudenberg, the scientific director of Siemens-Schuckertwerke, obtained a patent for an electron microscope. In 1932, Ernst Lubcke of Siemens & Halske built and obtained images from a prototype electron microscope, applying the concepts described in Rudenberg's patent.

In the following year, 1933, Ruska built the first electron microscope that exceeded the resolution attainable with an optical (light) microscope. Four years later, in 1937, Siemens financed the work of Ernst Ruska and Bodo von Borries, and employed Helmut Ruska, Ernst's brother, to develop applications for the microscope, especially with biological specimens. Also in 1937, Manfred von Ardenne pioneered the scanning electron microscope. Siemens produced the first commercial electron microscope in 1938. The first North American electron microscope was constructed in 1938, at the University of Toronto, by Eli Franklin Burton and students Cecil Hall, James Hillier, and Albert Prebus. Siemens produced a transmission electron microscope (TEM) in 1939. Although current transmission electron microscopes are capable of two million-power magnification, as scientific instruments, they remain based upon Ruska's prototype.

The original form of the electron microscope, the transmission electron microscope (TEM), uses a high voltage electron beam to illuminate the specimen and create an image. The electron beam is produced by an electron gun, commonly fitted with a tungsten filament cathode as the electron source. The electron beam is accelerated by an anode typically at +100 keV (40 to 400 keV) with respect to the cathode, focused by electrostatic and electromagnetic lenses, and transmitted through the specimen that is in part transparent to electrons and in part scatters them out of the beam. When it emerges from the specimen, the electron beam carries information about the structure of the specimen that is magnified by the objective lens system of the microscope. The spatial variation in this information (the "image") may be viewed by projecting the magnified electron image onto a fluorescent viewing screen coated with a phosphor or scintillator material such as zinc sulfide. Alternatively, the image can be photographically recorded by exposing a photographic film or plate directly to the electron beam, or a high-resolution phosphor may be coupled by means of a lens optical system or a fibre optic light-guide to the sensor of a digital camera. The image detected by the digital camera may be displayed on a monitor or computer.

The resolution of TEMs is limited primarily by spherical aberration, but a new generation of hardware correctors can reduce spherical aberration to increase the resolution in high-resolution transmission electron microscopy (HRTEM) to below 0.5 angstrom (50 picometres), enabling magnifications above 50 million times. The ability of HRTEM to determine the positions of atoms within materials is useful for nano-technologies research and development.

Transmission electron microscopes are often used in electron diffraction mode. The advantages of electron diffraction over X-ray crystallography are that the specimen need not be a single crystal or even a polycrystalline powder, and also that the Fourier transform reconstruction of the object's magnified structure occurs physically and thus avoids the need for solving the phase problem faced by the X-ray crystallographers after obtaining their X-ray diffraction patterns.

One major disadvantage of the transmission electron microscope is the need for extremely thin sections of the specimens, typically about 100 nanometers. Creating these thin sections for biological and materials specimens is technically very challenging. Semiconductor thin sections can be made using a focused ion beam. Biological tissue specimens are chemically fixed, dehydrated and embedded in a polymer resin to stabilize them sufficiently to allow ultrathin sectioning. Sections of biological specimens, organic polymers, and similar materials may require staining with heavy atom labels in order to achieve the required image contrast.

One application of TEM is serial-section electron microscopy (ssEM), for example in analyzing the connectivity in volumetric samples of brain tissue by imaging many thin sections in sequence.

The SEM produces images by probing the specimen with a focused electron beam that is scanned across a rectangular area of the specimen (raster scanning). When the electron beam interacts with the specimen, it loses energy by a variety of mechanisms. The lost energy is converted into alternative forms such as heat, emission of low-energy secondary electrons and high-energy backscattered electrons, light emission (cathodoluminescence) or X-ray emission, all of which provide signals carrying information about the properties of the specimen surface, such as its topography and composition. The image displayed by an SEM maps the varying intensity of any of these signals into the image in a position corresponding to the position of the beam on the specimen when the signal was generated. In the SEM image of an ant shown below and to the right, the image was constructed from signals produced by a secondary electron detector, the normal or conventional imaging mode in most SEMs.

Generally, the image resolution of an SEM is lower than that of a TEM. However, because the SEM images the surface of a sample rather than its interior, the electrons do not have to travel through the sample. This reduces the need for extensive sample preparation to thin the specimen to electron transparency. The SEM is able to image bulk samples that can fit on its stage and still be maneuvered, including a height less than the working distance being used, often 4 millimeters for high-resolution images. The SEM also has a great depth of field, and so can produce images that are good representations of the three-dimensional surface shape of the sample. Another advantage of SEMs comes with environmental scanning electron microscopes (ESEM) that can produce images of good quality and resolution with hydrated samples or in low, rather than high, vacuum or under chamber gases. This facilitates imaging unfixed biological samples that are unstable in the high vacuum of conventional electron microscopes.

In the reflection electron microscope (REM) as in the TEM, an electron beam is incident on a surface but instead of using the transmission (TEM) or secondary electrons (SEM), the reflected beam of elastically scattered electrons is detected. This technique is typically coupled with reflection high energy electron diffraction (RHEED) and "reflection high-energy loss spectroscopy (RHELS)". Another variation is spin-polarized low-energy electron microscopy (SPLEEM), which is used for looking at the microstructure of magnetic domains.

The STEM rasters a focused incident probe across a specimen that (as with the TEM) has been thinned to facilitate detection of electrons scattered "through" the specimen. The high resolution of the TEM is thus possible in STEM. The focusing action (and aberrations) occur before the electrons hit the specimen in the STEM, but afterward in the TEM. The STEMs use of SEM-like beam rastering simplifies annular dark-field imaging, and other analytical techniques, but also means that image data is acquired in serial rather than in parallel fashion. Often TEM can be equipped with the scanning option and then it can function both as TEM and STEM.

In STM, a conductive tip held at a voltage is brought near a surface, and a profile can be obtained based on the tunneling probability of an electron from the tip to the sample since it is a function of distance.

In their most common configurations, electron microscopes produce images with a single brightness value per pixel, with the results usually rendered in grayscale. However, often these images are then colorized through the use of feature-detection software, or simply by hand-editing using a graphics editor. This may be done to clarify structure or for aesthetic effect and generally does not add new information about the specimen.

In some configurations information about several specimen properties is gathered per pixel, usually by the use of multiple detectors. In SEM, the attributes of topography and material contrast can be obtained by a pair of backscattered electron detectors and such attributes can be superimposed in a single color image by assigning a different primary color to each attribute. Similarly, a combination of backscattered and secondary electron signals can be assigned to different colors and superimposed on a single color micrograph displaying simultaneously the properties of the specimen.

Some types of detectors used in SEM have analytical capabilities, and can provide several items of data at each pixel. Examples are the Energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and Cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors, it is common to color code the signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal, which is not modified in any way.

Materials to be viewed under an electron microscope may require processing to produce a suitable sample. The technique required varies depending on the specimen and the analysis required:

Electron microscopes are expensive to build and maintain, but the capital and running costs of confocal light microscope systems now overlaps with those of basic electron microscopes. Microscopes designed to achieve high resolutions must be housed in stable buildings (sometimes underground) with special services such as magnetic field canceling systems.

The samples largely have to be viewed in vacuum, as the molecules that make up air would scatter the electrons. An exception is liquid-phase electron microscopy using either a closed liquid cell or an environmental chamber, for example, in the environmental scanning electron microscope, which allows hydrated samples to be viewed in a low-pressure (up to ) wet environment. Various techniques for in situ electron microscopy of gaseous samples have been developed as well.

Scanning electron microscopes operating in conventional high-vacuum mode usually image conductive specimens; therefore non-conductive materials require conductive coating (gold/palladium alloy, carbon, osmium, etc.). The low-voltage mode of modern microscopes makes possible the observation of non-conductive specimens without coating. Non-conductive materials can be imaged also by a variable pressure (or environmental) scanning electron microscope.

Small, stable specimens such as carbon nanotubes, diatom frustules and small mineral crystals (asbestos fibres, for example) require no special treatment before being examined in the electron microscope. Samples of hydrated materials, including almost all biological specimens have to be prepared in various ways to stabilize them, reduce their thickness (ultrathin sectioning) and increase their electron optical contrast (staining). These processes may result in "artifacts", but these can usually be identified by comparing the results obtained by using radically different specimen preparation methods. Since the 1980s, analysis of cryofixed, vitrified specimens has also become increasingly used by scientists, further confirming the validity of this technique.

Biology and life sciences






</doc>
<doc id="9731" url="https://en.wikipedia.org/wiki?curid=9731" title="List of extinct bird species since 1500">
List of extinct bird species since 1500

Over 190 species of birds have become extinct since 1500, and the rate of extinction seems to be increasing. The situation is exemplified by Hawaii, where 30% of all known recently extinct bird taxa originally lived. Other areas, such as Guam, have also been hit hard; Guam has lost over 60% of its native bird taxa in the last 30 years, many of them due to the introduced brown tree snake.

Currently there are approximately 10,000 living species of birds, with an estimated 1,200 considered to be under threat of extinction.

Island species in general, and flightless island species in particular, are most at risk. The disproportionate number of rails in the list reflects the tendency of that family to lose the ability to fly when geographically isolated. Even more rails became extinct before they could be described by scientists; these taxa are listed in List of Late Quaternary prehistoric bird species.

The extinction dates given below are usually approximations of the actual date of extinction. In some cases, more exact dates are given as it is sometimes possible to pinpoint the date of extinction to a specific year or even day (the San Benedicto rock wren is possibly the most extreme example—its extinction could be timed with an accuracy of maybe half an hour). Extinction dates in the literature are usually the dates of the last verified record (credible observation or specimen taken); for many Pacific birds that became extinct shortly after European contact, however, this leaves an uncertainty period of over a century, because the islands on which they lived were only rarely visited by scientists.



Ducks, geese and swans

Quails and relatives
See also Bokaak "bustard" under Gruiformes below

Shorebirds, gulls and auks

Rails and allies - probably paraphyletic


Grebes


Pelicans and related birds


Boobies and related birds


Petrels, shearwaters, albatrosses and storm petrels.

Penguins

Pigeons, doves and dodos
For the "Réunion solitaire", see Réunion ibis.

Parrots


Cuckoos

Birds of prey

Typical owls and barn owls.

Caprimulgidae - nightjars and nighthawks
Reclusive ground-nesting birds that sally out at night to hunt for large insects and similar prey. They are easily located by the males' song, but this is not given all year. Habitat destruction represents currently the biggest threat, while island populations are threatened by introduced mammalian predators, notably dogs, cats, pigs and mongooses.

Swifts and hummingbirds

Kingfishers and related birds

Woodpeckers and related birds

Perching birds

Furnariidae- Ovenbirds

Acanthisittidae– New Zealand "wrens"

Mohoidae – Hawaiian "honeyeaters". Family established in 2008, previously in Meliphagidae.

Meliphagidae – honeyeaters and Australian chats

Acanthizidae – scrubwrens, thornbills, and gerygones

Pachycephalidae – whistlers, shrike-thrushes, pitohuis and allies

Dicruridae – monarch flycatchers and allies

Oriolidae – Old World orioles and allies

Callaeidae – New Zealand wattlebirds


Hirundinidae – swallows and martins

Acrocephalidae – marsh and tree warblers
Muscicapidae – Old World flycatchers and chats

Megaluridae – megalurid warblers or grass warblers

Cisticolidae – cisticolas and allies

Zosteropidae – white-eyes - probably belonging to Timaliidae

Timaliidae – Old World babblers

Pycnonotidae – bulbuls

Sylvioidea "incertae sedis"

Sturnidae – starlings


Turdidae – thrushes and relatives

Mimidae – mockingbirds and thrashers

Estrildidae– estrildid finches (waxbills, munias, etc.

Icteridae – grackles

Parulidae – New World warblers
Ploceidae – weavers

Fringillidae – true finches and Hawaiian honeycreepers

Emberizidae – buntings and American sparrows

Extinction of subspecies is a subject very dependent on guesswork. National and international conservation projects and research publications such as redlists usually focus on species as a whole. Reliable information on the status of threatened subspecies usually has to be assembled piecemeal from published observations, such as regional checklists. Therefore, the following listing contains a high proportion of taxa that may still exist, but are listed here due to any combination of absence of recent records, a known threat such as habitat destruction, or an observed decline.

Ratites and related birds




Tinamous

Ducks, geese and swans

Quails and relatives

Shorebirds, gulls and auks

Rails and allies - probably paraphyletic

Herons and related birds - possibly paraphyletic

Sandgrouses

Pigeons, doves and dodos

Parrots

Cuckoos

Birds of prey

Typical owls and barn owls

Nightjars and allies

Swifts and hummingbirds

Kingfishers and related birds

Woodpeckers and related birds

Perching birds

Pittidae – pittas

Tyrannidae – tyrant flycatchers

Furnariidae – ovenbirds

Formicariidae – antpittas and antthrushes

Maluridae – Australasian "wrens"

Pardalotidae – pardalotes, scrubwrens, thornbills and gerygones

PetroicidaeAustralasian "robins"

Cinclosomatidae – whipbirds and allies

Artamidae – woodswallows, currawongs and allies

Monarchidae – monarch flycatchers

Rhipiduridae – fantails

Campephagidae – cuckoo-shrikes and trillers

Oriolidae – orioles and figbird

Corvidae – crows, ravens, magpies and jays

Callaeidae – New Zealand wattlebirds

Regulidae – kinglets

Hirundinidae – swallows and martins

Phylloscopidae – phylloscopid warblers or leaf-warblers

Cettiidae – cettiid warblers or typical bush-warblers

Acrocephalidae – acrocephalid warblers or marsh- and tree warblers

Pycnonotidae – bulbuls

Cisticolidae – cisticolas and allies

Sylviidae – sylviid ("true") warblers and parrotbills

Zosteropidae – white-eyes. Probably belong into Timaliidae

Timaliidae – Old World babblers

"African warblers"

Sylvioidea "incertae sedis"

Troglodytidae – wrens

Paridae – tits, chickadees and titmice

Cinclidae – dippers

Muscicapidae – Old World flycatchers and chats

Turdidae – thrushes and allies

Mimidae – mockingbirds and thrashers

Estrildidae – estrildid finches (waxbills, munias, etc.)

Fringillidae – true finches and Hawaiian honeycreepers
Icteridae – grackles

Parulidae – New World warblers

Thraupidae – tanagers

Emberizidae – buntings and American sparrows





</doc>
<doc id="9732" url="https://en.wikipedia.org/wiki?curid=9732" title="Eli Whitney">
Eli Whitney

Eli Whitney (December 8, 1765January 8, 1825) was an American inventor, widely known for inventing the cotton gin, one of the key inventions of the Industrial Revolution and shaped the economy of the Antebellum South. 

Whitney's invention made upland short cotton into a profitable crop, which strengthened the economic foundation of slavery in the United States.

Despite the social and economic impact of his invention, Whitney lost many profits in legal battles over patent infringement for the cotton gin. Thereafter, he turned his attention into securing contracts with the government in the manufacture of muskets for the newly formed United States Army. He continued making arms and inventing until his death in 1825.

Whitney was born in Westborough, Massachusetts, on December 8, 1765, the eldest child of Eli Whitney Sr., a prosperous farmer, and his wife Elizabeth Fay, also of Westborough.

Although the younger Eli, born in 1765, could technically be called a "Junior", history has never known him as such. He was famous during his lifetime and afterward by the name "Eli Whitney". His son, born in 1820, also named Eli, was well known during his lifetime and afterward by the name "Eli Whitney, Jr."

Whitney's mother, Elizabeth Fay, died in 1777, when he was 11. At age 14 he operated a profitable nail manufacturing operation in his father's workshop during the Revolutionary War.

Because his stepmother opposed his wish to attend college, Whitney worked as a farm laborer and school teacher to save money. He prepared for Yale at Leicester Academy (now Becker College) and under the tutelage of Rev. Elizur Goodrich of Durham, Connecticut, he entered in the fall of 1789 and graduated Phi Beta Kappa in 1792. Whitney expected to study law but, finding himself short of funds, accepted an offer to go to South Carolina as a private tutor.
Instead of reaching his destination, he was convinced to visit Georgia. In the closing years of the 18th century, Georgia was a magnet for New Englanders seeking their fortunes (its Revolutionary-era governor had been Lyman Hall, a migrant from Connecticut). When he initially sailed for South Carolina, among his shipmates were the widow (Catherine Littlefield Greene) and family of the Revolutionary hero Gen. Nathanael Greene of Rhode Island. Mrs. Greene invited Whitney to visit her Georgia plantation, Mulberry Grove. Her plantation manager and husband-to-be was Phineas Miller, another Connecticut migrant and Yale graduate (class of 1785), who would become Whitney's business partner.

Whitney is most famous for two innovations which came to have significant impacts on the United States in the mid-19th century: the cotton gin (1793) and his advocacy of interchangeable parts. In the South, the cotton gin revolutionized the way cotton was harvested and reinvigorated slavery. Conversely, in the North the adoption of interchangeable parts revolutionized the manufacturing industry, contributing greatly to the U.S. victory in the Civil War.

The cotton gin is a mechanical device that removes the seeds from cotton, a process that had previously been extremely labor-intensive. The word "gin" is short for "engine." While staying at Mulberry Grove, Whitney constructed several ingenious household devices which led Mrs Greene to introduce him to some businessmen who were discussing the desirability of a machine to
separate the short staple upland cotton from its seeds, work that was then done by hand at the rate of a pound of lint a day. In a few weeks Whitney produced a model. The cotton gin was a wooden drum stuck with hooks that pulled the cotton fibers through a mesh. The cotton seeds would not fit through the mesh and fell outside. Whitney occasionally told a story wherein he was pondering an improved method of seeding the cotton when he was inspired by observing a cat attempting to pull a chicken through a fence, and able to only pull through some of the feathers.

A single cotton gin could generate up to of cleaned cotton daily. This contributed to the economic development of the Southern United States, a prime cotton growing area; some historians believe that this invention allowed for the African slavery system in the Southern United States to become more sustainable at a critical point in its development.

Whitney applied for the patent for his cotton gin on October 28, 1793, and received the patent (later numbered as X72) on March 14, 1794, but it was not validated until 1807. Whitney and his partner, Miller, did not intend to sell the gins. Rather, like the proprietors of grist and sawmills, they expected to charge farmers for cleaning their cotton – two-fifths of the value, paid in cotton. Resentment at this scheme, the mechanical simplicity of the device and the primitive state of patent law, made infringement inevitable. Whitney and Miller could not build enough gins to meet demand, so gins from other makers found ready sale. Ultimately, patent infringement lawsuits consumed the profits (one patent, later annulled, was granted in 1796 to Hogden Holmes for a gin which substituted circular saws for the spikes) and their cotton gin company went out of business in 1797. One oft-overlooked point is that there were drawbacks to Whitney's first design. There is significant evidence that the design flaws were solved by his sponsor, Mrs. Greene, but Whitney gave her no public credit or recognition.

After validation of the patent, the legislature of South Carolina voted $50,000 for the rights for that state, while North Carolina levied a license tax for five years, from which about $30,000 was realized. There is a claim that Tennessee paid, perhaps, $10,000.
While the cotton gin did not earn Whitney the fortune he had hoped for, it did give him fame. It has been argued by some historians that Whitney's cotton gin was an important if unintended cause of the American Civil War. After Whitney's invention, the plantation slavery industry was rejuvenated, eventually culminating in the Civil War.

The cotton gin transformed Southern agriculture and the national economy. Southern cotton found ready markets in Europe and in the burgeoning textile mills of New England. Cotton exports from the U.S. boomed after the cotton gin's appearance – from less than in 1793 to by 1810. Cotton was a staple that could be stored for long periods and shipped long distances, unlike most agricultural products. It became the U.S.'s chief export, representing over half the value of U.S. exports from 1820 to 1860.

Paradoxically, the cotton gin, a labor-saving device, helped preserve and prolong slavery in the United States for another 70 years. Before the 1790s, slave labor was primarily employed in growing rice, tobacco, and indigo, none of which were especially profitable anymore. Neither was cotton, due to the difficulty of seed removal. But with the invention of the gin, growing cotton with slave labor became highly profitable – the chief source of wealth in the American South, and the basis of frontier settlement from Georgia to Texas. "King Cotton" became a dominant economic force, and slavery was sustained as a key institution of Southern society.

Eli Whitney has often been incorrectly credited with inventing the idea of interchangeable parts, which he championed for years as a maker of muskets; however, the idea predated Whitney, and Whitney's role in it was one of promotion and popularizing, not invention. Successful implementation of the idea eluded Whitney until near the end of his life, occurring first in others' armories.

Attempts at interchangeability of parts can be traced back as far as the Punic Wars through both archaeological remains of boats now in Museo Archeologico Baglio Anselmi and contemporary written accounts. In modern times the idea developed over decades among many people. An early leader was Jean-Baptiste Vaquette de Gribeauval, an 18th-century French artillerist who created a fair amount of standardization of artillery pieces, although not true interchangeability of parts. He inspired others, including Honoré Blanc and Louis de Tousard, to work further on the idea, and on shoulder weapons as well as artillery. In the 19th century these efforts produced the "armory system," or American system of manufacturing. Certain other New Englanders, including Captain John H. Hall and Simeon North, arrived at successful interchangeability before Whitney's armory did. The Whitney armory finally succeeded not long after his death in 1825.

The motives behind Whitney's acceptance of a contract to manufacture muskets in 1798 were mostly monetary. By the late 1790s, Whitney was on the verge of bankruptcy and the cotton gin litigation had left him deeply in debt. His New Haven cotton gin factory had burned to the ground, and litigation sapped his remaining resources. The French Revolution had ignited new conflicts between Great Britain, France, and the United States. The new American government, realizing the need to prepare for war, began to rearm. The War Department issued contracts for the manufacture of 10,000 muskets. Whitney, who had never made a gun in his life, obtained a contract in January 1798 to deliver 10,000 to 15,000 muskets in 1800. He had not mentioned interchangeable parts at that time. Ten months later, the Treasury Secretary, Oliver Wolcott, Jr., sent him a "foreign pamphlet on arms manufacturing techniques," possibly one of Honoré Blanc's reports, after which Whitney first began to talk about interchangeability.
In May 1798, Congress voted for legislation that would use eight hundred thousand dollars in order to pay for small arms and cannons in case war with France erupted. It offered a 5,000 dollar incentive with an additional 5,000 dollars once that money was exhausted for the person that was able to accurately produce arms for the government. Because the cotton gin had not brought Whitney the rewards he believed it promised, he accepted the offer. Although the contract was for one year, Whitney did not deliver the arms until 1809, using multiple excuses for the delay. Recently, historians have found that during 1801–1806, Whitney took the money and headed into South Carolina in order to profit from the cotton gin.

Although Whitney's demonstration of 1801 appeared to show the feasibility of creating interchangeable parts, Merritt Roe Smith concludes that it was "staged" and "duped government authorities" into believing that he had been successful. The charade gained him time and resources toward achieving that goal.

When the government complained that Whitney's price per musket compared unfavorably with those produced in government armories, he was able to calculate an actual price per musket by including fixed costs such as insurance and machinery, which the government had not accounted for. He thus made early contributions to both the concepts of cost accounting, and economic efficiency in manufacturing.

Machine tool historian Joseph W. Roe credited Whitney with inventing the first milling machine circa 1818. Subsequent work by other historians (Woodbury; Smith; Muir; Battison [cited by Baida]) suggests that Whitney was among a group of contemporaries all developing milling machines at about the same time (1814 to 1818), and that the others were more important to the innovation than Whitney was. (The machine that excited Roe may not have been built until 1825, after Whitney's death.) Therefore, no one person can properly be described as the inventor of the milling machine.

Despite his humble origins, Whitney was keenly aware of the value of social and political connections. In building his arms business, he took full advantage of the access that his status as a Yale alumnus gave him to other well-placed graduates, such as Oliver Wolcott, Jr., Secretary of the Treasury (class of 1778), and James Hillhouse, a New Haven developer and political leader.

His 1817 marriage to Henrietta Edwards, granddaughter of the famed evangelist Jonathan Edwards, daughter of Pierpont Edwards, head of the Democratic Party in Connecticut, and first cousin of Yale's president, Timothy Dwight, the state's leading Federalist, further tied him to Connecticut's ruling elite. In a business dependent on government contracts, such connections were essential to success.

Whitney died of prostate cancer on January 8, 1825, in New Haven, Connecticut, just a month after his 59th birthday. He left a widow and his four children behind. During the course of his illness, he reportedly invented and constructed several devices to mechanically ease his pain. 

The Eli Whitney Students Program, Yale University's admissions program for non-traditional students, is named in honor of Whitney, who not only began his studies there when he was 23, but also went on to graduate Phi Beta Kappa in just three years.





</doc>
<doc id="9734" url="https://en.wikipedia.org/wiki?curid=9734" title="The American Prisoner">
The American Prisoner

The American Prisoner is a British novel written by Eden Phillpotts and published in 1904 and adapted into a film by the same name in 1929. The story concerns an English woman who lives at Fox Tor farm, and an American captured during the American Revolutionary War and held at the prison at Princetown on Dartmoor.

The heroine's father, Maurice Malherb, is based on Thomas Windeatt.

In the novel "Malherb" is a miscreant who destroys Childe's tomb and beats his servant. He is depicted as a victim of his own bad temper rather than a sadist.

Malherb is introduced as the younger son of a noble family and he builds the Fox Tor house to be the impressive gentleman's residence suggested by William Crossing rather than the humble cottage which it actually is.


</doc>
<doc id="9735" url="https://en.wikipedia.org/wiki?curid=9735" title="Electromagnetic field">
Electromagnetic field

An electromagnetic field (also EM field) is a classical (i.e. non-quantum) field produced by moving electric charges. It is the field described by classical electrodynamics and is the classical counterpart to the quantized electromagnetic field tensor in quantum electrodynamics. The electromagnetic field propagates at the speed of light (in fact, this field can be identified "as" light) and interacts with charges and currents. Its quantum counterpart is one of the four fundamental forces of nature (the others are gravitation, weak interaction and strong interaction.)

The field can be viewed as the combination of an electric field and a magnetic field. The electric field is produced by stationary charges, and the magnetic field by moving charges (currents); these two are often described as the sources of the field. The way in which charges and currents interact with the electromagnetic field is described by Maxwell's equations and the Lorentz force law. The force created by the electric field is much stronger than the force created by the magnetic field.

From a classical perspective in the history of electromagnetism, the electromagnetic field can be regarded as a smooth, continuous field, propagated in a wavelike manner. By contrast, from the perspective of quantum field theory, this field is seen as quantized; meaning that the free quantum field (i.e. non-interacting field) can be expressed as the Fourier sum of creation and annihilation operators in energy-momentum space while the effects of the interacting quantum field may be analyzed in perturbation theory via the S-matrix with the aid of a whole host of mathematical technologies such as the Dyson series, Wick's theorem, correlation functions, time-evolution operators, Feynman diagrams etc. Note that the quantized field is still spatially continuous; its "energy states" however are discrete (the field's energy states must not be confused with its "energy values", which are continuous; the quantum field's creation operators create multiple "discrete" states of energy called photons.)

The electromagnetic field may be viewed in two distinct ways: a continuous structure or a discrete structure.

Classically, electric and magnetic fields are thought of as being produced by smooth motions of charged objects. For example, oscillating charges produce variations in electric and magnetic fields that may be viewed in a 'smooth', continuous, wavelike fashion. In this case, energy is viewed as being transferred continuously through the electromagnetic field between any two locations. For instance, the metal atoms in a radio transmitter appear to transfer energy continuously. This view is useful to a certain extent (radiation of low frequency), but problems are found at high frequencies (see ultraviolet catastrophe).

The electromagnetic field may be thought of in a more 'coarse' way. Experiments reveal that in some circumstances electromagnetic energy transfer is better described as being carried in the form of packets called quanta (in this case, photons) with a fixed frequency. Planck's relation links the photon energy "E" of a photon to its frequency f through the equation:

where "h" is Planck's constant, and "f" is the frequency of the photon . Although modern quantum optics tells us that there also is a semi-classical explanation of the photoelectric effect—the emission of electrons from metallic surfaces subjected to electromagnetic radiation—the photon was historically (although not strictly necessarily) used to explain certain observations. It is found that increasing the intensity of the incident radiation (so long as one remains in the linear regime) increases only the number of electrons ejected, and has almost no effect on the energy distribution of their ejection. Only the frequency of the radiation is relevant to the energy of the ejected electrons.

This quantum picture of the electromagnetic field (which treats it as analogous to harmonic oscillators) has proven very successful, giving rise to quantum electrodynamics, a quantum field theory describing the interaction of electromagnetic radiation with charged matter. It also gives rise to quantum optics, which is different from quantum electrodynamics in that the matter itself is modelled using quantum mechanics rather than quantum field theory.

In the past, electrically charged objects were thought to produce two different, unrelated types of field associated with their charge property. An electric field is produced when the charge is stationary with respect to an observer measuring the properties of the charge, and a magnetic field as well as an electric field is produced when the charge moves, creating an electric current with respect to this observer. Over time, it was realized that the electric and magnetic fields are better thought of as two parts of a greater whole—the electromagnetic field. Until 1820, when the Danish physicist H. C. Ørsted showed the effect of electric current on a compass needle, electricity and magnetism had been viewed as unrelated phenomena. In 1831, Michael Faraday made the seminal observation that time-varying magnetic fields could induce electric currents and then, in 1864, James Clerk Maxwell published his famous paper "A Dynamical Theory of the Electromagnetic Field".

Once this electromagnetic field has been produced from a given charge distribution, other charged or magnetised objects in this field may experience a force. If these other charges and currents are comparable in size to the sources producing the above electromagnetic field, then a new net electromagnetic field will be produced. Thus, the electromagnetic field may be viewed as a dynamic entity that causes other charges and currents to move, and which is also affected by them. These interactions are described by Maxwell's equations and the Lorentz force law. This discussion ignores the radiation reaction force.

The behavior of the electromagnetic field can be divided into four different parts of a loop:

A common misunderstanding is that (a) the quanta of the fields act in the same manner as (b) the charged particles, such as electrons, that generate the fields. In our everyday world, electrons travel slowly through conductors with a drift velocity of a fraction of a centimeter (or inch) per second and through a vacuum tube at speeds of around 1 thousand km/s, but fields propagate at the speed of light, approximately 300 thousand kilometers (or 186 thousand miles) a second. The speed ratio between charged particles in a conductor and field quanta is on the order of one to a million. Maxwell's equations relate (a) the presence and movement of charged particles with (b) the generation of fields. Those fields can then affect the force on, and can then move other slowly moving charged particles. Charged particles can move at relativistic speeds nearing field propagation speeds, but, as Albert Einstein showed, this requires enormous field energies, which are not present in our everyday experiences with electricity, magnetism, matter, and time and space.

The feedback loop can be summarized in a list, including phenomena belonging to each part of the loop:

There are different mathematical ways of representing the electromagnetic field. The first one views the electric and magnetic fields as three-dimensional vector fields. These vector fields each have a value defined at every point of space and time and are thus often regarded as functions of the space and time coordinates. As such, they are often written as E(x, y, z, t) (electric field) and B(x, y, z, t) (magnetic field).

If only the electric field (E) is non-zero, and is constant in time, the field is said to be an electrostatic field. Similarly, if only the magnetic field (B) is non-zero and is constant in time, the field is said to be a magnetostatic field. However, if either the electric or magnetic field has a time-dependence, then both fields must be considered together as a coupled electromagnetic field using Maxwell's equations.

With the advent of special relativity, physical laws became susceptible to the formalism of tensors. Maxwell's equations can be written in tensor form, generally viewed by physicists as a more elegant means of expressing physical laws.

The behaviour of electric and magnetic fields, whether in cases of electrostatics, magnetostatics, or electrodynamics (electromagnetic fields), is governed by Maxwell's equations. In the vector field formalism, these are:

where formula_6 is the charge density, which can (and often does) depend on time and position, formula_7 is the permittivity of free space, formula_8 is the permeability of free space, and J is the current density vector, also a function of time and position. The units used above are the standard SI units. Inside a linear material, Maxwell's equations change by switching the permeability and permittivity of free space with the permeability and permittivity of the linear material in question. Inside other materials which possess more complex responses to electromagnetic fields, these terms are often represented by complex numbers, or tensors.

The Lorentz force law governs the interaction of the electromagnetic field with charged matter.

When a field travels across to different media, the properties of the field change according to the various boundary conditions. These equations are derived from Maxwell's equations.
The tangential components of the electric and magnetic fields as they relate on the boundary of two media are as follows:

The angle of refraction of an electric field between media is related to the permittivity formula_13 of each medium:

The angle of refraction of a magnetic field between media is related to the permeability formula_15 of each medium:

The two Maxwell equations, Faraday's Law and the Ampère-Maxwell Law, illustrate a very practical feature of the electromagnetic field. Faraday's Law may be stated roughly as 'a changing magnetic field creates an electric field'. This is the principle behind the electric generator.

Ampere's Law roughly states that 'a changing electric field creates a magnetic field'. Thus, this law can be applied to generate a magnetic field and run an electric motor.

Maxwell's equations take the form of an electromagnetic wave in a volume of space not containing charges or currents (free space) – that is, where formula_6 and J are zero. Under these conditions, the electric and magnetic fields satisfy the electromagnetic wave equation:

James Clerk Maxwell was the first to obtain this relationship by his completion of Maxwell's equations with the addition of a displacement current term to Ampere's circuital law.

Being one of the four fundamental forces of nature, it is useful to compare the electromagnetic field with the gravitational, strong and weak fields. The word 'force' is sometimes replaced by 'interaction' because modern particle physics models electromagnetism as an exchange of particles known as gauge bosons.

Sources of electromagnetic fields consist of two types of charge – positive and negative. This contrasts with the sources of the gravitational field, which are masses. Masses are sometimes described as "gravitational charges", the important feature of them being that there are only positive masses and no negative masses. Further, gravity differs from electromagnetism in that positive masses attract other positive masses whereas same charges in electromagnetism repel each other.

The relative strengths and ranges of the four interactions and other information are tabulated below:

When an EM field (see electromagnetic tensor) is not varying in time, it may be seen as a purely electrical field or a purely magnetic field, or a mixture of both. However the general case of a static EM field with both electric and magnetic components present, is the case that appears to most observers. Observers who see only an electric or magnetic field component of a static EM field, have the other (electric or magnetic) component suppressed, due to the special case of the immobile state of the charges that produce the EM field in that case. In such cases the other component becomes manifest in other observer frames.

A consequence of this, is that any case that seems to consist of a "pure" static electric or magnetic field, can be converted to an EM field, with both E and M components present, by simply moving the observer into a frame of reference which is moving with regard to the frame in which only the “pure” electric or magnetic field appears. That is, a pure static electric field will show the familiar magnetic field associated with a current, in any frame of reference where the charge moves. Likewise, any new motion of a charge in a region that seemed previously to contain only a magnetic field, will show that the space now contains an electric field as well, which will be found to produces an additional Lorentz force upon the moving charge.

Thus, electrostatics, as well as magnetism and magnetostatics, are now seen as studies of the static EM field when a particular frame has been selected to suppress the other type of field, and since an EM field with both electric and magnetic will appear in any other frame, these "simpler" effects are merely the observer's. The "applications" of all such non-time varying (static) fields are discussed in the main articles linked in this section.

An EM field that varies in time has two “causes” in Maxwell's equations. One is charges and currents (so-called “sources”), and the other cause for an E or M field is a change in the other type of field (this last cause also appears in “free space” very far from currents and charges).

An electromagnetic field very far from currents and charges (sources) is called electromagnetic radiation (EMR) since it radiates from the charges and currents in the source, and has no "feedback" effect on them, and is also not affected directly by them in the present time (rather, it is indirectly produced by a sequences of changes in fields radiating out from them in the past). EMR consists of the radiations in the electromagnetic spectrum, including radio waves, microwave, infrared, visible light, ultraviolet light, X-rays, and gamma rays. The many commercial applications of these radiations are discussed in the named and linked articles.

A notable application of visible light is that this type of energy from the Sun powers all life on Earth that either makes or uses oxygen.

A changing electromagnetic field which is physically close to currents and charges (see near and far field for a definition of “close”) will have a dipole characteristic that is dominated by either a changing electric dipole, or a changing magnetic dipole. This type of dipole field near sources is called an electromagnetic "near-field".

Changing "electric" dipole fields, as such, are used commercially as near-fields mainly as a source of dielectric heating. Otherwise, they appear parasitically around conductors which absorb EMR, and around antennas which have the purpose of generating EMR at greater distances.

Changing "magnetic" dipole fields (i.e., magnetic near-fields) are used commercially for many types of magnetic induction devices. These include motors and electrical transformers at low frequencies, and devices such as metal detectors and MRI scanner coils at higher frequencies. Sometimes these high-frequency magnetic fields change at radio frequencies without being far-field waves and thus radio waves; see RFID tags.
See also near-field communication.
Further uses of near-field EM effects commercially, may be found in the article on virtual photons, since at the quantum level, these fields are represented by these particles. Far-field effects (EMR) in the quantum picture of radiation, are represented by ordinary photons.


The potential effects of electromagnetic fields on human health vary widely depending on the frequency and intensity of the fields.

The potential health effects of the very low frequency EMFs surrounding power lines and electrical devices are the subject of on-going research and a significant amount of public debate. The US National Institute for Occupational Safety and Health (NIOSH) and other US government agencies do not consider EMFs a proven health hazard. NIOSH has issued some cautionary advisories but stresses that the data are currently too limited to draw good conclusions.

Employees working at electrical equipment and installations can always be assumed to be exposed to electromagnetic fields. The exposure of office workers to fields generated by computers, monitors, etc. is negligible owing to the low field strengths. However, industrial installations for induction hardening and melting or on welding equipment may produce considerably higher field strengths and require further examination. If the exposure cannot be determined upon manufacturers' information, comparisons with similar systems or analytical calculations, measurements have to be accomplished. The results of the evaluation help to assess possible hazards to the safety and health of workers and to define protective measures. Since electromagnetic fields may influence passive or active implants of workers, it is essential to consider the exposure at their workplaces separately in the risk assessment. 

On the other hand, radiation from other parts of the electromagnetic spectrum, such as ultraviolet light and gamma rays, are known to cause significant harm in some circumstances. For more information on the health effects due to specific electromagnetic phenomena and parts of the electromagnetic spectrum, see the following articles:



</doc>
<doc id="9736" url="https://en.wikipedia.org/wiki?curid=9736" title="Empire State Building">
Empire State Building

The Empire State Building is a 102-story Art Deco skyscraper in Midtown Manhattan in New York City. It was designed by Shreve, Lamb & Harmon and built from 1930 to 1931. Its name is derived from "Empire State", the nickname of the state of New York. The building has a roof height of and stands a total of tall, including its antenna. The Empire State Building stood as the world's tallest building until the construction of the World Trade Center in 1970; following its collapse in the September 11, 2001 attacks, the Empire State Building was again the city's tallest skyscraper until 2012. , the building is the seventh-tallest building in New York City, the ninth-tallest completed skyscraper in the United States, the 48th-tallest in the world, and the fifth-tallest freestanding structure in the Americas.

The site of the Empire State Building, located in Midtown South on the west side of Fifth Avenue between West 33rd and 34th Streets, was originally part of an early 18th-century farm. It was developed in 1893 as the site of the Waldorf–Astoria Hotel. In 1929, Empire State Inc. acquired the site and devised plans for a skyscraper there. The design for the Empire State Building was changed fifteen times until it was ensured to be the world's tallest building. Construction started on March 17, 1930, and the building opened thirteen and a half months afterward on May 1, 1931. Despite favorable publicity related to the building's construction, because of the Great Depression and World War II, its owners did not make a profit until the early 1950s.

The building's Art Deco architecture, height, and observation decks have made it a popular attraction. Around 4 million tourists from around the world annually visit the building's 86th and 102nd floor observatories; an additional indoor observatory on the 80th floor opened in 2019. The Empire State Building is an American cultural icon: it has been featured in more than 250 TV shows and movies since the film "King Kong" was released in 1933. A symbol of New York City, the tower has been named as one of the Seven Wonders of the Modern World by the American Society of Civil Engineers. It was ranked first on the American Institute of Architects' List of America's Favorite Architecture in 2007. Additionally, the Empire State Building and its ground-floor interior were designated city landmarks by the New York City Landmarks Preservation Commission in 1980, and were added to the National Register of Historic Places as a National Historic Landmark in 1986.

The Empire State Building is located on the west side of Fifth Avenue in Manhattan, between 33rd Street to the south and 34th Street to the north. Tenants enter the building through the Art Deco lobby located at 350 Fifth Avenue. Visitors to the observatories use an entrance at 20 West 34th Street; prior to August 2018, visitors entered through the Fifth Avenue lobby. Although physically located in South Midtown, a mixed residential and commercial area, the building is so large that it was assigned its own ZIP Code, 10118; , it is one of 43 buildings in New York City that has its own ZIP code.

The areas surrounding the Empire State Building are home to other major points of interest, including Macy's at Herald Square on Sixth Avenue and 34th Street, Koreatown on 32nd Street between Fifth and Sixth Avenues, Penn Station and Madison Square Garden on Seventh Avenue between 32nd and 34th Streets, and the Flower District on 28th Street between Sixth and Seventh Avenues. The nearest New York City Subway stations are 34th Street–Penn Station at Seventh Avenue, two blocks west; 34th Street–Herald Square, one block west; and 33rd Street at Park Avenue, two blocks east. There is also a PATH station at 33rd Street and Sixth Avenue.

To the east of the Empire State Building is Murray Hill, a neighborhood with a mix of residential, commercial, and entertainment activity. One block east of the Empire State Building, on Madison Avenue at 34th Street, is the New York Public Library's Science, Industry and Business Library, which is located on the same block as the City University of New York's Graduate Center. Bryant Park and the New York Public Library Main Branch are located six blocks north of the Empire State Building, on the block bounded by Fifth Avenue, Sixth Avenue, 40th Street, and 42nd Street. Grand Central Terminal is located two blocks east of the library's Main Branch, at Park Avenue and 42nd Street.

The tract was originally part of Mary and John Murray's farm on Murray Hill. The earliest recorded major action on the site was during the American Revolutionary War, when General George Washington's troops retreated from the British following the Battle of Kip's Bay. In 1799, John Thompson (or Thomson; accounts vary) bought a tract of land roughly bounded by present-day Madison Avenue, 36th Street, Sixth Avenue, and 33rd Street, immediately north of the Caspar Samler farm. He paid a total of 482 British pounds for the parcel, equivalent to roughly $2,400 at the time, or about £ ($) today. Thompson was said to have sold the farm to Charles Lawton for $10,000 (equal to $ today) on September 24, 1825. The full details of this sale are unclear, as parts of the deed that certified the sale were later lost. In 1826, John Jacob Astor of the prominent Astor family bought the land from Lawton for $20,500. The Astors also purchased a parcel from the Murrays. John Jacob's son William Backhouse Astor Sr. bought a half interest in the properties for $20,500 on July 28, 1827, securing a tract of land on Fifth Avenue from 32nd to 35th streets.
On March 13, 1893, John Jacob Astor Sr's grandson William Waldorf Astor opened the Waldorf Hotel on the site with the help of hotelier George Boldt. On November 1, 1897, Waldorf's cousin, John Jacob Astor IV, opened the 16-story Astoria Hotel on an adjacent site. Together, the combined hotels had a total of 1,300 bedrooms making it the largest hotel in the world at the time. After Boldt died, in early 1918, the hotel lease was purchased by Thomas Coleman du Pont. By the 1920s, the hotel was becoming dated and the elegant social life of New York had moved much farther north than 34th Street. The Astor family decided to build a replacement hotel further uptown, and sold the hotel to Bethlehem Engineering Corporation in 1928 for $14–16 million. The hotel on the site of today's Empire State Building closed on May 3, 1929.

Bethlehem Engineering Corporation originally intended to build a 25-story office building on the Waldorf–Astoria site. The company's president, Floyd De L. Brown, paid $100,000 of the $1 million down payment required to start construction on the tower, with the promise that the difference would be paid later. Brown borrowed $900,000 from a bank, but then defaulted on the loan.

The land was then resold to Empire State Inc., a group of wealthy investors that included Louis G. Kaufman, Ellis P. Earle, John J. Raskob, Coleman du Pont, and Pierre S. du Pont. The name came from the state nickname for New York. Alfred E. Smith, a former Governor of New York and U.S. presidential candidate whose 1928 campaign had been managed by Raskob, was appointed head of the company. The group also purchased nearby land so they would have the needed for the tower's base, with the combined plot measuring wide by long. The Empire State Inc. consortium was announced to the public in August 1929.

Empire State Inc. contracted William F. Lamb, of architectural firm Shreve, Lamb and Harmon, to create the building design. Lamb produced the building drawings in just two weeks using the firm's earlier designs for the Reynolds Building in Winston-Salem, North Carolina as the basis. Concurrently, Lamb's partner Richmond Shreve created "bug diagrams" of the project requirements. The 1916 Zoning Act forced Lamb to design a structure that incorporated setbacks resulting in the lower floors being larger than the upper floors. Consequently, the tower was designed from the top down, giving it a "pencil"-like shape.

The original plan of the building was 50 stories, but was later increased to 60 and then 80 stories. Height restrictions were placed on nearby buildings to ensure that the top fifty floors of the planned 80-story, building would have unobstructed views of the city. "The New York Times" lauded the site's proximity to mass transit, with the Brooklyn–Manhattan Transit's 34th Street station and the Hudson and Manhattan Railroad's 33rd Street terminal one block away, as well as Penn Station two blocks away and the Grand Central Terminal nine blocks away at its closest. It also praised the of proposed floor space near "one of the busiest sections in the world".

While plans for the Empire State Building were being finalized, an intense competition in New York for the title of "world's tallest building" was underway. 40 Wall Street (then the Bank of Manhattan Building) and the Chrysler Building in Manhattan both vied for this distinction and were already under construction when work began on the Empire State Building. The "Race into the Sky", as popular media called it at the time, was representative of the country's optimism in the 1920s, fueled by the building boom in major cities. The 40 Wall Street tower was revised, in April 1929, from to making it the world's tallest. The Chrysler Building added its steel tip to its roof in October 1929, thus bringing it to a height of and greatly exceeding the height of 40 Wall Street. The Chrysler Building's developer, Walter Chrysler, realized that his tower's height would exceed the Empire State Building's as well, having instructed his architect, William Van Alen, to change the Chrysler's original roof from a stubby Romanesque dome to a narrow steel spire. Raskob, wishing to have the Empire State Building be the world's tallest, reviewed the plans and had five floors added as well as a spire; however, the new floors would need to be set back because of projected wind pressure on the extension. On November 18, 1929, Smith acquired a lot at 27–31 West 33rd Street, adding to the width of the proposed office building's site. Two days later, Smith announced the updated plans for the skyscraper. The plans included an observation deck on the 86th-floor roof at a height of , higher than the Chrysler's 71st-floor observation deck.

The 1,050-foot Empire State Building would only be taller than the Chrysler Building, and Raskob was afraid that Chrysler might try to "pull a trick like hiding a rod in the spire and then sticking it up at the last minute." The plans were revised one last time in December 1929, to include a 16-story, metal "crown" and an additional mooring mast intended for dirigibles. The roof height was now , making it the tallest building in the world by far, even without the antenna. The addition of the dirigible station meant that another floor, the now-enclosed 86th floor, would have to be built below the crown; however, unlike the Chrysler's spire, the Empire State's mast would serve a practical purpose. The final plan was announced to the public on January 8, 1930, just before the start of construction. "The New York Times" reported that the spire was facing some "technical problems", but they were "no greater than might be expected under such a novel plan." By this time the blueprints for the building had gone through up to fifteen versions before they were approved. Lamb described the other specifications he was given for the final, approved plan:

The contractors were Starrett Brothers and Eken, Paul and William A. Starrett and Andrew J. Eken, who would later construct other New York City buildings such as Stuyvesant Town, Starrett City and Trump Tower. The project was financed primarily by Raskob and Pierre du Pont, while James Farley's General Builders Supply Corporation supplied the building materials. John W. Bowser was the construction superintendent of the project, and the structural engineer of the building was Homer G. Balcom. The tight completion schedule necessitated the commencement of construction even though the design had yet to be finalized.

Demolition of the old Waldorf–Astoria began on October 1, 1929. Stripping the building down was an arduous process, as the hotel had been constructed using more rigid material than earlier buildings had been. Furthermore, the old hotel's granite, wood chips, and "'precious' metals such as lead, brass, and zinc" were not in high demand resulting in issues with disposal. Most of the wood was deposited into a woodpile on nearby 30th Street or was burned in a swamp elsewhere. Much of the other materials that made up the old hotel, including the granite and bronze, were dumped into the Atlantic Ocean near Sandy Hook, New Jersey.

By the time the hotel's demolition started, Raskob had secured the required funding for the construction of the building. The plan was to start construction later that year but, on October 24, the New York Stock Exchange suffered a sudden crash marking the beginning of the decade-long Great Depression. Despite the economic downturn, Raskob refused to cancel the project because of the progress that had been made up to that point. Neither Raskob, who had ceased speculation in the stock market the previous year, nor Smith, who had no stock investments, suffered financially in the crash. However, most of the investors were affected and as a result, in December 1929, Empire State Inc. obtained a $27.5 million loan from Metropolitan Life Insurance Company so construction could begin. The stock market crash resulted in no demand in new office space, Raskob and Smith nonetheless started construction, as canceling the project would have resulted in greater losses for the investors.

A structural steel contract was awarded on January 12, 1930, with excavation of the site beginning ten days later on January 22, before the old hotel had been completely demolished. Two twelve-hour shifts, consisting of 300 men each, worked continuously to dig the foundation. Small pier holes were sunk into the ground to house the concrete footings that would support the steelwork. Excavation was nearly complete by early March, and construction on the building itself started on March 17, with the builders placing the first steel columns on the completed footings before the rest of the footings had been finished. Around this time, Lamb held a press conference on the building plans. He described the reflective steel panels parallel to the windows, the large-block Indiana Limestone facade that was slightly more expensive than smaller bricks, and the tower's lines and rise. Four colossal columns, intended for installation in the center of the building site, were delivered; they would support a combined when the building was finished.

The structural steel was pre-ordered and pre-fabricated in anticipation of a revision to the city's building code that would have allowed the Empire State Building's structural steel to carry , up from , thus reducing the amount of steel needed for the building. Although the 18,000-psi regulation had been safely enacted in other cities, Mayor Jimmy Walker did not sign the new codes into law until March 26, 1930, just before construction was due to commence. The first steel framework was installed on April 1, 1930. From there, construction proceeded at a rapid pace; during one stretch of 10 working days, the builders erected fourteen floors. This was made possible through precise coordination of the building's planning, as well as the mass production of common materials such as windows and spandrels. On one occasion, when a supplier could not provide timely delivery of dark Hauteville marble, Starrett switched to using Rose Famosa marble from a German quarry that was purchased specifically to provide the project with sufficient marble.

The scale of the project was massive, with trucks carrying "16,000 partition tiles, 5,000 bags of cement, of sand and 300 bags of lime" arriving at the construction site every day. There were also cafes and concession stands on five of the incomplete floors so workers did not have to descend to the ground level to eat lunch. Temporary water taps were also built so workers did not waste time buying water bottles from the ground level. Additionally, carts running on a small railway system transported materials from the basement storage to elevators that brought the carts to the desired floors where they would then be distributed throughout that level using another set of tracks. The of steel ordered for the project was the largest-ever single order of steel at the time, comprising more steel than was ordered for the Chrysler Building and 40 Wall Street combined. According to historian John Tauranac, building materials were sourced from numerous, and distant, sources with "limestone from Indiana, steel girders from Pittsburgh, cement and mortar from upper New York State, marble from Italy, France, and England, wood from northern and Pacific Coast forests, [and] hardware from New England." The facade, too, used a variety of material, most prominently Indiana limestone but also Swedish black granite, terracotta, and brick.

By June 20, the skyscraper's supporting steel structure had risen to the 26th floor, and by July 27, half of the steel structure had been completed. Starrett Bros. and Eken endeavored to build one floor a day in order to speed up construction, a goal that they almost reached with their pace of stories per week; prior to this, the fastest pace of construction for a building of similar height had been stories per week. While construction progressed, the final designs for the floors were being designed from the ground up (as opposed to the general design, which had been from the roof down). Some of the levels were still undergoing final approval, with several orders placed within an hour of a plan being finalized. On September 10, as steelwork was nearing completion, Smith laid the building's cornerstone during a ceremony attended by thousands. The stone contained a box with contemporary artifacts including the previous day's "New York Times", a U.S. currency set containing all denominations of notes and coins minted in 1930, a history of the site and building, and photographs of the people involved in construction. The steel structure was topped out at on September 19, twelve days ahead of schedule and 23 weeks after the start of construction. Workers raised a flag atop the 86th floor to signify this milestone.

Afterward, work on the building's interior and crowning mast commenced. The mooring mast topped out on November 21, two months after the steelwork had been completed. Meanwhile, work on the walls and interior was progressing at a quick pace, with exterior walls built up to the 75th floor by the time steelwork had been built to the 95th floor. The majority of the facade was already finished by the middle of November. Because of the building's height, it was deemed infeasible to have many elevators or large elevator cabins, so the builders contracted with the Otis Elevator Company to make 66 cars that could speed at , which represented the largest-ever elevator order at the time.

In addition to the time constraint builders had, there were also space limitations because construction materials had to be delivered quickly, and trucks needed to drop off these materials without congesting traffic. This was solved by creating a temporary driveway for the trucks between 33rd and 34th Streets, and then storing the materials in the building's first floor and basements. Concrete mixers, brick hoppers, and stone hoists inside the building ensured that materials would be able to ascend quickly and without endangering or inconveniencing the public. At one point, over 200 trucks made material deliveries at the building site every day. A series of relay and erection derricks, placed on platforms erected near the building, lifted the steel from the trucks below and installed the beams at the appropriate locations. The Empire State Building was structurally completed on April 11, 1931, twelve days ahead of schedule and 410 days after construction commenced. Al Smith shot the final rivet, which was made of solid gold.

The project involved more than 3,500 workers at its peak, including 3,439 on a single day, August 14, 1930. Many of the workers were Irish and Italian immigrants, with a sizable minority of Mohawk ironworkers from the Kahnawake reserve near Montreal. According to official accounts, five workers died during the construction, although the "New York Daily News" gave reports of 14 deaths and a headline in the socialist magazine "The New Masses" spread unfounded rumors of up to 42 deaths. The Empire State Building cost $40,948,900 to build, including demolition of the Waldorf–Astoria (equivalent to $ in ). This was lower than the $60 million budgeted for construction.

Lewis Hine captured many photographs of the construction, documenting not only the work itself but also providing insight into the daily life of workers in that era. Hine's images were used extensively by the media to publish daily press releases. According to the writer Jim Rasenberger, Hine "climbed out onto the steel with the ironworkers and dangled from a derrick cable hundreds of feet above the city to capture, as no one ever had before (or has since), the dizzy work of building skyscrapers". In Rasenberger's words, Hine turned what might have been an assignment of "corporate flak" into "exhilarating art". These images were later organized into their own collection. Onlookers were enraptured by the sheer height at which the steelworkers operated. "New York" magazine wrote of the steelworkers: "Like little spiders they toiled, spinning a fabric of steel against the sky".

The Empire State Building officially opened on May 1, 1931, forty-five days ahead of its projected opening date, and eighteen months from the start of construction. The opening was marked with an event featuring United States President Herbert Hoover, who turned on the building's lights with the ceremonial button push from Washington, D.C.. Over 350 guests attended the opening ceremony, and following luncheon, at the 86th floor including Jimmy Walker, Governor Franklin D. Roosevelt, and Al Smith. An account from that day stated that the view from the luncheon was obscured by a fog, with other landmarks such as the Statue of Liberty being "lost in the mist" enveloping New York City. The building officially opened the next day. Advertisements for the building's observatories were placed in local newspapers, while nearby hotels also capitalized on the events by releasing advertisements that lauded their proximity to the newly opened tower.

According to "The New York Times", builders and real estate speculators predicted that the Empire State Building would be the world's tallest building "for many years", thus ending the great New York City skyscraper rivalry. At the time, most engineers agreed that it would be difficult to build a building taller than , even with the hardy Manhattan bedrock as a foundation. Technically, it was believed possible to build a tower of up to , but it was deemed uneconomical to do so, especially during the Great Depression. As the tallest building in the world, at that time, and the first one to exceed 100 floors, the Empire State Building became an icon of the city and, ultimately, of the nation.

In 1932, the Fifth Avenue Association gave the tower its 1931 "gold medal" for architectural excellence, signifying that the Empire State had been the best-designed building on Fifth Avenue to open in 1931. A year later, on March 2, 1933, the movie "King Kong" was released. The movie, which depicted a large stop motion ape named Kong climbing the Empire State Building, made the still-new building into a cinematic icon.

The Empire State Building's opening coincided with the Great Depression in the United States, and as a result much of its office space was vacant from its opening. In the first year, only 23% of the available space was rented, as compared to the early 1920s, where the average building would have occupancy of 52% upon opening and 90% rented within five years. The lack of renters led New Yorkers to deride the building as the "Empty State Building".

Jack Brod, one of the building's longest resident tenants, co-established the Empire Diamond Corporation with his father in the building in mid-1931 and rented space in the building until he died in 2008. Brod recalled that there were only about 20 tenants at the time of opening, including him, and that Al Smith was the only real tenant in the space above his seventh-floor offices. Generally, during the early 1930s, it was rare for more than a single office space to be rented in the building, despite Smith's and Raskob's aggressive marketing efforts in the newspapers and to anyone they knew. The building's lights were continuously left on, even in the unrented spaces, to give the impression of occupancy. This was exacerbated by competition from Rockefeller Center as well as from buildings on 42nd Street, which, when combined with the Empire State Building, resulted in surplus of office space in a slow market during the 1930s.

Aggressive marketing efforts served to reinforce the Empire State Building's status as the world's tallest. The observatory was advertised in local newspapers as well as on railroad tickets. The building became a popular tourist attraction, with one million people each paying one dollar to ride elevators to the observation decks in 1931. In its first year of operation, the observation deck made approximately $2 million in revenue, as much as its owners made in rent that year. By 1936, the observation deck was crowded on a daily basis, with food and drink available for purchase at the top, and by 1944 the tower had received its 5 millionth visitor. In 1931, NBC took up tenancy, leasing space on the 85th floor for radio broadcasts. From the outset the building was in debt, losing $1 million per year by 1935. Real estate developer Seymour Durst recalled that the building was so underused in 1936 that there was no elevator service above the 45th floor, as the building above the 41st floor was empty except for the NBC offices and the Raskob/Du Pont offices on the 81st floor.

Per the original plans, the Empire State Building's spire was intended to be an airship docking station. Raskob and Smith had proposed dirigible ticketing offices and passenger waiting rooms on the 86th floor, while the airships themselves would be tied to the spire at the equivalent of the building's 106th floor. An elevator would ferry passengers from the 86th to the 101st floor after they had checked in on the 86th floor, after which passengers would have climbed steep ladders to board the airship. The idea, however, was impractical and dangerous due to powerful updrafts caused by the building itself, the wind currents across Manhattan, and the spires of nearby skyscrapers. Furthermore, even if the airship were to successfully navigate all these obstacles, its crew would have to jettison some ballast by releasing water onto the streets below in order to maintain stability, and then tie the craft's nose to the spire with no mooring lines securing the tail end of the craft. On September 15, 1931, in the first and only instance of an airship using the building's mast, a small commercial United States Navy airship circled 25 times in winds. The airship then attempted to dock at the mast, but its ballast spilled and the craft was rocked by unpredictable eddies. The near-disaster scuttled plans to turn the building's spire into an airship terminal, although one blimp did manage to make a single newspaper delivery afterward.

On July 28, 1945, a B-25 Mitchell bomber crashed into the north side of the Empire State Building, between the 79th and 80th floors. One engine completely penetrated the building and landed in a neighboring block, while the other engine and part of the landing gear plummeted down an elevator shaft. Fourteen people were killed in the incident, but the building escaped severe damage and was reopened two days later.

The Empire State Building only started becoming profitable in the 1950s, when it was finally able to break even for the first time. At the time, mass transit options in the building's vicinity were limited compared to the present day. Despite this challenge, the Empire State Building began to attract renters due to its reputation. A radio antenna was erected on top of the towers starting in 1950, allowing the area's television stations to be broadcast from the building.

However, despite the turnaround in the building's fortunes, Raskob put the tower up for sale in 1951, with a minimum asking price of $50 million. The property was purchased by business partners Roger L. Stevens, Henry Crown, Alfred R. Glancy and Ben Tobin. The sale was brokered by the Charles F. Noyes Company, a prominent real estate firm in upper Manhattan, for $51 million, the highest price paid for a single structure at the time. By this time, the Empire State had been fully leased for several years with a waiting list of parties looking to lease space in the building, according to the "Cortland Standard". That same year, six news companies formed a partnership to pay a combined annual fee of $600,000 to use the tower's antenna, which was completed in 1953. Crown bought out his partners' ownership stakes in 1954, becoming the sole owner. The following year, the American Society of Civil Engineers named the building one of the "Seven Modern Civil Engineering Wonders".

In 1961, Lawrence A. Wien signed a contract to purchase the Empire State Building for $65 million, with Harry B. Helmsley acting as partners in the building's operating lease. This became the new highest price for a single structure. Over 3,000 people paid $10,000 for one share each in a company called Empire State Building Associates. The company in turn subleased the building to another company headed by Helmsley and Wien, raising $33 million of the funds needed to pay the purchase price. In a separate transaction, the land underneath the building was sold to Prudential Insurance for $29 million. Helmsley, Wien, and Peter Malkin quickly started a program of minor improvement projects, including the first-ever full-building facade refurbishment and window-washing in 1962, the installation of new flood lights on the 72nd floor in 1964, and replacement of the manually operated elevators with automatic units in 1966. The little-used western end of the second floor was used as a storage space until 1964, at which point it received escalators to the first floor as part of its conversion into a highly sought retail area.

In 1961, the same year that Helmsley, Wien, and Malkin had purchased the Empire State Building, the Port Authority of New York and New Jersey formally backed plans for a new World Trade Center in Lower Manhattan. The plan originally included 66-story twin towers with column-free open spaces. The Empire State's owners and real estate speculators were worried that the twin towers' of office space would create a glut of rentable space in Manhattan as well as take away the Empire State Building's profits from lessees. A revision in the World Trade Center's plan brought the twin towers to each or 110 stories, taller than the Empire State. Opponents of the new project included prominent real-estate developer Robert Tishman, as well as Wien's Committee for a Reasonable World Trade Center. In response to Wien's opposition, Port Authority executive director Austin J. Tobin said that Wien was only opposing the project because it would overshadow his Empire State Building as the world's tallest building.

The World Trade Center's twin towers started construction in 1966. The following year, the Ostankino Tower succeeded the Empire State Building as the tallest freestanding structure in the world. In 1970, the Empire State surrendered its position as the world's tallest building, when the World Trade Center's still-under-construction North Tower surpassed it, on October 19; the North Tower was topped out, on December 23, 1970.

In December 1975, the observation deck was opened on the 110th floor of the Twin Towers, significantly higher than the 86th floor observatory on the Empire State Building. The latter was also losing revenue during this period, particularly as a number of broadcast stations had moved to the World Trade Center in 1971; although the Port Authority continued to pay the broadcasting leases for the Empire State until 1984.

By 1980, there were nearly two million annual visitors, although a building official had previously estimated between 1.5 million and 1.75 million annual visitors. The building received its own ZIP code in May 1980 in a roll out of 63 new postal codes in Manhattan. At the time, the tenants of the tower collectively received 35,000 pieces of mail daily. The Empire State Building celebrated its 50th anniversary on May 1, 1981, with a much-publicized, but poorly received, laser light show, as well as an "Empire State Building Week" that ran through to May 8.

The New York City Landmarks Preservation Commission voted to make the lobby a city landmark on May 19, 1981, citing the historic nature of the first and second floors, as well as "the fixtures and interior components" of the upper floors. The building became a National Historic Landmark in 1986 in close alignment to the New York City Landmarks report. The Empire State Building was added to the National Register of Historic Places the following year due to its architectural significance.

Capital improvements were made to the Empire State Building during the early to mid-1990s at a cost of $55 million. These improvements entailed replacing alarm systems, elevators, windows, and air conditioning; making the observation deck compliant with the Americans with Disabilities Act of 1990 (ADA); and refurbishing the limestone facade. The observatory renovation was added after disability rights groups and the United States Department of Justice filed a lawsuit against the building in 1992, in what was the first lawsuit filed by an organization under the new law. A settlement was reached in 1994, in which the Empire State Building Associates agreed to add ADA-compliant elements, such as new elevators, ramps, and automatic doors, during its ongoing renovation.

Prudential sold the land under the building in 1991 for $42 million to a buyer representing hotelier Hideki Yokoi, who was imprisoned at the time in connection with a deadly fire at the Hotel New Japan hotel in Tokyo. In 1994, Donald Trump entered into a joint-venture agreement with Yokoi, with a shared goal of breaking the Empire State Building's lease on the land in an effort to gain total ownership of the building so that, if successful, the two could reap the potential profits of merging the ownership of the building with the land beneath it. Having secured a half-ownership of the land, Trump devised plans to take ownership of the building itself so he could renovate it, even though Helmsley and Malkin had already started their refurbishment project. He sued Empire State Building Associates in February 1995, claiming that the latter had caused the building to become a "high-rise slum" and a "second-rate, rodent-infested" office tower. Trump had intended to have Empire State Building Associates evicted for violating the terms of their lease, but was denied. This led to Helmsley's companies countersuing Trump in May. This sparked a series of lawsuits and countersuits that lasted several years, partly arising from Trump's desire to obtain the building's master lease by taking it from Empire State Building Associates. Upon Harry Helmsley's death in 1997, the Malkins sued Helmsley's widow, Leona Helmsley, for control of the building.

Following the destruction of the World Trade Center during the September 11 attacks in 2001, the Empire State Building again became the tallest building in New York City, but was only the second-tallest building in the Americas after the Sears Tower (now Willis Tower) in Chicago. As a result of the attacks, transmissions from nearly all of the city's commercial television and FM radio stations were again broadcast from the Empire State Building. The attacks also led to an increase in security due to persistent terror threats against New York City landmarks.

In 2002, Trump and Yokoi sold their land claim to the Empire State Building Associates, now headed by Malkin, in a $57.5 million sale. This action merged the building's title and lease for the first time in half a century. Despite the lingering threat posed by the 9/11 attacks, the Empire State Building remained popular with 3.5 million visitors to the observatories in 2004, compared to about 2.8 million in 2003.

Even though she maintained her ownership stake in the building until the post-consolidation IPO in October 2013, Leona Helmsley handed over day-to-day operations of the building in 2006 to Peter Malkin's company. In 2008, the building was temporarily "stolen" by the "New York Daily News" to show how easy it was to transfer the deed on a property, since city clerks were not required to validate the submitted information, as well as to help demonstrate how fraudulent deeds could be used to obtain large mortgages and then have individuals disappear with the money. The paperwork submitted to the city included the names of Fay Wray, the famous star of "King Kong", and Willie Sutton, a notorious New York bank robber. The newspaper then transferred the deed back over to the legitimate owners, who at that time were Empire State Land Associates.

Starting in 2009, the building's public areas received a $550 million renovation, with improvements to the air conditioning and waterproofing, renovations to the observation deck and main lobby, and relocation of the gift shop to the 80th floor. About $120 million was spent on improving the energy efficiency of the building, with the goal of reducing energy emissions by 38% within five years. For example, all of the windows were refurbished onsite into film-coated "superwindows" which block heat but pass light. Air conditioning operating costs on hot days were reduced, saving $17 million of the project's capital cost immediately and partially funding some of the other retrofits. The Empire State Building won the Leadership in Energy and Environmental Design (LEED) Gold for Existing Buildings rating in September 2011, as well as the World Federation of Great Towers' Excellence in Environment Award for 2010. For the LEED Gold certification, the building's energy reduction was considered, as was a large purchase of carbon offsets. Other factors included low-flow bathroom fixtures, green cleaning supplies, and use of recycled paper products.

On April 30, 2012, One World Trade Center topped out, taking the Empire State Building's record of tallest in the city. By 2014, the building was owned by the Empire State Realty Trust (ESRT), with Anthony Malkin as chairman, CEO, and president. The ESRT was a public company, having begun trading publicly on the New York Stock Exchange the previous year. In August 2016, the Qatar Investment Authority (QIA) was issued new fully diluted shares equivalent to 9.9% of the trust; this investment gave them partial ownership of the entirety of the ESRT's portfolio, and as a result, partial ownership of the Empire State Building. The trust's president John Kessler called it an "endorsement of the company's irreplaceable assets". The investment has been described by the real-estate magazine "The Real Deal" as "an unusual move for a sovereign wealth fund", as these funds typically buy direct stakes in buildings rather than real estate companies. Other foreign entities that have a stake in the ESRT include investors from Norway, Japan, and Australia.

A renovation of the Empire State Building was commenced in the 2010s to further improve energy efficiency, public areas, and amenities. In August 2018, to improve the flow of visitor traffic, the main visitor's entrance was shifted to 20 West 34th Street as part of a major renovation of the observatory lobby. The new lobby includes several technological features, including large LED panels, digital ticket kiosks in nine languages, and a two-story architectural model of the building surrounded by two metal staircases. The first phase of the renovation, completed in 2019, features an updated exterior lighting system and digital hosts. The new lobby also features free Wi-Fi provided for those waiting. A exhibit with nine galleries, opened in July 2019. The 102nd floor observatory, the third phase of the redesign, re-opened to the public on October 12, 2019. That portion of the project included outfitting the space with floor-to-ceiling glass windows and a brand-new glass elevator. The final portion of the renovations to be completed was a new observatory on the 80th floor, which opened on December 2, 2019. In total, the renovation had cost $165 million and taken four years to finish.

The height of the Empire State Building, to its 102nd floor, is , including its pinnacle. The building has 85 stories of commercial and office space representing a total of of rentable space. It has an indoor and outdoor observation deck on the 86th floor, the highest floor within the actual tower. The remaining 16 stories are part of the Art Deco spire, which is capped by an observatory on the 102nd floor. The spire is hollow with no floors between levels 86 and 102. Atop the tower is the pinnacle, much of which is covered by broadcast antennas, and surmounted with a lightning rod.

According to the official fact sheets the building rises 1,860 steps from the first to the 102nd floor, weighs , has an internal volume of , and an exterior with of limestone and granite. Construction of the tower's exterior required ten million bricks and of aluminum and stainless steel, and the interior required of elevator cable and of electrical wires. The building has a capacity for 20,000 tenants and 15,000 visitors.

The building has been named as one of the Seven Wonders of the Modern World by the American Society of Civil Engineers. The building and its street floor interior are designated landmarks of the New York City Landmarks Preservation Commission, and confirmed by the New York City Board of Estimate. It was designated as a National Historic Landmark in 1986. In 2007, it was ranked number one on the AIA's List of America's Favorite Architecture.

The Empire State Building's art deco design is typical of pre–World War II architecture in New York. The modernistic, stainless steel canopies of the entrances on 33rd and 34th Streets lead to two-story-high corridors around the elevator core, crossed by stainless steel and glass-enclosed bridges at the second-floor level. The riveted steel frame of the building was originally designed to handle all of the building's gravitational stresses and wind loads. The exterior of the building is clad in Indiana limestone panels sourced from the Empire Mill in Sanders, Indiana, which give the building its signature blonde color. The amount of material used in the building's construction resulted in a very stiff structure when compared to other skyscrapers, with a structural stiffness of versus the Willis Tower's and the John Hancock Center's . A December 1930 feature in "Popular Mechanics" estimated that a building with the Empire State's dimensions would still stand even if hit with an impact of .

The Empire State Building design has a recessed design, including one major setback and several smaller ones that reduce the level dimensions as the height increases, thus making upper 81 floors much smaller than the lower five floors. However, this design allows sunlight to illuminate the interiors of the top floors and, in addition, positions these floors away from the noisy streets below. This design was mandated as per the 1916 Zoning Resolution, which was intended to allow sunlight to reach the streets as well. Normally, a building of the Empire State's dimensions would be permitted to build up to 12 stories on the Fifth Avenue side, and up to 17 stories on the 33rd/34th Streets side, before it would have to utilize setbacks. However, the setbacks were arranged such that the largest setback was on the sixth floor, above the five-floor "base", so the rest of the building above the sixth floor would have a facade of uniform shape.

The Empire State Building was the first building to have more than 100 floors. It has 6,514 windows; 73 elevators; a total floor area of ; and a base covering . Its original 64 elevators, built by the Otis Elevator Company, are located in a central core and are of varying heights, with the longest of these elevators reaching from the lobby to the 80th floor. As originally built, there were four "express" elevators that connected the lobby, 80th floor, and several landings in between; the other 60 "local" elevators connected the landings with the floors above these intermediate landings. Of the 64 total elevators, 58 were for passenger use (comprising the four express elevators and 54 local elevators), and eight were for freight deliveries. The elevators were designed to move at . At the time of the skyscraper's construction, their practical speed was limited to as per city law, but this limit was removed shortly after the building opened. Additional elevators connect the 80th floor to the six floors above it, as the six extra floors were built after the original 80 stories were approved. The elevators were mechanically operated until 2011, when they were replaced with digital elevators during the $550 million renovation of the building. The Empire State Building has 73 elevators in all, including service elevators.

Utilities are grouped in a central shaft. On each floor between levels 6 and 86, the central shaft is surrounded by a main corridor on all four sides. As per the final specifications of the building, the corridor is surrounded in turn by office space deep. Each of the floors has 210 structural columns that pass through it, which provide structural stability, but limits the amount of open space on these floors. However, the relative dearth of stone in the building allows for more space overall, with a 1:200 stone-to-building ratio in the Empire State compared to a 1:50 ratio in similar buildings.

The original main lobby is accessed from Fifth Avenue, on the building's east side, and contains an entrance with one set of double doors between a pair of revolving doors. At the top of each doorway is a bronze motif depicting one of three "crafts or industries" used in the building's construction—Electricity, Masonry, and Heating. The lobby contains two tiers of marble, a lighter marble on the top, above the storefronts, and a darker marble on the bottom, flush with the storefronts. There is a pattern of zigzagging terrazzo tiles on the lobby floor, which leads from the entrance on the east to the aluminum relief on the west. The chapel-like three-story-high lobby, which runs parallel to 33rd and 34th Streets, contains storefronts on both its northern and southern sides. These storefronts are framed on each side by tubes of dark "modernistically rounded marble", according to the New York City Landmarks Preservation Commission, and above by a vertical band of grooves set into the marble. Immediately inside the lobby is an airport-style security checkpoint.

The walls on both the northern and southern sides of the lobby house storefronts and escalators to a mezzanine level. At the west end of the lobby is an aluminum relief of the skyscraper as it was originally built (i.e. without the antenna). The relief, which was intended to provide a welcoming effect, contains an embossing of the building's outline, accompanied by what the Landmarks Preservation Commission describes as "the rays of an aluminum sun shining out behind [the tower] and mingling with aluminum rays emanating from the spire of the Empire State Building". In the background is a state map of New York with the building's location marked by a "medallion" in the very southeast portion of the outline. A compass is located in the bottom right and a plaque to the tower's major developers is on the bottom left.
The plaque at the western end of the lobby is located on the eastern interior wall of a one-story tall rectangular-shaped corridor that surrounds the banks of escalators, with a similar design to the lobby. The rectangular shaped corridor actually consists of two long hallways on the northern and southern sides of the rectangle, as well as a shorter hallway on the eastern side and another long hallway on the western side. At both ends of the northern and southern corridors, there is a bank of four low-rise elevators in between the corridors. The western side of the rectangular elevator-bank corridor extends north to the 34th Street entrance and south to the 33rd Street entrance. It borders three large storefronts and leads to escalators that go both to the second floor and to the basement. Going from west to east, there are secondary entrances to 34th and 33rd Streets from both the northern and southern corridors, respectively, at approximately the two-thirds point of each corridor.

Until the 1960s, an art deco mural, inspired by both the sky and the Machine Age, was installed in the lobby ceilings. Subsequent damage to these murals, designed by artist Leif Neandross, resulted in reproductions being installed. Renovations to the lobby in 2009, such as replacing the clock over the information desk in the Fifth Avenue lobby with an anemometer and installing two chandeliers intended to be part of the building when it originally opened, revived much of its original grandeur. The north corridor contained eight illuminated panels created in 1963 by Roy Sparkia and Renée Nemorov, in time for the 1964 World's Fair, depicting the building as the Eighth Wonder of the World alongside the traditional seven. The building's owners installed a series of paintings by the New York artist Kysa Johnson in the concourse level. Johnson later filed a federal lawsuit, in January 2014, under the Visual Artists Rights Act alleging the negligent destruction of the paintings and damage to her reputation as an artist. As part of the building's 2010 renovation, Denise Amses commissioned a work consisting of 15,000 stars and 5,000 circles, superimposed on a etched-glass installation, in the lobby.

The final stage of the building was the installation of a hollow mast, a steel shaft fitted with elevators and utilities, above the 86th floor. At the top would be a conical roof and the 102nd-floor docking station. The elevators would ascend from the 86th floor ticket offices to a 101st-floor waiting room. From there, stairs would lead to the 102nd floor, where passengers would enter the airships. The airships would have been moored to the spire at the equivalent of the building's 106th floor.

On the 102nd floor of the Empire State Building (formerly the 101st floor), there is a door with stairs ascending to the 103rd floor (formerly the 102nd). This was built as a disembarkation floor for airships tethered to the building's spire, and has a circular balcony outside. It is now an access point to reach the spire for maintenance. The room now contains electrical equipment, but celebrities and dignitaries may also be given permission to take pictures there. Above the 103rd floor, there is a set of stairs and a ladder to reach the spire for maintenance work. The mast's 480 windows were all replaced in 2015.

Broadcasting began at the Empire State Building on December 22, 1931, when NBC and RCA began transmitting experimental television broadcasts from a small antenna erected atop the spire, with two separate transmitters for the visual and audio data. They leased the 85th floor and built a laboratory there. In 1934, RCA was joined by Edwin Howard Armstrong in a cooperative venture to test his FM system from the building's antenna. This setup, which entailed the installation of the world's first FM transmitter, continued only until October of the next year due to disputes between RCA and Armstrong. Specifically, NBC wanted to install more TV equipment in the room where Armstrong's transmitter was located.

After some time, the 85th floor became home to RCA's New York television operations initially as experimental station W2XBS channel 1 then, from 1941, as commercial station WNBT channel 1 (now WNBC channel 4). NBC's FM station, W2XDG, began transmitting from the antenna in 1940. NBC retained exclusive use of the top of the building until 1950 when the Federal Communications Commission (FCC) ordered the exclusive deal be terminated. The FCC directive was based on consumer complaints that a common location was necessary for the seven extant New York-area television stations to transmit from so that receiving antennas would not have to be constantly adjusted. Other television broadcasters would later join RCA at the building on the 81st through 83rd floors, often along with sister FM stations. Construction of a dedicated broadcast tower began on July 27, 1950, with TV, and FM, transmissions starting in 1951. The broadcast tower was completed by 1953. From 1951, six broadcasters agreed to pay a combined $600,000 per year for the use of the antenna. In 1965, a separate set of FM antennae was constructed ringing the 103rd floor observation area to act as a master antenna.

The placement of the stations in the Empire State Building became a major issue with the construction of the World Trade Center Twin Towers in the late 1960s, and early 1970s. The greater height of the Twin Towers would reflect radio waves broadcast from the Empire State Building, eventually resulting in some broadcasters relocating to the newer towers instead of suing the developer, the Port Authority of New York and New Jersey. Even though the nine stations who were broadcasting from the Empire State Building were leasing their broadcast space until 1984, most of these stations moved to the World Trade Center as soon as it was completed in 1971. The broadcasters obtained a court order stipulating that the Port Authority had to build a mast and transmission equipment in the North Tower, as well as pay the broadcasters' leases in the Empire State Building until 1984. Only a few broadcasters renewed their leases in the Empire State Building.

The September 11 attacks in 2001 destroyed the World Trade Center and the broadcast centers atop it, leaving most of the city's stations without a station for ten days until a temporary tower was built in Alpine, New Jersey. By October 2001, nearly all of the city's commercial broadcast stations (both television and FM radio) were again transmitting from the top of the Empire State Building. In a report that Congress commissioned about the transition from analog television to digital television, it was stated that the placement of broadcast stations in the Empire State Building was considered "problematic" due to interference from other nearby towers. In comparison, the Congressional report stated that the former Twin Towers had very few buildings of comparable height nearby thus signals suffered little interference. In 2003, a few FM stations were relocated to the nearby Condé Nast Building to reduce the number of broadcast stations using the Empire State Building. Eleven television stations and twenty-two FM stations had signed 15-year leases in the building by May 2003. It was expected that a taller broadcast tower in Bayonne, New Jersey, or Governors Island, would be built in the meantime with the Empire State Building being used as a "backup" since signal transmissions from the building were generally of poorer quality. Following the construction of One World Trade Center in the late 2000s and early 2010s, some TV stations began moving their transmitting facilities there.

, the Empire State Building is home to the following stations:

The 80th, 86th, and 102nd floors contain observatories. The latter two observatories saw a combined average of 4 million visitors per year in 2010. Since opening, the observatories have been more popular than similar observatories at 30 Rockefeller Plaza, the Chrysler Building, the first One World Trade Center, or the Woolworth Building, despite being more expensive. There are variable charges to enter the observatories; one ticket allows visitors to go as high as the 86th floor, and there is an additional charge to visit the 102nd floor. Other ticket options for visitors include scheduled access to view the sunrise from the observatory, a "premium" guided tour with VIP access, and the "AM/PM" package which allows for two visits in the same day.
The 86th floor observatory contains both an enclosed viewing gallery and an open-air outdoor viewing gallery, allowing for it to remain open 365 days a year regardless of the weather. The 102nd floor observatory is completely enclosed and much smaller in size. The 102nd floor observatory was closed to the public from the late 1990s to 2005 due to limited viewing capacity and long lines. The observation decks were redesigned in mid-1979. The 102nd floor was again redesigned in a project that was completed in 2019. An observatory on the 80th floor, opened in 2019, includes various exhibits as well as a mural of the skyline drawn by British artist Stephen Wiltshire.

According to a 2010 report by Concierge.com, the five lines to enter the observation decks are "as legendary as the building itself". Concierge.com stated that there are five lines: the sidewalk line, the lobby elevator line, the ticket purchase line, the second elevator line, and the line to get off the elevator and onto the observation deck. However, in 2016, New York City's official tourism website, NYCgo.com, made note of only three lines: the security check line, the ticket purchase line, and the second elevator line. Following renovations completed in 2019, designed to streamline queuing and reduce wait times, guests enter from a single entrance on 34th Street, where they make their way through exhibits on their way up to the observatories. Guests were offered a variety of ticket packages, including a package that enables them to skip the lines throughout the duration of their stay. The Empire State Building garners significant revenue from ticket sales for its observation decks, making more money from ticket sales than it does from renting office space during some years.

In early 1994, a motion simulator attraction was built on the 2nd floor, as a complement to the observation deck. The original cinematic presentation lasted approximately 25 minutes, while the simulation was about eight minutes.

The ride had two incarnations. The original version, which ran from 1994 until around 2002, featured James Doohan, "" Scotty, as the airplane's pilot who humorously tried to keep the flight under control during a storm. After the World Trade Center terrorist attacks on September 11, 2001, the ride was closed. An updated version debuted in mid-2002, featuring actor Kevin Bacon as the pilot, with the new flight also going haywire. This new version served a more informative goal, as opposed to the old version's main purpose of entertainment, and contained details about the 9/11 attacks. The simulator received mixed reviews, with assessments of the ride ranging from "great" to "satisfactory" to "corny".

The building was originally equipped with white searchlights atop the tower. They saw their first use in November 1932 when they lit up to signal Roosevelt's victory over Hoover in the presidential election of that year. These were later swapped for four "Freedom Lights" in 1956. In February 1964, flood lights were added on the 72nd floor to illuminate the top of the building at night so that the building could be seen from the World Fair later that year. The lights were shut off from November 1973 to July 1974 because of the energy crisis at the time. In 1976, the businessman Douglas Leigh suggested that Wien and Helmsley install 204 metal-halide lights, which were four times as bright as the 1,000 incandescent lights they were to replace. New red, white, and blue metal-halide lights were installed in time for the country's bicentennial that July. After the bicentennial, Helmsley retained the new lights due to the reduced maintenance cost, about $116 a year.

Since 1976, the spire has been lit in colors chosen to match seasonal events and holidays. Organizations are allowed to make requests through the building's website. The building is also lit in the colors of New York-based sports teams on nights when they host games: for example, orange, blue, and white for the New York Knicks; red, white, and blue for the New York Rangers. It was twice lit in scarlet to support New Jersey's Rutgers University, once for a football game against the University of Louisville on November 9, 2006, and again on April 3, 2007, when the women's basketball team played in the national championship game.

There have been special occasions where the lights are modified from the usual schedule. The structure was lit in red, white, and blue for several months after the destruction of the World Trade Center in September 2001, then reverted to the standard schedule. On June 4, 2002, the Empire State Building was lit in purple and gold (the royal colors of Elizabeth II), in thanks for the United Kingdom playing the Star Spangled Banner during the Changing of the Guard at Buckingham Palace on the day after the September 2001 attacks. On January 13, 2012, the building was lit in red, orange, and yellow to honor the 60th anniversary of NBC program "The Today Show". During June 1–3, 2012, the building was lit in blue and white, the colors of the Israeli flag, in honor of the 49th annual Celebrate Israel Parade.

The building also has been lit to commemorate the deaths of notable personalities. After the eightieth birthday, and subsequent death, of Frank Sinatra in 1998, for example, the building was bathed in blue light to represent the singer's nickname "Ol' Blue Eyes". After actress Fay Wray, who starred in "King Kong", died in September 2004, the building lights were extinguished for 15 minutes. Following retired basketball player Kobe Bryant's January 2020 death, the building was lit in purple and gold, this time signifying the colors of his former team, the Los Angeles Lakers.
In 2012, the building's four hundred metal halide lamps and floodlights were replaced with 1,200 LED fixtures, increasing the available colors from nine to over 16 million. The computer-controlled system allows the building to be illuminated in ways that were unable to be done previously with plastic gels. For instance, on November 6, 2012, CNN used the top of the Empire State Building as a scoreboard for the 2012 United States presidential election. When incumbent president Barack Obama had reached the 270 electoral votes necessary to win re-election, the lights turned blue, representing the color of Obama's Democratic Party. Had Republican challenger Mitt Romney won, the building would have been lit red, the color of the Republican Party. Also, on November 26, 2012, the building had its first synchronized light show, using music from recording artist Alicia Keys. Artists such as Eminem and OneRepublic have been featured in later shows, including the building's annual Holiday Music-to-Lights Show. The building's owners adhere to strict standards in using the lights; for instance, they do not use the lights to play advertisements.

The longest world record held by the Empire State Building was for the tallest skyscraper (to structural height), which it held for 42 years until it was surpassed by the North Tower of the World Trade Center in October 1970. The Empire State Building was also the tallest man-made structure in the world before it was surpassed by the Griffin Television Tower Oklahoma (KWTV Mast) in 1954, and the tallest freestanding structure in the world until the completion of the Ostankino Tower in 1967. An early-1970s proposal to dismantle the spire and replace it with an additional 11 floors, which would have brought the building's height to 1,494 feet (455 m) and made it once again the world's tallest at the time, was considered but ultimately rejected.

With the destruction of the World Trade Center in the September 11 attacks, the Empire State Building again became the tallest building in New York City, and the second-tallest building in the Americas, surpassed only by the Willis Tower in Chicago. The Empire State Building remained the tallest building in New York until the new One World Trade Center reached a greater height in April 2012. , it is the fourth-tallest building in New York City after One World Trade Center, 432 Park Avenue, and 30 Hudson Yards. It is the fifth-tallest completed skyscraper in the United States behind the two other tallest buildings in New York City, as well as the Willis Tower and Trump International Hotel and Tower in Chicago. The Empire State Building is the 28th-tallest in the world , the tallest being Burj Khalifa in Dubai. It is also the sixth-tallest freestanding structure in the Americas behind the five tallest buildings and the CN Tower.

, the building houses around 1,000 businesses. Current tenants include:
Former tenants include:

At 9:40 am on July 28, 1945, a B-25 Mitchell bomber, piloted in thick fog by Lieutenant Colonel William Franklin Smith Jr., crashed into the north side of the Empire State Building between the 79th and 80th floors where the offices of the National Catholic Welfare Council were located. One engine completely penetrated the building, landing on the roof of a nearby building where it started a fire that destroyed a penthouse. The other engine and part of the landing gear plummeted down an elevator shaft causing a fire, which was extinguished in 40 minutes. Fourteen people were killed in the incident. Elevator operator Betty Lou Oliver survived a plunge of 75 stories inside an elevator, which still stands as the Guinness World Record for the longest survived elevator fall recorded.

Despite the damage and loss of life, the building was open for business on many floors two days later. The crash helped spur the passage of the long-pending Federal Tort Claims Act of 1946, as well as the insertion of retroactive provisions into the law, allowing people to sue the government for the incident. Also as a result of the crash, the Civil Aeronautics Administration enacted strict regulations regarding flying over New York City, setting a minimum flying altitude of above sea level regardless of the weather conditions.

A year later, on July 24, 1946, another aircraft narrowly missed striking the building. The unidentified twin-engine plane scraped past the observation deck, scaring the tourists there.

On January 24, 2000, an elevator in the building suddenly descended 40 stories after a cable that controlled the cabin's maximum speed was severed. The elevator fell from the 44th floor to the fourth floor, where a narrowed elevator shaft provided a second safety system. Despite the 40-floor fall, both of the passengers in the cabin at the time were only slightly injured. Since that elevator had no fourth-floor doors, the passengers were rescued by an adjacent elevator. After the fall, building inspectors reviewed all of the building's elevators.

Because of the building's iconic status, it and other Midtown landmarks are popular locations for suicide attempts. More than 30 people have attempted suicide over the years by jumping from the upper parts of the building, with most attempts being successful.

The first suicide from the building occurred on April 7, 1931, before the tower was even completed, when a carpenter who had been laid-off went to the 58th floor and jumped. The first suicide after the building's opening occurred from the 86th floor observatory in February 1935, when Irma P. Eberhardt fell onto a marquee sign. On December 16, 1943, William Lloyd Rambo jumped to his death from the 86th floor, landing amidst Christmas shoppers on the street below. In the early morning of September 27, 1946, shell-shocked Marine Douglas W. Brashear Jr. jumped from the 76th-floor window of the Grant Advertising Agency; police found his shoes from his body.

On May 1, 1947, Evelyn McHale leapt to her death from the 86th floor observation deck and landed on a limousine parked at the curb. Photography student Robert Wiles took a photo of McHale's oddly intact corpse a few minutes after her death. The police found a suicide note among possessions that she left on the observation deck: "He is much better off without me... I wouldn't make a good wife for anybody". The photo ran in the May 12, 1947 edition of "Life" magazine and is often referred to as "The Most Beautiful Suicide". It was later used by visual artist Andy Warhol in one of his prints entitled "Suicide (Fallen Body)". A mesh fence was put up around the 86th floor terrace in December 1947 after five people tried to jump during a three-week span in October and November of that year. By then, sixteen people had died from suicide jumps.

Only one person has jumped from the upper observatory. Frederick Eckert of Astoria ran past a guard in the enclosed 102nd floor gallery on November 3, 1932, and jumped a gate leading to an outdoor catwalk intended for dirigible passengers. He landed and died on the roof of the 86th floor observation promenade.

Two people have survived falls by not falling more than a floor. On December 2, 1979, Elvita Adams jumped from the 86th floor, only to be blown back onto a ledge on the 85th floor by a gust of wind and left with a broken hip. On April 25, 2013, a man fell from the 86th floor observation deck, but he landed alive with minor injuries on an 85th-floor ledge where security guards brought him inside and paramedics transferred him to a hospital for a psychiatric evaluation.

Two fatal shootings have occurred in the direct vicinity of the Empire State Building. Abu Kamal, a 69-year-old Palestinian teacher, shot seven people on the 86th floor observation deck during the afternoon of February 23, 1997. He killed one person and wounded six others before committing suicide. Kamal reportedly committed the shooting in response to events happening in Palestine and Israel.

On the morning of August 24, 2012, 58-year-old Jeffrey T. Johnson shot and killed a former co-worker on the building's Fifth Avenue sidewalk. He had been laid off from his job in 2011. Two police officers confronted the gunman, and he aimed his firearm at them. They responded by firing 16 shots, killing him but also wounding nine bystanders. Most of the injured were hit by bullet fragments, although three took direct hits from bullets.

As the tallest building in the world and the first one to exceed 100 floors, the Empire State Building immediately became an icon of the city and of the nation. In 2013, "Time" magazine noted that the Empire State Building "seems to completely embody the city it has become synonymous with". The historian John Tauranac calls the tower "'the' twentieth-century New York building", despite the existence of taller and more modernist buildings. Early in the building's history, travel companies such as Short Line Motor Coach Service and New York Central Railroad used the building as an icon to symbolize the city. After the construction of the first World Trade Center, architect Paul Goldberger noted that the Empire State Building "is famous for being tall, but it is good enough to be famous for being good."

As an icon of the United States, it is also very popular among Americans. In a 2007 survey, the American Institute of Architects found that the Empire State Building was "America's favorite building". The building was originally a symbol of hope in a country devastated by the Depression, as well as a work of accomplishment by newer immigrants. The writer Benjamin Flowers states that the Empire State was "a building intended to celebrate a new America, built by men (both clients and construction workers) who were themselves new Americans." The architectural critic Jonathan Glancey refers to the building as an "icon of American design".

The Empire State Building has been hailed as an example of a "wonder of the world" due to the massive effort expended during construction. "The Washington Star" listed it as part of one of the "seven wonders of the modern world" in 1931, while "Holiday" magazine wrote in 1958 that the Empire State's height would be taller than the combined heights of the Eiffel Tower and the Great Pyramid of Giza. The American Society of Civil Engineers also declared the building "A Modern Civil Engineering Wonder of the United States" in 1958, and one of the Seven Wonders of the Modern World in 1994. Ron Miller, in a 2010 book, also described the Empire State Building as one of the "seven wonders of engineering". It has often been called the Eighth Wonder of the World as well, an appellation that it has held since shortly after opening. The panels installed in the lobby in 1963 reflected this, showing the seven original wonders alongside the Empire State Building.

As an icon of New York City, the Empire State Building has been featured in various films, books, TV shows, and video games. According to the building's official website, more than 250 movies contain depictions of the Empire State Building. In his book about the building, John Tauranac writes that the first documented appearance of the tower in popular culture was "Swiss Family Manhattan", a 1932 children's story by Christopher Morley. A year later, the film "King Kong" depicted Kong, a large stop motion ape that climbs the Empire State Building, bringing the building into the popular imagination. Later movies such as "An Affair to Remember" (1957), "Sleepless in Seattle" (1993), and "Independence Day" (1996) also featured the building. The building has also been featured in other works, such as "Daleks in Manhattan", a 2007 episode of the TV series "Doctor Who"; and "Empire", an eight-hour black-and-white silent film by Andy Warhol, which was later added to the Library of Congress's National Film Registry.

The Empire State Building Run-Up, a foot race from ground level to the 86th-floor observation deck, has been held annually since 1978. Its participants are referred to both as runners and as climbers, and are often tower running enthusiasts. The race covers a vertical distance of and takes in 1,576 steps. The record time is 9 minutes and 33 seconds, achieved by Australian professional cyclist Paul Crake in 2003, at a climbing rate of per hour.





</doc>
<doc id="9737" url="https://en.wikipedia.org/wiki?curid=9737" title="Eugenics">
Eugenics

Eugenics (; from Greek εὐ- "good" and γενής "come into being, growing") is a set of beliefs and practices that aim to improve the genetic quality of a human population, historically by excluding people and groups judged to be inferior and promoting those judged to be superior.

The concept predates the term; Plato suggested applying the principles of selective breeding to humans around 400 BC. Early advocates of eugenics in the 19th century regarded it as a way of improving groups of people. In contemporary usage, the term "eugenics" is closely associated with scientific racism and white supremacy. Modern bioethicists who advocate new eugenics characterise it as a way of enhancing individual traits, regardless of group membership.

While eugenic principles have been practiced as early as ancient Greece, the contemporary history of eugenics began in the early 20th century, when a popular eugenics movement emerged in the United Kingdom, and then spread to many countries, including the United States, Canada, and most European countries. In this period, people from across the political spectrum espoused eugenic ideas. Consequently, many countries adopted eugenic policies, intended to improve the quality of their populations' genetic stock. Such programs included both "positive" measures, such as encouraging individuals deemed particularly "fit" to reproduce, and "negative" measures, such as marriage prohibitions and forced sterilization of people deemed unfit for reproduction. Those deemed "unfit to reproduce" often included people with mental or physical disabilities, people who scored in the low ranges on different IQ tests, criminals and "deviants," and members of disfavored minority groups.

The eugenics movement became associated with Nazi Germany and the Holocaust when the defense of many of the defendants at the Nuremberg trials of 1945 to 1946 attempted to justify their human-rights abuses by claiming there was little difference between the Nazi eugenics programs and the U.S. eugenics programs. In the decades following World War II, with more emphasis on human rights, many countries began to abandon eugenics policies, although some Western countries (the United States, Canada, and Sweden among them) continued to carry out forced sterilizations.

Since the 1980s and 1990s, with new assisted reproductive technology procedures available, such as gestational surrogacy (available since 1985), preimplantation genetic diagnosis (available since 1989), and cytoplasmic transfer (first performed in 1996), concern has grown about the possible revival of a more potent form of eugenics after decades of promoting human rights.

A criticism of eugenics policies is that, regardless of whether "negative" or "positive" policies are used, they are susceptible to abuse because the genetic selection criteria are determined by whichever group has political power at the time. Furthermore, many criticize "negative eugenics" in particular as a violation of basic human rights, seen since 1968's Proclamation of Tehran as including the right to reproduce. Another criticism is that eugenics policies eventually lead to a loss of genetic diversity, thereby resulting in inbreeding depression due to a loss of genetic variation. Yet another criticism of contemporary eugenics policies is that they propose to permanently and artificially disrupt millions of years of evolution, and that attempting to create genetic lines "clean" of "disorders" can have far-reaching ancillary downstream effects in the genetic ecology, including negative effects on immunity and on species resilience.

The concept of positive eugenics to produce better human beings has existed at least since Plato suggested selective mating to produce a guardian class. In Sparta, every Spartan child was inspected by the council of elders, the Gerousia, which determined if the child was fit to live or not. In the early years of the Roman Republic, a Roman father was obliged by law to immediately kill his child if they were "dreadfully deformed". According to Tacitus, a Roman of the Imperial Period, the Germanic tribes of his day killed any member of their community they deemed cowardly, unwarlike or "stained with abominable vices", usually by drowning them in swamps. Modern historians, however, see Tacitus' ethnographic writing as unreliable in such details.

The idea of a modern project for improving the human population through selective breeding was originally developed by Francis Galton, and was initially inspired by Darwinism and its theory of natural selection. Galton had read his half-cousin Charles Darwin's theory of evolution, which sought to explain the development of plant and animal species, and desired to apply it to humans. Based on his biographical studies, Galton believed that desirable human qualities were hereditary traits, although Darwin strongly disagreed with this elaboration of his theory. In 1883, one year after Darwin's death, Galton gave his research a name: "eugenics". With the introduction of genetics, eugenics became associated with genetic determinism, the belief that human character is entirely or in the majority caused by genes, unaffected by education or living conditions. Many of the early geneticists were not Darwinians, and evolution theory was not needed for eugenics policies based on genetic determinism. Throughout its recent history, eugenics has remained controversial.

Eugenics became an academic discipline at many colleges and universities and received funding from many sources. Organizations were formed to win public support and sway opinion towards responsible eugenic values in parenthood, including the British Eugenics Education Society of 1907 and the American Eugenics Society of 1921. Both sought support from leading clergymen and modified their message to meet religious ideals. In 1909, the Anglican clergymen William Inge and James Peile both wrote for the British Eugenics Education Society. Inge was an invited speaker at the 1921 International Eugenics Conference, which was also endorsed by the Roman Catholic Archbishop of New York Patrick Joseph Hayes. The book "The Passing of the Great Race" ("Or, The Racial Basis of European History") by American eugenicist, lawyer, and amateur anthropologist Madison Grant was published in 1916. Though influential, the book was largely ignored when it first appeared, and it went through several revisions and editions. Nevertheless, the book was used by people who advocated restricted immigration as justification for what became known as “scientific racism”.Three International Eugenics Conferences presented a global venue for eugenists with meetings in 1912 in London, and in 1921 and 1932 in New York City. Eugenic policies were first implemented in the early 1900s in the United States. It also took root in France, Germany, and Great Britain. Later, in the 1920s and 1930s, the eugenic policy of sterilizing certain mental patients was implemented in other countries including Belgium, Brazil, Canada, Japan and Sweden. Frederick Osborn's 1937 journal article "Development of a Eugenic Philosophy" framed it as a social philosophy—a philosophy with implications for social order. That definition is not universally accepted. Osborn advocated for higher rates of sexual reproduction among people with desired traits ("positive eugenics") or reduced rates of sexual reproduction or sterilization of people with less-desired or undesired traits ("negative eugenics").

In addition to being practiced in a number of countries, eugenics was internationally organized through the International Federation of Eugenics Organizations. Its scientific aspects were carried on through research bodies such as the Kaiser Wilhelm Institute of Anthropology, Human Heredity, and Eugenics, the Cold Spring Harbor Carnegie Institution for Experimental Evolution, and the Eugenics Record Office. Politically, the movement advocated measures such as sterilization laws. In its moral dimension, eugenics rejected the doctrine that all human beings are born equal and redefined moral worth purely in terms of genetic fitness. Its racist elements included pursuit of a pure "Nordic race" or "Aryan" genetic pool and the eventual elimination of "unfit" races. Many leading British politicians subscribed to the theories of eugenics. Winston Churchill supported the British Eugenics Society and was an honorary vice president for the organization. Churchill believed that eugenics could solve "race deterioration" and reduce crime and poverty.

Early critics of the philosophy of eugenics included the American sociologist Lester Frank Ward, the English writer G. K. Chesterton, the German-American anthropologist Franz Boas, who argued that advocates of eugenics greatly over-estimate the influence of biology, and Scottish tuberculosis pioneer and author Halliday Sutherland. Ward's 1913 article "Eugenics, Euthenics, and Eudemics", Chesterton's 1917 book "", and Boas' 1916 article "" (published in "The Scientific Monthly") were all harshly critical of the rapidly growing movement. Sutherland identified eugenists as a major obstacle to the eradication and cure of tuberculosis in his 1917 address "Consumption: Its Cause and Cure", and criticism of eugenists and Neo-Malthusians in his 1921 book "Birth Control" led to a writ for libel from the eugenist Marie Stopes. Several biologists were also antagonistic to the eugenics movement, including Lancelot Hogben. Other biologists such as J. B. S. Haldane and R. A. Fisher expressed skepticism in the belief that sterilization of "defectives" would lead to the disappearance of undesirable genetic traits.

Among institutions, the Catholic Church was an opponent of state-enforced sterilizations. Attempts by the Eugenics Education Society to persuade the British government to legalize voluntary sterilization were opposed by Catholics and by the Labour Party. The American Eugenics Society initially gained some Catholic supporters, but Catholic support declined following the 1930 papal encyclical "Casti connubii". In this, Pope Pius XI explicitly condemned sterilization laws: "Public magistrates have no direct power over the bodies of their subjects; therefore, where no crime has taken place and there is no cause present for grave punishment, they can never directly harm, or tamper with the integrity of the body, either for the reasons of eugenics or for any other reason."

As a social movement, eugenics reached its greatest popularity in the early decades of the 20th century, when it was practiced around the world and promoted by governments, institutions, and influential individuals. Many countries enacted various eugenics policies, including: genetic screenings, birth control, promoting differential birth rates, marriage restrictions, segregation (both racial segregation and sequestering the mentally ill), compulsory sterilization, forced abortions or forced pregnancies, ultimately culminating in genocide. By 2014, gene selection (rather than "people selection") was made possible through advances in genome editing, leading to what is sometimes called "new eugenics", also known as "neo-eugenics", "consumer eugenics", or "liberal eugenics".

Anti-miscegenation laws in the United States made it a crime for individuals to marry someone categorised as belonging to a different race. These laws were part of a broader policy of Racial segregation in the United States to minimise contact between people of different ethnicities. Race laws and practices in the United States were explicitly used as models by the Nazi regime when it developed the Nuremberg Laws, stripping Jewish citizens of their citizenship.

The scientific reputation of eugenics started to decline in the 1930s, a time when Ernst Rüdin used eugenics as a justification for the racial policies of Nazi Germany. Adolf Hitler had praised and incorporated eugenic ideas in "Mein Kampf" in 1925 and emulated eugenic legislation for the sterilization of "defectives" that had been pioneered in the United States once he took power. Some common early 20th century eugenics methods involved identifying and classifying individuals and their families, including the poor, mentally ill, blind, deaf, developmentally disabled, promiscuous women, homosexuals, and racial groups (such as the Roma and Jews in Nazi Germany) as "degenerate" or "unfit", and therefore led to segregation, institutionalization, sterilization, euthanasia, and even mass murder. The Nazi practice of euthanasia was carried out on hospital patients in the Aktion T4 centers such as Hartheim Castle.

By the end of World War II, many eugenics laws were abandoned, having become associated with Nazi Germany. H. G. Wells, who had called for "the sterilization of failures" in 1904, stated in his 1940 book "The Rights of Man: Or What Are We Fighting For?" that among the human rights, which he believed should be available to all people, was "a prohibition on mutilation, sterilization, torture, and any bodily punishment". After World War II, the practice of "imposing measures intended to prevent births within [a national, ethnical, racial or religious] group" fell within the definition of the new international crime of genocide, set out in the Convention on the Prevention and Punishment of the Crime of Genocide. The Charter of Fundamental Rights of the European Union also proclaims "the prohibition of eugenic practices, in particular those aiming at selection of persons". In spite of the decline in discriminatory eugenics laws, some government mandated sterilizations continued into the 21st century. During the ten years President Alberto Fujimori led Peru from 1990 to 2000, 2,000 persons were allegedly involuntarily sterilized. China maintained its one-child policy until 2015 as well as a suite of other eugenics based legislation to reduce population size and manage fertility rates of different populations. In 2007, the United Nations reported coercive sterilizations and hysterectomies in Uzbekistan. During the years 2005 to 2013, nearly one-third of the 144 California prison inmates who were sterilized did not give lawful consent to the operation.

Developments in genetic, genomic, and reproductive technologies at the beginning of the 21st century have raised numerous questions regarding the ethical status of eugenics, effectively creating a resurgence of interest in the subject. Some, such as UC Berkeley sociologist Troy Duster, have claimed that modern genetics is a back door to eugenics. This view was shared by then-White House Assistant Director for Forensic Sciences, Tania Simoncelli, who stated in a 2003 publication by the Population and Development Program at Hampshire College that advances in pre-implantation genetic diagnosis (PGD) are moving society to a "new era of eugenics", and that, unlike the Nazi eugenics, modern eugenics is consumer driven and market based, "where children are increasingly regarded as made-to-order consumer products". In a 2006 newspaper article, Richard Dawkins said that discussion regarding eugenics was inhibited by the shadow of Nazi misuse, to the extent that some scientists would not admit that breeding humans for certain abilities is at all possible. He believes that it is not physically different from breeding domestic animals for traits such as speed or herding skill. Dawkins felt that enough time had elapsed to at least ask just what the ethical differences were between breeding for ability versus training athletes or forcing children to take music lessons, though he could think of persuasive reasons to draw the distinction.

Lee Kuan Yew, the so-called "Founding Father" of Singapore, started promoting eugenics as early as 1983.

In October 2015, the United Nations' International Bioethics Committee wrote that the ethical problems of human genetic engineering should not be confused with the ethical problems of the 20th century eugenics movements. However, it is still problematic because it challenges the idea of human equality and opens up new forms of discrimination and stigmatization for those who do not want, or cannot afford, the technology.

Transhumanism is often associated with eugenics, although most transhumanists holding similar views nonetheless distance themselves from the term "eugenics" (preferring "germinal choice" or "reprogenetics") to avoid having their position confused with the discredited theories and practices of early-20th-century eugenic movements.

Prenatal screening can be considered a form of contemporary eugenics because it may lead to abortions of children with undesirable traits. A system was proposed by California Senator Skinner to compensate victims of the well-documented examples of prison sterilizations resulting from California's eugenics programs, but this did not pass by the bill's 2018 deadline in the Legislature.

The term "eugenics" and its modern field of study were first formulated by Francis Galton in 1883, drawing on the recent work of his half-cousin Charles Darwin. Galton published his observations and conclusions in his book "Inquiries into Human Faculty and Its Development".

The origins of the concept began with certain interpretations of Mendelian inheritance and the theories of August Weismann. The word "eugenics" is derived from the Greek word "eu" ("good" or "well") and the suffix "-genēs" ("born"); Galton intended it to replace the word "stirpiculture", which he had used previously but which had come to be mocked due to its perceived sexual overtones. Galton defined eugenics as "the study of all agencies under human control which can improve or impair the racial quality of future generations".

Historically, the term "eugenics" has referred to everything from prenatal care for mothers to forced sterilization and euthanasia. To population geneticists, the term has included the avoidance of inbreeding without altering allele frequencies; for example, J. B. S. Haldane wrote that "the motor bus, by breaking up inbred village communities, was a powerful eugenic agent." Debate as to what exactly counts as eugenics continues today.

Edwin Black, journalist and author of "War Against the Weak", claims eugenics is often deemed a pseudoscience because what is defined as a genetic improvement of a desired trait is often deemed a cultural choice rather than a matter that can be determined through objective scientific inquiry. The most disputed aspect of eugenics has been the definition of "improvement" of the human gene pool, such as what is a beneficial characteristic and what is a defect. Historically, this aspect of eugenics was tainted with scientific racism and pseudoscience.

Early eugenicists were mostly concerned with factors of perceived intelligence that often correlated strongly with social class. These included Karl Pearson and Walter Weldon, who worked on this at the University College London. In his lecture "Darwinism, Medical Progress and Eugenics", Pearson claimed that everything concerning eugenics fell into the field of medicine.

Eugenic policies have been conceptually divided into two categories. Positive eugenics is aimed at encouraging reproduction among the genetically advantaged; for example, the reproduction of the intelligent, the healthy, and the successful. Possible approaches include financial and political stimuli, targeted demographic analyses, "in vitro" fertilization, egg transplants, and cloning. Negative eugenics aimed to eliminate, through sterilization or segregation, those deemed physically, mentally, or morally "undesirable". This includes abortions, sterilization, and other methods of family planning. Both positive and negative eugenics can be coercive; in Nazi Germany, for example, abortion was illegal for women deemed by the state to be fit.

The first major challenge to conventional eugenics based on genetic inheritance was made in 1915 by Thomas Hunt Morgan. He demonstrated the event of genetic mutation occurring outside of inheritance involving the discovery of the hatching of a fruit fly ("Drosophila melanogaster") with white eyes from a family with red eyes, demonstrating that major genetic changes occurred outside of inheritance. Additionally, Morgan criticized the view that certain traits, such as intelligence and criminality, were hereditary because these traits were subjective. Despite Morgan's public rejection of eugenics, much of his genetic research was adopted by proponents of eugenics.

The heterozygote test is used for the early detection of recessive hereditary diseases, allowing for couples to determine if they are at risk of passing genetic defects to a future child. The goal of the test is to estimate the likelihood of passing the hereditary disease to future descendants.

Recessive traits can be severely reduced, but never eliminated unless the complete genetic makeup of all members of the pool was known, as aforementioned. As only very few undesirable traits, such as Huntington's disease, are dominant, it could be argued from certain perspectives that the practicality of "eliminating" traits is quite low.

There are examples of eugenic acts that managed to lower the prevalence of recessive diseases, although not influencing the prevalence of heterozygote carriers of those diseases. The elevated prevalence of certain genetically transmitted diseases among the Ashkenazi Jewish population (Tay–Sachs, cystic fibrosis, Canavan's disease, and Gaucher's disease), has been decreased in current populations by the application of genetic screening.

Pleiotropy occurs when one gene influences multiple, seemingly unrelated phenotypic traits, an example being phenylketonuria, which is a human disease that affects multiple systems but is caused by one gene defect. Andrzej Pękalski, from the University of Wrocław, argues that eugenics can cause harmful loss of genetic diversity if a eugenics program selects a pleiotropic gene that could possibly be associated with a positive trait. Pekalski uses the example of a coercive government eugenics program that prohibits people with myopia from breeding but has the unintended consequence of also selecting against high intelligence since the two go together.

Eugenic policies may lead to a loss of genetic diversity. Further, a culturally-accepted "improvement" of the gene pool may result in extinction, due to increased vulnerability to disease, reduced ability to adapt to environmental change, and other factors that may not be anticipated in advance. This has been evidenced in numerous instances, in isolated island populations. A long-term, species-wide eugenics plan might lead to such a scenario because the elimination of traits deemed undesirable would reduce genetic diversity by definition.

While the science of genetics has increasingly provided means by which certain characteristics and conditions can be identified and understood, given the complexity of human genetics, culture, and psychology, at this point there is no agreed objective means of determining which traits might be ultimately desirable or undesirable. Some conditions such as sickle-cell disease and cystic fibrosis respectively confer immunity to malaria and resistance to cholera when a single copy of the recessive allele is contained within the genotype of the individual, so eliminating these genes is undesirable in places where such diseases are common.

Societal and political consequences of eugenics call for a place in the discussion on the ethics behind the eugenics movement. Many of the ethical concerns regarding eugenics arise from its controversial past, prompting a discussion on what place, if any, it should have in the future. Advances in science have changed eugenics. In the past, eugenics had more to do with sterilization and enforced reproduction laws. Now, in the age of a progressively mapped genome, embryos can be tested for susceptibility to disease, gender, and genetic defects, and alternative methods of reproduction such as in vitro fertilization are becoming more common. Therefore, eugenics is no longer "ex post facto" regulation of the living but instead preemptive action on the unborn.

With this change, however, there are ethical concerns which lack adequate attention, and which must be addressed before eugenic policies can be properly implemented in the future. Sterilized individuals, for example, could volunteer for the procedure, albeit under incentive or duress, or at least voice their opinion. The unborn fetus on which these new eugenic procedures are performed cannot speak out, as the fetus lacks the voice to consent or to express his or her opinion. Philosophers disagree about the proper framework for reasoning about such actions, which change the very identity and existence of future persons.

Some have described potential "eugenics wars" as the worst-case outcome of eugenics. This scenario would mean the return of coercive state-sponsored genetic discrimination and human rights violations such as compulsory sterilization of persons with genetic defects, the killing of the institutionalized and, specifically, segregation and genocide of races perceived as inferior. Health law professor George Annas and technology law professor Lori Andrews are prominent advocates of the position that the use of these technologies could lead to such human-posthuman caste warfare.

In his 2003 book "Enough: Staying Human in an Engineered Age", environmental ethicist Bill McKibben argued at length against germinal choice technology and other advanced biotechnological strategies for human enhancement. He writes that it would be morally wrong for humans to tamper with fundamental aspects of themselves (or their children) in an attempt to overcome universal human limitations, such as vulnerability to aging, maximum life span and biological constraints on physical and cognitive ability. Attempts to "improve" themselves through such manipulation would remove limitations that provide a necessary context for the experience of meaningful human choice. He claims that human lives would no longer seem meaningful in a world where such limitations could be overcome with technology. Even the goal of using germinal choice technology for clearly therapeutic purposes should be relinquished, since it would inevitably produce temptations to tamper with such things as cognitive capacities. He argues that it is possible for societies to benefit from renouncing particular technologies, using as examples Ming China, Tokugawa Japan and the contemporary Amish.

Some, for example Nathaniel C. Comfort from Johns Hopkins University, claim that the change from state-led reproductive-genetic decision-making to individual choice has moderated the worst abuses of eugenics by transferring the decision-making from the state to the patient and their family. Comfort suggests that "the eugenic impulse drives us to eliminate disease, live longer and healthier, with greater intelligence, and a better adjustment to the conditions of society; and the health benefits, the intellectual thrill and the profits of genetic bio-medicine are too great for us to do otherwise." Others, such as bioethicist Stephen Wilkinson of Keele University and Honorary Research Fellow Eve Garrard at the University of Manchester, claim that some aspects of modern genetics can be classified as eugenics, but that this classification does not inherently make modern genetics immoral.

In their book published in 2000, "From Chance to Choice: Genetics and Justice", bioethicists Allen Buchanan, Dan Brock, Norman Daniels and Daniel Wikler argued that liberal societies have an obligation to encourage as wide an adoption of eugenic enhancement technologies as possible (so long as such policies do not infringe on individuals' reproductive rights or exert undue pressures on prospective parents to use these technologies) in order to maximize public health and minimize the inequalities that may result from both natural genetic endowments and unequal access to genetic enhancements.

In his book "A Theory of Justice" (1971), American philosopher John Rawls argued that "Over time a society is to take steps to preserve the general level of natural abilities and to prevent the diffusion of serious defects". The Original position, a hypothetical situation developed by Rawls, has been used as an argument for "negative eugenics".

The film "Gattaca" (1997) provides a fictional example of a dystopian society that uses eugenics to decide what people are capable of and their place in the world. Although critically acclaimed, "Gattaca" was not a box office success, but it is said to have crystallized the debate over the controversial topic of human genetic engineering. The film's dystopian depiction of "genoism" has been cited by many bioethicists and laypeople in support of their hesitancy about, or opposition to, eugenics and the societal acceptance of the genetic-determinist ideology that may frame it. In a 1997 review of the film for the journal "Nature Genetics", molecular biologist Lee M. Silver stated that ""Gattaca" is a film that all geneticists should see if for no other reason than to understand the perception of our trade held by so many of the public-at-large".

Notes






</doc>
<doc id="9738" url="https://en.wikipedia.org/wiki?curid=9738" title="Email">
Email

Electronic mail (email or e-mail) is a method of exchanging messages ("mail") between people using electronic devices. Email entered limited use in the 1960s, but users could only send to users of the same computer, and some early email systems required the author and the recipient to both be online simultaneously, similar to instant messaging. Ray Tomlinson is credited as the inventor of email; in 1971, he developed the first system able to send mail between users on different hosts across the ARPANET, using the @ sign to link the user name with a destination server. By the mid-1970s, this was the form recognized as email.

Email operates across computer networks, primarily the Internet. Today's email systems are based on a store-and-forward model. Email servers accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect, typically to a mail server or a webmail interface to send or receive messages or download it.

Originally an ASCII text-only communications medium, Internet email was extended by Multipurpose Internet Mail Extensions (MIME) to carry text in other character sets and multimedia content attachments. International email, with internationalized email addresses using UTF-8, is standardized but not widely adopted.

The history of modern Internet email services reaches back to the early ARPANET, with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s is similar to a basic email sent today. 

Historically, the term "electronic mail" is any electronic document transmission. For example, several writers in the early 1970s used the term to refer to fax document transmission. As a result, finding its first use is difficult with the specific meaning it has today.

The term "electronic mail" has been in use with its current meaning since at least 1975, and variations of the shorter "E-mail" have been in use since at least 1979:


In the original protocol, "RFC 524", none of these forms was used. The service is simply referred to as "mail", and a single piece of electronic mail is called a "message".

An Internet e-mail consists of an envelope and content; the content consists of a header and a body.

Computer-based mail and messaging became possible with the advent of time-sharing computers in the early 1960s, and informal methods of using shared files to pass messages were soon expanded into the first mail systems. Most developers of early mainframes and minicomputers developed similar, but generally incompatible, mail applications. Over time, a complex web of gateways and routing systems linked many of them. Many US universities were part of the ARPANET (created in the late 1960s), which aimed at software portability between its systems. In 1971 the first ARPANET network email was sent, introducing the now-familiar address syntax with the '@' symbol designating the user's system address. The Simple Mail Transfer Protocol (SMTP) protocol was introduced in 1981.

For a time in the late 1980s and early 1990s, it seemed likely that either a proprietary commercial system or the X.400 email system, part of the Government Open Systems Interconnection Profile (GOSIP), would predominate. However, once the final restrictions on carrying commercial traffic over the Internet ended in 1995, a combination of factors made the current Internet suite of SMTP, POP3 and IMAP email protocols the standard.

The following is a typical sequence of events that takes place when sender Alice transmits a message using a mail user agent (MUA) addressed to the email address of the recipient.

In addition to this example, alternatives and complications exist in the email system:

Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called "open mail relays". This was very important in the early days of the Internet when network connections were unreliable. However, this mechanism proved to be exploitable by originators of unsolicited bulk email and as a consequence open mail relays have become rare, and many MTAs do not accept messages from open mail relays.

The basic Internet message format used for email is defined by RFC 5322, with encoding of non-ASCII data and multimedia content attachments defined in RFC 2045 through RFC 2049, collectively called "Multipurpose Internet Mail Extensions" or "MIME". The extensions in International email apply only to email. RFC 5322 replaced the earlier RFC 2822 in 2008, then RFC 2822 in 2001 replaced RFC 822 – the standard for Internet email for decades. Published in 1982, RFC 822 was based on the earlier RFC 733 for the ARPANET.

Internet email messages consist of two sections, 'header' and 'body'. These are known as 'content'. 
The header is structured into fields such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a signature block at the end. The header is separated from the body by a blank line.

RFC 5322 specifies the syntax of the email header. Each email message has a header (the "header section" of the message, according to the specification), comprising a number of fields ("header fields"). Each field has a name ("field name" or "header field name"), followed by the separator character ":", and a value ("field body" or "header field body").

Each field name begins in the first character of a new line in the header section, and begins with a non-whitespace printable character. It ends with the separator character ":". The separator follows the field value (the "field body"). The value can continue onto subsequent lines if those lines have space or tab as their first character. Field names and, without SMTPUTF8, field bodies are restricted to 7-bit ASCII characters. Some non-ASCII values may be represented using MIME encoded words.

Email header fields can be multi-line, with each line recommended to be no more than 78 characters, although the limit is 998 characters. Header fields defined by RFC 5322 contain only US-ASCII characters; for encoding characters in other sets, a syntax specified in RFC 2047 may be used. In some examples, the IETF EAI working group defines some standards track extensions, replacing previous experimental extensions so UTF-8 encoded Unicode characters may be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some government agents.

The message header must include at least the following fields:

RFC 3864 describes registration procedures for message header fields at the IANA; it provides for permanent and provisional field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:


The "To:" field may be unrelated to the addresses to which the message is delivered. The delivery list is supplied separately to the transport protocol, SMTP, which may be extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter delivered according to the address on the outer envelope. In the same way, the "From:" field may not be the sender. Some mail servers apply email authentication systems to messages relayed. Data pertaining to the server's activity is also part of the header, as defined below.

SMTP defines the "trace information" of a message saved in the header using the following two fields:

Other fields added on top of the header by the receiving server may be called "trace fields".

Internet email was designed for 7-bit ASCII. Most email software is 8-bit clean, but must assume it will communicate with 7-bit servers and mail readers. The MIME standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: quoted printable for mostly 7-bit content with a few characters outside that range and base64 for arbitrary binary data. The 8BITMIME and BINARY extensions were introduced to allow transmission of mail without the need for these encodings, but many mail transport agents may not support them. In some countries, several encoding schemes co-exist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is a coincidence if the sender and receiver use the same encoding scheme). Therefore, for international character sets, Unicode is growing in popularity.

Most modern graphic email clients allow the use of either plain text or HTML for the message body at the option of the user. HTML email messages often include an automatic-generated plain text copy for compatibility. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in block quotes, wrap naturally on any display, use emphasis such as underlines and italics, and change font styles. Disadvantages include the increased size of the email, privacy concerns about web bugs, abuse of HTML email as a vector for phishing attacks and the spread of malicious software.

Some web-based mailing lists recommend all posts be made in plain-text, with 72 or 80 characters per line for all the above reasons, and because they have a significant number of readers using text-based email clients such as Mutt. Some Microsoft email clients may allow rich formatting using their proprietary Rich Text Format (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible email client.

Messages are exchanged between hosts using the Simple Mail Transfer Protocol with software programs called mail transfer agents (MTAs); and delivered to a mail store by programs called mail delivery agents (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it, and when a message cannot be delivered, that MTA must send a bounce message back to the sender, indicating the problem.

Users can retrieve their messages from servers using standard protocols such as POP or IMAP, or, as is more likely in a large corporate environment, with a proprietary protocol specific to Novell Groupwise, Lotus Notes or Microsoft Exchange Servers. Programs used by users for retrieving, reading, and managing email are called mail user agents (MUAs).

Mail can be stored on the client, on the server side, or in both places. Standard formats for mailboxes include Maildir and mbox. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as IMAP, moving email from one server to another can be done with any MUA supporting the protocol.

Many current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail or Yahoo! Mail, that performs the same tasks. Such webmail interfaces allow users to access their mail with any standard web browser, from any computer, rather than relying on a local email client.

Upon reception of email messages, email client applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the "mbox" format. The specific format used is often indicated by special filename extensions:

Some applications (like Apple Mail) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.

The URI scheme, as registered with the IANA, defines the codice_5 scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the "To:" field. Many clients also support query string parameters for the other email fields, such as its subject line or carbon copy recipients.

Many email providers have a web-based email client (e.g. AOL Mail, Gmail, Outlook.com and Yahoo! Mail). This allows users to log into the email account by using any compatible web browser to send and receive their email. Mail is typically not downloaded to the web client, so can't be read without a current Internet connection.

The Post Office Protocol 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the server. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).POP3 allows you to download email messages on your local computer and read them even when you are offline.

The Internet Message Access Protocol (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like smartphones are increasingly used to check email while traveling and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually, the mail is left in folders in the mail server.

Messaging Application Programming Interface (MAPI) is used by Microsoft Outlook to communicate to Microsoft Exchange Server - and to a range of other email server products such as Axigen Mail Server, Kerio Connect, Scalix, Zimbra, HP OpenMail, IBM Lotus Notes, Zarafa, and Bynari where vendors have added MAPI support to allow their products to be accessed directly via Outlook.

Email has been widely accepted by businesses, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed Internet). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.

It has some key benefits to business and other organizations, including:

Email marketing via "opt-in" is often successfully used to send special sales offerings and new product information. Depending on the recipient's culture, email sent without permission—such as an "opt-in"—is likely to be viewed as unwelcome "email spam".

Many users access their personal emails from friends and family members using a personal computer in their house or apartment.

Email has become used on smartphones and on all types of computers. Mobile "apps" for email increase accessibility to the medium for users who are out of their homes. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other devices to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. , there were approximately 1.4 billion email users worldwide and 50 billion non-spam emails that were sent daily.

Individuals often check emails on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their Facebook accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone. It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on a smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.

, the number of Americans visiting email web sites had fallen 6 percent after peaking in November 2009. For persons 12 to 17, the number was down 18 percent. Young people preferred instant messaging, texting and social media. Technology writer Matt Richtel said in "The New York Times" that email was like the VCR, vinyl records and film cameras—no longer cool and something older people do.

A 2015 survey of Android users showed that persons 13 to 24 used messaging apps 3.5 times as much as those over 45, and were far less likely to use email.

Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include Microsoft Word documents, PDF documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, servers and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less. Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ to what the user sees, which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, various file hosting services are available and commonly used.

The ubiquity of email for knowledge workers and "white collar" employees has led to concerns that recipients face an "information overload" in dealing with increasing volumes of email. With the growth in mobile devices, by default employees may also receive work-related emails outside of their working day. This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect, as efforts to read the many emails could reduce productivity.

Email "spam" is unsolicited bulk email. The low cost of sending such email meant that, by 2003, up to 30% of total email traffic was spam, and was threatening the usefulness of email as a practical tool. The US CAN-SPAM Act of 2003 and similar laws elsewhere had some impact, and a number of effective anti-spam techniques now largely mitigate the impact of spam by filtering or rejecting it for most users, but the volume sent is still very high—and increasingly consists not of advertisements for products, but malicious content or links. In September 2017, for example, the proportion of spam to legitimate email rose to 59.56%.

A range of malicious email types exist. These range from various types of email scams, including "social engineering" scams such as advance-fee scam "Nigerian letters", to phishing, email bombardment and email worms.

Email spoofing occurs when the email message header is designed to make the message appear to come from a known or trusted source. Email spam and phishing methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email that appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.

Email bombing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.

Today it can be important to distinguish between the Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.

Email privacy, without some security precautions, can be compromised because:

There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor anonymity network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail, or S/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.

Additionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this. Finally, the attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.

Emails can now often be considered as binding contracts as well, so users must be careful about what they send through email correspondence.

Flaming occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word "incendiary" to describe particularly heated email discussions. The ease and impersonality of email communications mean that the social norms that encourage civility in person or via telephone do not exist and civility may be forgotten.

Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a "boilerplate" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. Harvard University law professor Lawrence Lessig is credited with coining this term, but he may only have popularized it.

Originally Internet email was completely ASCII text-based. MIME now allows body content text and some header content text in international character sets, but other headers and email addresses using UTF-8, while standardized have yet to be widely adopted.
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the IETF introduced Delivery Status Notifications (delivery receipts) and Message Disposition Notifications (return receipts); however, these are not universally deployed in production.

Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:

In the absence of standard methods, a range of system based around the use of web bugs have been developed. However, these are often seen as underhand or raising privacy concerns, and only work with email clients that support rendering of HTML. Many mail clients now default to not showing "web content". Webmail providers can also disrupt web bugs by pre-caching images.



</doc>
<doc id="9739" url="https://en.wikipedia.org/wiki?curid=9739" title="Emoticon">
Emoticon

An emoticon (, , rarely pronounced ), short for "emotion icon", also known simply as an emote, is a pictorial representation of a facial expression using characters—usually punctuation marks, numbers, and letters—to express a person's feelings or mood, or as a time-saving method. The first ASCII emoticons, codice_1 and codice_2, were written by Scott Fahlman in 1982, but emoticons actually originated on the PLATO IV computer system in 1972.

In Western countries, emoticons are usually written at a right angle to the direction of the text. Users from Japan popularized a kind of emoticon called kaomoji (; lit. 顔(kao)=face, 文字(moji)=character(s)), utilizing the Katakana character set, that can be understood without tilting one's head to the left. This style arose on ASCII NET of Japan in 1986.

As SMS and the internet became widespread in the late 1990s, emoticons became increasingly popular and were commonly used on text messages, internet forums and e-mails. Emoticons have played a significant role in communication through technology, and some devices and applications have provided stylized pictures that do not use text punctuation. They offer another range of "tone" and feeling through texting that portrays specific emotions through facial gestures while in the midst of text-based cyber communication.

Emoticons began with the suggestion that combinations of punctuation could be used in typography to replace language. While Scott Fahlman's suggestion in the 1980s was the birth of the emoticon, it wasn't the first occasion that :) or :-) was used in language.

In 1648, poet Robert Herrick included the lines:

Herrick's work predated any other recorded use of brackets as a smiling face by around 200 years. However, experts have since weighed whether the inclusion of the colon in the poem was deliberate and if it was meant to represent a smiling face. English professor Alan Jacobs argued "punctuation in general was unsettled in the seventeenth century... Herrick was unlikely to have consistent punctuational practices himself, and even if he did he couldn't expect either his printers or his readers to share them."

Many different forms of communication are now seen as precursors to emoticons and more recently emojis. The use of emoticons can be traced back to the 17th century, drawn by a Slovak notary to indicate his satisfaction with the state of his town's municipal financial records in 1635, but they were commonly used in casual and humorous writing. Digital forms of emoticons on the Internet were included in a proposal by Scott Fahlman of Carnegie Mellon University in Pittsburgh, Pennsylvania, in a message on September 19, 1982.

The "National Telegraphic Review and Operators Guide" in April 1857 documented the use of the number 73 in Morse code to express "love and kisses" (later reduced to the more formal "best regards"). "Dodge's Manual" in 1908 documented the reintroduction of "love and kisses" as the number 88. Gajadhar and Green comment that both Morse code abbreviations are more succinct than modern abbreviations such as LOL. Aside from morse code, other communication tools such as generic prosigns were seen by some as an evolution of language. The first time an emoticon appeared in text was in the transcript of one of Abraham Lincoln's speeches written in 1862. It contained the following:

codice_3

According to the "New York Times", there has been some debate whether the emoticon in Abraham Lincoln's speech was a typo, a legitimate punctuation construct, or the first emoticon. In the late 1800s, the first emoticons were created as an art form in the U.S. satirical magazine "Puck." In total, four different emoticon designs were displayed, all using punctuation to create different typographical emoticon faces. The emoticon designs were similar to that which formed many years later in Japan, often referred to as ""Kaomoji"", due to their complicated design. Despite the innovation, complex emoticons didn't develop in Japan until nearly a century later. In 1912, American author Ambrose Bierce was the first to suggest that a bracket could be used to represent a smiling face. He stated, "an improvement in punctuation – the snigger point, or note of cachinnation: it is written thus ‿ and presents a smiling mouth. It is to be appended, with the full stop, to every jocular or ironical sentence".

Following this breakthrough statement, other writers and linguistic experts began to put out theories as to how punctuations could be used in collections to represent a face. Moving on from Bierce's theory that a horizontal brackets could be used for a smiling face, Alan Gregg was the first recorded person to suggest that by combining punctuation marks, more elaborate emotions could be demonstrated. There is an argument that this was the first real set of emoticons, despite later designs becoming the standard for emoticons. Gregg published his theory in 1936, in a "Harvard Lampoon" article. He suggested that by turning the bracket sideways, it could be used for the sides of the mouth or cheeks, with other punctuation used between the brackets to display various emotions. Gregg's theory took the step of creating more than one smiling face, with (-) for a normal smile and (--) for a laughing smile. The logic behind the design was that more teeth were showing on the wider design. Two other emoticons were proposed in the article, with (#) for a frown and (*) for a wink.

Emoticons had already come into use in sci-fi fandom in the 1940s, although there seems to have been a lapse in cultural continuity between the communities.

The September 1962 issue of "MAD" magazine included an article titled "Typewri-toons". The piece, featuring typewriter-generated artwork credited to "Royal Portable", was entirely made up of repurposed typography, including a capital letter P having a bigger bust than a capital I, a lowercase b and d discussing their pregnancies, an asterisk on top of a letter to indicate the letter had just come inside from a snowfall, and a classroom of lowercase n's interrupted by a lowercase h "raising its hand". Two additional "Typewri-toons" articles subsequently appeared in "Mad", in 1965 and 1987.

In a "New York Times" interview in April 1969, Alden Whitman asked writer Vladimir Nabokov: "How do you rank yourself among writers (living) and of the immediate past?" Nabokov answered: "I often think there should exist a special typographical sign for a smile – some sort of concave mark, a supine round bracket, which I would now like to trace in reply to your question."

Up until this point, many of the designs considered to be early emoticons were created using fairly basic punctuation, using a single punctuation mark instead of a word or to express feeling, before individuals started combining two punctuations (often a colon and bracket) to create something that resembled a smiling face.

Scott Fahlman is considered to be the first person to create the first true emoticon as he began to experiment with using multiple punctuation marks to display emotion and replace language. He is the first documented person to use a complex emoticon of three or more punctuation marks, with codice_1 and codice_2 with a specific suggestion that they be used to express emotion. Not only did Fahlman create two different emoticons, he also said with the emoticons that they could be used to express emotion. While Nabokov had suggested something similar to Fahlman, there was little analysis of the wider consideration of what Nabokov could do with the design. Fahlman on the other hand quickly theorized that his emoticons could be designed to replace language on a large scale. The two designs of colon, hyphen and bracket were also adapted very quickly to portray a range of emotions, therefore creating the first true set of emoticons.

The message from Fahlman was sent via the Carnegie Mellon University computer science general board on September 19, 1982. The conversation was taking place between many notable computer scientists, including David Touretzky, Guy Steele, and Jaime Carbonell. The messaging transcript was considered to have been lost, before it was recovered 20 years later by Jeff Baird from old backup tapes.

Within a few months, it had spread to the ARPANET and Usenet. Many variations on the theme were immediately suggested by Scott and others.

Inspired by Scott Fahlman's idea of using faces in language, the Loufrani family established The Smiley Company in 1996. Nicolas Loufrani developed hundreds of different emoticons, including 3D versions. His designs were registered at the United States Copyright Office in 1997 and appeared online as .gif files in 1998. These were the first graphical representations of the originally text-based emoticon. He published his icons as well as emoticons created by others, along with their ASCII versions, in an online Smiley Dictionary in the early 2000s. This dictionary included over 3,000 different smileys and was published as a book called "Dico Smileys" in 2002.

Fahlman has stated in numerous interviews that he sees emojis as ""the remote descendants of this thing I did.""

Usually, emoticons in Western style have the eyes on the left, followed by the nose and the mouth. The two-character version codice_6 which omits the nose is also very popular.

The most basic emoticons are relatively consistent in form, but each of them can be transformed by being rotated (making them tiny ambigrams), with or without a hyphen (nose).
There are also some possible variations to emoticons to get new definitions, like changing a character to express a new feeling, or slightly change the mood of the emoticon. For example, codice_7 equals sad and codice_8 equals very sad. Weeping can be written as codice_9. A blush can be expressed as codice_10. Others include wink codice_11, a grin codice_12, smug codice_13, and tongue out codice_14, such as when blowing a raspberry. An often used combination is also codice_15 for a heart, and codice_16 for a broken heart. codice_17 is also sometimes used to depict shock.

A broad grin is sometimes shown with crinkled eyes to express further amusement; codice_18 and the addition of further "D" letters can suggest laughter or extreme amusement e.g. codice_19. There are hundreds of other variations including codice_20 for anger, or codice_21 for an evil grin, which can be, again, used in reverse, for an unhappy angry face, in the shape of codice_22. codice_23 for vampire teeth, codice_24 for grimace, and codice_25 can be used to denote a flirting or joking tone, or may be implying a second meaning in the sentence preceding it.

As computers offer increasing built-in support for non-Western writing systems, it has become possible to use other glyphs to build emoticons. The 'shrug' emoticon, codice_26, uses the glyph ツ from the Japanese katakana writing system.

An equal sign is often used for the eyes in place of the colon, seen as codice_27, without changing the meaning of the emoticon. In these instances, the hyphen is almost always either omitted or, occasionally, replaced with an "o" as in codice_28. In most circles it has become acceptable to omit the hyphen, whether a colon or an equal sign is used for the eyes, but in some areas of usage people still prefer the larger, more traditional emoticon codice_1 or codice_30. One linguistic study has indicated that the use of a nose in an emoticon may be related to the user's age, with younger people less likely to use a nose. Similar-looking characters are commonly substituted for one another: for instance, codice_31, codice_32, and codice_33 can all be used interchangeably, sometimes for subtly different effect or, in some cases, one type of character may look better in a certain font and therefore be preferred over another. It is also common for the user to replace the rounded brackets used for the mouth with other, similar brackets, such as codice_34 instead of codice_35.

Some variants are also more common in certain countries due to keyboard layouts. For example, the smiley codice_27 may occur in Scandinavia, where the keys for codice_37 and codice_35 are placed right beside each other. However, the codice_6 variant is without a doubt the dominant one in Scandinavia, making the codice_27 version a rarity. Diacritical marks are sometimes used. The letters codice_41 and codice_42 can be seen as an emoticon, as the upright version of codice_17 (meaning that one is surprised) and codice_12 (meaning that one is very happy) respectively.

Some emoticons may be read right to left instead, and in fact, can only be written using standard ASCII keyboard characters this way round; for example codice_45 which refers to being shocked or anxious, opposite to the large grin of codice_12.

Users from Japan popularized a style of emoticons (, "kaomoji, lit." "face characters") that can be understood without tilting one's head to the left. This style arose on ASCII NET, an early Japanese online service, in 1986. Similar-looking emoticons were used on the Byte Information Exchange (BIX) around the same time.

These emoticons are usually found in a format similar to codice_47. The asterisks indicate the eyes; the central character, commonly an underscore, the mouth; and the parentheses, the outline of the face.

Different emotions can be expressed by changing the character representing the eyes: for example, "T" can be used to express crying or sadness: codice_48. codice_49 may also be used to mean "unimpressed". The emphasis on the eyes in this style is reflected in the common usage of emoticons that use only the eyes, e.g. codice_50. Looks of stress are represented by the likes of codice_51, while codice_52 is a generic emoticon for nervousness, the semicolon representing an anxiety-induced sweat drop (discussed further below). codice_53 can indicate embarrassment by symbolizing blushing. Characters like hyphens or periods can replace the underscore; the period is often used for a smaller, "cuter" mouth, or to represent a nose, e.g. codice_54. Alternatively, the mouth/nose can be left out entirely, e.g. codice_55.

Parentheses are sometimes replaced with braces or square brackets, e.g. codice_56 or codice_57. Many times, the parentheses are left out completely, e.g. codice_50,codice_59, codice_60, codice_61, codice_62, or codice_63. A quotation mark codice_64, apostrophe codice_65, or semicolon codice_66 can be added to the emoticon to imply apprehension or embarrassment, in the same way that a sweat drop is used in manga and anime.

Microsoft IME 2000 (Japanese) or later supports the input of emoticons like the above by enabling the Microsoft IME Spoken Language/Emotion Dictionary. In IME 2007, this support was moved to the Emoticons dictionary. Such dictionaries allow users to call up emoticons by typing words that represent them.

Communication software allowing the use of Shift JIS encoded Japanese characters rather than just ASCII allowed for the development of new kaomoji using the extended character set, such as codice_67 or codice_68.

Modern communication software generally utilizes Unicode, which allows for the incorporation of characters from other languages (e.g. from the Cyrillic alphabet), and a variety of symbols into the kaomoji, as in codice_69 or codice_70.

Further variations can be produced using Unicode combining characters, as in codice_71 or codice_72.

English-language anime forums adopted those Japanese-style emoticons that could be used with the standard ASCII characters available on Western keyboards. Because of this, they are often called "anime style" emoticons in English. They have since seen use in more mainstream venues, including online gaming, instant-messaging, and non-anime-related discussion forums. Emoticons such as codice_73, codice_74, codice_75, codice_76, codice_77, or codice_78 which include the parentheses, mouth or nose, and arms (especially those represented by the inequality signs < or >) also are often referred to as "" in reference to their likeness to Nintendo's video game character Kirby. The parentheses are sometimes dropped when used in the English language context, and the underscore of the mouth may be extended as an intensifier for the emoticon in question, e.g. codice_79 for very happy. The emoticon uses the Eastern style, but incorporates a depiction of the Western "middle-finger flick-off" using a "t" as the arm, hand, and finger. Using a lateral click for the nose such as in is believed to originate from the Finnish image-based message board Ylilauta, and is called a "Lenny face". Another apparently Western invention is the use of emoticons like codice_80 or codice_81 to indicate vampires or other mythical beasts with fangs.

Exposure to both Western and Japanese style emoticons or kaomoji through blogs, instant messaging, and forums featuring a blend of Western and Japanese pop culture has given rise to many emoticons that have an upright viewing format. The parentheses are often dropped, and these emoticons typically only use alphanumeric characters and the most commonly used English punctuation marks. Emoticons such as codice_82, codice_83, codice_84, codice_85, codice_86, codice_49, codice_88, and codice_89 are used to convey mixed emotions that are more difficult to convey with traditional emoticons. Characters are sometimes added to emoticons to convey an anime- or manga-styled sweat drop, for example codice_90, codice_91, codice_92, codice_93, and codice_94. The equals sign can also be used for closed, anime-looking eyes, for example codice_95, codice_96, codice_97, codice_98, and codice_99. The codice_100 face (and its variations codice_101 and codice_102), is an emoticon of Japanese origin which denotes a cute expression or emotion felt by the user.

In Brazil, sometimes combining characters (accents) are added to emoticons to represent eyebrows, as in codice_103, codice_104, codice_105, codice_106, or codice_107.

Users of the Japanese discussion board 2channel, in particular, have developed a wide variety of unique emoticons using characters from various languages, such as Kannada, as in codice_108 (for a look of disapproval, disbelief, or confusion). These were quickly picked up by 4chan and spread to other Western sites soon after. Some have taken on a life of their own and become characters in their own right, like Monā.

In South Korea, emoticons use Korean Hangul letters, and the Western style is rarely used. The structures of Korean and Japanese emoticons are somewhat similar, but they have some differences. Korean style contains Korean jamo (letters) instead of other characters. There are countless number of emoticons that can be formed with such combinations of Korean jamo letters. Consonant jamos codice_109, codice_110 or codice_111 as the mouth/nose component and codice_112, codice_113 or codice_114 for the eyes. For example: codice_115, codice_116, codice_117 and codice_118. Faces such as codice_119, codice_120, codice_121 and codice_122, using quotation marks codice_64 and apostrophes codice_65 are also commonly used combinations. Vowel jamos such as ㅜ,ㅠ depict a crying face. Example: codice_125, codice_126 and codice_127 (same function as T in western style). Sometimes ㅡ (not an em-dash "—" but a vowel jamo), a comma or an underscore is added, and the two character sets can be mixed together, as in codice_128, codice_129, codice_130, codice_131, codice_132 and codice_133. Also, semicolons and carets are commonly used in Korean emoticons; semicolons mean sweating (embarrassed). If they are used with ㅡ or – they depict a bad feeling. Examples: codice_134, codice_135, codice_136, codice_137 and codice_138. However, codice_139 means smile (almost all people use this without distinction of sex or age). Others include: codice_140, codice_141, codice_142, codice_143.

The character 囧 (U+56E7), which means "bright", may be combined with posture emoticon Orz, such as 囧rz. The character existed in Oracle bone script, but its use as emoticon was documented as early as January 20, 2005.

Other ideographic variants for 囧 include 崮 (king 囧), 莔 (queen 囧), 商 (囧 with hat), 囧興 (turtle), 卣 (Bomberman).

The character 槑 (U+69D1), which sounds like the word for "plum" (梅 (U+FA44)), is used to represent double of 呆 (dull), or further magnitude of dullness. In Chinese, normally full characters (as opposed to the stylistic use of 槑) might be duplicated to express emphasis.

On the Russian speaking internet, the right parenthesis codice_35 is used as a smiley. Multiple parentheses codice_145 are used to express greater happiness, amusement or laughter. It is commonly placed at the end of a sentence. The colon is omitted due to being in a lesser-known and difficult to type position on the ЙЦУКЕН keyboard layout.

Orz (other forms include: ) is an emoticon representing a kneeling or bowing person (the Japanese version of which is called "dogeza") with the "o" being the head, the "r" being the arms and part of the body, and the "z" being part of the body and the legs. This stick figure can represent respect or "kowtowing", but commonly appears along a range of responses, including "frustration, despair, sarcasm, or grudging respect".

It was first used in late 2002 at the forum on Techside, a Japanese personal website. At the "Techside FAQ Forum" (TECHSIDE教えて君BBS(教えてBBS) ), a poster asked about a cable cover, typing to show a cable and its cover. Others commented that it looked like a kneeling person, and the symbol became popular. These comments were soon deleted as they were considered off-topic. By 2005, Orz spawned a subculture: blogs have been devoted to the emoticon, and URL shortening services have been named after it. In Taiwan, Orz is associated with the phrase "nice guy"that is, the concept of males being rejected for a date by females, with a phrase like "You are a nice guy."

Orz should not be confused with m(_ _)m, which means "Thank you" or an apology.

A portmanteau of "emotion" and "sound", an emotisound is a brief sound transmitted and played back during the viewing of a message, typically an IM message or e-mail message. The sound is intended to communicate an emotional subtext. Many instant messaging clients automatically trigger sound effects in response to specific emoticons.

Some services, such as MuzIcons, combine emoticons and music player in an Adobe Flash-based widget.

In 2004, the Trillian chat application introduced a feature called "emotiblips", which allows Trillian users to stream files to their instant message recipients "as the voice and video equivalent of an emoticon".

In 2007, MTV and Paramount Home Entertainment promoted the "emoticlip" as a form of viral marketing for the second season of the show "The Hills". The emoticlips were twelve short snippets of dialogue from the show, uploaded to YouTube, which the advertisers hoped would be distributed between web users as a way of expressing feelings in a similar manner to emoticons. The emoticlip concept is credited to the Bradley & Montgomery advertising firm, which hopes they would be widely adopted as "greeting cards that just happen to be selling something".

In 2008, an emotion-sequence animation tool, called FunIcons was created. The Adobe Flash and Java-based application allows users to create a short animation. Users can then email or save their own animations to use them on similar social utility applications.

During the first half of the 2010s, there have been different forms of small audiovisual pieces to be sent through instant messaging systems to express one's emotion. These videos lack an established name, and there are several ways to designate them: "emoticlips" (named above), "emotivideos" or more recently "emoticon videos". These are tiny videos which can be easily transferred from one mobile phone to another. Current video compression codecs such as H.264 allow these pieces of video to be light in terms of file size and very portable. The popular computer and mobile app Skype use these in a separate keyboard or by typing the code of the "emoticon videos" between parentheses.

In 2000, Despair, Inc. obtained a U.S. trademark registration for the "frowny" emoticon codice_2 when used on "greeting cards, posters and art prints". In 2001, they issued a satirical press release, announcing that they would sue Internet users who typed the frowny; the joke backfired and the company received a storm of protest when its mock release was posted on technology news website Slashdot.

A number of patent applications have been filed on inventions that assist in communicating with emoticons. A few of these have been issued as US patents. , for example, discloses a method developed in 2001 to send emoticons over a cell phone using a drop-down menu. The stated advantage over the prior art was that the user saved on the number of keystrokes though this may not address the obviousness criteria.

The emoticon codice_1 was also filed in 2006 and registered in 2008 as a European Community Trademark (CTM). In Finland, the Supreme Administrative Court ruled in 2012 that the emoticon cannot be trademarked, thus repealing a 2006 administrative decision trademarking the emoticons codice_1, codice_27, codice_150, codice_6 and codice_7.

In 2005, a Russian court rejected a legal claim against Siemens by a man who claimed to hold a trademark on the codice_153 emoticon.

In 2008, Russian entrepreneur Oleg Teterin claimed to have been granted the trademark on the codice_153 emoticon. A license would not "cost that muchtens of thousands of dollars" for companies, but would be free of charge for individuals.

A different, but related, use of the term "emoticon" is found in the Unicode Standard, referring to a subset of emoji which display facial expressions. The standard explains this usage with reference to existing systems, which provided functionality for substituting certain textual emoticons with images or emoji of the expressions in question.

Some smiley faces were present in Unicode since 1.1, including a white frowning face, a white smiling face, and a black smiling face. ("Black" refers to a glyph which is filled, "white" refers to a glyph which is unfilled).

The Emoticons block was introduced in Unicode Standard version 6.0 (published in October 2010) and extended by 7.0. It covers Unicode range from U+1F600 to U+1F64F fully.
After that block had been filled, Unicode 8.0 (2015), 9.0 (2016) and 10.0 (2017) added additional emoticons in the range from U+1F910 to U+1F9FF. Currently, U+1F90CU+1F90F, U+1F93F, U+1F94DU+1F94F, U+1F96CU+1F97F, U+1F998U+1F9CF (excluding U+1F9C0 which contains the 🧀 emoji) and U+1F9E7U+1F9FF do not contain any emoticons since Unicode 10.0.

For historic and compatibility reasons, some other heads and figures, which mostly represent different aspects like genders, activities, and professions instead of emotions, are also found in Miscellaneous Symbols and Pictographs (especially U+1F466U+1F487) and Transport and Map Symbols. Body parts, mostly hands, are also encoded in the Dingbat and Miscellaneous Symbols blocks.



</doc>
<doc id="9740" url="https://en.wikipedia.org/wiki?curid=9740" title="Epoch (disambiguation)">
Epoch (disambiguation)

An epoch is an instant in time chosen as the origin of a particular calendar era.

Epoch or EPOCH may also refer to:









</doc>
<doc id="9742" url="https://en.wikipedia.org/wiki?curid=9742" title="Erdős number">
Erdős number

The Erdős number () describes the "collaborative distance" between mathematician and another person, as measured by authorship of mathematical papers. The same principle has been applied in other fields where a particular individual has collaborated with a large and broad number of peers.

Paul Erdős (1913–1996) was an influential Hungarian mathematician who in the latter part of his life spent a great deal of time writing papers with a large number of colleagues, working on solutions to outstanding mathematical problems. He published more papers during his lifetime (at least 1,525) than any other mathematician in history. (Leonhard Euler published more total pages of mathematics but fewer separate papers: about 800.) Erdős spent a large portion of his later life living out of a suitcase, visiting his over 500 collaborators around the world.

The idea of the Erdős number was originally created by the mathematician's friends as a tribute to his enormous output. Later it gained prominence as a tool to study how mathematicians cooperate to find answers to unsolved problems. Several projects are devoted to studying connectivity among researchers, using the Erdős number as a proxy. For example, Erdős collaboration graphs can tell us how authors cluster, how the number of co-authors per paper evolves over time, or how new theories propagate.

Several studies have shown that leading mathematicians tend to have particularly low Erdős numbers. The median Erdős number of Fields Medalists is 3. Only 7,097 (about 5% of mathematicians with a collaboration path) have an Erdős number of 2 or lower. As time passes, the smallest Erdős number that can still be achieved will necessarily increase, as mathematicians with low Erdős numbers die and become unavailable for collaboration. Still, historical figures can have low Erdős numbers. For example, renowned Indian mathematician Srinivasa Ramanujan has an Erdős number of only 3 (through G. H. Hardy, Erdős number 2), even though Paul Erdős was only 7 years old when Ramanujan died.

To be assigned an Erdős number, someone must be a coauthor of a research paper with another person who has a finite Erdős number. Paul Erdős has an Erdős number of zero. Anybody else's Erdős number is where is the lowest Erdős number of any coauthor. The American Mathematical Society provides a free online tool to determine the Erdős number of every mathematical author listed in the "Mathematical Reviews" catalogue.

Erdős wrote around 1,500 mathematical articles in his lifetime, mostly co-written. He had 511 direct collaborators; these are the people with Erdős number 1. The people who have collaborated with them (but not with Erdős himself) have an Erdős number of 2 (12,600 people as of 7 August, 2020), those who have collaborated with people who have an Erdős number of 2 (but not with Erdős or anyone with an Erdős number of 1) have an Erdős number of 3, and so forth. A person with no such coauthorship chain connecting to Erdős has an Erdős number of infinity (or an undefined one). Since the death of Paul Erdős, the lowest Erdős number that a new researcher can obtain is 2.

There is room for ambiguity over what constitutes a link between two authors. The American Mathematical Society collaboration distance calculator uses data from "Mathematical Reviews", which includes most mathematics journals but covers other subjects only in a limited way, and which also includes some non-research publications. The Erdős Number Project web site says:

but they do not include non-research publications such as elementary textbooks, joint editorships, obituaries, and the like. The "Erdős number of the second kind" restricts assignment of Erdős numbers to papers with only two collaborators.

The Erdős number was most likely first defined in print by Casper Goffman, an analyst whose own Erdős number is 2. Goffman published his observations about Erdős' prolific collaboration in a 1969 article entitled ""And what is your Erdős number?"" See also some comments in an obituary by Michael Golomb.

The median Erdős number among Fields medalists is as low as 3. Fields medalists with Erdős number 2 include Atle Selberg, Kunihiko Kodaira, Klaus Roth, Alan Baker, Enrico Bombieri, David Mumford, Charles Fefferman, William Thurston, Shing-Tung Yau, Jean Bourgain, Richard Borcherds, Manjul Bhargava, Jean-Pierre Serre and Terence Tao. There are no Fields medalists with Erdős number 1; however, Endre Szemerédi is an Abel Prize Laureate with Erdős number 1.

While Erdős collaborated with hundreds of co-authors, there were some individuals with whom he co-authored dozens of papers. This is a list of the ten persons who most frequently co-authored with Erdős and their number of papers co-authored with Erdős (i.e. their number of collaborations).
, all Fields Medalists have a finite Erdős number, with values that range between 2 and 6, and a median of 3. In contrast, the median Erdős number across all mathematicians (with a finite Erdős number) is 5, with an extreme value of 13. The table below summarizes the Erdős number statistics for Nobel prize laureates in Physics, Chemistry, Medicine and Economics. The first column counts the number of laureates. The second column counts the number of winners with a finite Erdős number. The third column is the percentage of winners with a finite Erdős number. The remaining columns report the minimum, maximum, average and median Erdős numbers among those laureates.

Among the Nobel Prize laureates in Physics, Albert Einstein and Sheldon Lee Glashow have an Erdős number of 2. Nobel Laureates with an Erdős number of 3 include Enrico Fermi, Otto Stern, Wolfgang Pauli, Max Born, Willis E. Lamb, Eugene Wigner, Richard P. Feynman, Hans A. Bethe, Murray Gell-Mann, Abdus Salam, Steven Weinberg, Norman F. Ramsey, Frank Wilczek, and David Wineland. Fields Medal-winning physicist Ed Witten has an Erdős number of 3.

Computational biologist Lior Pachter has an Erdős number of 2. Evolutionary biologist Richard Lenski has an Erdős number of 3, having co-authored a publication with Lior Pachter and with mathematician Bernd Sturmfels, each of whom has an Erdős number of 2.

There are at least two winners of the Nobel Prize in Economics with an Erdős number of 2: Harry M. Markowitz (1990) and Leonid Kantorovich (1975). Other financial mathematicians with Erdős number of 2 include David Donoho, Marc Yor, Henry McKean, Daniel Stroock, and Joseph Keller.

Nobel Prize laureates in Economics with an Erdős number of 3 include Kenneth J. Arrow (1972), Milton Friedman (1976), Herbert A. Simon (1978), Gerard Debreu (1983), John Forbes Nash, Jr. (1994), James Mirrlees (1996), Daniel McFadden (2000), Daniel Kahneman (2002), Robert J. Aumann (2005), Leonid Hurwicz (2007), Roger Myerson (2007), Alvin E. Roth (2012), and Lloyd S. Shapley (2012) and Jean Tirole (2014).

Some investment firms have been founded by mathematicians with low Erdős numbers, among them James B. Ax of Axcom Technologies, and James H. Simons of Renaissance Technologies, both with an Erdős number of 3.

Since the more formal versions of philosophy share reasoning with the basics of mathematics, these fields overlap considerably, and Erdős numbers are available for many philosophers. Philosopher John P. Burgess has an Erdős number of 2. Jon Barwise and Joel David Hamkins, both with Erdős number 2, have also contributed extensively to philosophy, but are primarily described as mathematicians.

Judge Richard Posner, having coauthored with Alvin E. Roth, has an Erdős number of at most 4. Roberto Mangabeira Unger, a politician, philosopher and legal theorist who teaches at Harvard Law School, has an Erdős number of at most 4, having coauthored with Lee Smolin.

Angela Merkel, Chancellor of Germany from 2005 to the present, has an Erdős number of at most 5.

Some fields of engineering, in particular communication theory and cryptography, make direct use of the discrete mathematics championed by Erdős. It is therefore not surprising that practitioners in these fields have low Erdős numbers. For example, Robert McEliece, a professor of electrical engineering at Caltech, had an Erdős number of 1, having collaborated with Erdős himself. Cryptographers Ron Rivest, Adi Shamir, and Leonard Adleman, inventors of the RSA cryptosystem, all have Erdős number 2.

Anthropologist Douglas R. White has an Erdős number of 2 via graph theorist Frank Harary. Sociologist Barry Wellman has an Erdős number of 3 via social network analyst and statistician Ove Frank, another collaborator of Harary's.

The Romanian mathematician and computational linguist Solomon Marcus had an Erdős number of 1 for a paper in "Acta Mathematica Hungarica" that he co-authored with Erdős in 1957.

Erdős numbers have been a part of the folklore of mathematicians throughout the world for many years. Among all working mathematicians at the turn of the millennium who have a finite Erdős number, the numbers range up to 15, the median is 5, and the mean is 4.65; almost everyone with a finite Erdős number has a number less than 8. Due to the very high frequency of interdisciplinary collaboration in science today, very large numbers of non-mathematicians in many other fields of science also have finite Erdős numbers. For example, political scientist Steven Brams has an Erdős number of 2. In biomedical research, it is common for statisticians to be among the authors of publications, and many statisticians can be linked to Erdős via John Tukey, who has an Erdős number of 2. Similarly, the prominent geneticist Eric Lander and the mathematician Daniel Kleitman have collaborated on papers, and since Kleitman has an Erdős number of 1, a large fraction of the genetics and genomics community can be linked via Lander and his numerous collaborators. Similarly, collaboration with Gustavus Simmons opened the door for 
Erdős numbers within the cryptographic research community, and many linguists have finite Erdős numbers, many due to chains of collaboration with such notable scholars as Noam Chomsky (Erdős number 4), William Labov (3), Mark Liberman (3), Geoffrey Pullum (3), or Ivan Sag (4). There are also connections with arts fields.

According to Alex Lopez-Ortiz, all the Fields and Nevanlinna prize winners during the three cycles in 1986 to 1994 have Erdős numbers of at most 9.

Earlier mathematicians published fewer papers than modern ones, and more rarely published jointly written papers. The earliest person known to have a finite Erdős number is either Antoine Lavoisier (born 1743, Erdős number 13), Richard Dedekind (born 1831, Erdős number 7), or Ferdinand Georg Frobenius (born 1849, Erdős number 3), depending on the standard of publication eligibility.

Martin Tompa proposed a directed graph version of the Erdős number problem, by orienting edges of the collaboration graph from the alphabetically earlier author to the alphabetically later author and defining the "monotone Erdős number" of an author to be the length of a longest path from Erdős to the author in this directed graph. He finds a path of this type of length 12.

Also, Michael Barr suggests "rational Erdős numbers", generalizing the idea that a person who has written p joint papers with Erdős should be assigned Erdős number 1/p. From the collaboration multigraph of the second kind (although he also has a way to deal with the case of the first kind)—with one edge between two mathematicians for "each" joint paper they have produced—form an electrical network with a one-ohm resistor on each edge. The total resistance between two nodes tells how "close" these two nodes are.

It has been argued that "for an individual researcher, a measure such as Erdős number captures the structural properties of [the] network whereas the "h"-index captures the citation impact of the publications," and that "One can be easily convinced that ranking in coauthorship networks should take into account both measures to generate a realistic and acceptable ranking."

In 2004 William Tozier, a mathematician with an Erdős number of 4, auctioned off a co-authorship on eBay, hence providing the buyer with an Erdős number of 5. The winning bid of $1031 was posted by a Spanish mathematician, who however did not intend to pay but just placed the bid to stop what he considered a mockery.

A number of variations on the concept have been proposed to apply to other fields.

The best known is the Bacon number (as in the game Six Degrees of Kevin Bacon), connecting actors that appeared in a film together to the actor Kevin Bacon. It was created in 1994, 25 years after Goffman's article on the Erdős number.

A small number of people are connected to both Erdős and Bacon and thus have an Erdős–Bacon number, which combines the two numbers by taking their sum. One example is the actress-mathematician Danica McKellar, best known for playing Winnie Cooper on the TV series "The Wonder Years". 
Her Erdős number is 4, and her Bacon number is 2.

Further extension is possible. For example, the "Erdős–Bacon–Sabbath number" is the sum of the Erdős–Bacon number and the collaborative distance to the band Black Sabbath in terms of singing in public. Physicist Stephen Hawking had an Erdős–Bacon–Sabbath number of 8, and actress Natalie Portman has one of 11 (her Erdős number is 5).




</doc>
<doc id="9750" url="https://en.wikipedia.org/wiki?curid=9750" title="School voucher">
School voucher

A school voucher, also called an education voucher, in a voucher system, is a certificate of government funding for a student at a school chosen by the student or the student's parents. The funding is usually for a particular year, term or semester. In some countries, states or local jurisdictions, the voucher can be used to cover or reimburse home schooling expenses. In some countries, vouchers only exist for tuition at private schools.

According to a 2017 review of the economics literature on school vouchers, "the evidence to date is not sufficient to warrant recommending that vouchers be adopted on a widespread basis; however, multiple positive findings support continued exploration." A 2006 survey of members of American Economic Association found that over two-thirds of economists support giving parents educational vouchers that can be used at government-operated or privately operated schools, and that support is greater if the vouchers are to be used by parents with low-incomes or parents with children in poorly performing schools.

France lost the Franco-Prussian War of 1870–1871 and many blamed the loss on France's inferior military education system. Following this defeat, the French assembly proposed a religious voucher that would hopefully improve schools by allowing students to seek out the best school. This proposal never moved forward due to the reluctance of the French to subsidize religious education. Despite its failure, this proposal is one of the earliest examples of a voucher system that closely resembles voucher systems proposed and used today in many countries.

The oldest continuing school voucher programs existing today in the United States are the Town Tuitioning programs in Vermont and Maine, beginning in 1869 and 1873 respectively. Because some towns in these states operate neither local high schools nor elementary schools, students in these towns "are eligible for a voucher to attend [either] public schools in other towns or non-religious private schools. In these cases, the 'sending' towns pay tuition directly to the 'receiving' schools."

A system of educational vouchers was introduced in the Netherlands in 1917. Today, more than 70% of pupils attend privately run but publicly funded schools, mostly split along denominational lines.

Milton Friedman argued for the modern concept of vouchers in the 1950s, stating that competition would improve schools, cost less and yield superior educational outcomes. Friedman's reasoning in favor of vouchers gained additional attention in 1980 with the broadcast of his ten part television series "Free to Choose" and the publication of its companion book of the same name (co-written with his wife Rose Friedman, who was also an economist). Episode 6 of the series and chapter 6 of the book were both entitled, "What's Wrong with Our Schools?" and asserted that permitting parents and students to use vouchers to choose their schools would expand freedom of choice and produce more well-educated students.

In some Southern states during the 1960s, school vouchers were used as a way to perpetuate segregation. In a few instances, public schools were closed outright and vouchers were issued to parents. The vouchers, then known as tuition grants, in many cases, were only good at new, private, segregated schools, known as segregation academies.

Today, all modern voucher programs prohibit racial discrimination.

There are important distinctions between different kinds of schools:

Education as a tool for human capital accumulation is often crucial to the development and progression of societies and thus governments have large incentives to continually intervene and improve public education. Additionally, education is often the tool with which societies instill a common set of values that underlie the basic norms of the society. Furthermore, there are positive externalities to society from education. These positive externalities can be in the form of reduced crime, more informed citizens and economic development, known as the neighborhood effect.

In terms of economic theory, families face a bundle of consumption choices that determine how much they will spend on education and private consumption. Any number of consumption bundles are available as long as they fit within the budget constraint. Meaning that any bundle of consumption of education and private consumption must not exceed budgetary constraints. Indifference curves represent the preferences of one good over another. The indifference curve determines how much education an individual will want to consume versus how much private consumption an individual will want to consume.

Government intervention in education typically takes two forms. The first approach can be broad, such as instituting charter schools, magnet schools, or for-profit schools and increasing competition. The second approach can be individually focused such as providing subsidies or loans for individuals to attend college or school vouchers for K-12.

Vouchers are typically instituted for two broad economic reasons. The first reason is consumer choice. A family can choose to where their child goes to school and pick the school that is closest to their preference of education provider.

The second reason why vouchers are proposed is to increase market competition amongst schools. Similar to the free market theorem, vouchers are intended to make schools more competitive while lowering costs for schools and increasing the educational quality for consumers, the families.

In many instances where school voucher programs have been instituted, there have been mixed results, with some programs showing increased benefits of school vouchers and some instances showing detrimental effects.

In the United States, vouchers are usually funded with state dollars, and in other countries, through a variety of government funding vehicles. It is important to note that schools in the United States retain their federal and local funding regardless of enrollment- only state funding is dependent on enrollment size. Part of improving student performance involves improving teacher and school performance. In theory, more school vouchers would prompt the formation of more private schools which will give parents more choice in school. This increased competition would make both the private and public schools, who are both competing for the voucher funds, maintain a high-quality of teaching as well as keeping costs low. 
Indeed, there is evidence that school vouchers result in cost savings for school systems. A fiscal analysis of Indiana's school voucher system showed annual savings, per student, for the state government.

Proponents of voucher schools argue that there is evidence of multiple benefits for students and families because of school vouchers. There is evidence to show that the use of school vouchers results in increased test scores and higher high school graduation rates for students. A case study in the country of Colombia showed that the presence of voucher programs resulted in an increase of 10 percentage points in a child's likelihood of finishing the 8th grade and showed a 0.2 standard deviations increase in achievement on standardized tests. Furthermore, evidence shows that African Americans experience increased college enrollment rates under voucher programs. It is important to note that these gains for African American students are not present for other racial and ethnic groups.

Research has also shown spatial benefits of voucher system. Public schools, which are near private schools that accept vouchers, often have better test scores than other public schools not near voucher ready private schools. Additional research by Caroline Hoxby shows that when voucher systems are available, both the public and private schools in that school system have increased test scores and graduation rates.

While there are some studies that show the positive effects of voucher programs, there is also research that shows the ineffectiveness of school vouchers. There have been some recent case studies showing that in voucher system school districts, students attending the public school, as opposed to the private school with a voucher, tend to outperform their private school peers.

Besides general lack of results, critics of school vouchers argue that vouchers will lead to segregation. Empirical studies show that there is some evidence that school vouchers can lead to racial or income segregation. However research on this topic is inconclusive, as there is also valid research that shows under certain circumstances, income and racial segregation can be reduced indirectly by increasing school choice.

Additionally, since school vouchers are funded by the government, the implementation could cause the funds for public schools to be reduced. Private-school vouchers affect government budgets through two channels: additional direct voucher expenditures, and public-school cost savings from lower enrollments. Voucher programs would be paid for by the government's education budget, which would be subtracted from the public school's budget. This might affect the public-school system by giving them less to spend and use for their student's education.

A 2018 study by Abdulkadiroğlu et al. found that disadvantaged students who won a lottery (the Louisiana Scholarship Program) to get vouchers to attend private schools had worse education outcomes than disadvantaged students who did not win vouchers: "LSP participation lowers math scores by 0.4 standard deviations and also reduces achievement in reading, science, and social studies. These effects may be due in part to selection of low-quality private schools into the program."

The PACES voucher program was established by the Colombian government in late 1991. It aimed to assist low-income households by distributing school vouchers to students living in neighborhoods situated in the two lowest socioeconomic strata. Between 1991 and 1997, the PACES program awarded 125,000 vouchers to lower-income secondary school students. Those vouchers were worth about US $190 in 1998, and data shows that matriculation fees and other monthly expenses incurred by voucher students attending private schools averaged about US $340 in 1998, so a majority of voucher recipients supplemented the voucher with personal funds.

The students selected to be in the program were selected by lottery. The vouchers were able to be renewed annually, conditional on students achieving satisfactory academic success as indicated by scheduled grade promotion. The program also included incentives to study harder as well as widening schooling options. Empirical evidence showed that the program had some success. Joshua Angrist shows that after 3 years into the program, lottery winners were 15 percentage points more likely to attend private school and complete .1 more years of schooling, and were about 10 percentage points more likely to have finished the 8th grade.The study also reported that there were larger voucher effects for boys than for girls, especially in mathematics performance. It is important to note that the program did not have a significant impact on dropout rates. Angrist reports that lottery winners scored .2 standard deviations higher on standardized tests. The voucher program also reported some social effects. Lottery winners worked less on average than non-lottery winners. Angrist reports that this was correlated with a decreased likelihood to marry or cohabit as teenagers. In general, the school voucher program's benefits outweighed the costs.

In 1981, Chile implemented a universal school voucher system for both elementary and secondary school students. As a result, over 1,000 private schools entered the market, and private enrollment increased by 20-40% by 1998, surpassing 50% in some urban areas. From 1981 to 1988, the private school enrollment rate in urban areas grew 11% more than the private school enrollment rate in rural areas. This change coincided with the transfer of public school administration from the central government to local municipalities. The financial value of a voucher didn't depend on the income of the family receiving it, and the program allowed private voucher schools to be selective, while public schools had to accept and enroll every interested student. At the turn of the 21 century, student achievement in Chile was low compared to students in other nations based on international test-scores. This disparity led to the Chilean government enacting substantial educational reforms in 2008, including major changes in the school voucher system.

The Chilean government passed the Preferential School Subsidy Law (SEP) in January 2008. This piece of legislation made the educational voucher system much more like the regulated compensatory model championed by Christopher Jencks. Under SEP, the voucher system was altered to take family incomes into account. The vouchers provided to "priority students," students whose family income was in the bottom 40% of Chileans in were worth 50% more than those given to the families of students in the upper 60%. Schools with larger numbers of priority students were eligible to receive per-student bonuses, the size of which was tied to the percentage of priority students in the student body. When SEP was started, it covered preschool to fourth grade, and an additional school-year of coverage was added each subsequent year. Almost every public school chose to participate in SEP in 2008, as well as almost two-thirds of private subsidized elementary schools.

There were three important requirements attached to the program. The first requirement stipulated that participating schools could not charge fees to priority students, although private schools in the voucher system could do so for non-priority students. The second requirement ensured that schools could not select students based on their academic ability, not expel them on academic grounds. The third requirement postulated that schools had to self-enroll themselves in an accountability system that ensured that schools were responsible for the utilization of financial resources and student test scores.

In most European countries, education for all primary and secondary schools is fully subsidized. In some countries (e.g. Belgium or France), parents are free to choose which school their child attends.

Most schools in the Republic of Ireland are state-aided parish schools, established under diocesan patronage but with capital costs, teachers salaries and a per head fee paid to the school. These are given to the school regardless of whether or not it requires its students to pay fees. (Although fee-paying schools are in the minority, there has been much criticism over the state aid they receive with opponents claiming this gives them an unfair advantage.)

There is a recent trend towards multi-denominational schools established by parents, which are organised as limited companies without share capital. Parents and students are free to choose their own school. In the event of a school failing to attract students it immediately loses its per-head fee and over time loses its teaching posts – and teachers are moved to other schools which are attracting students. The system is perceived to have achieved very successful outcomes for most Irish children.

The 1995–7 Rainbow Coalition (which contained parties of the centre right and the left) introduced free third-level education to primary degree level. Critics of the latter development charge that it has not increased the number of students from economically deprived backgrounds attending university. However, studies have shown that the removal of tuition fees at third level has increased the number of students overall and those from lower socio-economic backgrounds. This concurs with evidence from the UK of a decrease in attendance numbers after the introduction of fees. However, since the economic crisis, there has been extensive talk and debate regarding the reintroduction of third-level fees.

In Sweden, a system of school vouchers (called "skolpeng") was introduced in 1992 at primary and secondary school level, enabling free choice among publicly run schools and privately run "friskolor" ("free schools"). The voucher is paid with public funds from the local municipality ("kommun") directly to a school based solely on its number of students. Both public schools and free schools are funded the same way. Free schools can be run by not-for-profit groups as well as by for-profit companies, but may not charge top-up fees or select students other than on a first-come, first-served basis. Over 10% of Swedish pupils were enrolled in free schools in 2008 and the number is growing fast, leading the country to be viewed as a pioneer of the model.

Per Unckel, governor of Stockholm and former Minister of Education, has promoted the system, saying "Education is so important that you can't just leave it to one producer, because we know from monopoly systems that they do not fulfill all wishes." The Swedish system has been recommended to Barack Obama by some commentators, including the Pacific Research Institute, which has released a documentary called "Not As Good As You Think: Myth of the Middle Class Schools", a movie depicting positive benefits for middle class schools resulting from Sweden's voucher programs.

A 2004 study concluded that school results in public schools improved due to the increased competition. However, Per Thulberg, director general of the Swedish National Agency for Education, has said that the system "has not led to better results" and in the 2000s Sweden's ranking in the PISA league tables worsened. Though Rachel Wolf, director of the New Schools Network, has suggested that Sweden's education standards had slipped for reasons other than as a result of free schools.

A 2015 study was able to show that "an increase in the share of independent school students improves average short‐ and long‐run outcomes, explained primarily by external effects (e.g. school competition)".

A voucher system for children three to six years-old who attend a non-profit kindergarten was implemented in Hong Kong in 2007. Each child will get HK$13,000 per year. The $13,000 subsidy will be separated into two parts. $10,000 is used to subsidize the school fee and the remaining $3,000 is used for kindergarten teachers to pursue further education and obtain a certificate in Education. Also, there are some restrictions on the voucher system. Parents can only choose non-profit schools with a yearly fee less than $24,000. The government hoped that all kindergarten teachers can obtain an Education certificate by the year 2011–12, at which point the subsidies are to be adjusted to $16,000 for each student, all of which will go toward the school fee.

Milton Friedman criticised the system, saying "I do not believe that CE Mr. Tsang's proposal is properly structured." He said that the whole point of a voucher system is to provide a competitive market place so should not be limited to non-profit kindergartens. 

After protests by parents with children enrolled in for profit kindergartens, the program was extended to children in for- profit kindergartens, but only for children enrolled in or before September 2007. The government will also provide up to HK$30,000 subsidy to for profit kindergartens wanting to convert to non profit.

In Pakistani Punjab, the Education Voucher Scheme (EVS) was introduced by Dr. Allah Bakhsh Malik Managing Director and Chief Executive of Punjab Education Foundation (PEF), especially in urban slums and poorest of the poor in 2005. The initial study was sponsored by Open Society Institute New York USA. Professor Henry M. Levin extended Pro-Bono services for children of poor families from Punjab. To ensure educational justice and integration, the government must ensure that the poorest families have equal access to quality education. The voucher scheme was designed by the Teachers College, Columbia University, and the Open Society Institute. It aims to promote freedom of choice, efficiency, equity, and social cohesion.

A pilot project was started in 2006 in the urban slums of Sukhnehar, Lahore, where a survey showed that all households lived below the poverty line. Through the EVS, the foundation would deliver education vouchers to every household with children 5–16 years of age. The vouchers would be redeemable against tuition payments at participating private schools. In the pilot stage, 1,053 households were given an opportunity to send their children to a private school of their choice. The EVS makes its partner schools accountable to the parents rather than to the bureaucrats at the Ministry of Education. In the FAS program, every school principal has the choice to admit a student or not. However, in the EVS, a partner school cannot refuse a student if the student has a voucher and the family has chosen that school. The partner schools are also accountable to the PEF: they are subject to periodic reviews of their student learning outcomes, additional private investments, and improvements in working conditions of the teachers. The EVS provides an incentive to parents to send their children to school, and so it has become a source of competition among private schools seeking to join the program.

When it comes to the selection of schools, the following criteria are applied across the board: (i) The fee paid by the PEF to EVS partner schools is PKR 550 to per child per month. Schools charging higher fees can also apply to the program, but they will not be paid more than PKR 1200, and they will not be entitled to charge the difference to students' families. (ii) Total school enrollment should be at least 50 children. (iii) The school should have an adequate infrastructure and a good learning environment. (iv) EVS partner schools should be located within a half-kilometer radius of the residences of voucher holders. However, if the parents prefer a particular school farther away, the PEF will not object, provided that the school fulfills the EVS selection criteria. (v) The PEF advertises to stimulate the interest of potential partner schools. It then gives students at short-listed schools preliminary tests in selected subjects, and conducts physical inspections of these schools. PEF offices display a list of all the EVS partner schools so that parents may consult it and choose a school for their children.

By now more than 500,000 students are benefiting from EVS and the program is being scaled up by financing from Government of Punjab.

In the 1980s, the Reagan administration pushed for vouchers, as did the George W. Bush administration in the initial education-reform proposals leading up to the No Child Left Behind Act. As of December 2016, 14 states had traditional school voucher programs. These states consist of: Arkansas, Florida, Georgia, Indiana, Louisiana, Maine, Maryland, Mississippi, North Carolina, Ohio, Oklahoma, Utah, Vermont, and Wisconsin. The capital of the United States, Washington, D.C., also had operating school voucher programs as of December 2016. When including scholarship tax credits and education savings accounts – two alternatives to vouchers – there are 27 states plus the District of Columbia with private school choice programs. Most of these programs were offered to students in low-income families, low performing schools, or students with disabilities. By 2014, the number participating in either vouchers or tax-credit scholarships increased to 250,000, a 30% increase from 2010, but still a small fraction compared to the 55 million in traditional schools.

In 1990, the city of Milwaukee, Wisconsin's public schools were the first to offer vouchers and has nearly 15,000 students using vouchers as of 2011. The program, entitled the Milwaukee Parental Choice Program, originally funded school vouchers for nonreligious, private institutions. It was, however, eventually expanded to include private, religious institutions after it saw success with nonreligious, private institutions. The 2006/07 school year marked the first time in Milwaukee that more than $100 million was paid in vouchers. Twenty-six percent of Milwaukee students will receive public funding to attend schools outside the traditional Milwaukee Public School system. In fact, if the voucher program alone were considered a school district, it would mark the sixth-largest district in Wisconsin. St. Anthony Catholic School, located on Milwaukee's south side, boasts 966 voucher students, meaning that it very likely receives more public money for general school support of a parochial elementary or high school than any before it in American history. A 2013 study of Milwaukee's program posited that the use of vouchers increased the probability that a student would graduate from high school, go to college, and stay in college. A 2015 paper published by the National Bureau of Economic Research found that participation in Louisiana's voucher program "substantially reduces academic achievement" although that the result may be reflective of the poor quality of private schools in the program.

Recent analysis of the competitive effects of school vouchers in Florida suggests that more competition improves performance in the regular public schools.

The largest school voucher program in the United States is Indiana's Indiana Choice Scholarships program.

Proponents of school voucher and education tax credit systems argue that those systems promote free market competition among both private and public schools by allowing parents and students to choose the school where to use the vouchers. This choice available to parents forces schools to perpetually improve in order to maintain enrollment. Thus, proponents argue that a voucher system increases school performance and accountability because it provides consumer sovereignty – allowing individuals to choose what product to buy, as opposed to a bureaucracy.

This argument is supported by studies such as "When Schools Compete: The Effects of Vouchers on Florida Public School Achievement" (Manhattan Institute for Policy Research, 2003), which concluded that public schools located near private schools that were eligible to accept voucher students made significantly more improvements than did similar schools not located near eligible private schools. Stanford's Caroline Hoxby, who has researched the systemic effects of school choice, determined that areas with greater residential school choice have consistently higher test scores at a lower per-pupil cost than areas with very few school districts. Hoxby studied the effects of vouchers in Milwaukee and of charter schools in Arizona and Michigan on nearby public schools. Public schools forced to compete made greater test-score gains than schools not faced with such competition, and that the so-called effect of cream skimming did not exist in any of the voucher districts examined. Hoxby's research has found that both private and public schools improved through the use of vouchers. Also, similar competition has helped in manufacturing, energy, transportation, and parcel postal (UPS, FedEx vs. USPS) sectors of government that have been socialized and later opened up to free market competition.

Similarly, it is argued that such competition has helped in higher education, with publicly funded universities directly competing with private universities for tuition money provided by the Government, such as the GI Bill and the Pell Grant in the United States. The Foundation for Educational Choice alleges that a school voucher plan "embodies exactly the same principle as the GI bills that provide for educational benefits to military veterans. The veteran gets a voucher good only for educational expense and he is completely free to choose the school at which he uses it, provided that it satisfies certain standards." The Pell Grant, a need-based aid, like the Voucher, can only be used for authorized school expenses at qualified schools, and, like the Pell, the money follows the student, for use against those authorized expenses (not all expenses are covered).

Proponents are encouraged by private school sector growth, as they believe that private schools are typically more efficient at achieving results at a much lower per-pupil cost than public schools. A CATO Institute study of public and private school per pupil spending in Phoenix, Los Angeles, D.C., Chicago, New York City, and Houston found that public schools spend 93% more than estimated median private schools.

Proponents claim that institutions often are forced to operate more efficiently when they are made to compete and that any resulting job losses in the public sector would be offset by the increased demand for jobs in the private sector.

Friedrich von Hayek on the privatizing of education:

Other notable supporters include New Jersey Senator Cory Booker, former Governor of South Carolina Mark Sanford, billionaire and American philanthropist John T. Walton, Former Mayor of Baltimore Kurt L. Schmoke, Former Massachusetts Governor Mitt Romney and John McCain. A random survey of 210 Ph.D. holding members of the American Economic Association, found that over two-thirds of economists support giving parents educational vouchers that can be used at government-operated or privately operated schools, and that support is greater if the vouchers are to be used by parents with low-incomes or parents with children in poorly performing schools.

Another prominent proponent of the voucher system was Apple co-founder and CEO, Steve Jobs, who said:

As a practical matter, proponents note, most U.S. programs only offer poor families the same choice more affluent families already have, by providing them with the means to leave a failing school and attend one where the child can get an education. Because public schools are funded on a per-pupil basis, the money simply follows the child, but the cost to taxpayers is less because the voucher generally is less than the actual cost.

In addition, they say, the comparisons of public and private schools on average are meaningless. Vouchers usually are utilized by children in failing schools, so they can hardly be worse off even if the parents fail to choose a better school. Also, focusing on the effect on the public school suggests that is more important than the education of children.

Some proponents of school vouchers, including the Sutherland Institute and many supporters of the Utah voucher effort, see it as a remedy for the negative cultural impact caused by under-performing public schools, which falls disproportionately on demographic minorities. During the run-up to the November referendum election, Sutherland issued a controversial publication: Voucher, Vows, & Vexations. Sutherland called the publication an important review of the history of education in Utah, while critics just called it revisionist history. Sutherland then released a companion article in a law journal as part of an academic conference about school choice.

EdChoice, founded by Milton and Rose Friedman in 1996, is a non-profit organization that promotes universal school vouchers and other forms of school choice. In defense of vouchers, it cites empirical research showing that students who were randomly assigned to receive vouchers had higher academic outcomes than students who applied for vouchers but lost a random lottery and did not receive them; and that vouchers improve academic outcomes at public schools, reduce racial segregation, deliver better services to special education students, and do not drain money from public schools.

EdChoice also argues that education funding should belong to children, not a specific school type or building. Their purpose for the argument is to try to argue that people should prioritize a student's education and their opportunity over making a specific type of school better. They also emphasize that if a family chooses a public school, the funds also go to that school. This would mean that it would also benefit those who value the public education system.

The main critique of school vouchers and education tax credits is that they put public education in competition with private education, threatening to reduce and reallocate public school funding to private schools. Opponents question the belief that private schools are more efficient.

Public school teachers and teacher unions have also fought against school vouchers. In the United States, public school teacher unions, most notably the National Education Association (the largest labor union in the USA), argue that school vouchers erode educational standards and reduce funding, and that giving money to parents who choose to send their child to a religious or other school is unconstitutional. The latter issue was struck down by the Supreme Court case "Zelman v. Simmons-Harris", which upheld Ohio's voucher plan in a 5-4 ruling. In contrast, the use of public school funding for vouchers to private schools was disallowed by the Louisiana Supreme Court in 2013. The Louisiana Supreme Court did not declare vouchers unconstitutional, just the use of money earmarked for public schools via the Louisiana Constitution for funding Louisiana's voucher program. The National Education Association also points out that access to vouchers is just like "a chance in a lottery" where parents had to be lucky in order to get a space in this program. Since almost all students and their families would like to choose the best schools, those schools, as a result, quickly reach its maximum capacity number for students that state law permits. Those who did not get vouchers then have to compete again to look for some other less preferred and competitive schools or give up searching and go back to their assigned local schools. Jonathan Kozol, a prominent public school reform thinker and former public school teacher called vouchers the "single worst, most dangerous idea to have entered education discourse in my adult life".

The National Education Association additionally argues that more money should go towards the Public Education to help the schools struggling and improve the schools overall, instead of reducing the public school's fund to go towards school vouchers. Their argument claims that increasing that amount of money that goes towards public education would also increase the amount of resources put into public schools, therefore, improving the education. This argument made towards school vouchers reflect the way the organization values public education. For example, in an interview in May 2017 regarding Donald Trump's 2018 Budget Proposal, the organization's president, Lily Eskelsen García, claimed:

"We should invest in what makes schools great, the things that build curiosity and instill a love of learning. That is what every student deserves and what every parent wants for his or her child. It should not depend on how much their parents make, what language they speak at home, and certainly, not what neighborhood they live in." -National Education Association President Lily Eskelsen García.

Furthermore, there are multiple studies that support the arguments made by opponents of school vouchers. One of these studies, conducted by the Tulane University's Education Research Alliance, consists of observing the relationship between voucher programs and student's test scores. They found that students in the Louisiana voucher program initially had lower test scores, but after three years, their scores matched those of students who stayed in public schools from standardized test scores spanning from 2012 to 2015.

People who can benefit from vouchers may not know it. In April 2012, a bill passed in Louisiana that made vouchers available to low-income families whose children attended poorly ranked schools. A student whose household income was low (up to about $44,000 for a family of three) who attended a school ranked "C", "D", or "F" could apply for vouchers to attend another school. Of the estimated 380,000 eligible students during the school year when the bill was passed (2012/13), only 5,000 students knew about and applied for the vouchers, and accepted them.

In 2006, the United States Department of Education released a report concluding that average test scores for reading and mathematics, when adjusted for student and school characteristics, tend to be very similar among public schools and private schools. Private schools performed significantly better than public schools only if results were not adjusted for factors such as race, gender, and free or reduced price lunch program eligibility. Other research questions assumptions that large improvements would result from a more comprehensive voucher system.

Given the limited budget for schools, it is claimed that a voucher system would weaken public schools while not providing enough money for people to attend private schools. 76% of the money given in Arizona's voucher program went to children already in private schools.

Some sources claim that public schools' higher per-pupil spending is due to having a higher proportion of students with behavioral, physical and emotional problems, since in the United States, public schools must by law accept any student regardless of race, gender, religion, disability, educational aptitude, and so forth, while private schools are not so bound. They argue that some, if not all, of the cost difference between public and private schools comes from "cream skimming", whereby the private schools select only those students who belong to a preferred group – whether economic, religious, educational aptitude level, or ethnicity – rather than from differences in administration. The end result, it has been argued, is that a voucher system has led or would lead students who do not belong to the private schools' preferred groupings to become concentrated at public schools. However, of the ten state-run voucher programs in the United States at the beginning of 2011, four targeted low-income students, two targeted students in failing schools, and six targeted students with special needs. (Louisiana ran a single program targeting all three groups.)

It is also argued that voucher programs are often implemented without the necessary safeguards that prevent institutions from discriminating against marginalized communities. In the United States, as of 2016, there are currently no state laws that require voucher programs to not discriminate against marginalized communities. Further, while some voucher programs may explicitly be aimed at marginalized communities, this is not necessarily always the case. A common argument for school vouchers is that it allows for marginalized communities of color to be uplifted from poverty. Historically, however, data suggests that voucher programs have been used to further segregate Americans. Further, some data has shown that the effects of voucher programs such as the New York City School Choice Scholarship Program, are marginal when it comes to increasing student achievement.

Another argument against a school voucher system is its lack of accountability to taxpayers. In many states, members of a community's board of education are elected by voters. Similarly, a school budget faces a referendum. Meetings of the Board of Education must be announced in advance, and members of the public are permitted to voice their concerns directly to board members. By contrast, although vouchers may be used in private and religious schools, taxpayers cannot vote on budget issues, elect members of the board or even attend board meetings. Kevin Welner points out that vouchers funded through a convoluted tax credit system—a policy he calls "neovouchers"—present additional accountability concerns. With neovoucher systems, a taxpayer owing money to the state instead donates that money to a private, nonprofit organization. That organization then bundles donations and gives them to parents as vouchers to be used for private school tuition. The state then steps in and forgives (through a tax credit) some or all of the taxes that the donor has given to the organization. While conventional tax credit systems are structured to treat all private school participants equally, neovoucher systems effectively delegate to individual private taxpayers (those owing money to the state) the power to decide which private schools will benefit.

An example of lack of accountability is the voucher situation in Louisiana. In 2012, Louisiana State Superintendent of Education John White selected private schools to receive vouchers, then tried to fabricate criteria (including site visits) after schools had already received approval letters. One school of note, New Living Word in Ruston, Louisiana, did not have sufficient facilities for the over-300 students White and the state board of education had approved. Following a voucher audit in 2013, New Living Word had overcharged the state $395,000. White referred to the incident as a "lone substantive issue". However, most voucher schools did not undergo a complete audit for not having a separate checking account for state voucher money.

According to Susanne Wiborg, an expert on comparative education, Sweden's voucher system introduced in 1992 has "augmented social and ethnic segregation, particularly in relation to schools in deprived areas".

Tax-credit scholarships which are in most part disbursed to current private school students or to families which made substantial donations to the scholarship fund, rather than to low-income students attempting to escape from failing schools, amount to nothing more than a mechanism to use public funds in the form of foregone taxes to support private, often religiously based, private schools.

The school voucher question in the United States has also received a considerable amount of judicial review in the early 2000s.

A program launched in the city of Cleveland in 1995 and authorized by the state of Ohio was challenged in court on the grounds that it violated both the federal constitutional principle of separation of church and state and the guarantee of religious liberty in the Ohio Constitution. These claims were rejected by the Ohio Supreme Court, but the federal claims were upheld by the local federal district court and by the Sixth Circuit appeals court. The fact that nearly all of the families using vouchers attended Catholic schools in the Cleveland area was cited in the decisions.

This was later reversed during 2002 in a landmark case before the US Supreme Court, "Zelman v. Simmons-Harris", in which the divided court, in a 5–4 decision, ruled the Ohio school voucher plan constitutional and removed any constitutional barriers to similar voucher plans in the future, with conservative justices Anthony Kennedy, Sandra Day O'Connor, William Rehnquist, Antonin Scalia, and Clarence Thomas in the majority.

Chief Justice William Rehnquist, writing for the majority, stated that "The incidental advancement of a religious mission, or the perceived endorsement of a religious message, is reasonably attributable to the individual aid recipients not the government, whose role ends with the disbursement of benefits." The Supreme Court ruled that the Ohio program did not violate the Establishment Clause, because it passed a five-part test developed by the Court in this case, titled the Private Choice Test.

Dissenting opinions included Justice Stevens's, who wrote "...the voluntary character of the private choice to prefer a parochial education over an education in the public school system seems to me quite irrelevant to the question whether the government's choice to pay for religious indoctrination is constitutionally permissible." and Justice Souter's, whose opinion questioned how the Court could keep "Everson v. Board of Education" on as precedent and decide this case in the way they did, feeling it was contradictory. He also found that religious instruction and secular education could not be separated and this itself violated the Establishment Clause.

In 2006, the Florida Supreme Court struck down legislation known as the Florida Opportunity Scholarship Program (OSP), which would have implemented a system of school vouchers in Florida. The court ruled that the OSP violated article IX, section 1(a) of the Florida Constitution: "Adequate provision shall be made by law for a uniform, efficient, safe, secure, and high quality system of free public schools." This decision was criticized by Clark Neily, Institute for Justice senior attorney and legal counsel to Pensacola families using Florida Opportunity Scholarships, as, "educational policymaking".

Political support for school vouchers in the United States is mixed. On the left/right spectrum, conservatives are more likely to support vouchers. Some state legislatures have enacted voucher laws. In New Mexico, then-Republican Gary Johnson made school voucher provision the major issue of his second term as Governor. As of 2006, the federal government operates the largest voucher program, for evacuees from the region affected by Hurricane Katrina. The Federal government provided a voucher program for 7,500 residents of Washington, D.C. - the D.C. Opportunity Scholarship Program. until in early March 2009 congressional Democrats were moving to close down the program and remove children from their voucher-funded school places at the end of the 2009/10 school year under the $410 billion Omnibus Appropriations Act of 2009 which, as of March 7 had passed the House and was pending in the Senate. The Obama administration stated that it preferred to allow children already enrolled in the program to finish their schooling while closing the program to new entrants. However, its preference on this matter does not appear to be strong enough to prevent the President from signing the Bill.

Whether or not the public generally supports vouchers is debatable. Majorities seem to favor improving existing schools over providing vouchers, yet as many as 40% of those surveyed admit that they do not know enough to form an opinion or do not understand the system of school vouchers.

In November 2000, a voucher system proposed by Tim Draper was placed on the California ballot as Proposition 38. It was unusual among school voucher proposals in that it required neither accreditation on the part of schools accepting vouchers, nor proof of need on the part of families applying for them; neither did it have any requirement that schools accept vouchers as payment-in-full, nor any other provision to guarantee a reduction in the real cost of private school tuition. The measure was defeated by a final percentage tally of 70.6 to 29.4.

A statewide universal school voucher system providing a maximum tuition subsidy of $3,000 was passed in Utah in 2007, but 62% of voters repealed it in a statewide referendum before it took effect. On April 27, 2011 Indiana passed a statewide voucher program, the largest in the U.S. It offers up to $4,500 to students with household incomes under $41,000, and lesser benefits to households with higher incomes. The vouchers can be used to fund a variety of education options outside the public school system. In March 2013, the Indiana Supreme Court found that the program does not violate the state constitution.

President Donald Trump proposed a 2018 budget that includes $250 million for voucher initiatives, which are state-funded programs that pay for students to go to private school. This 2018 budget served the purpose of, "Expanding school choice, ensuring more children have an equal opportunity to receive a great education, maintaining strong support for the Nation's most vulnerable students, simplifying funding for post secondary education, continuing to build evidence around educational innovation, and eliminating or reducing Department programs consistent with the limited Federal role in education." The Budget reduces more than 30 programs that duplicate other programs, which are ineffective; or are more appropriately supported with State, local, or private funds. Another $1 billion is set aside for encouraging schools to adopt school choice-friendly policies.
Betsy DeVos, Trump's education secretary, is also an advocate for voucher programs, and has argued that they would lead to better educational outcomes for students. Both Trump and DeVos want to propose cutting the Education Department's budget by about $3.6 billion and spend more than $1 billion on private school vouchers and other school choice plans.

DeVos makes a statement regarding the purpose and importance of the budget. DeVos claims:

"This budget makes an historic investment in America's students. President Trump is committed to ensuring the Department focuses on returning decision-making power back to the States, where it belongs, and on giving parents more control over their child's education. By refocusing the Department's funding priorities on supporting students, we can usher in a new era of creativity and ingenuity and lay a new foundation for American greatness." – Betsy DeVos, U.S. Secretary of Education

Some private religious schools in voucher programs teach creationism instead of the theory of evolution, including religious schools that teach religious theology side-by-side with or in place of science. Over 300 schools in the US have been documented as teaching creation and receive taxpayer money. Contrary to popular belief , a strict definition of state-funded religious education was narrowly deemed constitutional in "Zelman v. Simmons-Harris" (2002). However, currently 35 states have passed various Blaine Amendments restricting or prohibiting public funding of religious education.




</doc>
<doc id="9751" url="https://en.wikipedia.org/wiki?curid=9751" title="E. B. White">
E. B. White

Elwyn Brooks White (July 11, 1899 – October 1, 1985) was an American writer. He was the author of several highly popular books for children, including "Stuart Little" (1945), "Charlotte's Web" (1952), and "The Trumpet of the Swan" (1970). In a 2012 survey of "School Library Journal" readers, "Charlotte's Web" came in first in their poll of the top one hundred children's novels. In addition, he was a writer and contributing editor to "The New Yorker" magazine, and also a co-author of the English language style guide "The Elements of Style."

E.B. White was born in Mount Vernon, New York, the sixth and youngest child of Samuel Tilly White, the president of a piano firm, and Jessie Hart White, the daughter of Scottish-American painter William Hart. Elwyn's older brother Stanley Hart White, known as Stan, a professor of landscape architecture and the inventor of the Vertical Garden, taught E. B. White to read and to explore the natural world. White graduated from Cornell University with a bachelor of arts degree in 1921. He got the nickname "Andy" at Cornell, where tradition confers that moniker on any male student whose surname is White, after Cornell co-founder Andrew Dickson White. While at Cornell, he worked as editor of "The Cornell Daily Sun" with classmate Allison Danzig, who later became a sportswriter for "The New York Times". White was also a member of the Aleph Samach and Quill and Dagger societies and Phi Gamma Delta ("Fiji") fraternity.

After graduation, White worked for the United Press (now United Press International) and the American Legion News Service in 1921 and 1922. From September 1922 to June 1923, he was a cub reporter for "The Seattle Times". On one occasion, when White was stuck writing a story, a Times editor said, "Just say the words." He was fired from the "Times" and later wrote for the "Seattle Post-Intelligencer" before a stint in Alaska on a fireboat. He then worked for almost two years with the Frank Seaman advertising agency as a production assistant and copywriter before returning to New York City in 1924. When "The New Yorker" was founded in 1925, White submitted manuscripts to it. Katharine Angell, the literary editor, recommended to editor-in-chief and founder Harold Ross that White be hired as a staff writer. However, it took months to convince him to come to a meeting at the office and additional weeks to convince him to work on the premises. Eventually, he agreed to work in the office on Thursdays.

White was shy around women, claiming he had "too small a heart, too large a pen." But in 1929, culminating an affair which led to her divorce, White and Katherine Angell were married. They had a son, Joel White, a naval architect and boat builder, who later owned Brooklin Boat Yard in Brooklin, Maine. Katharine's son from her first marriage, Roger Angell, has spent decades as a fiction editor for "The New Yorker" and is well known as the magazine's baseball writer.

In her foreword to "Charlotte's Web", Kate DiCamillo quotes White as saying, "All that I hope to say in books, all that I ever hope to say, is that I love the world." White also loved animals, farms and farming implements, seasons, and weather formats.

James Thurber described White as a quiet man who disliked publicity and who, during his time at "The New Yorker", would slip out of his office via the fire escape to a nearby branch of Schrafft's to avoid visitors whom he didn't know. 

White had Alzheimer's disease and died on October 1, 1985, at his farm home in North Brooklin, Maine. He is buried in the Brooklin Cemetery beside Katharine, who died in 1977.

E. B. White published his first article in "The New Yorker" in 1925, then joined the staff in 1927 and continued to contribute for almost six decades. Best recognized for his essays and unsigned "Notes and Comment" pieces, he gradually became the magazine's most important contributor. From the beginning to the end of his career at "The New Yorker," he frequently provided what the magazine calls "Newsbreaks" (short, witty comments on oddly worded printed items from many sources) under various categories such as "Block That Metaphor." He also was a columnist for "Harper's Magazine" from 1938 to 1943.

In 1949, White published "Here Is New York", a short book based on an article he had been commissioned to write for "Holiday". Editor Ted Patrick approached White about writing the essay telling him it would be fun. "Writing is never 'fun'", replied White. That article reflects the writer's appreciation of a city that provides its residents with both "the gift of loneliness and the gift of privacy." It concludes with a dark note touching on the forces that could destroy the city that he loved. This prescient "love letter" to the city was re-published in 1999 on his centennial with an introduction by his stepson, Roger Angell.

In 1959, White edited and updated "The Elements of Style". This handbook of grammatical and stylistic guidance for writers of American English was first written and published in 1918 by William Strunk Jr., one of White's professors at Cornell. White's reworking of the book was extremely well received, and later editions followed in 1972, 1979, and 1999. Maira Kalman illustrated an edition in 2005. That same year, a New York composer named Nico Muhly premiered a short opera based on the book. The volume is a standard tool for students and writers and remains required reading in many composition classes. The complete history of "The Elements of Style "is detailed in Mark Garvey's "Stylized: A Slightly Obsessive History of Strunk & White's The Elements of Style".

In 1978, White won a special Pulitzer Prize citing "his letters, essays and the full body of his work". He also received the Presidential Medal of Freedom in 1963 and honorary memberships in a variety of literary societies throughout the United States. The 1973 Oscar-nominated Canadian animated short "The Family That Dwelt Apart" is narrated by White and is based on his short story of the same name.

In the late 1930s, White turned his hand to children's fiction on behalf of a niece, Janice Hart White. His first children's book, "Stuart Little", was published in 1945, and "Charlotte's Web" followed in 1952. "Stuart Little" initially received a lukewarm welcome from the literary community. However, both books went on to receive high acclaim, and "Charlotte's Web" won a Newbery Honor from the American Library Association, though it lost to "Secret of the Andes" by Ann Nolan Clark.

White received the Laura Ingalls Wilder Medal from the U.S. professional children's librarians in 1970. It recognized his "substantial and lasting contributions to children's literature." That year, he was also the U.S. nominee and eventual runner-up for the biennial Hans Christian Andersen Award, as he was again in 1976. Also, in 1970, White's third children's novel was published, "The Trumpet of the Swan". In 1973 it won the Sequoyah Award from Oklahoma and the William Allen White Award from Kansas, both selected by students voting for their favorite book of the year. In 2012, the "School Library Journal" sponsored a survey of readers, which identified "Charlotte's Web" as the best children's novel ("fictional title for readers 9–12" years old). The librarian who conducted it said, "It is impossible to conduct a poll of this sort and expect [White's novel] to be anywhere but #1."


The E.B. White Read Aloud Award is given by The Association of Booksellers for Children (ABC) to honor books that its membership feel embodies the universal read-aloud standards that E. B. White's works created.





</doc>
<doc id="9752" url="https://en.wikipedia.org/wiki?curid=9752" title="Evangelist (Latter Day Saints)">
Evangelist (Latter Day Saints)

In the Latter Day Saint movement, an evangelist is an ordained office of the ministry. In some denominations of the movement, an evangelist is referred to as a patriarch. However, the latter term was deprecated by the Community of Christ after the church began ordaining women to the priesthood. Other denominations, such as The Church of Jesus Christ (Bickertonite), have an evangelist position independent of the original "patriarch" office instituted movement founder Joseph Smith.

The first use of the term "evangelist" in Latter Day Saint theology were mainly consistent with how the term is used by Protestants and Catholics.

In 1833, Joseph Smith introduced the new office of Patriarch, to which he ordained his father. The elder Smith was given the "keys of the patriarchal Priesthood over the kingdom of God on earth", the same power said to be held by the Biblical Patriarchs, which included the power to give blessings upon one's posterity. The elder Smith, however, was also called to give patriarchal blessings to the fatherless within the church, and the church as a whole, a calling he passed onto his eldest surviving son Hyrum Smith prior to his death. Hyrum himself was killed in 1844 along with Joseph, resulting in a succession crisis that broke the Latter Day Saint movement into multiple denominations.

It is not known who first identified the term "evangelist" with the office of patriarch. However, in an 1835 church publication, W. W. Phelps stated,

In 1839, Joseph Smith equated an evangelist with the office of patriarch, stating that "an Evangelist is an Patriarch".

The necessity of an evangelist in the church organization has been reinforced repeatedly, based on the passage in Ephesians 4:11, which states, "And he gave some, apostles; and some, prophets; and some, evangelists; and some, pastors and teachers". In 1834, while writing what he called the "principles of salvation", prominent early Latter Day Saint Oliver Cowdery stated that:

Joseph Smith echoed Cowdery's statement in 1842, in a letter to a Chicago newspaper editor outlining the church's basic beliefs. Smith said that his religion "believe[s] in the same organization that existed in the primitive church, viz: apostles, prophets, pastors, teachers, evangelists".

In the Community of Christ, which was formerly known as the Reorganized Church of Jesus Christ of Latter Day Saints (RLDS Church), an evangelist is an office in the Melchizedec Order of the priesthood. The evangelist was originally called an evangelist-patriarch. This name derived from Latter Day Saint founder Joseph Smith's statement that "an Evangelist is a Patriarch. ... Wherever the Church of Christ is established in the earth, there should be a Patriarch for the benefit of the posterity of the Saints".

An evangelist-patriarch's primary responsibility was to provide special blessings to members of the church; these blessings were considered one of the eight sacraments in the RLDS Church. The local evangelist–patriarchs of the church were governed by an individual with church-wide authority known as the Presiding Patriarch.

In 1984, when the first women began to be ordained to the office of evangelist-patriarch, the RLDS Church changed the title of the local evangelist-patriarchs to simply "evangelist". Similarly, it changed the title of the Presiding Patriarch to the "Presiding Evangelist". To be an evangelist, a person must also be a high priest of the Melchizedec Order of the priesthood.

The primary duty of an evangelist in the Community of Christ remains the giving of sacramental "evangelist's blessings"; it is for this reason that evangelists are often referred to as "ministers of blessing". Ideally, an evangelist is free from administrative responsibilities in the church in order to allow them to be fully responsive to the Holy Spirit. Their blessings—which are given by the laying on of hands—provide counsel and advice and confer spiritual blessings upon the recipient. Evangelist's blessings may or may not be recorded. If it is recorded, a copy is stored in the church archives at Independence, Missouri. A recipient may receive multiple evangelist's blessings in their life.

All evangelists belong to the Order of Evangelists, which is directed by the Presiding Evangelist (currently Jane M. Gardner, since 2016).

In The Church of Jesus Christ (Bickertonite), the prescribed duties of an evangelist are to preach the gospel of Jesus Christ to every nation, kindred, language, and people. An evangelist is part of the Quorum of Seventy Evangelists.

The Quorum of Seventy Evangelists is responsible for management of the International Missionary Programs of the church and assists Regions of the church with their individual Domestic Missionary Programs. The Quorum of Seventy oversees the activities of its Missionary Operating Committees to ensure the fulfilling of Christ’s commandment to take the gospel to the entire world.

In 2007, the officers of the Quorum of Seventy Evangelists were:


In The Church of Jesus Christ of Latter-day Saints (LDS Church), an evangelist is considered to be an office of the Melchizedek priesthood. However, the term "evangelist" is rarely used for this position; instead, the church has retained the term "patriarch", the term most commonly used by Joseph Smith.

The most prominent reference to the term "evangelist" in the LDS Church's literature is found in its "Articles of Faith", derived from the Wentworth letter—a statement by Smith in 1842 to a Chicago newspaper editor—that the church believes in "the same organization that existed in the primitive church", including "evangelists". Smith taught that "an Evangelist is an Patriarch".



</doc>
<doc id="9755" url="https://en.wikipedia.org/wiki?curid=9755" title="Elegiac couplet">
Elegiac couplet

The elegiac couplet is a poetic form used by Greek lyric poets for a variety of themes usually of smaller scale than the epic. Roman poets, particularly Catullus, Propertius, Tibullus, and Ovid, adopted the same form in Latin many years later. As with the English heroic, each couplet usually makes sense on its own, while forming part of a larger work.

Each couplet consists of a hexameter verse followed by a pentameter verse. The following is a graphic representation of its scansion:

The form was felt by the ancients to contrast the rising action of the first verse with a falling quality in the second. The sentiment is summarized in a line from Ovid's "Amores" I.1.27 "Sex mihi surgat opus numeris, in quinque residat"—"Let my work rise in six steps, fall back in five." The effect is illustrated by Coleridge as:
translating Schiller,

The elegiac couplet is presumed to be the oldest Greek form of epodic poetry (a form where a later verse is sung in response or comment to a previous one). Scholars, who even in the past did not know who created it, theorize the form was originally used in Ionian dirges, with the name "elegy" derived from the Greek "ε, λεγε ε, λεγε"—"Woe, cry woe, cry!" Hence, the form was used initially for funeral songs, typically accompanied by an aulos, a double-reed instrument. Archilochus expanded use of the form to treat other themes, such as war, travel, or homespun philosophy. Between Archilochus and other imitators, the verse form became a common poetic vehicle for conveying any strong emotion.

At the end of the 7th century BCE, Mimnermus of Colophon struck on the innovation of using the verse for erotic poetry. He composed several elegies celebrating his love for the flute girl Nanno, and though fragmentary today his poetry was clearly influential in the later Roman development of the form. Propertius, to cite one example, notes "Plus in amore valet Mimnermi versus Homero"—"The verse of Mimnermus is stronger in love than Homer".

The form continued to be popular throughout the Greek period and treated a number of different themes. Tyrtaeus composed elegies on a war theme, apparently for a Spartan audience. Theognis of Megara vented himself in couplets as an embittered aristocrat in a time of social change. Popular leaders were writers of elegy—Solon the lawgiver of Athens composed on political and ethical subjects—and even Plato and Aristotle dabbled with the meter.

By the Hellenistic period, the Alexandrian school made elegy its favorite and most highly developed form. They preferred the briefer style associated with elegy in contrast to the lengthier epic forms, and made it the singular medium for short epigrams. The founder of this school was Philitas of Cos. He was eclipsed only by the school's most admired exponent, Callimachus; their learned character and intricate art would have a heavy influence on the Romans.

Like many Greek forms, elegy was adapted by the Romans for their own literature. The fragments of Ennius contain a few couplets, and scattered verses attributed to Roman public figures like Cicero and Julius Caesar also survive.

But it is the elegists of the mid-to-late first century BCE who are most commonly associated with the distinctive Roman form of the elegiac couplet. Catullus, the first of these, is an invaluable link between the Alexandrine school and the subsequent elegies of Tibullus and Propertius a generation later. His collection, for example, shows a familiarity with the usual Alexandrine style of terse epigram and a wealth of mythological learning, while his 66th poem is a direct translation of Callimachus' "Coma Berenices". Arguably the most famous elegiac couplet in Latin is his two-line 85th poem "Odi et Amo":
Many people, particularly students of Latin, who read this poem aloud often miss the metre because of the high amount of elision in this poem.

Cornelius Gallus is another important statesman/writer of this period, one who was generally regarded by the ancients as among the greatest of the elegists. Other than a few scant lines, all of his work has been lost.

The form reached its zenith with the collections of Tibullus and Propertius and several collections of Ovid (the "Amores, Heroides, Tristia", and "Epistulae ex Ponto"). The vogue of elegy during this time is seen in the so-called 3rd and 4th books of Tibullus. Many poems in these books were clearly not written by Tibullus but by others, perhaps part of a circle under Tibullus' patron Mesalla. Notable in this collection are the poems of Sulpicia, among the few surviving works by Classical Latin female poets.

Through these poets—and in comparison with the earlier Catullus—it is possible to trace specific characteristics and evolutionary patterns in the Roman form of the verse:


Although no classical poet wrote collections of love elegies after Ovid, the verse retained its popularity as a vehicle for popular occasional poetry. Elegiac verses appear, for example, in Petronius' "Satyricon", and Martial's Epigrams uses it for many witty stand-alone couplets and for longer pieces. The trend continues through the remainder of the empire; short elegies appear in Apuleius's story "Psyche and Cupid" and the minor writings of Ausonius.

After the fall of the empire, one writer who produced elegiac verse was Maximianus. Various Christian writers also adopted the form; Venantius Fortunatus wrote some of his hymns in the meter, while later Alcuin and the Venerable Bede dabbled in the verse. The form also remained popular among the educated classes for gravestone epitaphs; many such epitaphs can be found in European cathedrals.

"De tribus puellis" is an example of a Latin "fabliau", a genre of comedy which employed elegiac couplets in imitation of Ovid. The medieval theorist John of Garland wrote that "all comedy is elegy, but the reverse is not true." Medieval Latin had a developed comedic genre known as elegiac comedy. Sometimes narrative, sometimes dramatic, it deviated from ancient practice because, as Ian Thompson writes, "no ancient drama would ever have been written in elegiacs."

With the Renaissance, more skilled writers interested in the revival of Roman culture took on the form in a way which attempted to recapture the spirit of the Augustan writers. The Dutch Latinist Johannes Secundus, for example, included Catullus-inspired love elegies in his "Liber Basiorum", while the English poet John Milton wrote several lengthy elegies throughout his career. This trend continued down through the Recent Latin writers, whose close study of their Augustan counterparts reflects their general attempts to apply the cultural and literary forms of the ancient world to contemporary themes.




</doc>
<doc id="9756" url="https://en.wikipedia.org/wiki?curid=9756" title="Exabyte">
Exabyte

The exabyte is a multiple of the unit byte for digital information. In the International System of Units (SI), the prefix "exa" indicates multiplication by the sixth power of 1000 (10). Therefore, one exabyte is one quintillion bytes (short scale). The unit symbol for the exabyte is EB.

A related unit, the exbibyte, using a binary prefix, is equal to (=, about 15% larger.


Allegedly, "all words ever spoken by human beings" could be stored in approximately 5 exabytes of data. This claim often cites a project at the UC Berkeley School of Information in support (although this project is now outdated and therefore not entirely accurate). The 2003 University of California, Berkeley, report credits the estimate to the website of Caltech researcher Roy Williams, where the statement can be found as early as May 1999. This statement has been criticized. Mark Liberman calculated the storage requirements for all human speech at 42 zettabytes (42,000 exabytes, and 8,400 times the original estimate) if digitized as 16 kHz 16-bit audio, although he did freely confess that "maybe the authors [of the exabyte estimate] were thinking about text".

Earlier studies from the University of California, Berkeley, estimated that by the end of 1999, the sum of human-produced information (including all audio, video recordings, and text/books) was about 12 exabytes of data. The 2003 Berkeley report stated that in 2002 alone, "telephone calls worldwide on both landlines and mobile phones contained 17.3 exabytes of new information if stored in digital form" and that "it would take 9.25 exabytes of storage to hold all U.S. [telephone] calls each year". International Data Corporation estimates that approximately 160 exabytes of digital information were created, captured, and replicated worldwide in 2006. Research from the University of Southern California estimates that the amount of data stored in the world by 2007 was 295 exabytes and the amount of information shared on two-way communications technology, such as cell phones, in 2007 as 65 exabytes.

The content of the Library of Congress is commonly estimated to hold 10 terabytes of data in all printed material. Recent estimates of the size including audio, video, and digital materials start at 3 petabytes to 20 petabytes. Therefore, one exabyte could hold a hundred thousand times the printed material or 50 to 300 times all the content of the Library of Congress.

In 2013, Randall Munroe compiled published assertions about Google's data centers, and estimated that the company has about 10 exabytes stored on disk, and additionally approximately 5 exabytes on tape backup. The company has not commented on Munroe's estimate.



</doc>
<doc id="9758" url="https://en.wikipedia.org/wiki?curid=9758" title="Era">
Era

An era is a span of time defined for the purposes of chronology or historiography, as in the regnal eras in the history of a given monarchy, a calendar era used for a given calendar, or the geological eras defined for the history of Earth.

Comparable terms are epoch, age, period, saeculum, aeon (Greek "aion") and Sanskrit yuga.

The word has been in use in English since 1615, and is derived from Late Latin "aera" "an era or epoch from which time is reckoned," probably identical to Latin "æra" "counters used for calculation," plural of "æs" "brass, money".

The Latin word use in chronology seems to have begun in 5th century Visigothic Spain, where it appears in the "History" of Isidore of Seville, and in later texts. The Spanish era is calculated from 38 BC, perhaps because of a tax (cfr. indiction) levied in that year, or due to a miscalculation of the Battle of Actium, which occurred in 31 BC.

Like epoch, "era" in English originally meant "the starting point of an age"; the meaning "system of chronological notation" is c.1646; that of "historical period" is 1741.
In chronology, an "era" is the highest level for the organization of the measurement of time. A "calendar era" indicates a span of many years which are numbered beginning at a specific reference date (epoch), which often marks the origin of a political state or cosmology, dynasty, ruler, the birth of a leader, or another significant historical or mythological event; it is generally called after its focus accordingly as in "Victorian era".

In large-scale natural science, there is need for another time perspective, independent from human activity, and indeed spanning a far longer period (mainly prehistoric), where "geologic era" refers to well-defined time spans.
The next-larger division of geologic time is the eon. The Phanerozoic Eon, for example, is subdivided into eras. There are currently three eras defined in the Phanerozoic; the following table lists them from youngest to oldest (BP is an abbreviation for "before present").

The older Proterozoic and Archean eons are also divided into eras.

For periods in the history of the universe, the term "epoch" is typically preferred, but "era" is used e.g. of the "Stelliferous Era".

Calendar eras count the years since a particular date (epoch), often one with religious significance. "Anno mundi" (year of the world) refers to a group of calendar eras based on a calculation of the age of the world, assuming it was created as described in the Book of Genesis. In Jewish religious contexts one of the versions is still used, and many Eastern Orthodox religious calendars used another version until 1728. Hebrew year 5772 AM began at sunset on 28 September 2011 and ended on 16 September 2012. In the Western church, "Anno Domini" ("AD" also written "CE"), counting the years since the birth of Jesus on traditional calculations, was always dominant.

The Islamic calendar, which also has variants, counts years from the Hijra or emigration of the Islamic prophet Muhammad from Mecca to Medina, which occurred in 622 AD. The Islamic year is some days shorter than 365; January 2012 fell in 1433 AH ("After Hijra").

For a time ranging from 1872 to the Second World War, the Japanese used the imperial year system ("kōki"), counting from the year when the legendary Emperor Jimmu founded Japan, which occurred in 660 BC.

Many Buddhist calendars count from the death of the Buddha, which according to the most commonly used calculations was in 545-543 BCE or 483 BCE. Dates are given as "BE" for "Buddhist Era"; 2000 AD was 2543 BE in the Thai solar calendar.

Other calendar eras of the past counted from political events, such as the Seleucid era and the Ancient Roman "ab urbe condita" ("AUC"), counting from the foundation of the city.

The word era also denotes the units used under a different, more arbitrary system where time is not represented as an endless continuum with a single reference year, but each unit starts counting from one again as if time starts again. The use of regnal years is a rather impractical system, and a challenge for historians if a single piece of the historical chronology is missing, and often reflects the preponderance in public life of an absolute ruler in many ancient cultures. Such traditions sometimes outlive the political power of the throne, and may even be based on mythological events or rulers who may not have existed (for example Rome numbering from the rule of Romulus and Remus). In a manner of speaking the use of the supposed date of the birth of Christ as a base year is a form of an era.

In East Asia, each emperor's reign may be subdivided into several reign periods, each being treated as a new era. The name of each was a motto or slogan chosen by the emperor. Different East Asian countries utilized slightly different systems, notably:


A similar practice survived in the United Kingdom until quite recently, but only for formal official writings: in daily life the ordinary year A.D. has been used for a long time, but Acts of Parliament were dated according to the years of the reign of the current monarch, so that "61 & 62 Vict c. 37" refers to the Local Government (Ireland) Act 1898 passed in the session of Parliament in the 61st/62nd year of the reign of Queen Victoria.

"Era" can be used to refer to well-defined periods in historiography, such as the Roman era, Elizabethan era, Victorian era, etc.
Use of the term for more recent periods or topical history might include Soviet era, and "" in the history of modern popular music, such as the "big band era", "disco era", etc.



</doc>
<doc id="9760" url="https://en.wikipedia.org/wiki?curid=9760" title="Eschatology">
Eschatology

Eschatology is a part of theology concerned with the final events of history, or the ultimate destiny of humanity. This concept is commonly referred to as the "end of the world" or "end times".

The word arises from the Greek "eschatos" meaning "last" and "-logy" meaning "the study of", and first appeared in English around 1844. The "Oxford English Dictionary" defines eschatology as "the part of theology concerned with death, judgment, and the final destiny of the soul and of humankind".

In the context of mysticism, the term refers metaphorically to the end of ordinary reality and to reunion with the Divine. Many religions treat eschatology as a future event prophesied in sacred texts or in folklore.

Most modern eschatology and apocalypticism, both religious and secular, involves the violent disruption or destruction of the world; Christian and Jewish eschatologies view the end times as the consummation or perfection of God's creation of the world, albeit with violent overtures, such as the Great Tribulation. For example, according to some ancient Hebrew worldviews, reality unfolds along a linear path (or rather, a spiral path, with cyclical components that nonetheless have a linear trajectory); the world began with God and is ultimately headed toward God's final goal for creation, the world to come.

Eschatologies vary as to their degree of optimism or pessimism about the future. In some eschatologies, conditions are better for some and worse for others, e.g. "heaven and hell". They also vary as to time frames. Groups claiming "imminent" eschatology are also referred to as doomsday cults.

In Bahá'í belief, creation has neither a beginning nor an end; Bahá'ís regard the eschatologies of other religions as symbolic. In Bahá'í belief, human time is marked by a series of progressive revelations in which successive messengers or prophets come from God. The coming of each of these messengers is seen as the day of judgment to the adherents of the previous religion, who may choose to accept the new messenger and enter the "heaven" of belief, or denounce the new messenger and enter the "hell" of denial. In this view, the terms "heaven" and "hell" become symbolic terms for a person's spiritual progress and their nearness to or distance from God. In Bahá'í belief, the coming of Bahá'u'lláh (1817-1892), the founder of the Bahá'í Faith, signals the fulfilment of previous eschatological expectations of Islam, Christianity and other major religions.

Christian eschatology is the study concerned with the ultimate destiny of the individual soul and of the entire created order, based primarily upon biblical texts within the Old and New Testaments.

Christian eschatological research looks to study and discuss matters such as the nature of the Divine and the divine nature of Jesus Christ, death and the afterlife, Heaven and Hell, the Second Coming of Jesus, the resurrection of the dead, the Rapture, the Tribulation, Millennialism, the end of the world, the Last Judgment, and the New Heaven and New Earth in the world to come.

Eschatological passages occur in many places in the Bible, in both the Old and the New Testaments. In the Old Testament, apocalyptic eschatology can be found notably in Isaiah 24–27, Isaiah 56–66, Joel, Zechariah 9–14 as well as in the closing chapters of Daniel, and in Ezekiel. In the New Testament, applicable passages include Matthew 24, Mark 13, the parable of "The Sheep and the Goats" and the Book of Revelation — Revelation often occupies a central place in Christian eschatology.

The Second Coming of Christ is the central event in Christian eschatology within the broader context of the fullness of the Kingdom of God. Most Christians believe that death and suffering will continue to exist until Christ's return. There are, however, various views concerning the order and significance of other eschatological events.

The Book of Revelation stands at the core of much of Christian eschatology. The study of Revelation is usually divided into four interpretative methodologies or hermeneutics:


The Vaishnavite tradition links contemporary Hindu eschatology to the figure of Kalki, the tenth and last avatar of Vishnu. Before the age draws to a close Kalki will reincarnate as Shiva and simultaneously dissolve and regenerate the universe.

Most Hindus believe that the current period is the Kali Yuga, the last of four "Yuga" that make up the current age. Each period has seen successive degeneration in the moral order, to the point that in the Kali Yuga quarrel and hypocrisy are the norm. In Hinduism, time is cyclic, consisting of cycles or "kalpas". Each kalpa lasts for 4.32 billion years and is followed by a pralaya of equal length, which together makes one full day and night of Brahma's 100 360-year lifespan, who lives for 311 trillion, 40 billion years. The cycle of birth, growth, decay, and renewal at the individual level finds its echo in the cosmic order, yet is affected by vagaries of divine intervention in Vaishnavite belief.

Some Shaivites hold the view that Shiva is incessantly destroying and creating the world.

The sayings of the Prophet Muhammad regarding the Signs of the Day of Judgement document Islamic eschatology. 

The Prophet's sayings on the subject have been traditionally divided into Major and Minor Signs. He spoke about several Minor Signs of the approach of the Day of Judgment, including:

Regarding the Major Signs, a Companion of the Prophet narrated: "Once we were sitting together and talking amongst ourselves when the Prophet appeared. He asked us what it was we were discussing. We said it was the Day of Judgment. He said: 'It will not be called until ten signs have appeared: Smoke(Ad Dukhan), Dajjal (the Antichrist), the creature (that will wound the people), the rising of the sun in the West, the Second Coming of Jesus, the emergence of Gog and Magog, and three sinkings (or cavings in of the earth): one in the East, another in the West and a third in the Arabian Peninsula.'" (note: the previous events were not listed in the chronological order of appearance)

Jewish eschatology discusses events that will happen in the end of days, according to the Hebrew Bible and Jewish thought. This includes the ingathering of the exiled diaspora, the coming of the Jewish Messiah, afterlife, and the revival of the dead Tzadikim.

Judaism usually refers to the end times as the "end of days" ("aḥarit ha-yamim", אחרית הימים), a phrase that appears several times in the Tanakh. The idea of a messianic age has a prominent place in Jewish thought and is incorporated as part of the end of days.

Judaism addresses the end times in the Book of Daniel and in numerous other prophetic passages in the Hebrew scriptures, and also in the Talmud, particularly Tractate Avodah Zarah.

Frashokereti is the Zoroastrian doctrine of a final renovation of the universe when evil will be destroyed, and everything else will then be in perfect unity with God (Ahura Mazda). The doctrinal premises are:

Researchers in futures studies and transhumanists investigate how the accelerating rate of scientific progress may lead to a "technological singularity" in the future that would profoundly and unpredictably change the course of human history, and result in "Homo sapiens" no longer being the dominant life form on Earth.

Occasionally the term "physical eschatology" is applied to the long-term predictions of astrophysics. The Sun will turn into a red giant in approximately 6 billion years. Life on Earth will become impossible due to a rise in temperature long before the planet is actually swallowed up by the Sun. Even later, the Sun will become a white dwarf.




</doc>
<doc id="9762" url="https://en.wikipedia.org/wiki?curid=9762" title="Ecumenical council">
Ecumenical council

An ecumenical council (or oecumenical council; also general council) is a conference of ecclesiastical dignitaries and theological experts convened to discuss and settle matters of Church doctrine and practice in which those entitled to vote are convoked from the whole world (oikoumene) and which secures the approbation of the whole Church.

The word "ecumenical" derives from the Late Latin "oecumenicus" "general, universal", from Greek "oikoumenikos" "from the whole world", from "he oikoumene ge" "the inhabited world (as known to the ancient Greeks); the Greeks and their neighbors considered as developed human society (as opposed to barbarian lands)", in later use "the Roman world" and in the Christian sense in ecclesiastical Greek, from "oikoumenos", present passive participle of "oikein" "inhabit", from "oikos" "house, habitation." The first seven ecumenical councils, recognised by both the eastern and western denominations comprising Chalcedonian Christianity, were convoked by Roman Emperors, who also enforced the decisions of those councils within the state church of the Roman Empire.

Starting with the third ecumenical council, noteworthy schisms led to non-participation by some members of what had previously been considered a single Christian Church. Thus, some parts of Christianity did not attend later councils, or attended but did not accept the results. Bishops belonging to what became known as the Eastern Orthodox Church accept only seven ecumenical councils, as described below. Bishops belonging to what became known as the Church of the East only participated in the first two councils. Bishops belonging to what became known as Oriental Orthodoxy participated in the first four councils, but rejected the decisions of the fourth and did not attend any subsequent ecumenical councils.

Acceptance of councils as ecumenical and authoritative varies between different Christian denominations. Disputes over Christological and other questions have led certain branches to reject some councils that others accept.

The Church of the East (accused by others of adhering to Nestorianism) accepts as ecumenical only the first two councils. Oriental Orthodox Churches accept the first three. Both the Eastern Orthodox Church and Catholic Church recognise as ecumenical the first seven councils, held from the 4th to the 9th centuries. While the Eastern Orthodox Church accepts no later council or synod as ecumenical, the Catholic Church continues to hold general councils of the bishops in full communion with the Pope, reckoning them as ecumenical. In all, the Catholic Church recognises twenty-one councils as ecumenical. Anglicans and confessional Protestants accept either the first seven or the first four as ecumenical councils.

The doctrine of the "infallibility of ecumenical councils" states that solemn definitions of ecumenical councils, which concern faith or morals, and to which the whole Church must adhere, are infallible. Such decrees are often labeled as 'Canons' and they often have an attached anathema, a penalty of excommunication, against those who refuse to believe the teaching. The doctrine does not claim that every aspect of every ecumenical council is dogmatic, but that every aspect of an ecumenical council is free of errors or is indefectible.

Both the Eastern Orthodox and the Catholic churches uphold versions of this doctrine. However, the Catholic Church holds that solemn definitions of ecumenical councils meet the conditions of infallibility only when approved by the Pope, while the Eastern Orthodox Church holds that an ecumenical council is itself infallible when pronouncing on a specific matter.

Protestant churches would generally view ecumenical councils as fallible human institutions that have no more than a derived authority to the extent that they correctly expound Scripture (as most would generally consider occurred with the first four councils in regard to their dogmatic decisions).

Church councils were, from the beginning, bureaucratic exercises. Written documents were circulated, speeches made and responded to, votes taken, and final documents published and distributed. A large part of what is known about the beliefs of heresies comes from the documents quoted in councils in order to be refuted, or indeed only from the deductions based on the refutations.

Most councils dealt not only with doctrinal but also with disciplinary matters, which were decided in "canons" ("laws"). Study of the canons of church councils is the foundation of the development of canon law, especially the reconciling of seemingly contradictory canons or the determination of priority between them. Canons consist of doctrinal statements and disciplinary measures—most Church councils and local synods dealt with immediate disciplinary concerns as well as major difficulties of doctrine. Eastern Orthodoxy typically views the purely doctrinal canons as dogmatic and applicable to the entire church at all times, while the disciplinary canons apply to a particular time and place and may or may not be applicable in other situations.

Of the seven councils recognised in whole or in part by both the Catholic and the Eastern Orthodox Church as ecumenical, all were called by a Roman emperor. The emperor gave them legal status within the entire Roman Empire. All were held in the eastern part of the Roman Empire. The bishop of Rome (self-styled as "pope" since the end of the fourth century) did not attend, although he sent legates to some of them.

Church councils were traditional and the ecumenical councils were a continuation of earlier councils (also known as synods) held in the Empire before Christianity was made legal. These include the Council of Jerusalem (c. 50), the Council of Rome (155), the Second Council of Rome (193), the Council of Ephesus (193), the Council of Carthage (251), the Council of Iconium (258), the Council of Antioch (264), the Councils of Arabia (246–247), the Council of Elvira (306), the Council of Carthage (311), the Synod of Neo-Caesarea (c. 314), the Council of Ancyra (314) and the Council of Arles (314).

The first seven councils recognised in both East and West as ecumenical and several others to which such recognition is refused were called by the Byzantine emperors. In the first millennium, various theological and political differences such as Nestorianism or Dyophysitism caused parts of the Church to separate after councils such as those of Ephesus and Chalcedon, but councils recognised as ecumenical continued to be held.

The Council of Hieria of 754, held at the imperial palace of that name close to Chalcedon in Anatolia, was summoned by Byzantine Emperor Constantine V and was attended by 338 bishops, who regarded it as the seventh ecumenical council The Second Council of Nicaea, which annulled that of Hieria, was itself annulled at a synod held in 815 in Constantinople under Emperor Leo V. This synod, presided over by Patriarch Theodotus I of Constantinople, declared the Council of Hieria to be the seventh ecumenical council, but, although the Council of Hieria was called by an emperor and confirmed by another, and although it was held in the east, it later ceased to be considered ecumenical.

Similarly, the Second Council of Ephesus of 449, also held in Anatolia, was called by the Byzantine Emperor Theodosius II and, though annulled by the Council of Chalcedon, was confirmed by Emperor Basiliscus, who annulled the Council of Chalcedon. This too ceased to be considered an ecumenical council.

The Catholic Church does not consider the validity of an ecumenical council's teaching to be in any way dependent on where it is held or on the granting or withholding of prior authorization or legal status by any state, in line with the attitude of the 5th-century bishops who "saw the definition of the church's faith and canons as supremely their affair, with or without the leave of the Emperor" and who "needed no one to remind them that Synodical process pre-dated the Christianisation of the royal court by several centuries".

The Catholic Church recognizes as ecumenical various councils held later than the First Council of Ephesus (after which churches out of communion with the Holy See because of the Nestorian Schism did not participate), later than the Council of Chalcedon (after which there was no participation by churches that rejected Dyophysitism), later than the Second Council of Nicaea (after which there was no participation by the Eastern Orthodox Church), and later than the Fifth Council of the Lateran (after which groups that adhered to Protestantism did not participate).

Of the twenty-one ecumenical councils recognised by the Catholic Church, some gained recognition as ecumenical only later. Thus the Eastern First Council of Constantinople became ecumenical only when its decrees were accepted in the West also.

In the history of Christianity, the first seven ecumenical councils, from the First Council of Nicaea (325) to the Second Council of Nicaea (787), represent an attempt to reach an orthodox consensus and to unify Christendom.

All of the original seven ecumenical councils as recognized in whole or in part were called by an emperor of the Eastern Roman Empire and all were held in the Eastern Roman Empire, a recognition denied to other councils similarly called by an Eastern Roman emperor and held in his territory, in particular the Council of Serdica (343), the Second Council of Ephesus (449) and the Council of Hieria (754), which saw themselves as ecumenical or were intended as such.

As late as the 11th century, only seven councils were recognised as ecumenical in the Catholic Church. Then, in the time of Pope Gregory VII (1073–1085), canonists who in the Investiture Controversy quoted the prohibition in canon 22 of the Council of Constantinople of 869–870 against laymen influencing the appointment of prelates elevated this council to the rank of ecumenical council. Only in the 16th century was recognition as ecumenical granted by Catholic scholars to the Councils of the Lateran, of Lyon and those that followed. The following is a list of further councils generally recognised as ecumenical by Catholic theologians:

Eastern Orthodox catechisms teach that there are seven ecumenical councils and there are feast days for seven ecumenical councils. Nonetheless, some Eastern Orthodox consider events like the Council of Constantinople of 879–880, that of Constantinople in 1341–1351 and that of Jerusalem in 1672 to be ecumenical:

It is unlikely that formal ecumenical recognition will be granted to these councils, despite the acknowledged orthodoxy of their decisions, so that only seven are universally recognized among the Eastern Orthodox as ecumenical.

The 2016 Pan-Orthodox Council was sometimes referred to as a potential "Eighth Ecumenical Council" following debates on several issues facing Eastern Orthodoxy, however not all autocephalous churches were represented.

Although some Protestants reject the concept of an ecumenical council establishing doctrine for the entire Christian faith, Catholics, Lutherans, Anglicans, Eastern Orthodox and Oriental Orthodox all accept the authority of ecumenical councils in principle. Where they differ is in which councils they accept and what the conditions are for a council to be considered "ecumenical". The relationship of the Papacy to the validity of ecumenical councils is a ground of controversy between Catholicism and the Eastern Orthodox Churches. The Catholic Church holds that recognition by the Pope is an essential element in qualifying a council as ecumenical; Eastern Orthodox view approval by the Bishop of Rome (the Pope) as being roughly equivalent to that of other patriarchs.
Some have held that a council is ecumenical only when all five patriarchs of the Pentarchy are represented at it. Others reject this theory in part because there were no patriarchs of Constantinople and Jerusalem at the time of the first ecumenical council.

Both the Catholic and Eastern Orthodox churches recognize seven councils in the early centuries of the church, but Catholics also recognize fourteen councils in later times called or confirmed by the Pope. At the urging of German King Sigismund, who was to become Holy Roman Emperor in 1433, the Council of Constance was convoked in 1414 by Antipope John XXIII, one of three claimants to the papal throne, and was reconvened in 1415 by the Roman Pope Gregory XII. The Council of Florence is an example of a council accepted as ecumenical in spite of being rejected by the East, as the Councils of Ephesus and Chalcedon are accepted in spite of being rejected respectively by the Church of the East and Oriental Orthodoxy.

The Catholic Church teaches that an ecumenical council is a gathering of the College of Bishops (of which the Bishop of Rome is an essential part) to exercise in a solemn manner its supreme and full power over the whole Church. It holds that "there never is an ecumenical council which is not confirmed or at least recognized as such by Peter's successor". Its present canon law requires that an ecumenical council be convoked and presided over, either personally or through a delegate, by the Pope, who is also to decide the agenda; but the church makes no claim that all past ecumenical councils observed these present rules, declaring only that the Pope's confirmation or at least recognition has always been required, and saying that the version of the Nicene Creed adopted at the First Council of Constantinople (381) was accepted by the Church of Rome only seventy years later, in 451. One writer has even claimed that this council was summoned without the knowledge of the pope.

The Eastern Orthodox Church accepts seven ecumenical councils, with the disputed Council in Trullo—rejected by Catholics—being incorporated into, and considered as a continuation of, the Third Council of Constantinople.

To be considered ecumenical, Orthodox accept a council that meets the condition that it was accepted by the whole church. That it was called together legally is also an important factor. A case in point is the Third Ecumenical Council, where two groups met as duly called for by the emperor, each claiming to be the legitimate council. The Emperor had called for bishops to assemble in the city of Ephesus. Theodosius did not attend but sent his representative Candidian to preside. However, Cyril managed to open the council over Candidian's insistent demands that the bishops disperse until the delegation from Syria could arrive. Cyril was able to completely control the proceedings, completely neutralizing Candidian, who favored Cyril's antagonist, Nestorius. When the pro-Nestorius Antiochene delegation finally arrived, they decided to convene their own council, over which Candidian presided. The proceedings of both councils were reported to the emperor, who decided ultimately to depose Cyril, Memnon and Nestorius. Nonetheless, the Orthodox accept Cyril's group as being the legitimate council because it maintained the same teaching that the church has always taught.

Paraphrasing a rule by St Vincent of Lérins, Hasler states
Orthodox believe that councils could over-rule or even depose popes. At the Sixth Ecumenical Council, Pope Honorius and Patriarch Sergius were declared heretics. The council anathematized them and declared them tools of the devil and cast them out of the church.

It is their position that, since the Seventh Ecumenical Council, there has been no synod or council of the same scope. Local meetings of hierarchs have been called "pan-Orthodox", but these have invariably been simply meetings of local hierarchs of whatever Eastern Orthodox jurisdictions are party to a specific local matter. From this point of view, there has been no fully "pan-Orthodox" (Ecumenical) council since 787. Unfortunately, the use of the term "pan-Orthodox" is confusing to those not within Eastern Orthodoxy, and it leads to mistaken impressions that these are "ersatz" ecumenical councils rather than purely local councils to which nearby Orthodox hierarchs, regardless of jurisdiction, are invited.

Others, including 20th-century theologians Metropolitan Hierotheos (Vlachos) of Naupactus, Fr. John S. Romanides, and Fr. George Metallinos (all of whom refer repeatedly to the "Eighth and Ninth Ecumenical Councils"), Fr. George Dragas, and the 1848 Encyclical of the Eastern Patriarchs (which refers explicitly to the "Eighth Ecumenical Council" and was signed by the patriarchs of Constantinople, Jerusalem, Antioch, and Alexandria as well as the Holy Synods of the first three), regard other synods beyond the Seventh Ecumenical Council as being ecumenical.
From the Eastern Orthodox perspective, a council is accepted as being ecumenical if it is accepted by the Eastern Orthodox church at large – clergy, monks and assembly of believers. Teachings from councils that purport to be ecumenical, but which lack this acceptance by the church at large, are, therefore, not considered ecumenical.

Oriental Orthodoxy accepts three ecumenical councils, the First Council of Nicaea, the First Council of Constantinople, and the Council of Ephesus. The formulation of the Chalcedonian Creed caused a schism in the Alexandrian and Syriac churches. Reconciliatory efforts between Oriental Orthodox with the Eastern Orthodox and the Catholic Church in the mid- and late 20th century have led to common Christological declarations. The Oriental and Eastern Churches have also been working toward reconciliation as a consequence of the ecumenical movement.

The Oriental Orthodox hold that the Dyophysite formula of two natures formulated at the Council of Chalcedon is inferior to the Miaphysite formula of "One Incarnate Nature of God the Word" (Byzantine Greek: Mia physis tou theou logou sarkousomene) and that the proceedings of Chalcedon themselves were motivated by imperial politics. The Alexandrian Church, the main Oriental Orthodox body, also felt unfairly underrepresented at the council following the deposition of their Pope, Dioscorus of Alexandria at the council.

The Church of the East accepts two ecumenical councils, the First Council of Nicaea and the First Council of Constantinople. It was the formulation of Mary as the Theotokos which caused a schism with the Church of the East, now divided between the Assyrian Church of the East and the Ancient Church of the East, while the Chaldean Catholic Church entered into full communion with Rome in the 16th century. Meetings between Pope John Paul II and the Assyrian Patriarch Mar Dinkha IV led to a common Christological declaration on 11 November 1994 that "the humanity to which the Blessed Virgin Mary gave birth always was that of the Son of God himself". Both sides recognised the legitimacy and rightness, as expressions of the same faith, of the Assyrian Church's liturgical invocation of Mary as "the Mother of Christ our God and Saviour" and the Catholic Church's use of "the Mother of God" and also as "the Mother of Christ".

While the Councils are part of the "historic formularies" of Anglican tradition, it is difficult to locate an explicit reference in Anglicanism to the unconditional acceptance of all Seven Ecumenical Councils. There is little evidence of dogmatic or canonical acceptance beyond the statements of individual Anglican theologians and bishops.

Bishop Chandler Holder Jones, SSC, explains:

He quotes William Tighe, Associate Professor of History at Muhlenberg College in Allentown, Pennsylvania (another member of the Anglo-Catholic wing of Anglicanism):
Article XXI teaches: "General Councils ... when they be gathered together, forasmuch as they be an assembly of men, whereof all be not governed with the Spirit and word of God, they may err and sometime have erred, even in things pertaining to God. Wherefore things ordained by them as necessary to salvation have neither strength nor authority, unless it may be declared that they be taken out of Holy Scripture."

The 19th Canon of 1571 asserted the authority of the Councils in this manner: "Let preachers take care that they never teach anything...except what is agreeable to the doctrine of the Old and New Testament, and what the Catholic Fathers and ancient Bishops have collected from the same doctrine." This remains the Church of England's teaching on the subject. A modern version of this appeal to catholic consensus is found in the Canon Law of the Church of England and also in the liturgy published in "Common Worship":

The 1559 Act of Supremacy made a distinction between the decisions of the first four ecumenical councils, which were to be used as sufficient proof that something was heresy, as opposed to those of later councils, which could only be used to that purpose if "the same was declared heresy by the express and plain words of the...canonical Scriptures".

Many Protestants (especially those belonging to the magisterial traditions, such as Lutherans, or those such as Methodists, that broke away from the Anglican Communion) accept the teachings of the first seven councils but do not ascribe to the councils themselves the same authority as Catholics and the Eastern Orthodox do. The Lutheran World Federation, in ecumenical dialogues with the Ecumenical Patriarch of Constantinople has affirmed all of the first seven councils as ecumenical and authoritative.

Some, including some fundamentalist Christians, condemn the ecumenical councils for other reasons. Independency or congregationalist polity among Protestants may involve the rejection of any governmental structure or binding authority above local congregations; conformity to the decisions of these councils is therefore considered purely voluntary and the councils are to be considered binding only insofar as those doctrines are derived from the Scriptures. Many of these churches reject the idea that anyone other than the authors of Scripture can directly lead other Christians by original divine authority; after the New Testament, they assert, the doors of revelation were closed and councils can only give advice or guidance, but have no authority. They consider new doctrines not derived from the sealed canon of Scripture to be both impossible and unnecessary whether proposed by church councils or by more recent prophets. Catholic and Orthodox objections to this position point to the fact that the Canon of Scripture itself was fixed by these councils. They conclude that this would lead to a logical inconsistency of a non-authoritative body fixing a supposedly authoritative source.

Ecumenical councils are not recognised by nontrinitarian churches such as The Church of Jesus Christ of Latter-day Saints (and other denominations within the Latter Day Saint movement), Jehovah's Witnesses, Church of God (Seventh-Day), their descendants and Unitarians. They view the ecumenical councils as misguided human attempts to establish doctrine, and as attempts to define dogmas by debate rather than by revelation.





</doc>
<doc id="9763" url="https://en.wikipedia.org/wiki?curid=9763" title="Exoplanet">
Exoplanet

An exoplanet or extrasolar planet is a planet outside the Solar System. The first possible evidence of an exoplanet was noted in 1917, but was not recognized as such. The first confirmation of detection occurred in 1992. This was followed by the confirmation of a different planet, originally detected in 1988. 

There are many methods of detecting exoplanets. Transit photometry and Doppler spectroscopy have found the most, but these methods suffer from a clear observational bias favoring the detection of planets near the star; thus, 85% of the exoplanets detected are inside the tidal locking zone. In several cases, multiple planets have been observed around a star. About 1 in 5 Sun-like stars have an "Earth-sized" planet in the habitable zone. Assuming there are 200 billion stars in the Milky Way, it can be hypothesized that there are 11 billion potentially habitable Earth-sized planets in the Milky Way, rising to 40 billion if planets orbiting the numerous red dwarfs are included.

The least massive planet known is Draugr (also known as PSR B1257+12 A or PSR B1257+12 b), which is about twice the mass of the Moon. The most massive planet listed on the NASA Exoplanet Archive is HR 2562 b, about 30 times the mass of Jupiter, although according to some definitions of a planet (based on the nuclear fusion of deuterium), it is too massive to be a planet and may be a brown dwarf instead. Known orbital times for exoplanets vary from a few hours (for those closest to their star) to thousands of years. Some exoplanets are so far away from the star that it is difficult to tell whether they are gravitationally bound to it. Almost all of the planets detected so far are within the Milky Way. There is evidence that extragalactic planets, exoplanets farther away in galaxies beyond the local Milky Way galaxy, may exist. The nearest exoplanet is Proxima Centauri b, located 4.2 light-years (1.3 parsecs) from Earth and orbiting Proxima Centauri, the closest star to the Sun.

The discovery of exoplanets has intensified interest in the search for extraterrestrial life. There is special interest in planets that orbit in a star's habitable zone, where it is possible for liquid water, a prerequisite for life on Earth, to exist on the surface. The study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.

Rogue planets do not orbit any star. Such objects are considered as a separate category of planet, especially if they are gas giants, which are often counted as sub-brown dwarfs. The rogue planets in the Milky Way possibly number in the billions or more.

The official definition of "planet" used by the International Astronomical Union (IAU) only covers the Solar System and thus does not apply to exoplanets. The only defining statement issued by the IAU that pertains to exoplanets is a working definition issued in 2001 and modified in 2003.
That definition contains the following criteria:

The IAU's working definition is not always used. One alternate suggestion is that planets should be distinguished from brown dwarfs on the basis of formation. It is widely thought that giant planets form through core accretion, which may sometimes produce planets with masses above the deuterium fusion threshold; massive planets of that sort may have already been observed. 
Brown dwarfs form like stars from the direct gravitational collapse of clouds of gas and this formation mechanism also produces objects that are below the limit and can be as low as . Objects in this mass range that orbit their stars with wide separations of hundreds or thousands of AU and have large star/object mass ratios likely formed as brown dwarfs; their atmospheres would likely have a composition more similar to their host star than accretion-formed planets which would contain increased abundances of heavier elements. Most directly imaged planets as of April 2014 are massive and have wide orbits so probably represent the low-mass end of brown dwarf formation.
One study suggests that objects above formed through gravitational instability and should not be thought of as planets.

Also, the 13-Jupiter-mass cutoff does not have precise physical significance. Deuterium fusion can occur in some objects with a mass below that cutoff. The amount of deuterium fused depends to some extent on the composition of the object. As of 2011 the Extrasolar Planets Encyclopaedia included objects up to 25 Jupiter masses, saying, "The fact that there is no special feature around in the observed mass spectrum reinforces the choice to forget this mass limit". 
As of 2016 this limit was increased to 60 Jupiter masses based on a study of mass–density relationships.
The Exoplanet Data Explorer includes objects up to 24 Jupiter masses with the advisory: "The 13 Jupiter-mass distinction by the IAU Working Group is physically unmotivated for planets with rocky cores, and observationally problematic due to the sin i ambiguity."
The NASA Exoplanet Archive includes objects with a mass (or minimum mass) equal to or less than 30 Jupiter masses.
Another criterion for separating planets and brown dwarfs, rather than deuterium fusion, formation process or location, is whether the core pressure is dominated by coulomb pressure or electron degeneracy pressure with the dividing line at around 5 Jupiter masses.

<section begin=nomenclature />
The convention for designating exoplanets is an extension of the system used for designating multiple-star systems as adopted by the International Astronomical Union (IAU). For exoplanets orbiting a single star, the IAU designation is formed by taking the designated or proper name of its parent star, and adding a lower case letter. Letters are given in order of each planet's discovery around the parent star, so that the first planet discovered in a system is designated "b" (the parent star is considered to be "a") and later planets are given subsequent letters. If several planets in the same system are discovered at the same time, the closest one to the star gets the next letter, followed by the other planets in order of orbital size. A provisional IAU-sanctioned standard exists to accommodate the designation of circumbinary planets. A limited number of exoplanets have IAU-sanctioned proper names. Other naming systems exist.<section end=nomenclature />

For centuries scientists, philosophers, and science fiction writers suspected that extrasolar planets existed, but there was no way of knowing whether they existed, how common they were, or how similar they might be to the planets of the Solar System. Various detection claims made in the nineteenth century were rejected by astronomers.

The first evidence of a possible exoplanet, orbiting Van Maanen 2, was noted in 1917, but was not recognized as such. The astronomer Walter Sydney Adams, who later became director of the Mount Wilson Observatory, produced a spectrum of the star using Mount Wilson's 60-inch telescope. He interpreted the spectrum to be of an F-type main-sequence star, but it is now thought that such a spectrum could be caused by the residue of a nearby exoplanet that had been pulverized into dust by the gravity of the star, the resulting dust then falling onto the star. 

The first suspected scientific detection of an exoplanet occurred in 1988. Shortly afterwards, the first confirmation of detection came in 1992, with the discovery of several terrestrial-mass planets orbiting the pulsar PSR B1257+12. The first confirmation of an exoplanet orbiting a main-sequence star was made in 1995, when a giant planet was found in a four-day orbit around the nearby star 51 Pegasi. Some exoplanets have been imaged directly by telescopes, but the vast majority have been detected through indirect methods, such as the transit method and the radial-velocity method. In February 2018, researchers using the Chandra X-ray Observatory, combined with a planet detection technique called microlensing, found evidence of planets in a distant galaxy, stating "Some of these exoplanets are as (relatively) small as the moon, while others are as massive as Jupiter. Unlike Earth, most of the exoplanets are not tightly bound to stars, so they're actually wandering through space or loosely orbiting between stars. We can estimate that the number of planets in this [faraway] galaxy is more than a trillion.

In the sixteenth century, the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets.

In the eighteenth century, the same possibility was mentioned by Isaac Newton in the "General Scholium" that concludes his "Principia". Making a comparison to the Sun's planets, he wrote "And if the fixed stars are the centres of similar systems, they will all be constructed according to a similar design and subject to the dominion of "One"."

In 1952, more than 40 years before the first hot Jupiter was discovered, Otto Struve wrote that there is no compelling reason why planets could not be much closer to their parent star than is the case in the Solar System, and proposed that Doppler spectroscopy and the transit method could detect super-Jupiters in short orbits.

Claims of exoplanet detections have been made since the nineteenth century. Some of the earliest involve the binary star 70 Ophiuchi. In 1855 William Stephen Jacob at the East India Company's Madras Observatory reported that orbital anomalies made it "highly probable" that there was a "planetary body" in this system. In the 1890s, Thomas J. J. See of the University of Chicago and the United States Naval Observatory stated that the orbital anomalies proved the existence of a dark body in the 70 Ophiuchi system with a 36-year period around one of the stars. However, Forest Ray Moulton published a paper proving that a three-body system with those orbital parameters would be highly unstable. During the 1950s and 1960s, Peter van de Kamp of Swarthmore College made another prominent series of detection claims, this time for planets orbiting Barnard's Star. Astronomers now generally regard all the early reports of detection as erroneous.

In 1991 Andrew Lyne, M. Bailes and S. L. Shemar claimed to have discovered a pulsar planet in orbit around PSR 1829-10, using pulsar timing variations. The claim briefly received intense attention, but Lyne and his team soon retracted it.

As of , a total of confirmed exoplanets are listed in the Extrasolar Planets Encyclopedia, including a few that were confirmations of controversial claims from the late 1980s. The first published discovery to receive subsequent confirmation was made in 1988 by the Canadian astronomers Bruce Campbell, G. A. H. Walker, and Stephenson Yang of the University of Victoria and the University of British Columbia. Although they were cautious about claiming a planetary detection, their radial-velocity observations suggested that a planet orbits the star Gamma Cephei. Partly because the observations were at the very limits of instrumental capabilities at the time, astronomers remained skeptical for several years about this and other similar observations. It was thought some of the apparent planets might instead have been brown dwarfs, objects intermediate in mass between planets and stars. In 1990, additional observations were published that supported the existence of the planet orbiting Gamma Cephei, but subsequent work in 1992 again raised serious doubts. Finally, in 2003, improved techniques allowed the planet's existence to be confirmed.

On 9 January 1992, radio astronomers Aleksander Wolszczan and Dale Frail announced the discovery of two planets orbiting the pulsar PSR 1257+12. This discovery was confirmed, and is generally considered to be the first definitive detection of exoplanets. Follow-up observations solidified these results, and confirmation of a third planet in 1994 revived the topic in the popular press. These pulsar planets are thought to have formed from the unusual remnants of the supernova that produced the pulsar, in a second round of planet formation, or else to be the remaining rocky cores of gas giants that somehow survived the supernova and then decayed into their current orbits.

On 6 October 1995, Michel Mayor and Didier Queloz of the University of Geneva announced the first definitive detection of an exoplanet orbiting a main-sequence star, nearby G-type star 51 Pegasi. This discovery, made at the Observatoire de Haute-Provence, ushered in the modern era of exoplanetary discovery, and was recognized by a share of the 2019 Nobel Prize in Physics. Technological advances, most notably in high-resolution spectroscopy, led to the rapid detection of many new exoplanets: astronomers could detect exoplanets indirectly by measuring their gravitational influence on the motion of their host stars. More extrasolar planets were later detected by observing the variation in a star's apparent luminosity as an orbiting planet transited in front of it.

Initially, most known exoplanets were massive planets that orbited very close to their parent stars. Astronomers were surprised by these "hot Jupiters", because theories of planetary formation had indicated that giant planets should only form at large distances from stars. But eventually more planets of other sorts were found, and it is now clear that hot Jupiters make up the minority of exoplanets. In 1999, Upsilon Andromedae became the first main-sequence star known to have multiple planets. Kepler-16 contains the first discovered planet that orbits around a binary main-sequence star system.

On 26 February 2014, NASA announced the discovery of 715 newly verified exoplanets around 305 stars by the "Kepler" Space Telescope. These exoplanets were checked using a statistical technique called "verification by multiplicity". Before these results, most confirmed planets were gas giants comparable in size to Jupiter or larger because they are more easily detected, but the "Kepler" planets are mostly between the size of Neptune and the size of Earth.

On 23 July 2015, NASA announced Kepler-452b, a near-Earth-size planet orbiting the habitable zone of a G2-type star.

On 6 September 2018, NASA discovered an exoplanet about 145 light years away from Earth in the constellation Virgo. This exoplanet, Wolf 503b, is twice the size of Earth and was discovered orbiting a type of star known as an "Orange Dwarf". Wolf 503b completes one orbit in as few as six days because it is very close to the star. Wolf 503b is the only exoplanet that large that can be found near the so-called Fulton gap. The Fulton gap, first noticed in 2017, is the observation that it is unusual to find planets within a certain mass range. Under the Fulton gap studies, this opens up a new field for astronomers, who are still studying whether planets found in the Fulton gap are gaseous or rocky.

In January 2020, scientists announced the discovery of TOI 700 d, the first Earth-sized planet in the habitable zone detected by TESS.

As of January 2020, NASA's "Kepler" and TESS missions had identified 4374 planetary candidates yet to be confirmed, several of them being nearly Earth-sized and located in the habitable zone, some around Sun-like stars.

<section begin=detection />

About 97% of all the confirmed exoplanets have been discovered by indirect techniques of detection, mainly by radial velocity measurements and transit monitoring techniques.<section end="detection" /> Recently the techniques of singular optics have been applied in the search for exoplanets.

Planets may form within a few to tens (or more) of millions of years of their star forming. 
The planets of the Solar System can only be observed in their current state, but observations of different planetary systems of varying ages allows us to observe planets at different stages of evolution. Available observations range from young proto-planetary disks where planets are still forming to planetary systems of over 10 Gyr old. When planets form in a gaseous protoplanetary disk, they accrete hydrogen/helium envelopes. These envelopes cool and contract over time and, depending on the mass of the planet, some or all of the hydrogen/helium is eventually lost to space. This means that even terrestrial planets may start off with large radii if they form early enough. An example is Kepler-51b which has only about twice the mass of Earth but is almost the size of Saturn which is a hundred times the mass of Earth. Kepler-51b is quite young at a few hundred million years old.

There is at least one planet on average per star.
About 1 in 5 Sun-like stars have an "Earth-sized" planet in the habitable zone.

Most known exoplanets orbit stars roughly similar to the Sun, i.e. main-sequence stars of spectral categories F, G, or K. Lower-mass stars (red dwarfs, of spectral category M) are less likely to have planets massive enough to be detected by the radial-velocity method. Despite this, several tens of planets around red dwarfs have been discovered by the "Kepler" spacecraft, which uses the transit method to detect smaller planets.

Using data from "Kepler", a correlation has been found between the metallicity of a star and the probability that the star host planets. Stars with higher metallicity are more likely to have planets, especially giant planets, than stars with lower metallicity.

Some planets orbit one member of a binary star system, and several circumbinary planets have been discovered which orbit around both members of binary star. A few planets in triple star systems are known and one in the quadruple system Kepler-64.

In 2013 the color of an exoplanet was determined for the first time. The best-fit albedo measurements of HD 189733b suggest that it is deep dark blue. Later that same year, the colors of several other exoplanets were determined, including GJ 504 b which visually has a magenta color, and Kappa Andromedae b, which if seen up close would appear reddish in color.
Helium planets are expected to be white or grey in appearance.

The apparent brightness (apparent magnitude) of a planet depends on how far away the observer is, how reflective the planet is (albedo), and how much light the planet receives from its star, which depends on how far the planet is from the star and how bright the star is. So, a planet with a low albedo that is close to its star can appear brighter than a planet with high albedo that is far from the star.

The darkest known planet in terms of geometric albedo is TrES-2b, a hot Jupiter that reflects less than 1% of the light from its star, making it less reflective than coal or black acrylic paint. Hot Jupiters are expected to be quite dark due to sodium and potassium in their atmospheres but it is not known why TrES-2b is so dark—it could be due to an unknown chemical compound.

For gas giants, geometric albedo generally decreases with increasing metallicity or atmospheric temperature unless there are clouds to modify this effect. Increased cloud-column depth increases the albedo at optical wavelengths, but decreases it at some infrared wavelengths. Optical albedo increases with age, because older planets have higher cloud-column depths. Optical albedo decreases with increasing mass, because higher-mass giant planets have higher surface gravities, which produces lower cloud-column depths. Also, elliptical orbits can cause major fluctuations in atmospheric composition, which can have a significant effect.

There is more thermal emission than reflection at some near-infrared wavelengths for massive and/or young gas giants. So, although optical brightness is fully phase-dependent, this is not always the case in the near infrared.

Temperatures of gas giants reduce over time and with distance from their star. Lowering the temperature increases optical albedo even without clouds. At a sufficiently low temperature, water clouds form, which further increase optical albedo. At even lower temperatures ammonia clouds form, resulting in the highest albedos at most optical and near-infrared wavelengths.

In 2014, a magnetic field around HD 209458 b was inferred from the way hydrogen was evaporating from the planet. It is the first (indirect) detection of a magnetic field on an exoplanet. The magnetic field is estimated to be about one tenth as strong as Jupiter's.

Exoplanets magnetic fields may be detectable by their auroral radio emissions with sensitive enough radio telescopes such as LOFAR. The radio emissions could enable determination of the rotation rate of the interior of an exoplanet, and may yield a more accurate way to measure exoplanet rotation than by examining the motion of clouds.

Earth's magnetic field results from its flowing liquid metallic core, but in massive super-Earths with high pressure, different compounds may form which do not match those created under terrestrial conditions. Compounds may form with greater viscosities and high melting temperatures which could prevent the interiors from separating into different layers and so result in undifferentiated coreless mantles. Forms of magnesium oxide such as MgSiO could be a liquid metal at the pressures and temperatures found in super-Earths and could generate a magnetic field in the mantles of super-Earths.

Hot Jupiters have been observed to have a larger radius than expected. This could be caused by the interaction between the stellar wind and the planet's magnetosphere creating an electric current through the planet that heats it up causing it to expand. The more magnetically active a star is the greater the stellar wind and the larger the electric current leading to more heating and expansion of the planet. This theory matches the observation that stellar activity is correlated with inflated planetary radii.

In August 2018, scientists announced the transformation of gaseous deuterium into a liquid metallic form. This may help researchers better understand giant gas planets, such as Jupiter, Saturn and related exoplanets, since such planets are thought to contain a lot of liquid metallic hydrogen, which may be responsible for their observed powerful magnetic fields.

Although scientists previously announced that the magnetic fields of close-in exoplanets may cause increased stellar flares and starspots on their host stars, in 2019 this claim was demonstrated to be false in the HD 189733 system. The failure to detect "star-planet interactions" in the well-studied HD 189733 system calls other related claims of the effect into question.

In 2019 the strength of the surface magnetic fields of 4 hot Jupiters were estimated and ranged between 20 and 120 gauss compared to Jupiter's surface magnetic field of 4.3 gauss.

In 2007, two independent teams of researchers came to opposing conclusions about the likelihood of plate tectonics on larger super-Earths with one team saying that plate tectonics would be episodic or stagnant and the other team saying that plate tectonics is very likely on super-Earths even if the planet is dry.

If super-Earths have more than 80 times as much water as Earth then they become ocean planets with all land completely submerged. However, if there is less water than this limit, then the deep water cycle will move enough water between the oceans and mantle to allow continents to exist.

Large surface temperature variations on 55 Cancri e have been attributed to possible volcanic activity releasing large clouds of dust which blanket the planet and block thermal emissions.

The star 1SWASP J140747.93-394542.6 is orbited by an object that is circled by a ring system much larger than Saturn's rings. However, the mass of the object is not known; it could be a brown dwarf or low-mass star instead of a planet.

The brightness of optical images of Fomalhaut b could be due to starlight reflecting off a circumplanetary ring system with a radius between 20 and 40 times that of Jupiter's radius, about the size of the orbits of the Galilean moons.

The rings of the Solar System's gas giants are aligned with their planet's equator. However, for exoplanets that orbit close to their star, tidal forces from the star would lead to the outermost rings of a planet being aligned with the planet's orbital plane around the star. A planet's innermost rings would still be aligned with the planet's equator so that if the planet has a tilted rotational axis, then the different alignments between the inner and outer rings would create a warped ring system.

In December 2013 a candidate exomoon of a rogue planet was announced. On 3 October 2018, evidence suggesting a large exomoon orbiting Kepler-1625b was reported.

Atmospheres have been detected around several exoplanets. The first to be observed was HD 209458 b in 2001.

In May 2017, glints of light from Earth, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere. The technology used to determine this may be useful in studying the atmospheres of distant worlds, including those of exoplanets.

KIC 12557548 b is a small rocky planet, very close to its star, that is evaporating and leaving a trailing tail of cloud and dust like a comet. The dust could be ash erupting from volcanos and escaping due to the small planet's low surface-gravity, or it could be from metals that are vaporized by the high temperatures of being so close to the star with the metal vapor then condensing into dust.

In June 2015, scientists reported that the atmosphere of GJ 436 b was evaporating, resulting in a giant cloud around the planet and, due to radiation from the host star, a long trailing tail long.

Tidally locked planets in a 1:1 spin-orbit resonance would have their star always shining directly overhead on one spot which would be hot with the opposite hemisphere receiving no light and being freezing cold. Such a planet could resemble an eyeball with the hotspot being the pupil. Planets with an eccentric orbit could be locked in other resonances. 3:2 and 5:2 resonances would result in a double-eyeball pattern with hotspots in both eastern and western hemispheres. Planets with both an eccentric orbit and a tilted axis of rotation would have more complicated insolation patterns.

As more planets are discovered, the field of exoplanetology continues to grow into a deeper study of extrasolar worlds, and will ultimately tackle the prospect of life on planets beyond the Solar System. At cosmic distances, life can only be detected if it is developed at a planetary scale and strongly modified the planetary environment, in such a way that the modifications cannot be explained by classical physico-chemical processes (out of equilibrium processes). For example, molecular oxygen () in the atmosphere of Earth is a result of photosynthesis by living plants and many kinds of microorganisms, so it can be used as an indication of life on exoplanets, although small amounts of oxygen could also be produced by non-biological means. Furthermore, a potentially habitable planet must orbit a stable star at a distance within which planetary-mass objects with sufficient atmospheric pressure can support liquid water at their surfaces.




</doc>
<doc id="9764" url="https://en.wikipedia.org/wiki?curid=9764" title="Emma Goldman">
Emma Goldman

Emma Goldman (, 1869May 14, 1940) was an anarchist political activist and writer. She played a pivotal role in the development of anarchist political philosophy in North America and Europe in the first half of the 20th century.

Born in Kaunas, Russian Empire (now Lithuania) to a Jewish family, Goldman emigrated to the United States in 1885. Attracted to anarchism after the Chicago Haymarket affair, Goldman became a writer and a renowned lecturer on anarchist philosophy, women's rights, and social issues, attracting crowds of thousands. She and anarchist writer Alexander Berkman, her lover and lifelong friend, planned to assassinate industrialist and financier Henry Clay Frick as an act of propaganda of the deed. Frick survived the attempt on his life in 1892, and Berkman was sentenced to 22 years in prison. Goldman was imprisoned several times in the years that followed, for "inciting to riot" and illegally distributing information about birth control. In 1906, Goldman founded the anarchist journal "Mother Earth".

In 1917, Goldman and Berkman were sentenced to two years in jail for conspiring to "induce persons not to register" for the newly instated draft. After their release from prison, they were arrested—along with 248 others—and deported to Russia. Initially supportive of that country's October Revolution that brought the Bolsheviks to power, Goldman changed her opinion in the wake of the Kronstadt rebellion; she denounced the Soviet Union for its violent repression of independent voices. She left the Soviet Union and in 1923 published a book about her experiences, "My Disillusionment in Russia". While living in England, Canada, and France, she wrote an autobiography called "Living My Life". It was published in two volumes, in 1931 and 1935. After the outbreak of the Spanish Civil War, Goldman traveled to Spain to support the anarchist revolution there. She died in Toronto, Canada, on May 14, 1940, aged 70.

During her life, Goldman was lionized as a freethinking "rebel woman" by admirers, and denounced by detractors as an advocate of politically motivated murder and violent revolution. Her writing and lectures spanned a wide variety of issues, including prisons, atheism, freedom of speech, militarism, capitalism, marriage, free love, and homosexuality. Although she distanced herself from first-wave feminism and its efforts toward women's suffrage, she developed new ways of incorporating gender politics into anarchism. After decades of obscurity, Goldman gained iconic status in the 1970s by a revival of interest in her life, when feminist and anarchist scholars rekindled popular interest.

Emma Goldman was born into an Orthodox Jewish family in Kovno in the Russian Empire, which is now known as Kaunas in Lithuania. Goldman's mother Taube Bienowitch had been married before to a man with whom she had two daughters—Helena in 1860 and Lena in 1862. When her first husband died of tuberculosis, Taube was devastated. Goldman later wrote: "Whatever love she had had died with the young man to whom she had been married at the age of fifteen."

Taube's second marriage was arranged by her family and, as Goldman puts it, "mismated from the first". Her second husband, Abraham Goldman, invested Taube's inheritance in a business that quickly failed. The ensuing hardship, combined with the emotional distance of husband and wife, made the household a tense place for the children. When Taube became pregnant, Abraham hoped desperately for a son; a daughter, he believed, would be one more sign of failure. They eventually had three sons, but their first child was Emma.

Emma Goldman was born on June 27, 1869. Her father used violence to punish his children, beating them when they disobeyed him. He used a whip on Emma, the most rebellious of them. Her mother provided scarce comfort, rarely calling on Abraham to tone down his beatings. Goldman later speculated that her father's furious temper was at least partly a result of sexual frustration.

Goldman's relationships with her elder half-sisters, Helena and Lena, were a study in contrasts. Helena, the oldest, provided the comfort the children lacked from their mother; she filled Goldman's childhood with "whatever joy it had". Lena, however, was distant and uncharitable. The three sisters were joined by brothers Louis (who died at the age of six), Herman (born in 1872), and Moishe (born in 1879).

When Emma was a young girl, the Goldman family moved to the village of Papilė, where her father ran an inn. While her sisters worked, she became friends with a servant named Petrushka, who excited her "first erotic sensations". Later in Papilė she witnessed a peasant being whipped with a knout in the street. This event traumatized her and contributed to her lifelong distaste for violent authority.

At the age of seven, Goldman moved with her family to the Prussian city of Königsberg (then part of the German Empire), and she was enrolled in a "Realschule". One teacher punished disobedient students—targeting Goldman in particular—by beating their hands with a ruler. Another teacher tried to molest his female students and was fired when Goldman fought back. She found a sympathetic mentor in her German-language teacher, who loaned her books and took her to an opera. A passionate student, Goldman passed the exam for admission into a gymnasium, but her religion teacher refused to provide a certificate of good behavior and she was unable to attend.

The family moved to the Russian capital of Saint Petersburg, where her father opened one unsuccessful store after another. Their poverty forced the children to work, and Goldman took an assortment of jobs, including one in a corset shop. As a teenager Goldman begged her father to allow her to return to school, but instead he threw her French book into the fire and shouted: "Girls do not have to learn much! All a Jewish daughter needs to know is how to prepare gefilte fish, cut noodles fine, and give the man plenty of children."

Goldman pursued an independent education on her own, however, and soon began to study the political turmoil around her, particularly the Nihilists responsible for assassinating Alexander II of Russia. The ensuing turmoil intrigued Goldman, although she did not fully understand it at the time. When she read Nikolai Chernyshevsky's novel, "What Is to Be Done?" (1863), she found a role model in the protagonist Vera. She adopts a Nihilist philosophy and escapes her repressive family to live freely and organize a sewing cooperative. The book enthralled Goldman and remained a source of inspiration throughout her life.

Her father, meanwhile, continued to insist on a domestic future for her, and he tried to arrange for her to be married at the age of fifteen. They fought about the issue constantly; he complained that she was becoming a "loose" woman, and she insisted that she would marry for love alone. At the corset shop, she was forced to fend off unwelcome advances from Russian officers and other men. One man took her into a hotel room and committed what Goldman described as "violent contact"; two biographers call it rape. She was stunned by the experience, overcome by "shock at the discovery that the contact between man and woman could be so brutal and painful." Goldman felt that the encounter forever soured her interactions with men.

In 1885, her sister Helena made plans to move to New York in the United States to join her sister Lena and her husband. Goldman wanted to join her sister, but their father refused to allow it. Despite Helena's offer to pay for the trip, Abraham turned a deaf ear to their pleas. Desperate, Goldman threatened to throw herself into the Neva River if she could not go. Their father finally agreed. On December 29, 1885, Helena and Emma arrived at New York City's Castle Garden, the entry for immigrants.

They settled upstate, living in the Rochester home which Lena had made with her husband Samuel. Fleeing the rising antisemitism of Saint Petersburg, their parents and brothers joined them a year later. Goldman began working as a seamstress, sewing overcoats for more than ten hours a day, earning two and a half dollars a week. She asked for a raise and was denied; she quit and took work at a smaller shop nearby.

At her new job, Goldman met a fellow worker named Jacob Kershner, who shared her love for books, dancing, and traveling, as well as her frustration with the monotony of factory work. After four months, they married in February 1887. Once he moved in with Goldman's family, however, their relationship faltered. On their wedding night she discovered that he was impotent; they became emotionally and physically distant. Before long he became jealous and suspicious. She, meanwhile, was becoming more engaged with the political turmoil around her—particularly the aftermath of executions related to the 1886 Haymarket affair in Chicago and the anti-authoritarian political philosophy of anarchism.

Less than a year after the wedding, the couple were divorced; Kershner begged Goldman to return and threatened to poison himself if she did not. They reunited, but after three months she left once again. Her parents considered her behavior "loose" and refused to allow Goldman into their home. Carrying her sewing machine in one hand and a bag with five dollars in the other, she left Rochester and headed southeast to New York City.

On her first day in the city, Goldman met two men who greatly changed her life. At Sachs's Café, a gathering place for radicals, she was introduced to Alexander Berkman, an anarchist who invited her to a public speech that evening. They went to hear Johann Most, editor of a radical publication called "Freiheit" and an advocate of "propaganda of the deed"—the use of violence to instigate change. She was impressed by his fiery oration, and Most took her under his wing, training her in methods of public speaking. He encouraged her vigorously, telling her that she was "to take my place when I am gone." One of her first public talks in support of "the Cause" was in Rochester. After convincing Helena not to tell their parents of her speech, Goldman found her mind a blank once on stage. She later wrote, suddenly:
Excited by the experience, Goldman refined her public persona during subsequent engagements. Quickly, however, she found herself arguing with Most over her independence. After a momentous speech in Cleveland, she felt as though she had become "a parrot repeating Most's views" and resolved to express herself on the stage. When she returned to New York, Most became furious and told her: "Who is not with me is against me!" She left "Freiheit" and joined another publication, "Die Autonomie".

Meanwhile, Goldman had begun a friendship with Berkman, whom she affectionately called Sasha. Before long they became lovers and moved into a communal apartment with his cousin Modest "Fedya" Stein and Goldman's friend, Helen Minkin, on 42nd Street. Although their relationship had numerous difficulties, Goldman and Berkman would share a close bond for decades, united by their anarchist principles and commitment to personal equality.
In 1892, Goldman joined with Berkman and Stein in opening an ice cream shop in Worcester, Massachusetts. After a few months of operating the shop, however, Goldman and Berkman were diverted by becoming involved in the Homestead Strike in western Pennsylvania near Pittsburgh.

Berkman and Goldman came together through the Homestead Strike. In June 1892, a steel plant in Homestead, Pennsylvania owned by Andrew Carnegie became the focus of national attention when talks between the Carnegie Steel Company and the Amalgamated Association of Iron and Steel Workers (AA) broke down. The factory's manager was Henry Clay Frick, a fierce opponent of the union. When a final round of talks failed at the end of June, management closed the plant and locked out the workers, who immediately went on strike. Strikebreakers were brought in and the company hired Pinkerton guards to protect them. On July 6, a fight broke out between 300 Pinkerton guards and a crowd of armed union workers. During the twelve-hour gunfight, seven guards and nine strikers were killed.

When a majority of the nation's newspapers expressed support of the strikers, Goldman and Berkman resolved to assassinate Frick, an action they expected would inspire the workers to revolt against the capitalist system. Berkman chose to carry out the assassination, and ordered Goldman to stay behind in order to explain his motives after he went to jail. He would be in charge of "the deed"; she of the associated propaganda. Berkman tried and failed to make a bomb, then set off for Pittsburgh to buy a gun and a suit of decent clothes.

Goldman, meanwhile, decided to help fund the scheme through prostitution. Remembering the character of Sonya in Fyodor Dostoevsky's novel "Crime and Punishment" (1866), she mused: "She had become a prostitute in order to support her little brothers and sisters...Sensitive Sonya could sell her body; why not I?" Once on the street, Goldman caught the eye of a man who took her into a saloon, bought her a beer, gave her ten dollars, informed her she did not have "the knack," and told her to quit the business. She was "too astounded for speech". She wrote to Helena, claiming illness, and asked her for fifteen dollars.

On July 23, Berkman gained access to Frick's office while carrying a concealed handgun; he shot Frick three times, and stabbed him in the leg. A group of workers—far from joining in his "attentat"—beat Berkman unconscious, and he was carried away by the police. Berkman was convicted of attempted murder and sentenced to 22 years in prison. Goldman suffered during his long absence.

Convinced Goldman was involved in the plot, police raided her apartment. Although they found no evidence, they pressured her landlord into evicting her. Worse, the "attentat" had failed to rouse the masses: workers and anarchists alike condemned Berkman's action. Johann Most, their former mentor, lashed out at Berkman and the assassination attempt. Furious at these attacks, Goldman brought a toy horsewhip to a public lecture and demanded, onstage, that Most explain his betrayal. He dismissed her, whereupon she struck him with the whip, broke it on her knee, and hurled the pieces at him. She later regretted her assault, confiding to a friend: "At the age of twenty-three, one does not reason."

When the Panic of 1893 struck in the following year, the United States suffered one of its worst economic crises. By year's end, the unemployment rate was higher than 20%, and "hunger demonstrations" sometimes gave way to riots. Goldman began speaking to crowds of frustrated men and women in New York City. On August 21, she spoke to a crowd of nearly 3,000 people in Union Square, where she encouraged unemployed workers to take immediate action. Her exact words are unclear: undercover agents insist she ordered the crowd to "take everything ... by force". But Goldman later recounted this message: "Well then, demonstrate before the palaces of the rich; demand work. If they do not give you work, demand bread. If they deny you both, take bread." Later in court, Detective-Sergeant Charles Jacobs offered yet another version of her speech.

A week later, Goldman was arrested in Philadelphia and returned to New York City for trial, charged with "inciting to riot". During the train ride, Jacobs offered to drop the charges against her if she would inform on other radicals in the area. She responded by throwing a glass of ice water in his face. As she awaited trial, Goldman was visited by Nellie Bly, a reporter for the "New York World." She spent two hours talking to Goldman and wrote a positive article about the woman she described as a "modern Joan of Arc."

Despite this positive publicity, the jury was persuaded by Jacobs' testimony and frightened by Goldman's politics. The assistant District Attorney questioned Goldman about her anarchism, as well as her atheism; the judge spoke of her as "a dangerous woman". She was sentenced to one year in the Blackwell's Island Penitentiary. Once inside she suffered an attack of rheumatism and was sent to the infirmary; there she befriended a visiting doctor and began studying medicine. She also read dozens of books, including works by the American activist-writers Ralph Waldo Emerson and Henry David Thoreau; novelist Nathaniel Hawthorne; poet Walt Whitman, and philosopher John Stuart Mill. When Goldman was released after ten months, a raucous crowd of nearly 3,000 people greeted her at the Thalia Theater in New York City. She soon became swamped with requests for interviews and lectures.

To make money, Goldman decided to pursue the medical work she had studied in prison. However, her preferred fields of specialization—midwifery and massage—were not available to nursing students in the US. She sailed to Europe, lecturing in London, Glasgow, and Edinburgh. She met with renowned anarchists such as Errico Malatesta, Louise Michel, and Peter Kropotkin. In Vienna, she received two diplomas for midwifery and put them immediately to use back in the US.

Alternating between lectures and midwifery, Goldman conducted the first cross-country tour by an anarchist speaker. In November 1899 she returned to Europe to speak, where she met the Czech anarchist Hippolyte Havel in London. They went together to France and helped organize the 1900 International Anarchist Congress on the outskirts of Paris. Afterward Havel immigrated to the United States, traveling with Goldman to Chicago. They shared a residence there with friends of Goldman.

On September 6, 1901, Leon Czolgosz, an unemployed factory worker and registered Republican with a history of mental illness, shot US President William McKinley twice during a public speaking event in Buffalo, New York. McKinley was hit in the breastbone and stomach, and died eight days later. Czolgosz was arrested, and interrogated around the clock. During interrogation he claimed to be an anarchist and said he had been inspired to act after attending a speech by Goldman. The authorities used this as a pretext to charge Goldman with planning McKinley's assassination. They tracked her to the residence in Chicago she shared with Havel, as well as with Mary and Abe Isaak, an anarchist couple and their family. Goldman was arrested, along with Isaak, Havel, and ten other anarchists.
Earlier, Czolgosz had tried but failed to become friends with Goldman and her companions. During a talk in Cleveland, Czolgosz had approached Goldman and asked her advice on which books he should read. In July 1901, he had appeared at the Isaak house, asking a series of unusual questions. They assumed he was an infiltrator, like a number of police agents sent to spy on radical groups. They had remained distant from him, and Abe Isaak sent a notice to associates warning of "another spy".

Although Czolgosz repeatedly denied Goldman's involvement, the police held her in close custody, subjecting her to what she called the "third degree". She explained her housemates' distrust of Czolgosz, and the police finally recognized that she had not had any significant contact with the attacker. No evidence was found linking Goldman to the attack, and she was released after two weeks of detention. Before McKinley died, Goldman offered to provide nursing care, referring to him as "merely a human being". Czolgosz, despite considerable evidence of mental illness, was convicted of murder and executed.

Throughout her detention and after her release, Goldman steadfastly refused to condemn Czolgosz's actions, standing virtually alone in doing so. Friends and supporters—including Berkman—urged her to quit his cause. But Goldman defended Czolgosz as a "supersensitive being" and chastised other anarchists for abandoning him. She was vilified in the press as the "high priestess of anarchy", while many newspapers declared the anarchist movement responsible for the murder. In the wake of these events, socialism gained support over anarchism among US radicals. McKinley's successor, Theodore Roosevelt, declared his intent to crack down "not only against anarchists, but against all active and passive sympathizers with anarchists".

After Czolgosz was executed, Goldman withdrew from the world. Scorned by her fellow anarchists, vilified by the press, and separated from her love, Berkman, she retreated into anonymity and nursing. "It was bitter and hard to face life anew," she wrote later.

Using the name E. G. Smith, she left public life and took on a series of private nursing jobs. When the US Congress passed the Anarchist Exclusion Act (1903), however, a new wave of activism rose to oppose it, and Goldman was pulled back into the movement. A coalition of people and organizations across the left end of the political spectrum opposed the law on grounds that it violated freedom of speech, and she had the nation's ear once again.

After an English anarchist named John Turner was arrested under the Anarchist Exclusion Act and threatened with deportation, Goldman joined forces with the Free Speech League to champion his cause. The league enlisted the aid of noted attorneys Clarence Darrow and Edgar Lee Masters, who took Turner's case to the US Supreme Court. Although Turner and the League lost, Goldman considered it a victory of propaganda. She had returned to anarchist activism, but it was taking its toll on her. "I never felt so weighed down," she wrote to Berkman. "I fear I am forever doomed to remain public property and to have my life worn out through the care for the lives of others."

In 1906, Goldman decided to start a publication, "a place of expression for the young idealists in arts and letters". "Mother Earth" was staffed by a cadre of radical activists, including Hippolyte Havel, Max Baginski, and Leonard Abbott. In addition to publishing original works by its editors and anarchists around the world, "Mother Earth" reprinted selections from a variety of writers. These included the French philosopher Pierre-Joseph Proudhon, Russian anarchist Peter Kropotkin, German philosopher Friedrich Nietzsche, and British writer Mary Wollstonecraft. Goldman wrote frequently about anarchism, politics, labor issues, atheism, sexuality, and feminism, and was the first editor of the magazine.

On May 18 of the same year, Alexander Berkman was released from prison. Carrying a bouquet of roses, Goldman met him on the train platform and found herself "seized by terror and pity" as she beheld his gaunt, pale form. Neither was able to speak; they returned to her home in silence. For weeks, he struggled to readjust to life on the outside. An abortive speaking tour ended in failure, and in Cleveland he purchased a revolver with the intent of killing himself. He returned to New York, however, and learned that Goldman had been arrested with a group of activists meeting to reflect on Czolgosz. Invigorated anew by this violation of freedom of assembly, he declared, "My resurrection has come!" and set about securing their release.

Berkman took the helm of "Mother Earth" in 1907, while Goldman toured the country to raise funds to keep it operating. Editing the magazine was a revitalizing experience for Berkman. But his relationship with Goldman faltered, and he had an affair with a 15-year-old anarchist named Becky Edelsohn. Goldman was pained by his rejection of her, but considered it a consequence of his prison experience. Later that year she served as a delegate from the US to the International Anarchist Congress of Amsterdam. Anarchists and syndicalists from around the world gathered to sort out the tension between the two ideologies, but no decisive agreement was reached. Goldman returned to the US and continued speaking to large audiences.

For the next ten years, Goldman traveled around the country nonstop, delivering lectures and agitating for anarchism. The coalitions formed in opposition to the Anarchist Exclusion Act had given her an appreciation for reaching out to those of other political positions. When the US Justice Department sent spies to observe, they reported the meetings as "packed". Writers, journalists, artists, judges, and workers from across the spectrum spoke of her "magnetic power", her "convincing presence", her "force, eloquence, and fire".

In the spring of 1908, Goldman met and fell in love with Ben Reitman, the so-called "Hobo doctor." Having grown up in Chicago's Tenderloin District, Reitman spent several years as a drifter before earning a medical degree from the College of Physicians and Surgeons of Chicago. As a doctor, he treated people suffering from poverty and illness, particularly venereal diseases. He and Goldman began an affair. They shared a commitment to free love and Reitman took a variety of lovers, but Goldman did not. She tried to reconcile her feelings of jealousy with a belief in freedom of the heart, but found it difficult.

Two years later, Goldman began feeling frustrated with lecture audiences. She yearned to "reach the few who really want to learn, rather than the many who come to be amused". She collected a series of speeches and items she had written for "Mother Earth" and published a book titled "Anarchism and Other Essays." Covering a wide variety of topics, Goldman tried to represent "the mental and soul struggles of twenty-one years". In addition to a comprehensive look at anarchism and its criticisms, the book includes essays on patriotism, women's suffrage, marriage, and prisons.

When Margaret Sanger, an advocate of access to contraception, coined the term "birth control" and disseminated information about various methods in the June 1914 issue of her magazine "The Woman Rebel," she received aggressive support from Goldman. The latter had already been active in efforts to increase birth control access for several years. In 1916, Goldman was arrested for giving lessons in public on how to use contraceptives. Sanger, too, was arrested under the Comstock Law, which prohibited the dissemination of "obscene, lewd, or lascivious articles", which authorities defined as including information relating to birth control.

Although they later split from Sanger over charges of insufficient support, Goldman and Reitman distributed copies of Sanger's pamphlet "Family Limitation" (along with a similar essay of Reitman's). In 1915 Goldman conducted a nationwide speaking tour, in part to raise awareness about contraception options. Although the nation's attitude toward the topic seemed to be liberalizing, Goldman was arrested on February 11, 1916, as she was about to give another public lecture. Goldman was charged with violating the Comstock Law. Refusing to pay a $100 fine, Goldman spent two weeks in a prison workhouse, which she saw as an "opportunity" to reconnect with those rejected by society.

Although President Woodrow Wilson was re-elected in 1916 under the slogan "He kept us out of the war", at the start of his second term, he announced that Germany's continued deployment of unrestricted submarine warfare was sufficient cause for the US to enter the Great War. Shortly afterward, Congress passed the Selective Service Act of 1917, which required all males aged 21–30 to register for military conscription. Goldman saw the decision as an exercise in militarist aggression, driven by capitalism. She declared in "Mother Earth" her intent to resist conscription, and to oppose US involvement in the war.

To this end, she and Berkman organized the No Conscription League of New York, which proclaimed: "We oppose conscription because we are internationalists, antimilitarists, and opposed to all wars waged by capitalistic governments." The group became a vanguard for anti-draft activism, and chapters began to appear in other cities. When police began raiding the group's public events to find young men who had not registered for the draft, however, Goldman and others focused their efforts on distributing pamphlets and other writings. In the midst of the nation's patriotic fervor, many elements of the political left refused to support the League's efforts. The Women's Peace Party, for example, ceased its opposition to the war once the US entered it. The Socialist Party of America took an official stance against US involvement, but supported Wilson in most of his activities.

On June 15, 1917, Goldman and Berkman were arrested during a raid of their offices, in which authorities seized "a wagon load of anarchist records and propaganda". "The New York Times" reported that Goldman asked to change into a more appropriate outfit, and emerged in a gown of "royal purple". The pair were charged with conspiracy to "induce persons not to register" under the newly enacted Espionage Act, and were held on US$25,000 bail each. Defending herself and Berkman during their trial, Goldman invoked the First Amendment, asking how the government could claim to fight for democracy abroad while suppressing free speech at home:

We say that if America has entered the war to make the world safe for democracy, she must first make democracy safe in America. How else is the world to take America seriously, when democracy at home is daily being outraged, free speech suppressed, peaceable assemblies broken up by overbearing and brutal gangsters in uniform; when free press is curtailed and every independent opinion gagged? Verily, poor as we are in democracy, how can we give of it to the world? 

The jury found Goldman and Berkman guilty. Judge Julius Marshuetz Mayer imposed the maximum sentence: two years' imprisonment, a $10,000 fine each, and the possibility of deportation after their release from prison. As she was transported to Missouri State Penitentiary, Goldman wrote to a friend: "Two years imprisonment for having made an uncompromising stand for one's ideal. Why that is a small price."

In prison, she was assigned to work as a seamstress, under the eye of a "miserable gutter-snipe of a 21-year-old boy paid to get results". She met the socialist Kate Richards O'Hare, who had also been imprisoned under the Espionage Act. Although they differed on political strategy— O'Hare believed in voting to achieve state power—the two women came together to agitate for better conditions among prisoners. Goldman also met and became friends with Gabriella Segata Antolini, an anarchist and follower of Luigi Galleani. Antolini had been arrested transporting a satchel filled with dynamite on a Chicago-bound train. She had refused to cooperate with authorities, and was sent to prison for 14 months. Working together to make life better for the other inmates, the three women became known as "The Trinity". Goldman was released on September 27, 1919.

Goldman and Berkman were released from prison during the United States' Red Scare of 1919–20, when public anxiety about wartime pro-German activities had expanded into a pervasive fear of Bolshevism and the prospect of an imminent radical revolution. It was a time of social unrest due to union organizing strikes and actions by activist immigrants. Attorney General Alexander Mitchell Palmer and J. Edgar Hoover, head of the US Department of Justice's General Intelligence Division (now the FBI), were intent on using the Anarchist Exclusion Act and its 1918 expansion to deport any non-citizens they could identify as advocates of anarchy or revolution. "Emma Goldman and Alexander Berkman," Hoover wrote while they were in prison, "are, beyond doubt, two of the most dangerous anarchists in this country and return to the community will result in undue harm."

At her deportation hearing on October 27, Goldman refused to answer questions about her beliefs, on the grounds that her American citizenship invalidated any attempt to deport her under the Anarchist Exclusion Act, which could be enforced only against non-citizens of the US. She presented a written statement instead: "Today so-called aliens are deported. Tomorrow native Americans will be banished. Already some patrioteers are suggesting that native American sons to whom democracy is a sacred ideal should be exiled." Louis Post at the Department of Labor, which had ultimate authority over deportation decisions, determined that the revocation of her husband Kershner's American citizenship in 1908 after his conviction had revoked hers as well. After initially promising a court fight, Goldman decided not to appeal his ruling.

The Labor Department included Goldman and Berkman among 249 aliens it deported "en masse," mostly people with only vague associations with radical groups, who had been swept up in government raids in November. "Buford", a ship the press nicknamed the "Soviet Ark," sailed from the Army's New York Port of Embarkation on December 21. Some 58 enlisted men and four officers provided security on the journey, and pistols were distributed to the crew. Most of the press approved enthusiastically. The Cleveland "Plain Dealer" wrote: "It is hoped and expected that other vessels, larger, more commodious, carrying similar cargoes, will follow in her wake." The ship landed her charges in Hanko, Finland on Saturday, January 17, 1920. Upon arrival in Finland, authorities there conducted the deportees to the Russian frontier under a flag of truce.

Goldman initially viewed the Bolshevik revolution in a positive light. She wrote in "Mother Earth" that despite its dependence on Communist government, it represented "the most fundamental, far-reaching and all-embracing principles of human freedom and of economic well-being". By the time she neared Europe, however, she expressed fears about what was to come. She was worried about the ongoing Russian Civil War and the possibility of being seized by anti-Bolshevik forces. The state, anti-capitalist though it was, also posed a threat. "I could never in my life work within the confines of the State," she wrote to her niece, "Bolshevist or otherwise."
She quickly discovered that her fears were justified. Days after returning to Petrograd (Saint Petersburg), she was shocked to hear a party official refer to free speech as a "bourgeois superstition". As she and Berkman traveled around the country, they found repression, mismanagement, and corruption instead of the equality and worker empowerment they had dreamed of. Those who questioned the government were demonized as counter-revolutionaries, and workers labored under severe conditions. They met with Vladimir Lenin, who assured them that government suppression of press liberties was justified. He told them: "There can be no free speech in a revolutionary period." Berkman was more willing to forgive the government's actions in the name of "historical necessity", but he eventually joined Goldman in opposing the Soviet state's authority.

In March 1921, strikes erupted in Petrograd when workers took to the streets demanding better food rations and more union autonomy. Goldman and Berkman felt a responsibility to support the strikers, stating: "To remain silent now is impossible, even criminal." The unrest spread to the port town of Kronstadt, where the government ordered a military response to suppress striking soldiers and sailors. In the Kronstadt rebellion, approximately 1,000 rebelling sailors and soldiers were killed and two thousand more were arrested; many were later executed. In the wake of these events, Goldman and Berkman decided there was no future in the country for them. "More and more", she wrote, "we have come to the conclusion that we can do nothing here. And as we can not keep up a life of inactivity much longer we have decided to leave."

In December 1921, they left the country and went to the Latvian capital city of Riga. The US commissioner in that city wired officials in Washington DC, who began requesting information from other governments about the couple's activities. After a short trip to Stockholm, they moved to Berlin for several years; during this time Goldman agreed to write a series of articles about her time in Russia for Joseph Pulitzer's newspaper, the "New York World." These were later collected and published in book form as "My Disillusionment in Russia" (1923) and "My Further Disillusionment in Russia" (1924). The publishers added these titles to attract attention; Goldman protested, albeit in vain.

Goldman found it difficult to acclimate to the German leftist community in Berlin. Communists despised her outspokenness about Soviet repression; liberals derided her radicalism. While Berkman remained in Berlin helping Russian exiles, Goldman moved to London in September 1924. Upon her arrival, the novelist Rebecca West arranged a reception dinner for her, attended by philosopher Bertrand Russell, novelist H. G. Wells, and more than 200 other guests. When she spoke of her dissatisfaction with the Soviet government, the audience was shocked. Some left the gathering; others berated her for prematurely criticizing the Communist experiment. Later, in a letter, Russell declined to support her efforts at systemic change in the Soviet Union and ridiculed her anarchist idealism.

In 1925, the spectre of deportation loomed again, but a Scottish anarchist named James Colton offered to marry her and provide British citizenship. Although they were only distant acquaintances, she accepted and they were married on June 27, 1925. Her new status gave her peace of mind, and allowed her to travel to France and Canada. Life in London was stressful for Goldman; she wrote to Berkman: "I am awfully tired and so lonely and heartsick. It is a dreadful feeling to come back here from lectures and find not a kindred soul, no one who cares whether one is dead or alive." She worked on analytical studies of drama, expanding on the work she had published in 1914. But the audiences were "awful," and she never finished her second book on the subject.

Goldman traveled to Canada in 1927, just in time to receive news of the impending executions of Italian anarchists Nicola Sacco and Bartolomeo Vanzetti in Boston. Angered by the many irregularities of the case, she saw it as another travesty of justice in the US. She longed to join the mass demonstrations in Boston; memories of the Haymarket affair overwhelmed her, compounded by her isolation. "Then," she wrote, "I had my life before me to take up the cause for those killed. Now I have nothing."

In 1928, she began writing her autobiography, with the support of a group of American admirers, including journalist H. L. Mencken, poet Edna St. Vincent Millay, novelist Theodore Dreiser and art collector Peggy Guggenheim, who raised $4,000 for her. She secured a cottage in the French coastal city of Saint-Tropez and spent two years recounting her life. Berkman offered sharply critical feedback, which she eventually incorporated at the price of a strain on their relationship. Goldman intended the book, "Living My Life," as a single volume for a price the working class could afford (she urged no more than $5.00); her publisher Alfred A. Knopf, however, released it as two volumes sold together for $7.50. Goldman was furious, but unable to force a change. Due in large part to the Great Depression, sales were sluggish despite keen interest from libraries around the US. Critical reviews were generally enthusiastic; "The New York Times", "The New Yorker", and "Saturday Review of Literature" all listed it as one of the year's top non-fiction books.

In 1933, Goldman received permission to lecture in the United States under the condition that she speak only about drama and her autobiography—but not current political events. She returned to New York on February 2, 1934 to generally positive press coverage—except from Communist publications. Soon she was surrounded by admirers and friends, besieged with invitations to talks and interviews. Her visa expired in May, and she went to Toronto in order to file another request to visit the US. However, this second attempt was denied. She stayed in Canada, writing articles for US publications.

In February and March 1936, Berkman underwent a pair of prostate gland operations. Recuperating in Nice and cared for by his companion, Emmy Eckstein, he missed Goldman's sixty-seventh birthday in Saint-Tropez in June. She wrote in sadness, but he never read the letter; she received a call in the middle of the night that Berkman was in great distress. She left for Nice immediately but when she arrived that morning, Goldman found that he had shot himself and was in a nearly comatose paralysis. He died later that evening.

In July 1936, the Spanish Civil War started after an attempted "coup d'état" by parts of the Spanish Army against the government of the Second Spanish Republic. At the same time, the Spanish anarchists, fighting against the Nationalist forces, started an anarchist revolution. Goldman was invited to Barcelona and in an instant, as she wrote to her niece, "the crushing weight that was pressing down on my heart since Sasha's death left me as by magic". She was welcomed by the Confederación Nacional del Trabajo (CNT) and Federación Anarquista Ibérica (FAI) organizations, and for the first time in her life lived in a community run by and for anarchists, according to true anarchist principles. "In all my life", she wrote later, "I have not met with such warm hospitality, comradeship and solidarity." After touring a series of collectives in the province of Huesca, she told a group of workers: "Your revolution will destroy forever [the notion] that anarchism stands for chaos." She began editing the weekly "CNT-FAI Information Bulletin" and responded to English-language mail.

Goldman began to worry about the future of Spain's anarchism when the CNT-FAI joined a coalition government in 1937—against the core anarchist principle of abstaining from state structures—and, more distressingly, made repeated concessions to Communist forces in the name of uniting against fascism. In November 1936, she wrote that cooperating with Communists in Spain was "a denial of our comrades in Stalin's concentration camps". Russia, meanwhile, refused to send weapons to anarchist forces, and disinformation campaigns were being waged against the anarchists across Europe and the US. Her faith in the movement unshaken, Goldman returned to London as an official representative of the CNT-FAI.

Delivering lectures and giving interviews, Goldman enthusiastically supported the Spanish anarcho-syndicalists. She wrote regularly for "Spain and the World", a biweekly newspaper focusing on the civil war. In May 1937, however, Communist-led forces attacked anarchist strongholds and broke up agrarian collectives. Newspapers in England and elsewhere accepted the timeline of events offered by the Second Spanish Republic at face value. British journalist George Orwell, present for the crackdown, wrote: "[T]he accounts of the Barcelona riots in May ... beat everything I have ever seen for lying."

Goldman returned to Spain in September, but the CNT-FAI appeared to her like people "in a burning house". Worse, anarchists and other radicals around the world refused to support their cause. The Nationalist forces declared victory in Spain just before she returned to London. Frustrated by England's repressive atmosphere—which she called "more fascist than the fascists"—she returned to Canada in 1939. Her service to the anarchist cause in Spain was not forgotten, however. On her seventieth birthday, the former Secretary-General of the CNT-FAI, Mariano Vázquez, sent a message to her from Paris, praising her for her contributions and naming her as "our spiritual mother". She called it "the most beautiful tribute I have ever received".

As the events preceding World War II began to unfold in Europe, Goldman reiterated her opposition to wars waged by governments. "[M]uch as I loathe Hitler, Mussolini, Stalin and Franco", she wrote to a friend, "I would not support a war against them and for the democracies which, in the last analysis, are only Fascist in disguise." She felt that Britain and France had missed their opportunity to oppose fascism, and that the coming war would only result in "a new form of madness in the world".

On Saturday, February 17, 1940, Goldman suffered a debilitating stroke. She became paralyzed on her right side, and although her hearing was unaffected, she could not speak. As one friend described it: "Just to think that here was Emma, the greatest orator in America, unable to utter one word." For three months she improved slightly, receiving visitors and on one occasion gesturing to her address book to signal that a friend might find friendly contacts during a trip to Mexico. She suffered another stroke on May 8, however, and on May 14 she died in Toronto, aged 70.

The US Immigration and Naturalization Service allowed her body to be brought back to the United States. She was buried in German Waldheim Cemetery (now named Forest Home Cemetery) in Forest Park, Illinois, a western suburb of Chicago, near the graves of those executed after the Haymarket affair. The bas relief on her grave marker was created by sculptor Jo Davidson, and the stone includes the quote "Liberty will not descend to a people, a people must raise themselves to liberty".

Goldman spoke and wrote extensively on a wide variety of issues. While she rejected orthodoxy and fundamentalist thinking, she was an important contributor to several fields of modern political philosophy. 

She was influenced by many diverse thinkers and writers, including Mikhail Bakunin, Henry David Thoreau, Peter Kropotkin, Ralph Waldo Emerson, Nikolai Chernyshevsky, and Mary Wollstonecraft. Another philosopher who influenced Goldman was Friedrich Nietzsche. In her autobiography, she wrote: "Nietzsche was not a social theorist, but a poet, a rebel, and innovator. His aristocracy was neither of birth nor of purse; it was the spirit. In that respect Nietzsche was an anarchist, and all true anarchists were aristocrats."

Anarchism was central to Goldman's view of the world and she is today considered one of the most important figures in the history of anarchism. First drawn to it during the persecution of anarchists after the 1886 Haymarket affair, she wrote and spoke regularly on behalf of anarchism. In the title essay of her book "Anarchism and Other Essays", she wrote:
Anarchism, then, really stands for the liberation of the human mind from the dominion of religion; the liberation of the human body from the dominion of property; liberation from the shackles and restraint of government. Anarchism stands for a social order based on the free grouping of individuals for the purpose of producing real social wealth; an order that will guarantee to every human being free access to the earth and full enjoyment of the necessities of life, according to individual desires, tastes, and inclinations.
Goldman's anarchism was intensely personal. She believed it was necessary for anarchist thinkers to live their beliefs, demonstrating their convictions with every action and word. "I don't care if a man's theory for tomorrow is correct," she once wrote. "I care if his spirit of today is correct." Anarchism and free association were to her logical responses to the confines of government control and capitalism. "It seems to me that "these" are the new forms of life," she wrote, "and that they will take the place of the old, not by preaching or voting, but by living them."

At the same time, she believed that the movement on behalf of human liberty must be staffed by liberated humans. While dancing among fellow anarchists one evening, she was chided by an associate for her carefree demeanor. In her autobiography, Goldman wrote:
I told him to mind his own business, I was tired of having the Cause constantly thrown in my face. I did not believe that a Cause which stood for a beautiful ideal, for anarchism, for release and freedom from conventions and prejudice, should demand denial of life and joy. I insisted that our Cause could not expect me to behave as a nun and that the movement should not be turned into a cloister. If it meant that, I did not want it. "I want freedom, the right to self-expression, everybody's right to beautiful, radiant things."
Goldman, in her political youth, held targeted violence to be a legitimate means of revolutionary struggle. Goldman at the time believed that the use of violence, while distasteful, could be justified in relation to the social benefits it might accrue. She advocated propaganda of the deed—"attentat", or violence carried out to encourage the masses to revolt. She supported her partner Alexander Berkman's attempt to kill industrialist Henry Clay Frick, and even begged him to allow her to participate. She believed that Frick's actions during the Homestead strike were reprehensible and that his murder would produce a positive result for working people. "Yes," she wrote later in her autobiography, "the end in this case justified the means." While she never gave explicit approval of Leon Czolgosz's assassination of US President William McKinley, she defended his ideals and believed actions like his were a natural consequence of repressive institutions. As she wrote in "The Psychology of Political Violence": "the accumulated forces in our social and economic life, culminating in an act of violence, are similar to the terrors of the atmosphere, manifested in storm and lightning."

Her experiences in Russia led her to qualify her earlier belief that revolutionary ends might justify violent means. In the afterword to "My Disillusionment in Russia", she wrote: "There is no greater fallacy than the belief that aims and purposes are one thing, while methods and tactics are another... The means employed become, through individual habit and social practice, part and parcel of the final purpose..." In the same chapter, however, Goldman affirmed that "Revolution is indeed a violent process," and noted that violence was the "tragic inevitability of revolutionary upheavals..." Some misinterpreted her comments on the Bolshevik terror as a rejection of all militant force, but Goldman corrected this in the preface to the first US edition of "My Disillusionment in Russia":

The argument that destruction and terror are part of revolution I do not dispute. I know that in the past every great political and social change necessitated violence...Black slavery might still be a legalized institution in the United States but for the militant spirit of the John Browns. I have never denied that violence is inevitable, nor do I gainsay it now. Yet it is one thing to employ violence in combat, as a means of defense. It is quite another thing to make a principle of terrorism, to institutionalize it, to assign it the most vital place in the social struggle. Such terrorism begets counter-revolution and in turn itself becomes counter-revolutionary.

Goldman saw the militarization of Soviet society not as a result of armed resistance per se, but of the statist vision of the Bolsheviks, writing that "an insignificant minority bent on creating an absolute State is necessarily driven to oppression and terrorism."

Goldman believed that the economic system of capitalism was incompatible with human liberty. "The only demand that property recognizes," she wrote in "Anarchism and Other Essays", "is its own gluttonous appetite for greater wealth, because wealth means power; the power to subdue, to crush, to exploit, the power to enslave, to outrage, to degrade." She also argued that capitalism dehumanized workers, "turning the producer into a mere particle of a machine, with less will and decision than his master of steel and iron."

Originally opposed to anything less than complete revolution, Goldman was challenged during one talk by an elderly worker in the front row. In her autobiography, she wrote:
He said that he understood my impatience with such small demands as a few hours less a day, or a few dollars more a week... But what were men of his age to do? They were not likely to live to see the ultimate overthrow of the capitalist system. Were they also to forgo the release of perhaps two hours a day from the hated work? That was all they could hope to see realized in their lifetime.
Goldman realized that smaller efforts for improvement such as higher wages and shorter hours could be part of a social revolution.

Goldman viewed the state as essentially and inevitably a tool of control and domination. As a result, Goldman believed that voting was useless at best and dangerous at worst. Voting, she wrote, provided an illusion of participation while masking the true structures of decision-making. Instead, Goldman advocated targeted resistance in the form of strikes, protests, and "direct action against the invasive, meddlesome authority of our moral code". She maintained an anti-voting position even when many anarcho-syndicalists in 1930s Spain voted for the formation of a liberal republic. Goldman wrote that any power anarchists wielded as a voting bloc should instead be used to strike across the country. She disagreed with the movement for women's suffrage, which demanded the right of women to vote. In her essay "Woman Suffrage", she ridicules the idea that women's involvement would infuse the democratic state with a more just orientation: "As if women have not sold their votes, as if women politicians cannot be bought!" She agreed with the suffragists' assertion that women are equal to men, but disagreed that their participation alone would make the state more just. "To assume, therefore, that she would succeed in purifying something which is not susceptible of purification, is to credit her with supernatural powers."

Goldman was also a passionate critic of the prison system, critiquing both the treatment of prisoners and the social causes of crime. Goldman viewed crime as a natural outgrowth of an unjust economic system, and in her essay "Prisons: A Social Crime and Failure", she quoted liberally from the 19th-century authors Fyodor Dostoevsky and Oscar Wilde on prisons, and wrote: Year after year the gates of prison hells return to the world an emaciated, deformed, will-less, shipwrecked crew of humanity, with the Cain mark on their foreheads, their hopes crushed, all their natural inclinations thwarted. With nothing but hunger and inhumanity to greet them, these victims soon sink back into crime as the only possibility of existence.

Goldman was a committed war resister, believing that wars were fought by the state on behalf of capitalists. She was particularly opposed to the draft, viewing it as one of the worst of the state's forms of coercion, and was one of the founders of the No-Conscription League—for which she was ultimately arrested (1917), imprisoned and deported (1919).

Goldman was routinely surveilled, arrested, and imprisoned for her speech and organizing activities in support of workers and various strikes, access to birth control, and in opposition to World War I. As a result, she became active in the early 20th century free speech movement, seeing freedom of expression as a fundamental necessity for achieving social change. Her outspoken championship of her ideals, in the face of persistent arrests, inspired Roger Baldwin, one of the founders of the American Civil Liberties Union. Goldman's and Reitman's experiences in the San Diego free speech fight (1912) were notorious examples of state and capitalist repression of the Industrial Workers of the World's campaign of free speech fights.

Although she was hostile to the suffragist goals of first-wave feminism, Goldman advocated passionately for the rights of women, and is today heralded as a founder of anarcha-feminism, which challenges patriarchy as a hierarchy to be resisted alongside state power and class divisions. In 1897, she wrote: "I demand the independence of woman, her right to support herself; to live for herself; to love whomever she pleases, or as many as she pleases. I demand freedom for both sexes, freedom of action, freedom in love and freedom in motherhood."

A nurse by training, Goldman was an early advocate for educating women concerning contraception. Like many feminists of her time, she saw abortion as a tragic consequence of social conditions, and birth control as a positive alternative. Goldman was also an advocate of free love, and a strong critic of marriage. She saw early feminists as confined in their scope and bounded by social forces of Puritanism and capitalism. She wrote: "We are in need of unhampered growth out of old traditions and habits. The movement for women's emancipation has so far made but the first step in that direction."

Goldman was also an outspoken critic of prejudice against homosexuals. Her belief that social liberation should extend to gay men and lesbians was virtually unheard of at the time, even among anarchists. As German sexologist Magnus Hirschfeld wrote, "she was the first and only woman, indeed the first and only American, to take up the defense of homosexual love before the general public." In numerous speeches and letters, she defended the right of gay men and lesbians to love as they pleased and condemned the fear and stigma associated with homosexuality. As Goldman wrote in a letter to Hirschfeld, "It is a tragedy, I feel, that people of a different sexual type are caught in a world which shows so little understanding for homosexuals and is so crassly indifferent to the various gradations and variations of gender and their great significance in life."

A committed atheist, Goldman viewed religion as another instrument of control and domination. Her essay "The Philosophy of Atheism" quoted Bakunin at length on the subject and added:
Consciously or unconsciously, most theists see in gods and devils, heaven and hell, reward and punishment, a whip to lash the people into obedience, meekness and contentment... The philosophy of Atheism expresses the expansion and growth of the human mind. The philosophy of theism, if we can call it a philosophy, is static and fixed.
In essays like "The Hypocrisy of Puritanism" and a speech entitled "The Failure of Christianity", Goldman made more than a few enemies among religious communities by attacking their moralistic attitudes and efforts to control human behavior. She blamed Christianity for "the perpetuation of a slave society", arguing that it dictated individuals' actions on Earth and offered poor people a false promise of a plentiful future in heaven. She was also critical of Zionism, which she saw as another failed experiment in state control.

Goldman was well known during her life, described as—among other things—"the most dangerous woman in America". After her death and through the middle part of the 20th century, her fame faded. Scholars and historians of anarchism viewed her as a great speaker and activist, but did not regard her as a philosophical or theoretical thinker on par with, for example, Kropotkin.

In 1970, Dover Press reissued Goldman's biography, "Living My Life", and in 1972, feminist writer Alix Kates Shulman issued a collection of Goldman's writing and speeches, "Red Emma Speaks". These works brought Goldman's life and writings to a larger audience, and she was in particular lionized by the women's movement of the late 20th century. In 1973, Shulman was asked by a printer friend for a quotation by Goldman for use on a T-shirt. She sent him the selection from "Living My Life" about "the right to self-expression, everybody's right to beautiful, radiant things", recounting that she had been admonished "that it did not behoove an agitator to dance". The printer created a statement based on these sentiments that has become one of Goldman's most famous quotations, even though she probably never said or wrote it as such: "If I can't dance I don't want to be in your revolution." Variations of this saying have appeared on thousands of T-shirts, buttons, posters, bumper stickers, coffee mugs, hats, and other items.

The women's movement of the 1970s that "rediscovered" Goldman was accompanied by a resurgent anarchist movement, beginning in the late 1960s, which also reinvigorated scholarly attention to earlier anarchists. The growth of feminism also initiated some reevaluation of Goldman's philosophical work, with scholars pointing out the significance of Goldman's contributions to anarchist thought in her time. Goldman's belief in the value of aesthetics, for example, can be seen in the later influences of anarchism and the arts. Similarly, Goldman is now given credit for significantly influencing and broadening the scope of activism on issues of sexual liberty, reproductive rights, and freedom of expression.

Goldman has been depicted in numerous works of fiction over the years, including Warren Beatty's 1981 film "Reds", in which she was portrayed by Maureen Stapleton, who won an Academy Award for her performance. Goldman has also been a character in two Broadway musicals, "Ragtime" and "Assassins". Plays depicting Goldman's life include Howard Zinn's play, "Emma"; Martin Duberman's "Mother Earth"; Jessica Litwak's "Emma Goldman: Love, Anarchy, and Other Affairs" (about Goldman's relationship with Berkman and her arrest in connection with McKinley's assassination); Lynn Rogoff's "Love Ben, Love Emma" (about Goldman's relationship with Reitman); Carol Bolt's "Red Emma"; and Alexis Roblan's "Red Emma and the Mad Monk". Ethel Mannin's 1941 novel "Red Rose" is also based on Goldman's Life.

Goldman has been honored by a number of organizations named in her memory. The Emma Goldman Clinic, a women's health center located in Iowa City, Iowa, selected Goldman as a namesake "in recognition of her challenging spirit." Red Emma's Bookstore Coffeehouse, an infoshop in Baltimore, Maryland adopted her name out of their belief "in the ideas and ideals that she fought for her entire life: free speech, sexual and racial equality and independence, the right to organize in our jobs and in our own lives, ideas and ideals that we continue to fight for, even today".

Paul Gailiunas and his late wife Helen Hill co-wrote the anarchist song "Emma Goldman", which was performed and released by the band Piggy: The Calypso Orchestra of the Maritimes in 1999. The song was later performed by Gailiunas' new band The Troublemakers and released on their 2004 album "Here Come The Troublemakers".

UK punk band Martha's song "Goldman's Detective Agency" reimagines Goldman as a private detective investigating police and political corruption.

Goldman was a prolific writer, penning countless pamphlets and articles on a diverse range of subjects. She authored six books, including an autobiography, "Living My Life", and a biography of fellow anarchist Voltairine de Cleyre.







</doc>
<doc id="9765" url="https://en.wikipedia.org/wiki?curid=9765" title="Equuleus">
Equuleus

Equuleus is a constellation. Its name is Latin for "little horse", a foal. It was one of the 48 constellations listed by the 2nd century astronomer Ptolemy, and remains one of the 88 modern constellations. It is the second smallest of the modern constellations (after Crux), spanning only 72 square degrees. It is also very faint, having no stars brighter than the fourth magnitude.

The brightest star in Equuleus is Alpha Equulei, traditionally called Kitalpha, a yellow star magnitude 3.9, 186 light-years from Earth. Its traditional name means "the section of the horse".

There are few variable stars in Equuleus. Only around 25 are known, most of which are faint. Gamma Equulei is an alpha CVn star, ranging between magnitudes 4.58 and 4.77 over a period of around 12½ minutes. It is a white star 115 light-years from Earth, and has an optical companion of magnitude 6.1, 6 Equulei. It is divisible in binoculars. R Equulei is a Mira variable that ranges between magnitudes 8.0 and 15.7 over nearly 261 days.

Equuleus contains some double stars of interest. γ Equ consists of a primary star with a magnitude around 4.7 (slightly variable) and a secondary star of magnitude 11.6, separated by 2 arcseconds. Epsilon Equulei is a triple star also designated 1 Equulei. The system, 197 light-years away, has a primary of magnitude 5.4 that is itself a binary star; its components are of magnitude 6.0 and 6.3 and have a period of 101 years. The secondary is of magnitude 7.4 and is visible in small telescopes. The components of the primary are becoming closer together and will not be divisible in amateur telescopes beginning in 2015. δ Equ is a binary star with an orbital period of 5.7 years, which at one time was the shortest known orbital period for an optical binary. The two components of the system are never more than 0.35 arcseconds apart.

Due to its small size and its distance from the plane of the Milky Way, Equuleus contains no notable deep sky objects. Some very faint galaxies between magnitudes 13 and 15 include NGC 7015, NGC 7040, NGC 7045 and NGC 7046.

In Greek mythology, one myth associates Equuleus with the foal Celeris (meaning "swiftness" or "speed"), who was the offspring or brother of the winged horse Pegasus. Celeris was given to Castor by Mercury. Other myths say that Equuleus is the horse struck from Poseidon's trident, during the contest between him and Athena when deciding which would be the superior. Because this section of stars rises before Pegasus, it is often called Equus Primus, or the First Horse. Equuleus is also linked to the story of Philyra and Saturn.

Created by Hipparchus and included by Ptolemy, it abuts Pegasus; unlike the larger horse it is depicted as a horse's head alone.
In Chinese astronomy, the stars that correspond to Equuleus are located within the Black Tortoise of the North (北方玄武, "Běi Fāng Xuán Wǔ").

Equuleus is briefly mentioned in the "Martha Speaks" episode "Dogs in Space" as one of Helen Lorraine's favorite constellations.



</doc>
<doc id="9766" url="https://en.wikipedia.org/wiki?curid=9766" title="Eridanus">
Eridanus

Eridanus can refer to:





</doc>
