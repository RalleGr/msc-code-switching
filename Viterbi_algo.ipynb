{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Viterbi-Algo",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyE1ye3kAvIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required libraries \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from models.ngrams.word_ngrams import NGramModel\n",
        "from tools.utils import is_other\n",
        "from tools.utils import printStatus\n",
        "from tools.utils import merge_dictionaries\n",
        "import sys\n",
        "\n",
        "WORD_LEVEL_DICTIONARIES_PATH = \"./dictionaries/word-level/\"\n",
        "n = 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# States\n",
        "states = ['lang1', 'lang2']\n",
        "\n",
        "# Observations\n",
        "observations = [['this', 'is', 'a', 'test', 'seq']]\n",
        "\n",
        "# Start probability\n",
        "start_probability = {'lang1': 0.5, 'lang2': 0.5}\n",
        "\n",
        "# Transition probability\n",
        "transition_probability = {\n",
        "   'lang1' : {'lang1': 0.8, 'lang2': 0.2},\n",
        "   'lang2' : {'lang1': 0.2, 'lang2': 0.8}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFTrD8LyAxpE",
        "colab_type": "code",
        "outputId": "690e7297-099e-40b3-9105-b6a27e39e801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "tags": []
      },
      "source": [
        "# Emission probability (our probability dictionaries)\n",
        "printStatus(\"Getting dictionaries...\")\n",
        "probability_en_df = pd.read_csv(WORD_LEVEL_DICTIONARIES_PATH + 'probability_dict_en.csv', encoding='utf-16')\n",
        "probability_en_dict = probability_en_df.set_index('word')['probability'].to_dict()\n",
        "\n",
        "probability_es_df = pd.read_csv(WORD_LEVEL_DICTIONARIES_PATH + 'probability_dict_es.csv', encoding='utf-16')\n",
        "probability_es_dict = probability_es_df.set_index('word')['probability'].to_dict()\n",
        "printStatus(\"Dictionaries ready!\")\n",
        "\n",
        "data_en = {'lang1': probability_en_dict}\n",
        "data_es = {'lang2': probability_es_dict}\n",
        "emission_probability = merge_dictionaries(data_en, data_es)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[22:16:07] Getting dictionaries...\n[22:16:08] Dictionaries ready!\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "#Executing the Viterbi Algorithm\n",
        "predicted_tags = []                #intializing the predicted tags\n",
        "for x in range(len(observations)):   # for each tokenized sentence in the test data\n",
        "  s = observations[x]\n",
        "  #storing_values is a dictionary which stores the required values\n",
        "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
        "  storing_values = {}              \n",
        "  for q in range(len(s)):\n",
        "    step = s[q]\n",
        "    #for the starting word of the sentence\n",
        "    if q == 1:                \n",
        "      storing_values[q] = {}\n",
        "      for t in states:\n",
        "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
        "        try:\n",
        "          storing_values[q][t] = ['<s>',emission_probability[t][step]]\n",
        "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
        "        except:\n",
        "          storing_values[q][t] = ['<s>',0.0001]#*train_emission_prob[t][step]]\n",
        "\n",
        "    #if the word is not at the start of the sentence\n",
        "    if q>1:\n",
        "      storing_values[q] = {}\n",
        "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
        "      current_states  = tags_of_tokens[step]               # loading the current states\n",
        "      #calculation of the best previous state for each current state and then storing\n",
        "      #it in storing_values\n",
        "      for t in current_states:                             \n",
        "        temp = []\n",
        "        for pt in previous_states:                         \n",
        "          try:\n",
        "            temp.append(storing_values[q-1][pt][1]*[pt][t]*emission_probability[t][step])\n",
        "          except:\n",
        "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
        "        max_temp_index = temp.index(max(temp))\n",
        "        best_pt = previous_states[max_temp_index]\n",
        "        storing_values[q][t]=[best_pt,max(temp)]\n",
        "\n",
        "  #Backtracing to extract the best possible tags for the sentence\n",
        "  pred_tags = []\n",
        "  total_steps_num = storing_values.keys()\n",
        "  last_step_num = max(total_steps_num)\n",
        "  for bs in range(len(total_steps_num)):\n",
        "    step_num = last_step_num - bs\n",
        "    if step_num == last_step_num:\n",
        "      pred_tags.append('</s>')\n",
        "      pred_tags.append(storing_values[step_num]['<s>'][0])\n",
        "    if step_num<last_step_num and step_num>0:\n",
        "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0])\n",
        "  predicted_tags.append(list(reversed(pred_tags)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(predicted_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Calculating the accuracy based on tagging each word in the test data.\n",
        "right = 0 \n",
        "wrong = 0\n",
        "for i in range(len(observations)):\n",
        "  gt = test_tags[i]\n",
        "  pred = predicted_tags[i]\n",
        "  for h in range(len(gt)):\n",
        "    if gt[h] == pred[h]:\n",
        "      right = right+1\n",
        "    else:\n",
        "      wrong = wrong +1 \n",
        "\n",
        "print('Accuracy on the test data is: ',right/(right+wrong))\n",
        "print('Loss on the test data is: ',wrong/(right+wrong))"
      ]
    }
  ]
}