<doc id="26798" url="https://en.wikipedia.org/wiki?curid=26798" title="Saxhorn">
Saxhorn

The saxhorn is a family of valved brass instruments that have conical bores and deep cup-shaped mouthpieces. The saxhorn family was developed by Adolphe Sax, who is also known for creating the saxophone family. The sound of the saxhorn has a characteristic mellow tone quality and blends well with other brass.

The saxhorns form a family of seven brass instruments (although at one point ten different sizes seem to have existed). Designed for band use, they are pitched alternately in E and B, like the saxophone group. 

Modern saxhorns still manufactured and in use:

Historically, much confusion exists as to the nomenclature of the various instruments in different languages.

The following table lists the members of the saxhorn family as described in the orchestration texts of Hector Berlioz and Cecil Forsyth, the J. Howard Foote catalog of 1893, and modern names. The modern instrument names continue to exhibit inconsistency, denoted by a "/" between the two names in use. In the table "Pitch" means the concert pitch of notational Middle C on each instrument (2nd partial, no valves depressed) in scientific pitch notation.

This list is not exhaustive of historic nomenclature for the saxhorns, for which there may exist no comprehensive and authoritative source.

The saxhorn is based on the same three-valve system as most other valved brass instruments. Each member of the family is named after the root note produced by the second partial with no valves actuated. Each member nominally possesses or possessed the typical three-valve brass range from the note one tritone below that root note (second partial, all valves actuated) to the note produced by eighth partial with no valves actuated, i.e., the note two octaves above the root note.

All the modern members of the family are transposing instruments written in the treble clef with the root note produced by the second partial with no valves actuated being written as middle C, though the baritone horn often plays bass clef parts, especially in concert band music and when playing parts written for the trombone.

Developed during the mid-to-late 1830s, the saxhorn family was patented in Paris in 1845 by Adolphe Sax. During the 19th century, the debate as to whether the saxhorn family was truly new, or rather a development of previously existing instruments, was the subject of prolonged lawsuits.

Throughout the mid-1850s, Sax continued to experiment with the instrument's valve pattern.

The Trojan March ("Marche Troyenne") of the Berlioz opera Les Troyens (185658) features an on-stage band which includes a family of saxhorns.

Saxhorns were popularized by the distinguished Distin Quintet, who toured Europe during the mid-19th century. This family of musicians, publishers and instrument manufacturers had a significant impact on the growth of the brass band movement in Britain during the mid- to late-19th century.

The saxhorn was the most common brass instrument in American Civil War bands. The over-the-shoulder variety of the instrument was used, as the backward-pointing bell of the instrument allowed troops marching behind the band to hear the music.

Contemporary works featuring this instrument are Désiré Dondeyne's "Tubissimo" for bass tuba or saxhorn and piano (1983) and Olivier Messiaen's "Et exspecto resurrectionem mortuorum" (1964).






</doc>
<doc id="26799" url="https://en.wikipedia.org/wiki?curid=26799" title="Scanner">
Scanner

Scanner may refer to:







Barbara Sher uses the word "scanner" for someone who scans the surface of things, as opposed to "divers" or experts.
Other words for scanner includes polymath, renaissance soul, multitalent, generalist and multipotentialite (as in Multipotentiality).



</doc>
<doc id="26800" url="https://en.wikipedia.org/wiki?curid=26800" title="Sonic Team">
Sonic Team

The initial team, formed in 1990, was composed of staff from Sega's Consumer Development division, including programmer Yuji Naka, artist Naoto Ohshima, and level designer Hirokazu Yasuhara. The team took the name Sonic Team in 1991 with the release of their first game, "Sonic the Hedgehog," for the Sega Genesis. The game was a major success and contributed to millions of Genesis sales. The next "Sonic" games were developed by Naka and Yasuhara in America at Sega Technical Institute, while Ohshima worked on "Sonic CD" in Japan. Naka returned to Japan in late 1994 to become the head of CS3, later renamed R&D No. 8. During this time, the division took on the Sonic Team brand but developed games that do not feature Sonic, such as "Nights into Dreams" (1996) and "Burning Rangers" (1998).

Following the release of "Sonic Adventure" in 1998, some Sonic Team staff moved to the United States to form Sonic Team USA and develop "Sonic Adventure 2" (2001). With Sega's divestiture of its studios into separate companies, R&D No. 8 became SONICTEAM Ltd. in 2000, with Naka as CEO and Sonic Team USA as its subsidiary. Sega's financial troubles led to several major structural changes in the early 2000s; the United Game Artists studio was absorbed by Sonic Team in 2003, and Sonic Team USA became Sega Studios USA in 2004. After Sammy Corporation purchased Sega in 2004, Sonic Team was reincorporated to become Sega's GE1 research and development department, later renamed CS2.

Naka departed Sonic Team during the development of "Sonic the Hedgehog" (2006), and Sega Studios USA was merged back into Sonic Team in 2008. The following decade was marked by "Sonic" titles of varying reception, with department head Takashi Iizuka acknowledging that Sonic Team had prioritized shipping over quality.

In 1983, programmer Yuji Naka was hired into Sega's Consumer Development division. His first project was "Girl's Garden", which he and Hiroshi Kawaguchi created as part of their training process. For his next game, "Phantasy Star" (1987) for the Master System, Naka created pseudo-3D animation effects. He met artist Naoto Ohshima while working on the game.

During the late 1980s and early 1990s, a rivalry formed between Sega and Nintendo due to the release of their 16-bit video game consoles: the Sega Genesis and the Super Nintendo Entertainment System. Sega needed a mascot character that would be as synonymous with their brand as Mario was with Nintendo. Sega wanted a killer app and character that could appeal to an older demographic than preteens, demonstrate the capabilities of the Genesis, and ensure commercial success in North America.

Some sources indicate Sega of Japan held an internal competition to submit characters designs for a mascot, while designer Hirokazu Yasuhara said the direction was given only to himself, Ohshima, and Naka. Ohshima designed a blue hedgehog named Sonic, who was inserted into a prototype game created by Naka. The Sonic design was refined to be less aggressive and appeal to a wider audience before the division began development on their platform game "Sonic the Hedgehog". According to Ohshima, Sega was looking for a game that would sell well in the United States as well as in Japan. Ohshima and Naka already had the game and character ready, with Ohshima having worked with Sega's toy and stationery department on design ideas. Ohshima claims that the progress they had already made encouraged the company to select their proposal, as theirs was the only team to have put in a high amount of time and effort. This left him confident their proposal would be selected.

The "Sonic the Hedgehog" project began with just Naka and Ohshima, but grew to involve two programmers, two sound engineers, and three designers. Yasuhara joined to supervise Naka and Ohshima and develop levels, and became the lead designer. He satisfied Naka's request for a simple, one-button design by having Sonic do damage by jumping. "Sonic the Hedgehog" was released in 1991 and proved a major success, contributing to millions of sales of the Genesis. The development team took the name Sonic Team for the game's release. Naka has referred to Sonic Team as only a "team name" at this point.

Shortly after the release of "Sonic the Hedgehog", Naka, Yasuhara, and a number of other Japanese developers relocated to California to join Sega Technical Institute (STI), a development division established by Mark Cerny intended as an elite studio combining the design philosophies of American and Japanese developers. While Naka and Yasuhara developed "Sonic the Hedgehog 2" with STI, Ohshima worked on "Sonic CD," a sequel for the Sega CD add-on. Though Naka was not directly involved in the "Sonic CD" development, he exchanged design ideas with Ohshima.

Following the release of "Sonic & Knuckles" in 1994, Yasuhara quit, citing differences with Naka. Naka returned to Japan, having been offered a role as a producer. He was placed in charge of Sega's Consumer Development Department 3, also known as CS3. Naka was reunited with Ohshima and brought with him Takashi Iizuka, who had also worked with Naka's team at STI. In the mid-1990s, Sonic Team started work on new intellectual property, leading to the creation of "Nights into Dreams" (1996) and "Burning Rangers" (1998) for the Sega Saturn. Naka stated that the release of "Nights" is when Sonic Team was truly formed as a brand.

The Saturn did not achieve the commercial success of the Genesis, so Sega focused its efforts on a new console, the Dreamcast, which debuted in Japan in 1998. The Dreamcast was seen as opportunity for Sonic Team to revisit the "Sonic" series, which had stalled in recent years. Sonic Team was originally creating a fully 3D "Sonic" game for the Saturn, but development moved to the Dreamcast to align with Sega's plans. Takashi Iizuka led the project; Iizuka had long wanted to create a "Sonic" role-playing video game and felt the Dreamcast was powerful enough to achieve his vision. The game became "Sonic Adventure", launched in 1998, which became the bestselling Dreamcast game.

Around this time, CS3 was renamed to Sega Research and Development Department 8 (R&D #8). While sometimes referred to as AM8 or "Sega-AM8", based on the R&D structure being titled the Sega Amusement Machine Research and Development (AM) teams, Sonic Team focused solely on home console games. Until 2000, media referred to Sonic Team's designation as both R&D #8 and AM8.

In 1999, shortly after the release of "Sonic Adventure", twelve Sonic Team members relocated to San Francisco to establish Sonic Team USA, while others remained in Japan. Shortly afterward, a number of key employees—including Ohshima—left Sega to form a new studio, Artoon. Sonic Team achieved success in the arcade game market in 1999 with the launch of rhythm game "Samba de Amigo", released the following year for the Dreamcast. The studio also began developing online games; in 1999, they released "ChuChu Rocket!", a puzzle game that made use of the Dreamcast's online capabilities. In 2000, Sonic Team launched the role-playing game "Phantasy Star Online" to critical and commercial success.

Sega began to restructure its studios as part of the dissolution of Sega Enterprises and spun off its software divisions into subsidiary companies. When the departments took new names, Naka felt it important to preserve the Sonic Team brand name, and the division's new legal name as a company was SONICTEAM, Ltd. Naka was installed as the CEO, and Sonic Team USA became a subsidiary of the new company.

Despite a number of well-received games, Sega discontinued the Dreamcast in 2001 and exited the hardware business. Sega transitioned into a third-party developer and began developing games for multiple platforms. From 2000, Sonic Team in Japan began to release fewer games, with a few releases such as the puzzle game "Puyo Pop" and the action game "Billy Hatcher and the Giant Egg". The company changes and lack of a Sega console affected Sonic Team; according to Naka, in a 2006 interview, "Our approach was always to create strategic title concepts, which included the hardware. We do somewhat miss the idea of being able to address these constant challenges."

Early in 2003, Sega president Hideki Sato and COO Tetsu Kamaya announced they were stepping down from their roles, with Sato being replaced by Hisao Oguchi, the head of Hitmaker. As part of Oguchi's restructuring plan, he announced his intention to consolidate Sega's studios into "four or five core operations." Sonic Team was financially solvent and absorbed United Game Artists, another Sega subsidiary led by Tetsuya Mizuguchi and known for the music games "Space Channel 5" (1999) and "Rez" (2001).

In 2004, Japanese company Sammy acquired a controlling interest in Sega and formed Sega Sammy Corporation. Prior to the merger, Sega began the process of re-integrating its subsidiaries into the main company. Sonic Team USA became Sega Studios USA, while SONICTEAM Ltd. became Sega's Global Entertainment 1 research and development division (GE1). The team is still referred to as Sonic Team. Naka announced his departure on 8 May 2006 and formed a new studio, Prope, to focus on creating original games. In a 2012 interview, Naka stated that a reason that he left the company was that he would have been required to continue making "Sonic" games, and he no longer wished to do that. He left Sonic Team during the development of the 2006 game "Sonic the Hedgehog" (2006), released as part of the 15-year anniversary of the "Sonic" franchise. Noted for its bugs and design flaws, "Sonic the Hedgehog" was panned, as was "Sonic Unleashed" (2008). Both games were released for the PlayStation 3 and Xbox 360; Sonic Team also developed a series of "Sonic" games for the Wii and Nintendo DS, such as 2007's "Sonic and the Secret Rings".

By 2010, Sonic Team became CS Research and Development No. 2 (CS2), Sega Studios USA was reintegrated into the Japanese team, and Iizuka was installed as the head of the department. After a series of poorly received "Sonic" releases, Sonic Team refocused on speed and more traditional side-scrolling in "" and "", "Sonic Generations", and "Sonic Colors", which all received better reviews. In 2015, Iizuka recognized in an interview with "Polygon" that Sonic Team had prioritized shipping games over quality, and had not had enough involvement in later third-party "Sonic" games, such as "". He hoped the Sonic Team logo would stand as a "mark of quality"; he planned to release quality games and expand the "Sonic" brand, while retaining the modern Sonic design. In another interview, Iizuka stated that Sonic Team was not involved in the "Sonic Boom" and that they were developed by "Sega of America from back in the day". Sonic Team's first "Sonic" game exclusive to smartphones, "Sonic Runners", was released in 2015. An endless runner, it was designed to have more replay value than other games in the genre. "Sonic Runners" received mixed reviews and was unprofitable, resulting in its discontinuation a year later.

In 2017, Sonic Team developed and released "Sonic Forces", and oversaw the development of "Sonic Mania" by Christian Whitehead. "Forces" was aimed at a broad audience of young and adult players, while "Mania" was focused on fans of the original Genesis games. "Mania" became the best reviewed "Sonic" game in fifteen years following nearly two decades of mixed reviews for the franchise. At SXSW in March 2019, Iizuka confirmed Sonic Team was also working on a new "Sonic" title.

Sega Studios USA, formerly Sonic Team USA, was a division of Sega and of Sonic Team while Sonic Team was a subsidiary company. It was founded when twelve Sonic Team members, including Takashi Iizuka, relocated to San Francisco, California, in 1999, and were set as a subsidiary of SonicTeam Ltd. by 2000. The team worked on game development, translation, and market studies in the United States, until they returned to Japan and merged back into Sonic Team in 2008.

Sonic Team USA translated "Sonic Adventure" and tested "ChuChu Rocket!" in America before beginning work on "Sonic Adventure 2". They took inspiration from their location in San Francisco, as well as Yosemite National Park and other areas of the United States. "Sonic Adventure 2" was released on 20 June 2001, and was ported to the GameCube. The next Sonic Team USA project was "Sonic Heroes" (2003), the first "Sonic" game developed for multiple platforms. Sonic Team USA took a different approach with "Heroes" from the "Sonic Adventure" games, focusing on gameplay more similar to the Genesis games to which even casual gamers could adapt.

After SonicTeam Ltd. merged back into Sega in 2004, Sonic Team USA was renamed Sega Studios USA. The division's next project was "Shadow the Hedgehog", released in 2005, a spin-off starring Shadow. Unlike previous games, "Shadow the Hedgehog" was targeted at older players and featured different gameplay styles, including the use of guns and different endings to the game. "Shadow the Hedgehog" was critically panned for its mature themes and level design, but was a commercial success, selling at least 1.59 million units.

The final Sega Studios USA game was "", the sequel to "Nights into Dreams" and the first "Nights" game since the cancellation of "Air Nights" in 2000. Iizuka felt it was important to retain the original game's concepts while developing new mechanics, and released it on the Wii, a more family-oriented console. "Journey of Dreams" was also designed to have a more European feel, in contrast to the "Sonic" games, which were more American. The sound and CGI were completed by Sonic Team in Japan, while Sega Studios USA handled the rest of the development for the 2007 release.

Sega Studios USA oversaw the development of "Sonic Rivals" (2006) and "Sonic Rivals 2" (2007) by Backbone Entertainment. In 2008, Sega Studios USA merged with Sonic Team, making Iizuka the head of Sonic Team and a vice president of product development for Sega. In 2016, Iizuka relocated to Los Angeles to oversee development with the goal of making Sega's studios in Los Angeles "a centralized hub for the global brand".

Sonic Team has developed a number of video games, with many of them becoming bestsellers. The studio is best known for its "Sonic the Hedgehog" series of platform games, which account for the majority of Sonic Team's work; the 1991 release of "Sonic the Hedgehog" is considered one of the most important moments in video game history, as it propelled Genesis sales and displaced Nintendo as the leading video game company. Sonic Team have also developed a wide variety of other games, including action games such as "Nights into Dreams", "Burning Rangers", and "Billy Hatcher and the Giant Egg," the online puzzle game "ChuChu Rocket!", the online role-playing game "Phantasy Star Online," and the music game "Samba de Amigo". "Phantasy Star Online" is credited for introducing online RPGs to consoles and was the first online RPG for many players. According to Sean Smith of "Retro Gamer", few companies could claim to have released as many AAA games over such a long period, especially between 1991 and 2000. Some Sonic Team games, such as the original "Sonic" games for the Genesis and "Nights", are considered some of the best video games ever made. Iizuka has said Sonic Team would be open to developing a third "Nights" game or a sequel to "Knuckles' Chaotix" (1995), if Sega were to commission them.

Sega and Sonic Team have been criticized for their handling of "Sonic the Hedgehog" after the beginning of the 3D era of video games. Edwin Evans-Thirlwell of "Eurogamer" described the 3D Sonic games as "20-odd years of slowly accumulating bullshit", and that unlike Sonic's main competitor, Nintendo's "Mario" series, "Sonic" in 3D never had a "transcendental hit". Zolani Stewart of "Kotaku" argued that Sonic's portrayal starting with "Sonic Adventure" with the addition of voice acting and a greater focus on plot and character narrative changed Sonic into "a flat, lifeless husk of a character, who spits out slogans and generally has only one personality mode, the radical attitude dude, the sad recycled image of vague '90s cultural concept." Sega of America marketing director Al Nilsen and "Sonic Mania" developer Christian Whitehead said they felt the number of additional characters added to the series was problematic, with Whitehead describing the characters as "padding". In 2015, Sega CEO Haruki Satomi acknowledged that Sega had "partially betrayed" the trust of the longtime fans of their games and hoped to focus on quality over quantity.


</doc>
<doc id="26805" url="https://en.wikipedia.org/wiki?curid=26805" title="Sex">
Sex

Organisms of many species are specialized into male and female varieties, each known as a sex. Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. The gametes produced by an organism define its sex: males produce small gametes (e.g. spermatozoa, or sperm, in animals) while females produce large gametes (ova, or egg cells). Individual organisms which produce both male and female gametes are termed hermaphroditic. Gametes can be identical in form and function (known as isogamy), but, in many cases, an asymmetry has evolved such that two different types of gametes (heterogametes) exist (known as anisogamy).

Physical differences are often associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience. For instance, mate choice and sexual selection can accelerate the evolution of physical differences between the sexes.

Among humans and other mammals, males typically carry an X and a Y chromosome (XY), whereas females typically carry two X chromosomes (XX), which are a part of the XY sex-determination system. Humans may also be intersex. Other animals have various sex-determination systems, such as the ZW system in birds, the X0 system in insects, and various environmental systems, for example in reptiles and crustaceans. Fungi may also have more complex allelic mating systems, with sexes not accurately described as male, female, or hermaphroditic.

One of the basic properties of life is reproduction, the capacity to generate new individuals, and sex is an aspect of this process. Life has evolved from simple stages to more complex ones, and so have the reproduction mechanisms. Initially the reproduction was a replicating process that consists in producing new individuals that contain the same genetic information as the original or parent individual. This mode of reproduction is called "asexual", and it is still used by many species, particularly unicellular, but it is also very common in multicellular organisms, including many of those with sexual reproduction. In sexual reproduction, the genetic material of the offspring comes from two different individuals. Bacteria reproduce asexually, but undergo a process by which a part of the genetic material of an individual donor is transferred to another recipient.

Disregarding intermediates, the basic distinction between asexual and sexual reproduction is the way in which the genetic material is processed. Typically, prior to an asexual division, a cell duplicates its genetic information content, and then divides. This process of cell division is called mitosis. In sexual reproduction, there are special kinds of cells that divide without prior duplication of its genetic material, in a process named meiosis. The resulting cells are called gametes, and contain only half the genetic material of the parent cells. These gametes are the cells that are prepared for the sexual reproduction of the organism. Sex comprises the arrangements that enable sexual reproduction, and has evolved alongside the reproduction system, starting with similar gametes (isogamy) and progressing to systems that have different gamete types, such as those involving a large female gamete (ovum) and a small male gamete (sperm).

In complex organisms, the sex organs are the parts that are involved in the production and exchange of gametes in sexual reproduction. Many species, both plants and animals, have sexual specialization, and their populations are divided into male and female individuals. Conversely, there are also species in which there is no sexual specialization, and the same individuals both contain masculine and feminine reproductive organs, and they are called hermaphrodites. This is very frequent in plants.

Sexual reproduction first probably evolved about a billion years ago within ancestral single-celled eukaryotes. The reason for the evolution of sex, and the reason(s) it has survived to the present, are still matters of debate. Some of the many plausible theories include: that sex creates variation among offspring, sex helps in the spread of advantageous traits, that sex helps in the removal of disadvantageous traits, and that sex facilitates repair of germ-line DNA.

Sexual reproduction is a process specific to eukaryotes, organisms whose cells contain a nucleus and mitochondria. In addition to animals, plants, and fungi, other eukaryotes (e.g. the malaria parasite) also engage in sexual reproduction. Some bacteria use conjugation to transfer genetic material between cells; while not the same as sexual reproduction, this also results in the mixture of genetic traits.

The defining characteristic of sexual reproduction in eukaryotes is the difference between the gametes and the binary nature of fertilization. Multiplicity of gamete types within a species would still be considered a form of sexual reproduction. However, no third gamete type is known in multicellular plants or animals.

While the evolution of sex dates to the prokaryote or early eukaryote stage, the origin of chromosomal sex determination may have been fairly early in eukaryotes (see evolution of anisogamy). The ZW sex-determination system is shared by birds, some fish and some crustaceans. XY sex determination is used by most mammals, but also some insects, and plants ("Silene latifolia"). The X0 sex-determination is found in most arachnids, insects such as silverfish (Apterygota), dragonflies (Paleoptera) and grasshoppers (Exopterygota), and some nematodes, crustaceans, and gastropods.

No genes are shared between the avian ZW and mammal XY chromosomes, and from a comparison between chicken and human, the Z chromosome appeared similar to the autosomal chromosome 9 in human, rather than X or Y, suggesting that the ZW and XY sex-determination systems do not share an origin, but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals.
A paper from 2004 compared the chicken Z chromosome with platypus X chromosomes and suggested that the two systems are related.

Sexual reproduction in eukaryotes is a process whereby organisms produce offspring that combine genetic traits from both parents. Chromosomes are passed on from one generation to the next in this process. Each cell in the offspring has half the chromosomes of the mother and half of the father. Genetic traits are contained within the deoxyribonucleic acid (DNA) of chromosomes—by combining one of each type of chromosomes from each parent, an organism is formed containing a doubled set of chromosomes. This double-chromosome stage is called "diploid", while the single-chromosome stage is "haploid". Diploid organisms can, in turn, form haploid cells (gametes) that randomly contain one of each of the chromosome pairs, via meiosis. Meiosis also involves a stage of chromosomal crossover, in which regions of DNA are exchanged between matched types of chromosomes, to form a new pair of mixed chromosomes. Crossing over and fertilization (the recombining of single sets of chromosomes to make a new diploid) result in the new organism containing a different set of genetic traits from either parent.

In many organisms, the haploid stage has been reduced to just gametes specialized to recombine and form a new diploid organism. In plants the diploid organism produces haploid spores that undergo cell division to produce multicellular haploid organisms known as gametophytes that produce haploid gametes at maturity. In either case, gametes may be externally similar, particularly in size (isogamy), or may have evolved an asymmetry such that the gametes are different in size and other aspects (anisogamy). By convention, the larger gamete (called an ovum, or egg cell) is considered female, while the smaller gamete (called a spermatozoon, or sperm cell) is considered male. An individual that produces exclusively large gametes is female, and one that produces exclusively small gametes is male. An individual that produces both types of gametes is a hermaphrodite; in some cases hermaphrodites are able to self-fertilize and produce offspring on their own, without a second organism.

Most sexually reproducing animals spend their lives as diploid, with the haploid stage reduced to single-cell gametes. The gametes of animals have male and female forms—spermatozoa and egg cells. These gametes combine to form embryos which develop into a new organism.

The male gamete, a spermatozoon (produced in vertebrates within the testes), is a small cell containing a single long flagellum which propels it. Spermatozoa are extremely reduced cells, lacking many cellular components that would be necessary for embryonic development. They are specialized for motility, seeking out an egg cell and fusing with it in a process called fertilization.

Female gametes are egg cells (produced in vertebrates within the ovaries), large immobile cells that contain the nutrients and cellular components necessary for a developing embryo. Egg cells are often associated with other cells which support the development of the embryo, forming an egg. In mammals, the fertilized embryo instead develops within the female, receiving nutrition directly from its mother.

Animals are usually mobile and seek out a partner of the opposite sex for mating. Animals which live in the water can mate using external fertilization, where the eggs and sperm are released into and combine within the surrounding water. Most animals that live outside of water, however, use internal fertilization, transferring sperm directly into the female to prevent the gametes from drying up.

In most birds, both excretion and reproduction is done through a single posterior opening, called the cloaca—male and female birds touch cloaca to transfer sperm, a process called "cloacal kissing". In many other terrestrial animals, males use specialized sex organs to assist the transport of sperm—these male sex organs are called intromittent organs. In humans and other mammals this male organ is the penis, which enters the female reproductive tract (called the vagina) to achieve insemination—a process called sexual intercourse. The penis contains a tube through which semen (a fluid containing sperm) travels. In female mammals the vagina connects with the uterus, an organ which directly supports the development of a fertilized embryo within (a process called gestation).

Because of their motility, animal sexual behavior can involve coercive sex. Traumatic insemination, for example, is used by some insect species to inseminate females through a wound in the abdominal cavity—a process detrimental to the female's health.

Like animals, plants have specialized male and female gametes. Within seed plants, male gametes are produced by extremely reduced multicellular gametophytes known as pollen. The female gametes of seed plants are contained within ovules; once fertilized by male gametes produced by pollen these form seeds which, like eggs, contain the nutrients necessary for the development of the embryonic plant.

Many plants have flowers and these are the sexual organs of those plants. Flowers are usually hermaphroditic, producing both male and female gametes. The female parts, in the center of a flower, are the pistils, each unit consisting of a carpel, a style and a stigma. One or more of these reproductive units may be merged to form a single compound pistil. Within the carpels are ovules which develop into seeds after fertilization. The male parts of the flower are the stamens: these consist of long filaments arranged between the pistil and the petals that produce pollen in anthers at their tips. When a pollen grain lands upon the stigma on top of a carpel's style, it germinates to produce a pollen tube that grows down through the tissues of the style into the carpel, where it delivers male gamete nuclei to fertilize an ovule that eventually develops into a seed.

In pines and other conifers the sex organs are conifer cones and have male and female forms. The more familiar female cones are typically more durable, containing ovules within them. Male cones are smaller and produce pollen which is transported by wind to land in female cones. As with flowers, seeds form within the female cone after pollination.

Because plants are immobile, they depend upon passive methods for transporting pollen grains to other plants. Many plants, including conifers and grasses, produce lightweight pollen which is carried by wind to neighboring plants. Other plants have heavier, sticky pollen that is specialized for transportation by animals. The plants attract these insects or larger animals such as humming birds and bats with nectar-containing flowers. These animals transport the pollen as they move to other flowers, which also contain female reproductive organs, resulting in pollination.

Most fungi reproduce sexually, having both a haploid and diploid stage in their life cycles. These fungi are typically isogamous, lacking male and female specialization: haploid fungi grow into contact with each other and then fuse their cells. In some of these cases, the fusion is asymmetric, and the cell which donates only a nucleus (and not accompanying cellular material) could arguably be considered "male". Fungi may also have more complex allelic mating systems, with other sexes not accurately described as male, female, or hermaphroditic.

Some fungi, including baker's yeast, have mating types that create a duality similar to male and female roles. Yeast with the same mating type will not fuse with each other to form diploid cells, only with yeast carrying the other mating type.

Many species of higher fungi produce mushrooms as part of their sexual reproduction. Within the mushroom diploid cells are formed, later dividing into haploid spores. The height of the mushroom aids the dispersal of these sexually produced offspring.

The most basic sexual system is one in which all organisms are hermaphrodites, producing both male and female gametes. This is true of some animals (e.g. snails) and the majority of flowering plants. In many cases, however, specialization of sex has evolved such that some organisms produce only male or only female gametes. The biological cause for an organism developing into one sex or the other is called "sex determination". The cause may be genetic or non-genetic. Within animals and other organisms that have genetic sex determination systems, the determining factor may be the presence of a sex chromosome or other genetic differences. In plants also, such as the liverwort "Marchantia polymorpha" and the flowering plant genus "Silene" that have sexual dimorphism (monoicy or dioicy, respectively), sex may be determined by sex chromosomes. Non-genetic systems may use environmental cues, such as the temperature during early development in crocodiles, to determine the sex of the offspring.

In the majority of species with sex specialization, organisms are either male (producing only male gametes) or female (producing only female gametes). Exceptions are common—for example, the roundworm "C. elegans" has an hermaphrodite and a male sex (a system called androdioecy).

Sometimes an organism's development is intermediate between male and female, a condition called intersex. Sometimes intersex individuals are called "hermaphrodite"; but, unlike biological hermaphrodites, intersex individuals are unusual cases and are not typically fertile in both male and female aspects.

In genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring.

Humans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible for triggering male development. The "default sex," in the absence of a Y chromosome, is female-like. Thus, XX mammals are female and XY are male. In humans, biological sex is determined by five factors present at birth: the presence or absence of a Y chromosome (which alone determines the individual's "genetic sex"), the type of gonads, the sex hormones, the internal reproductive anatomy (such as the uterus in females), and the external genitalia.

XY sex determination is found in other organisms, including the common fruit fly and some plants. In some cases, including in the fruit fly, it is the number of X chromosomes that determines sex rather than the presence of a Y chromosome (see below).

In birds, which have a ZW sex-determination system, the opposite is true: the W chromosome carries factors responsible for female development, and default development is male. In this case ZZ individuals are male and ZW are female. The majority of butterflies and moths also have a ZW sex-determination system. In both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.

Many insects use a sex determination system based on the number of sex chromosomes. This is called X0 sex-determination—the 0 indicates the absence of the sex chromosome. All other chromosomes in these organisms are diploid, but organisms may inherit one or two X chromosomes. In field crickets, for example, insects with a single X chromosome develop as male, while those with two develop as female. In the nematode "C. elegans" most worms are self-fertilizing XX hermaphrodites, but occasionally abnormalities in chromosome inheritance regularly give rise to individuals with only one X chromosome—these X0 individuals are fertile males (and half their offspring are male).

Other insects, including honey bees and ants, use a haplodiploid sex-determination system. In this case, diploid individuals are generally female, and haploid individuals (which develop from unfertilized eggs) are male. This sex-determination system results in highly biased sex ratios, as the sex of offspring is determined by fertilization rather than the assortment of chromosomes during meiosis.

For many species, sex is not determined by inherited traits, but instead by environmental factors experienced during development or later in life. Many reptiles have temperature-dependent sex determination: the temperature embryos experience during their development determines the sex of the organism. In some turtles, for example, males are produced at lower incubation temperatures than females; this difference in critical temperatures can be as little as 1–2 °C.

Many fish change sex over the course of their lifespan, a phenomenon called sequential hermaphroditism. In clownfish, smaller fish are male, and the dominant and largest fish in a group becomes female. In many wrasses the opposite is true—most fish are initially female and become male when they reach a certain size. Sequential hermaphrodites may produce both types of gametes over the course of their lifetime, but at any given point they are either female or male.

In some ferns the default sex is hermaphrodite, but ferns which grow in soil that has previously supported hermaphrodites are influenced by residual hormones to instead develop as male.

Many animals and some plants have differences between the male and female sexes in size and appearance, a phenomenon called sexual dimorphism. Sex differences in humans include, generally, a larger size and more body hair in men; women have breasts, wider hips, and a higher body fat percentage. In other species, the differences may be more extreme, such as differences in coloration or bodyweight.

Sexual dimorphisms in animals are often associated with sexual selection—the competition between individuals of one sex to mate with the opposite sex. Antlers in male deer, for example, are used in combat between males to win reproductive access to female deer. In many cases the male of a species is larger than the female. Mammal species with extreme sexual size dimorphism tend to have highly polygynous mating systems—presumably due to selection for success in competition with other males—such as the elephant seals. Other examples demonstrate that it is the preference of females that drive sexual dimorphism, such as in the case of the stalk-eyed fly.

Other animals, including most insects and many fish, have larger females. This may be associated with the cost of producing egg cells, which requires more nutrition than producing sperm—larger females are able to produce more eggs. For example, female southern black widow spiders are typically twice as long as the males. Occasionally this dimorphism is extreme, with males reduced to living as parasites dependent on the female, such as in the anglerfish. Some plant species also exhibit dimorphism in which the females are significantly larger than the males, such as in the moss "Dicranum" and the liverwort "Sphaerocarpos". There is some evidence that, in these genera, the dimorphism may be tied to a sex chromosome, or to chemical signalling from females.

In birds, males often have a more colourful appearance and may have features (like the long tail of male peacocks) that would seem to put the organism at a disadvantage (e.g. bright colors would seem to make a bird more visible to predators). One proposed explanation for this is the handicap principle. This hypothesis says that, by demonstrating he can survive with such handicaps, the male is advertising his genetic fitness to females—traits that will benefit daughters as well, who will not be encumbered with such handicaps.




</doc>
<doc id="26808" url="https://en.wikipedia.org/wiki?curid=26808" title="Star">
Star

A star is an astronomical object consisting of a luminous spheroid of plasma held together by its own gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye from Earth during the night, appearing as a multitude of fixed luminous points in the sky due to their immense distance from Earth. Historically, the most prominent stars were grouped into constellations and asterisms, the brightest of which gained proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable Universe contains an estimated stars, but most are invisible to the naked eye from Earth, including all stars outside our galaxy, the Milky Way.

For most of its active life, a star shines due to thermonuclear fusion of hydrogen into helium in its core, releasing energy that traverses the star's interior and then radiates into outer space. Almost all naturally occurring elements heavier than helium are created by stellar nucleosynthesis during the star's lifetime, and for some stars by supernova nucleosynthesis when it explodes. Near the end of its life, a star can also contain degenerate matter. Astronomers can determine the mass, age, metallicity (chemical composition), and many other properties of a star by observing its motion through space, its luminosity, and spectrum respectively. The total mass of a star is the main factor that determines its evolution and eventual fate. Other characteristics of a star, including diameter and temperature, change over its life, while the star's environment affects its rotation and movement. A plot of the temperature of many stars against their luminosities produces a plot known as a Hertzsprung–Russell diagram (H–R diagram). Plotting a particular star on that diagram allows the age and evolutionary state of that star to be determined.

A star's life begins with the gravitational collapse of a gaseous nebula of material composed primarily of hydrogen, along with helium and trace amounts of heavier elements. When the stellar core is sufficiently dense, hydrogen becomes steadily converted into helium through nuclear fusion, releasing energy in the process. The remainder of the star's interior carries energy away from the core through a combination of radiative and convective heat transfer processes. The star's internal pressure prevents it from collapsing further under its own gravity. A star with mass greater than 0.4 times the Sun's will expand to become a red giant when the hydrogen fuel in its core is exhausted. In some cases, it will fuse heavier elements at the core or in shells around the core. As the star expands it throws a part of its mass, enriched with those heavier elements, into the interstellar environment, to be recycled later as new stars. Meanwhile, the core becomes a stellar remnant: a white dwarf, a neutron star, or, if it is sufficiently massive, a black hole.

Binary and multi-star systems consist of two or more stars that are gravitationally bound and generally move around each other in stable orbits. When two such stars have a relatively close orbit, their gravitational interaction can have a significant impact on their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.

Historically, stars have been important to civilizations throughout the world. They have been part of religious practices and used for celestial navigation and orientation. Many ancient astronomers believed that stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped stars into constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth's rotational axis relative to its local star, the Sun.

The oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c. 1531–1155 BC).

The first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1020 stars, and was used to assemble Ptolemy's star catalogue. Hipparchus is known for the discovery of the first recorded "nova" (new star). Many of the constellations and star names in use today derive from Greek astronomy.

In spite of the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as the SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.

Medieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly for the purpose of producing "Zij" star catalogues. Among these, the "Book of Fixed Stars" (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi's Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and also gave the latitudes of various stars during a lunar eclipse in 1019.

According to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence. 
Early European astronomers such as Tycho Brahe identified new stars in the night sky (later termed "novae"), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.

The Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.

William Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is also noted for his discovery that some stars do not merely lie along the same line of sight, but are also physical companions that form binary star systems.

The science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere's absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. However, the modern version of the stellar classification scheme was developed by Annie J. Cannon during the 1900s.
The first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.
The twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.

Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.
With the exception of supernovae, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for our
galaxy). But some stars have been observed in the M100 galaxy of the Virgo Cluster, about 100 million light years from the Earth.
In the Local Supercluster it is possible to see star clusters, and current telescopes could in principle observe faint individual stars in the Local Group (see Cepheids). However, outside the Local Supercluster of galaxies, neither individual stars nor clusters of stars have been observed. The only exception is a faint image of a large star cluster containing hundreds of thousands of stars located at a distance of one billion light years—ten times further than the most distant star cluster previously observed.

In February 2018, astronomers reported, for the first time, a signal of the reionization epoch, an indirect detection of light from the earliest stars formed—about 180 million years after the Big Bang.

In April, 2018, astronomers reported the detection of the most distant "ordinary" (i.e., main sequence) star, named Icarus (formally, MACS J1149 Lensed Star 1), at 9 billion light-years away from Earth.

In May 2018, astronomers reported the detection of the most distant oxygen ever detected in the Universe—and the most distant galaxy ever observed by Atacama Large Millimeter Array or the Very Large Telescope—with the team inferring that the signal was emitted 13.3 billion years ago (or 500 million years after the Big Bang). They found that the observed brightness of the galaxy is well-explained by a model where the onset of star formation corresponds to only 250 million years after the Universe began, corresponding to a redshift of about 15.

The concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were also given names, particularly with Arabic or Latin designations.

As well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were also Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)

Circa 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star's right ascension was invented and added to John Flamsteed's star catalogue in his book ""Historia coelestis Britannica"" (the 1712 edition), whereby this numbering system came to be called "Flamsteed designation" or "Flamsteed numbering".

The only internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars, which the British Library calls an unregulated commercial enterprise. The IAU has disassociated itself from this commercial practice, and these names are neither recognized by the IAU, professional astronomers, nor the amateur astronomy community. One such star-naming company is the International Star Registry, which, during the 1980s, was accused of deceptive practice for making it appear that the assigned name was official. This now-discontinued ISR practice was informally labeled a scam and a fraud, and the New York City Department of Consumer and Worker Protection issued a violation against ISR for engaging in a deceptive trade practice.

Although stellar parameters can be expressed in SI units or CGS units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of "nominal" solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:

The solar mass M was not explicitly defined by the IAU due to the large relative uncertainty (10) of the Newtonian gravitational constant G. However, since the product of the Newtonian gravitational constant and solar mass
together (GM) has been determined to much greater precision, the IAU defined the "nominal" solar mass parameter to be:

However, one can combine the nominal solar mass parameter with the most recent (2014) CODATA estimate of the Newtonian gravitational constant G to derive the solar mass to be approximately 1.9885 × 10 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.

Large lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.

Stars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as "molecular clouds"—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars.
Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.

All stars spend the majority of their existence as "main sequence stars", fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:

The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.

As the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 to 15 million years.
Early stars of less than 2 are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.
These jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.

Early in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.

Most stars are observed to be members of binary star systems, and the properties of those binaries are the result of the conditions in which they formed. A gas cloud must lose its angular momentum in order to collapse and form a star. The fragmentation of the cloud into multiple stars distributes some of that angular momentum. The primordial binaries transfer some angular momentum by gravitational interactions during close encounters with other stars in young stellar clusters. These interactions tend to split apart more widely separated (soft) binaries while causing hard binaries to become more tightly bound. This produces the separation of binaries into their two observed populations distributions.

Stars spend about 90% of their existence fusing hydrogen into helium in high-temperature and high-pressure reactions near the core. Such stars are said to be on the main sequence, and are called dwarf stars. Starting at zero-age main sequence, the proportion of helium in a star's core will steadily increase, the rate of nuclear fusion at the core will slowly increase, as will the star's temperature and luminosity.
The Sun, for example, is estimated to have increased in luminosity by about 40% since it reached the main sequence 4.6 billion (4.6 × 10) years ago.

Every star generates a stellar wind of particles that causes a continual outflow of gas into space. For most stars, the mass lost is negligible. The Sun loses 10 every year, or about 0.01% of its total mass over its entire lifespan. However, very massive stars can lose 10 to 10 each year, significantly affecting their evolution. Stars that begin with more than 50 can lose over half their total mass while on the main sequence.

The time a star spends on the main sequence depends primarily on the amount of fuel it has and the rate at which it fuses it. The Sun is expected to live 10 billion (10) years. Massive stars consume their fuel very rapidly and are short-lived. Low mass stars consume their fuel very slowly. Stars less massive than 0.25 , called red dwarfs, are able to fuse nearly all of their mass while stars of about 1 can only fuse about 10% of their mass. The combination of their slow fuel-consumption and relatively large usable fuel supply allows low mass stars to last about one trillion (10) years; the most extreme of 0.08 ) will last for about 12 trillion years. Red dwarfs become hotter and more luminous as they accumulate helium. When they eventually run out of hydrogen, they contract into a white dwarf and decline in temperature. However, since the lifespan of such stars is greater than the current age of the universe (13.8 billion years), no stars under about 0.85 are expected to have moved off the main sequence.

Besides mass, the elements heavier than helium can play a significant role in the evolution of stars. Astronomers label all elements heavier than helium "metals", and call the chemical concentration of these elements in a star, its metallicity. A star's metallicity can influence the time the star takes to burn its fuel, and controls the formation of its magnetic fields, which affects the strength of its stellar wind. Older, population II stars have substantially less metallicity than the younger, population I stars due to the composition of the molecular clouds from which they formed. Over time, such clouds become increasingly enriched in heavier elements as older stars die and shed portions of their atmospheres.

As stars of at least 0.4 exhaust their supply of hydrogen at their core, they start to fuse hydrogen in a shell outside the helium core. Their outer layers expand and cool greatly as they form a red giant. In about 5 billion years, when the Sun enters the helium burning phase, it will expand to a maximum radius of roughly , 250 times its present size, and lose 30% of its current mass.

As the hydrogen shell burning produces more helium, the core increases in mass and temperature. In a red giant of up to 2.25 , the mass of the helium core becomes degenerate prior to helium fusion. Finally, when the temperature increases sufficiently, helium fusion begins explosively in what is called a helium flash, and the star rapidly shrinks in radius, increases its surface temperature, and moves to the horizontal branch of the HR diagram. For more massive stars, helium core fusion starts before the core becomes degenerate, and the star spends some time in the red clump, slowly burning helium, before the outer convective envelope collapses and the star then moves to the horizontal branch.

After the star has fused the helium of its core, the carbon product fuses producing a hot core with an outer shell of fusing helium. The star then follows an evolutionary path called the asymptotic giant branch (AGB) that parallels the other described red giant phase, but with a higher luminosity. The more massive AGB stars may undergo a brief period of carbon fusion before the core becomes degenerate.

During their helium-burning phase, a star of more than 9 solar masses expands to form first a blue and then a red supergiant. Particularly massive stars may evolve to a Wolf-Rayet star, characterised by spectra dominated by emission lines of elements heavier than hydrogen, which have reached the surface due to strong convection and intense mass loss.

When helium is exhausted at the core of a massive star, the core contracts and the temperature and pressure rises enough to fuse carbon (see Carbon-burning process). This process continues, with the successive stages being fueled by neon (see neon-burning process), oxygen (see oxygen-burning process), and silicon (see silicon-burning process). Near the end of the star's life, fusion continues along a series of onion-layer shells within a massive star. Each shell fuses a different element, with the outermost shell fusing hydrogen; the next shell fusing helium, and so forth.

The final stage occurs when a massive star begins producing iron. Since iron nuclei are more tightly bound than any heavier nuclei, any fusion beyond iron does not produce a net release of energy.

As a star's core shrinks, the intensity of radiation from that surface increases, creating such radiation pressure on the outer shell of gas that it will push those layers away, forming a planetary nebula. If what remains after the outer atmosphere has been shed is less than roughly 1.4 , it shrinks to a relatively tiny object about the size of Earth, known as a white dwarf. White dwarfs lack the mass for further gravitational compression to take place. The electron-degenerate matter inside a white dwarf is no longer a plasma, even though stars are generally referred to as being spheres of plasma. Eventually, white dwarfs fade into black dwarfs over a very long period of time.

In massive stars, fusion continues until the iron core has grown so large (more than 1.4 ) that it can no longer support its own mass. This core will suddenly collapse as its electrons are driven into its protons, forming neutrons, neutrinos, and gamma rays in a burst of electron capture and inverse beta decay. The shockwave formed by this sudden collapse causes the rest of the star to explode in a supernova. Supernovae become so bright that they may briefly outshine the star's entire home galaxy. When they occur within the Milky Way, supernovae have historically been observed by naked-eye observers as "new stars" where none seemingly existed before.

A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 . In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core. Within a black hole, the matter is in a state that is not currently understood.

The blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.

The post–main-sequence evolution of binary stars may be significantly different from the evolution of single stars of the same mass. If stars in a binary system are sufficiently close, when one of the stars expands to become a red giant it may overflow its Roche lobe, the region around a star where material is gravitationally bound to that star, leading to transfer of material to the other. When the Roche lobe is violated, a variety of phenomena can result, including contact binaries, common-envelope binaries, cataclysmic variables, and type Ia supernovae.

Stars are not spread uniformly across the universe, but are normally grouped into galaxies along with interstellar gas and dust. A typical galaxy contains hundreds of billions of stars, and there are more than 2 trillion (10) galaxies. Overall, there are as many as an estimated stars (more stars than all the grains of sand on planet Earth). While it is often believed that stars only exist within galaxies, intergalactic stars have been discovered.

A multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars are also found. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups called star clusters also exist. These range from loose stellar associations with only a few stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy.

It has been a long-held assumption that the majority of stars occur in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, where 80% of the stars are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, most stars in the Milky Way are likely single from birth.
The nearest star to the Earth, apart from the Sun, is Proxima Centauri, which is 39.9 trillion kilometres, or 4.2 light-years. Travelling at the orbital speed of the Space Shuttle (8 kilometres per second—almost 30,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.

Due to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature than the other main sequence stars with the same luminosity of the cluster to which it belongs.

Almost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution, lifespan, and its eventual fate.

Most stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the Universe, determined by the Planck satellite as 13.799 ± 0.021).

The more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.

When stars form in the present Milky Way galaxy they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.

The star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. There also exist chemically peculiar stars that show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules.

Due to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is also a star, but it is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.

The disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, and so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.

Stars range in size from neutron stars, which vary anywhere from 20 to in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 1,000 times that of our sun. Betelgeuse, however, has a much lower density than the Sun.

The motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.

Radial velocity is measured by the doppler shift of the star's spectral lines, and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.

When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds, and are referred to as stellar associations.

The magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.

Young, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During
the Maunder Minimum, for example, the Sun underwent a
70-year period with almost no sunspot activity.

One of the most massive stars known is Eta Carinae, which,
with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests as an upper limit for stars in the current era of the universe. This
represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but
it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 limit on massive star formation.
The first stars to form after the Big Bang may have been larger, up to 300 , due
to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at .

With a mass only 80 times that of Jupiter (), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For
stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 . When the metallicity is very low, however, the minimum star size seems to be about 8.3% of the solar mass, or about 87 . Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.

The combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.

The rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.

Degenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.

The surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. Note that the effective temperature is only a representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.

The stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).

Massive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they also have a high luminosity due to their large exterior surface area.

The energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which
streams from the outer layers as electrically charged protons and alpha and beta particles. Although almost massless, there also exists a steady stream of neutrinos emanating from the star's core.

The production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.

The color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides
visible light, stars also emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.

Using the stellar spectrum, astronomers can also determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can also estimate the age of the star.

The luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.

Patches of the star's surface with a lower temperature and luminosity than average are known as starspots. Small, "dwarf" stars such as our Sun generally have essentially featureless disks with only small starspots. "Giant" stars have much larger, more obvious starspots, and
they also exhibit strong stellar limb darkening. That is, the brightness decreases towards the edge of the stellar disk. Red
dwarf flare stars such as UV Ceti may also possess prominent starspot features.

The apparent brightness of a star is expressed in terms of its apparent magnitude. It is a function of the star's luminosity, its distance from Earth, the extinction effect of interstellar dust and gas, and the altering of the star's light as it passes through Earth's atmosphere. Intrinsic or absolute magnitude is directly related to a star's luminosity, and is what the apparent magnitude a star would be if the distance between the Earth and the star were 10 parsecs (32.6 light-years).

Both the apparent and absolute magnitude scales are logarithmic units: one whole number difference in magnitude is equal to a brightness variation of about 2.5 times (the 5th root of 100 or approximately 2.512). This means that a first magnitude star (+1.00) is about 2.5 times brighter than a second magnitude (+2.00) star, and about 100 times brighter than a sixth magnitude star (+6.00). The faintest stars visible to the naked eye under good seeing conditions are about magnitude +6.

On both apparent and absolute magnitude scales, the smaller the magnitude number, the brighter the star; the larger the magnitude number, the fainter the star. The brightest stars, on either scale, have negative magnitude numbers. The variation in brightness (Δ"L") between two stars is calculated by subtracting the magnitude number of the brighter star ("m") from the magnitude number of the fainter star ("m"), then using the difference as an exponent for the base number 2.512; that is to say:

Relative to both luminosity and distance from Earth, a star's absolute magnitude ("M") and apparent magnitude ("m") are not equivalent; for example, the bright star Sirius has an apparent magnitude of −1.44, but it has an absolute magnitude of +1.41.

The Sun has an apparent magnitude of −26.7, but its absolute magnitude is only +4.83. Sirius, the brightest star in the night sky as seen from Earth, is approximately 23 times more luminous than the Sun, while Canopus, the second brightest star in the night sky with an absolute magnitude of −5.53, is approximately 14,000 times more luminous than the Sun. Despite Canopus being vastly more luminous than Sirius, however, Sirius appears brighter than Canopus. This is because Sirius is merely 8.6 light-years from the Earth, while Canopus is much farther away at a distance of 310 light-years.

As of 2006, the star with the highest known absolute magnitude is LBV 1806-20, with a magnitude of −14.2. This star is at least 5,000,000 times more luminous than the Sun. The least luminous stars that are currently known are located in the NGC 6397 cluster. The faintest red dwarfs in the cluster were magnitude 26, while a 28th magnitude white dwarf was also discovered. These faint stars are so dim that their light is as bright as a birthday candle on the Moon when viewed from the Earth.

The current stellar classification system originated in the early 20th century, when stars were classified from "A" to "Q" based on the strength of the hydrogen line. It was thought that the hydrogen line strength was a simple linear function of temperature. Instead, it was more complicated: it strengthened with increasing temperature, peaked near 9000 K, and then declined at greater temperatures. The classifications were since reordered by temperature, on which the modern scheme is based.

Stars are given a single-letter classification according to their spectra, ranging from type "O", which are very hot, to "M", which are so cool that molecules may form in their atmospheres. The main classifications in order of decreasing surface temperature are: "O, B, A, F, G, K", and "M". A variety of rare spectral types are given special classifications. The most common of these are types "L" and "T", which classify the coldest low-mass stars and brown dwarfs. Each letter has 10 sub-divisions, numbered from 0 to 9, in order of decreasing temperature. However, this system breaks down at extreme high temperatures as classes "O0" and "O1" may not exist.

In addition, stars may be classified by the luminosity effects found in their spectral lines, which correspond to their spatial size and is determined by their surface gravity. These range from "0" (hypergiants) through "III" (giants) to "V" (main sequence dwarfs); some authors add "VII" (white dwarfs). Main sequence stars fall along a narrow, diagonal band when graphed according to their absolute magnitude and spectral type. The Sun is a main sequence "G2V" yellow dwarf of intermediate temperature and ordinary size.

Additional nomenclature, in the form of lower-case letters added to the end of the spectral type to indicate peculiar features of the spectrum. For example, an ""e"" can indicate the presence of emission lines; ""m"" represents unusually strong levels of metals, and ""var"" can mean variations in the spectral type.

White dwarf stars have their own class that begins with the letter "D". This is further sub-divided into the classes "DA", "DB", "DC", "DO", "DZ", and "DQ", depending on the types of prominent lines found in the spectrum. This is followed by a numerical value that indicates the temperature.

Variable stars have periodic or random changes in luminosity because of intrinsic or extrinsic properties. Of the intrinsically variable stars, the primary types can be subdivided into three principal groups.

During their stellar evolution, some stars pass through phases where they can become pulsating variables. Pulsating variable stars vary in radius and luminosity over time, expanding and contracting with periods ranging from minutes to years, depending on the size of the star. This category includes Cepheid and Cepheid-like stars, and long-period variables such as Mira.

Eruptive variables are stars that experience sudden increases in luminosity because of flares or mass ejection events. This group includes protostars, Wolf-Rayet stars, and flare stars, as well as giant and supergiant stars.

Cataclysmic or explosive variable stars are those that undergo a dramatic change in their properties. This group includes novae and supernovae. A binary star system that includes a nearby white dwarf can produce certain types of these spectacular stellar explosions, including the nova and a Type 1a supernova. The explosion is created when the white dwarf accretes hydrogen from the companion star, building up mass until the hydrogen undergoes fusion. Some novae are also recurrent, having periodic outbursts of moderate amplitude.

Stars can also vary in luminosity because of extrinsic factors, such as eclipsing binaries, as well as rotating stars that produce extreme starspots. A notable example of an eclipsing binary is Algol, which regularly varies in magnitude from 2.1 to 3.4 over a period of 2.87 days.

The interior of a stable star is in a state of hydrostatic equilibrium: the forces on any small volume almost exactly counterbalance each other. The balanced forces are inward gravitational force and an outward force due to the pressure gradient within the star. The pressure gradient is established by the temperature gradient of the plasma; the outer part of the star is cooler than the core. The temperature at the core of a main sequence or giant star is at least on the order of 10 K. The resulting temperature and pressure at the hydrogen-burning core of a main sequence star are sufficient for nuclear fusion to occur and for sufficient energy to be produced to prevent further collapse of the star.

As atomic nuclei are fused in the core, they emit energy in the form of gamma rays. These photons interact with the surrounding plasma, adding to the thermal energy at the core. Stars on the main sequence convert hydrogen into helium, creating a slowly but steadily increasing proportion of helium in the core. Eventually the helium content becomes predominant, and energy production ceases at the core. Instead, for stars of more than 0.4 , fusion occurs in a slowly expanding shell around the degenerate helium core.

In addition to hydrostatic equilibrium, the interior of a stable star will also maintain an energy balance of thermal equilibrium. There is a radial temperature gradient throughout the interior that results in a flux of energy flowing toward the exterior. The outgoing flux of energy leaving any layer within the star will exactly match the incoming flux from below.

The radiation zone is the region of the stellar interior where the flux of energy outward is dependent on radiative heat transfer, since convective heat transfer is inefficient in that zone. In this region the plasma will not be perturbed, and any mass motions will die out. If this is not the case, however, then the plasma becomes unstable and convection will occur, forming a convection zone. This can occur, for example, in regions where very high energy fluxes occur, such as near the core or in areas with high opacity (making radiatative heat transfer inefficient) as in the outer envelope.

The occurrence of convection in the outer envelope of a main sequence star depends on the star's mass. Stars with several times the mass of the Sun have a convection zone deep within the interior and a radiative zone in the outer layers. Smaller stars such as the Sun are just the opposite, with the convective zone located in the outer layers. Red dwarf stars with less than 0.4 are convective throughout, which prevents the accumulation of a helium core. For most stars the convective zones will also vary over time as the star ages and the constitution of the interior is modified.
The photosphere is that portion of a star that is visible to an observer. This is the layer at which the plasma of the star becomes transparent to photons of light. From here, the energy generated at the core becomes free to propagate into space. It is within the photosphere that sun spots, regions of lower than average temperature, appear.

Above the level of the photosphere is the stellar atmosphere. In a main sequence star such as the Sun, the lowest level of the atmosphere, just above the photosphere, is the thin chromosphere region, where spicules appear and stellar flares begin. Above this is the transition region, where the temperature rapidly increases within a distance of only . Beyond this is the corona, a volume of super-heated plasma that can extend outward to several million kilometres. The existence of a corona appears to be dependent on a convective zone in the outer layers of the star. Despite its high temperature, and the corona emits very little light, due to its low gas density. The corona region of the Sun is normally only visible during a solar eclipse.

From the corona, a stellar wind of plasma particles expands outward from the star, until it interacts with the interstellar medium. For the Sun, the influence of its solar wind extends throughout a bubble-shaped region called the heliosphere.

A variety of nuclear fusion reactions take place in the cores of stars, that depend upon their mass and composition. When nuclei fuse, the mass of the fused product is less than the mass of the original parts. This lost mass is converted to electromagnetic energy, according to the mass–energy equivalence relationship "E" = "mc".

The hydrogen fusion process is temperature-sensitive, so a moderate increase in the core temperature will result in a significant increase in the fusion rate. As a result, the core temperature of main sequence stars only varies from 4 million kelvin for a small M-class star to 40 million kelvin for a massive O-class star.

In the Sun, with a 10-million-kelvin core, hydrogen fuses to form helium in the proton–proton chain reaction:

These reactions result in the overall reaction:

where e is a positron, γ is a gamma ray photon, ν is a neutrino, and H and He are isotopes of hydrogen and helium, respectively. The energy released by this reaction is in millions of electron volts, which is actually only a tiny amount of energy. However enormous numbers of these reactions occur constantly, producing all the energy necessary to sustain the star's radiation output. In comparison, the combustion of two hydrogen gas molecules with one oxygen gas molecule releases only 5.7 eV.

In more massive stars, helium is produced in a cycle of reactions catalyzed by carbon called the carbon-nitrogen-oxygen cycle.

In evolved stars with cores at 100 million kelvin and masses between 0.5 and 10 , helium can be transformed into carbon in the triple-alpha process that uses the intermediate element beryllium:

For an overall reaction of:

In massive stars, heavier elements can also be burned in a contracting core through the neon-burning process and oxygen-burning process. The final stage in the stellar nucleosynthesis process is the silicon-burning process that results in the production of the stable isotope iron-56. Any further fusion would be an endothermic process that consumes energy, and so further energy can only be produced through gravitational collapse.
The table at the left shows the amount of time required for a star of 20 to consume all of its nuclear fuel. As an O-class main sequence star, it would be 8 times the solar radius and 62,000 times the Sun's luminosity.



</doc>
<doc id="26809" url="https://en.wikipedia.org/wiki?curid=26809" title="StarCraft (video game)">
StarCraft (video game)

StarCraft is a 1998 military science fiction real-time strategy game developed and published by Blizzard Entertainment for Microsoft Windows. The game spawned the StarCraft franchise, and became the first game of the video game series. A Classic Mac OS version was released in 1999, and a Nintendo 64 adaptation, co-developed with Mass Media, was released in 2000.

Blizzard started work on the game shortly after "", another real-time strategy game, was released in 1995. The first incarnation debuted at the 1996 Electronic Entertainment Expo, where it was unfavorably compared to "Warcraft II". As a result, the project was entirely overhauled before being showcased to the public in early 1997, at which time it received a far more positive response. The game's multiplayer is particularly popular in South Korea, as of 2006, where players and teams participate in , earn sponsorships, and compete in televised tournaments.

Set in a fictitious future timeline during the 25th century AD in a distant part of the Milky Way galaxy known as the "Koprulu Sector", the game revolves around three intelligent species fighting for dominance: the Terrans are humans exiled from Earth who are now skilled at adapting to any situation; the Zerg are a race of insectoid aliens in pursuit of genetic perfection and obsessed with assimilating other races; the Protoss are a humanoid species with advanced technology and psionic abilities who are attempting to preserve their civilization and strict philosophy about their way of life from the Zerg.

Many journalists of the video game industry have praised "StarCraft" as one of the most important, and one of the greatest video games of all time. The game is also said to have raised the bar for developing real-time strategy (RTS) games. With more than 11 million copies sold worldwide by February 2009, "StarCraft" became one of the best-selling games for the personal computer. It has been praised for pioneering the use of unique "factions" in RTS gameplay, and for having a compelling story.

"StarCraft" has had its storyline adapted and expanded through a series of novels published between 2000 and 2016, the expansion pack "", and two officially authorized add-ons, and "Retribution". A sequel, "", was released in July 2010, which generated two expansion packs and a campaign pack between 2013 and 2016, while of the original and its expansion pack was released in August 2017. The original game, along with the expansion, was released for free in April 2017.

Blizzard Entertainment's use of three distinct races in "StarCraft" is widely credited with revolutionizing the real-time strategy genre. All units are unique to their respective races, and while rough comparisons can be drawn between certain types of units in the technology tree, every unit performs differently and requires different tactics for a player to succeed.

The psionic and technologically adept Protoss have access to powerful units and machinery and advanced technologies such as energy shields and localized warp capabilities, powered by their psionic traits. However, their forces have lengthy and expensive manufacturing processes, encouraging players to follow a strategy of the quality of their units over the quantity. The insectoid Zerg possess entirely organic units and structures, which can be produced quickly and at a far cheaper cost to resources, but are accordingly weaker, relying on sheer numbers and speed to overwhelm enemies. The humanoid Terrans provide a middle ground between the other two races, providing units that are versatile and flexible. They have access to a range of more ballistic military technologies and machinery, such as tanks and nuclear weapons.

Although each race is unique in its composition, no race has an innate advantage over the other. Each species is balanced out so that while they have different strengths, powers, and abilities, their overall strength is the same. The balance stays complete via infrequent patches (game updates) provided by Blizzard.

"StarCraft" features artificial intelligence that scales in difficulty, although the player cannot change the difficulty level in the single-player campaigns. Each campaign starts with enemy factions running easy AI modes, scaling through the course of the campaign to the hardest AI modes. In the level editor provided with the game, a designer has access to four levels of AI difficulties: "easy", "medium", "hard", and "insane", each setting differing in the units and technologies allowed to an AI faction and the extent of the AI's tactical and strategic planning. The single-player campaign consists of thirty missions, split into ten for each race.

Each race relies on two resources to sustain their game economies and to build their forces: minerals and vespene gas. Minerals are needed for all units and structures, and they are obtained by using a worker unit to harvest the resource directly from mineral nodes scattered around the battlefield. Players require vespene gas to construct advanced units and buildings, and they acquire it by constructing a gas extraction building on top of a geyser and using worker units to extract the gas from it. In addition, players need to regulate the supplies for their forces to ensure that they can construct the number of units they need. Although the nature of the supply differs between the races—Terrans use physical supplies held in depots, Protoss use psionic energy channeled from their homeworld via pylons, and Zerg are regulated by the number of controlling overlord units present—the supply mechanic essentially works in exactly the same way for each race (just with differing impacts on gameplay), allowing players to create new units when there are sufficient resources to sustain them.

Protoss and Zerg building construction is limited to specific locations: Protoss buildings need to be linked to a power grid, while almost every Zerg structure must be placed on a carpet of biomass, called "creep", that is produced by certain structures. Terran buildings are far less limited, with certain primary base structures possessing the ability to take off and fly slowly to a new location. Terran buildings, however, require the worker unit to continue construction on the building until it is completed. Also, once a Terran building has taken a certain amount of damage, it will catch fire and can eventually burn to the ground without further enemy action if repairs are not performed by a worker unit. The Protoss, by contrast, only require a worker unit to begin the process of transporting a building to the theater of operations via warp, and their buildings' shields (but not their structure) are regenerative. The Zerg worker unit physically transforms into the structure created, which is capable of slowly healing itself.

Multiplayer on "StarCraft" is powered through Blizzard Entertainment's Battle.net Internet service. Through this, a maximum of eight players can compete in a variety of game modes, including simply destroying all other players (which may be competitive, as in Ladder play, or non-ranked, as in melee play), to king of the hill and capture the flag objective-based games. In addition, the game incorporates a variety of specialized scenarios for different types of game, such as simulating a football game, using the Terran hoverbike unit to conduct a bike race, or hosting a Zerg hunting competition. "StarCraft" is also one of the few games that include a spawn installation, which allows for limited multiplayer. It must be installed from a disc, and requires a product key to work just as the full version does. However, one product key can support up to eight spawned installations with access to Battle.net. Limitations of a spawned installation include the inability to play single-player missions, create multiplayer games, or use the campaign editor. Newer releases of the game available through Battle.net or discs that include the Windows Vista label don't support the spawn installation.

"StarCraft" takes place in a science fiction universe created by Chris Metzen and James Phinney for Blizzard Entertainment. According to the story presented in the game's manual, the overpopulation of Earth in the early 24th century has caused the international governing body, known as United Powers League (which was later succeeded by United Earth Directorate), to exile certain members of the human race, such as criminals, the cybernetically enhanced, and genetic mutants, to colonize the far reaches of the galaxy. An attempt to colonize a nearby solar system goes wrong, resulting in humanity's arrival in the Koprulu Sector. In the distant Koprulu Sector of the galaxy, the exiles form several governments, but quickly fall into conflict with each other. One government, the Confederacy of Man, eventually emerges as the strongest faction, but its oppressive nature and brutal methods of suppressing dissidents stir up major rebel opposition in the form of a terrorist group called the Sons of Korhal. Just prior to the beginning of the game, in December 2499, an alien race possessing advanced technology and psionic power, the Protoss, makes first contact with humanity by destroying a Confederate colony world without any prior warning. Soon after this, the Terrans discover that a second alien race, the insectoid Zerg, has been stealthily infesting the surface of several of the Terran colonies, and that the Protoss are destroying the planets to prevent the Zerg from spreading. With the Confederacy threatened by two alien races and internal rebellion, it begins to crumble.

The player assumes the role of three nameless characters over the course of the game. In the first act, the player acts as the Confederate magistrate of an outlying colony world of Mar Sara, threatened by both the Zerg and the Protoss, and is forced through events to join the rebel Sons of Korhal under its leader Arcturus Mengsk. Mengsk's campaign is accompanied by Jim Raynor, a morally conscious law enforcement officer from Mar Sara, and Sarah Kerrigan, a psychic assassin and Mengsk's second-in-command. The second episode of the game sees the player as a cerebrate, a commander within the Zerg Swarm. The player is ruled over by the Zerg Overmind — the manifestation of the collective consciousness of the Swarm and the game's primary antagonist — and is given advice from other cerebrates of higher rank and status while accomplishing the objectives of the Swarm. In the final part of "StarCraft", the player is a newly appointed Executor within the Protoss military reporting to Aldaris, a representative of the Protoss government. Aldaris is at odds with Tassadar — the former occupant of the player's position — over his association with Zeratul, a member of a heretical group known as dark templar.

The story of "StarCraft" is presented through its instruction manual, the briefings to each mission, and conversations within the missions themselves, along with the use of cinematic cutscenes at key points. The game itself is split into three episodes, one for the player to command each race. In the first segment of the game, the player and Jim Raynor are attempting to control the colony of Mar Sara in the wake of the Zerg attacks on other Terran worlds. After the Confederacy arrests Raynor for destroying Confederate property, despite the fact that it had been infested by the Zerg, the player joins Arcturus Mengsk and the Sons of Korhal. Raynor, who is freed by Mengsk's troops, also joins and frequently accompanies the player on missions. Mengsk then begins to use Confederate technology captured on Mar Sara to lure the Zerg to Confederate installations and further his own goals. After forcing Confederate general Edmund Duke to join him, Mengsk sacrifices his own second-in-command, Sarah Kerrigan, to ensure the destruction of the Confederacy by luring the Zerg to the Confederate capital Tarsonis. Raynor is outraged by Mengsk's true aims of obtaining power at any cost and deserts, taking with him a small army of the former colonial militia of Mar Sara. Mengsk reorganizes what remains of the Terran population into the Terran Dominion, crowning himself as emperor.

The second campaign reveals that Kerrigan was not killed by the Zerg, but rather is captured and infested in an effort to incorporate her psionic traits into the Zerg gene pool. She emerges with far more psionic powers and physical strength, her DNA completely altered. Meanwhile, the Protoss commander Tassadar discovers that the Zerg's cerebrates cannot be killed by conventional means, but that they can be harmed by the powers wielded by the heretical dark templar. Tassadar allies himself with the dark templar prelate Zeratul, who assassinates Zasz, one of the Zerg's cerebrates in their hive clusters on Char. The cerebrate's death results in its forces running amok through the Zerg hives, but briefly links the minds of Zeratul and the Zerg Overmind, allowing the Overmind to finally learn the location of the Protoss homeworld Aiur, which the Overmind has been seeking for millennia. The main Zerg swarm promptly invades Aiur, while Kerrigan is dispatched to deal with Tassadar, and despite facing heavy Protoss resistance, the Overmind is able to embed itself into the crust of the planet.

The final episode of the game sees Aldaris and the Protoss government branding Tassadar a traitor and a heretic for conspiring with the dark templar. The player (later hinted to be Artanis) initially serves Aldaris in defending Aiur from the Zerg invasion, but while on a mission to arrest Tassadar, the player joins him instead. A Protoss civil war erupts, pitting Tassadar, Zeratul, and their allies against the Protoss establishment. The dark templar prove their worth when they use their energies to slay two more of the Zerg cerebrates on Aiur, and the Conclave reconciles with them. Aided by Raynor's forces—who sided with Tassadar back on Char—the Protoss break through the Overmind's weakened defenses and destroy the Overmind's outer shell, but take heavy casualties in the process. Tassadar channels his own psionic energies in combination with those of the dark templar through the hull of his command ship and crashes it into the Overmind, sacrificing himself in order to destroy it.

Blizzard Entertainment began development on "StarCraft" in 1995, shortly after the release of highly successful "". Using the "Tides of Darkness" game engine as a base, "StarCraft" made its debut at E3 1996. The version of the game displayed, assembled by the team's lead programmer Bob Fitch, received a rather weak response from the convention and was criticized by many for being ""Warcraft" in space." As a consequence the entire project was overhauled, bringing the focus onto creating three distinct species. Bill Roper, one of the game's producers, stated this would be a major departure from the "Warcraft" approach, comparing its two equal sides to those of chess and stating that "StarCraft" would allow players to "develop very unique strategies based on which species is being played, and will require players to think of different strategies to combat the other two species." The hand-drawn graphics seen in the E3 version were also replaced with rendered graphics. In early 1997, the new version of "StarCraft" was unveiled, receiving a far more positive response.
However, the game was still marred by technical difficulties, so Bob Fitch completely redesigned the "Warcraft II" engine within two months to ensure that many of the features desired by the designers, such as the abilities for units to burrow and cloak, could be implemented. Later improvements to the game included pre-rendered sprites and backgrounds, constructed using 3D Studio Max. An isometric in-game view was also adopted, in contrast to "Warcraft II"s 3/4s birdseye perspective. In addition, the game utilized high quality music, composed by Blizzard's resident composers, and professional voice actors were hired.

Despite the progress, "StarCraft" was slow to emerge. The continual delays inspired a group of "StarCraft" fans on the official forums who labeled themselves "Operation: Can't Wait Any Longer" to write a series of fictional stories in which the members of Operation CWAL attempted to retrieve the beta version of "StarCraft" from Blizzard's headquarters in Irvine, California. To pay homage to their presence on the forums and enthusiasm for the game, Blizzard Entertainment later incorporated the group's name into "StarCraft" as a cheat code to speed up the production of units and gave the group thanks in the game's credits. The game was released for Windows on March 31, 1998, with the Classic Mac OS version following a year later in 1999. Development on a Nintendo 64 version, "StarCraft 64", began in 1999, converted from PC by Mass Media Interactive Entertainment—a subsidiary of THQ—and published by Nintendo. "StarCraft 64" was released on June 13, 2000 in the US and Europe. It was also released in Australia on May 25, 2001.
The musical score to "StarCraft" was composed by Blizzard Entertainment's composers. Glenn Stafford composed the Terran and Protoss in-game themes, while Derek Duke, who was a contracted composer at the time, wrote all the in-game music for the Zerg. The cinematic scores were composed by Stafford and Hayes. Hayes also collaborated with Stafford on one of the Protoss in-game tracks. Tracy W. Bush provided additional support in composing. The musical score of the game was received well by reviewers, who have described it as "appropriately melodic and dark" and "impressive", with one reviewer noting that some of the music owed much of its inspiration to Jerry Goldsmith's score for the film "Alien". The first official game soundtrack, "StarCraft: Game Music Vol. 1", was released in 2000, comprising tracks from both "StarCraft" and "", as well as a sizable portion of remix tracks and music inspired by "StarCraft", created by several South Korean disc jockeys. The soundtrack was distributed by Net Vision Entertainment. In September 2008, Blizzard Entertainment announced that a second soundtrack, "StarCraft Original Soundtrack", had been released on iTunes. This soundtrack consisted entirely of the original music from "StarCraft" and "Brood War", both from in-game themes to music used in the cinematic cut scenes.

Before the release of "StarCraft", Blizzard Entertainment released a free-to-download game demo entitled "Loomings", comprising three missions and a tutorial. The prequel was made available for the full game in October 1999 as a custom map campaign, adding two extra missions and hosting it on Battle.net. In addition, the full release of "StarCraft" included a secondary campaign entitled "Enslavers". Consisting of five missions played as both the Terrans and the Protoss, "Enslavers" is set in the second campaign in "StarCraft" and follows the story of a Terran smuggler who manages to take control of a Zerg cerebrate and is pursued by both the Protoss and Terran Dominion. "Enslavers" acts as an exemplar single-player campaign for the game's level editor, highlighting how to use the features of the program.
"StarCraft"s first expansion, Insurrection, was released for Windows on July 31, 1998. The expansion was developed by Aztech New Media and authorized by Blizzard Entertainment. Its story focused on a separate Confederate colony alluded to in the manual to "StarCraft", following a group of Terran colonists and a Protoss fleet in their fight against the Zerg and a rising local insurgency. "Insurrection" was not received well, being criticized by reviewers for lacking the quality of the original game. "Insurrection" was followed within a few months by a second expansion, Retribution. Developed by Stardock, published by WizardWorks and authorized by Blizzard Entertainment, "Retribution" follows all three races attempting to seize control of a powerful crystal on a Terran Dominion colony. The expansion was not received with critical support, instead being regarded as average but at least challenging. After the release of "Retribution", Blizzard Entertainment announced a new official expansion pack that would continue on the story of "StarCraft". "" was consequently created, developed jointly by Blizzard Entertainment and Saffire. "Brood War" continues the story of "StarCraft" from days after its conclusion, and was released for both Windows and Mac to critical praise on December 18, 1998 in the US and in March 1999 in Europe.

Before Insurrection, an unauthorized expansion pack, called Stellar Forces, was published by Micro Star but was recalled weeks later when Blizzard won the court case against it. It consisted of 22 single player maps and 32 multi-player maps which are considered to be rather plain.

In 2000, "StarCraft 64" was released In North America for the Nintendo 64, co-developed by Blizzard Entertainment and Mass Media Inc. and published by Nintendo. The game featured all of the missions from both "StarCraft" and the expansion "Brood War", as well as some exclusive missions, such as two different tutorials and a new secret mission, "Resurrection IV". Blizzard Entertainment had previously considered a PlayStation port of the game, but it was decided that the game would instead be released on the Nintendo 64. "Resurrection IV" is set after the conclusion of "Brood War", and follows Jim Raynor embarking on a mission to rescue the "Brood War" character Alexei Stukov, a vice admiral from Earth who has been captured by the Zerg. The "Brood War" missions required the use of a Nintendo 64 memory Expansion Pak to run. In addition, "StarCraft 64" features a split screen cooperative mode, also requiring the expansion pak, allowing two players to control one force in-game. "StarCraft 64" lacked the online multiplayer capabilities and speech in mission briefings. In addition, cut scenes were shortened.

A remastered edition of the game, "", released August 14, 2017, preserves the gameplay of the original while adding support for ultra-high-definition graphics, Blizzard's modern online features, and re-recorded audio (soundtrack and sound effects).

On June 8, 2019, as part of the grand finals of the third season of the KSL, Blizzard announced a graphics overhaul pack for the game by Carbot Animations, the producers of multiple Blizzard-related parody animations, including their first and longest-running one, the "StarCrafts" series. As a graphical overhaul, its effect applies to all game modes and menus in "StarCraft: Remastered". It was released on July 10, 2019 as "StarCraft: Cartooned" alongside an announcer pack featuring South Korean YouTuber and children's television host Hyejin "Hey Jini" Kang.

"StarCraft" was released internationally on March 31, 1998 and became the best-selling PC game for that year, selling over 1.5 million copies worldwide. In the United States, it was the best-selling computer game of 1998, with 746,365 units sold. It was the country's 14th best-selling release of the period between 1993 and 1999, selling 916,000 copies. By April 1999, South Korean players had purchased almost 300,000 units of the game. "StarCraft"s worldwide sales reached 4 million units by July 2001; South Korea accounted for 50% of these copies. By May 2007, "StarCraft" had sold over 9.5 million copies across the globe, with 4.5 million of these being sold in South Korea. Since the initial release of "StarCraft", Blizzard Entertainment reported that its Battle.net online multiplayer service grew by 800 percent.

Generally, "StarCraft" was received positively by critics, with many contemporary reviewers noting that while the game may not have deviated significantly from the status quo of most real-time strategy games, it was one of the best to have applied the formula. In addition, "StarCraft"s pioneering use of three distinct, unique, and balanced races over two equal sides was praised by critics, with GameSpot commenting that this helped the game to "avoid the problem that has plagued every other game in the genre". Many critics also praised the strength of the story accompanying the game, with some reviewers being impressed by how well the story was folded into the gameplay. The game's voice acting in particular was praised; GameSpot later hailed the voice work in the game as one of the ten best in the industry at the time. Equally, the multiplayer aspects of the game were positively received. "StarCraft" has received multiple awards, including being named as one of the best games of all time by GameSpot, IGN, and "Game Informer". According to Blizzard Entertainment, "StarCraft" has won 37 awards and has received a star on the floor of the Metreon as part of the Walk of Game in San Francisco in early 2006.

"Next Generation" reviewed the PC version of the game, rating it five stars out of five, and stated that "The quality of the play balancing and the elegance of the design mean that "StarCraft" sets a new high watermark for all real-time strategy games."

The reviewer from the online second volume of "Pyramid" stated that "One of the most hotly anticipated computer games of the last two years, Blizzard Entertainment's "Starcraft" has had a tremendous amount of hype to live up to. The fact that it "does" live up to the high expectations set for it may be the only recommendation it needs."

Although at the time "StarCraft"s graphics and audio were praised by critics, later reviews have noted that the graphics do not compare to more modern games. The capacity for the game's artificial intelligence to navigate units to waypoints also faced some heavy criticism, with "PC Zone" stating that the inability for developers to make an effective pathfinding system was "the single most infuriating element of the real-time strategy genre". In addition, several reviewers expressed concern over some familiarities between the unit structures of each race, as well as over the potential imbalance of players using rushing tactics early in multiplayer games. Blizzard Entertainment has strived to balance rush tactics in later updates. The Nintendo 64 version of the game was not received as positively by reviewers, and was criticized for poor graphics in comparison to the PC version. However, critics did praise the game and Mass Media for using effective controls on the gamepad and maintaining the high quality audio.

"Starcraft" won the Origins Award for Best Strategy Computer Game of 1998.

In 1998, "PC Gamer" declared it the 5th-best computer game ever released, and the editors called it "a strategy game that continues to evolve and surprise many months after its release, and that currently represents the state of the genre's art".

With more than 11 million copies sold worldwide by February 2009, "StarCraft" became one of the best-selling games for the personal computer. It has been praised for pioneering the use of unique "factions" in RTS gameplay, and for having a compelling story.

GameSpot described "StarCraft" as "The defining game of its genre. It is the standard by which all real-time strategy games are judged." IGN stated that "StarCraft" "is hands down one of the best, if not the best, real-time strategy games ever created." "StarCraft" is frequently included in the industry's best games rankings, for example it ranked 37 in "Edge"s top 100 games of all time. "StarCraft" has even been taken into space, as Daniel Barry took a copy of the game with him on the Space Shuttle mission STS-96 in 1999. "StarCraft"s popularity resulted in "Guinness World Records" awarding the game four world records, including "Best Selling PC Strategy Game," "Largest Income in Professional Gaming," and "Largest Audience for a Game Competition" when 120,000 fans turned out to watch the final of the SKY proleague season 2005 in Busan, South Korea. Researchers have shown that the audience for watching "StarCraft" games is diverse and that "StarCraft" uses instances of information asymmetry to make the game more entertaining for spectators. In addition, "StarCraft" has been the subject of an academic course; the University of California, Berkeley offered a student-run introductory course on theory and strategy in spring 2009.

After its release, "StarCraft" rapidly grew in popularity in South Korea, eventually making its way to become the country's national e-sport after establishing a successful pro-gaming scene. Professional gamers in South Korea are media celebrities, and "StarCraft" games are broadcast over three television channels dedicated to the professional gaming scene. Professional gamers in South Korea have gained television contracts, sponsorships, and tournament prizes, allowing one of the most famous players, Lim "BoxeR" Yo-hwan, to gain a fan club of over half a million people. One player, Lee Yun-yeol, reported earnings in 2005 of .

"StarCraft" was part of the United States Air Force's Air and Space Basic Course, used to teach newly active officers about crisis planning under stress and joint service teamwork. Other efforts to make more 'realistic' current-day battle software led to distractions when simulated hardware didn't align with the real hardware active duty officers knew about. The science fiction setting allowed students to focus on the battle tactics.

The annual Conference on Artificial Intelligence and Interactive Digital Entertainment hosts a competition for AIs playing the game. As of 2015, humans still win.

In 2014, an unofficial version for the Pandora handheld and the ARM architecture became available by static recompilation and reverse engineering of the original x86 version.

The original game, along with the expansion, was released for free in April 2017.

The storyline of "StarCraft" has been adapted into several novels. The first novel, "Uprising", which was written by Blizzard employee Micky Neilson and published in December 2000, acts as a prequel to the events of "StarCraft". Other novels—"Liberty's Crusade" by Jeff Grubb and Aaron Rosenberg's "Queen of Blades"—retell the story of the game from different perspectives. At BlizzCon 2007, "StarCraft" creator Chris Metzen stated that he hoped to novelize the entirety of "StarCraft" and its expansion "Brood War" into a definitive text-based story. Later novels, such as Gabriel Mesta's "Shadow of the Xel'Naga" and Christie Golden's "The Dark Templar Saga", further expand the storyline, creating the setting for "".

A number of action figures and collectable statues based upon the characters and units in "StarCraft" have been produced by ToyCom. A number of model kits, made by Academy Hobby Model Kits, were also produced, displaying 1/30 scale versions of the marine and the hydralisk. In addition, Blizzard Entertainment teamed up with Fantasy Flight Games to create a board game with detailed sculptures of game characters. Blizzard Entertainment also licensed Wizards of the Coast to produce an Alternity based game entitled "StarCraft Adventures".




</doc>
<doc id="26810" url="https://en.wikipedia.org/wiki?curid=26810" title="Skepticism">
Skepticism

Skepticism (American English and Canadian English) or scepticism (British English and Australian English) is generally a questioning attitude or doubt towards one or more putative instances of knowledge which are asserted to be mere belief or dogma. Formally, skepticism is a topic of interest in philosophy, particularly epistemology. More informally, skepticism as an expression of questioning or doubt can be applied to any topic, such as politics, religion, or pseudoscience. It is often applied within restricted domains, such as morality (moral skepticism), theism (skepticism about the existence of God), or the supernatural.

Philosophical skepticism comes in various forms. Radical forms of philosophical skepticism deny that knowledge or rational belief is possible and urge us to suspend judgment on many or all controversial matters. More moderate forms of philosophical skepticism claim only that nothing can be known with certainty, or that we can know little or nothing about nonempirical matters, such as whether God exists, whether human beings have free will, or whether there is an afterlife. 

Skepticism has also inspired a number of contemporary social movements. Religious skepticism advocates for doubt concerning basic religious principles, such as immortality, providence, and revelation. Scientific skepticism advocates for testing beliefs for reliability, by subjecting them to systematic investigation using the scientific method, to discover empirical evidence for them.

In ordinary usage, skepticism (US) or scepticism (UK) (Greek: 'σκέπτομαι' "skeptomai", to search, to think about or look for; see also spelling differences) can refer to:


In philosophy, skepticism can refer to:

As a philosophical school or movement, skepticism arose both in ancient Greece and India. In India the Ajñana school of philosophy espoused skepticism. It was a major early rival of Buddhism and Jainism, and a major influence on Buddhism. Two of the foremost disciples of the Buddha, Sariputta and Moggallāna, were initially the students of the Ajñana philosopher Sanjaya Belatthiputta, and a strong element of skepticism is found in Early Buddhism, most particularly in the Aṭṭhakavagga sutra. Since skepticism is a philosophical attitude and a style of philosophising rather than a position, the Ajñanins may have influenced other skeptical thinkers of India such as Nagarjuna, Jayarāśi Bhaṭṭa, and Shriharsha.

In Greece philosophers as early as Xenophanes (c. 570 – c. 475 BC) expressed skeptical views, as did Democritus 
and a number of Sophists. Gorgias, for example, reputedly argued that nothing exists, that even if there were something we could not know it, and that even if we could know it we could not communicate it. The Heraclitean philosopher Cratylus refused to discuss anything and would merely wriggle his finger, claiming that communication is impossible since meanings are constantly changing. Socrates also had skeptical tendencies, claiming to know nothing worthwhile.

There were two major schools of skepticism in the ancient Greek and Roman world. The first was Pyrrhonism, was founded by Pyrrho of Elis (c. 360–270 BCE). The second was Academic Skepticism, so-called because its two leading defenders, Arcesilaus (c. 315–240 BCE) who initiated the philosophy, and Carneades (c. 217–128 BCE), the philosophy's most famous proponent, were heads of Plato's Academy. Pyrrhonism's aims are psychological. It urges suspension of judgment ("epoche") to achieve mental tranquility ("ataraxia"). The Academic Skeptics denied that knowledge is possible. The Academic Skeptics claimed that some beliefs are more reasonable or probable than others, whereas Pyrrhonian skeptics argue that equally compelling arguments can be given for or against any disputed view. Nearly all the writings of the ancient skeptics are now lost. Most of what we know about ancient skepticism is from Sextus Empiricus, a Pyrrhonian skeptic who lived in the second or third century CE. His works contain a lucid summary of stock skeptical arguments.

Ancient skepticism faded out during the late Roman Empire, particularly after Augustine (354–430 CE) attacked the skeptics in his work "Against the Academics" (386 CE). There was little knowledge of, or interest in, ancient skepticism in Christian Europe during the Middle Ages. Interest revived during the Renaissance and Reformation, particularly after the complete writings of Sextus Empiricus were translated into Latin in 1569. A number of Catholic writers, including Francisco Sanches (c. 1550–1623), Michel de Montaigne (1533–1592), Pierre Gassendi (1592–1655), and Marin Mersenne (1588–1648) deployed ancient skeptical arguments to defend moderate forms of skepticism and to argue that faith, rather than reason, must be the primary guide to truth. Similar arguments were offered later (perhaps ironically) by the Protestant thinker Pierre Bayle in his influential Historical and Critical Dictionary (1697–1702).

The growing popularity of skeptical views created an intellectual crisis in seventeenth-century Europe. One major response was offered by the French philosopher and mathematician René Descartes (1596–1650). In his classic work, "Meditations of First Philosophy" (1641), Descartes sought to refute skepticism, but only after he had formulated the case for skepticism as powerfully as possible. Descartes argued that no matter what radical skeptical possibilities we imagine there are certain truths (e.g., that thinking is occurring, or that I exist) that are absolutely certain. Thus, the ancient skeptics were wrong to claim that knowledge is impossible. Descartes also attempted to refute skeptical doubts about the reliability of our senses, our memory, and other cognitive faculties. To do this, Descartes tried to prove that God exists and that God would not allow us to be systematically deceived about the nature of reality. Many contemporary philosophers question whether this second stage of Descartes' critique of skepticism is successful.

In the eighteenth century a powerful new case for skepticism was offered by the Scottish philosopher David Hume (1711–1776). Hume was an empiricist, claiming that all genuine ideas can be traced back to original impressions of sensation or introspective consciousness. Hume argued forcefully that on empiricist grounds there are no sound reasons for belief in God, an enduring self or soul, an external world, causal necessity, objective morality, or inductive reasoning. In fact, he argued that "Philosophy would render us entirely Pyrrhonian, were not Nature too strong for it." As Hume saw it, the real basis of human belief is not reason, but custom or habit. We are hard-wired by nature to trust, say, our memories or inductive reasoning, and no skeptical arguments, however powerful, can dislodge those beliefs. In this way, Hume embraced what he called a "mitigated" skepticism, while rejecting an "excessive" Pyrrhonian skepticism that he saw as both impractical and psychologically impossible.

Hume's skepticism provoked a number of important responses. Hume's Scottish contemporary, Thomas Reid (1710–1796), challenged Hume's strict empiricism and argued that it is rational to accept "common-sense" beliefs such as the basic reliability of our senses, our reason, our memories, and inductive reasoning, even though none of these things can be proved. In Reid's view, such common-sense beliefs are foundational and require no proof in order to be rationally justified. Not long after Hume's death, the great German philosopher Immanuel Kant (1724–1804) argued that human moral awareness makes no sense unless we reject Hume's skeptical conclusions about the existence of God, the soul, free will, and an afterlife. According to Kant, while Hume was right to claim that we cannot strictly "know" any of these things, our moral experience entitles us to believe in them.

Today, skepticism continues to be a topic of lively debate among philosophers.

Religious skepticism generally refers to doubting given religious beliefs or claims. Historically, religious skepticism can be traced back to Xenophanes, who doubted many religious claims of his time. Modern religious skepticism typically emphasizes scientific and historical methods or evidence, with Michael Shermer writing that skepticism is a process for discovering the truth rather than general non-acceptance. For example, a religious skeptic might believe that Jesus existed while questioning claims that he was the messiah or performed miracles (see historicity of Jesus). Religious skepticism is not the same as atheism or agnosticism, though these often do involve skeptical attitudes toward religion and philosophical theology (for example, towards divine omnipotence). Religious people are generally skeptical about claims of other religions, at least when the two denominations conflict concerning some stated belief. Additionally, they may also be skeptical of the claims made by atheists. The historian Will Durant writes that Plato was "as skeptical of atheism as of any other dogma".

A scientific or empirical skeptic is one who questions beliefs on the basis of scientific understanding and empirical evidence.

Scientific skepticism may discard beliefs pertaining to "purported phenomena" not subject to reliable observation and thus not systematic or testable empirically. Most scientists, being scientific skeptics, test the reliability of certain kinds of claims by subjecting them to a systematic investigation using some type of the scientific method. As a result, a number of claims are considered as "pseudoscience", if they are found to improperly apply or ignore the fundamental aspects of the scientific method.

Professional skepticism is an important concept in auditing. It requires an auditor to have a "questioning mind", to make a critical assessment of evidence, and to consider the sufficiency of the evidence.





</doc>
<doc id="26818" url="https://en.wikipedia.org/wiki?curid=26818" title="Stagflation">
Stagflation

In economics, stagflation or recession-inflation is a situation in which the inflation rate is high, the economic growth rate slows, and unemployment remains steadily high. It presents a dilemma for economic policy, since actions intended to lower inflation may exacerbate unemployment.

The term, a portmanteau of "stagnation" and "inflation", is generally attributed to Iain Macleod, a British Conservative Party politician who became Chancellor of the Exchequer in 1970. Macleod used the word in a 1965 speech to Parliament during a period of simultaneously high inflation and unemployment in the United Kingdom. Warning the House of Commons of the gravity of the situation, he said: 

Macleod used the term again on 7 July 1970, and the media began also to use it, for example in "The Economist" on 15 August 1970, and "Newsweek" on 19 March 1973.

John Maynard Keynes did not use the term, but some of his work refers to the conditions that most would recognise as stagflation. In the version of Keynesian macroeconomic theory that was dominant between the end of World War II and the late 1970s, inflation and recession were regarded as mutually exclusive, the relationship between the two being described by the Phillips curve. Stagflation is very costly and difficult to eradicate once it starts, both in social terms and in budget deficits.

The term "stagflation", a portmanteau of "stagnation" and "inflation", was first coined during a period of inflation and unemployment in the United Kingdom. The United Kingdom experienced an outbreak of inflation in the 1960s and 1970s. Inflation rose in the 1960s and 1970s, UK policy makers failed to recognize the primary role of monetary policy in controlling inflation. Instead, they attempted to use non-monetary policies and devices to respond to the economic crisis. Policy makers also made "inaccurate estimates of the degree of excess demand in the economy, [which] contributed significantly to the outbreak of inflation in the United Kingdom in the 1960s and 1970s.

Stagflation was not limited to the United Kingdom, however. Economists have shown that stagflation was prevalent among seven major market economies from 1973 to 1982. After inflation rates began to fall in 1982, economists' focus shifted from the causes of stagflation to the "determinants of productivity growth and the effects of real wages on the demand for labor".

Economists offer two principal explanations for why stagflation occurs. First, stagflation can result when the economy faces a supply shock, such as a rapid increase in the price of oil. An unfavorable situation like that tends to raise prices at the same time as it slows economic growth by making production more costly and less profitable.

Second, the government can cause stagflation if it creates policies that harm industry while growing the money supply too quickly. These two things would probably have to occur simultaneously because policies that slow economic growth do not usually cause inflation, and policies that cause inflation do not usually slow economic growth.

Both explanations are offered in analyses of the 1970s stagflation in the West. It began with a huge rise in oil prices, but then continued as central banks used excessively stimulative monetary policy to counteract the resulting recession, causing a price/wage spiral.

Up to the 1960s, many Keynesian economists ignored the possibility of stagflation, because historical experience suggested that high unemployment was typically associated with low inflation, and vice versa (this relationship is called the Phillips curve). The idea was that high demand for goods drives up prices, and also encourages firms to hire more; and likewise high employment raises demand. However, in the 1970s and 1980s, when stagflation occurred, it became obvious that the relationship between inflation and employment levels was not necessarily stable: that is, the Phillips relationship could shift. Macroeconomists became more skeptical of Keynesian theories, and Keynesians themselves reconsidered their ideas in search of an explanation for stagflation.

The explanation for the shift of the Phillips curve was initially provided by the monetarist economist Milton Friedman, and also by Edmund Phelps. Both argued that when workers and firms begin to expect more inflation, the Phillips curve shifts up (meaning that more inflation occurs at any given level of unemployment). In particular, they suggested that if inflation lasted for several years, workers and firms would start to take it into account during wage negotiations, causing workers' wages and firms' costs to rise more quickly, thus further increasing inflation. While this idea was a severe criticism of early Keynesian theories, it was gradually accepted by most Keynesians, and has been incorporated into New Keynesian economic models.

Neo-Keynesian theory distinguished two distinct kinds of inflation: demand-pull (caused by shifts of the aggregate demand curve) and cost-push (caused by shifts of the aggregate supply curve). Stagflation, in this view, is caused by cost-push inflation. Cost-push inflation occurs when some force or condition increases the costs of production. This could be caused by government policies (such as taxes) or from purely external factors such as a shortage of natural resources or an act of war.

Contemporary Keynesian analyses argue that stagflation can be understood by distinguishing factors that affect aggregate demand from those that affect aggregate supply. While monetary and fiscal policy can be used to stabilise the economy in the face of aggregate demand fluctuations, they are not very useful in confronting aggregate supply fluctuations. In particular, an adverse shock to aggregate supply, such as an increase in oil prices, can give rise to stagflation.

Supply theories are based on the neo-Keynesian cost-push model and attribute stagflation to significant disruptions to the supply side of the supply-demand market equation, such as when there is a sudden real or relative scarcity of key commodities, natural resources, or natural capital needed to produce goods and services. Other factors may also cause supply problems, for example, social and political conditions such as policy changes, acts of war, extremely restrictive government control of production. In this view, stagflation is thought to occur when there is an adverse supply shock (for example, a sudden increase in the price of oil or a new tax) that causes a subsequent jump in the "cost" of goods and services (often at the wholesale level). In technical terms, this results in contraction or negative shift in an economy's aggregate supply curve.

In the resource scarcity scenario (Zinam 1982), stagflation results when economic growth is inhibited by a restricted supply of raw materials. That is, when the actual or relative supply of basic materials (fossil fuels (energy), minerals, agricultural land in production, timber, etc.) decreases and/or cannot be increased fast enough in response to rising or continuing demand. The resource shortage may be a real physical shortage, or a relative scarcity due to factors such as taxes or bad monetary policy influencing the "cost" or availability of raw materials. This is consistent with the cost-push inflation factors in neo-Keynesian theory (above). The way this plays out is that after supply shock occurs, the economy first tries to maintain momentum. That is, consumers and businesses begin paying higher prices to maintain their level of demand. The central bank may exacerbate this by increasing the money supply, by lowering interest rates for example, in an effort to combat a recession. The increased money supply props up the demand for goods and services, though demand would normally drop during a recession.

In the Keynesian model, higher prices prompt increases in the supply of goods and services. However, during a supply shock (i.e., scarcity, "bottleneck" in resources, etc.), supplies do not respond as they normally would to these price pressures. So, inflation jumps and output drops, producing stagflation.

Following Richard Nixon's imposition of wage and price controls on 15 August 1971, an initial wave of cost-push shocks in commodities were blamed for causing spiraling prices. The second major shock was the 1973 oil crisis, when the Organization of Petroleum Exporting Countries (OPEC) constrained the worldwide supply of oil. Both events, combined with the overall energy shortage that characterized the 1970s, resulted in actual or relative scarcity of raw materials. The price controls resulted in shortages at the point of purchase, causing, for example, queues of consumers at fuelling stations and increased production costs for industry.

Through the mid-1970s, it was alleged that none of the major macroeconomic models (Keynesian, New Classical, and monetarist) were able to explain stagflation.

Later, an explanation was provided based on the effects of adverse supply shocks on both inflation and output. According to Blanchard (2009), these adverse events were one of two components of stagflation; the other was "ideas"—which Robert Lucas, Thomas Sargent, and Robert Barro were cited as expressing as "wildly incorrect" and "fundamentally flawed" predictions (of Keynesian economics) which, they said, left stagflation to be explained by "contemporary students of the business cycle". In this discussion, Blanchard hypothesizes that the recent oil price increases could trigger another period of stagflation, although this has not yet happened (pg. 152).

A purely neoclassical view of the macroeconomy rejects the idea that monetary policy can have real effects. Neoclassical macroeconomists argue that real economic quantities, like real output, employment, and unemployment, are determined by real factors only. Nominal factors like changes in the money supply only affect nominal variables like inflation. The neoclassical idea that nominal factors cannot have real effects is often called "monetary neutrality" or also the "classical dichotomy".

Since the neoclassical viewpoint says that real phenomena like unemployment are essentially unrelated to nominal phenomena like inflation, a neoclassical economist would offer two separate explanations for 'stagnation' and 'inflation'. Neoclassical explanations of stagnation (low growth and high unemployment) include inefficient government regulations or high benefits for the unemployed that give people less incentive to look for jobs. Another neoclassical explanation of stagnation is given by real business cycle theory, in which any decrease in labour productivity makes it efficient to work less. The main neoclassical explanation of inflation is very simple: it happens when the monetary authorities increase the money supply too much.

In the neoclassical viewpoint, the real factors that determine output and unemployment affect the aggregate supply curve only. The nominal factors that determine inflation affect the aggregate demand curve only. When some adverse changes in real factors are shifting the aggregate supply curve left at the same time that unwise monetary policies are shifting the aggregate demand curve right, the result is stagflation.

Thus the main explanation for stagflation under a classical view of the economy is simply policy errors that affect both inflation and the labour market. Ironically, a very clear argument in favour of the classical explanation of stagflation was provided by Keynes himself. In 1919, John Maynard Keynes described the inflation and economic stagnation gripping Europe in his book "The Economic Consequences of the Peace". Keynes wrote:

Lenin is said to have declared that the best way to destroy the Capitalist System was to debauch the currency. By a continuing process of inflation, governments can confiscate, secretly and unobserved, an important part of the wealth of their citizens. By this method they not only confiscate, but they confiscate arbitrarily; and, while the process impoverishes many, it actually enriches some. [...] Lenin was certainly right. There is no subtler, no surer means of overturning the existing basis of society than to debauch the currency. The process engages all the hidden forces of economic law on the side of destruction, and does it in a manner which not one man in a million is able to diagnose.

Keynes explicitly pointed out the relationship between governments printing money and inflation.

The inflationism of the currency systems of Europe has proceeded to extraordinary lengths. The various belligerent Governments, unable, or too timid or too short-sighted to secure from loans or taxes the resources they required, have printed notes for the balance.

Keynes also pointed out how government price controls discourage production.

The presumption of a spurious value for the currency, by the force of law expressed in the regulation of prices, contains in itself, however, the seeds of final economic decay, and soon dries up the sources of ultimate supply. If a man is compelled to exchange the fruits of his labours for paper which, as experience soon teaches him, he cannot use to purchase what he requires at a price comparable to that which he has received for his own products, he will keep his produce for himself, dispose of it to his friends and neighbours as a favour, or relax his efforts in producing it. A system of compelling the exchange of commodities at what is not their real relative value not only relaxes production, but leads finally to the waste and inefficiency of barter.

Keynes detailed the relationship between German government deficits and inflation.

In Germany the total expenditure of the Empire, the Federal States, and the Communes in 1919–20 is estimated at 25 milliards of marks, of which not above 10 milliards are covered by previously existing taxation. This is without allowing anything for the payment of the indemnity. In Russia, Poland, Hungary, or Austria such a thing as a budget cannot be seriously considered to exist at all. Thus the menace of inflationism described above is not merely a product of the war, of which peace begins the cure. It is a continuing phenomenon of which the end is not yet in sight. 

While most economists believe that changes in money supply can have some real effects in the short run, neoclassical and neo-Keynesian economists tend to agree that there are no long-run effects from changing the money supply. Therefore, even economists who consider themselves neo-Keynesians usually believe that in the long run, money is neutral. In other words, while neoclassical and neo-Keynesian models are often seen as competing points of view, they can also be seen as two descriptions appropriate for different time horizons. Many mainstream textbooks today treat the neo-Keynesian model as a more appropriate description of the economy in the short run, when prices are 'sticky', and treat the neoclassical model as a more appropriate description of the economy in the long run, when prices have sufficient time to adjust fully.

Therefore, while mainstream economists today might often attribute short periods of stagflation (not more than a few years) to adverse changes in supply, they would not accept this as an explanation of very prolonged stagflation. More prolonged stagflation would be explained as the effect of inappropriate government policies: excessive regulation of product markets and labor markets leading to long-run stagnation, and excessive growth of the money supply leading to long-run inflation.

Political economists Jonathan Nitzan and Shimshon Bichler have proposed an explanation of stagflation as part of a theory they call differential accumulation, which says firms seek to beat the average profit and capitalisation rather than maximise. According to this theory, periods of mergers and acquisitions oscillate with periods of stagflation. When mergers and acquisitions are no longer politically feasible (governments clamp down with anti-monopoly rules), stagflation is used as an alternative to have higher relative profit than the competition. With increasing mergers and acquisitions, the power to implement stagflation increases.

Stagflation appears as a societal crisis, such as during the period of the oil crisis in the 70s and in 2007 to 2010. Inflation in stagflation, however, does not affect all firms equally. Dominant firms are able to increase their own prices at a faster rate than competitors. While in the aggregate no one appears to profit, differentially dominant firms improve their positions with higher relative profits and higher relative capitalisation. Stagflation is not due to any actual supply shock, but because of the societal crisis that hints at a supply crisis. It is mostly a 20th and 21st century phenomenon that has been mainly used by the "weapondollar-petrodollar coalition" creating or using Middle East crises for the benefit of pecuniary interests.

Demand-pull stagflation theory explores the idea that stagflation can result exclusively from monetary shocks without any concurrent supply shocks or negative shifts in economic output potential. Demand-pull theory describes a scenario where stagflation can occur following a period of monetary policy implementations that cause inflation. This theory was first proposed in 1999 by Eduardo Loyo of Harvard University's John F. Kennedy School of Government.

Supply-side economics emerged as a response to US stagflation in the 1970s. It largely attributed inflation to the ending of the Bretton Woods system in 1971 and the lack of a specific price reference in the subsequent monetary policies (Keynesian and Monetarism). Supply-side economists asserted that the contraction component of stagflation resulted from an inflation-induced rise in real tax rates (see bracket creep)

Adherents to the Austrian School maintain that creation of new money ex nihilo benefits the creators and early recipients of the new money relative to late recipients. Money creation is not wealth creation; it merely allows early money recipients to outbid late recipients for resources, goods, and services.
Since the actual producers of wealth are typically late recipients, increases in the money supply weakens wealth formation and undermines the rate of economic growth. Says Austrian economist Frank Shostak:

"The increase in the money supply rate of growth coupled with the slowdown in the rate of growth of goods produced is what the increase in the rate of price inflation is all about. (Note that a price is the amount of money paid for a unit of a good.) What we have here is a faster increase in price inflation and a decline in the rate of growth in the production of goods. But this is exactly what stagflation is all about, i.e., an increase in price inflation and a fall in real economic growth. Popular opinion is that stagflation is totally made up. It seems therefore that the phenomenon of stagflation is the normal outcome of loose monetary policy. This is in agreement with [Phelps and Friedman (PF)]. Contrary to PF, however, we maintain that stagflation is not caused by the fact that in the short run people are fooled by the central bank. Stagflation is the natural result of monetary pumping which weakens the pace of economic growth and at the same time raises the rate of increase of the prices of goods and services."

In 1984, journalist and activist Jane Jacobs proposed the failure of major macroeconomic theories to explain stagflation was due to their focus on the nation as the salient unit of economic analysis, rather than the city. She proposed that the key to avoiding stagflation was for a nation to focus on the development of "import-replacing cities" that would experience economic ups and downs at different times, providing overall national stability and avoiding widespread stagflation. According to Jacobs, import-replacing cities are those with developed economies that balance their own production with domestic imports—so they can respond with flexibility as economic supply and demand cycles change. While lauding her originality, clarity, and consistency, urban planning scholars have criticized Jacobs for not comparing her own ideas to those of major theorists (e.g., Adam Smith, Karl Marx) with the same depth and breadth they developed, as well as a lack of scholarly documentation. Despite these issues, Jacobs' work is notable for having widespread public readership and influence on decision-makers.

Stagflation undermined support for the Keynesian consensus.

Federal Reserve chairman Paul Volcker very sharply increased interest rates from 1979–1983 in what was called a "disinflationary scenario". After U.S. prime interest rates had soared into the double-digits, inflation did come down; these interest rates were the highest long-term prime interest rates that had ever existed in modern capital markets. Volcker is often credited with having stopped at least the inflationary side of stagflation, although the American economy also dipped into recession. Starting in approximately 1983, growth began a recovery. Both fiscal stimulus and money supply growth were policy at this time. A five- to six-year jump in unemployment during the Volcker disinflation suggests Volcker may have trusted unemployment to self-correct and return to its natural rate within a reasonable period.



</doc>
<doc id="26819" url="https://en.wikipedia.org/wiki?curid=26819" title="Soundness">
Soundness

In logic, more precisely in deductive reasoning, an argument is sound if it is both valid in form and its premises are true. Soundness also has a related meaning in mathematical logic, wherein logical systems are sound if and only if every formula that can be proved in the system is logically valid with respect to the semantics of the system

In deductive reasoning, a sound argument is an argument that is both valid, and all of whose premises are true (and as a consequence its conclusion is true as well). An argument is valid if, assuming its premises are true, the conclusion "must" be true. An example of a sound argument the following well-known syllogism:

Because of the logical necessity of the conclusion, this argument is valid; and because the argument is valid and its premises are true, the argument is sound. 

However, an argument can be valid without being sound. For example:

This argument is valid because, assuming the premises are true, the conclusion must be true. However, the first premise is false. (The second premise may be false as well if the notion of "animal" is taken to exclude humans, but whether or not that is the case here does not matter: one false premise suffices to make an argument not sound.) For an argument to be sound, the argument must be valid "and" its premises must be true.

In mathematical logic, a logical system has the soundness property if and only if every formula that can be proved in the system is logically valid with respect to the semantics of the system.
In most cases, this comes down to its rules having the property of "preserving truth". The converse of soundness is known as completeness. 

A logical system with syntactic entailment formula_1 and semantic entailment formula_2 is sound if for any sequence formula_3 of sentences in its language, if formula_4, then formula_5. In other words, a system is sound when all of its theorems are tautologies.

Soundness is among the most fundamental properties of mathematical logic. The soundness property provides the initial reason for counting a logical system as desirable. The completeness property means that every validity (truth) is provable. Together they imply that all and only validities are provable.

Most proofs of soundness are trivial. For example, in an axiomatic system, proof of soundness amounts to verifying the validity of the axioms and that the rules of inference preserve validity (or the weaker property, truth). If the system allows Hilbert-style deduction, it requires only verifying the validity of the axioms and one rule of inference, namely modus ponens. (and sometimes substitution)

Soundness properties come in two main varieties: weak and strong soundness, of which the former is a restricted form of the latter.

Soundness of a deductive system is the property that any sentence that is provable in that deductive system is also true on all interpretations or structures of the semantic theory for the language upon which that theory is based. In symbols, where "S" is the deductive system, "L" the language together with its semantic theory, and "P" a sentence of "L": if ⊢ "P", then also ⊨ "P".

Strong soundness of a deductive system is the property that any sentence "P" of the language upon which the deductive system is based that is derivable from a set Γ of sentences of that language is also a logical consequence of that set, in the sense that any model that makes all members of Γ true will also make "P" true. In symbols where Γ is a set of sentences of "L": if Γ ⊢ "P", then also Γ ⊨ "P". Notice that in the statement of strong soundness, when Γ is empty, we have the statement of weak soundness.

If "T" is a theory whose objects of discourse can be interpreted as natural numbers, we say "T" is "arithmetically sound" if all theorems of "T" are actually true about the standard mathematical integers. For further information, see ω-consistent theory.

The converse of the soundness property is the semantic completeness property. A deductive system with a semantic theory is strongly complete if every sentence "P" that is a semantic consequence of a set of sentences Γ can be derived in the deduction system from that set. In symbols: whenever , then also . Completeness of first-order logic was first explicitly established by Gödel, though some of the main results were contained in earlier work of Skolem.

Informally, a soundness theorem for a deductive system expresses that all provable sentences are true. Completeness states that all true sentences are provable.

Gödel's first incompleteness theorem shows that for languages sufficient for doing a certain amount of arithmetic, there can be no consistent and effective deductive system that is complete with respect to the intended interpretation of the symbolism of that language. Thus, not all sound deductive systems are complete in this special sense of completeness, in which the class of models (up to isomorphism) is restricted to the intended one. The original completeness proof applies to "all" classical models, not some special proper subclass of intended ones.





</doc>
<doc id="26820" url="https://en.wikipedia.org/wiki?curid=26820" title="Syllabary">
Syllabary

In the linguistic study of written languages, a syllabary is a set of written symbols that represent the syllables or (more frequently) moras which make up words. 

A symbol in a syllabary, called a syllabogram, typically represents an (optional) consonant sound (simple onset) followed by a vowel sound (nucleus)—that is, a CV or V syllable—but other phonographic mappings, such as CVC, CV- tone, and C (normally nasals at the end of syllables), are also found in syllabaries.

A writing system using a syllabary is "complete" when it covers all syllables in the corresponding spoken language without requiring complex orthographic / graphemic rules, like implicit codas ( ⇒ /CVC/) silent vowels ( ⇒ /CVC/) or echo vowels ( ⇒ /CVC/). This loosely corresponds to "shallow" orthographies in alphabetic writing systems.

"True" syllabograms are those that encompass all parts of a syllable, i.e. initial onset, medial nucleus and final coda, but since onset and coda are optional in at least some languages, there are "middle" (nucleus), "start" (onset-nucleus), "end" (nucleus-coda) and "full" (onset-nucleus-coda) true syllabograms. Most syllabaries only feature one or two kinds of syllabograms and form other syllables by graphemic rules.

Syllabograms, hence syllabaries, are "pure", "analytic" or "arbitrary" if they do not share graphic similarities that correspond to phonic similarities, e.g. the symbol for "ka" does not resemble in any predictable way the symbol for "ki", nor the symbol for "a".
Otherwise they are "synthetic", if they vary by onset, rime, nucleus "or" coda, or "systematic", if they vary by all of them.
Some scholars, e.g. Daniels, reserve the general term for analytic syllabaries and invent other terms (abugida, abjad) as necessary. Some systems provide katakana language conversion.

Languages that use syllabic writing include Japanese, Cherokee, Vai, the Yi languages of eastern Asia, the English-based creole language Ndyuka, Shaozhou Tuhua, and the ancient language Mycenaean Greek (Linear B). In addition, the undecoded Cretan Linear A is also believed by some to be a syllabic script, though this is not proven.

Chinese characters, the cuneiform script used for Sumerian, Akkadian and other languages, and the former Maya script are largely syllabic in nature, although based on logograms. They are therefore sometimes referred to as "logosyllabic".

The contemporary Japanese language uses two syllabaries together called kana (in addition to the non-syllabic systems kanji and romaji), namely hiragana and katakana, which were developed around 700. Because Japanese uses mainly CV (consonant + vowel) syllables, a syllabary is well suited to write the language. As in many syllabaries, vowel sequences and final consonants are written with separate glyphs, so that both "atta" and "kaita" are written with three kana: あった ("a-t-ta") and かいた ("ka-i-ta"). It is therefore sometimes called a "moraic" writing system.

Languages that use syllabaries today tend to have simple phonotactics, with a predominance of monomoraic (CV) syllables. For example, the modern Yi script is used to write languages that have no diphthongs or syllable codas; unusually among syllabaries, there is a separate glyph for every consonant-vowel-tone combination (CVT) in the language (apart from one tone which is indicated with a diacritic).

Few syllabaries have glyphs for syllables that are not monomoraic, and those that once did have simplified over time to eliminate that complexity. 
For example, the Vai syllabary originally had separate glyphs for syllables ending in a coda "(doŋ)," a long vowel "(soo)," or a diphthong "(bai)," though not enough glyphs to distinguish all CV combinations (some distinctions were ignored). The modern script has been expanded to cover all moras, but at the same time reduced to exclude all other syllables. Bimoraic syllables are now written with two letters, as in Japanese: diphthongs are written with the help of V or "h"V glyphs, and the nasal coda is written with the glyph for "ŋ", which can form a syllable of its own in Vai.

In Linear B, which was used to transcribe Mycenaean Greek, a language with complex syllables, complex consonant onsets were either written with two glyphs or simplified to one, while codas were generally ignored, e.g. "ko-no-so" for "Knōsos", "pe-ma" for "sperma."

The Cherokee syllabary generally uses dummy vowels for coda consonants, but also has a segmental grapheme for /s/, which can be used both as a coda and in an initial /sC/ consonant cluster.

The languages of India and Southeast Asia, as well as the Ethiopian Semitic languages, have a type of alphabet called an "abugida" or "alphasyllabary". In these scripts, unlike in pure syllabaries, syllables starting with the same consonant are generally expressed with graphemes based in a regular way on a common graphical elements. Usually each character representing a syllable consists of several elements which designate the individual sounds of that syllable.

In the 19th century these systems were called "syllabics", a term which has survived in the name of Canadian Aboriginal syllabics (also an abugida).

In a true syllabary there may be graphic similarity between characters that share a common consonant or vowel sound, but it is not systematic or at all regular. For example, the characters for 'ke', 'ka', and 'ko' in Japanese hiragana have no similarity to indicate their common "k" sound (these being: け, か and こ). Compare this with Devanagari, an abugida, where the characters for 'ke', 'ka' and 'ko' are के, का and को respectively, with क indicating their common "k" sound.

English, along with many other Indo-European languages like German and Russian, allows for complex syllable structures, making it cumbersome to write English words with a syllabary. A "pure" syllabary based on English would require a separate glyph for every possible syllable. Thus one would need separate symbols for "bag", "beg", "big", "bog", "bug", "bad", "bed", "bid", "bod", "bud", "bead", "bide", "bode", etc. Since English has well over 10,000 different possibilities for individual syllables, a syllabary would be poorly suited to represent the English language. However, such pure systems are rare. A workaround to this problem, common to several syllabaries around the world (including English loanwords in Japanese), is to write an echo vowel, as if the syllable coda were a second syllable: "ba-gu" for "bag", etc. Another common approach is to simply ignore the coda, so that "bag" would be written "ba". This obviously would not work well for English, but was done in Mycenaean Greek when the root word was two or three syllables long and the syllable coda was a weak consonant such as "n" or "s" (example: χρυσός "chrysos" written as "ku-ru-so").



</doc>
<doc id="26822" url="https://en.wikipedia.org/wiki?curid=26822" title="Steve Reich">
Steve Reich

Stephen Michael Reich ( ; born October 3, 1936) is an American composer known for his contribution to the development of minimal music in the mid to late 1960s.

Reich's work is marked by its use of repetitive figures, slow harmonic rhythm, and canons. His innovations include using tape loops to create phasing patterns, as on the early compositions "It's Gonna Rain" (1965) and "Come Out" (1966), and the use of simple, audible processes, as on "Pendulum Music" (1968) and "Four Organs" (1970). The 1978 recording "Music for 18 Musicians" would help entrench minimalism as a movement. Reich's work took on a darker character in the 1980s with the introduction of historical themes as well as themes from his Jewish heritage, notably "Different Trains" (1988).

Reich's style of composition has influenced many contemporary composers and groups, especially in the US. Writing in "The Guardian", music critic Andrew Clements suggested that Reich is one of "a handful of living composers who can legitimately claim to have altered the direction of musical history". 

Reich was born in New York City to the Broadway lyricist June Sillman and Leonard Reich. When he was one year old, his parents divorced, and Reich divided his time between New York and California. He is the half-brother of writer Jonathan Carroll. He was given piano lessons as a child and describes growing up with the "middle-class favorites", having no exposure to music written before 1750 or after 1900. At the age of 14 he began to study music in earnest, after hearing music from the Baroque period and earlier, as well as music of the 20th century. Reich studied drums with Roland Kohloff in order to play jazz. While attending Cornell University, he minored in music and graduated in 1957 with a B.A. in Philosophy. Reich's B.A. thesis was on Ludwig Wittgenstein; later he would set texts by that philosopher to music in "Proverb" (1995) and "You Are (variations)" (2006).

For a year following graduation, Reich studied composition privately with Hall Overton before he enrolled at Juilliard to work with William Bergsma and Vincent Persichetti (1958–1961). Subsequently, he attended Mills College in Oakland, California, where he studied with Luciano Berio and Darius Milhaud (1961–1963) and earned a master's degree in composition. At Mills, Reich composed "Melodica" for melodica and tape, which appeared in 1986 on the three-LP release "Music from Mills".

Reich worked with the San Francisco Tape Music Center along with Pauline Oliveros, Ramon Sender, Morton Subotnick, Phil Lesh and Terry Riley. He was involved with the premiere of Riley's "In C" and suggested the use of the eighth note pulse, which is now standard in performance of the piece.

Reich's early forays into composition involved experimentation with twelve-tone composition, but he found the rhythmic aspects of the number twelve more interesting than the pitch aspects. Reich also composed film soundtracks for "Plastic Haircut" (1963), "Oh Dem Watermelons" (1965), and "Thick Pucker" (1965), three films by Robert Nelson. The soundtrack of "Plastic Haircut", composed in 1963, was a short tape collage, possibly Reich's first. The "Watermelons" soundtrack used two 19th-century minstrel tunes as its basis, and used repeated phrasing together in a large five-part canon. The music for "Thick Pucker" arose from street recordings Reich made walking around San Francisco with Nelson, who filmed in black and white 16mm. This film no longer survives. A fourth film from 1965, about 25 minutes long and tentatively entitled "Thick Pucker II", was assembled by Nelson from outtakes of that shoot and more of the raw audio Reich had recorded. Nelson was not happy with the resulting film and never showed it.

Reich was influenced by fellow minimalist Terry Riley, whose work "In C" combines simple musical patterns, offset in time, to create a slowly shifting, cohesive whole. Reich adopted this approach to compose his first major work, "It's Gonna Rain". Composed in 1965, the piece used a fragment of a sermon about the end of the world given by a black Pentecostal street-preacher known as Brother Walter. Reich built on his early tape work, transferring the last three words of the fragment, "it's gonna rain!", to multiple tape loops which gradually move out of phase with one another.

The 13-minute "Come Out" (1966) uses similarly manipulated recordings of a single spoken line given by Daniel Hamm, one of the falsely accused Harlem Six, who was severely injured by police. The survivor, who had been beaten, punctured a bruise on his own body to convince police about his beating. The spoken line includes the phrase "to let the bruise’s blood come out to show them." Reich rerecorded the fragment "come out to show them" on two channels, which are initially played in unison. They quickly slip out of sync; gradually the discrepancy widens and becomes a reverberation. The two voices then split into four, looped continuously, then eight, and continues splitting until the actual words are unintelligible, leaving the listener with only the speech's rhythmic and tonal patterns. 

"Melodica" (1966) takes the phase looping idea of his previous works and applies it to instrumental music. Steve Reich took a simple melody, which he played on a melodica, then recorded it. He then sets the melody to two separate channels, and slowly moves them out of phase, creating an intricate interlocking melody. This piece is very similar to "Come Out" in rhythmic structure, and are an example of how one rhythmic process can be realized in different sounds to create two different pieces of music. Reich was inspired to compose this piece from a dream he had on May 22, 1966, and put the piece together in one day. "Melodica" was the last piece Reich composed solely for tape, and he considers it his transition from tape music to instrumental music.

Reich's first attempt at translating this phasing technique from recorded tape to live performance was the 1967 "Piano Phase", for two pianos. In "Piano Phase" the performers repeat a rapid twelve-note melodic figure, initially in unison. As one player keeps tempo with robotic precision, the other speeds up very slightly until the two parts line up again, but one sixteenth note apart. The second player then resumes the previous tempo. This cycle of speeding up and then locking in continues throughout the piece; the cycle comes full circle three times, the second and third cycles using shorter versions of the initial figure. "Violin Phase", also written in 1967, is built on these same lines. "Piano Phase" and "Violin Phase" both premiered in a series of concerts given in New York art galleries.

A similar, lesser known example of this so-called process music is "Pendulum Music" (1968), which consists of the sound of several microphones swinging over the loudspeakers to which they are attached, producing feedback as they do so. "Pendulum Music" has never been recorded by Reich himself, but was introduced to rock audiences by Sonic Youth in the late 1990s.

Reich also tried to create the phasing effect in a piece "that would need no instrument beyond the human body". He found that the idea of phasing was inappropriate for the simple ways he was experimenting to make sound. Instead, he composed "Clapping Music" (1972), in which the players do not phase in and out with each other, but instead one performer keeps one line of a 12-eighth-note-long (12-quaver-long) phrase and the other performer shifts by one eighth note beat every 12 bars, until both performers are back in unison 144 bars later.

The 1967 prototype piece "Slow Motion Sound" was not performed although Chris Hughes performed it 27 years later as "Slow Motion Blackbird" on his Reich-influenced 1994 album "Shift". It introduced the idea of slowing down a recorded sound until many times its original length without changing pitch or timbre, which Reich applied to "Four Organs" (1970), which deals specifically with augmentation. The piece has maracas playing a fast eighth note pulse, while the four organs stress certain eighth notes using an 11th chord. This work therefore dealt with repetition and subtle rhythmic change. In contrast to Reich's typical cyclical structure, "Four Organs" is unique among his work in using a linear structure—the superficially similar "Phase Patterns", also for four organs but without maracas, is (as the name suggests) a cyclical phase piece similar to others composed during the period. "Four Organs" was performed as part of a Boston Symphony Orchestra program, and was Reich's first composition to be performed in a large traditional setting.

In 1970, Reich embarked on a five-week trip to study music in Ghana, during which he learned from the master drummer Gideon Alorwoyie. Reich also studied Balinese gamelan in Seattle in 1973 and 1974. From his African experience, as well as A. M. Jones's "Studies in African Music" about the music of the Ewe people, Reich drew inspiration for his 90-minute piece "Drumming", which he composed shortly after his return. Composed for a nine-piece percussion ensemble with female voices and piccolo, "Drumming" marked the beginning of a new stage in his career, for around this time he formed his ensemble, Steve Reich and Musicians, and increasingly concentrated on composition and performance with them. Steve Reich and Musicians, which was to be the sole ensemble to interpret his works for many years, still remains active with many of its original members.

After "Drumming", Reich moved on from the "phase shifting" technique that he had pioneered, and began writing more elaborate pieces. He investigated other musical processes such as augmentation (the temporal lengthening of phrases and melodic fragments). It was during this period that he wrote works such as "Music for Mallet Instruments, Voices and Organ" (1973) and "Six Pianos" (1973).

In 1974, Reich began writing "Music for 18 Musicians". This piece involved many new ideas, although it also hearkened back to earlier pieces. It is based on a cycle of eleven chords introduced at the beginning (called "Pulses"), followed by a small section of music based on each chord ("Sections I-XI"), and finally a return to the original cycle ("Pulses"). This was Reich's first attempt at writing for larger ensembles. The increased number of performers resulted in more scope for psychoacoustic effects, which fascinated Reich, and he noted that he would like to "explore this idea further". Reich remarked that this one work contained more harmonic movement in the first five minutes than any other work he had written. Steve Reich and Musicians made the premier recording of this work on ECM Records.

Reich explored these ideas further in his frequently recorded pieces "Music for a Large Ensemble" (1978) and "Octet" (1979). In these two works, Reich experimented with "the human breath as the measure of musical duration ... the chords played by the trumpets are written to take one comfortable breath to perform". Human voices are part of the musical palette in "Music for a Large Ensemble" but the wordless vocal parts simply form part of the texture (as they do in "Drumming"). With "Octet" and his first orchestral piece "Variations for Winds, Strings and Keyboards" (also 1979), Reich's music showed the influence of Biblical cantillation, which he had studied in Israel since the summer of 1977. After this, the human voice singing a text would play an increasingly important role in Reich's music.

In 1974 Reich published the book "Writings About Music", containing essays on his philosophy, aesthetics, and musical projects written between 1963 and 1974. An updated and much more extensive collection, "Writings On Music (1965–2000)", was published in 2002.

Reich's work took on a darker character in the 1980s with the introduction of historical themes as well as themes from his Jewish heritage. "Tehillim" (1981), Hebrew for "psalms", is the first of Reich's works to draw explicitly on his Jewish background. The work is in four parts, and is scored for an ensemble of four women's voices (one high soprano, two lyric sopranos and one alto), piccolo, flute, oboe, English horn, two clarinets, six percussion (playing small tuned tambourines without jingles, clapping, maracas, marimba, vibraphone and crotales), two electronic organs, two violins, viola, cello and double bass, with amplified voices, strings, and winds. A setting of texts from Psalms 19:2–5 (19:1–4 in Christian translations), 34:13–15 (34:12–14), 18:26–27 (18:25–26), and 150:4–6, "Tehillim" is a departure from Reich's other work in its formal structure; the setting of texts several lines long rather than the fragments used in previous works makes melody a substantive element. Use of formal counterpoint and functional harmony also contrasts with the loosely structured minimalist works written previously.
"Different Trains" (1988), for string quartet and tape, uses recorded speech, as in his earlier works, but this time as a melodic rather than a rhythmic element. In "Different Trains", Reich compares and contrasts his childhood memories of his train journeys between New York and California in 1939–1941 with the very different trains being used to transport contemporaneous European children to their deaths under Nazi rule. The Kronos Quartet recording of "Different Trains" was awarded the Grammy Award for Best Classical Contemporary Composition in 1990. The composition was described by Richard Taruskin as "the only adequate musical response—one of the few adequate artistic responses in any medium—to the Holocaust", and he credited the piece with earning Reich a place among the great composers of the 20th century.

In 1993, Reich collaborated with his wife, the video artist Beryl Korot, on an opera, "The Cave", which explores the roots of Judaism, Christianity and Islam through the words of Israelis, Palestinians, and Americans, echoed musically by the ensemble. The work, for percussion, voices, and strings, is a musical documentary, named for the Cave of Machpelah in Hebron, where a mosque now stands and Abraham is said to have been buried.

Reich and Korot collaborated on the opera "Three Tales", which concerns the "Hindenburg" disaster, the testing of nuclear weapons on Bikini Atoll, and other more modern concerns, specifically Dolly the sheep, cloning, and the technological singularity.

Reich used sampling techniques for pieces like "Three Tales" and "City Life" from 1994. Reich returned to composing purely instrumental works for the concert hall, starting with "Triple Quartet" in 1998 written for the Kronos Quartet that can either be performed by string quartet and tape, three string quartets or 36-piece string orchestra. According to Reich, the piece is influenced by Bartók's and Alfred Schnittke's string quartets, and Michael Gordon's "Yo Shakespeare".

The instrumental series for the concert hall continued with "Dance Patterns" (2002), "Cello Counterpoint" (2003), and sequence of works centered around Variations: "You Are (Variations)" (2004), a work which looks back to the vocal writing of works like "Tehillim" or "The Desert Music", "Variations for Vibes, Pianos, and Strings" in 2005, for the London Sinfonietta and "Daniel Variations" (2006).

in 2002 Reich was invited by Walter Fink to the annual Komponistenporträt of the Rheingau Musik Festival, as the 12th composer featured.

In an interview with "The Guardian", Reich stated that he continued to follow this direction with his piece "Double Sextet" (2007), which was commissioned by eighth blackbird, an American ensemble consisting of the instrumental quintet (flute, clarinet, violin or viola, cello and piano) of Schoenberg's piece "Pierrot Lunaire" (1912) plus percussion. Reich states that he was thinking about Stravinsky's "Agon" (1957) as a model for the instrumental writing.

December 2010 Nonesuch Records and Indaba Music held a community remix contest in which over 250 submissions were received, and Steve Reich and Christian Carey judged the finals. Reich spoke in a related BBC interview that once he composed a piece he would not alter it again himself; "When it's done, it's done," he said. On the other hand, he acknowledged that remixes have an old tradition e.g. famous religious music pieces where melodies were further developed into new songs.

Reich premiered a piece, "WTC 9/11", written for String Quartet and Tape (a similar instrumentation to that of "Different Trains") in March 2011. It was performed by the Kronos Quartet, at Duke University, North Carolina, US.

On March 5, 2013, the London Sinfonietta, conducted by Brad Lubman, at the Royal Festival Hall in London gave the world premiere of "Radio Rewrite" for ensemble with 11 players, inspired by the music of Radiohead. The programme also included "Double Sextet" for ensemble with 12 players, "Clapping Music", for two people and four hands featuring Reich himself alongside percussionist Colin Currie, "Electric Counterpoint", with electric guitar by Mats Bergström accompanied by a layered soundtrack, as well as two of Reich's small ensemble pieces, one for acoustic instruments, the other for electric instruments and tape.

"Music for Ensemble and Orchestra" was premiered on November 4, 2018 by the Los Angeles Philharmonic under Susanna Mälkki at Walt Disney Concert Hall, marking Reich's return to writing for orchestra after an interval of more than thirty years.

In 2005, Reich was awarded the Edward MacDowell Medal.

Reich was awarded with the Praemium Imperiale Award in Music in October 2006.

On January 25, 2007, Reich was named 2007 recipient of the Polar Music Prize with jazz saxophonist Sonny Rollins.

On April 20, 2009, Reich was awarded the 2009 Pulitzer Prize for Music, recognizing "Double Sextet", first performed in Richmond March 26, 2008. The citation called it "a major work that displays an ability to channel an initial burst of energy into a large-scale musical event, built with masterful control and consistently intriguing to the ear".

In May 2011 Steve Reich received an honorary doctorate from the New England Conservatory of Music.

In 2012, Steve Reich received the Gold Medal in Music by the American Academy of Arts and Letters.

In 2013 Reich received the US$400,000 BBVA Foundation Frontiers of Knowledge Award in contemporary music for bringing a new conception of music, based on the use of realist elements from the realm of daily life and others drawn from the traditional music of Africa and Asia.

In September 2014, Reich was awarded the "Leone d'Oro" (Golden Lion for Lifetime Achievement in Music) from the Venice Biennale.

In March 2016, Reich was awarded an Honorary Doctorate by the Royal College of Music in London.

The American composer and critic Kyle Gann has said that Reich "may ... be considered, by general acclamation, America's greatest living composer". Reich's style of composition has influenced many other composers and musical groups, including John Adams, the progressive rock band King Crimson, the new-age guitarist Michael Hedges, the art-pop and electronic musician Brian Eno, the experimental art/music group The Residents, the electronic group Underworld, the composers associated with the Bang on a Can festival (including David Lang, Michael Gordon, and Julia Wolfe), and numerous indie rock musicians including songwriters Sufjan Stevens and Matthew Healy of the 1975, and instrumental ensembles Tortoise, The Mercury Program (themselves influenced by Tortoise), and Godspeed You! Black Emperor (who titled an unreleased song "Steve Reich").

John Adams commented, "He didn't reinvent the wheel so much as he showed us a new way to ride." He has also influenced visual artists such as Bruce Nauman, and many notable choreographers have made dances to his music, Eliot Feld, Jiří Kylián, Douglas Lee and Jerome Robbins among others; he has expressed particular admiration of Anne Teresa De Keersmaeker's work set to his pieces.

In featuring a sample of Reich's "Electric Counterpoint" (1987) the British ambient techno act the Orb exposed a new generation of listeners to the composer's music with its 1990 production "Little Fluffy Clouds". In 1999 the album "Reich Remixed" featured "re-mixes" of a number of Reich's works by various electronic dance-music producers, such as DJ Spooky, Kurtis Mantronik, Ken Ishii, and Coldcut among others.

Reich's "Cello Counterpoint" (2003) was the inspiration for a series of commissions for solo cello with pre-recorded cellos made by Ashley Bathgate in 2017 including new works by Emily Cooley and Alex Weiser.

Reich often cites Pérotin, J. S. Bach, Debussy, Bartók, and Stravinsky as composers whom he admires and who greatly influenced him when he was young. Jazz is a major part of the formation of Reich's musical style, and two of the earliest influences on his work were vocalists Ella Fitzgerald and Alfred Deller, whose emphasis on the artistic capabilities of the voice alone with little vibrato or other alteration was an inspiration to his earliest works. John Coltrane's style, which Reich has described as "playing a lot of notes to very few harmonies", also had an impact; of particular interest was the album "Africa/Brass", which "was basically a half-an-hour in E." Reich's influence from jazz includes its roots, also, from the West African music he studied in his readings and visit to Ghana. Other important influences are Kenny Clarke and Miles Davis, and visual artist friends such as Sol LeWitt and Richard Serra. Reich has also stated that he admires the music of the band Radiohead, which led to his composition "Radio Rewrite".












</doc>
<doc id="26823" url="https://en.wikipedia.org/wiki?curid=26823" title="Simon &amp; Garfunkel">
Simon &amp; Garfunkel

Simon & Garfunkel were an American folk-rock duo consisting of singer-songwriter Paul Simon and singer Art Garfunkel. One of the best-selling music groups of the 1960s, their biggest hits—including "The Sound of Silence" (1965), "Mrs. Robinson" (1968), "The Boxer" (1969), and "Bridge over Troubled Water" (1970)—reached number one on singles charts worldwide.

Simon and Garfunkel met in elementary school in Queens, New York, in 1953, where they learned to harmonize together and began writing material. By 1957, under the name Tom & Jerry, the teenagers had their first minor success with "Hey Schoolgirl", a song imitating their idols the Everly Brothers. In 1963, aware of a growing public interest in folk music, they regrouped and were signed to Columbia Records as Simon & Garfunkel. Their debut, "Wednesday Morning, 3 A.M.," sold poorly, and they once again disbanded; Simon returned to a solo career, this time in England. In June 1965, a new version of "The Sound of Silence" overdubbed with electric guitar and drums became a major U.S. AM radio hit, reaching number one on the "Billboard" Hot 100. The duo reunited to release a second studio album, "Sounds of Silence," and tour colleges nationwide. On their third release, "Parsley, Sage, Rosemary and Thyme" (1966), the duo assumed more creative control. Their music was featured in the 1967 film "The Graduate", giving them further exposure. Their next album "Bookends" (1968) topped the "Billboard" 200 chart and included the number-one single "Mrs. Robinson" from the film.

The duo's often rocky relationship led to artistic disagreements and their breakup in 1970. Their final studio album, "Bridge over Troubled Water", was released that year and became their most successful, becoming one of the world's best-selling albums. After their breakup, Simon released a number of acclaimed albums, including 1986's "Graceland". Garfunkel released solo hits such as "All I Know" and briefly pursued an acting career, with leading roles in two Mike Nichols films, "Catch-22" and "Carnal Knowledge", and in Nicolas Roeg's 1980 "Bad Timing." The duo have reunited several times, most famously in 1981 for "The Concert in Central Park", which attracted more than 500,000 people, one of the largest concert attendances in history.

Simon & Garfunkel won 10 Grammy Awards and were inducted into the Rock and Roll Hall of Fame in 1990. "Bridge over Troubled Water" is ranked at number 51 on Rolling Stone's 500 Greatest Albums of All Time. Richie Unterberger described them as "the most successful folk-rock duo of the 1960s" and one of the most popular artists from the decade. They are among the best-selling music artists, having sold more than 100 million records.

Paul Simon and Art Garfunkel grew up in the 1940s and 1950s in their predominantly Jewish neighborhood of Kew Gardens Hills in Queens, New York, three blocks away from one another. They attended the same schools: Public School 164 in Kew Gardens Hills, Parsons Junior High School, and Forest Hills High School. They were both fascinated by music; both listened to the radio and were taken with rock and roll as it emerged, particularly the Everly Brothers. Simon first noticed Garfunkel when Garfunkel was singing in a fourth grade talent show, which Simon thought was a good way to attract girls; he hoped for a friendship, which started in 1953, when they appeared in a sixth grade adaptation of "Alice in Wonderland". They formed a streetcorner doo-wop group called the Peptones with three friends and learned to harmonize. They began performing as a duo at school dances.

Simon and Garfunkel moved to Forest Hills High School, where in 1956 they wrote their first song, "The Girl for Me"; Simon's father sent a handwritten copy to the Library of Congress to register a copyright. While trying to remember the lyrics to the Everly Brothers song "Hey Doll Baby", they wrote "Hey Schoolgirl", which they recorded for $25 at Sanders Recording Studio in Manhattan. While recording they were overheard by promoter Sid Prosen, who signed them to his independent label Big Records after speaking to their parents. They were 15.

Under Big Records, Simon and Garfunkel assumed the name Tom & Jerry; Garfunkel named himself Tom Graph, a reference to his interest in mathematics, and Simon Jerry Landis, after the surname of a girl he had dated. Their first single, "Hey Schoolgirl", was released with the B-side "Dancin' Wild" in 1957. Prosen, using the payola system, bribed DJ Alan Freed $200 to play the single on his radio show, where it became a nightly staple. "Hey Schoolgirl" attracted regular rotation on nationwide AM pop stations, leading it to sell over 100,000 copies and to land on "Billboard" charts at number 49. Prosen promoted the group heavily, getting them a headlining spot on Dick Clark's "American Bandstand" alongside Jerry Lee Lewis. Simon and Garfunkel shared approximately $4,000 from the song – earning two percent each from royalties, the rest staying with Prosen. They released two more singles on Big Records ("Our Song" and "That's My Story") neither of them successful.

After graduating from Forest Hills High School in 1958, the pair continued their education should a music career not unfold. Simon studied English at Queens College, City University of New York, and Garfunkel studied architecture before switching to art history at Columbia College, Columbia University. While still with Big Records as a duo, Simon released a solo single, "True or False", under the name "True Taylor". This upset Garfunkel, who regarded it as a betrayal; the emotional tension from the incident occasionally surfaced throughout their relationship.

Simon and Garfunkel continued recording as solo artists: Garfunkel composed and recorded "Private World" for Octavia Records, and—under the name Artie Garr—"Beat Love" for Warwick; Simon recorded with the Mystics and Tico & The Triumphs, and wrote and recorded under the names Jerry Landis and Paul Kane. Simon also wrote and performed demos for other artists, working for a while with Carole King and Gerry Goffin.

After graduating in 1963, Simon joined Garfunkel, who was still at Columbia University, to perform again as a duo, this time with a shared interest in folk music. Simon enrolled part-time in Brooklyn Law School. By late 1963, billing themselves as Kane & Garr, they performed at Gerde's Folk City, a Greenwich club that hosted Monday night open mic performances. They performed three new songs—"Sparrow", "He Was My Brother", and "The Sound of Silence"—and attracted the attention of Columbia Records staffer Tom Wilson, a prominent A&R man and producer (who would later become a key architect of Bob Dylan's transition from folk to rock). As a "star producer" for the label, he wanted to record "He Was My Brother" with a new British act, the Pilgrims. Simon convinced Wilson to let him and Garfunkel audition in the studio, where they performed "The Sound of Silence". At Wilson's urging, Columbia signed them.

Simon & Garfunkel's debut studio album, "Wednesday Morning, 3 A.M.", produced by Wilson, was recorded over three sessions in March 1964 and released in October. It contains five compositions by Simon, three traditional folk songs, and four folk-influenced singer-songwriter songs. Simon was adamant that they would no longer use stage names. Columbia set up a promotional showcase at Folk City on March 31, 1964, the duo's first public concert as Simon & Garfunkel. The showcase, as well as other scheduled performances, did not go well.

"Wednesday Morning, 3 A.M." sold only 3,000 copies on release. Simon moved to England, where he toured small folk clubs and befriended folk artists such as Bert Jansch, Martin Carthy, Al Stewart, and Sandy Denny. He also met Kathy Chitty, who became the object of his affection and is the Kathy in "Kathy's Song" and "America".

A small music publishing company, Lorna Music, licensed "Carlos Dominguez", a single Simon had recorded two years prior as Paul Kane, for a cover by Val Doonican that sold well. Simon visited Lorna to thank them, and the meeting resulted in a publishing and recording contract. He signed to the Oriole label and released "He Was My Brother" as a single. Simon invited Garfunkel to stay for the summer of 1964.

Near the end of the season, Garfunkel returned to Columbia for class. Simon also returned to the US, and resumed his studies at Brooklyn Law School for one semester, partially at his parents' insistence. He returned to England in January 1965, now certain that music was his calling. In the meantime, his landlady, Judith Piepe, had compiled a tape from his work at Lorna and sent it to the BBC in hopes they would play it. The demos aired on the "Five to Ten" morning show, and were instantly successful. Oriole had folded into CBS by that point, and hoped to record a new Simon album. 

Simon recorded his first solo album, "The Paul Simon Songbook," in June 1965, featuring future Simon & Garfunkel staples including "I Am a Rock" and "April Come She Will". CBS flew Wilson over to produce the record, and he stayed at Simon's flat. The album was released in August; although sales were poor, Simon felt content with his future in England. Garfunkel graduated in 1965, returning to Columbia University to do a master's degree in mathematics.

In the United States, Dick Summer, a late-night DJ at WBZ in Boston, played "The Sound of Silence"; it became popular with a college audience. It was picked up the next day along the East Coast of the United States, down to Cocoa Beach, Florida. When Wilson heard about this new wave of interest, he took inspiration from the success of the folk-rock hybrid that he and Dylan had created with "Like a Rolling Stone", and crafted a rock remix of the song using studio musicians. The remix was issued in September 1965, where it reached the "Billboard" Hot 100. Wilson did not inform the duo of his plan, and Simon was "horrified" when he first heard it.

By January 1966, "The Sound of Silence" had topped the Hot 100, selling over one million copies. Simon reunited with Garfunkel in New York, leaving Chitty and his friends in England behind. CBS demanded a new album, to be called "Sounds of Silence", to ride the wave of the hit. Recorded in three weeks, and consisting of rerecorded songs from "The Paul Simon Songbook" plus four new tracks, "Sounds of Silence" was rush-released in mid-January 1966, peaking at number 21 "Billboard" Top LPs chart. A week later, "Homeward Bound" was released as a single, entering the USA top ten, followed by "I Am a Rock" peaking at number three. The duo supported the recordings with a nationwide tour of America, while CBS continued their promotion by re-releasing "Wednesday Morning, 3 A.M.", which charted at number 30. Despite the success, the duo received critical derision, as many considered them a manufactured imitation of folk music.

As they considered "The Sounds of Silence" a "rush job" to capitalize on their sudden success, Simon & Garfunkel spent more time crafting the follow-up. It was the first time Simon insisted on total control in aspects of recording. Work began in 1966 and took nine months. Garfunkel considered the recording of "Scarborough Fair" to be the point at which they stepped into the role of producer, as they were constantly beside engineer Roy Halee mixing. "Parsley, Sage, Rosemary and Thyme" was issued in October 1966, following the release of several singles and sold-out college campus shows. The duo resumed their college circuit tour eleven days later, crafting an image that was described as "alienated", "weird", and "poetic". Manager Mort Lewis also was responsible for this public perception, as he withheld them from television appearances unless they were allowed to play an uninterrupted set or choose the setlist. Simon, then 26, felt he had "made it" into an upper echelon of rock and roll while retaining artistic integrity; according to his biographer Marc Eliot, this made him "spiritually closer to Bob Dylan than to, say, Bobby Darin". The duo chose William Morris as their booking agency after a recommendation from Wally Amos, a mutual friend of Wilson.

During the sessions for "Parsley", Simon and Garfunkel recorded "A Hazy Shade of Winter"; it was released as a single, peaking at number 13 on the national charts. Similarly, they recorded "At the Zoo" for single release in early 1967; it charted at number 16. Simon began work for their next album around this time, telling "High Fidelity" that "I'm not interested in singles anymore". He developed writer's block, which led to no new album on the horizon for 1967. Artists at the time were expected to release two or three albums each year, and the lack of productivity worried Columbia executives. Amid concerns for Simon's idleness, Columbia Records chairman Clive Davis arranged for up-and-coming producer John Simon to kick-start the recording. Simon was distrustful of label executives; on one occasion, he and Garfunkel recorded a meeting with Davis, who was giving a "fatherly talk" on speeding up production, to laugh at it later. The rare television appearances at this time saw the duo performing on network broadcasts as "The Ed Sullivan Show", "The Mike Douglas Show", and "The Andy Williams Show" in 1966, and twice on "The Smothers Brothers Comedy Hour" in 1967.

Meanwhile, director Mike Nichols, then filming "The Graduate", had become fascinated with Simon & Garfunkel's records, listening to them extensively before and after filming. He met Davis to ask for permission to license Simon & Garfunkel music for his film. Davis viewed it as a perfect fit and envisioned a bestselling soundtrack album. Simon was not as receptive and was cautious of "selling out". However, after meeting Nichols and being impressed by his wit and the script, he agreed to write new songs for the film. Leonard Hirshan, a powerful agent at William Morris, negotiated a deal that paid Simon $25,000 to submit three songs to Nichols and producer Lawrence Turman. When Nichols was not impressed by Simon's songs "Punky's Dilemma" and "Overs", Simon and Garfunkel offered another, incomplete song, which became "Mrs. Robinson"; Nichols loved it.

Simon & Garfunkel's fourth studio album, "Bookends", was recorded in fits and starts from late 1966 to early 1968. Although the album had long been planned, work did not begin in earnest until late 1967. The duo were signed under an older contract that specified the label pay for sessions, and Simon & Garfunkel took advantage of this, hiring viola and brass players and percussionists. The record's brevity reflects its concise and perfectionist production; the team spent over 50 hours recording "Punky's Dilemma", for example, and rerecorded vocal parts, sometimes note by note, until they were satisfied. Garfunkel's songs and voice took a lead role on some of the songs, and the harmonies for which the duo was known gradually disappeared. For Simon, "Bookends" represented the end of the collaboration and became an early indicator of his intentions to go solo.

Prior to release, the band helped put together and performed at the Monterey Pop Festival, which signaled the beginning of the Summer of Love on the West Coast. "Fakin' It" was issued as a single that summer and found only modest success on AM radio; the duo were much more focused on the rising FM format, which played album tracks and treated their music with respect. In January 1968, the duo appeared on a Kraft Music Hall special, "Three for Tonight", performing ten songs, largely taken from their previous album. "Bookends" was released by Columbia Records in April 1968, 24 hours before the assassination of civil rights movement activist Martin Luther King Jr., which spurred nationwide outrage and riots. The album debuted on the "Billboard" Top LPs in the issue dated April 27, 1968, climbing to number one and staying at that position for seven non-consecutive weeks; it remained on the chart as a whole for 66 weeks. "Bookends" received such heavy orders weeks in advance of its release that Columbia was able to apply for award certification before copies left the warehouse, a fact it touted in magazine ads. The album became the duo's bestselling to date, helped by the attention for the "Graduate" soundtrack ten weeks earlier, creating an initial combined sales figure of over five million units.

Davis had predicted this, and suggested raising the list price of "Bookends" by one dollar to $5.79, above the then standard retail price, to compensate for a large poster included in vinyl copies. Simon scoffed and viewed it as charging a premium on "what was sure to be that year's best-selling Columbia album". According to biographer Marc Eliot, Davis was "offended by what he perceived as their lack of gratitude for what he believed was his role in turning them into superstars". Rather than implement Davis' plan, Simon & Garfunkel signed a contract extension with Columbia that guaranteed them a higher royalty rate. At the 1969 Grammy Awards, the lead single "Mrs. Robinson" became the first rock and roll song to receive Record of the Year, and also won Best Contemporary Pop Performance by a Duo or Group.

"Bookends", alongside the "Graduate" soundtrack, made Simon & Garfunkel the biggest rock duo in the world. Simon was approached by producers to write music for films or license songs; he turned down Franco Zeffirelli, who was preparing to film "Brother Sun, Sister Moon", and John Schlesinger, who was preparing to film "Midnight Cowboy". In addition to Hollywood proposals, Simon declined a request by producers from the Broadway show "Jimmy Shine" (starring Simon's friend Dustin Hoffman, also the lead in "Midnight Cowboy"). He collaborated briefly with Leonard Bernstein on a sacred mass before withdrawing from the project due to "finding it perhaps too far afield from his comfort zone".

Garfunkel began acting, and played Captain Nately in the Nichols film "Catch-22" based on the novel of the same name. Simon was to play the character of Dunbar, but screenwriter Buck Henry felt the film was already crowded with characters and wrote Simon's part out. Filming began in January 1969 and lasted about eight months, longer than expected. The production endangered the duo's relationship; Simon had completed no new songs, and the duo planned to collaborate after filming ended. Following the end of filming in October, the first performance of what was planned to be their last tour took place in Ames, Iowa. The US leg of the tour ended in the sold-out Carnegie Hall on November 27. Meanwhile, the duo, working with director Charles Grodin, produced an hourlong CBS special, "Songs of America", a mixture of scenes featuring notable political events and leaders concerning the US, such as the Vietnam War, Martin Luther King, John F. Kennedy's funeral procession, Cesar Chavez and the Poor People's March. It was broadcast only once, due to tension at the network regarding its content.

"Bridge over Troubled Water", Simon & Garfunkel's final studio album, was released in January 1970 and charted in over 11 countries, topping the charts in 10, including the "Billboard" Top LP's chart in the US and the UK Albums Chart. It was the best-selling album in 1970, 1971 and 1972 and was at that time the best-selling album of all time. It was also CBS Records' best-selling album before the release of Michael Jackson's "Thriller" in 1982. The album topped the "Billboard" charts for 10 weeks and stayed in the charts for 85 weeks. In the United Kingdom, the album topped the charts for 35 weeks, and spent 285 weeks in the top 100, from 1970 to 1975. It has since sold over 25 million copies worldwide. "Bridge over Troubled Water", the lead single, reached number one in five countries and became the duo's biggest seller. The song has been covered by over 50 artists, including Elvis Presley, Johnny Cash, Aretha Franklin, Willie Nelson, Roy Orbison, Michael W. Smith and Josh Groban. "Cecilia", the follow-up, reached number four in the US, and "El Condor Pasa" hit number 18. A brief British tour followed the album release, and the duo's last concert as Simon & Garfunkel took place at Forest Hills Stadium. In 1971, the album won six awards at the 13th Annual Grammy Awards, including Album of the Year.

The recording of "Bridge over Troubled Water" was difficult and Simon and Garfunkel's relationship had deteriorated. "At that point, I just wanted out," Simon later said. At the urging of his wife, Peggy Harper, Simon called Davis to confirm the duo's breakup. For the next several years, they spoke only two or three times a year. 

In the 1970s, the duo reunited several times. Their first reunion was Together for McGovern, a benefit concert for presidential candidate George McGovern at New York's Madison Square Garden in June 1972. In 1975, they reconciled when they visited a recording session with John Lennon and Harry Nilsson. For the rest of the year, they attempted to make the reunion work, but their collaboration only yielded one song, "My Little Town", that was featured on Simon's "Still Crazy After All These Years" and Garfunkel's "Breakaway", both released in 1975. The song peaked at number nine on the Hot 100. In 1975, Garfunkel joined Simon for a medley of three songs on "Saturday Night Live," guest-hosted by Simon. In 1977, Garfunkel joined Simon for a brief performance of their old songs on "The Paul Simon Special", and later that year they recorded a cover of Sam Cooke's "(What a) Wonderful World" with James Taylor. Old tensions appeared to dissipate upon Garfunkel's return to New York in 1978, when the duo began interacting more often. On May 1, 1978, Simon joined Garfunkel for a concert held at Carnegie Hall to benefit the hearing disabled.
By 1980, the duo's solo careers were not doing well. To help alleviate New York's economic decline, concert promoter Ron Delsener suggested a free concert in Central Park. Delsener contacted Simon with the idea of a Simon & Garfunkel reunion, and once Garfunkel had agreed, plans were made. The concert, held on September 19, 1981 attracted more than 500,000 people, at that time the largest ever concert attendance. Warner Bros. Records released a live album of the show, "The Concert in Central Park", which went double platinum in the US. A 90-minute recording of the concert was sold to Home Box Office (HBO) for over $1 million. The concert created a renewed interest in Simon & Garfunkel's work. They had several "heart-to-heart talks", attempting to put their disagreements behind them. The duo undertook a world tour beginning in May 1982, but their relationship grew contentious: for the majority of the tour, they did not speak to one another.

Warner Bros. pushed for the duo to extend the tour and release a new studio album. Simon had new material ready, and, according to Simon, "Artie made a persuasive case that he could make it into a natural duo record." However, the duo quarrelled again; Garfunkel refused to learn the songs in the studio and would not give up his longstanding cannabis and cigarette habits, despite Simon's requests. Instead, the material became Simon's 1983 album "Hearts and Bones". A spokesperson said: "Paul simply felt the material he wrote is so close to his own life that it had to be his own record. Art was hoping to be on the album, but I'm sure there will be other projects that they will work on together." Another rift opened when the lengthy recording of Simon's 1986 album "Graceland" prevented Garfunkel from working with engineer Roy Halee on his Christmas album "The Animals' Christmas" (1985). In 1986, Simon said he and Garfunkel remained friends and got on well, "like when we were 10 years old", when they were not working together.

In 1990, Simon and Garfunkel were inducted into the Rock and Roll Hall of Fame. Garfunkel thanked Simon, calling him "the person who most enriched my life by putting those songs through me"; Simon responded, "Arthur and I agree about almost nothing. But it's true, I have enriched his life quite a bit." After performing three songs, the duo left without speaking. In August 1991, Simon staged his own concert in Central Park, released as a live album, "Paul Simon's Concert in the Park," a few months later. He declined an offer from Garfunkel to perform with him at the park.

By 1993, their relationship had thawed, and Simon invited Garfunkel on an international tour. Following a 21-date sold-out run at the Paramount Theater in New York and an appearance at that year's Bridge School Benefit in California, they toured the Far East. They became acrimonious again for the rest of the decade. Simon thanked Garfunkel at his 2001 induction into the Rock and Roll Hall of Fame as a solo artist: "I regret the ending of our friendship. I hope that some day before we die we will make peace with each other," adding after a pause, "No rush."

In 2003, Simon and Garfunkel received a Lifetime Achievement Award at the 45th Annual Grammy Awards, for which the promoters convinced them to open with a performance of "The Sound of Silence". The performance was satisfying for both, and they planned a full-scale reunion tour. The Old Friends tour began in October 2003 and played to sold-out audiences across the United States for 40 dates until mid-December, earning an estimated $123 million. A second U.S. leg commenced in June, 2004, consisting of 20 cities. Following a 12-city run in Europe in 2004, they ended their nine-month tour with a free concert along Via dei Fori Imperiali, in front of the Colosseum in Rome, on 31 July 2004. It attracted 600,000 fans, more than their Concert in Central Park. In 2005, Simon and Garfunkel performed three songs for a Hurricane Katrina benefit concert in Madison Square Garden, including a performance with singer Aaron Neville.
In February 2009, Simon and Garfunkel reunited for three songs during Simon's two-night engagement at New York's Beacon Theatre. This led to a reunion tour of Asia and Australia in June and July, 2009. On October 29, 2009, they performed five songs at the 25th Anniversary Rock and Roll Hall of Fame Concert at Madison Square Garden. Their headlining set at the 2010 New Orleans Jazz and Heritage Festival was difficult for Garfunkel, who had vocal problems. "I was terrible, and crazy nervous. I leaned on Paul Simon and the affection of the crowd," he told "Rolling Stone" several years later. Garfunkel was diagnosed with vocal cord paresis, and the remaining tour dates were cancelled. His manager, John Scher, informed Simon's camp that Garfunkel would be ready within a year, which did not happen, damaging relations between the two. Simon continued to publicly wish Garfunkel better health and praised his "angelic" voice. Garfunkel regained his vocal strength over the course of the next four years, performing shows in a Harlem theater and to underground audiences.

In 2014, Garfunkel told "Rolling Stone" that he believed he and Simon would tour again, but said: "I know that audiences all over the world like Simon and Garfunkel. I'm with them. But I don't think Paul Simon's with them." Asked about a reunion in 2016, Simon said: "Quite honestly, we don't get along. So it's not like it's fun. If it was fun, I'd say, OK, sometimes we'll go out and sing old songs in harmony. That's cool. But when it's not fun, you know, and you're going to be in a tense situation, well, then I have a lot of musical areas that I like to play in. So that'll never happen again. That's that." In February 2018, Simon announced his retirement from touring.

Over the course of their career, Simon & Garfunkel's music gradually moved from a basic folk rock sound to incorporate more experimental elements for the time, including Latin and gospel music. Their music, according to "Rolling Stone", struck a chord among lonely, alienated young adults near the end of the decade.

Simon & Garfunkel received criticism at the height of their success. In 1968, "Rolling Stone" critic Arthur Schmidt described their music as "questionable ... it exudes a sense of process, and it is slick, and nothing too much happens." "New York Times" critic Robert Shelton said that the duo had "a kind of Mickey Mouse, timid, contrived" approach. According to Richie Unterberger of AllMusic, their clean sound and muted lyricism "cost them some hipness points during the psychedelic era ... the pair inhabited the more polished end of the folk-rock spectrum and was sometimes criticized for a certain collegiate sterility." He noted that some critics regard Simon's later solo work as superior to Simon & Garfunkel.

According to "Pitchfork", though Simon & Garfunkel were a highly regarded folk act "distinguished by their intuitive harmonies and Paul Simon's articulate songwriting", they were more conservative than the folk music revivalists of Greenwich Village. By the late 1960s, they had become the "folk establishment ... primarily unthreatening and accessible, which forty years later makes them an ideal gateway act to the weirder, harsher, more complex folkies of the 60s counterculture". However, their later albums explored more ambitious production techniques and incorporated elements of gospel, rock, R&B, and classical, revealing a "voracious musical vocabulary".

The Grammy Awards are held annually by the National Academy of Recording Arts and Sciences. Simon & Garfunkel have won 9 total competitive awards, 4 Hall of Fame awards, and a Lifetime Achievement Award.








</doc>
<doc id="26824" url="https://en.wikipedia.org/wiki?curid=26824" title="State Street Corporation">
State Street Corporation

State Street Corporation is an American financial services and bank holding company headquartered at One Lincoln Street in Boston with operations worldwide. It is the second-oldest continually operating United States bank; its predecessor, Union Bank, was founded in 1792. State Street is ranked 15th on the list of largest banks in the United States by assets. It is one of the largest asset management companies in the world with US$2.511 trillion under management and US$31.62 trillion under custody and administration. It is the second largest custodian bank in the world.

The company is ranked 247th on the Fortune 500 as of 2019. The company is on the list of the banks that are too big to fail published by the Financial Stability Board.

The company is named after State Street in Boston, which was known as the "Great Street to the Sea" in the 18th century as Boston became a flourishing maritime capital. The company's logo includes a clipper to reflect the maritime industry in Boston during this time.

State Street Bank and Trust Company, also known as Global Services, is the investment servicing division of State Street. It provides asset owners and managers with custodian bank services (safekeeping, corporate actions), fund accounting (pricing and valuation), and administration (financial reporting, tax, compliance, and legal) services. Global Services handles assets from many classes, including stocks, derivatives, exchange-traded funds, fixed income assets, private equity, and real estate. State Street administers 40% of the assets under administration in the US mutual fund market. Global Services also provides outsourcing for operations activities and handles US$10.2 trillion of middle-office assets.
State Street Global Advisors dates back to 1978. It provides asset management, investment management, research, and advisory services to corporations, mutual funds, insurance companies, and other institutional investors. Global Advisors develops both passive management and active management strategies using both quantitative and fundamental approaches.

In 1993, the company created the SPDR S&P 500 Trust ETF, the first exchange-traded fund (ETF), and is now one of the largest ETF providers worldwide.

Trading on SPDR began January 29, 1993.

Global Markets is State Street's securities business. It offers research, trading, and securities lending services for foreign exchange, stocks, fixed income, and derivatives. To avoid a conflict of interest, the company does not run proprietary trading books. Global Markets maintains trading desks in Boston, London, Sydney, Toronto, and Tokyo.

The company traces its roots to Union Bank, which received a charter in 1792 from Massachusetts Governor John Hancock. It was the third bank to be chartered in Boston and its office was at the corner of State and Exchange Streets. In 1865, Union Bank received a national charter and became the National Union Bank of Boston. The bank later built a headquarters at Washington and State streets.

State Street Deposit & Trust Co opened in 1891. It became the custodian of the first U.S. mutual fund in 1924, the Massachusetts Investors Trust (now MFS Investment Management).

State Street and National Union merged in 1925. The merged bank took the State Street name, but National Union was the nominal survivor, and it operated under National Union's charter, thus giving the current entity its rank among the oldest banks in the United States.

The company merged with Second National Bank in 1955 and with the Rockland-Atlas National Bank in 1961.

In 1966, the company completed construction of the State Street Bank Building, a new headquarters building, the first high-rise office tower in downtown Boston.

In 1972, the company opened its first international office in Munich.

In 1973, as a 50/50 joint venture with DST Systems, the company formed Boston Financial Data Services, a provider of shareholder record-keeping, intermediary and investor services, and regulatory compliance. More than 100 top staff from IBM were hired by State Street as it set about implementing IBM mainframe computer systems.

In 1975, William Edgerly became president and chief executive officer of the bank and shifted the company's strategy from commercial banking to investments and securities processing.

During the 1980s and 1990s, the company opened offices in Montreal, Toronto, Dublin, London, Paris, Dubai, Sydney, Wellington, Hong Kong, and Tokyo.

By 1992, most of State Street's revenue came from fees for holding securities, settling trades, keeping records, and performing accounting. In 1994, the company formed State Street Global Advisors, a global asset management business.

In 1995, State Street acquired Investors Fiduciary Trust of Kansas City for $162 million from DST Systems and Kemper Financial Services. In 1996, Bank of New York acquired the unit investment trust servicing business of Investors Fiduciary Trust Co., Kansas City, Mo.

In 1999, State Street sold its retail and commercial banking businesses to Citizens Financial Group.

In 1990, State Street Bank Luxembourg was founded, and is the largest player in the country's fund industry by assets.

In 2003, the company acquired the securities services division of Deutsche Bank for $1.5 billion. The company also sold its corporate trust business to U.S. Bancorp for $725 million. Also in 2003, State Street sold its private asset management business to U.S. Trust. 

In July 2007, the company acquired Investors Bank & Trust for $4.5 billion.

In October 2008, the United States Department of the Treasury invested $2 billion in the company as part of the Troubled Asset Relief Program and in July 2009, the company became the first major financial firm to repay the Treasury.

In 2010, the company acquired Mourant International Finance Administration. It also acquired the securities services group of Intesa Sanpaolo for $1.87 billion. In December 2010, the company announced that it would be retrenching 5% of its workforce and effectively reducing the hourly wages of remaining employees by 10% via increased standard work hours.

In November 2011, the company was named as amongst the world's 29 systemic banks.

In 2012, the company acquired Goldman Sachs Administration Services, a hedge fund administrator, for $550 million.

In November 2014, the company sold SSARIS Advisors, its hedge fund unit, to senior management.

In 2016, State Street launched a program called Beacon, focused on cutting costs and improving reporting technology. Their main focus was to shrink their US workforce in order to bolster profits in excess of $2.5 billion (2018 figures). Also in 2016, the company acquired the asset management business of General Electric. 

In 2017, the company announced that Jay Hooley, the chief executive officer of the company, would retire.

In 2018, State Street completed its acquisition of Charles River Development, a Burlington, Massachusetts provider of investment management software. The deal closed October 1, 2018, at a cost of approximately $2.6 billion that will be financed by the suspension of share repurchases and the issuing of common and preferred equity. News of the acquisition led to a drop in State Street shares of nearly 10% with share prices remaining flat since the purchase.

In January 2019, State Street announced that it planned to lay off 1,500 employees, increasing the number to 2,300 in July. The company is shifting their workforce from the United States to countries like China, India and Poland and is operating under a hiring freeze. Offsetting this is increased overseas hiring, resulting in a 3K+ net gain of employment.

In 2009, California alleged on behalf of its pension funds CalPERS and CalSTRS that State Street had committed fraud on currency trades handled by the custodian bank. In October 2011, two executives from State Street Global Markets left the company following charges over the pricing of a fixed income transaction. In April 2016, they were charged by the United States Department of Justice.

On February 28, 2012, State Street Global Advisors entered into a consent order with the Massachusetts Securities Division. The Division was investigating the firm's role as the investment manager of a $1.65 billion (USD) hybrid collateralized debt obligation. The investigation resulted in a fine of $5 million (USD) for the non-disclosure of certain initial investors taking a short position on portions of the CDO.

During the May 2012 annual shareholders meeting, chairman and chief executive Jay Hooley was shouted down on numerous occasions by protesters in relation to the outsourcing and other grievances.

On January 18, 2017, State Street agreed to pay $64.6 million to resolve U.S. investigations into what prosecutors said was a scheme to defraud six clients through secret commissions on billions of dollars of trades.

In 2018 former employee of State Street, Edward Pennings was sentenced to six months in prison for his role in the scheme.  Employee Ross McLellan was also sentenced in the United States to 18 months in prison.

In March 2017, State Street Global Advisors commissioned a statue called "Fearless Girl" by Kristen Visbal and placed it temporarily in the Financial District, Manhattan, in front of the Wall Street icon "Charging Bull". The statue is an advertisement for an index fund which comprises gender diverse companies that have a higher percentage of women among their senior leadership. While some have seen it as an encouragement of women in business, some women criticized the statue as "corporate feminism" that violated their own feminist principles. In October 2017, the company paid $5 million to settle a lawsuit charging that it had paid certain female and African-American executives less than their male and European-American peers.




</doc>
<doc id="26825" url="https://en.wikipedia.org/wiki?curid=26825" title="Spanish language">
Spanish language

Spanish () or Castilian (, ) is a Romance language that originated in the Iberian Peninsula of Europe and today is a global language with more than 483 million native speakers, mainly in Spain and the Americas. It is the world's second-most spoken native language, after Mandarin Chinese, and the world's fourth-most spoken language, after English, Mandarin Chinese and Hindi.

Spanish is a part of the Ibero-Romance group of languages, which evolved from several dialects of Vulgar Latin in Iberia after the collapse of the Western Roman Empire in the 5th century. The oldest Latin texts with traces of Spanish come from mid-northern Iberia in the 9th century, and the first systematic written use of the language happened in Toledo, a prominent city of the Kingdom of Castile, in the 13th century. Beginning in 1492, the Spanish language was taken to the viceroyalties of the Spanish Empire, most notably to the Americas, as well as territories in Africa, Oceania and the Philippines.

A 1949 study by Italian-American linguist Mario Pei, analyzing the degree of difference from a language's parent (Latin, in the case of Romance languages) by comparing phonology, inflection, syntax, vocabulary, and intonation, indicated the following percentages (the higher the percentage, the greater the distance from Latin): In the case of Spanish, it is one of the closest Romance languages to Latin (20% distance), only behind Sardinian (8% distance) and Italian (12% distance). Around 75% of modern Spanish vocabulary is derived from Latin, including Latin borrowings from Ancient Greek.
Spanish vocabulary has been in contact with Arabic from an early date, having developed during the Al-Andalus era in the Iberian Peninsula and around 8% of its vocabulary has an Arabic lexical root. It has also had small influences from Basque, Iberian, Celtiberian, Visigothic, and other neighboring Ibero-Romance languages. Additionally, it has absorbed vocabulary from other languages, particularly other Romance languages—French, Italian, Andalusi Romance, Portuguese, Galician, Catalan, Occitan, and Sardinian—as well as from Quechua, Nahuatl, and other indigenous languages of the Americas.

Spanish is one of the six official languages of the United Nations. It is also used as an official language by the European Union, the Organization of American States, the Union of South American Nations, the Community of Latin American and Caribbean States, the African Union and many other international organizations.

Despite its large number of speakers, the Spanish language does not feature prominently in scientific writing, though it is better represented in the humanities. Approximately 75% of scientific production in Spanish is divided into three thematic areas: social sciences, medical sciences and arts/humanities. Spanish is the third most used language on the internet after English and Chinese. 
It is estimated that there are more than 437 million people who speak Spanish as a native language, which qualifies it as second on the lists of languages by number of native speakers. Instituto Cervantes claims that there are an estimated 477 million Spanish speakers with native competence and 572 million Spanish speakers as a first or second language—including speakers with limited competence—and more than 21 million students of Spanish as a foreign language.

Spanish is the official, or national language in Spain, Equatorial Guinea, and 18 countries and one territory in the Americas. Speakers in the Americas total some 418 million. It is also an optional language in the Philippines as it was a Spanish colony from 1569 to 1899. In the European Union, Spanish is the mother tongue of 8% of the population, with an additional 7% speaking it as a second language. The country with the largest number of native speakers is Mexico. Spanish is the most popular second language learned in the United States. In 2011 it was estimated by the American Community Survey that of the 55 million Hispanic United States residents who are five years of age and over, 38 million speak Spanish at home.

According to a 2011 paper by U.S. Census Bureau Demographers Jennifer Ortman and Hyon B. Shin, the number of Spanish speakers is projected to rise through 2020 to anywhere between 39 million and 43 million, depending on the assumptions one makes about immigration. Most of these Spanish speakers will be Hispanic, with Ortman and Shin projecting between 37.5 million and 41 million Hispanic Spanish speakers by 2020.

In Spain and in some other parts of the Spanish-speaking world, Spanish is called not only (Spanish) but also (Castilian), the language from the kingdom of Castile, contrasting it with other languages spoken in Spain such as Galician, Basque, Asturian, Catalan, Aragonese and Occitan.

The Spanish Constitution of 1978 uses the term to define the official language of the whole Spanish State in contrast to (lit. "the other Spanish languages"). Article III reads as follows:
The Spanish Royal Academy, on the other hand, currently uses the term in its publications, but from 1713 to 1923 called the language .

The (a language guide published by the Spanish Royal Academy) states that, although the Royal Spanish Academy prefers to use the term in its publications when referring to the Spanish language, both terms— and —are regarded as synonymous and equally valid.

The term comes from the Latin word , which means "of or pertaining to a fort or castle".

Different etymologies have been suggested for the term (Spanish). According to the Royal Spanish Academy, derives from the Provençal word "espaignol" and that, in turn, derives from the Vulgar Latin . It comes from the Latin name of the province of Hispania that included the current territory of the Iberian Peninsula.

There are other hypotheses apart from the one suggested by the Royal Spanish Academy. Spanish philologist Menéndez Pidal suggested that the classic or took the suffix from Vulgar Latin, as it happened with other words such as (Breton) or (Saxon). The word evolved into the Old Spanish , which eventually, became .

The Spanish language evolved from Vulgar Latin, which was brought to the Iberian Peninsula by the Romans during the Second Punic War, beginning in 210 BC. Previously, several pre-Roman languages (also called Paleohispanic languages)—some related to Latin via Indo-European, and some that are not related at all—were spoken in the Iberian Peninsula. These languages included Basque (still spoken today), Iberian, Celtiberian and Gallaecian.

The first documents to show traces of what is today regarded as the precursor of modern Spanish are from the 9th century. Throughout the Middle Ages and into the modern era, the most important influences on the Spanish lexicon came from neighboring Romance languages—Mozarabic (Andalusi Romance), Navarro-Aragonese, Leonese, Catalan, Portuguese, Galician, Occitan, and later, French and Italian. Spanish also borrowed a considerable number of words from Arabic, as well as a minor influence from the Germanic Gothic language through the migration of tribes and a period of Visigoth rule in Iberia. In addition, many more words were borrowed from Latin through the influence of written language and the liturgical language of the Church. The loanwords were taken from both Classical Latin and Renaissance Latin, the form of Latin in use at that time.

According to the theories of Ramón Menéndez Pidal, local sociolects of Vulgar Latin evolved into Spanish, in the north of Iberia, in an area centered in the city of Burgos, and this dialect was later brought to the city of Toledo, where the written standard of Spanish was first developed, in the 13th century. In this formative stage, Spanish developed a strongly differing variant from its close cousin, Leonese, and, according to some authors, was distinguished by a heavy Basque influence (see Iberian Romance languages). This distinctive dialect spread to southern Spain with the advance of the , and meanwhile gathered a sizable lexical influence from the Arabic of Al-Andalus, much of it indirectly, through the Romance Mozarabic dialects (some 4,000 Arabic-derived words, make up around 8% of the language today). The written standard for this new language was developed in the cities of Toledo, in the 13th to 16th centuries, and Madrid, from the 1570s.

The development of the Spanish sound system from that of Vulgar Latin exhibits most of the changes that are typical of Western Romance languages, including lenition of intervocalic consonants (thus Latin > Spanish ). The diphthongization of Latin stressed short and —which occurred in open syllables in French and Italian, but not at all in Catalan or Portuguese—is found in both open and closed syllables in Spanish, as shown in the following table:
Spanish is marked by the palatalization of the Latin double consonants and (thus Latin

The consonant written or in Latin and pronounced in Classical Latin had probably "fortified" to a bilabial fricative in Vulgar Latin. In early Spanish (but not in Catalan or Portuguese) it merged with the consonant written "b" (a bilabial with plosive and fricative allophones). In modern Spanish, there is no difference between the pronunciation of orthographic and , with some exceptions in Caribbean Spanish.

Peculiar to Spanish (as well as to the neighboring Gascon dialect of Occitan, and attributed to a Basque substratum) was the mutation of Latin initial into whenever it was followed by a vowel that did not diphthongize. The , still preserved in spelling, is now silent in most varieties of the language, although in some Andalusian and Caribbean dialects it is still aspirated in some words. Because of borrowings from Latin and from neighboring Romance languages, there are many -/-doublets in modern Spanish: and (both Spanish for "Ferdinand"), and (both Spanish for "smith"), and (both Spanish for "iron"), and and (both Spanish for "deep", but means "bottom" while means "deep"); (Spanish for "to make") is cognate to the root word of (Spanish for "to satisfy"), and ("made") is similarly cognate to the root word of (Spanish for "satisfied").

Compare the examples in the following table:

Some consonant clusters of Latin also produced characteristically different results in these languages, as shown in the examples in the following table:

In the 15th and 16th centuries, Spanish underwent a dramatic change in the pronunciation of its sibilant consonants, known in Spanish as the , which resulted in the distinctive velar pronunciation of the letter and—in a large part of Spain—the characteristic interdental ("th-sound") for the letter (and for before or ). See History of Spanish (Modern development of the Old Spanish sibilants) for details.

The , written in Salamanca in 1492 by Elio Antonio de Nebrija, was the first grammar written for a modern European language. According to a popular anecdote, when Nebrija presented it to Queen Isabella I, she asked him what was the use of such a work, and he answered that language is the instrument of empire. In his introduction to the grammar, dated 18 August 1492, Nebrija wrote that "... language was always the companion of empire."

From the sixteenth century onwards, the language was taken to the Spanish-discovered America and the Spanish East Indies via Spanish colonization of America. Miguel de Cervantes, author of "Don Quixote", is such a well-known reference in the world that Spanish is often called ("the language of Cervantes").

In the twentieth century, Spanish was introduced to Equatorial Guinea and the Western Sahara, and to areas of the United States that had not been part of the Spanish Empire, such as Spanish Harlem in New York City. For details on borrowed words and other external influences upon Spanish, see Influences on the Spanish language.

Most of the grammatical and typological features of Spanish are shared with the other Romance languages. Spanish is a fusional language. The noun and adjective systems exhibit two genders and two numbers. In addition, articles and some pronouns and determiners have a neuter gender in their singular form. There are about fifty conjugated forms per verb, with 3 tenses: past, present, future; 2 aspects for past: perfect, imperfect; 4 moods: indicative, subjunctive, conditional, imperative; 3 persons: first, second, third; 2 numbers: singular, plural; 3 verboid forms: infinitive, gerund, and past participle. The indicative mood expresses an event that has, is, or will occur (there is no uncertainty of the occurrence of said event); the subjunctive mood expresses an uncertainty in the occurrence of an event, and is commonly paired with the conditional, which is a mood used to express "would" (as in, "I would eat if I had food); the imperative is a mood to express a command, commonly a one word phrase -- "¡Di!", "Talk!".

Verbs express T-V distinction by using different persons for formal and informal addresses. (For a detailed overview of verbs, see Spanish verbs and Spanish irregular verbs.)

Spanish syntax is considered right-branching, meaning that subordinate or modifying constituents tend to be placed after their head words. The language uses prepositions (rather than postpositions or inflection of nouns for case), and usually—though not always—places adjectives after nouns, as do most other Romance languages.

The language is classified as a subject–verb–object language; however, as in most Romance languages, constituent order is highly variable and governed mainly by topicalization and focus rather than by syntax. It is a "pro-drop", or "null-subject" language—that is, it allows the deletion of subject pronouns when they are pragmatically unnecessary. Spanish is described as a "verb-framed" language, meaning that the "direction" of motion is expressed in the verb while the "mode" of locomotion is expressed adverbially (e.g. "subir corriendo" or "salir volando"; the respective English equivalents of these examples—'to run up' and 'to fly out'—show that English is, by contrast, "satellite-framed", with mode of locomotion expressed in the verb and direction in an adverbial modifier).

Subject/verb inversion is not required in questions, and thus the recognition of declarative or interrogative may depend entirely on intonation.

The Spanish phonemic system is originally descended from that of Vulgar Latin. Its development exhibits some traits in common with the neighboring dialects—especially Leonese and Aragonese—as well as other traits unique to Castilian. Castilian is unique among its neighbors in the aspiration and eventual loss of the Latin initial sound (e.g. Cast. vs. Leon. and Arag. ). The Latin initial consonant sequences , , and in Spanish typically become (originally pronounced ), while in Aragonese they are preserved, and in Leonese they present a variety of outcomes, including , , and . Where Latin had before a vowel (e.g. ) or the ending , (e.g. ), Old Spanish produced , that in Modern Spanish became the velar fricative (, , where neighboring languages have the palatal lateral (e.g. Portuguese , ; Catalan , ).

The Spanish phonemic inventory consists of five vowel phonemes (, , , , ) and 17 to 19 consonant phonemes (the exact number depending on the dialect). The main allophonic variation among vowels is the reduction of the high vowels and to glides— and respectively—when unstressed and adjacent to another vowel. Some instances of the mid vowels and , determined lexically, alternate with the diphthongs and respectively when stressed, in a process that is better described as morphophonemic rather than phonological, as it is not predictable from phonology alone.

The Spanish consonant system is characterized by (1) three nasal phonemes, and one or two (depending on the dialect) lateral phoneme(s), which in syllable-final position lose their contrast and are subject to assimilation to a following consonant; (2) three voiceless stops and the affricate ; (3) three or four (depending on the dialect) voiceless fricatives; (4) a set of voiced obstruents—, , , and sometimes —which alternate between approximant and plosive allophones depending on the environment; and (5) a phonemic distinction between the "tapped" and "trilled" "r"-sounds (single and double in orthography).

In the following table of consonant phonemes, is marked with an asterisk (*) to indicate that it is preserved only in some dialects. In most dialects it has been merged with in the merger called . Similarly, is also marked with an asterisk to indicate that most dialects do not distinguish it from (see ), although this is not a true merger but an outcome of different evolution of sibilants in Southern Spain.

The phoneme is in parentheses () to indicate that it appears only in loanwords. Each of the voiced obstruent phonemes , , , and appears to the right of a "pair" of voiceless phonemes, to indicate that, while the "voiceless" phonemes maintain a phonemic contrast between plosive (or affricate) and fricative, the "voiced" ones alternate allophonically (i.e. without phonemic contrast) between plosive and approximant pronunciations.

Spanish is classified by its rhythm as a syllable-timed language: each syllable has approximately the same duration regardless of stress.

Spanish intonation varies significantly according to dialect but generally conforms to a pattern of falling tone for declarative sentences and wh-questions (who, what, why, etc.) and rising tone for yes/no questions. There are no syntactic markers to distinguish between questions and statements and thus, the recognition of declarative or interrogative depends entirely on intonation.

Stress most often occurs on any of the last three syllables of a word, with some rare exceptions at the fourth-last or earlier syllables. The "tendencies" of stress assignment are as follows:

In addition to the many exceptions to these tendencies, there are numerous minimal pairs that contrast solely on stress such as ('sheet') and ('savannah'); ('boundary'), ('[that] he/she limits') and ('I limited'); ('liquid'), ('I sell off') and ('he/she sold off').

The orthographic system unambiguously reflects where the stress occurs: in the absence of an accent mark, the stress falls on the last syllable unless the last letter is , , or a vowel, in which cases the stress falls on the next-to-last (penultimate) syllable. Exceptions to those rules are indicated by an acute accent mark over the vowel of the stressed syllable. (See Spanish orthography.)

Spanish is the primary language of 20 countries worldwide. It is estimated that the combined total number of Spanish speakers is between 470 and 500 million, making it the second most widely spoken language in terms of native speakers.

Spanish is the third most spoken language by total number of speakers (after Mandarin and English). Internet usage statistics for 2007 also show Spanish as the third most commonly used language on the Internet, after English and Mandarin.

In Europe, Spanish is an official language of Spain, the country after which it is named and from which it originated. It is widely spoken in Gibraltar, and also commonly spoken in Andorra, although Catalan is the official language there.

Spanish is also spoken by small communities in other European countries, such as the United Kingdom, France, Italy, and Germany. Spanish is an official language of the European Union. In Switzerland, which had a massive influx of Spanish migrants in the 20th century, Spanish is the native language of 2.2% of the population.

Most Spanish speakers are in Hispanic America; of all countries with a majority of Spanish speakers, only Spain and Equatorial Guinea are outside the Americas. Nationally, Spanish is the official language—either "de facto" or "de jure"—of Argentina, Bolivia (co-official with Quechua, Aymara, Guarani, and 34 other languages), Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Honduras, Mexico (co-official with 63 indigenous languages), Nicaragua, Panama, Paraguay (co-official with Guaraní), Peru (co-official with Quechua, Aymara, and "the other indigenous languages"), Puerto Rico (co-official with English), Uruguay, and Venezuela.
Spanish has no official recognition in the former British colony of Belize; however, per the 2000 census, it is spoken by 43% of the population. Mainly, it is spoken by the descendants of Hispanics who have been in the region since the seventeenth century; however, English is the official language.

Due to their proximity to Spanish-speaking countries, Trinidad and Tobago and Brazil have implemented Spanish language teaching into their education systems. The Trinidad government launched the "Spanish as a First Foreign Language" (SAFFL) initiative in March 2005. In 2005, the National Congress of Brazil approved a bill, signed into law by the President, making it mandatory for schools to offer Spanish as an alternative foreign language course in both public and private secondary schools in Brazil. In September 2016 this law was revoked by Michel Temer after impeachment of Dilma Rousseff. In many border towns and villages along Paraguay and Uruguay, a mixed language known as Portuñol is spoken.

According to 2006 census data, 44.3 million people of the U.S. population were Hispanic or Hispanic American by origin; 38.3 million people, 13 percent of the population over five years old speak Spanish at home. The Spanish language has a long history of presence in the United States due to early Spanish and, later, Mexican administration over territories now forming the southwestern states, also Louisiana ruled by Spain from 1762 to 1802, as well as Florida, which was Spanish territory until 1821.

Spanish is by far the most common second language in the US, with over 50 million total speakers if non-native or second-language speakers are included. While English is the de facto national language of the country, Spanish is often used in public services and notices at the federal and state levels. Spanish is also used in administration in the state of New Mexico. The language also has a strong influence in major metropolitan areas such as those of Los Angeles, Miami, San Antonio, New York, San Francisco, Dallas, and Phoenix; as well as more recently, Chicago, Las Vegas, Boston, Denver, Houston, Indianapolis, Philadelphia, Cleveland, Salt Lake City, Atlanta, Nashville, Orlando, Tampa, Raleigh and Baltimore-Washington, D.C. due to 20th- and 21st-century immigration.

In Africa, Spanish is official (along with Portuguese and French) in Equatorial Guinea, as well as an official language of the African Union. In Equatorial Guinea, Spanish is the predominant language when native and non-native speakers (around 500,000 people) are counted, while Fang is the most spoken language by number of native speakers.

Spanish is also spoken in the integral territories of Spain in North Africa, which include the Spanish cities of Ceuta and Melilla, the Plazas de soberanía, and the Canary Islands archipelago (population 2,000,000), located some off the northwest coast of mainland Africa. In northern Morocco, a former Spanish protectorate that is also geographically close to Spain, approximately 20,000 people speak Spanish as a second language, while Arabic is the "de jure" official language. A small number of Moroccan Jews also speak the Sephardic Spanish dialect Haketia (related to the Ladino dialect spoken in Israel). Spanish is spoken by some small communities in Angola because of the Cuban influence from the Cold War and in South Sudan among South Sudanese natives that relocated to Cuba during the Sudanese wars and returned in time for their country's independence.

In Western Sahara, formerly Spanish Sahara, Spanish was officially spoken during the late nineteenth and twentieth centuries. Today, Spanish in this disputed territory is maintained by populations of Sahrawi nomads numbering about 500,000 people, and is de facto official alongside Arabic in the Sahrawi Arab Democratic Republic, although this entity receives limited international recognition.

Spanish and Philippine Spanish was an official language of the Philippines from the beginning of Spanish administration in 1565 to a constitutional change in 1973. During Spanish colonization (1565–1898), it was the language of government, trade and education, and spoken as a first language by Spaniards and educated Filipinos. In the mid-nineteenth century, the colonial government set up a free public education system with Spanish as the medium of instruction. This increased use of Spanish throughout the islands led to the formation of a class of Spanish-speaking intellectuals called the "Ilustrados". By the time of Philippine independence in 1898, around 70% of the population had knowledge of Spanish, with 10% speaking it as their first and only language and about 60% of the population spoke it as their second or third language.

Despite American administration after the defeat of Spain in the Spanish–American War in 1898, the usage of Spanish continued in Philippine literature and press during the early years of American administration. Gradually, however, the American government began increasingly promoting the use of English, and it characterized Spanish as a negative influence of the past. Eventually, by the 1920s, English became the primary language of administration and education. But despite a significant decrease in influence and speakers, Spanish remained an official language of the Philippines when it became independent in 1946, alongside English and Filipino, a standardized version of Tagalog.
Spanish was removed from official status in 1973 under the administration of Ferdinand Marcos, but regained its status as an official language two months later under Presidential Decree No. 155, dated 15 March 1973. It remained an official language until 1987, with the ratification of the present constitution, in which it was re-designated as a voluntary and optional auxiliary language. In 2010, President Gloria Macapagal-Arroyo encouraged the reintroduction of Spanish-language teaching in the Philippine education system. But by 2012, the number of secondary schools at which the language was either a compulsory subject or an elective had become very limited. Today, despite government promotions of Spanish, less than 0.5% of the population report being able to speak the language proficiently. Aside from standard Spanish, a Spanish-based creole language—Chavacano—developed in the southern Philippines. The number of Chavacano-speakers was estimated at 1.2 million in 1996. However, it is not mutually intelligible with Spanish. Speakers of the Zamboangueño variety of Chavacano were numbered about 360,000 in the 2000 census. The local languages of the Philippines also retain Spanish influence, with many words being derived from Mexican Spanish, owing to the administration of the islands by Spain through New Spain until 1821, and then directly from Madrid until 1898.

Philippine Spanish is a dialect of the Spanish language in the Philippines. The variant is very similar to Mexican Spanish, because of Mexican and Latin American emigration to the Spanish East Indies over the years. 
From 1565 to 1821, the Philippines, which were a part of the Spanish East Indies, were governed by the Captaincy General of the Philippines as a territory of the Viceroyalty of New Spain centered in Mexico. It was only administered directly from Spain in 1821 after Mexico gained its independence that same year. Since the Philippines was a former territory of the Viceroyalty of New Spain for most of the Spanish colonial period, Spanish as was spoken in the Philippines had a greater affinity to American Spanish rather than to Peninsular Spanish.

Chavacano or Chabacano [tʃaβaˈkano] is a group of Spanish-based creole language varieties spoken in the Philippines. The variety spoken in Zamboanga City, located in the southern Philippine island group of Mindanao, has the highest concentration of speakers. Other currently existing varieties are found in Cavite City and Ternate, located in the Cavite province on the island of Luzon.[4] Chavacano is the only Spanish-based creole in Asia.

Spanish is also the official language and the most spoken on Easter Island which is geographically part of Polynesia in Oceania and politically part of Chile. Easter Island's traditional language is Rapa Nui, an Eastern Polynesian language.

Spanish loan words are present in the local languages of Guam, Northern Mariana Islands, Palau, Marshall Islands and Micronesia, all of which formerly comprised the Spanish East Indies.

The following table shows the number of Spanish speakers in some 79 countries.
There are important variations (phonological, grammatical, and lexical) in the spoken Spanish of the various regions of Spain and throughout the Spanish-speaking areas of the Americas.

The variety with the most speakers is Mexican Spanish. It is spoken by more than twenty percent of the world's Spanish speakers (more than 112 million of the total of more than 500 million, according to the table above). One of its main features is the reduction or loss of unstressed vowels, mainly when they are in contact with the sound /s/.

In Spain, northern dialects are popularly thought of as closer to the standard, although positive attitudes toward southern dialects have increased significantly in the last 50 years. Even so, the speech of Madrid, which has typically southern features such as yeísmo and s-aspiration, is the standard variety for use on radio and television. However, the variety used in the media is that of Madrid's educated classes, where southern traits are less evident, in contrast with the variety spoken by working-class Madrid, where those traits are pervasive. The educated variety of Madrid is indicated by many as the one that has most influenced the written standard for Spanish.

The four main phonological divisions are based respectively on (1) the phoneme ("theta"), (2) the debuccalization of syllable-final , (3) the sound of the spelled , (4) and the phoneme ("turned "y""),

The main morphological variations between dialects of Spanish involve differing uses of pronouns, especially those of the second person and, to a lesser extent, the object pronouns of the third person.

Virtually all dialects of Spanish make the distinction between a formal and a familiar register in the second-person singular and thus have two different pronouns meaning "you": in the formal and either or in the familiar (and each of these three pronouns has its associated verb forms), with the choice of or varying from one dialect to another. The use of (and/or its verb forms) is called . In a few dialects, all three pronouns are used, with , , and denoting respectively formality, familiarity, and intimacy.

In , is the subject form (, "you say") and the form for the object of a preposition (, "I am going with you"), while the direct and indirect object forms, and the possessives, are the same as those associated with : ("You know your friends respect you").

The verb forms of "general voseo" are the same as those used with except in the present tense (indicative and imperative) verbs. The forms for generally can be derived from those of (the traditional second-person familiar "plural") by deleting the glide , or , where it appears in the ending: > ; > , () > (), () > () .
In Chilean on the other hand, almost all verb forms are distinct from their standard -forms.
The use of the pronoun with the verb forms of () is called "pronominal ". Conversely, the use of the verb forms of with the pronoun ( or ) is called "verbal ". 
In Chile, for example, "verbal voseo" is much more common than the actual use of the pronoun "vos", which is usually reserved for highly informal situations.

And in Central American , one can see even further distinction.
Although is not used in Spain, it occurs in many Spanish-speaking regions of the Americas as the primary spoken form of the second-person singular familiar pronoun, with wide differences in social consideration. Generally, it can be said that there are zones of exclusive use of (the use of ) in the following areas: almost all of Mexico, the West Indies, Panama, most of Colombia, Peru, Venezuela and coastal Ecuador.

Areas of generalized include Argentina, Nicaragua, eastern Bolivia, El Salvador, Guatemala, Honduras, Costa Rica, Paraguay, Uruguay and the Colombian departments of Antioquia, Caldas, Risaralda, Quindio and Valle del Cauca.

 functions as formal and informal second person plural in over 90% of the Spanish-speaking world, including all of Hispanic America, the Canary Islands, and some regions of Andalusia. In Seville, Huelva, Cadiz, and other parts of western Andalusia, the familiar form is constructed as , using the traditional second-person plural form of the verb. Most of Spain maintains the formal/familiar distinction with and respectively.

 is the usual second-person singular pronoun in a formal context, but it is used jointly with the third-person singular voice of the verb. It is used to convey respect toward someone who is a generation older or is of higher authority ("you, sir"/"you, ma'am"). It is also used in a "familiar" context by many speakers in Colombia and Costa Rica and in parts of Ecuador and Panama, to the exclusion of or . This usage is sometimes called in Spanish.

In Central America, especially in Honduras, is often used as a formal pronoun to convey respect between the members of a romantic couple. is also used that way between parents and children in the Andean regions of Ecuador, Colombia and Venezuela.

Most speakers use (and the prefers) the pronouns and for "direct" objects (masculine and feminine respectively, regardless of animacy, meaning "him", "her", or "it"), and for "indirect" objects (regardless of gender or animacy, meaning "to him", "to her", or "to it"). The usage is sometimes called "etymological", as these direct and indirect object pronouns are a continuation, respectively, of the accusative and dative pronouns of Latin, the ancestor language of Spanish.

Deviations from this norm (more common in Spain than in the Americas) are called "", "", or "", according to which respective pronoun, , , or , has expanded beyond the etymological usage ( as a direct object, or or as an indirect object).

Some words can be significantly different in different Hispanophone countries. Most Spanish speakers can recognize other Spanish forms even in places where they are not commonly used, but Spaniards generally do not recognize specifically American usages. For example, Spanish , and (respectively, 'butter', 'avocado', 'apricot') correspond to (word used for lard in Peninsular Spanish), , and , respectively, in Argentina, Chile (except ), Paraguay, Peru (except and ), and Uruguay.

Spanish is closely related to the other West Iberian Romance languages, including Asturian, Aragonese, Galician, Ladino, Leonese, Mirandese and Portuguese.

It is generally acknowledged that Portuguese and Spanish speakers can communicate in written form, with varying degrees of mutual intelligibility.
Mutual intelligibility of the "written" Spanish and Portuguese languages is remarkably high, and the difficulties of the spoken forms are based more on phonology than on grammatical and lexical dissimilarities. "Ethnologue" gives estimates of the lexical similarity between related languages in terms of precise percentages. For Spanish and Portuguese, that figure is 89%. Italian, on the other hand its phonology similar to Spanish, but has a lower lexical similarity of 82%. Mutual intelligibility between Spanish and French or between Spanish and Romanian is lower still, given lexical similarity ratings of 75% and 71% respectively. And comprehension of Spanish by French speakers who have not studied the language is much lower, at an estimated 45%. In general, thanks to the common features of the writing systems of the Romance languages, interlingual comprehension of the written word is greater than that of oral communication.

The following table compares the forms of some common words in several Romance languages:

Judaeo-Spanish, also known as Ladino, is a variety of Spanish which preserves many features of medieval Spanish and Portuguese and is spoken by descendants of the Sephardi Jews who were expelled from Spain in the 15th century. Conversely, in Portugal the vast majority of the Portuguese Jews converted and became 'New Christians'. Therefore, its relationship to Spanish is comparable with that of the Yiddish language to German. Ladino speakers today are almost exclusively Sephardi Jews, with family roots in Turkey, Greece, or the Balkans, and living mostly in Israel, Turkey, and the United States, with a few communities in Hispanic America. Judaeo-Spanish lacks the Native American vocabulary which was acquired by standard Spanish during the Spanish colonial period, and it retains many archaic features which have since been lost in standard Spanish. It contains, however, other vocabulary which is not found in standard Spanish, including vocabulary from Hebrew, French, Greek and Turkish, and other languages spoken where the Sephardim settled.

Judaeo-Spanish is in serious danger of extinction because many native speakers today are elderly as well as elderly "olim" (immigrants to Israel) who have not transmitted the language to their children or grandchildren. However, it is experiencing a minor revival among Sephardi communities, especially in music. In the case of the Latin American communities, the danger of extinction is also due to the risk of assimilation by modern Castilian.

A related dialect is Haketia, the Judaeo-Spanish of northern Morocco. This too tended to assimilate with modern Spanish, during the Spanish occupation of the region.

Spanish is written in the Latin script, with the addition of the character (, representing the phoneme , a letter distinct from , although typographically composed of an with a tilde). Formerly the digraphs (, representing the phoneme ) and (, representing the phoneme ), were also considered single letters. However, the digraph (, 'strong r', , 'double r', or simply ), which also represents a distinct phoneme , was not similarly regarded as a single letter. Since 1994 and have been treated as letter pairs for collation purposes, though they remained a part of the alphabet until 2010. Words with are now alphabetically sorted between those with and , instead of following as they used to. The situation is similar for .

Thus, the Spanish alphabet has the following 27 letters:

Since 2010, none of the digraphs () is considered a letter by the Spanish Royal Academy.

The letters and are used only in words and names coming from foreign languages (, etc.).

With the exclusion of a very small number of regional terms such as (see Toponymy of Mexico), pronunciation can be entirely determined from spelling. Under the orthographic conventions, a typical Spanish word is stressed on the syllable before the last if it ends with a vowel (not including ) or with a vowel followed by or an ; it is stressed on the last syllable otherwise. Exceptions to this rule are indicated by placing an acute accent on the stressed vowel.

The acute accent is used, in addition, to distinguish between certain homophones, especially when one of them is a stressed word and the other one is a clitic: compare ('the', masculine singular definite article) with ('he' or 'it'), or ('you', object pronoun) with ('tea'), (preposition 'of') versus ('give' [formal imperative/third-person present subjunctive]), and (reflexive pronoun) versus ('I know' or imperative 'be').

The interrogative pronouns (, , , , etc.) also receive accents in direct or indirect questions, and some demonstratives (, , , etc.) can be accented when used as pronouns. Accent marks used to be omitted on capital letters (a widespread practice in the days of typewriters and the early days of computers when only lowercase vowels were available with accents), although the advises against this and the orthographic conventions taught at schools enforce the use of the accent.

When is written between and a front vowel or , it indicates a "hard g" pronunciation. A diaeresis indicates that it is not silent as it normally would be (e.g., , 'stork', is pronounced ; if it were written *, it would be pronounced *).

Interrogative and exclamatory clauses are introduced with inverted question and exclamation marks ( and , respectively).

The Royal Spanish Academy (), founded in 1713, together with the 21 other national ones (see Association of Spanish Language Academies), exercises a standardizing influence through its publication of dictionaries and widely respected grammar and style guides.
Because of influence and for other sociohistorical reasons, a standardized form of the language (Standard Spanish) is widely acknowledged for use in literature, academic contexts and the media.

The Association of Spanish Language Academies (, or ) is the entity which regulates the Spanish language. It was created in Mexico in 1951 and represents the union of all the separate academies in the Spanish-speaking world. It comprises the academies of 23 countries, ordered by date of Academy foundation: Spain (1713), Colombia (1871), Ecuador (1874), Mexico (1875), El Salvador (1876), Venezuela (1883), Chile (1885), Peru (1887), Guatemala (1887), Costa Rica (1923), Philippines (1924), Panama (1926), Cuba (1926),
Paraguay (1927), Dominican Republic (1927), Bolivia (1927), Nicaragua (1928), Argentina (1931), Uruguay (1943), Honduras (1949), Puerto Rico (1955), United States (1973) and Equatorial Guinea (2016).
The (Cervantes Institute) is a worldwide nonprofit organization created by the Spanish government in 1991. This organization has branched out in over 20 different countries, with 75 centers devoted to the Spanish and Hispanic American cultures and Spanish language. The ultimate goals of the Institute are to promote universally the education, the study, and the use of Spanish as a second language, to support methods and activities that help the process of Spanish-language education, and to contribute to the advancement of the Spanish and Hispanic American cultures in non-Spanish-speaking countries. The institute's 2015 report "El español, una lengua viva" (Spanish, a living language) estimated that there were 559 million Spanish speakers worldwide. Its latest annual report "El español en el mundo 2018" (Spanish in the world 2018) counts 577 million Spanish speakers worldwide. Among the sources cited in the report is the U.S. Census Bureau, which estimates that the U.S. will have 138 million Spanish speakers by 2050, making it the biggest Spanish-speaking nation on earth, with Spanish the mother tongue of almost a third of its citizens.

Spanish is one of the official languages of the United Nations, the European Union, the World Trade Organization, the Organization of American States, the Organization of Ibero-American States, the African Union, the Union of South American Nations, the Antarctic Treaty Secretariat, the Latin Union, the Caricom, the North American Free Trade Agreement, the Inter-American Development Bank, and numerous other international organizations.










</doc>
<doc id="26826" url="https://en.wikipedia.org/wiki?curid=26826" title="Sodium">
Sodium

Sodium is a chemical element with the symbol Na (from Latin "natrium") and atomic number 11. It is a soft, silvery-white, highly reactive metal. Sodium is an alkali metal, being in group 1 of the periodic table. Its only stable isotope is Na. The free metal does not occur in nature, and must be prepared from compounds. Sodium is the sixth most abundant element in the Earth's crust and exists in numerous minerals such as feldspars, sodalite, and rock salt (NaCl). Many salts of sodium are highly water-soluble: sodium ions have been leached by the action of water from the Earth's minerals over eons, and thus sodium and chlorine are the most common dissolved elements by weight in the oceans.

Sodium was first isolated by Humphry Davy in 1807 by the electrolysis of sodium hydroxide. Among many other useful sodium compounds, sodium hydroxide (lye) is used in soap manufacture, and sodium chloride (edible salt) is a de-icing agent and a nutrient for animals including humans.

Sodium is an essential element for all animals and some plants. Sodium ions are the major cation in the extracellular fluid (ECF) and as such are the major contributor to the ECF osmotic pressure and ECF compartment volume. Loss of water from the ECF compartment increases the sodium concentration, a condition called hypernatremia. Isotonic loss of water and sodium from the ECF compartment decreases the size of that compartment in a condition called ECF hypovolemia.

By means of the sodium-potassium pump, living human cells pump three sodium ions out of the cell in exchange for two potassium ions pumped in; comparing ion concentrations across the cell membrane, inside to outside, potassium measures about 40:1, and sodium, about 1:10. In nerve cells, the electrical charge across the cell membrane enables transmission of the nerve impulse—an action potential—when the charge is dissipated; sodium plays a key role in that activity.

Sodium at standard temperature and pressure is a soft silvery metal that combines with oxygen in the air and forms grayish white sodium oxide unless immersed in oil or inert gas, which are the conditions it is usually stored in. Sodium metal can be easily cut with a knife and is a good conductor of electricity and heat because it has only one electron in its valence shell, resulting in weak metallic bonding and free electrons, which carry energy. Due to having low atomic mass and large atomic radius, sodium is third-least dense of all elemental metals and is one of only three metals that can float on water, the other two being lithium and potassium. The melting (98 °C) and boiling (883 °C) points of sodium are lower than those of lithium but higher than those of the heavier alkali metals potassium, rubidium, and caesium, following periodic trends down the group. These properties change dramatically at elevated pressures: at 1.5 Mbar, the color changes from silvery metallic to black; at 1.9 Mbar the material becomes transparent with a red color; and at 3 Mbar, sodium is a clear and transparent solid. All of these high-pressure allotropes are insulators and electrides.
In a flame test, sodium and its compounds glow yellow because the excited 3s electrons of sodium emit a photon when they fall from 3p to 3s; the wavelength of this photon corresponds to the D line at about 589.3 nm. Spin-orbit interactions involving the electron in the 3p orbital split the D line into two, at 589.0 and 589.6 nm; hyperfine structures involving both orbitals cause many more lines.

Twenty isotopes of sodium are known, but only Na is stable. Na is created in the carbon-burning process in stars by fusing two carbon atoms together; this requires temperatures above 600 megakelvins and a star of at least three solar masses. Two radioactive, cosmogenic isotopes are the byproduct of cosmic ray spallation: Na has a half-life of 2.6 years and Na, a half-life of 15 hours; all other isotopes have a half-life of less than one minute. Two nuclear isomers have been discovered, the longer-lived one being Na with a half-life of around 20.2 milliseconds. Acute neutron radiation, as from a nuclear criticality accident, converts some of the stable Na in human blood to Na; the neutron radiation dosage of a victim can be calculated by measuring the concentration of Na relative to Na.

Sodium atoms have 11 electrons, one more than the stable configuration of the noble gas neon. The first and second ionization energies are 495.8 kJ/mol and 4562 kJ/mol), respectively. As a result, sodium usually forms ionic compounds involving the Na cation.

Metallic sodium is generally less reactive than potassium and more reactive than lithium. Sodium metal is highly reducing, with the standard reduction potential for the Na/Na couple being −2.71 volts, though potassium and lithium have even more negative potentials.

Sodium compounds are of immense commercial importance, being particularly central to industries producing glass, paper, soap, and textiles. The most important sodium compounds are table salt (NaCl), soda ash (NaCO), baking soda (NaHCO), caustic soda (NaOH), sodium nitrate (NaNO), di- and tri-sodium phosphates, sodium thiosulfate (NaSO·5HO), and borax (NaBO·10HO). In compounds, sodium is usually ionically bonded to water and anions and is viewed as a hard Lewis acid.
Most soaps are sodium salts of fatty acids. Sodium soaps have a higher melting temperature (and seem "harder") than potassium soaps.

Like all the alkali metals, sodium reacts exothermically with water. The reaction produces caustic soda (sodium hydroxide) and flammable hydrogen gas. When burned in air, it forms primarily sodium peroxide with some sodium oxide.

Sodium tends to form water-soluble compounds, such as halides, sulfates, nitrates, carboxylates and carbonates. The main aqueous species are the aquo complexes [Na(HO)], where "n" = 4–8; with "n" = 6 indicated from X-ray diffraction data and computer simulations.

Direct precipitation of sodium salts from aqueous solutions is rare because sodium salts typically have a high affinity for water. An exception is sodium bismuthate (NaBiO). Because of the high solubility of its compounds, sodium salts are usually isolated as solids by evaporation or by precipitation with an organic antisolvent, such as ethanol; for example, only 0.35 g/L of sodium chloride will dissolve in ethanol. Crown ethers, like 15-crown-5, may be used as a phase-transfer catalyst.

Sodium content of samples is determined by atomic absorption spectrophotometry or by potentiometry using ion-selective electrodes.

Like the other alkali metals, sodium dissolves in ammonia and some amines to give deeply colored solutions; evaporation of these solutions leaves a shiny film of metallic sodium. The solutions contain the coordination complex (Na(NH)), with the positive charge counterbalanced by electrons as anions; cryptands permit the isolation of these complexes as crystalline solids. Sodium forms complexes with crown ethers, cryptands and other ligands. For example, 15-crown-5 has a high affinity for sodium because the cavity size of 15-crown-5 is 1.7–2.2 Å, which is enough to fit the sodium ion (1.9 Å). Cryptands, like crown ethers and other ionophores, also have a high affinity for the sodium ion; derivatives of the alkalide Na are obtainable by the addition of cryptands to solutions of sodium in ammonia via disproportionation.

Many organosodium compounds have been prepared. Because of the high polarity of the C-Na bonds, they behave like sources of carbanions (salts with organic anions). Some well-known derivatives include sodium cyclopentadienide (NaCH) and trityl sodium ((CH)CNa). Sodium naphthalenide, Na[CH•], a strong reducing agent, forms upon mixing Na and naphthalene in ethereal solutions.

Sodium forms alloys with many metals, such as potassium, calcium, lead, and the group 11 and 12 elements. Sodium and potassium form KNa and NaK. NaK is 40–90% potassium and it is liquid at ambient temperature. It is an excellent thermal and electrical conductor. Sodium-calcium alloys are by-products of the electrolytic production of sodium from a binary salt mixture of NaCl-CaCl and ternary mixture NaCl-CaCl-BaCl. Calcium is only partially miscible with sodium. In a liquid state, sodium is completely miscible with lead. There are several methods to make sodium-lead alloys. One is to melt them together and another is to deposit sodium electrolytically on molten lead cathodes. NaPb, NaPb, NaPb, NaPb, and NaPb are some of the known sodium-lead alloys. Sodium also forms alloys with gold (NaAu) and silver (NaAg). Group 12 metals (zinc, cadmium and mercury) are known to make alloys with sodium. NaZn and NaCd are alloys of zinc and cadmium. Sodium and mercury form NaHg, NaHg, NaHg, NaHg, and NaHg.

Because of its importance in human health, salt has long been an important commodity, as shown by the English word "salary", which derives from "salarium", the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe, a compound of sodium with the Latin name of "sodanum" was used as a headache remedy. The name sodium is thought to originate from the Arabic "suda", meaning headache, as the headache-alleviating properties of sodium carbonate or soda were well known in early times. Although sodium, sometimes called "soda", had long been recognized in compounds, the metal itself was not isolated until 1807 by Sir Humphry Davy through the electrolysis of sodium hydroxide. In 1809, the German physicist and chemist Ludwig Wilhelm Gilbert proposed the names "Natronium" for Humphry Davy's "sodium" and "Kalium" for Davy's "potassium". The chemical abbreviation for sodium was first published in 1814 by Jöns Jakob Berzelius in his system of atomic symbols, and is an abbreviation of the element's New Latin name "natrium", which refers to the Egyptian "natron", a natural mineral salt mainly consisting of hydrated sodium carbonate. Natron historically had several important industrial and household uses, later eclipsed by other sodium compounds.

Sodium imparts an intense yellow color to flames. As early as 1860, Kirchhoff and Bunsen noted the high sensitivity of a sodium flame test, and stated in Annalen der Physik und Chemie:

In a corner of our 60 m room farthest away from the apparatus, we exploded 3 mg of sodium chlorate with milk sugar while observing the nonluminous flame before the slit. After a while, it glowed a bright yellow and showed a strong sodium line that disappeared only after 10 minutes. From the weight of the sodium salt and the volume of air in the room, we easily calculate that one part by weight of air could not contain more than 1/20 millionth weight of sodium.

The Earth's crust contains 2.27% sodium, making it the seventh most abundant element on Earth and the fifth most abundant metal, behind aluminium, iron, calcium, and magnesium and ahead of potassium. Sodium's estimated oceanic abundance is 1.08 milligrams per liter. Because of its high reactivity, it is never found as a pure element. It is found in many minerals, some very soluble, such as halite and natron, others much less soluble, such as amphibole and zeolite. The insolubility of certain sodium minerals such as cryolite and feldspar arises from their polymeric anions, which in the case of feldspar is a polysilicate.

Atomic sodium has a very strong spectral line in the yellow-orange part of the spectrum (the same line as is used in sodium vapour street lights). This appears as an absorption line in many types of stars, including the Sun. The line was first studied in 1814 by Joseph von Fraunhofer during his investigation of the lines in the solar spectrum, now known as the Fraunhofer lines. Fraunhofer named it the 'D' line, although it is now known to actually be a group of closely spaced lines split by a fine and hyperfine structure.

The strength of the D line means it has been detected in many other astronomical environments. In stars, it is seen in any whose surfaces are cool enough for sodium to exist in atomic form (rather than ionised). This corresponds to stars of roughly F-type and cooler. Many other stars appear to have a sodium absorption line, but this is actually caused by gas in the foreground interstellar medium. The two can be distinguished via high-resolution spectroscopy, because interstellar lines are much narrower than those broadened by stellar rotation.

Sodium has also been detected in numerous Solar System environments, including Mercury's atmosphere, the exosphere of the Moon, and numerous other bodies. Some comets have a sodium tail, which was first detected in observations of Comet Hale-Bopp in 1997. Sodium has even been detected in the atmospheres of some extrasolar planets via transit spectroscopy.

Employed only in rather specialized applications, only about 100,000 tonnes of metallic sodium are produced annually. Metallic sodium was first produced commercially in the late 19th century by carbothermal reduction of sodium carbonate at 1100 °C, as the first step of the Deville process for the production of aluminium:

The high demand for aluminium created the need for the production of sodium. The introduction of the Hall–Héroult process for the production of aluminium by electrolysing a molten salt bath ended the need for large quantities of sodium. A related process based on the reduction of sodium hydroxide was developed in 1886.

Sodium is now produced commercially through the electrolysis of molten sodium chloride, based on a process patented in 1924. This is done in a Downs cell in which the NaCl is mixed with calcium chloride to lower the melting point below 700 °C. As calcium is less electropositive than sodium, no calcium will be deposited at the cathode. This method is less expensive than the previous Castner process (the electrolysis of sodium hydroxide).

The market for sodium is volatile due to the difficulty in its storage and shipping; it must be stored under a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of sodium oxide or sodium superoxide.

Though metallic sodium has some important uses, the major applications for sodium use compounds; millions of tons of sodium chloride, hydroxide, and carbonate are produced annually. Sodium chloride is extensively used for anti-icing and de-icing and as a preservative; examples of the uses of sodium bicarbonate include baking, as a raising agent, and sodablasting. Along with potassium, many important medicines have sodium added to improve their bioavailability; though potassium is the better ion in most cases, sodium is chosen for its lower price and atomic weight. Sodium hydride is used as a base for various reactions (such as the aldol reaction) in organic chemistry, and as a reducing agent in inorganic chemistry.

Metallic sodium is used mainly for the production of sodium borohydride, sodium azide, indigo, and triphenylphosphine. A once-common use was the making of tetraethyllead and titanium metal; because of the move away from TEL and new titanium production methods, the production of sodium declined after 1970. Sodium is also used as an alloying metal, an anti-scaling agent, and as a reducing agent for metals when other materials are ineffective. Note the free element is not used as a scaling agent, ions in the water are exchanged for sodium ions. Sodium plasma ("vapor") lamps are often used for street lighting in cities, shedding light that ranges from yellow-orange to peach as the pressure increases. By itself or with potassium, sodium is a desiccant; it gives an intense blue coloration with benzophenone when the desiccate is dry. In organic synthesis, sodium is used in various reactions such as the Birch reduction, and the sodium fusion test is conducted to qualitatively analyse compounds. Sodium reacts with alcohol and gives alkoxides, and when sodium is dissolved in ammonia solution, it can be used to reduce alkynes to trans-alkenes. Lasers emitting light at the sodium D line are used to create artificial laser guide stars that assist in the adaptive optics for land-based visible-light telescopes.

Liquid sodium is used as a heat transfer fluid in some types of nuclear reactors because it has the high thermal conductivity and low neutron absorption cross section required to achieve a high neutron flux in the reactor. The high boiling point of sodium allows the reactor to operate at ambient (normal) pressure, but the drawbacks include its opacity, which hinders visual maintenance, and its explosive properties. Radioactive sodium-24 may be produced by neutron bombardment during operation, posing a slight radiation hazard; the radioactivity stops within a few days after removal from the reactor. If a reactor needs to be shut down frequently, NaK is used; because NaK is a liquid at room temperature, the coolant does not solidify in the pipes. In this case, the pyrophoricity of potassium requires extra precautions to prevent and detect leaks. Another heat transfer application is poppet valves in high-performance internal combustion engines; the valve stems are partially filled with sodium and work as a heat pipe to cool the valves.

In humans, sodium is an essential mineral that regulates blood volume, blood pressure, osmotic equilibrium and pH. The minimum physiological requirement for sodium is estimated to range from about 120 milligrams per day in newborns to 500 milligrams per day over the age of 10.

Sodium chloride is the principal source of sodium in the diet, and is used as seasoning and preservative in such commodities as pickled preserves and jerky; for Americans, most sodium chloride comes from processed foods. Other sources of sodium are its natural occurrence in food and such food additives as monosodium glutamate (MSG), sodium nitrite, sodium saccharin, baking soda (sodium bicarbonate), and sodium benzoate.

The U.S. Institute of Medicine set its Tolerable Upper Intake Level for sodium at 2.3 grams per day, but the average person in the United States consumes 3.4 grams per day. The American Heart Association recommends no more than 1.5 g of sodium per day.

Studies have found that lowering sodium intake by 2 g per day tends to lower systolic blood pressure by about two to four mm Hg. It has been estimated that such a decrease in sodium intake would lead to between 9 and 17% fewer cases of hypertension.

Hypertension causes 7.6 million premature deaths worldwide each year. (Note that salt contains about 39.3% sodiumthe rest being chlorine and trace chemicals; thus, 2.3 g sodium is about 5.9 g, or 5.3 ml, of saltabout one US teaspoon.)

One study found that people with or without hypertension who excreted less than 3 grams of sodium per day in their urine (and therefore were taking in less than 3 g/d) had a "higher" risk of death, stroke, or heart attack than those excreting 4 to 5 grams per day. Levels of 7 g per day or more in people with hypertension were associated with higher mortality and cardiovascular events, but this was not found to be true for people without hypertension. The US FDA states that adults with hypertension and prehypertension should reduce daily intake to 1.5 g.

The renin–angiotensin system regulates the amount of fluid and sodium concentration in the body. Reduction of blood pressure and sodium concentration in the kidney result in the production of renin, which in turn produces aldosterone and angiotensin, which stimulates the reabsorption of sodium back into the bloodstream. When the concentration of sodium increases, the production of renin decreases, and the sodium concentration returns to normal. The sodium ion (Na) is an important electrolyte in neuron function, and in osmoregulation between cells and the extracellular fluid. This is accomplished in all animals by Na/K-ATPase, an active transporter pumping ions against the gradient, and sodium/potassium channels. Sodium is the most prevalent metallic ion in extracellular fluid.

Unusually low or high sodium levels in humans are recognized in medicine as hyponatremia and hypernatremia. These conditions may be caused by genetic factors, ageing, or prolonged vomiting or diarrhea.

In C4 plants, sodium is a micronutrient that aids metabolism, specifically in regeneration of phosphoenolpyruvate and synthesis of chlorophyll. In others, it substitutes for potassium in several roles, such as maintaining turgor pressure and aiding in the opening and closing of stomata. Excess sodium in the soil can limit the uptake of water by decreasing the water potential, which may result in plant wilting; excess concentrations in the cytoplasm can lead to enzyme inhibition, which in turn causes necrosis and chlorosis. In response, some plants have developed mechanisms to limit sodium uptake in the roots, to store it in cell vacuoles, and restrict salt transport from roots to leaves; excess sodium may also be stored in old plant tissue, limiting the damage to new growth. Halophytes have adapted to be able to flourish in sodium rich environments.

Sodium forms flammable hydrogen and caustic sodium hydroxide on contact with water; ingestion and contact with moisture on skin, eyes or mucous membranes can cause severe burns. Sodium spontaneously explodes in the presence of water due to the formation of hydrogen (highly explosive) and sodium hydroxide (which dissolves in the water, liberating more surface). However, sodium exposed to air and ignited or reaching autoignition (reported to occur when a molten pool of sodium reaches about 290 °C) displays a relatively mild fire. In the case of massive (non-molten) pieces of sodium, the reaction with oxygen eventually becomes slow due to formation of a protective layer. Fire extinguishers based on water accelerate sodium fires; those based on carbon dioxide and bromochlorodifluoromethane should not be used on sodium fire. Metal fires are Class D, but not all Class D extinguishers are workable with sodium. An effective extinguishing agent for sodium fires is Met-L-X. Other effective agents include Lith-X, which has graphite powder and an organophosphate flame retardant, and dry sand. Sodium fires are prevented in nuclear reactors by isolating sodium from oxygen by surrounding sodium pipes with inert gas. Pool-type sodium fires are prevented using diverse design measures called catch pan systems. They collect leaking sodium into a leak-recovery tank where it is isolated from oxygen.



</doc>
<doc id="26828" url="https://en.wikipedia.org/wiki?curid=26828" title="Suriname">
Suriname

Suriname (, , sometimes spelled Surinam), officially known as the Republic of Suriname ( ), is a country on the northeastern Atlantic coast of South America. It is bordered by the Atlantic Ocean to the north, French Guiana to the east, Guyana to the west and Brazil to the south. At just under , it is the smallest sovereign state in South America. Suriname has a population of approximately , most of whom live on the country's north coast, in and around the capital and largest city, Paramaribo.

Situated slightly north of the Equator, Suriname is a tropical country dominated by rain forests. Its extensive tree cover is vital to the country's efforts to mitigate climate change and reach carbon neutrality. A developing country with a high level of human development, Suriname's economy is heavily dependent on its abundant natural resources, namely bauxite, gold, petroleum and agricultural products.

Suriname was inhabited as early as the fourth millennium BC by various indigenous peoples, including the Arawaks, Caribs, and Wayana. Europeans arrived in the 16th century, with the Dutch establishing control over much of the country's current territory by the late 17th century. During the Dutch colonial period, Suriname was a lucrative source of sugar, its plantation economy driven by African slave labor and, after abolition of slavery in 1863, indentured servants from Asia. In 1954, Suriname became one of the constituent countries of the Kingdom of the Netherlands. On 25 November 1975, Suriname left the Kingdom to become an independent state, nonetheless maintaining close economic, diplomatic, and cultural ties to its former colonizer.

Suriname is considered to be a culturally Caribbean country, and is a member of the Caribbean Community (CARICOM). Suriname is the only sovereign nation outside Europe where Dutch is the official and prevailing language of government, business, media, and education. Sranan Tongo, an English-based creole language, is a widely used "lingua franca". As a legacy of centuries of colonialism, the people of Suriname are among the most diverse in the world, spanning a multitude of ethnic, religious, and linguistic groups.

The name "Suriname" may derive from an indigenous people called "Surinen," who inhabited the area at the time of European contact. It may also be derived from a corruption of the name ""Surryham"" which was the name given to the Suriname River by Lord Willoughby in honour of the Earl of Surrey when an English colony was established under a grant from King Charles II.

British settlers, who founded the first European colony at Marshall's Creek along the Suriname River, spelled the name as "Surinam".

When the territory was taken over by the Dutch, it became part of a group of colonies known as Dutch Guiana. The official spelling of the country's English name was changed from "Surinam" to "Suriname" in January 1978, but "Surinam" can still be found in English; a notable example is Suriname's national airline, Surinam Airways. The older English name is reflected in the English pronunciation, . In Dutch, the official language of Suriname, the pronunciation is , with the main stress on the third syllable and a schwa terminal vowel.

Indigenous settlement of Suriname dates back to 3,000 BC. The largest tribes were the Arawak, a nomadic coastal tribe that lived from hunting and fishing. They were the first inhabitants in the area. The Carib also settled in the area and conquered the Arawak by using their superior sailing ships. They settled in Galibi ("Kupali Yumï," meaning "tree of the forefathers") at the mouth of the Marowijne River. While the larger Arawak and Carib tribes lived along the coast and savanna, smaller groups of indigenous people lived in the inland rainforest, such as the Akurio, Trió, Warrau, and Wayana.

Beginning in the 16th century, French, Spanish and English explorers visited the area. A century later, Dutch and English settlers established plantation colonies along the many rivers in the fertile Guiana plains. The earliest documented colony in Guiana was an English settlement named Marshall's Creek along the Suriname River. After that there was another short-lived English colony called Willoughbyland that lasted from 1650 to 1674.

Disputes arose between the Dutch and the English for control of this territory. In 1667, during negotiations leading to the Treaty of Breda, the Dutch decided to keep the nascent plantation colony of Suriname they had gained from the English. The English were able to keep New Amsterdam, the main city of the former colony of New Netherland in North America on the mid-Atlantic coast. Already a cultural and economic hub in those days, they renamed it after the Duke of York: New York City.

In 1683, the Society of Suriname was founded by the city of Amsterdam, the Van Aerssen van Sommelsdijck family, and the Dutch West India Company. The society was chartered to manage and defend the colony. The planters of the colony relied heavily on African slaves to cultivate, harvest and process the commodity crops of coffee, cocoa, sugar cane and cotton plantations along the rivers. Planters' treatment of the slaves was notoriously brutal—historian C. R. Boxer wrote that "man's inhumanity to man just about reached its limits in Surinam"—and many slaves escaped the plantations. In November 1795, the Society was nationalized by the Batavian Republic and from then on, the Batavian Republic and its legal successors (the Kingdom of Holland and the Kingdom of the Netherlands) governed the territory as a national colony, barring a period of British occupation between 1799 and 1802, and between 1804 and 1816.

With the help of the native South Americans living in the adjoining rain forests, these runaway slaves established a new and unique culture in the interior that was highly successful in its own right. They were known collectively in English as Maroons, in French as "Nèg'Marrons" (literally meaning "brown negroes", that is "pale-skinned negroes"), and in Dutch as "Marrons." The Maroons gradually developed several independent tribes through a process of ethnogenesis, as they were made up of slaves from different African ethnicities. These tribes include the Saramaka, Paramaka, Ndyuka or Aukan, Kwinti, Aluku or Boni, and Matawai.
The Maroons often raided plantations to recruit new members from the slaves and capture women, as well as to acquire weapons, food and supplies. They sometimes killed planters and their families in the raids; colonists built defenses, which were so important they were shown on 18th-century maps.

The colonists also mounted armed campaigns against the Maroons, who generally escaped through the rain forest, which they knew much better than did the colonists. To end hostilities, in the 18th century the European colonial authorities signed several peace treaties with different tribes. They granted the Maroons sovereign status and trade rights in their inland territories, giving them autonomy.

From 1861 to 1863, with the American Civil War underway, and enslaved people escaping to Southern territory controlled by the Union, United States President Abraham Lincoln and his administration looked abroad for places to relocate people who were freed from enslavement and who wanted to leave the United States. It opened negotiations with the Dutch government regarding African-American emigration to and colonization of the Dutch colony of Suriname. Nothing came of the idea, and the idea was dropped after 1864.

The Netherlands abolished slavery in Suriname in 1863, under a gradual process that required enslaved people to work on plantations for 10 transition years for minimal pay, which was considered as partial compensation for their masters. After 1873, most freedmen largely abandoned the plantations where they had worked for several generations in favor of the capital city, Paramaribo. Some of them bought the plantation they worked on, especially in the district of Para and Coronie. Their descendants still live on those grounds today. Several plantation owners did not pay their former enslaved workers in the ten years after 1863. They payed the workers with the owning rights of the ground of the plantation to get out of debt.
As a plantation colony, Suriname had an economy dependent on labor-intensive commodity crops. To make up for a shortage of labor, the Dutch recruited and transported contract or indentured laborer from the Dutch East Indies (modern Indonesia) and India (the latter through an arrangement with the British, who then ruled the area). In addition, during the late 19th and early 20th centuries, small numbers of laborers, mostly men, were recruited from China and the Middle East.

Although Suriname's population remains relatively small, because of this complex colonization and exploitation, it is one of the most ethnically and culturally diverse countries in the world.

During World War II, on 23 November 1941, under an agreement with the Netherlands government-in-exile, the United States occupied Suriname to protect the bauxite mines to support the Allies' war effort. In 1942, the Dutch government-in-exile began to review the relations between the Netherlands and its colonies in terms of the post-war period.

In 1954, Suriname became one of the constituent countries of the Kingdom of the Netherlands, along with the Netherlands Antilles and the Netherlands. In this construction, the Netherlands retained control of its defense and foreign affairs. In 1974, the local government, led by the National Party of Suriname (NPS) (whose membership was largely Creole, meaning ethnically African or mixed African-European) started negotiations with the Dutch government leading towards full independence, which was granted on 25 November 1975. A large part of Suriname's economy for the first decade following independence was fueled by foreign aid provided by the Dutch government.

The first President of the country was Johan Ferrier, the former governor, with Henck Arron (the then leader of the NPS) as Prime Minister. In the years leading up to independence, nearly one-third of the population of Suriname emigrated to the Netherlands, amidst concern that the new country would fare worse under independence than it had as a constituent country of the Kingdom of the Netherlands. Surinamese politics did degenerate into ethnic polarisation and corruption soon after independence, with the NPS using Dutch aid money for partisan purposes. Its leaders were accused of fraud in the 1977 elections, in which Arron won a further term, and the discontent was such that a large portion of the population fled to the Netherlands, joining the already significant Surinamese community there.

On 25 February 1980, a military coup overthrew Arron's government. It was initiated by a group of 16 sergeants, led by Dési Bouterse. Opponents of the military regime attempted counter-coups in April 1980, August 1980, 15 March 1981, and again on 12 March 1982. The first counter attempt was led by Fred Ormskerk, the second by Marxist-Leninists, the third by Wilfred Hawker, and the fourth by Surendre Rambocus.

Hawker escaped from prison during the fourth counter-coup attempt, but he was captured and summarily executed. Between 2 am and 5 am on 7 December 1982, the military, under the leadership of Dési Bouterse, rounded up 13 prominent citizens who had criticized the military dictatorship and held them at Fort Zeelandia in Paramaribo. The dictatorship had all these men executed over the next three days, along with Rambocus and Jiwansingh Sheombar (who was also involved in the fourth counter-coup attempt).

National elections were held in 1987. The National Assembly adopted a new constitution that allowed Bouterse to remain in charge of the army. Dissatisfied with the government, Bouterse summarily dismissed the ministers in 1990, by telephone. This event became popularly known as the "Telephone Coup". His power began to wane after the 1991 elections.

The brutal civil war between the Suriname army and Maroons loyal to rebel leader Ronnie Brunswijk, begun in 1986, continued and its effects further weakened Bouterse's position during the 1990s. Due to the civil war, more than 10,000 Surinamese, mostly Maroons, fled to French Guiana in the late 1980s.

In 1999, the Netherlands tried Bouterse "in absentia" on drug smuggling charges. He was convicted and sentenced to prison but remained in Suriname.

On 19 July 2010, the former dictator Dési Bouterse returned to power when he was elected as the new President of Suriname. Before his election in 2010, he, along with 24 others, had been charged with the murders of 15 prominent dissidents in the December murders. However, in 2012, two months before the verdict in the trial, the National Assembly extended its amnesty law and provided Bouterse and the others with amnesty of these charges. He was reelected on 14 July 2015. However, Bouterse was convicted by a Surinamese court on 29 November 2019 and given a 20-year sentence for his role in the 1982 killings.

After winning the 2020 elections, Chan Santokhi was the sole nomination for President of Suriname. On 13 July, Santokhi was elected president by acclamation in an uncontested election. He was inaugurated on 16 July in ceremony without public due to the COVID-19 pandemic.

The Republic of Suriname is a representative democratic republic, based on the Constitution of 1987. The legislative branch of government consists of a 51-member unicameral National Assembly, simultaneously and popularly elected for a five-year term.

In the elections held on Tuesday, 25 May 2010, the "Megacombinatie" won 23 of the National Assembly seats followed by "Nationale Front" with 20 seats. A much smaller number, important for coalition-building, went to the "A‑combinatie" and to the "Volksalliantie." The parties held negotiations to form coalitions. Elections were held on 25 May 2015, and the National Assembly again elected Desire Bouterse as president.

The President of Suriname is elected for a five-year term by a two-thirds majority of the National Assembly. If at least two-thirds of the National Assembly cannot agree to vote for one presidential candidate, a People's Assembly is formed from all National Assembly delegates and regional and municipal representatives who were elected by popular vote in the most recent national election. The president may be elected by a majority of the People's Assembly called for the special election.

As head of government, the president appoints a sixteen-minister cabinet. A vice president, is normally elected for a five-year term at the same time as the president, by a simple majority in the National Assembly or People's Assembly. There is no constitutional provision for removal or replacement of the president, except in the case of resignation.

The judiciary is headed by the High Court of Justice of Suriname (Supreme Court). This court supervises the magistrate courts. Members are appointed for life by the president in consultation with the National Assembly, the State Advisory Council, and the National Order of Private Attorneys.

President Dési Bouterse was convicted and sentenced in the Netherlands to 11 years of imprisonment for drug trafficking. He is the main suspect in the court case concerning the 'December murders,' the 1982 assassination of opponents of military rule in Fort Zeelandia, Paramaribo. These two cases still strain relations between the Netherlands and Suriname.

Due to Suriname's Dutch colonial history, Suriname had a long-standing special relationship with the Netherlands. The Dutch government has stated that it will only maintain limited contact with the president.

Bouterse was elected as president of Suriname in 2010. The Netherlands in July 2014 dropped Suriname as a member of its development program.

Since 1991, the United States has maintained positive relations with Suriname. The two countries work together through the Caribbean Basin Security Initiative (CBSI) and the U.S. President's Emergency Plan for AIDS Relief (PEPFAR). Suriname also receives military funding from the U.S. Department of Defense.

European Union relations and cooperation with Suriname are carried out both on a bilateral and a regional basis. There are ongoing EU-Community of Latin American and Caribbean States (CELAC) and EU-CARIFORUM dialogues. Suriname is party to the Cotonou Agreement, the partnership agreement among the members of the African, Caribbean and Pacific Group of States and the European Union.

On 17 February 2005, the leaders of Barbados and Suriname signed the "Agreement for the deepening of bilateral cooperation between the Government of Barbados and the Government of the Republic of Suriname." On 23–24 April 2009, both nations formed a Joint Commission in Paramaribo, Suriname, to improve relations and to expand into various areas of cooperation. They held a second meeting toward this goal on 3–4 March 2011, in Dover, Barbados. Their representatives reviewed issues of agriculture, trade, investment, as well as international transport.

In the late 2000s, Suriname intensified development cooperation with other developing countries. China's South-South cooperation with Suriname has included a number of large-scale infrastructure projects, including port rehabilitation and road construction. Brazil signed agreements to cooperate with Suriname in education, health, agriculture, and energy production.

The Armed Forces of Suriname have three branches: the Army, the Air Force, and the Navy. The President of the Republic, Chan Santokhi, is the Supreme Commander-in-Chief of the Armed Forces ("Opperbevelhebber van de Strijdkrachten"). The President is assisted by the Minister of Defence. Beneath the President and Minister of Defence is the Commander of the Armed Forces ("Bevelhebber van de Strijdkrachten"). The Military Branches and regional Military Commands report to the Commander.

After the creation of the Statute of the Kingdom of the Netherlands, the Royal Netherlands Army was entrusted with the defense of Suriname, while the defense of the Netherlands Antilles was the responsibility of the Royal Netherlands Navy. The army set up a separate "Troepenmacht in Suriname" (Forces in Suriname, TRIS). Upon independence in 1975, this force was turned into the "Surinaamse Krijgsmacht" (SKM):, Surinamese Armed Forces. On 25 February 1980, a group of 15 non-commissioned officers and one junior SKM officer, under the leadership of sergeant major Dési Bouterse, overthrew the Government. Subsequently, the SKM was rebranded as "Nationaal Leger" (NL), National Army.

In 1965 the Dutch and Americans used Suriname's Coronie site for multiple Nike Apache sounding rocket launches.

The country is divided into ten administrative districts, each headed by a district commissioner appointed by the president, who also has the power of dismissal. Suriname is further subdivided into 62 resorts (ressorten).

Suriname is the smallest independent country in South America. Situated on the Guiana Shield, it lies mostly between latitudes 1° and 6°N, and longitudes 54° and 58°W. The country can be divided into two main geographic regions. The northern, lowland coastal area (roughly above the line Albina-Paranam-Wageningen) has been cultivated, and most of the population lives here. The southern part consists of tropical rainforest and sparsely inhabited savanna along the border with Brazil, covering about 80% of Suriname's land surface.

The two main mountain ranges are the Bakhuys Mountains and the Van Asch Van Wijck Mountains. Julianatop is the highest mountain in the country at above sea level. Other mountains include Tafelberg at , Mount Kasikasima at , Goliathberg at and Voltzberg at .

Suriname's forest cover is 90.2%, the highest of any nation in the world.

Suriname is situated between French Guiana to the east and Guyana to the west. The southern border is shared with Brazil and the northern border is the Atlantic coast. The southernmost borders with French Guiana and Guyana are disputed by these countries along the Marowijne and Corantijn rivers, respectively, while a part of the disputed maritime boundary with Guyana was arbitrated by a tribunal convened under the rules set out in of the United Nations Convention on the Law of the Sea on 20 September 2007.

Lying 2 to 5 degrees north of the equator, Suriname has a very hot and wet tropical climate, and temperatures do not vary much throughout the year. Average relative humidity is between 80% and 90%. Its average temperature ranges from 29 to 34 degrees Celsius (84 to 93 degrees Fahrenheit). Due to the high humidity, actual temperatures are distorted and may therefore feel up to 6 degrees Celsius (11 degrees Fahrenheit) hotter than the recorded temperature. The year has two wet seasons, from April to August and from November to February. It also has two dry seasons, from August to November and February to April.

Located in the upper Coppename River watershed, the Central Suriname Nature Reserve has been designated a UNESCO World Heritage Site for its unspoiled forests and biodiversity. There are many national parks in the country including Galibi National Reserve along the coast; Brownsberg Nature Park and Eilerts de Haan Nature Park in central Suriname; and the Sipaliwani Nature Reserve on the Brazilian border. In all, 16% of the country's land area is national parks and lakes, according to the UNEP World Conservation Monitoring Centre.

Suriname's democracy gained some strength after the turbulent 1990s, and its economy became more diversified and less dependent on Dutch financial assistance. Bauxite (aluminium ore) mining used to be a strong revenue source. The discovery and exploitation of oil and gold has added substantially to Suriname's economic independence. Agriculture, especially rice and bananas, remains a strong component of the economy, and ecotourism is providing new economic opportunities. More than 80% of Suriname's land-mass consists of unspoiled rain forest; with the establishment of the Central Suriname Nature Reserve in 1998, Suriname signalled its commitment to conservation of this precious resource. The Central Suriname Nature Reserve became a World Heritage Site in 2000.
The economy of Suriname was dominated by the bauxite industry, which accounted for more than 15% of GDP and 70% of export earnings up to 2016. Other main export products include rice, bananas and shrimp. Suriname has recently started exploiting some of its sizeable oil and gold reserves. About a quarter of the people work in the agricultural sector. The Surinamese economy is very dependent on commerce, its main trade partners being the Netherlands, the United States, Canada, and Caribbean countries, mainly Trinidad and Tobago and the islands of the former Netherlands Antilles.

After assuming power in the fall of 1996, the Wijdenbosch government ended the structural adjustment program of the previous government, claiming it was unfair to the poorer elements of society. Tax revenues fell as old taxes lapsed and the government failed to implement new tax alternatives. By the end of 1997, the allocation of new Dutch development funds was frozen as Surinamese Government relations with the Netherlands deteriorated. Economic growth slowed in 1998, with decline in the mining, construction, and utility sectors. Rampant government expenditures, poor tax collection, a bloated civil service, and reduced foreign aid in 1999 contributed to the fiscal deficit, estimated at 11% of GDP. The government sought to cover this deficit through monetary expansion, which led to a dramatic increase in inflation. It takes longer on average to register a new business in Suriname than virtually any other country in the world (694 days or about 99 weeks).


According to the 2012 census, Suriname had a population of 541,638 inhabitants. The Surinamese populace is characterized by its high level of diversity, wherein no particular demographic group constitutes a majority. This is a legacy of centuries of Dutch rule, which entailed successive periods of forced, contracted, or voluntary migration by various nationalities and ethnic groups from around the world.

The largest ethnic group are the Afro-Surinamese which form about 37% of the population, and are usually divided into two groups: the Creoles and the Maroons. Surinamese Maroons, whose ancestors are mostly runaway slaves that fled to the interior, comprise 21.7% of the population; they are divided into six main groups: Ndyuka (Aucans), Saramaccans, Paramaccans, Kwinti, Aluku (Boni) and Matawai. Surinamese Creoles, mixed people descending from African slaves and mostly Dutch Europeans, form 15.7% of the population. East Indians, who form 27% of the population, are the second largest group. They are descendants of 19th-century contract workers from India, hailing mostly from the modern Indian states of Bihar, Jharkhand, and Eastern Uttar Pradesh along the Nepali border. Javanese make up 14% of the population, and like the East Indians, descend largely from workers contracted from the island of Java in the former Dutch East Indies (modern Indonesia). 13.4% of the population identifies as being of mixed ethnic heritage.

Other sizeable groups include the Chinese, originating from 19th-century contract workers and some recent migration, who number over 40,000 ; Lebanese, primarily Maronites; Jews of Sephardic and Ashkenazi origin, whose center of population was the community of Jodensavanne; and Brazilians, many of them laborers mining for gold.

A small but influential number of Europeans remain in the country, comprising about 1 percent of the population. They are descended mostly from Dutch 19th-century immigrant farmers, known as "Boeroes" (derived from "boer", the Dutch word for "farmer"), and to a lesser degree other European groups, such as Portuguese from Madeira. Many Boeroes left after independence in 1975.

Various indigenous peoples make up 3.7% of the population, with the main groups being the Akurio, Arawak, Kalina (Caribs), Tiriyó and Wayana. They live mainly in the districts of Paramaribo, Wanica, Para, Marowijne and Sipaliwini.

The vast majority of Suriname's inhabitants (about 90%) live in Paramaribo or on the coast.

The choice of becoming Surinamese or Dutch citizens in the years leading up to Suriname's independence in 1975 led to a mass migration to the Netherlands. This migration continued in the period immediately after independence and during military rule in the 1980s and for largely economic reasons extended throughout the 1990s. The Surinamese community in the Netherlands numbered 350,300 (including children and grandchildren of Suriname migrants born in The Netherlands); this is compared to approximately 566,000 Surinamese in Suriname itself.

According to the International Organization for Migration, around 272,600 people from Suriname lived in other countries in the late 2010s, in particular in the Netherlands (ca 192,000), the French Republic (ca 25,000, most of them in French Guiana), the United States (ca 15,000), Guyana (ca 5,000), Aruba (ca 1,500), and Canada (ca 1,000).

Suriname's religious makeup is heterogeneous and reflective of the country's multicultural character.

According to the 2012 census, 48.4% were Christians; 26.7% of Surinamese were Protestants (11.18% Pentecostal, 11.16% Moravian, and 4.4% of various other Protestant denominations) and 21.6% were Catholics. Hindus formed the second-largest religious group in Suriname, comprising 22.3% of the population, the third largest proportion of any country in the Western Hemisphere after Guyana and Trinidad and Tobago, both of which also have large proportions of Indians. Almost all practitioners of Hinduism are found among the Indo-Surinamese population. Muslims constitute 13.9% of the population, the highest proportion of Muslims in the Americas; they are largely of Javanese or Indian descent. Other religious groups include Winti (1.8%), an Afro-American religion practiced mostly by those of Maroon ancestry; Javanism (0.8%), a syncretic faith found among some Javanese Surinamese; and various indigenous folk traditions that are often incorporated into one of the larger religions (usually Christianity). In the 2012 census, 7.5% of the population declared they had "no religion", while a further 3.2% left the question unanswered.

Dutch is the sole official language, and is the language of education, government, business, and the media. Over 60% of the population speaks Dutch as a mother tongue, and most of the rest speaks it as a second language. In 2004, Suriname became an associate member of the Dutch Language Union. It is the only Dutch-speaking country in South America as well as the only independent nation in the Americas where Dutch is spoken by a majority of the population, and one of the two non-Romance-speaking countries in South America, the other being English-speaking Guyana.

In Paramaribo, Dutch is the main home language in two-thirds of the households. The recognition of ""Surinaams-Nederlands"" (""Surinamese Dutch"") as a national dialect equal to ""Nederlands-Nederlands"" (""Dutch Dutch"") and ""Vlaams-Nederlands"" (""Flemish Dutch"") was expressed in 2009 by the publication of the "Woordenboek Surinaams Nederlands" ("Surinamese–Dutch Dictionary"). It is the most commonly spoken language in urban areas; only in the interior of Suriname (namely parts of Sipaliwini and Brokopondo) is Dutch seldom spoken.

Sranan Tongo, a local creole language originally spoken by the Creole population group, is the most widely used vernacular language in day-to-day life and business. It and Dutch are considered to be the two principal languages of Surinamese diglossia; both are further influenced by other spoken languages which are spoken primarily within ethnic communities. Sranan Tongo is often used interchangeably with Dutch depending on the formality of the setting, where Dutch is seen as a prestige dialect and Sranan Tongo the common vernacular.

Caribbean Hindustani or Sarnami, a dialect of Bhojpuri, is the fourth-most used language (after English), spoken by the descendants of South Asian contract workers from then British India. The Javanese language is used by the descendants of Javanese contract workers, and is common in Suriname. The Maroon languages, somewhat intelligible with Sranan, include Saramaka, Paramakan, Ndyuka (also called "Aukan"), Kwinti and Matawai. Amerindian languages, spoken by Amerindians, include Carib and Arawak. Hakka and Cantonese are spoken by the descendants of the Chinese contract workers. Mandarin is spoken by some few recent Chinese immigrants. English, Spanish, and Portuguese are also used as second languages.

The national capital, Paramaribo, is by far the dominant urban area, accounting for nearly half of Suriname's population and most of its urban residents; indeed, its population is greater than the next nine largest cities combined. Most municipalities are located within the capital's metropolitan area, or along the densely populated coastline.

Owing to the country's multicultural heritage, Suriname celebrates a variety of distinct ethnic and religious festivals.


There are several Hindu and Islamic national holidays like Diwali (deepavali), Phagwa and Eid ul-Fitr and Eid-ul-adha. These holidays do not have fixed dates on the Gregorian calendar, as they are based on the Hindu and Islamic calendars, respectively. As of 2020, Eid-ul-adha is a national holiday, and equal to a Sunday.

There are several holidays which are unique to Suriname. These include the Indian, Javanese and Chinese arrival days. They celebrate the arrival of the first ships with their respective immigrants.

New Year's Eve in Suriname is called "Oud jaar", "Owru Yari", or "old year". It is during this period that the Surinamese population goes to the city's commercial district to watch "demonstrational fireworks". The bigger stores invest in these firecrackers and display them out in the streets. Every year the length of them is compared, and high praises are given for the company that has imported the largest ribbon.

These celebrations start at 10 in the morning and finish the next day. The day is usually filled with laughter, dance, music, and drinking. When the night starts, the big street parties are already at full capacity. The most popular fiesta is the one that is held at café 't Vat in the main tourist district. The parties there stop between 10 and 11 at night, after which people go home to light their pagaras (red-firecracker-ribbons) at midnight.
After 12, the parties continue and the streets fill again until daybreak.

The major sports in Suriname are football, basketball, and volleyball. The Suriname Olympic Committee is the national governing body for sports in Suriname. The major mind sports are chess, draughts, bridge and troefcall.

Many Suriname-born football players and Dutch-born football players of Surinamese descent, like Gerald Vanenburg, Ruud Gullit, Frank Rijkaard, Edgar Davids, Clarence Seedorf, Patrick Kluivert, Aron Winter, Georginio Wijnaldum, Virgil van Dijk and Jimmy Floyd Hasselbaink have turned out to play for the Dutch national team. In 1999, Humphrey Mijnals, who played for both Suriname and the Netherlands, was elected Surinamese footballer of the century. Another famous player is André Kamperveen, who captained Suriname in the 1940s and was the first Surinamese to play professionally in the Netherlands.

The most famous international track & field athlete from Suriname is Letitia Vriesde, who won a silver medal at the 1995 World Championships behind Ana Quirot in the 800 metres, the first medal won by a South American female athlete in World Championship competition. In addition, she also won a bronze medal at the 2001 World Championships and won several medals in the 800 and 1500 metres at the Pan-American Games and Central American and Caribbean Games. Tommy Asinga also received acclaim for winning a bronze medal in the 800 metres at the 1991 Pan American Games.

Swimmer Anthony Nesty is the only Olympic medalist for Suriname. He won gold in the 100-meter butterfly at the 1988 Summer Olympics in Seoul and he won bronze in the same discipline at the 1992 Summer Olympics in Barcelona. Originally from Trinidad and Tobago, he now lives in Gainesville, Florida, and is the coach of the University of Florida, mainly coaching distance swimmers.

Cricket is popular in Suriname to some extent, influenced by its popularity in the Netherlands and in neighbouring Guyana. The Surinaamse Cricket Bond is an associate member of the International Cricket Council (ICC). Suriname and Argentina are the only ICC associates in South America, although Guyana is represented on the West Indies Cricket Board, a full member. The national cricket team was ranked 47th in the world and sixth in the ICC Americas region as of June 2014, and competes in the World Cricket League (WCL) and ICC Americas Championship. Iris Jharap, born in Paramaribo, played women's One Day International matches for the Dutch national side, the only Surinamese to do so.

In the sport of badminton the local heroes are Virgil Soeroredjo & Mitchel Wongsodikromo and also Crystal Leefmans. All winning medals for Suriname at the Carebaco Caribbean Championships, the Central American and Caribbean Games (CACSO Games) and also at the South American Games, better known as the ODESUR Games. Virgil Soeroredjo also participated for Suriname at the 2012 London Summer Olympics, only the second badminton player, after Oscar Brandon, for Suriname to achieve this. Current National Champion Sören Opti was the third Surinamese badminton player to participate at the Summer Olympics in 2016.

Multiple time K-1 kickboxing world champions Ernesto Hoost and Remy Bonjasky were born in Suriname or are of Surinamese descent. Other kickboxing world champions include Rayen Simson, Melvin Manhoef, Tyrone Spong, Jairzinho Rozenstruik and Regian Eersel.

Suriname also has a national korfball team, with korfball being a Dutch sport. Vinkensport is also practised.

Suriname, along with neighboring Guyana, is one of only two countries on the mainland South American continent that drive on the left, although many vehicles are left hand drive as well as right hand drive. One explanation for this practice is that at the time of its colonization of Suriname, the Netherlands itself used left-hand traffic, also introducing the practice in the Dutch East Indies, now Indonesia. Another is that Suriname was first colonized by the British, and for practical reasons, this was not changed when it came under Dutch administration. Although the Netherlands converted to driving to the right at the end of the 18th century, Suriname did not.

Airlines with departures from Suriname:

Airlines with arrivals in Suriname:

Other national companies with an air operator certification:

The Global Burden of Disease Study provides an on-line data source for analyzing updated estimates of health for 359 diseases and injuries and 84 risk factors from 1990 to 2017 in most of the world's countries. Comparing Suriname with other Caribbean nations show that in 2017 the age-standardized death rate for all causes was 793 (males 969, females 641) per 100,000, far below the 1219 of Haiti, somewhat below the 944 of Guyana but considerably above the 424 of Bermuda. In 1990 the death rate was 960 per 100,000. Life expectancy in 2017 was 72 years (males 69, females 75). The death rate for children < 5 years was 581 per 100,000 compared to 1308 in Haiti and 102 in Bermuda. In 1990 and 2017, leading causes of age-standardized death rates were cardiovascular disease, cancer and diabetes/chronic kidney disease.

Education in Suriname is compulsory until the age of 12, and the nation had a net primary enrollment rate of 94% in 2004. Literacy is very common, particularly among men. The main university in the country is the Anton de Kom University of Suriname.

From elementary school to high school there are 13 grades. The elementary school has six grades, middle school four grades and high school three grades. Students take a test in the end of elementary school to determine whether they will go to the MULO (secondary modern school) or a middle school of lower standards like LBO. Students from the elementary school wear a green shirt with jeans, while middle school students wear a blue shirt with jeans.

Students going from the second grade of middle school to the third grade have to choose between the business or science courses. This will determine what their major subjects will be. In order to go on to study math and physics, the student must have a total of 12 points. If the student has fewer points, he/she will go into the business courses or fail the grade.

Due to the variety of habitats and temperatures, biodiversity in Suriname is considered high. In October 2013, 16 international scientists researching the ecosystems during a three-week expedition in Suriname's Upper Palumeu River Watershed catalogued 1,378 species and found 60—including six frogs, one snake, and 11 fish—that may be previously unknown species. According to the environmental non-profit Conservation International, which funded the expedition, Suriname's ample supply of fresh water is vital to the biodiversity and healthy ecosystems of the region.

Snakewood ("Brosimum guianense"), a shrub-like tree, is native to this tropical region of the Americas. Customs in Suriname report that snakewood is often illegally exported to French Guiana, thought to be for the crafts industry.

On 21 March 2013, Suriname's REDD+ Readiness Preparation Proposal (R-PP 2013) was approved by the member countries of the Participants Committee of the Forest Carbon Partnership Facility (FCPF).

As in other parts of Central and South America, indigenous communities have increased their activism to protect their lands and preserve habitat. In March 2015, the "Trio and Wayana communities presented a declaration of cooperation to the National Assembly of Suriname that announces an indigenous conservation corridor spanning 72,000 square kilometers (27,799 square miles) of southern Suriname. The declaration, led by these indigenous communities and with the support of Conservation International (CI) and World Wildlife Fund (WWF) Guianas, comprises almost half of the total area of Suriname." This area includes large forests and is considered "essential for the country's climate resilience, freshwater security, and green development strategy."

Traditionally, "De Ware Tijd" was the major newspaper of the country, but since the '90s "Times of Suriname, De West" and "Dagblad Suriname" have also been well-read newspapers; all publish primarily in Dutch.

Suriname has twenty-four radio stations, most of them also broadcast through the Internet. There are twelve television sources:
ABC (Ch. 4–1, 2), RBN (Ch. 5–1, 2), Rasonic TV (Ch. 7), STVS (Ch. 8–1, 2, 3, 4, 5, 6), Apintie (Ch. 10–1), ATV (Ch. 12–1, 2, 3, 4), Radika (Ch. 14), SCCN (Ch. 17–1, 2, 3), Pipel TV (Ch. 18–1, 2), Trishul (Ch. 20–1, 2, 3, 4), Garuda (Ch. 23–1, 2, 3), Sangeetmala (Ch. 26), Ch. 30, Ch. 31, Ch.32, Ch.38, SCTV (Ch. 45). Also listened to is mArt, a broadcaster from Amsterdam founded by people from Suriname. Kondreman is one of the popular cartoons in Suriname.

There are also three major news sites: Starnieuws, Suriname Herald and GFC Nieuws.

In 2012, Suriname was ranked joint 22nd with Japan in the worldwide Press Freedom Index by the organization Reporters Without Borders. This was ahead of the US (47th), the UK (28th), and France (38th).

Most tourists visit Suriname for the biodiversity of the Amazonian rain forests in the south of the country, which are noted for their flora and fauna. The Central Suriname Nature Reserve is the biggest and one of the most popular reserves, along with the Brownsberg Nature Park which overlooks the Brokopondo Reservoir, one of the largest man-made lakes in the world. In 2008, the Berg en Dal Eco & Cultural Resort opened in Brokopondo. Tonka Island in the reservoir is home to a rustic eco-tourism project run by the Saramaccaner Maroons. Pangi wraps and bowls made of calabashes are the two main products manufactured for tourists. The Maroons have learned that colorful and ornate pangis are popular with tourists. Other popular decorative souvenirs are hand-carved purple-hardwood made into bowls, plates, canes, wooden boxes, and wall decors.

There are also many waterfalls throughout the country. Raleighvallen, or Raleigh Falls, is a nature reserve on the Coppename River, rich in bird life. Also are the Blanche Marie Falls on the Nickerie River and the Wonotobo Falls. Tafelberg Mountain in the centre of the country is surrounded by its own reserve – the Tafelberg Nature Reserve – around the source of the Saramacca River, as is the Voltzberg Nature Reserve further north on the Coppename River at Raleighvallen. In the interior are many Maroon and Amerindian villages, many of which have their own reserves that are generally open to visitors.

Suriname is one of the few countries in the world where at least one of each biome that the state possesses has been declared a wildlife reserve. Around 30% of the total land area of Suriname is protected by law as reserves.

Other attractions include plantations such as Laarwijk, which is situated along the Suriname River. This plantation can be reached only by boat via Domburg, in the north central Wanica District of Suriname.

Crime rates continue to rise in Paramaribo and armed robberies are not uncommon. According to the current U.S. Department of State Travel Advisory at the date of the 2018 report's publication, Suriname has been assessed as Level 1: exercise normal precautions.

The Jules Wijdenbosch Bridge is a bridge over the river Suriname between Paramaribo and Meerzorg in the Commewijne district. The bridge was built during the tenure of President Jules Albert Wijdenbosch (1996–2000) and was completed in 2000. The bridge is high, and long. It connects Paramaribo with Commewijne, a connection which previously could only be made by ferry. The purpose of the bridge was to facilitate and promote the development of the eastern part of Suriname. The bridge consists of two lanes (one lane each way) and is not accessible to pedestrians.

The construction of the Sts. Peter and Paul Cathedral started on 13 January 1883. Before it became a cathedral it was a theatre. The theatre was built in 1809 and burned down in 1820.

Suriname is one of the few countries in the world where a synagogue is located next to a mosque.
The two buildings are located next to each other in the centre of Paramaribo and have been known to share a parking facility during their respective religious rites, should they happen to coincide with one another.

A relatively new landmark is the Hindu Arya Dewaker temple in the Johan Adolf Pengelstraat in Wanica, Paramaribo, which was inaugurated in 2001. A special characteristic of the temple is that it does not have images of the Hindu divinities, as they are forbidden in the Arya Samaj, the Hindu movement to which the people who built the temple belong. Instead, the building is covered by many texts derived from the Vedas and other Hindu scriptures. The beautiful architecture makes the temple a tourist attraction.




</doc>
<doc id="26829" url="https://en.wikipedia.org/wiki?curid=26829" title="Category of sets">
Category of sets

In the mathematical field of category theory, the category of sets, denoted as Set, is the category whose objects are sets. The arrows or morphisms between sets "A" and "B" are the total functions from "A" to "B", and the composition of morphisms is the composition of functions.

Many other categories (such as the category of groups, with group homomorphisms as arrows) add structure to the objects of the category of sets and/or restrict the arrows to functions of a particular kind.

The axioms of a category are satisfied by Set because composition of functions is associative, and because every set "X" has an identity function "id : X → X" which serves as identity element for function composition.

The epimorphisms in Set are the surjective maps, the monomorphisms are the injective maps, and the isomorphisms are the bijective maps.

The empty set serves as the initial object in Set with empty functions as morphisms. Every singleton is a terminal object, with the functions mapping all elements of the source sets to the single target element as morphisms. There are thus no zero objects in Set. 

The category Set is complete and co-complete. The product in this category is given by the cartesian product of sets. The coproduct is given by the disjoint union: given sets "A" where "i" ranges over some index set "I", we construct the coproduct as the union of "A"×{"i"} (the cartesian product with "i" serves to ensure that all the components stay disjoint).

Set is the prototype of a concrete category; other categories are concrete if they are "built on" Set in some well-defined way.

Every two-element set serves as a subobject classifier in Set. The power object of a set "A" is given by its power set, and the exponential object of the sets "A" and "B" is given by the set of all functions from "A" to "B". Set is thus a topos (and in particular cartesian closed and exact in the sense of Barr).

Set is not abelian, additive nor preadditive.

Every non-empty set is an injective object in Set. Every set is a projective object in Set (assuming the axiom of choice).

The finitely presentable objects in Set are the finite sets. Since every set is a direct limit of its finite subsets, the category Set is a locally finitely presentable category.

If "C" is an arbitrary category, the contravariant functors from "C" to Set are often an important object of study. If "A" is an object of "C", then the functor from "C" to Set that sends "X" to Hom("X","A") (the set of morphisms in "C" from "X" to "A") is an example of such a functor. If "C" is a small category (i.e. the collection of its objects forms a set), then the contravariant functors from "C" to Set, together with natural transformations as morphisms, form a new category, a functor category known as the category of presheaves on "C".

In Zermelo–Fraenkel set theory the collection of all sets is not a set; this follows from the axiom of foundation. One refers to collections that are not sets as proper classes. One cannot handle proper classes as one handles sets; in particular, one cannot write that those proper classes belong to a collection (either a set or a proper class). This is a problem because it means that the category of sets cannot be formalized straightforwardly in this setting. Categories like Set whose collection of objects forms a proper class are known as large categories, to distinguish them from the small categories whose objects form a set. 

One way to resolve the problem is to work in a system that gives formal status to proper classes, such as NBG set theory. In this setting, categories formed from sets are said to be "small" and those (like Set) that are formed from proper classes are said to be "large".

Another solution is to assume the existence of Grothendieck universes. Roughly speaking, a Grothendieck universe is a set which is itself a model of ZF(C) (for instance if a set belongs to a universe, its elements and its powerset will belong to the universe). The existence of Grothendieck universes (other than the empty set and the set formula_1 of all hereditarily finite sets) is not implied by the usual ZF axioms; it is an additional, independent axiom, roughly equivalent to the existence of strongly inaccessible cardinals. Assuming this extra axiom, one can limit the objects of Set to the elements of a particular universe. (There is no "set of all sets" within the model, but one can still reason about the class "U" of all inner sets, i.e., elements of "U".)

In one variation of this scheme, the class of sets is the union of the entire tower of Grothendieck universes. (This is necessarily a proper class, but each Grothendieck universe is a set because it is an element of some larger Grothendieck universe.) However, one does not work directly with the "category of all sets". Instead, theorems are expressed in terms of the category Set whose objects are the elements of a sufficiently large Grothendieck universe "U", and are then shown not to depend on the particular choice of "U". As a foundation for category theory, this approach is well matched to a system like Tarski–Grothendieck set theory in which one cannot reason directly about proper classes; its principal disadvantage is that a theorem can be true of all Set but not of Set.

Various other solutions, and variations on the above, have been proposed.

The same issues arise with other concrete categories, such as the category of groups or the category of topological spaces.




</doc>
<doc id="26830" url="https://en.wikipedia.org/wiki?curid=26830" title="Slovakia">
Slovakia

Slovakia (; ), officially the Slovak Republic (, ), is a landlocked country in Central Europe. It is bordered by Poland to the north, Ukraine to the east, Hungary to the south, Austria to the southwest, and Czech Republic to the northwest. Slovakia's territory spans about and is mostly mountainous. The population is over 5.4 million and consists mostly of ethnic Slovaks. The capital and largest city is Bratislava, and the second-largest city is Košice. The official language is Slovak.

The Slavs arrived in the territory of present-day Slovakia in the 5th and 6th centuries. In the 7th century they played a significant role in the creation of Samo's Empire and in the 9th century established the Principality of Nitra, which was later conquered by the Principality of Moravia to establish Great Moravia. In the 10th century, after the dissolution of Great Moravia, the territory was integrated into the Principality of Hungary, which would become the Kingdom of Hungary in 1000. In 1241 and 1242, much of the territory was destroyed by the Mongols during their invasion of Central and Eastern Europe. The area was recovered largely thanks to Béla IV of Hungary who also settled Germans who became an important ethnic group in the area, especially in what are today parts of central and eastern Slovakia. After World War I and the dissolution of the Austro-Hungarian Empire, the Czechoslovak National Council established Czechoslovakia (1918–1939). A separate (First) Slovak Republic (1939–1945) existed during World War II as a totalitarian, clero-fascist one-party client state of Nazi Germany. At the end of World War II, Czechoslovakia was re-established as an independent country. After a "coup" in 1948 Czechoslovakia became a totalitarian one-party socialist state under a communist administration, during which the country was part of the Soviet-led Eastern Bloc. Attempts to liberalize communism in Czechoslovakia culminated in the Prague Spring, which was crushed by the Warsaw Pact invasion of Czechoslovakia in August 1968. In 1989, the Velvet Revolution ended the Communist rule in Czechoslovakia peacefully. Slovakia became an independent state on 1 January 1993 after the peaceful dissolution of Czechoslovakia, sometimes known as the Velvet Divorce.

Slovakia is a developed country with an advanced, high-income economy, a very high Human Development Index, a very high standard of living and performs favourably in measurements of civil liberties, press freedom, internet freedom, democratic governance and peacefulness. The country maintains a combination of a market economy with a comprehensive social security system. Citizens of Slovakia are provided with universal health care, free education and one of the longest paid parental leaves in the OECD. The country joined the European Union on 1 May 2004 and joined the Eurozone on 1 January 2009. Slovakia is also a member of the Schengen Area, NATO, the United Nations, the OECD, the WTO, CERN, the OSCE, the Council of Europe and the Visegrád Group. As part of Eurozone, Slovak legal tender is the euro, the world's 2nd-most-traded currency. Slovakia is the world's largest per-capita car producer with a total of 1,110,000 cars manufactured in the country in 2019 alone and the 5th largest car producer in the European Union, representing 43% of Slovakia's total industrial output.

The first written mention of name "Slovakia" is in 1586 (). It derives from the Czech word "Slováky"; previous German forms were "Windischen landen" and "Windenland" (the 15th century). The native name "Slovensko" (1791) derives from an older name of Slovaks, "Sloven", which may indicate its origin before the 15th century. The original meaning was geographic (not political), since Slovakia was a part of the multiethnic Kingdom of Hungary and did not form a separate administrative unit in this period.

The oldest surviving human artefacts from Slovakia are found near Nové Mesto nad Váhom and are dated at 270,000 BCE, in the Early Paleolithic era. These ancient tools, made by the Clactonian technique, bear witness to the ancient habitation of Slovakia.

Other stone tools from the Middle Paleolithic era (200,000–80,000 BCE) come from the Prévôt (Prepoštská) cave in Bojnice and from other nearby sites. The most important discovery from that era is a Neanderthal cranium (c. 200,000 BCE), discovered near Gánovce, a village in northern Slovakia.

Archaeologists have found prehistoric human skeletons in the region, as well as numerous objects and vestiges of the Gravettian culture, principally in the river valleys of Nitra, Hron, Ipeľ, Váh and as far as the city of Žilina, and near the foot of the Vihorlat, Inovec, and Tribeč mountains, as well as in the Myjava Mountains. The most well-known finds include the oldest female statue made of mammoth bone (22,800 BCE), the famous Venus of Moravany. The statue was found in the 1940s in Moravany nad Váhom near Piešťany. Numerous necklaces made of shells from Cypraca thermophile gastropods of the Tertiary period have come from the sites of Zákovská, Podkovice, Hubina, and Radošina. These findings provide the most ancient evidence of commercial exchanges carried out between the Mediterranean and Central Europe.

During the Bronze Age, the geographical territory of modern-day Slovakia went through three stages of development, stretching from 2000 to 800 BCE. Major cultural, economic, and political development can be attributed to the significant growth in production of copper, especially in central Slovakia (for example in Špania Dolina) and northwest Slovakia. Copper became a stable source of prosperity for the local population.

After the disappearance of the Čakany and Velatice cultures, the Lusatian people expanded building of strong and complex fortifications, with the large permanent buildings and administrative centres. Excavations of Lusatian hill forts document the substantial development of trade and agriculture at that period. The richness and diversity of tombs increased considerably. The inhabitants of the area manufactured arms, shields, jewellery, dishes, and statues.

The arrival of tribes from Thrace disrupted the people of the Kalenderberg culture, who lived in the hamlets located on the plain (Sereď) and in the hill forts like Molpír, near Smolenice, in the Little Carpathians. During Hallstatt times, monumental burial mounds were erected in western Slovakia, with princely equipment consisting of richly decorated vessels, ornaments and decorations. The burial rites consisted entirely of cremation. Common people were buried in flat urnfield cemeteries.

A special role was given to weaving and the production of textiles. The local power of the "Princes" of the Hallstatt period disappeared in Slovakia during the century before the middle of first millennium BC, after strife between the Scytho-Thracian people and locals, resulting in abandonment of the old hill-forts. Relatively depopulated areas soon caught the interest of emerging Celtic tribes, who advanced from the south towards the north, following the Slovak rivers, peacefully integrating into the remnants of the local population.

From around 500 BCE, the territory of modern-day Slovakia was settled by Celts, who built powerful "oppida" on the sites of modern-day Bratislava and Devín. Biatecs, silver coins with inscriptions in the Latin alphabet, represent the first known use of writing in Slovakia. At the northern regions, remnants of the local population of Lusatian origin, together with Celtic and later Dacian influence, gave rise to the unique Púchov culture, with advanced crafts and iron-working, many hill-forts and fortified settlements of central type with the coinage of the "Velkobysterecky" type (no inscriptions, with a horse on one side and ahead on the other). This culture is often connected with the Celtic tribe mentioned in Roman sources as Cotini.

From 2 AD, the expanding Roman Empire established and maintained a series of outposts around and just south of the Danube, the largest of which were known as Carnuntum (whose remains are on the main road halfway between Vienna and Bratislava) and Brigetio (present-day Szőny at the Slovak-Hungarian border). Such Roman border settlements were built on the present area of Rusovce, currently a suburb of Bratislava. The military fort was surrounded by a civilian vicus and several farms of the villa rustica type. The name of this settlement was Gerulata. The military fort had an auxiliary cavalry unit, approximately 300 horses strong, modelled after the Cananefates. The remains of Roman buildings have also survived in Devín Castle (present-day downtown Bratislava), the suburbs of Dúbravka and Stupava, and Bratislava Castle Hill.

Near the northernmost line of the Roman hinterlands, the Limes Romanus, there existed the winter camp of Laugaricio (modern-day Trenčín) where the Auxiliary of Legion II fought and prevailed in a decisive battle over the Germanic Quadi tribe in 179 CE during the Marcomannic Wars. The Kingdom of Vannius, a kingdom founded by the Germanic Suebi tribes of Quadi and Marcomanni, as well as several small Germanic and Celtic tribes, including the Osi and Cotini, existed in western and central Slovakia from 8–6 BCE to 179 CE.

In the 2nd and 3rd centuries AD, the Huns began to leave the Central Asian steppes. They crossed the Danube in 377 AD and occupied Pannonia, which they used for 75 years as their base for launching looting-raids into Western Europe. However, Attila's death in 453 brought about the disappearance of the Hun tribe. In 568, a Turko-Mongol tribal confederacy, the Avars, conducted its invasion into the Middle Danube region. The Avars occupied the lowlands of the Pannonian Plain and established an empire dominating the Carpathian Basin.

In 623, the Slavic population living in the western parts of Pannonia seceded from their empire after a revolution led by Samo, a Frankish merchant. After 626, the Avar power started a gradual decline but its reign lasted to 804.

The Slavic tribes settled in the territory of present-day Slovakia in the 5th century. Western Slovakia was the centre of Samo's empire in the 7th century. A Slavic state known as the Principality of Nitra arose in the 8th century and its ruler Pribina had the first known Christian church of the territory of present-day Slovakia consecrated by 828. Together with neighbouring Moravia, the principality formed the core of the Great Moravian Empire from 833. The high point of this Slavonic empire came with the arrival of Saints Cyril and Methodius in 863, during the reign of Duke Rastislav, and the territorial expansion under King Svätopluk I.

Great Moravia arose around 830 when Mojmír I unified the Slavic tribes settled north of the Danube and extended the Moravian supremacy over them. When Mojmír I endeavoured to secede from the supremacy of the king of East Francia in 846, King Louis the German deposed him and assisted Mojmír's nephew Rastislav (846–870) in acquiring the throne. The new monarch pursued an independent policy: after stopping a Frankish attack in 855, he also sought to weaken the influence of Frankish priests preaching in his realm. Duke Rastislav asked the Byzantine Emperor Michael III to send teachers who would interpret Christianity in the Slavic vernacular.

Upon Rastislav's request, two brothers, Byzantine officials and missionaries Saints Cyril and Methodius came in 863. Cyril developed the first Slavic alphabet and translated the Gospel into the Old Church Slavonic language. Rastislav was also preoccupied with the security and administration of his state. Numerous fortified castles built throughout the country are dated to his reign and some of them (e.g., "Dowina", sometimes identified with Devín Castle) are also mentioned in connection with Rastislav by Frankish chronicles.
During Rastislav's reign, the Principality of Nitra was given to his nephew Svätopluk as an appanage. The rebellious prince allied himself with the Franks and overthrew his uncle in 870. Similarly to his predecessor, Svätopluk I (871–894) assumed the title of the king ("rex"). During his reign, the Great Moravian Empire reached its greatest territorial extent, when not only present-day Moravia and Slovakia but also present-day northern and central Hungary, Lower Austria, Bohemia, Silesia, Lusatia, southern Poland and northern Serbia belonged to the empire, but the exact borders of his domains are still disputed by modern authors. Svatopluk also withstood attacks of the Magyar tribes and the Bulgarian Empire, although sometimes it was he who hired the Magyars when waging war against East Francia.

In 880, Pope John VIII set up an independent ecclesiastical province in Great Moravia with Archbishop Methodius as its head. He also named the German cleric Wiching the Bishop of Nitra.
After the death of Prince Svatopluk in 894, his sons Mojmír II (894–906?) and Svatopluk II succeeded him as the Prince of Great Moravia and the Prince of Nitra respectively. However, they started to quarrel for domination of the whole empire. Weakened by an internal conflict as well as by constant warfare with Eastern Francia, Great Moravia lost most of its peripheral territories.

In the meantime, the semi-nomadic Magyar tribes, possibly having suffered defeat from the similarly nomadic Pechenegs, left their territories east of the Carpathian Mountains, invaded the Carpathian Basin and started to occupy the territory gradually around 896. Their armies' advance may have been promoted by continuous wars among the countries of the region whose rulers still hired them occasionally to intervene in their struggles.

It is not known what happened with both Mojmír II and Svatopluk II because they are not mentioned in written sources after 906. In three battles (4–5 July and 9 August 907) near Bratislava, the Magyars routed Bavarian armies. Some historians put this year as the date of the break-up of the Great Moravian Empire, due to the Hungarian conquest; other historians take the date a little bit earlier (to 902).

Great Moravia left behind a lasting legacy in Central and Eastern Europe. The Glagolitic script and its successor Cyrillic were disseminated to other Slavic countries, charting a new path in their sociocultural development. The administrative system of Great Moravia may have influenced the development of the administration of the Kingdom of Hungary.

Following the disintegration of the Great Moravian Empire at the turn of the 10th century, the Hungarians annexed the territory comprising modern Slovakia. After their defeat on the Lech River they abandoned their nomadic ways; they settled in the centre of the Carpathian valley, adopted Christianity and began to build a new state—the Hungarian kingdom.

From the 11th century, when the territory inhabited by the Slavic-speaking population of Danubian Basin was incorporated into the Kingdom of Hungary, until 1918, when the Austro-Hungarian empire collapsed, the territory of modern Slovakia was an integral part of the Hungarian state. The ethnic composition became more diverse with the arrival of the Carpathian Germans in the 13th century, and the Jews in the 14th century.

A significant decline in the population resulted from the invasion of the Mongols in 1241 and the subsequent famine. However, in medieval times the area of the present-day Slovakia was characterised by German and Jewish immigration, burgeoning towns, construction of numerous stone castles, and the cultivation of the arts. In 1465, King Matthias Corvinus founded the Hungarian Kingdom's third university, in Pressburg (Bratislava, Pozsony), but it was closed in 1490 after his death. Hussites also settled in the region after the Hussite Wars.

Owing to the Ottoman Empire's expansion into Hungarian territory, Bratislava was designated the new capital of Hungary in 1536, ahead of the old Hungarian capital of Buda falling in 1541. It became part of the Austrian Habsburg monarchy, marking the beginning of a new era. The territory comprising modern Slovakia, then known as Upper Hungary, became the place of settlement for nearly two-thirds of the Magyar nobility fleeing the Turks and far more linguistically and culturally Hungarian than it was before. Partly thanks to old Hussite families, and Slovaks studying under Martin Luther, the region then experienced a growth in Protestantism. For a short period in the 17th century, most Slovaks were Lutherans. They defied the Catholic Habsburgs and sought protection from neighbouring Transylvania, a rival continuation of the Magyar state that practised religious tolerance and normally had Ottoman backing. Upper Hungary, modern Slovakia, became the site of frequent wars between Catholics in the west territory and Protestants in the east, also against Turks, the frontier was on a constant state of military alert and heavily fortified by castles and citadels often manned by Catholic German and Slovak troops on the Habsburg side. By 1648, Slovakia was not spared the Counter-Reformation, which brought the majority of its population from Lutheranism back to Roman Catholicism. In 1655, the printing press at the Trnava university produced the Jesuit Benedikt Szöllősi's Cantus Catholici, a Catholic hymnal in the Slovak language that reaffirmed links to the earlier works of Cyril and Methodius.

The Ottoman wars, the rivalry between Austria and Transylvania, and the frequent insurrections against the Habsburg Monarchy inflicted a great deal of devastation, especially in the rural areas. In the Austro-Turkish War (1663–1664) a Turkish army led by the Grand Vizier decimated Slovakia. Even so, Thököly's kuruc rebels from the Principality of Upper Hungary fought alongside the Turks against the Austrians and Poles at the Battle of Vienna of 1683 led by John III Sobieski. As the Turks withdrew from Hungary in the late 17th century, the importance of the territory comprising modern Slovakia decreased, although Pressburg retained its status as the capital of Hungary until 1848 when it was transferred back to Buda.

During the revolution of 1848–49, the Slovaks supported the Austrian Emperor, hoping for independence from the Hungarian part of the Dual Monarchy, but they failed to achieve their aim. Thereafter relations between the nationalities deteriorated (see Magyarization), culminating in the secession of Slovakia from Hungary after World War I.

In late October 1918, the Czech nationalist Tomáš Masaryk declared the "independence" for the territories of Bohemia, Moravia, Silesia, Upper Hungary and Carpathian Ruthenia and proclaimed a common state, Czechoslovakia. The Slovaks were not consulted. In 1919, during the chaos following the break-up of Austria-Hungary, Czechoslovakia was formed with numerous Germans, Slovaks, Hungarians and Ruthenians within the newly set borders. The borders were set by the Treaty of Saint Germain and Treaty of Trianon. In the peace following the World War, Czechoslovakia emerged as a sovereign European state. It provided what were at the time rather extensive rights to its minorities, at least on paper.

During the Interwar period, democratic Czechoslovakia was allied with France, and also with Romania and Yugoslavia (Little Entente); however, the Locarno Treaties of 1925 left East European security open. Both Czechs and Slovaks enjoyed a period of relative prosperity. There was progress in not only the development of the country's economy but also culture and educational opportunities. Yet the Great Depression caused a sharp economic downturn, followed by political disruption and insecurity in Europe.

In the 1930s Czechoslovakia came under continuous pressure from the revisionist governments of Germany, Hungary and Poland who used the aggrieved minorities in the country as a useful vehicle. Revision of the borders was called for, as Czechs constituted only 43% of the population. Eventually, this pressure led to the Munich Agreement of September 1938, which allowed the majority ethnic Germans in the Sudetenland, borderlands of Czechoslovakia, to join with Germany. The remaining minorities stepped up their pressures for autonomy and the State became federalised, with Diets in Slovakia and Ruthenia. The remainder of Czechoslovakia was renamed Czecho-Slovakia and promised a greater degree of Slovak political autonomy. This, however, failed to materialize. Parts of southern and eastern Slovakia were also reclaimed by Hungary at the First Vienna Award of November 1938.

After the Munich Agreement and its Vienna Award, Nazi Germany threatened to annex part of Slovakia and allow the remaining regions to be partitioned by Hungary or Poland unless independence was declared. Thus, Slovakia seceded from Czecho-Slovakia in March 1939 and allied itself, as demanded by Germany, with Hitler's coalition. Secession had created the first Slovak state in history. The government of the First Slovak Republic, led by Jozef Tiso and Vojtech Tuka, was strongly influenced by Germany and gradually became a puppet regime in many respects.

Meanwhile, the Czechoslovak government-in-exile sought to reverse the Munich Agreement and the subsequent German occupation of Czechoslovakia and to return the Republic to its 1937 boundaries. The government operated from London and it was ultimately considered, by those countries that recognised it, the legitimate government for Czechoslovakia throughout the Second World War.
As part of the Holocaust in Slovakia, 75,000 Jews out of 80,000 who remained on Slovak territory after Hungary had seized southern regions were deported and taken to German death camps. Thousands of Jews, Gypsies and other politically undesirable people remained in Slovak forced labor camps in Sereď, Vyhne, and Nováky. Tiso, through the granting of presidential exceptions, allowed between 1,000 and 4,000 people crucial to the war economy to avoid deportations.
Under Tiso's government and Hungarian occupation, the vast majority of Slovakia's pre-war Jewish population (between 75,000–105,000 individuals including those who perished from the occupied territory) were murdered. The Slovak state paid Germany 500 RM per every deported Jew for "retraining and accommodation" (a similar but smaller payment of 30 RM was paid by Croatia).

After it became clear that the Soviet Red Army was going to push the Nazis out of eastern and central Europe, an anti-Nazi resistance movement launched a fierce armed insurrection, known as the Slovak National Uprising, near the end of summer 1944. A bloody German occupation and a guerilla war followed. Germans and their local collaborators completely destroyed 93 villages and massacred thousands of civilians, often hundreds at a time. The territory of Slovakia was liberated by Soviet and Romanian forces by the end of April 1945.

After World War II, Czechoslovakia was reconstituted and Jozef Tiso was executed in 1947 for collaboration with the Nazis. More than 80,000 Hungarians and 32,000 Germans were forced to leave Slovakia, in a series of population transfers initiated by the Allies at the Potsdam Conference. Out of about 130,000 Carpathian Germans in Slovakia in 1938, by 1947 only some 20,000 remained.

As a result of the Yalta Conference, Czechoslovakia came under the influence and later under direct occupation of the Soviet Union and its Warsaw Pact, after a coup in 1948. Eight thousand two hundred and forty people went to forced labour camps in 1948–1953.

The country was invaded by the Warsaw Pact forces (People's Republic of Bulgaria, People's Republic of Hungary, People's Republic of Poland, and Soviet Union, with the exception of Socialist Republic of Romania and People's Socialist Republic of Albania) in 1968, ending a period of liberalisation under the leadership of Alexander Dubček. 137 Czechoslovakian civilians were killed and 500 seriously wounded during the occupation. In 1969 Czechoslovakia became a federation of the Czech Socialist Republic and the Slovak Socialist Republic. Czechoslovakia became a puppet state of the Soviet Union. Czechoslovak Socialist Republic was never part of the Soviet Union and remained independent to a degree.

Borders with the West were protected by the Iron Curtain. About 600 people, men, women, and children, were killed on the Czechoslovak border with Austria and West Germany between 1948 and 1989.

The end of Communist rule in Czechoslovakia in 1989, during the peaceful Velvet Revolution, was followed once again by the country's dissolution, this time into two successor states. The word "socialist" was dropped in the names of the two republics, with the Slovak Socialist Republic renamed as Slovak Republic. On 17 July 1992, Slovakia, led by Prime Minister Vladimír Mečiar, declared itself a sovereign state, meaning that its laws took precedence over those of the federal government. Throughout the autumn of 1992, Mečiar and Czech Prime Minister Václav Klaus negotiated the details for disbanding the federation. In November, the federal parliament voted to dissolve the country officially on 31 December 1992.

The Slovak Republic and the Czech Republic went their separate ways after 1 January 1993, an event sometimes called the Velvet Divorce. Slovakia has, nevertheless, remained a close partner with the Czech Republic. Both countries co-operate with Hungary and Poland in the Visegrád Group. Slovakia became a member of NATO on 29 March 2004 and of the European Union on 1 May 2004. On 1 January 2009, Slovakia adopted the Euro as its national currency. In 2019, Zuzana Čaputová became Slovakia's first female president.

Slovakia lies between latitudes 47° and 50° N, and longitudes 16° and 23° E. The Slovak landscape is noted primarily for its mountainous nature, with the Carpathian Mountains extending across most of the northern half of the country. Among these mountain ranges are the high peaks of the Fatra-Tatra Area (including Tatra Mountains, Greater Fatra and Lesser Fatra), Slovak Ore Mountains, Slovak Central Mountains or Beskids. The largest lowland is the fertile Danubian Lowland in the southwest, followed by the Eastern Slovak Lowland in the southeast. Forests cover 41% of Slovak land surface.

The Tatra Mountains, with 29 peaks higher than AMSL, are the highest mountain range in the Carpathian Mountains. The Tatras occupy an area of , of which the greater part lies in Slovakia. They are divided into several parts.

To the north, close to the Polish border, are the High Tatras which are a popular hiking and skiing destination and home to many scenic lakes and valleys as well as the highest point in Slovakia, the Gerlachovský štít at and the country's highly symbolic mountain Kriváň. To the west are the Western Tatras with their highest peak of Bystrá at and to the east are the Belianske Tatras, smallest by area.

Separated from the Tatras proper by the valley of the Váh river are the Low Tatras, with their highest peak of Ďumbier at .

The Tatra mountain range is represented as one of the three hills on the coat of arms of Slovakia.

There are 9 national parks in Slovakia, covering 6.5% of the Slovak land surface.

Slovakia has hundreds of caves and caverns under its mountains, of which 30 are open to the public. Most of the caves have stalagmites rising from the ground and stalactites hanging from above. There are currently five Slovak caves under UNESCO's World Heritage Site status. They are Dobšiná Ice Cave, Domica, Gombasek Cave, Jasovská Cave and Ochtinská Aragonite Cave. Other caves open to the public include Belianska Cave, Demänovská Cave of Liberty, Demänovská Ice Cave or Bystrianska Cave.

Most of the rivers arise in the Slovak mountains. Some only pass through Slovakia, while others make a natural border with surrounding countries (more than ). For example, the Dunajec () to the north, the Danube () to the south or the Morava () to the West. The total length of the rivers on Slovak territory is .

The longest river in Slovakia is the Váh (), the shortest is the Čierna voda. Other important and large rivers are the Myjava, the Nitra (), the Orava, the Hron (), the Hornád (), the Slaná (), the Ipeľ (, forming the border with Hungary), the Bodrog, the Laborec, the Latorica and the Ondava.

The biggest volume of discharge in Slovak rivers is during spring, when the snow melts from the mountains. The only exception is the Danube, whose discharge is the greatest during summer when the snow melts in the Alps. The Danube is the largest river that flows through Slovakia.

The Slovak climate lies between the temperate and continental climate zones with relatively warm summers and cold, cloudy and humid winters. Temperature extremes are between although temperatures below are rare. The weather differs from the mountainous north to the plains in the south.

The warmest region is Bratislava and Southern Slovakia where the temperatures may reach in summer, occasionally to in Hurbanovo. During night, the temperatures drop to . The daily temperatures in winter average in the range of to . During night it may be freezing, but usually not below .

In Slovakia, there are four seasons, each season (spring, summer, autumn and winter) lasts three months. The dry continental air brings in the summer heat and winter frosts. In contrast, oceanic air brings rainfalls and reduces summer temperatures. In the lowlands and valleys, there is often fog, especially in winter.

Spring starts with 21 March and is characterised by colder weather with an average daily temperature of in the first weeks and about in May and in June. In Slovakia, the weather and climate in the spring are very unstable.

Summer starts on 22 June and is usually characterised by hot weather with daily temperatures exceeding . July is the warmest month with temperatures up to about , especially in regions of southern Slovakia—in the urban area of Komárno, Hurbanovo or Štúrovo. Showers or thunderstorms may occur because of the summer monsoon called Medardova kvapka (Medard drop—40 days of rain). Summer in Northern Slovakia is usually mild with temperatures around (less in the mountains).

Autumn in Slovakia starts on 23 September and is mostly characterised by wet weather and wind, although the first weeks can be very warm and sunny. The average temperature in September is around , in November to . Late September and early October is a dry and sunny time of year (so-called Indian Summer).

Winter starts on 21 December with temperatures around . In December and January, it is usually snowing, these are the coldest months of the year. At lower altitudes, snow does not stay the whole winter, it changes into the thaw and frost. Winters are colder in the mountains, where the snow usually lasts until March or April and the night temperatures fall to and colder.

Slovakia signed the Rio Convention on Biological Diversity on 19 May 1993, and became a party to the convention on 25 August 1994. It has subsequently produced a National Biodiversity Strategy and Action Plan, which was received by the convention on 2 November 1998.

The biodiversity of Slovakia comprises animals (such as annelids, arthropods, molluscs, nematodes and vertebrates), fungi (Ascomycota, Basidiomycota, Chytridiomycota, Glomeromycota and Zygomycota), micro-organisms (including Mycetozoa), and plants. The geographical position of Slovakia determines the richness of the diversity of fauna and flora. More than 11,000 plant species have been described throughout its territory, nearly 29,000 animal species and over 1,000 species of protozoa. Endemic biodiversity is also common.

Slovakia is located in the biome of temperate broadleaf and mixed forests. As the altitude changes, the vegetation associations and animal communities are forming height levels (oak, beech, spruce, scrub pine, alpine meadows and subsoil). Forests cover 44% of the territory of Slovakia. In terms of forest stands, 60% are broadleaf trees and 40% are coniferous trees. The occurrence of animal species is strongly connected to the appropriate types of plant associations and biotopes.

Over 4,000 species of fungi have been recorded from Slovakia. Of these, nearly 1,500 are lichen-forming species. Some of these fungi are undoubtedly endemic, but not enough is known to say how many. Of the lichen-forming species, about 40% have been classified as threatened in some way. About 7% are apparently extinct, 9% endangered, 17% vulnerable, and 7% rare. The conservation status of non-lichen-forming fungi in Slovakia is not well documented, but there is a red list for its larger fungi.

Slovakia is a parliamentary democratic republic with a multi-party system. The last parliamentary elections were held on 29 February 2020 and two rounds of presidential elections took place on 16 and 30 March 2019.

The Slovak head of state and the formal head of the executive is the president (currently Zuzana Čaputová, the first female president), though with very limited powers. The president is elected by direct, popular vote under the two-round system for a five-year term. Most executive power lies with the head of government, the prime minister (currently Igor Matovič), who is usually the leader of the winning party and who needs to form a majority coalition in the parliament. The prime minister is appointed by the president. The remainder of the cabinet is appointed by the president on the recommendation of the prime minister.

Slovakia's highest legislative body is the 150-seat unicameral National Council of the Slovak Republic ("Národná rada Slovenskej republiky"). Delegates are elected for a four-year term on the basis of proportional representation.

Slovakia's highest judicial body is the Constitutional Court of Slovakia ("Ústavný súd"), which rules on constitutional issues. The 13 members of this court are appointed by the president from a slate of candidates nominated by parliament.

The Constitution of the Slovak Republic was ratified 1 September 1992, and became effective 1 January 1993. It was amended in September 1998 to allow direct election of the president and again in February 2001 due to EU admission requirements. The civil law system is based on Austro-Hungarian codes. The legal code was modified to comply with the obligations of Organization on Security and Cooperation in Europe (OSCE) and to expunge the Marxist–Leninist legal theory. Slovakia accepts the compulsory International Court of Justice jurisdiction with reservations.

The Ministry of Foreign and European Affairs () is responsible for maintaining the Slovak Republic's external relations and the management of its international diplomatic missions. The ministry's director is Ivan Korčok. The ministry oversees Slovakia's affairs with foreign entities, including bilateral relations with individual nations and its representation in international organizations.

Slovakia joined the European Union and NATO in 2004 and the Eurozone in 2009.

Slovakia is a member of the United Nations (since 1993) and participates in its specialized agencies. The country was, on 10 October 2005, elected to a two-year term on the UN Security Council from 2006 to 2007. It is also a member of the Schengen Area, the Council of Europe (CoE), the Organization for Security and Cooperation in Europe (OSCE), the World Trade Organization (WTO), the Organisation for Economic Co-operation and Development (OECD), the European Organization for Nuclear Research (CERN) and part of the Visegrád Four (V4: Slovakia, Hungary, the Czech Republic, and Poland).

In 2020, Slovak citizens had visa-free or visa-on-arrival access to 181 countries and territories, ranking the Slovak passport 11th in the world.
Slovakia maintains diplomatic relations with 134 countries, primarily through its Ministry of Foreign Affairs. As of December 2013, Slovakia maintained 90 missions abroad, including 64 embassies, seven missions to multilateral organisations, nine consulates-general, one consular office, one Slovak Economic and Cultural Office and eight Slovak Institutes. There are 44 embassies and 35 honorary consulates in Bratislava.

Slovakia and the United States retain strong diplomatic ties and cooperate in the military and law enforcement areas. The U.S. Department of Defense programs has contributed significantly to Slovak military reforms. Hundreds of thousands of Americans have their roots in Slovakia, and many retain strong cultural and familial ties to the Slovak Republic. President Woodrow Wilson and the United States played a major role in the establishment of the original Czechoslovak state on 28 October 1918.

The Armed Forces of the Slovak Republic number 14,000 uniformed personnel. Slovakia joined NATO in March 2004. The country has been an active participant in US- and NATO-led military actions. There is a joint Czech-Slovak peacekeeping force in Kosovo. From 2006 the army transformed into a fully professional organisation and compulsory military service was abolished.

Slovak Ground Forces are made up of two active mechanised infantry brigades. The Air and Air Defence Forces comprise one wing of fighters, one wing of utility helicopters, and one SAM brigade. Training and support forces comprise a National Support Element (Multifunctional Battalion, Transport Battalion, Repair Battalion), a garrison force of the capital city Bratislava, as well as a training battalion, and various logistics and communication and information bases. Miscellaneous forces under the direct command of the General Staff include the 5th Special Forces Regiment.

The US State Department in 2017 reported:

The government generally respected the human rights of its citizens; however, there were problems in some areas. The most significant human rights issues included incidents of interference with privacy; corruption; widespread discrimination against Roma minority; and security force violence against ethnic and racial minorities government actions and rhetoric did little to discourage. The government investigated reports of abuses by members of the security forces and other government institutions, although some observers questioned the thoroughness of these investigations. Some officials engaged in corrupt practices with impunity. Two former ministers were convicted of corruption during the year.

Human rights in Slovakia are guaranteed by the Constitution of Slovakia from the year 1992 and by multiple international laws signed in Slovakia between 1948 and 2006.

According to the European Roma Rights Centre (ERRC), Romani people in Slovakia "endure racism in the job market, housing and education fields and are often subjected to forced evictions, vigilante intimidation, disproportionate levels of police brutality and more subtle forms of discrimination."

Slovakia is divided into 8 "kraje" (singular—"kraj", usually translated as "region"), each of which is named after its principal city. Regions have enjoyed a certain degree of autonomy since 2002. Their self-governing bodies are referred to as Self-governing (or autonomous) Regions (sg. "samosprávny kraj", pl. "samosprávne kraje") or Upper-Tier Territorial Units (sg. "vyšší územný celok", pl. "vyššie územné celky", abbr. VÚC).

The "kraje" are subdivided into many "okresy" (sg. "okres", usually translated as districts). Slovakia currently has 79 districts.

The "okresy" are further divided into "obce" (sg. "obec", usually translated as "municipality"). There are currently 2,890 municipalities.

In terms of economics and unemployment rate, the western regions are richer than eastern regions. Bratislava is the third-richest region of the European Union by GDP (PPP) per capita (after Hamburg and Luxembourg City); GDP at purchasing power parity is about three times higher than in other Slovak regions.

The Slovak economy is a developed, high-income economy, with the GDP per capita equalling 78% of the average of the European Union in 2018. The country has difficulties addressing regional imbalances in wealth and employment. GDP per capita ranges from 188% of EU average in Bratislava to 54% in Eastern Slovakia. Although regional income inequality is high, 90% of citizens own their homes.

The OECD in 2017 reported:

The Slovak Republic continues exhibiting robust economic performance, with strong growth backed by a sound financial sector, low public debt and high international competitiveness drawing on large inward investment.

In 2020, Slovakia was ranked by the International Monetary Fund as the 38th richest country in the world (out of 187 countries), with purchasing power parity per capita GDP of $38,321. The country used to be dubbed the "Tatra Tiger". Slovakia successfully transformed from a centrally planned economy to a market-driven economy. Major privatisations are completed, the banking sector is almost completely in private hands, and foreign investment has risen.
The Slovak economy is one of the fastest-growing economies in Europe and 3rd-fastest in eurozone (2017). In 2007, 2008 and 2010 (with GDP growth of 10.5%, 6% and 4%, retrospectively). In 2016, more than 86% of Slovak exports went to European Union, and more than 50% of Slovak imports came from other European Union member states.

The ratio of government debt to GDP in Slovakia reached 49.4% by the end of 2018, far below the OECD average.

Unemployment, peaking at 19% at the end of 1999, decreased to 4,9% in 2019, lowest recorded rate in Slovak history.

Slovakia adopted the Euro currency on 1 January 2009 as the 16th member of the Eurozone. The euro in Slovakia was approved by the European commission on 7 May 2008. The Slovak koruna was revalued on 28 May 2008 to 30.126 for 1 euro, which was also the exchange rate for the euro.
The Slovak government encourages foreign investment since it is one of the driving forces of the economy. Slovakia is an attractive country for foreign investors mainly because of its low wages, low tax rates, well educated labour force, favourable geographic location in the heart of Central Europe, strong political stability and good international relations reinforced by the country's accession to the European Union. Some regions, mostly at the east of Slovakia have failed to attract major investment, which has aggravated regional disparities in many economic and social areas. Foreign direct investment inflow grew more than 600% from 2000 and cumulatively reached an all-time high of $17.3 billion in 2006, or around $22,000 per capita by the end of 2008.

Slovakia ranks 45th out of 190 economies in terms of ease of doing business, according to the 2020 World Bank Doing Business Report and 57th out of the 63 countries in terms of competitive economy, according to the 2020 World Competitiveness Yearbook Report.

Although Slovakia's GDP comes mainly from the tertiary (services) sector, the industrial sector also plays an important role within its economy. The main industry sectors are car manufacturing and electrical engineering. Since 2007, Slovakia has been the world's largest producer of cars per capita, with a total of 1,090,000 cars manufactured in the country in 2018 alone. 275,000 people are employed directly and indirectly
by the automotive industry. There are currently four automobile assembly plants: Volkswagen's in Bratislava (models: Volkswagen Up, Volkswagen Touareg, Audi Q7, Audi Q8, Porsche Cayenne, Lamborghini Urus), PSA Peugeot Citroën's in Trnava (models: Peugeot 208, Citroën C3 Picasso), Kia Motors' Žilina Plant (models: Kia Cee'd, Kia Sportage, Kia Venga) and Jaguar Land Rover's in Nitra (model: Land Rover Discovery). Hyundai Mobis in Žilina is the largest suppliers for the automotive industry in Slovakia.

From electrical engineering companies, Foxconn has a factory at Nitra for LCD TV manufacturing, Samsung at Galanta for computer monitors and television sets manufacturing. Slovnaft based in Bratislava with 4,000 employees, is an oil refinery with a processing capacity of 5.5 - 6 million tonnes of crude oil, annually. Steel producer U. S. Steel in Košice is the largest employer in the east of Slovakia with 12,000 employees.
ESET is an IT security company from Bratislava with more than 1,000 employees worldwide at present. Their branch offices are in the United States, Ireland, United Kingdom, Argentina, the Czech Republic, Singapore and Poland. In recent years, service and high-tech-oriented businesses have prospered in Bratislava. Many global companies, including IBM, Dell, Lenovo, AT&T, SAP, and Accenture, have built outsourcing and service centres here. Reasons for the influx of multi-national corporations include proximity to Western Europe, skilled labour force and the high density of universities and research facilities. Other large companies and employers with headquarters in Bratislava include Amazon, Slovak Telekom, Orange Slovensko, Slovenská sporiteľňa, Tatra banka, Doprastav, Hewlett-Packard Slovakia, Henkel Slovensko, Slovenský plynárenský priemysel, Microsoft Slovakia, Mondelez Slovakia, Whirlpool Slovakia and Zurich Insurance Group Slovakia.

Bratislava's geographical position in Central Europe has long made Bratislava a crossroads for international trade traffic. Various ancient trade routes, such as the Amber Road and the Danube waterway, have crossed territory of present-day Bratislava. Today, Bratislava is the road, railway, waterway and airway hub.

In 2012, Slovakia produced a total of 28,393 GWh of electricity while at the same time consumed 28 786 GWh. The slightly higher level of consumption than the capacity of production (- 393 GWh) meant the country was not self-sufficient in energy sourcing. Slovakia imported electricity mainly from the Czech Republic (9,961 GWh—73.6% of total import) and exported mainly to Hungary (10,231 GWh—78.2% of total export).

Nuclear energy accounts for 53.8% of total electricity production in Slovakia, followed by 18.1% of thermal power energy, 15.1% by hydro power energy, 2% by solar energy, 9.6% by other sources and the rest 1.4% is imported.

The two nuclear power-plants in Slovakia are in Jaslovské Bohunice and Mochovce, each of them containing two operating reactors. Before the accession of Slovakia to the EU in 2004, the government agreed to turn-off the V1 block of Jaslovské Bohunice power-plant, built-in 1978. After deactivating the last of the two reactors of the V1 block in 2008, Slovakia stopped being self-dependent in energy production. Currently there is another block (V2) with two active reactors in Jaslovské Bohunice. It is scheduled for decommissioning in 2025. Two new reactors are under construction in Mochovce plant. The nuclear power production in Slovakia occasionally draws the attention of Austrian green-energy activists who organise protests and block the borders between the two countries.

There are four main highways D1 to D4 and eight expressways R1 to R8. Many of them are still under construction.

The D1 motorway connects Bratislava to Trnava, Nitra, Trenčín, Žilina and beyond, while the D2 motorway connects it to Prague, Brno and Budapest in the north–south direction. A large part of D4 motorway (an outer bypass), which should ease the pressure on Bratislava's highway system, is scheduled to open in 2020. The A6 motorway to Vienna connects Slovakia directly to the Austrian motorway system and was opened on 19 November 2007.

Slovakia has four international airports. Bratislava's M. R. Štefánik Airport is the main and largest international airport. It is located northeast of the city centre. It serves civil and governmental, scheduled and unscheduled domestic and international flights. The current runways support the landing of all common types of aircraft currently used. The airport has enjoyed rapidly growing passenger traffic in recent years; it served 279,028 passengers in 2000 and 2,292,712 in 2018. Košice International Airport is an airport serving Košice. It is the second-largest international airport in Slovakia. The Poprad–Tatry Airport is the third busiest airport, the airport is located 5 km east—northeast of ski resort town Poprad. It is an airport with one of the highest elevations in Central Europe, at 718 m, which is 150 m higher than Innsbruck Airport in Austria. The Sliač Airport is the smallest international airport and currently operates only summer charter flights to popular sea resort destinations.

Railways of Slovak Republic provides railway transport services on national and international lines.

The Port of Bratislava is one of the two international river ports in Slovakia. The port connects Bratislava to international boat traffic, especially the interconnection from the North Sea to the Black Sea via the Rhine-Main-Danube Canal.
Additionally, tourist boats operate from Bratislava's passenger port, including routes to Devín, Vienna and elsewhere. The Port of Komárno is the second largest port in Slovakia with an area of over 20 hectares and is located approximately 100 km east of Bratislava. It lies at the confluence of two rivers - the Danube and Váh.

Slovakia features natural landscapes, mountains, caves, medieval castles and towns, folk architecture, spas and ski resorts. More than 5,4 million tourists visited Slovakia in 2017, and the most attractive destinations are the capital of Bratislava and the High Tatras. Most visitors come from the Czech Republic (about 26%), Poland (15%) and Germany (11%).

Slovakia contains many castles, most of which are in ruins. The best known castles include Bojnice Castle (often used as a filming location), Spiš Castle, (on the UNESCO list), Orava Castle, Bratislava Castle, and the ruins of Devín Castle. Čachtice Castle was once the home of the world's most prolific female serial killer, the 'Bloody Lady', Elizabeth Báthory.
Slovakia's position in Europe and the country's past (part of the Kingdom of Hungary, the Habsburg monarchy and Czechoslovakia) made many cities and towns similar to the cities in the Czech Republic (such as Prague), Austria (such as Salzburg) or Hungary (such as Budapest). A historical centre with at least one square has been preserved in many towns. Large historical centers can be found in Bratislava, Trenčín, Košice, Banská Štiavnica, Levoča, and Trnava. Historical centres have been going through a restoration in recent years.

Historical churches can be found in virtually every village and town in Slovakia. Most of them are built in the Baroque style, but there are also many examples of Romanesque and Gothic architecture, for example Banská Bystrica, Bardejov and Spišská Kapitula. The Basilica of St. James in Levoča with the tallest wood-carved altar in the world and the Church of the Holy Spirit in Žehra with medieval frescos are UNESCO World Heritage Sites. The St. Martin's Concathedral in Bratislava served as the coronation church for the Kingdom of Hungary. The oldest sacral buildings in Slovakia stem from the Great Moravian period in the 9th century.

Very precious structures are the complete wooden churches of northern and northern-eastern Slovakia. Most were built from the 15th century onwards by Catholics, Lutherans and members of eastern-rite churches.

Tourism in Slovak Republic is one of the main sectors of the economy, but not using its whole capacity. It is based on internal tourism, where Slovaks spend holidays within the country. Major areas are: Bratislava and Vysoké Tatry. To other regions belong: Pieniny National Park, Malá Fatra NP, and Nízke Tatry NP.

There are many castles located throughout the country. To the biggest and the most beautiful ones belong: Spiš castle, Stará Ľubovňa castle, Kežmarok castle, Orava castle, Trenčín castle, Bratislava castle, and Devín castle. To the castle ruins belong Šariš castle, Gýmeš castle, Považský hrad (castle), and Strečno castle, where they filmed Braveheart movie.

Caves opened for public are mainly located in Northern Slovakia. In the south-west of the country only Jaskyňa Driny is opened to the public. The most popular ones are: Dobšinsá Ice Cave, Demänovská ľadová cave, Demänovská jaskyňa slobody, Belianska cave, and Domica cave. To the other caves which are opened belong Ochtinská aragonitová cave, Gombasecká cave, and Jasovská cave.

There are many spas throughout the whole country. The biggest and the most favorite center is Piešťany spa, where a big portion of visitors come from The Gulf countries, i.e. United Arab Emirates, Qatar, Kuwait, and Bahrain. To the other famous spas belong: Bardejovské kúpele, Trenčianske Teplice spa, Turčianske Teplice spa, and Spa Rajecké Teplice. There are many smaller ones: Kúpele Štós, Kúpele Číž, Kúpele Dudince, Kováčová, Kúpele Nimnica, Kúpele Smrdáky, Kúpele Lúčky, and Kúpele Vyšné Ružbachy with treatments against schisophrenia.

Typical souvenirs from Slovakia are dolls dressed in folk costumes, ceramic objects, crystal glass, carved wooden figures, črpáks (wooden pitchers), fujaras (a folk instrument on the UNESCO list) and valaškas (a decorated folk hatchet) and above all products made from corn husks and wire, notably human figures. Souvenirs can be bought in the shops run by the state organisation ÚĽUV ("Ústredie ľudovej umeleckej výroby"—Centre of Folk Art Production). "Dielo" shop chain sells works of Slovak artists and craftsmen. These shops are mostly found in towns and cities.

Prices of imported products are generally the same as in the neighbouring countries, whereas prices of local products and services, especially food, are usually lower.

The Slovak Academy of Sciences has been the most important scientific and research institution in the country since 1953. Slovaks have made notable scientific and technical contributions during history. Slovakia is currently in the negotiation process of becoming a member of the European Space Agency. Observer status was granted in 2010, when Slovakia signed the General Agreement on Cooperation in which information about ongoing education programmes was shared and Slovakia was invited to various negotiations of the ESA. In 2015, Slovakia signed the European Cooperating State Agreement based on which Slovakia committed to the finance entrance programme named PECS (Plan for the European Cooperating States) which serves as preparation for full membership. Slovak research and development organizations can apply for funding of projects regarding space technologies advancement. Full membership of Slovakia in the ESA is expected in 2020 after signing the ESA Convention. Slovakia will be obliged to set state budget inclusive ESA funding.

The population is over 5.4 million and consists mostly of Slovaks. The average population density is 110 inhabitants per km². According to the 2011 census, the majority of the inhabitants of Slovakia are Slovaks (80.7%). Hungarians are the largest ethnic minority (8.5%). Other ethnic groups include Roma (2%), Czechs (0.6%), Rusyns (0.6%) and others or unspecified (7.6%). Unofficial estimates on the Roma population are much higher, around 5.6%.

In 2018 the median age of the Slovak population was 41 years.

The largest waves of Slovak emigration occurred in the 19th and early 20th centuries. In the 1990 US census, 1.8 million people self-identified as having Slovak ancestry.

The official language is Slovak, a member of the Slavic language family. Hungarian is widely spoken in the southern regions, and Rusyn is used in some parts of the Northeast. Minority languages hold co-official status in the municipalities in which the size of the minority population meets the legal threshold of 15% in two consecutive censuses.

Slovakia is ranked among the top EU countries regarding the knowledge of foreign languages. In 2007, 68% of the population aged from 25 to 64 years claimed to speak two or more foreign languages, finishing 2nd highest in the European Union. The best known foreign language in Slovakia is Czech. Eurostat report also shows that 98.3% of Slovak students in the upper secondary education take on two foreign languages, ranking highly over the average 60.1% in the European Union. According to a Eurobarometer survey from 2012, 26% of the population have knowledge of English at a conversational level, followed by German (22%) and Russian (17%).

The deaf community uses the Slovak Sign Language. Even though spoken Czech and Slovak are similar, the Slovak Sign language is not particularly close to Czech Sign Language.

The Slovak constitution guarantees freedom of religion. In 2011, 62.0% of Slovaks identified themselves as Roman Catholics, 8.9% as Protestants, 3.8% as Greek Catholics, 0.9% as Orthodox, 13.4% identified themselves as atheists or non-religious, and 10.6% did not answer the question about their belief. In 2004, about one third of the church members regularly attended church services. The Slovak Greek Catholic Church is an Eastern rite sui iuris Catholic Church. Before World War II, an estimated 90,000 Jews lived in Slovakia (1.6% of the population), but most were murdered during the Holocaust. After further reductions due to postwar emigration and assimilation, only about 2,300 Jews remain today (0.04% of the population).

There are 18 state-registered religions in Slovakia, of which 16 are Christian, one is Jewish, and one is Bahá'í. In 2016, a two-third majority of the Slovak parliament passed a new bill that will obstruct Islam and other religious organisations from becoming state-recognised religions by doubling the minimum followers threshold from 25,000 to 50,000; however, Slovak president Andrej Kiska vetoed the bill. In 2010, there were an estimated 5,000 Muslims in Slovakia representing less than 0.1% of the country's population. Slovakia is the only member state of the European Union without a mosque.

The Programme for International Student Assessment, coordinated by the OECD, currently ranks Slovak secondary education the 30th in the world (placing it just below the United States and just above Spain).
Education in Slovakia is compulsory from age 6 to 16. The education system consists of elementary school which is divided into two parts, the first grade (age 6–10) and the second grade (age 10–15) which is finished by taking nationwide testing called Monitor, from Slovak language and math. Parents may apply for social assistance for a child that is studying on an elementary school or a high-school. If approved, the state provides basic study necessities for the child. Schools provide books to all their students with usual exceptions of books for studying a foreign language and books which require taking notes in them, which are mostly present in the first grade of elementary school.

After finishing elementary school, students are obliged to take one year in high school.

After finishing high school, students can go to university and are highly encouraged to do so. Slovakia has a wide range of universities. The biggest university is Comenius University, established in 1919. Although it's not the first university ever established on Slovak territory, it's the oldest university that is still running. Most universities in Slovakia are public funded, where anyone can apply. Every citizen has a right to free education in public schools.

Slovakia has several privately funded universities, however public universities consistently score better in the ranking than their private counterparts. Universities have different criteria for accepting students. Anyone can apply to any number of universities.

Folk tradition has rooted strongly in Slovakia and is reflected in literature, music, dance and architecture. The prime example is a Slovak national anthem, ""Nad Tatrou sa blýska"", which is based on a melody from ""Kopala studienku"" folk song.

The manifestation of Slovak folklore culture is the ""Východná"" Folklore Festival. It is the oldest and largest nationwide festival with international participation, which takes place in Východná annually. Slovakia is usually represented by many groups but mainly by SĽUK ("Slovenský ľudový umelecký kolektív—Slovak folk art collective"). SĽUK is the largest Slovak folk art group, trying to preserve the folklore tradition.

An example of wooden folk architecture in Slovakia can be seen in the well-preserved village of Vlkolínec which has been the UNESCO World Heritage Site since 1993. The Prešov Region preserves the world's most remarkable folk wooden churches. Most of them are protected by Slovak law as cultural heritage, but some of them are on the UNESCO list too, in Bodružal, Hervartov, Ladomirová and Ruská Bystrá.

The best known Slovak hero, found in many folk mythologies, is Juraj Jánošík (1688–1713) (the Slovak equivalent of Robin Hood). The legend says he was taking from the rich and giving to the poor. Jánošík's life was depicted in a list of literary works and many movies throughout the 20th century. One of the most popular is a film "Jánošík" directed by Martin Frič in 1935.

Visual art in Slovakia is represented through painting, drawing, printmaking, illustration, arts and crafts, sculpture, photography or conceptual art. The Slovak National Gallery founded in 1948, is the biggest network of galleries in Slovakia. Two displays in Bratislava are situated in Esterházy Palace ("Esterházyho palác") and the Water Barracks ("Vodné kasárne"), adjacent one to another. They are located on the Danube riverfront in the Old Town.

The Bratislava City Gallery, founded in 1961 is the second biggest Slovak gallery of its kind. It stores about 35,000 pieces of Slovak international art and offers permanent displays in Pálffy Palace and Mirbach Palace, located in the Old Town. Danubiana Art Museum, one of the youngest art museums in Europe, is situated near Čunovo waterworks (part of Gabčíkovo Waterworks). Other major galleries include: Andy Warhol Museum of Modern Art (Warhol's parents were from Miková), East Slovak Gallery, Ernest Zmeták Art Gallery, Zvolen Castle.

For a list of notable Slovak writers and poets, see List of Slovak authors.

Christian topics include poem Proglas as a foreword to the four Gospels, partial translations of the Bible into Old Church Slavonic, "Zakon sudnyj ljudem".

Medieval literature, in the period from the 11th to the 15th centuries, was written in Latin, Czech and Slovakised Czech. Lyric (prayers, songs and formulas) was still controlled by the Church, while epic was concentrated on legends. Authors from this period include Johannes de Thurocz, author of the Chronica Hungarorum and Maurus, both of them Hungarians. The worldly literature also emerged and chronicles were written in this period.

Two leading persons codified the Slovak language. The first was Anton Bernolák whose concept was based on the western Slovak dialect in 1787. It was the codification of the first-ever literary language of Slovaks. The second was Ľudovít Štúr, whose formation of the Slovak language took principles from the central Slovak dialect in 1843.

Slovakia is also known for its polyhistors, of whom include Pavol Jozef Šafárik, Matej Bel, Ján Kollár, and its political revolutionaries and reformists, such Milan Rastislav Štefánik and Alexander Dubček.

Traditional Slovak cuisine is based mainly on pork, poultry (chicken is the most widely eaten, followed by duck, goose, and turkey), flour, potatoes, cabbage, and milk products. It is relatively closely related to Hungarian, Czech, Polish and Austrian cuisine. On the east it is also influenced by Ukrainian, including Lemko and Rusyn. In comparison with other European countries, "game meat" is more accessible in Slovakia due to vast resources of forest and because hunting is relatively popular. Boar, rabbit, and venison are generally available throughout the year. Lamb and goat are eaten but are not widely popular.

The traditional Slovak meals are bryndzové halušky, bryndzové pirohy and other meals with potato dough and bryndza. Bryndza is a salty cheese made of sheep milk, characterised by a strong taste and aroma. Bryndzové halušky especially is considered a national dish, and is very commonly found on the menu of traditional Slovak restaurants.

A typical soup is a sauerkraut soup ("kapustnica"). A blood sausage called "krvavnica", made from any parts of a butchered pig is also a specific Slovak meal.

Wine is enjoyed throughout Slovakia. Slovak wine comes predominantly from the southern areas along the Danube and its tributaries; the northern half of the country is too cold and mountainous to grow grapevines. Traditionally, white wine was more popular than red or rosé (except in some regions), and sweet wine more popular than dry, but in recent years tastes seem to be changing. Beer (mainly of the pilsener style, though dark lagers are also consumed) is also popular.

Sporting activities are practised widely in Slovakia, many of them on a professional level. Ice hockey and football have traditionally been regarded as the most popular sports in Slovakia, though tennis, handball, basketball, volleyball, whitewater slalom, cycling and athletics are also popular.


One of the most popular team sports in Slovakia is ice hockey. Slovakia became a member of the IIHF on 2 February 1993 and since then has won 4 medals in Ice Hockey World Championships, consisting of 1 gold, 2 silver and 1 bronze. The most recent success was a silver medal at the 2012 IIHF World Championship in Helsinki. The Slovak national hockey team made five appearances in the Olympic games, finishing 4th in the 2010 Winter Olympics in Vancouver. The country has 8,280 registered players and is ranked 7th in the IIHF World Ranking at present. Before 2012, the Slovak team HC Slovan Bratislava participated in the Kontinental Hockey League, considered the strongest hockey league in Europe, and the second-best in the world.

Slovakia hosted the 2011 IIHF World Championship, where Finland won the gold medal and 2019 IIHF World Championship, where Finland also won the gold medal. Both competitions took place in Bratislava and Košice.


Association football is the most popular sport in Slovakia, with over 400,000 registered players. Since 1993, the Slovak national football team has qualified for the FIFA World Cup once, in 2010. They progressed to the last 16, where they were defeated by the Netherlands. The most notable result was the 3–2 victory over Italy. In 2016, the Slovak national football team qualified for the UEFA Euro 2016 tournament, under head coach Ján Kozák. This helped the team reach its best-ever position of 14th in the FIFA World Rankings.

In club competitions, only three teams have qualified for the UEFA Champions League Group Stage, namely MFK Košice in 1997–98, FC Artmedia Bratislava in 2005–06 season, and MŠK Žilina in 2010–11. FC Artmedia Bratislava has been the most successful team, finishing 3rd at the Group Stage of the UEFA Cup, therefore qualifying for the knockout stage. They remain the only Slovak club that has won a match at the group stage.






</doc>
<doc id="26833" url="https://en.wikipedia.org/wiki?curid=26833" title="Scientific method">
Scientific method

The scientific method is an empirical method of acquiring knowledge that has characterized the development of science since at least the 17th century. It involves careful observation, applying rigorous skepticism about what is observed, given that cognitive assumptions can distort how one interprets the observation. It involves formulating hypotheses, via induction, based on such observations; experimental and measurement-based testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings. These are "principles" of the scientific method, as distinguished from a definitive series of steps applicable to all scientific enterprises.

Though diverse models for the scientific method are available, there is in general a continuous process that includes observations about the natural world. People are naturally inquisitive, so they often come up with questions about things they see or hear, and they often develop ideas or hypotheses about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways. The most conclusive testing of hypotheses comes from reasoning based on carefully controlled experimental data. Depending on how well additional tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported, a general theory may be developed.

Although procedures vary from one field of inquiry to another, they are frequently the same from one to another. The process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture, based on knowledge obtained while seeking answers to the question. The hypothesis might be very specific, or it might be broad. Scientists then test hypotheses by conducting experiments or studies. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.

The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis. Experiments can take place anywhere from a garage to CERN's Large Hadron Collider. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order.

Important debates in the history of science concern rationalism, especially as advocated by René Descartes; inductivism and/or empiricism, as argued for by Francis Bacon, and rising to particular prominence with Isaac Newton and his followers; and hypothetico-deductivism, which came to the fore in the early 19th century.

The term "scientific method" emerged in the 19th century, when a significant institutional development of science was taking place and terminologies establishing clear boundaries between science and non-science, such as "scientist" and "pseudoscience", appeared. Throughout the 1830s and 1850s, by which time Baconianism was popular, naturalists like William Whewell, John Herschel, John Stuart Mill engaged in debates over "induction" and "facts" and were focused on how to generate knowledge. In the late 19th and early 20th centuries, a debate over realism vs. antirealism was conducted as powerful scientific theories extended beyond the realm of the observable.

The term "scientific method" came into popular use in the twentieth century, popping up in dictionaries and science textbooks, although there was little scientific consensus over its meaning. Although there was a growth through the middle of the twentieth century, by the 1960s and 1970s numerous influential philosophers of science such as Thomas Kuhn and Paul Feyerabend had questioned the universality of the "scientific method" and in doing so largely replaced the notion of science as a homogeneous and universal method with that of it being a heterogeneous and local practice. In particular, Paul Feyerabend, in the 1975 first edition of his book "Against Method", argued against there being any universal rules of science. Later examples include physicist Lee Smolin's 2013 essay "There Is No Scientific Method" and historian of science Daniel Thurs's chapter in the 2015 book "Newton's Apple and Other Myths about Science", which concluded that the scientific method is a myth or, at best, an idealization. Philosophers Robert Nola and Howard Sankey, in their 2007 book "Theories of Scientific Method", said that debates over scientific method continue, and argued that Feyerabend, despite the title of "Against Method", accepted certain rules of method and attempted to justify those rules with a metamethodology.

The scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time. This model can be seen to underlie the scientific revolution.

The ubiquitous element in scientific method is empiricism. This is in opposition to stringent forms of rationalism: the scientific method embodies that reason alone cannot solve a particular scientific problem. A strong formulation of the scientific method is not always aligned with a form of empiricism in which the empirical data is put forward in the form of experience or other abstracted forms of knowledge; in current scientific practice, however, the use of scientific modelling and reliance on abstract typologies and theories is normally accepted. The scientific method is of necessity also an expression of an opposition to claims that e.g. revelation, political or religious dogma, appeals to tradition, commonly held beliefs, common sense, or, importantly, currently held theories, are the only possible means of demonstrating truth.

Different early expressions of empiricism and the scientific method can be found throughout history, for instance with the ancient Stoics, Epicurus, Alhazen, Roger Bacon, and William of Ockham. From the 16th century onwards, experiments were advocated by Francis Bacon, and performed by Giambattista della Porta, Johannes Kepler, and Galileo Galilei. There was particular development aided by theoretical works by Francisco Sanches, John Locke, George Berkeley, and David Hume.

The hypothetico-deductive model formulated in the 20th century, is the ideal although it has undergone significant revision since first proposed (for a more formal discussion, see below). Staddon (2017) argues it is a mistake to try following rules which are best learned through careful study of examples of scientific investigation.

The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, these actions are better considered as general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always done in the same order. As noted by scientist and philosopher William Whewell (1794–1866), "invention, sagacity, [and] genius" are required at every step.

The question can refer to the explanation of a specific observation, as in "Why is the sky blue?" but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves finding and evaluating evidence from previous experiments, personal scientific observations or assertions, as well as the work of other scientists. If the answer is already known, a different question that builds on the evidence can be posed. When applying the scientific method to research, determining a good question can be very difficult and it will affect the outcome of the investigation.

A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain any given behavior. The hypothesis might be very specific; for example, Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein", or it might be broad; for example, unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about a given statistical population. For example, the population might be "people with a particular disease." The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false; for example, that the new drug does nothing and that any cure is caused by chance. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.

This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).

This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk. Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.

This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence or to build up many answers to highly specific questions in order to answer a single broader question.

The basic elements of the scientific method are illustrated by the following example from the discovery of the structure of DNA:

The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.

The scientific method also includes other components required even when all the iterations of the steps above have been completed:

If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.
Replication has become a contentious issue in social and biomedical science where treatments are administered to groups of individuals. Typically an "experimental group" gets the treatment, such as drug, and the "control group" gets a placebo. John Ioannidis in 2005 pointed out that the method being used has led to many findings that cannot be replicated.

The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer-review does not certify the correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.

Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others. Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.

Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that scientists can use to
predict the results of future experiments. This allows scientists to gain a better understanding of the topic under study, and later to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most successful explanations – those which explain and make accurate predictions in a wide range of circumstances – are often called scientific theories.

Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding typically result from a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question proves more powerful than its alternatives at explaining the evidence. Often subsequent researchers re-formulate the explanations over time, or combined explanations to produce new explanations.

Tow sees the scientific method in terms of an evolutionary algorithm applied to science and technology.

Scientific knowledge is closely tied to empirical findings and can remain subject to falsification if new experimental observations are incompatible with what is found. That is, no theory can ever be considered final since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to relate to how long it has persisted without major alteration to its core principles.

Theories can also become subsumed by other theories. For example, Newton's laws explained thousands of years of scientific observations of the planets . However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicted and explained other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.

Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors. For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world; its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.

Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy.

The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).

A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touch the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.

Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true. Poorly attested beliefs can be believed and acted upon via a less rigorous heuristic.

Goldhaber and Nieto published in 2010 the observation that if theoretical structures with "many closely neighboring subjects are described by connecting theoretical concepts, then the theoretical structure acquires a robustness which makes it increasingly hard—though certainly never impossible—to overturn". When a narrative is constructed its elements become easier to believe. For more on the narrative fallacy, see also : "Words and ideas are originally phonetic and mental equivalences of the experiences coinciding with them. ... Such proto-ideas are at first always too broad and insufficiently specialized. ... Once a structurally complete and closed system of opinions consisting of many details and relations has been formed, it offers enduring resistance to anything that contradicts it." Sometimes, these have their elements assumed "a priori", or contain some other logical or methodological flaw in the process that ultimately produced them. Donald M. MacKay has analyzed these elements in terms of limits to the accuracy of measurement and has related them to instrumental elements in a category of measurement.

There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.

The scientific method is an iterative, cyclical process through which information is continually revised. It is generally recognized to develop advances in knowledge through the following elements, in varying combinations or contributions:

Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as "the scientific method".

The scientific method is not a single recipe: it requires intelligence, imagination, and creativity. In this sense, it is not a mindless set of standards and procedures to follow,
but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's "Principia". On the contrary, if the astronomically massive, the feather-light, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase confidence in Newton's work.

A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:


The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.

While this schema outlines a typical hypothesis/testing method, a number of philosophers, historians, and sociologists of science, including Paul Feyerabend, claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.

The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The "subjects" can also be called or the "unknowns".) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.

The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.

Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.

Measurements demand the use of "operational definitions" of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or "idealized" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.

The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.

New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define , space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood. In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.

 The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's "transforming principle"). But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. .."2. DNA-hypotheses"

The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Albert Einstein as a possible early test of his theory of General relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.

A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.

Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.

Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Albert Einstein once observed that "there is no logical bridge between phenomena and their theoretical principles." Charles Sanders Peirce, borrowing a page from Aristotle ("Prior Analytics", 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as "abductive reasoning". The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.

William Glen observes that

In general scientists tend to look for theories that are "elegant" or "beautiful". Scientists often use these terms to refer to a theory that is in accordance with the known facts, but is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.

To minimize the confirmation bias which results from entertaining a single hypothesis, strong inference emphasizes the need for entertaining multiple alternative hypotheses.

 Linus Pauling proposed that DNA might be a triple helix. This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) "..3. DNA-predictions"

Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.

It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.

If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. For example, while a hypothesis on the existence of other intelligent species may be convincing with scientifically based speculation, there is no known experiment that can test this hypothesis. Therefore, science itself can have little to say about the possibility. In the future, a new technique may allow for an experimental test and the speculation would then become part of accepted science.

 James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'. This prediction followed from the work of Cochran, Crick and Vand (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.

In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material". " ..4. DNA-experiments"

Einstein's theory of general relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.

Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is. Factor analysis is one technique for discovering the important factor in an effect.

Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment that tests the aerodynamical hypotheses used for constructing the plane.

Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record-keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).

 Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape and was able to confirm the structure was helical. This rekindled Watson and Crick's model building and led to the correct structure. "..1. DNA-characterizations"

The scientific method is iterative. At any stage, it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.

Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.

 After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts, Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it. They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. .."DNA Example"

Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.

To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including "Nature" and "Science", have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.

The classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.

The hypothetico-deductive model or method is a proposed description of scientific method. Here, predictions from the hypothesis are central: if you assume the hypothesis to be true, what consequences follow?

If subsequent empirical investigation does not demonstrate that these consequences or predictions correspond to the observable world, the hypothesis can be concluded to be false.

In 1877, Charles Sanders Peirce (1839–1914) characterized inquiry in general not as the pursuit of truth "per se" but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless. He outlined four methods of settling opinion, ordered from least to most successful:

Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth "per se" but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.

For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators "would" reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points. In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge. As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.

Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument" except as otherwise noted):

Science applied to complex systems can involve elements such as transdisciplinarity, systems theory and scientific modelling. The Santa Fe Institute studies such systems; Murray Gell-Mann interconnects these topics with message passing.

In general, the scientific method may be difficult to apply stringently to diverse, interconnected systems and large data sets. In particular, practices used within Big data, such as predictive analytics, may be considered to be at odds with the scientific method.

Frequently the scientific method is employed not only by a single person but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.

Scientific journals use a process of "peer review", in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three, and usually anonymous) fellow scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer-review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.

Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.

Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.

When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.

Since it is impossible for a scientist to record "everything" that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.

Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world. These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized.

Thomas Kuhn examined the history of science in his "The Structure of Scientific Revolutions", and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.

Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory-laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description. He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sun rise despite the same physiological phenomenon. Kuhn and Feyerabend acknowledge the pioneering significance of his work.

Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed".

Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book "Against Method" he argues that scientific progress is "not" the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'. Criticisms such as his led to the strong programme, a radical approach to the sociology of science.

The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.

In anthropology and sociology, following the field research in an academic scientific laboratory by Latour and Woolgar, Karin Knorr Cetina has conducted a comparative study of two scientific fields (namely high energy physics and molecular biology) to conclude that the epistemic practices and reasonings within both scientific communities are different enough to introduce the concept of "epistemic cultures", in contradiction with the idea that a so-called "scientific method" is unique and a unifying concept.

Somewhere between 33% and 50% of all scientific discoveries are estimated to have been "stumbled upon", rather than sought out. This may explain why scientists so often express that they were lucky. Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected. This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.

Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they "think" is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.

Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines try to distinguish what is "known" from what is "unknown" at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be "falsifiable" (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.

Mathematical work and scientific work can inspire each other. For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).

Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences", is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well-known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.

George Pólya's work on problem solving, the construction of mathematical proofs, and heuristic show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.

In Pólya's view, "understanding" involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; "analysis", which Pólya takes from Pappus, involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; "synthesis" is the strict Euclidean exposition of step-by-step details of the proof; "review" involves reconsidering and re-examining the result and the path taken to it.

Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).

Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work. In like manner to science, where truth is sought, but certainty is not found, in "Proofs and refutations" (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré ("Proofs and Refutations", 1976).)

Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In "Proofs and Refutations", Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.

When the scientific method employs statistics as part of its arsenal, there are mathematical and practical issues that can have a deleterious effect on the reliability of the output of scientific methods. This is described in a popular 2005 scientific paper "Why Most Published Research Findings Are False" by John Ioannidis, which is considered foundational to the field of metascience. Much research in metascience seeks to identify poor use of statistics and improve its use.

The particular points raised are statistical ("The smaller the studies conducted in a scientific field, the less likely the research findings are to be true" and "The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.") and economical ("The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true" and "The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.") Hence: "Most research findings are false for most research designs and for most fields" and "As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings." However: "Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds," which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries.



</doc>
<doc id="26838" url="https://en.wikipedia.org/wiki?curid=26838" title="Shotgun">
Shotgun

A shotgun (also known as a scattergun, or historically as a fowling piece) is a firearm that is usually designed to be fired from the shoulder, which uses the energy of a fixed shell to fire a number of small spherical pellets called shot, or a solid projectile called a slug. Shotguns come in a wide variety of sizes, ranging from 5.5 mm (.22 inch) bore up to bore, and in a range of firearm operating mechanisms, including breech loading, single-barreled, double or combination gun, pump-action, bolt-, and lever-action, revolver, semi-automatic, and even fully automatic variants.

A shotgun was originally a smoothbore firearm, which means that the inside of the barrel is not rifled but later rifled shotgun barrels and slugs become available. Preceding smoothbore firearms, such as the musket, were widely used by armies in the 18th century. The direct ancestor to the shotgun, the blunderbuss, was also used in a similar variety of roles from self-defense to riot control. It was often used by cavalry troops because of its generally shorter length and ease of use, as well as by coachmen for its substantial power. In the 19th century, however, these weapons were largely replaced on the battlefield with breechloading rifled firearms, which were more accurate over longer ranges. The military value of shotguns was rediscovered in the First World War, when American forces used 12-gauge pump action shotguns in close-quarters trench fighting to great effect. Since then, it has been used in a variety of roles in civilian, law enforcement, and military applications.

The shot pellets from a shotgun spread upon leaving the barrel, and the power of the burning charge is divided among the pellets, which means that the energy of any one ball of shot is fairly low. In a hunting context, this makes shotguns useful primarily for hunting birds and other small game. However, in a military or law enforcement context, the large number of projectiles makes the shotgun useful as a close quarters combat weapon or a defensive weapon. Militants or insurgents may use shotguns in asymmetric engagements, as shotguns are commonly owned civilian weapons in many countries. Shotguns are also used for target shooting sports such as skeet, trap, and sporting clays. These involve shooting clay disks, known as clay pigeons, thrown in various ways.

Shotguns come in a wide variety of forms, from very small up to massive punt guns, and in nearly every type of firearm operating mechanism. The common characteristics that make a shotgun unique center on the requirements of firing shot. These features are the features typical of a shotgun shell, namely a relatively short, wide cartridge, with straight walls, and operating at a relatively low pressure.

Ammunition for shotguns is referred to in the US as shotgun shells, shotshells, or just shells (when it is not likely to be confused with artillery shells). The term cartridges is standard usage in the United Kingdom.

The shot is usually fired from a smoothbore barrel; another configuration is the rifled slug barrel, which fires more accurate solitary projectiles.

The typical use of a shotgun is against small and fast moving targets, often while in the air. The spreading of the shot allows the user to point the shotgun close to the target, rather than having to aim precisely as in the case of a single projectile. The disadvantages of shot are limited range and limited penetration of the shot, which is why shotguns are used at short ranges, and typically against smaller targets. Larger shot sizes, up to the extreme case of the single projectile slug load, result in increased penetration, but at the expense of fewer projectiles and lower probability of hitting the target.

Aside from the most common use against small, fast moving targets, the shotgun has several advantages when used against still targets. First, it has enormous stopping power at short range, more than nearly all handguns and many rifles. Though many believe the shotgun is a great firearm for inexperienced shooters, the truth is, at close range, the spread of shot is not very large at all, and competency in aiming is still required. A typical self-defense load of buckshot contains 8–27 large lead pellets, resulting in many wound tracks in the target. Also, unlike a fully jacketed rifle bullet, each pellet of shot is less likely to penetrate walls and hit bystanders (though in the case of traditional 00-Buck, overpenetration of soft and hard targets may be an issue). It is favored by law enforcement for its low penetration and high stopping power.

On the other hand, the hit potential of a defensive shotgun is often overstated. The typical defensive shot is taken at very close ranges, at which the shot charge expands no more than a few centimeters. This means the shotgun must still be aimed at the target with some care. Balancing this is the fact that shot spreads further upon entering the target, and the multiple wound channels are far more likely to produce a disabling wound than a rifle or handgun.

Some of the most common uses of shotguns are the sports of skeet shooting, trap shooting, and sporting clays. These involve shooting clay discs, also known as clay pigeons, thrown in by hand and by machine. Both skeet and trap competitions are featured at the Olympic Games.

The shotgun is popular for bird hunting (called "game-shooting" in the UK, where "hunting" refers to hunting mammals with a pack of hounds), it is also used for more general forms of hunting especially in semi-populated areas where the range of rifle bullets may pose a hazard. Use of a smooth bore shotgun with a rifled slug or, alternatively, a rifled barrel shotgun with a sabot slug, improves accuracy to or more. This is well within the range of the majority of kill shots by experienced hunters using shotguns.

However, given the relatively low muzzle velocity of slug ammunition, typically around 500 m/s (about 1600 feet per second), and the blunt, poorly streamlined shape of typical slugs (which cause them to lose velocity very rapidly, compared to rifle bullets), a hunter must pay close attention to the ballistics of the particular ammunition used to ensure an effective and humane kill shot.

At any reasonable range, shotgun slugs make effective lethal wounds due to their tremendous mass, reducing the length of time that an animal might suffer. For example, a typical 12 gauge shotgun slug is a blunt piece of metal that could be described as an 18 mm (.729 inch) caliber that weighs 28 grams (432 grains). For comparison, a common deer-hunting rifle round is a 7.62 mm (.308 inch) slug weighing 9.7 grams (150 grains), but the dynamics of the rifle cartridge allow for a different type of wound, and a much further reach.

Shotguns are often used with rifled barrels in locations where it is not lawful to hunt with a rifle. Typically, a sabot slug is used in these barrels for maximum accuracy and performance. Shotguns are often used to hunt whitetail deer in the thick brush and briers of the Southeastern and upper Midwestern United States, where, due to the dense cover, ranges tend to be close – 25m or less.

Sabot slugs are essentially very large hollow point bullets, and are streamlined for maximum spin and accuracy when shot through a rifled barrel. They have greater ranges than older Foster and Brenneke-type slugs.

People often use semiautomatic or pump action shotguns for hunting waterfowl to small game.

In the US and Canada, shotguns are widely used as a support weapon by police forces. One of the rationales for issuing shotguns is that, even without much training, an officer will probably be able to hit targets at close to intermediate range, due to the "spreading" effect of buckshot. This is largely a myth, as the spread of buckshot at 25 feet averages 8 inches, which is still very capable of missing a target. Some police forces are replacing shotguns in this role with carbine rifles such as AR-15s. Shotguns are also used in roadblock situations, where police are blocking a highway to search cars for suspects. In the US, law enforcement agencies often use riot shotguns, especially for crowd and riot control where they may be loaded with less-lethal rounds such as rubber bullets or bean bags. Shotguns are also often used as breaching devices to defeat locks.

Shotguns are common weapons in military use, particularly for special purposes. Shotguns are found aboard naval vessels for shipboard security, because the weapon is very effective at close range as a way of repelling enemy boarding parties. In a naval setting, stainless steel shotguns are often used, because regular steel is more prone to corrosion in the marine environment. Shotguns are also used by military police units. U.S. Marines have used shotguns since their inception at the squad level, often in the hands of NCOs, while the U.S. Army often issued them to a squad's point man. Shotguns were modified for and used in the trench warfare of WWI, in the jungle combat of WWII and the Vietnam War. Shotguns were also used in the Iraq War, being popular with soldiers in urban combat environments. Some U.S. units in Iraq used shotguns with special frangible breaching rounds to blow the locks off doors when making a surprise entry into a dwelling.

Shotguns are a popular means of home defense for many of the same reasons they are preferred for close-quarters tasks in law enforcement and the military.

Compared to handguns, shotguns are heavier, larger, and not as maneuverable in close quarters (which also presents a greater retention problem), but do have these advantages:

The wide range of forms the shotgun can take leads to some significant differences between what is technically a shotgun and what is legally considered a shotgun. A fairly broad attempt to define a shotgun is made in the United States Code (18 USC 921), which defines the shotgun as "a weapon designed or redesigned, made or remade, and intended to be fired from the shoulder, and designed or redesigned and made or remade to use the energy of the explosive in a fixed shotgun shell to fire through a smooth bore either a number of ball shot or a single projectile for each single pull of the trigger." It is even more broadly defined in English law: "a smooth bore gun not being an air gun" (s.1(3)(a) Firearms Act 1968).

A rifled slug, with finned rifling designed to enable the projectile to be safely fired through a choked barrel, is an example of a single projectile. Some shotguns have rifled barrels and are designed to be used with a "saboted" bullet, one which is typically encased in a two-piece plastic ring ("sabot") designed to peel away after it exits the barrel, leaving the bullet, now spinning after passing through the rifled barrel, to continue toward the target. These shotguns, although they have rifled barrels, still use a shotgun-style shell instead of a rifle cartridge and may in fact still fire regular multipellet shotgun shells, but the rifling in the barrel will affect the shot pattern. The use of a rifled barrel blurs the distinction between rifle and shotgun. Hunting laws may differentiate between smooth barreled and rifled barreled guns.

Combat shotgun is a shotgun designed for offensive purposes, typically for the military.

Riot shotgun has long been a synonym for a shotgun, especially a short-barrelled shotgun. During the 19th and early 20th century, these were used to disperse protesters, rioters and revolutionaries. The wide spray of the shot ensured a large group would be hit, but the light shot would ensure more wounds than fatalities. When the ground was paved, police officers would often ricochet the shot off the ground, slowing down the shot and spreading pattern even further. To this day specialized police and defensive shotguns are called riot shotguns. The introduction of rubber bullets and bean bag rounds ended the practice of using shot for the most part, but riot shotguns are still used to fire a variety of less-lethal rounds for riot control.

A sawed-off shotgun (or "sawn-off") refers to a shotgun whose barrel has been shortened, leaving it more maneuverable, easier to use at short range and more readily concealed. Many countries establish a legal minimum barrel length that precludes easy concealment (this length is in the U.S. and 24 inches in the UK). The sawed-off shotgun is sometimes known as a "lupara" (in Italian a generic reference to the word ""lupo"" ("wolf")) in Southern Italy and Sicily.

Coach guns are similar to sawn-off shotguns, except they are manufactured with a 46 cm (18") barrel and are legal for civilian ownership in some jurisdictions. Coach guns are also more commonly associated with the American Old West or Australian Colonial period, and often used for hunting in bush, scrub, or marshland where a longer barrel would be unwieldy or impractical.

Snake Charmer shotguns are commonly used by gardeners and farmers for pest control. They have short barrels and either a full-size stocks or pistol grips, depending on legislation in intended markets. The overall length of these weapons is frequently less than , with some measuring up at less than . These weapons are typically single-shot break-action .410 "gauge" (caliber), which may or may not hold extra shot-shells in the butt-stock. They typically have a cylinder bore and sometimes are available in modified choke as well. Snake Charmers are popular for "home defense" purposes and as "survival" weapons.

Other examples include a variety of .410 / rifle "survival" guns manufactured in over/under designs. In the combination gun arrangement, a rimfire or centrefire rifle barrel is located beneath the barrel of a .410 gauge shotgun. Generally, there is one manually cocked external hammer and an external selection lever to select which caliber of cartridge to fire. A notable example is the Springfield Armory M6 Scout, a .410 / .22 issued to United States Air Force personnel as a "survival" gun in the event of a forced landing or accident in a wilderness area. Variants have been used by Israeli, Canadian, and American armed forces. Shotgun-rifle combination guns with two, three, and occasionally even four barrels are available from a number of makers, primarily European. These provided flexibility, enabling the hunter to effectively shoot at flushing birds or more distant small mammals while only carrying one gun.

Most early firearms, such as the blunderbuss, arquebus, and musket had large diameter, smoothbore barrels, and could fire shot as well as solid balls. A firearm intended for use in wing shooting of birds was known as a fowling piece. The 1728 " Cyclopaedia" defines a "fowling piece" as:

For example, the Brown Bess musket, in service with the British army from 1722 to 1838, had a 19 mm (.75 inch) smoothbore barrel, roughly the same as a 10 gauge shotgun, and was long, just short of the above recommended 168 cm (5 feet). On the other hand, records from the Plymouth colony show a maximum length of 137 cm (4 feet) for fowling pieces, shorter than the typical musket.

Shot was also used in warfare; the buck and ball loading, combining a musket ball with three or six buckshot, was used throughout the history of the smoothbore musket. The first recorded use of the term "shotgun" was in 1776 in Kentucky. It was noted as part of the "frontier language of the West" by James Fenimore Cooper.

With the adoption of smaller bores and rifled barrels, the shotgun began to emerge as a separate entity. Shotguns have long been the preferred method for sport hunting of birds, and the largest shotguns, the punt guns, were used for commercial hunting. The double-barreled shotgun has changed little since the development of the boxlock action in 1875. Modern innovations such as interchangeable chokes and subgauge inserts make the double-barreled shotgun the shotgun of choice in skeet, trap shooting, and sporting clays, as well as with many hunters.

As wing shooting has been a prestige sport, specialty gunsmiths such as Krieghoff or Perazzi have produced fancy double-barrel guns for wealthy European and American hunters. These weapons can cost US$5,000 or more; some elaborately decorated presentation guns have sold for up to US$100,000.

During its long history, the shotgun has been favored by bird hunters, guards, and law enforcement officials. The shotgun has fallen in and out of favor with military forces several times in its long history. Shotguns and similar weapons are simpler than long-range rifles, and were developed earlier. The development of more accurate and deadlier long-range rifles minimized the usefulness of the shotgun on the open battlefields of European wars. But armies have "rediscovered" the shotgun for specialty uses many times.

During the 19th century, shotguns were mainly employed by cavalry units. Both sides of the American Civil War employed shotguns. U.S. cavalry used the shotgun extensively during the Indian Wars in the latter half of the 19th century. Mounted units favored the shotgun for its moving target effectiveness, and devastating close-range firepower. The shotgun was also favored by citizen militias and similar groups.

With the exception of cavalry units, the shotgun saw less and less use throughout the 19th century on the battlefield. As a defense weapon it remained popular with guards and lawmen, however, and the shotgun became one of many symbols of the American Old West. Lawman Cody Lyons killed two men with a shotgun; his friend Doc Holliday's only confirmed kill was with a shotgun. The weapon both these men used was the short-barreled version favored by private strongbox guards on stages and trains. These guards, called express messengers, became known as shotgun messengers, since they rode with the weapon (loaded with buckshot) for defense against bandits. Passenger carriages carrying a strongbox usually had at least one private guard armed with a shotgun riding in front of the coach, next to the driver. This practice has survived in American slang; the term "riding shotgun" is used for the passenger who sits in the front passenger seat. The shotgun was a popular weapon for personal protection in the American Old West, requiring less skill on the part of the user than a revolver.

The origins of the hammerless shotgun are European but otherwise obscure. The earliest breechloading shotguns originated in France and Belgium in the early 19th century (see also the history of the Pinfire) and a number of them such as those by Robert and Chateauvillard from the 1830s and 1840s did not use hammers. In fact during these decades a wide variety of ingenious weapons, including rifles, adopted what is now often known as a 'needle-fire' method of igniting the charge, where a firing pin or a longer sharper needle provided the necessary impact. The most widely used British hammerless needle-fire shotgun was the unusual hinged-chamber fixed-barrel breech-loader by Joseph Needham, produced from the 1850s. By the 1860s hammerless guns were increasingly used in Europe both in war and sport although hammer guns were still very much in the majority. The first significant encroachment on hammer guns was a hammerless patent which could be used with a conventional side-lock. This was British gunmaker T Murcott's 1871 action nicknamed the 'mousetrap' on account of its loud snap action. However, the most successful hammerless innovation of the 1870s was Anson and Deeley's boxlock patent of 1875. This simple but ingenious design only used four moving parts allowing the production of cheaper and reliable shotguns.

Daniel Myron LeFever is credited with the invention of the American hammerless shotgun. Working for Barber & LeFever in Syracuse, N.Y. he introduced his first hammerless shotgun in 1878. This gun was cocked with external cocking levers on the side of the breech. He went on to patent the first truly automatic hammerless shotgun in 1883. This gun automatically cocked itself when the breech was closed. He later developed the mechanism to automatically eject the shells when the breech was opened.

One of the men most responsible for the modern development of the shotgun was prolific gun designer John Browning. While working for Winchester Firearms, Browning revolutionized shotgun design. In 1887, Browning introduced the Model 1887 Lever Action Repeating Shotgun, which loaded a fresh cartridge from its internal magazine by the operation of the action lever. Before this time most shotguns were the 'break open' type.

This development was greatly overshadowed by two further innovations he introduced at the end of the 19th century. In 1893, Browning produced the Model 1893 Pump Action Shotgun, introducing the now familiar pump action to the market. And in 1900, he patented the Browning Auto-5, America's first semi-automatic shotgun. The first semi-automatic shotgun in the world was patented in 1891-1893 by the Clair brothers of France. The Browning Auto-5 remained in production until 1998.

The decline in military use of shotguns reversed in World War I. American forces under General Pershing employed 12-gauge pump action shotguns when they were deployed to the Western front in 1917. These shotguns were fitted with bayonets and a heat shield so the barrel could be gripped while the bayonet was deployed. Shotguns fitted in this fashion became known as "trench guns" by the United States Army. Those without such modifications were known as "riot guns". After World War I, the United States military began referring to all shotguns as "riot guns".

Due to the cramped conditions of trench warfare, the American shotguns were extremely effective. Germany even filed an official diplomatic protest against their use, alleging they violated the laws of warfare. The judge advocate general reviewed the protest, and it was rejected because the Germans protested use of lead shot (which would have been illegal) but military shot was plated. This is the only occasion the legality of the shotgun's use in warfare has been questioned.

During World War II, the shotgun was not heavily used in the war in Europe by official military forces. However, the shotgun was a favorite weapon of Allied-supported partisans, such as the French Resistance. By contrast, in the Pacific theater, thick jungles and heavily fortified positions made the shotgun a favorite weapon of the United States Marines. Marines tended to use pump shotguns, since the pump action was less likely to jam in the humid and dirty conditions of the Pacific campaign. Similarly, the United States Navy used pump shotguns to guard ships when in port in Chinese harbors (e.g., Shanghai). The United States Army Air Forces also used pump shotguns to guard bombers and other aircraft against saboteurs when parked on airbases across the Pacific and on the West Coast of the United States. Pump and semi-automatic shotguns were used in marksmanship training, particularly for bomber gunners. The most common pump shotguns used for these duties were the 12 gauge Winchester Model 97 and Model 12. The break-open action, single barrel shotgun was used by the British Home Guard and U.S. home security forces. Notably, industrial centers (such as the Gopher State Steel Works) were guarded by National Guard soldiers with Winchester Model 37 12 gauge shotguns.

Since the end of World War II, the shotgun has remained a specialty weapon for modern armies. It has been deployed for specialized tasks where its strengths were put to particularly good use. It was used to defend machine gun emplacements during the Korean War, American and French jungle patrols used shotguns during the Vietnam War, and shotguns saw extensive use as door breaching and close quarter weapons in the early stages of the Iraq War, and saw limited use in tank crews. Many modern navies make extensive use of shotguns by personnel engaged in boarding hostile ships, as any shots fired will almost certainly be over a short range. Nonetheless, shotguns are far less common in military use than rifles, carbines, submachineguns, or pistols.

On the other hand, the shotgun has become a standard in law enforcement use. A variety of specialty less-lethal or non-lethal ammunitions, such as tear gas shells, bean bags, flares, explosive sonic stun rounds, and rubber projectiles, all packaged into 12 gauge shotgun shells, are produced specifically for the law enforcement market. Recently, Taser International introduced a self-contained electronic weapon which is fired from a standard 12 gauge shotgun.

The shotgun remains a standard firearm for hunting throughout the world for all sorts of game from birds and small game to large game such as deer. The versatility of the shotgun as a hunting weapon has steadily increased as slug rounds and more advanced rifled barrels have given shotguns longer range and higher killing power. The shotgun has become a ubiquitous firearm in the hunting community.

Action is the term for the operating mechanism of a gun. There are many types of shotguns, typically categorized by the number of barrels or the way the gun is reloaded.

For most of the history of the shotgun, the break-action breech loading double was the most common type, typically divided into two subtypes: the traditional "side by side" shotgun features two barrels mounted one beside the other (as the name suggests), whereas the "over and under" shotgun has the two barrels mounted one on top of the other. Side by side shotguns were traditionally used for hunting and other sporting pursuits (early long barreled side-by side shotguns were known as "fowling pieces" for their use hunting ducks and other birds), whereas over and under shotguns are more commonly associated with recreational use (such as clay pigeon and skeet shooting). Both types of double-barrel shotgun are used for hunting and sporting use, with the individual configuration largely being a matter of personal preference.

Another, less commonly encountered type of break-action shotgun is the combination gun, which is an over and under design with one shotgun barrel and one rifle barrel (more often rifle on top, but rifle on bottom was not uncommon). There is also a class of break action guns called "drillings", which contain three barrels, usually two shotgun barrels of the same gauge and a rifle barrel, though the only common theme is that at least one barrel be a shotgun barrel. The most common arrangement was essentially a side-by-side shotgun with the rifle barrel below and centered. Usually a drilling containing more than one rifle barrel would have both rifle barrels in the same caliber, but examples do exist with different caliber barrels, usually a .22 long rifle and a centerfire cartridge. Although very rare, drillings with three and even four (a "vierling") shotgun barrels were made.

In pump-action shotguns, a sliding forearm handle (the "pump") works the action, extracting the spent shell and inserting a new one while cocking the hammer or striker as the pump is worked. A pump gun is typically fed from a tubular magazine underneath the barrel, which also serves as a guide for the pump. The rounds are fed in one by one through a port in the receiver, where they are lifted by a lever called the "elevator" and pushed forward into the chamber by the bolt. A pair of latches at the rear of the magazine hold the rounds in place and facilitate feeding of one shell at a time. If it is desired to load the gun fully, a round may be loaded through the ejection port directly into the chamber, or cycled from the magazine, which is then topped off with another round. Well-known examples include the Winchester Model 1897, Remington 870 and Mossberg 500/590.

Pump-action shotguns are common hunting, fowling and sporting shotguns. Hunting models generally have a barrel between 600 and 700 mm (24"-28"). Tube-fed models designed for hunting often come with a dowel rod or other stop that is inserted into the magazine and reduces the capacity of the gun to three shells (two in the magazine and one chambered) as is mandated by U.S. federal law when hunting migratory birds. They can also easily be used with an empty magazine as a single-shot weapon, by simply dropping the next round to be fired into the open ejection port after the spent round is ejected. For this reason, pump-actions are commonly used to teach novice shooters under supervision, as the trainer can load each round more quickly than with a break-action, while unlike a break-action the student can maintain his grip on the gun and concentrate on proper handling and firing of the weapon.

Pump action shotguns with shorter barrels and little or no barrel choke are highly popular for use in home defense, military and law enforcement, and are commonly known as riot guns. The minimum barrel length for shotguns in most of the U.S. is , and this barrel length (sometimes to increase magazine capacity and/or ensure the gun is legal regardless of measuring differences) is the primary choice for riot shotguns. The shorter barrel makes the weapon easier to maneuver around corners and in tight spaces, though slightly longer barrels are sometimes used outdoors for a tighter spread pattern or increased accuracy of slug projectiles. Home-defense and law enforcement shotguns are usually chambered for 12-gauge shells, providing maximum shot power and the use of a variety of projectiles such as buckshot, rubber, sandbag and slug shells, but 20-gauge (common in bird-hunting shotguns) or .410 (common in youth-size shotguns) are also available in defense-type shotgun models allowing easier use by novice shooters.

A riot shotgun has many advantages over a handgun or rifle. Compared to "defense-caliber" handguns (chambered for 9mm Parabellum, .38 Special, .357 Magnum, .40 S&W, .45 ACP and similar), a shotgun has far more power and damage potential (up to 10 times the muzzle energy of a .45 ACP cartridge), allowing a "one-shot stop" that is more difficult to achieve with typical handgun loads. Compared to a rifle, riot shotguns are easier to maneuver due to the shorter barrel, still provide better damage potential at indoor distances (generally 3–5 meters/yards), and reduce the risk of "overpenetration"; that is, the bullet or shot passing completely through the target and continuing beyond, which poses a risk to those behind the target through walls. The wide spread of the shot reduces the importance of shot placement compared to a single projectile, which increases the effectiveness of "point shooting" – rapidly aiming simply by pointing the weapon in the direction of the target. This allows easy, fast use by novices.

"See article: Mare's Leg"

Early attempts at repeating shotguns invariably centred around either bolt-or lever-action designs, drawing inspiration from contemporary repeating rifles, with the earliest successful repeating shotgun being the lever-action Winchester M1887, designed by John Browning at the behest of the Winchester Repeating Arms Company.

Lever shotguns, while less common, were popular in the late 19th century with the Winchester Model 1887 and Model 1901 being prime examples. Initially very popular, demand waned after the introduction of pump-action shotguns around the start of the 20th century, and production was eventually discontinued in 1920.

One major issue with lever-actions (and to a lesser extent pump-actions) was that early shotgun shells were often made of paper or similar fragile materials (modern hulls are plastic or metal). As a result, the loading of shells, or working of the action of the shotgun, could often result in cartridges getting crushed and becoming unusable, or even damaging the gun.

Lever shotguns have seen a return to the gun market in recent years, however, with Winchester producing the Model 9410 (chambering the .410 gauge shotgun shell and using the action of the Winchester Model 94 series lever-action rifle, hence the name), and a handful of other firearm manufacturers (primarily Norinco of China and ADI Ltd. of Australia) producing versions of the Winchester Model 1887/1901 designed for modern 12-gauge smokeless shotshells with more durable plastic casings. There has been a notable uptick in lever-action shotgun sales in Australia since 1997, when pump-actions were effectively outlawed.

Bolt-action shotguns, while uncommon, do exist. One of the best-known examples is a 12-gauge manufactured by Mossberg featuring a 3-round magazine, marketed in Australia just after changes to the gun laws in 1997 heavily restricted the ownership and use of pump-action and semi-automatic shotguns. They were not a huge success, as they were somewhat slow and awkward to operate, and the rate of fire was noticeably slower (on average) than a double-barrelled gun. The Rifle Factory Ishapore in India also manufactured a single-shot .410 bore shotgun based on the SMLE Mk III* rifle. The Russian Berdana shotgun was effectively a single-shot bolt-action rifle that became obsolete, and was subsequently modified to chamber 16-gauge shotgun shells for civilian sale. The U.S. military M26 is also a bolt-action weapon. Bolt-action shotguns have also been used in the "goose gun" application, intended to kill birds such as geese at greater range. Typically, goose guns have long barrels (up to 36 inches), and small bolt-fed magazines. Bolt-action shotguns are also used in conjunction with slug shells for the maximum possible accuracy from a shotgun.

In Australia, some straight-pull bolt-action shotguns, such as the Turkish-made Pardus BA12 and Dickinson T1000, the American C-More Competition M26, as well as the indigenous-designed SHS STP 12, have become increasingly popular alternatives to lever-action shotguns, largely due to the better ergonomics with less stress on the shooter's trigger hand and fingers when cycling the action.

Colt briefly manufactured several revolving shotguns that were met with mixed success. The Colt Model 1839 Shotgun was manufactured between 1839 and 1841. Later, the Colt Model 1855 Shotgun, based on the Model 1855 revolving rifle, was manufactured between 1860 and 1863. Because of their low production numbers and age they are among the rarest of all Colt firearms.

The Armsel Striker was a modern take on the revolving shotgun that held 10 rounds of 12 Gauge ammunition in its cylinder. It was copied by Cobray as the Streetsweeper.

Taurus manufactures a carbine variant of the Taurus Judge revolver along with its Australian partner company, Rossi known as the "Taurus/Rossi Circuit Judge". It comes in the original combination chambering of .410 bore and .45 Long Colt, as well as the .44 Remington Magnum chambering. The rifle has small blast shields attached to the cylinder to protect the shooter from hot gases escaping between the cylinder and barrel.

The MTs255 () is a shotgun fed by a 5-round internal revolving cylinder. It is produced by the TsKIB SOO, Central Design and Research Bureau of Sporting and Hunting Arms. They are available in 12, 20, 28 and 32 gauges, and .410 bore.

Gas, inertia, or recoil operated actions are other popular methods of increasing the rate of fire of a shotgun; these are generally referred to as autoloaders or semi-automatics. Instead of having the action manually operated by a pump or lever, the action automatically cycles each time the shotgun is fired, ejecting the spent shell and reloading a fresh one into the chamber. The first successful semi-automatic shotgun was John Browning's Auto-5, first produced by Fabrique Nationale beginning in 1902. Other well-known examples include the Remington 1100, Benelli M1, and Saiga-12.

Some, such as the Franchi SPAS-12 and Benelli M3, are capable of switching between semi-automatic and pump action. These are popular for two reasons; first, some jurisdictions forbid the use of semi-automatic actions for hunting, and second, lower-powered rounds, like "reduced-recoil" buckshot shells and many less-lethal cartridges, have insufficient power to reliably cycle a semi-automatic shotgun.

Fully automatic shotguns, such as Auto Assault-12 (AA-12) also exist, but they're still rare.

In addition to the commonly encountered shotgun actions already listed, there are also shotguns based on the Martini-Henry rifle design, originally designed by British arms maker W.W. Greener.

Some of the more interesting advances in shotgun technology include the versatile NeoStead 2000 and fully automatics such as the Pancor Jackhammer or Auto-Assault 12.

In 1925, Rodolfo Cosmi produced the first working hybrid prototype semi-automatic shotgun, which had an 8-round magazine located in the stock. While it reloaded automatically after each shot like a semi-automatic, it had a break-action to load the first shell. This design has only been repeated once, by Beretta with their UGB25 automatic shotgun. The user loads the first shell by breaking the gun in the manner of a break-action shotgun, then closes it and inserts the second shell into a clip on the gun's right side. The spent hulls are ejected downwards. The guns combine the advantages of the break action (they can be proven to be safe by breaking open, there are no flying hulls) with those of the semi-automatic (low recoil, low barrel axis position hence low muzzle flip).

The French firearm manufacturer Verney-Carron produces the Véloce shotgun, a lever-release blowback firearm like the similarly designed SpeedLine rifle. The Véloce is in essence an inertia-driven semi-automatic shotgun, but after blowback the bolt is trapped by a bolt stop and will not return to battery unless the bolt stop is manually released by depressing a thumb lever near the tang of the gunstock. This design makes the gun technically not really a self-loading weapon, and Verney-Carron described it as a "manual repeating shotgun".

The gauge number is determined by the weight, in fractions of a pound, of a solid sphere of lead with a diameter equal to the inside diameter of the barrel. So, a 10 gauge shotgun nominally should have an inside diameter equal to that of a sphere made from one-tenth of a pound of lead. Each gauge has a set caliber. By far the most common gauges are 12 (0.729 in, 18.5 mm diameter) and 20 (0.614 in, 15.6 mm), although 67 (.410 in diameter), 32, 28, 24, 16, and 10 (19.7 mm) gauge also exist.

Different gauges have different typical applications. Twelve gauge shotguns are common for hunting geese, large ducks, or other big larger gamebirds; professional skeet and trap shooting; military applications; and home-defense applications. Sixteen gauge shotguns were once common for hunters who wanted to use only a single shotgun for gamebirds normally pursued with twelve or twenty gauge shotguns, but have become rarer in recent years. Twenty gauge shotguns are often used for gamebirds such as doves, smaller ducks, and quail. Twenty-eight gauge shotguns are not common, but are classic quail-hunting guns. .410 shotguns are typically used for squirrel hunting or for sportsmen seeking the challenge of killing game with a smaller load.

Other, less common shotgun cartridges have their own unique uses. Ammunition manufacturer CCI produces 9 mm (.355 in.) and several other popular pistol calibers up to .45 ACP as well as .22 (5.5 mm) for firing from handguns. These are commonly called snake shot cartridges. Larger gauges, up to 4 bore, too powerful to shoulder, have been built, but were generally affixed to small boats and referred to as punt guns. These were used for commercial waterfowl hunting, to kill large numbers of birds resting on the water.
Handguns have also been produced that are capable of firing either .45 (Long) Colt or .410 shotgun shells from the same chamber; they are commonly known as "snake guns". Derringers such as the "Snake Slayer and Cowboy Defender" are popular among some outdoors-men in the South and Southwest regions of the United States. There are also some revolvers, such as the Taurus Judge and Smith & Wesson Governor, that are capable of shooting the .45LC/.410 rounds; but as with derringers they are not considered shotguns.

The .410 bore (10.4 mm) is unusual, being measured in inches, and would be approximately 67 "real" gauge, though its short hull versions are nominally called 36 gauge in Europe. It uses a relatively small charge of shot. It is used for hunting and for skeet. Because of its very light recoil (approx 10 N), it is often used as a beginner's gun. However, the small charge and typically tight choke make it more difficult to hit targets. It is also frequently used by expert shooters because of the difficulty, especially in expensive side by side and over/under models for hunting small bird game such as quail and doves. Inexpensive bolt-action .410 shotguns are a very common first hunting shotgun among young pre-teen hunters, as they are used mostly for hunting squirrels, while additionally teaching bolt-action manipulation skills that will transfer easily later to adult-sized hunting rifles. Most of these young hunters move up to a 20-gauge within a few years, and to 12 gauge shotguns and full-size hunting rifles by their late teens. Still, many who are particularly recoil-averse choose to stay with 20-gauge shotguns all their adult life, as it is a suitable gauge for many popular hunting uses.

A recent innovation is the back-boring of barrels, in which the barrels are bored out slightly larger than their actual gauge. This reduces the compression forces on the shot when it transitions from the chamber to the barrel. This leads to a slight reduction in perceived recoil, and an improvement in shot pattern due to reduced deformation of the shot.

Most shotguns are used to fire "a number of ball shot", in addition to slugs and sabots. The ball shot or pellets is for the most part made of lead but this has been partially replaced by bismuth, steel, tungsten-iron, tungsten-nickel-iron and even tungsten polymer loads. Non-toxic loads are required by Federal law for waterfowl hunting in the US, as the shot may be ingested by the waterfowl, which some authorities believe can lead to health problems due to the lead exposure. Shot is termed either birdshot or buckshot depending on the shot size. Informally, birdshot pellets have a diameter smaller than and buckshot are larger than that. Pellet size is indicated by a number; for bird shot this ranges from the smallest 12 (1.2 mm, 0.05 in) to 2 (3.8 mm, 0.15 in) and then BB (4.6 mm, 0.18 in).

For buckshot, the numbers usually start at 4 (6.1 mm, 0.24 in) and go down to 1, 0, 00 ("double aught"), 000, and finally 0000 (9.7 mm, .38 in). A different informal distinction is that "bird shot" pellets are small enough that they can be measured into the cartridge by weight, and simply poured in, whereas "buckshot" pellets are so large they must be stacked inside the cartridge in a fixed geometric arrangement in order to fit. The diameter in hundredths of an inch of bird shot sizes from #9 to #1 can be obtained by subtracting the shot size from 17. Thus, #4 bird shot is 17 – 4 = 13 = in diameter. Different terminology is used outside the United States. In England and Australia, for example, 00 buckshot cartridges are commonly referred to as "S.G." (small game) cartridges.

Shot, small and round and delivered without spin, is ballistically inefficient. As the shot leaves the barrel it begins to disperse in the air. The resulting cloud of pellets is known as the shot pattern, or shotgun shot spread. The ideal pattern would be a circle with an even distribution of shot throughout, with a density sufficient to ensure enough pellets will intersect the target to achieve the desired result, such as a kill when hunting or a break when shooting clay targets. In reality the pattern is closer to a Gaussian, or normal distribution, with a higher density in the center that tapers off at the edges. Patterns are usually measured by firing at a diameter circle on a large sheet of paper placed at varying distances. The hits inside the circle are counted, and compared to the total number of pellets, and the density of the pattern inside the circle is examined. An "ideal" pattern would put nearly 100% of the pellets in the circle and would have no voids—any region where a target silhouette will fit and not cover 3 or more holes is considered a potential problem.

A constriction in the end of the barrel known as the choke is used to tailor the pattern for different purposes. Chokes may either be formed as part of the barrel at the time of manufacture, by squeezing the end of the bore down over a mandrel, or by threading the barrel and screwing in an interchangeable choke tube. The choke typically consists of a conical section that smoothly tapers from the bore diameter down to the choke diameter, followed by a cylindrical section of the choke diameter. Briley Manufacturing, a maker of interchangeable shotgun chokes, uses a conical portion about 3 times the bore diameter in length, so the shot is gradually squeezed down with minimal deformation. The cylindrical section is shorter, usually . The use of interchangeable chokes has made it easy to tune the performance of a given combination of shotgun and shotshell to achieve the desired performance.

The choke should be tailored to the range and size of the targets. A skeet shooter shooting at close targets might use 127 micrometres (0.005 inches) of constriction to produce a diameter pattern at a distance of . A trap shooter shooting at distant targets might use 762 micrometres (0.030 inches) of constriction to produce a diameter pattern at . Special chokes for turkey hunting, which requires long range shots at the small head and neck of the bird, can go as high as 1500 micrometres (0.060 inches). The use of too much choke and a small pattern increases the difficulty of hitting the target, whereas the use of too little choke produces large patterns with insufficient pellet density to reliably break targets or kill game. "Cylinder barrels" have no constriction. See also: Slug barrel

Other specialized choke tubes exist as well. Some turkey hunting tubes have constrictions greater than "Super Full", or additional features like porting to reduce recoil, or "straight rifling" that is designed to stop any spin that the shot column might acquire when traveling down the barrel. These tubes are often extended tubes, meaning they project beyond the end of the bore, giving more room for things like a longer conical section. Shot spreaders or diffusion chokes work opposite of normal chokes—they are designed to spread the shot more than a cylinder bore, generating wider patterns for very short range use. A number of recent spreader chokes, such as the Briley "Diffusion" line, actually use rifling in the choke to spin the shot slightly, creating a wider spread. The Briley Diffusion uses a 1 in 36 cm twist, as does the FABARM Lion Paradox shotgun.

Oval chokes, which are designed to provide a shot pattern wider than it is tall, are sometimes found on combat shotguns, primarily those of the Vietnam War era. They were available for aftermarket addition in the 1970s from companies like A & W Engineering. Military versions of the Ithaca 37 with "duckbill" choke were used in limited numbers during the Vietnam War by US Navy Seals. It arguably increased effectiveness in close range engagements against multiple targets. Two major disadvantages plagued the system. One was erratic patterning. The second was that the shot would spread too quickly providing a limited effective zone.

Offset chokes, where the pattern is intentionally slightly off of center, are used to change the point of impact. For instance, an offset choke can be used to make a double barrelled shotgun with poorly aligned barrels hit the same spot with both barrels.

Shotguns generally have longer barrels than modern rifles. Unlike rifles, however, the long shotgun barrel is not for ballistic purposes; shotgun shells use small powder charges in large diameter bores, and this leads to very low muzzle pressures (see internal ballistics) and very little velocity change with increasing barrel length. According to Remington, modern powder in a shotgun burns completely in 25 (9.8425 in) to 36 (14.173 in) cm barrels.

Since shotguns are generally used for shooting at small, fast moving targets, it is important to "lead" the target by firing slightly ahead of the target, so that when the shot reaches the range of the target, the target will have moved into the pattern. On uphill shooting, this means to shoot "above" the target. Conversely, on downhill shooting, this means to shoot "below" the target, which is somewhat counterintuitive for many beginning hunters. Depending on the barrel length, the amount of "lead" employed will vary for different barrel lengths, and must be learned by experience.

Shotguns made for close ranges, where the angular speed of the targets is great (such as skeet or upland bird hunting), tend to have shorter barrels, around . Shotguns for longer range shooting, where angular speeds are small (trap shooting; quail, pheasant, and waterfowl hunting), tend to have longer barrels, 28 to . The longer barrels have more angular momentum, and will therefore swing more slowly but more steadily. The short, low angular momentum barrels swing faster, but are less steady. These lengths are for pump or semi-auto shotguns; break open guns have shorter overall lengths for the same barrel length, and so will use longer barrels. The break open design saves between in overall length, but in most cases pays for this by having two barrels, which adds weight at the muzzle. Barrels for shotguns have been getting longer as modern steels and production methods make the barrels stronger and lighter; a longer, lighter barrel gives the same inertia for less overall weight.

Shotguns for use against larger, slower targets generally have even shorter barrels. Small game shotguns, for hunting game like rabbits and squirrels, or shotguns for use with buckshot for deer, are often .

Shotguns intended for all-round hunting are a compromise, but a barrel pump-action 12-gauge shotgun with a modified choke can serve admirably for use as one gun intended for general all-round hunting of small-game such as quails, rabbits, pheasants, doves, and squirrels in semi-open wooded or farmland areas in many parts of the eastern US (Kentucky, Indiana, Tennessee) where dense brush is less of a hindrance and the ability to have more reach is important. For hunting in dense brush, shorter barrel lengths are often preferred when hunting the same types of game.

Shotguns are well suited for the use caliber conversion sleeves, allowing most single- and double-barrel shotguns to fire a wide range of ammunition. The X Caliber system consists of eight adapter sleeves that allow the 12 gauge models to fire: .380 ACP, 9mm Luger, .38 Special, .357 Magnum, .40 S&W, .44 Special, .44 Magnum, .45 ACP, .45 Long Colt, .410 gauge and 20 gauge ammunition. The X caliber 12 gauge adapter sleeves also come in .22 Long Rifle, .223 Remington, 7.62x39mm and .308 Winchester as well. They even make four adapter sleeves that allow the 20 gauge models to fire: 9mm Luger, .38 Special, .357 Magnum, .45 ACP, .45 Long Colt, and .410 gauge ammunition.

The extremely large caliber of shotgun shells has led to a wide variety of different ammunition.

Shotshells are the most commonly used round, filled with lead or lead substitute pellets.

Of this general class, the most common subset is birdshot, which uses a large number (from dozens to hundreds) of small pellets, meant to create a wide "kill spread" to hunt birds in flight. Shot shells are described by the size and number of the pellets within, and numbered in reverse order (the smaller the number, the bigger the pellet size, similar to bore gauge). Size nine (#9) shot is the smallest size normally used for hunting and is used on small upland game birds such as dove and quail. Larger sizes are used for hunting larger upland game birds and waterfowl.

Buckshot is similar to but larger than birdshot, and was originally designed for hunting larger game, such as deer (hence the name). While the advent of new, more accurate slug technologies is making buckshot less attractive for hunting, it is still the most common choice for police, military, and home defense uses. Like birdshot, buckshot is described by pellet size, with larger numbers indicating smaller shot. From the smallest to the largest, buckshot sizes are: #4, (called "number four"), #1, 0 ("one-aught"), 00 ("double-aught"), 000 ("triple-aught") and 0000 ("four-aught"). A typical round for defensive use would be a 12 gauge length 00 buck shell, which contains 9 pellets roughly 8.4 mm (.33 inch) in diameter, each comparable to a .38 Special bullet in damage potential. New "tactical" buckshot rounds, designed specifically for defensive use, use slightly fewer shot at lower velocity to reduce recoil and increase controllability of the shotgun. There are some shotgun rounds designed specifically for police use that shoot effectively from with a 20" diameter grouping of the balls.

Slug rounds are rounds that fire a single solid slug. They are used for hunting large game, and in certain military and law enforcement applications. Modern slugs are moderately accurate, especially when fired from special rifled slug barrels. They are often used in "shotgun-only" hunting zones near inhabited areas, where rifles are prohibited due to their greater range.

Sabots are a common type of slug round. While some slugs are exactly that—a 12-gauge metal projectile in a cartridge—a sabot is a smaller but more aerodynamic projectile surrounded by a "shoe" of some other material. This "sabot" jacket seals the barrel, increasing pressure and acceleration, while also inducing spin on the projectile in a rifled barrel. Once the projectile clears the barrel, the sabot material falls away, leaving an unmarked, aerodynamic bullet to continue toward the target. The advantages over a traditional slug are increased shot power, increased bullet velocity due to the lighter-mass bullet, and increased accuracy due to the velocity and the reduction in deformation of the slug itself. Disadvantages versus a traditional slug include lower muzzle momentum due to reduced mass, reduced damage due to smaller bullet diameter, and significantly higher per-unit cost.

The unique properties of the shotgun, such as large case capacity, large bore, and the lack of rifling, has led to the development of a large variety of specialty shells, ranging from novelties to high tech military rounds.

Brenneke and Foster type slugs have the same basic configuration as normal slugs, but have increased accuracy. The hollowed rear of the Foster slug improves accuracy by placing more mass in the front of the projectile, therefore inhibiting the "tumble" that normal slugs may generate. The Brenneke slug takes this concept a bit further, with the addition of a wad that stays connected to the projectile after discharge, increasing accuracy. Both slugs are commonly found with fins or rib, which are meant to allow the projectile to safely squeeze down during passage through chokes, but they do not increase stability in flight.

Flechette rounds contain aerodynamic darts, typically from 8 to 20 in number. The flechette provide greatly extended range due to their aerodynamic shape, and improved penetration of light armor. American troops during the Vietnam War packed their own flechette shotgun rounds, called "beehive rounds", after the similar artillery rounds. However, terminal performance was poor due to the very light weight of the flechettes, and their use was quickly dropped.

Grenade rounds use exploding projectiles to increase long range lethality. These are currently experimental, but the British FRAG-12, which comes in High Explosive (HE), High Explosive Armor-piercing (HEAP) and High Explosive Fragmenting Antipersonnel (HEFA) forms, is under consideration by military forces.

Flexible baton rounds, commonly called "bean bags", fire a fabric bag filled with birdshot or a similar loose, dense substance. The "punch" effect of the bag is useful for knocking down targets; the rounds are used by police to subdue violent suspects. The bean bag round is by far the most common less-lethal round used. Due to the large surface area of these rounds, they lose velocity rapidly, and must be used at fairly short ranges to be effective, though use at extremely short ranges, under , can result in broken bones or other serious or lethal injuries. The rounds can also fly in a frisbee-like fashion and cut the person or animal being fired at. For this reason, these types of rounds are referred to as less-lethal, as opposed to less-than-lethal.

Gas shells spray a cone of gas for several meters. These are primarily used by riot police. They normally contain pepper gas or tear gas. Other variations launch a gas-grenade-like projectile.

Rock salt shells are hand loaded with coarse rock salt crystals, replacing the standard lead or steel shot. Rock salt shells could be seen as the forerunners of modern less-lethal rounds. In the United States, rock salt shells were and are sometimes still used by rural civilians to defend their property. The brittle salt was unlikely to cause serious injury at long ranges, but would cause painful stinging injuries and served as a warning. British gamekeepers have used rock salt shells to deter poachers. Rather than get into a physical confrontation, they stalk the poachers, making themselves known by a loud shout of "Run!" just before firing, to avoid hitting the now-fleeing subject in the eyes.

Rubber slugs or rubber buckshot are similar in principle to the bean bag rounds. Composed of flexible rubber or plastic and fired at low velocities, these rounds are probably the most common choice for riot control.

Taser International announced in 2007 a new 12 gauge eXtended Range Electronic Projectile or XREP, which contains a small electroshock weapon unit in a carrier that can be fired from a standard 12 gauge shotgun. The XREP projectile is fin stabilized, and travels at an initial velocity of 100 m/s (300 ft/s). Barbs on the front attach the electroshock unit to the target, with a tassel deploying from the rear to widen the circuit. A twenty-second burst of electrical energy is delivered to the target. This product was expected to be released to market in 2008. They were used—despite still being subject to testing, in breach of the supplier's license—by Northumbria police in their standoff with Raoul Moat in 2010.

Breaching rounds, often called frangible, Disintegrator, or Hatton rounds, are designed to destroy door locking mechanisms without risking lives. They are constructed of a very brittle substance that transfers most of the energy to the primary target but then fragment into much smaller pieces or dust so as not to injure unseen targets such as hostages or non-combatants that may be standing behind a breached door.

Bird bombs are low-powered rounds that fire a firecracker that is fused to explode a short time after firing. They are designed to scare animals, such as birds that congregate on airport runways.

Screechers fire a pyrotechnic whistle that emits a loud whistling sound for the duration of its flight. These are also used to scare animals.

Blank shells contain only a small amount of powder and no actual load. When fired, the blanks provide the sound and flash of a real load, but with no projectile. These may be used for simulation of gunfire, scaring wildlife, or as power for a launching device such as the Mossberg #50298 marine line launcher.

Stinger is a type of shotgun shell which contains sixteen 00-buck balls made of Zytel, and is designed as a non-lethal ammunition ideally used in small spaces.

Bolo rounds are made of two or more slugs molded onto steel wire. When fired, the slugs separate, pulling the wire taut creating a flying blade, which could theoretically decapitate people and animals or amputate limbs. However, many active shotgun users consider this to be overstated, and view bolo shells as being less effective than conventional ammunition. Bolo shell rounds are banned in many locations (including the US states of Florida and Illinois) due to concerns about their potential lethality. The round is named in reference to bolas, which use two or more weighted balls on a rope to trap cattle or game.

Dragon's breath usually refers to a zirconium-based pyrotechnic shotgun round. When fired, a gout of flame erupts from the barrel of the gun (up to 20 ft). The visual effect it produces is impressive, similar to that of a short ranged flamethrower. However, it has few tactical uses, mainly distraction/disorientation.

Flare rounds are sometimes carried by hunters for safety and rescue purposes. They are available in low and high altitude versions. Some brands claim they can reach a height of up to .

Globally, shotguns are generally not as heavily regulated as rifles or handguns, likely because they lack the range of rifles and are not easily concealable as handguns are; thus, they are perceived as a lesser threat by legislative authorities. The one exception is a sawed-off shotgun, especially a lupara, as it is more easily concealed than a normal shotgun.

Within Australia, all shotguns manufactured after 1 January 1901 are considered firearms and are subject to registration and licensing. Most shotguns (including break-action, bolt-action and lever-action shotguns) are classed as "Category A" weapons and, as such, are comparatively easy to obtain a licence for, given a legally recognised "legitimate reason" (compare to the British requirement for "good reason" for a FAC), such as sport shooting or hunting. However, pump-action and semi-automatic shotguns are classed as "Category C" (magazine capacity no more than 5 rounds) or "Category D" (magazine capacity more than 5 rounds) weapons; a licence for this type of firearm is, practically speaking, unavailable to the average citizen due to the difficulty and red tape of acquiring one. For more information, see Gun politics in Australia.

Canada has three classifications of firearms: non-restricted, restricted, and prohibited. Shotguns are found in all three classes.

All non-restricted shotguns must have an overall length of at least . Semi-automatic shotguns must also have a barrel length of more than and have a capacity of 5 shells or less in the magazine to remain non-restricted. All other shotgun action types (pump/slide, break open, lever, bolt) do not have a magazine limit restriction or a minimum barrel length provided the overall length of the firearm remains more than and the barrel was produced by an approved manufacturer. Shotgun barrels may only be reduced in length to a minimum of . Non-restricted shotguns may be possessed with any Possession and Acquisition Licence (PAL) or Possession-Only License (POL) and may be transported throughout the country without special authorization and may be used for hunting certain species at certain times of the year.

Semi-automatic shotguns with a barrel length of less than are considered restricted and any shotgun that has been altered so its barrel length is less than or if its overall length is less than is considered prohibited. Restricted and prohibited shotguns may be possessed with a PAL or POL that has been endorsed for restricted or prohibited grandfathered firearms. These shotguns require special Authorization to Transport (ATT).

The Canadian Firearms Registry was a government-run registry of all legally owned firearms in Canada. The government provided amnesty from prosecution to shotgun and rifle owners if they fail to register non-restricted shotguns and rifles. The long gun portion of the registry was scrapped in 2011.

See online for an official Canadian list of non-restricted and restricted and prohibited firearms.

In the United Kingdom, a Shotgun Certificate (SGC) is required to possess a "Section 2" shotgun. These cost £50 and can only be denied if the chief of police in the area believes and can prove that the applicant poses a real danger to the public, or if the applicant has been convicted of a crime punishable by imprisonment for a term of three years or more or if the applicant cannot securely store a shotgun (gun clamps, wire locks and locking gun cabinets are considered secure). The round number restrictions apply only to the magazine, not the chamber, so it is legal to have a single-barreled semi-auto or pump-action shotgun that holds three rounds in total, or a shotgun with separate chambers (which would need to also be multi-barrelled). For a shotgun to qualify as a section 2 shotgun, it must meet the following criteria:

(a) has a barrel not less than in length and does not have any barrel with a bore more than in diameter;

(b) either has no magazine or has a non-detachable magazine not capable of holding more than two cartridges;

(c) is not a revolver gun.

Prior to a SGC being issued an interview is conducted with the local Firearms Officer, in the past this was a duty undertaken by the local police although more recently this function has been "contracted out" to civilian staff. The officer will check the location and suitability of the gun safe that is to be used for storage and conduct a general interview to establish the reasons behind the applicant requiring a SGC.

An SGC holder can own any number of shotguns meeting these requirements so long as he/she can store them securely. No certificate is required to own shotgun ammunition, but one is required to buy it. There is no restriction on the amount of shotgun ammunition that can be bought or owned. There are also no rules regarding the storage of ammunition.

However, shotgun ammunition which contains fewer than 6 projectiles requires a section 1 Firearms Certificate (FAC). Shotguns with a magazine capacity greater than 2 rounds are also considered to be section 1 firearms and, as such, require an FAC to own. An FAC costs £50 but is much more restrictive than an SGC. The applicant must nominate two referees who are known to the applicant to vouch for his or her character; a new 'variation' is required for each new caliber of gun to be owned; limits are set on how much ammunition a person can own at any one time; and an FAC can be denied if the applicant does not have sufficient 'good reason'. 'Good reason' generally means hunting, collecting, or target shooting – though other reasons may be acceptable. Personal defense is not an acceptable reason.

Any pump-action or semi-automatic smooth-bore gun (such as a shotgun) with a barrel length of less than 24 inches or total length of less than 40 inches is considered to be a section 5 firearm, that is, one that is subject to general prohibition, unless it is chambered for .22 caliber rimfire ammunition.

In the US, federal law prohibits shotguns from being capable of holding more than three shells including the round in the chamber when used for hunting migratory gamebirds such as doves, ducks, and geese. For other uses, a capacity of any number of shells is generally permitted. Most magazine-fed shotguns come with a removable magazine plug to limit capacity to 2, plus one in the chamber, for hunting migratory gamebirds. Certain states have restrictions on magazine capacity or design features under hunting or assault weapon laws.

Shotguns intended for defensive use have barrels as short as for private use (the minimum shotgun barrel length allowed by law in the United States without federal registration. Barrel lengths of less than as measured from the breechface to the muzzle when the weapon is in battery, or have an overall length of less than are classified as short barreled shotguns (SBS) under the 1934 National Firearms Act and are regulated. A similar short barreled weapon having a pistol grip may be classified as an AOW or "Any Other Weapon" or "Firearm," depending on barrel length. A shotgun is defined as a weapon (with a buttstock) designed to be fired from the shoulder. The classification varies depending on how the weapon was originally manufactured.

Shotguns used by military, police, and other government agencies are regulated under the National Firearms Act of 1934; however, they are exempt from transfer taxes. These weapons commonly have barrels as short as so that they are easier to handle in confined spaces. Non-prohibited private citizens may own short-barreled shotguns by passing extensive background checks (state and local laws may be more restrictive) as well as paying a $200 federal tax and being issued a stamp. Defensive shotguns sometimes have no buttstock or will have a folding stock to reduce overall length even more when required. AOWs transfer with a $5 tax stamp from the BATFE.





</doc>
<doc id="26840" url="https://en.wikipedia.org/wiki?curid=26840" title="Saskatchewan">
Saskatchewan

Saskatchewan (; ) is a prairie and boreal province in western Canada, the only province without a natural border. It has an area of , nearly 10 percent of which () is fresh water, composed mostly of rivers, reservoirs, and the province's 100,000 lakes.

Saskatchewan is bordered on the west by Alberta, on the north by the Northwest Territories, on the east by Manitoba, to the northeast by Nunavut, and on the south by the U.S. states of Montana and North Dakota. As of Q1 2020, Saskatchewan's population was estimated at 1,181,987. Residents primarily live in the southern prairie half of the province, while the northern boreal half is mostly forested and sparsely populated. Of the total population, roughly half live in the province's largest city Saskatoon or the provincial capital Regina. Other notable cities include Prince Albert, Moose Jaw, Yorkton, Swift Current, North Battleford, Melfort, and the border city Lloydminster (partially within Alberta).

Saskatchewan is a landlocked province with large distances to moderating bodies of waters. As a result, its climate is extremely continental, rendering severe winters throughout the province. Southern areas have very warm or hot summers. Midale and Yellow Grass (both near the U.S. border) are tied for the highest ever recorded temperatures in Canada, with observed at both locations on July 5, 1937. In winter, temperatures below are possible even in the south during extreme cold snaps.

Saskatchewan has been inhabited for thousands of years by various indigenous groups. Europeans first explored the area in 1690 and first settled in the area in 1774. It became a province in 1905, carved out from the vast North-West Territories, which had until then included most of the Canadian Prairies. In the early 20th century the province became known as a stronghold for Canadian social democracy; North America's first social-democratic government was elected in 1944. The province's economy is based on agriculture, mining, and energy.

The former Lieutenant Governor, Thomas Molloy, died in office on July 2, 2019. On July 17, 2019, the federal government announced the appointment of Russell Mirasty, former Assistant Commissioner with the Royal Canadian Mounted Police, as the new Lieutenant Governor. The current premier is Scott Moe.

In 1992, the federal and provincial governments signed a historic land claim agreement with First Nations in Saskatchewan. The First Nations received compensation and were permitted to buy land on the open market for the bands; they have acquired about , now reserve lands. Some First Nations have used their settlement to invest in urban areas, including Saskatoon.

Its name derived from the Saskatchewan River. The river was known as ("swift flowing river") in the Cree language. Henday's spelling was Keiskatchewan, with the modern rendering, Saskatchewan, being officially adopted in 1882 when a portion of the present-day province was designated a provisional district of the North West Territories. Achieved provincial status in 1905.

As Saskatchewan's borders largely follow the geographic coordinates of longitude and latitude, the province is roughly a quadrilateral, or a shape with four sides. However, the 49th parallel boundary and the 60th northern border appear curved on globes and many maps. Additionally, the eastern boundary of the province is partially crooked rather than following a line of longitude, as correction lines were devised by surveyors prior to the homestead program (1880–1928).

Saskatchewan is part of the Western Provinces and is bounded on the west by Alberta, on the north by the Northwest Territories, on the north-east by Nunavut, on the east by Manitoba, and on the south by the U.S. states of Montana and North Dakota. Saskatchewan has the distinction of being the only Canadian province for which no borders correspond to physical geographic features (i.e. they are all parallels and meridians). Along with Alberta, Saskatchewan is one of only two land-locked provinces.

The overwhelming majority of Saskatchewan's population is located in the southern third of the province, south of the 53rd parallel.

Saskatchewan contains two major natural regions: the Boreal Forest in the north and the Prairies in the south. They are separated by an aspen parkland transition zone near the North Saskatchewan River on the western side of the province, and near to south of the Saskatchewan River on the eastern side. Northern Saskatchewan is mostly covered by forest except for the Lake Athabasca Sand Dunes, the largest active sand dunes in the world north of 58°, and adjacent to the southern shore of Lake Athabasca. Southern Saskatchewan contains another area with sand dunes known as the "Great Sand Hills" covering over . The Cypress Hills, located in the southwestern corner of Saskatchewan and Killdeer Badlands (Grasslands National Park), are areas of the province that were unglaciated during the last glaciation period, the Wisconsin glaciation.

The province's highest point, at , is located in the Cypress Hills less than 2 km from the provincial boundary with Alberta. The lowest point is the shore of Lake Athabasca, at . The province has 14 major drainage basins made up of various rivers and watersheds draining into the Arctic Ocean, Hudson Bay and the Gulf of Mexico.

Saskatchewan receives more hours of sunshine than any other Canadian province. The province lies far from any significant body of water. This fact, combined with its northerly latitude, gives it a warm summer, corresponding to its humid continental climate (Köppen type "Dfb") in the central and most of the eastern parts of the province, as well as the Cypress Hills; drying off to a semi-arid steppe climate (Köppen type "BSk") in the southwestern part of the province. Drought can affect agricultural areas during long periods with little or no precipitation at all. The northern parts of Saskatchewan – from about La Ronge northward – have a subarctic climate (Köppen "Dfc") with a shorter summer season. Summers can get very hot, sometimes above during the day, and with humidity decreasing from northeast to southwest. Warm southern winds blow from the plains and intermontane regions of the Western United States during much of July and August, very cool or hot but changeable air masses often occur during spring and in September. Winters are usually bitterly cold, with frequent Arctic air descending from the north. with high temperatures not breaking for weeks at a time. Warm chinook winds often blow from the west, bringing periods of mild weather. Annual precipitation averages 30 to 45 centimetres (12 to 18 inches) across the province, with the bulk of rain falling in June, July, and August.

Saskatchewan is one of the most tornado-active parts of Canada, averaging roughly 12 to 18 tornadoes per year, some violent. In 2012, 33 tornadoes were reported in the province. The Regina Cyclone took place in June 1912 when 28 people died in an F4 Fujita scale tornado. Severe and non-severe thunderstorm events occur in Saskatchewan, usually from early spring to late summer. Hail, strong winds and isolated tornadoes are a common occurrence.

The hottest temperature ever recorded anywhere in Canada happened in Saskatchewan. The temperature rose to in Midale and Yellow Grass. The coldest ever recorded in the province was in Prince Albert, which is north of Saskatoon.

The effects of climate change in Saskatchewan are now being observed in parts of the province. There is evidence of reduction of biomass in Saskatchewan's boreal forests (as with those of other Canadian prairie provinces) is linked by researchers to drought-related water stress, stemming from global warming, most likely caused by greenhouse gas emissions. While studies, as early as 1988 (Williams, et al., 1988) have shown climate change will affect agriculture, whether the effects can be mitigated through adaptations of cultivars, or crops, is less clear. Resiliency of ecosystems may decline with large changes in temperature. The provincial government has responded to the threat of climate change by introducing a plan to reduce carbon emissions, "The Saskatchewan Energy and Climate Change Plan", in June 2007.

Saskatchewan has been populated by various indigenous peoples of North America, including members of the Sarcee, Niitsitapi, Atsina, Cree, Saulteaux, Assiniboine (Nakoda), Lakota and Sioux. The first known European to enter Saskatchewan was Henry Kelsey in 1690, who travelled up the Saskatchewan River in hopes of trading fur with the region's indigenous peoples. The first permanent European settlement was a Hudson's Bay Company post at Cumberland House, founded in 1774 by Samuel Hearne. In 1762 the south of the province was part of the Spanish Louisiana until 1802.

In 1803 the Louisiana Purchase transferred from France to the United States part of what is now Alberta and Saskatchewan. In 1818 the U.S. ceded the area to Britain. Most of what is now Saskatchewan was part of Rupert's Land and controlled by the Hudson's Bay Company, which claimed rights to all watersheds flowing into Hudson Bay, including the Saskatchewan River, Churchill, Assiniboine, Souris, and Qu'Appelle River systems.

In the late 1850s and early 1860s, scientific expeditions led by John Palliser and Henry Youle Hind explored the prairie region of the province.

In 1870, Canada acquired the Hudson's Bay Company's territories and formed the North-West Territories to administer the vast territory between British Columbia and Manitoba. The Crown also entered into a series of numbered treaties with the indigenous peoples of the area, which serve as the basis of the relationship between First Nations, as they are called today, and the Crown. Since the late twentieth century, land losses and inequities as a result of those treaties have been subject to negotiation for settlement between the First Nations in Saskatchewan and the federal government, in collaboration with provincial governments.

In 1876, following their defeat of United States Army forces at the Battle of the Little Bighorn in Montana Territory in the United States, the Lakota Chief Sitting Bull led several thousand of his people to Wood Mountain. Survivors and descendants founded Wood Mountain Reserve in 1914.

The North-West Mounted Police set up several posts and forts across Saskatchewan, including Fort Walsh in the Cypress Hills, and Wood Mountain Post in south-central Saskatchewan near the United States border.

Many Métis people, who had not been signatories to a treaty, had moved to the Southbranch Settlement and Prince Albert district north of present-day Saskatoon following the Red River Rebellion in Manitoba in 1870. In the early 1880s, the Canadian government refused to hear the Métis' grievances, which stemmed from land-use issues. Finally, in 1885, the Métis, led by Louis Riel, staged the North-West Rebellion and declared a provisional government. They were defeated by a Canadian militia brought to the Canadian prairies by the new Canadian Pacific Railway. Riel, who surrendered and was convicted of treason in a packed Regina courtroom, was hanged on November 16, 1885. Since then, the government has recognized the Métis as an aboriginal people with status rights and provided them with various benefits.

The national policy set by the federal government, the Canadian Pacific Railway, the Hudson's Bay Company and associated land companies encouraged immigration. The "Dominion Lands Act" of 1872 permitted settlers to acquire one quarter of a square mile of land to homestead and offered an additional quarter upon establishing a homestead. In 1874, the North-West Mounted Police began providing police services. In 1876, the "North-West Territories Act" provided for appointment, by the Ottawa, of a Lieutenant Governor and a Council to assist him.

Highly optimistic advertising campaigns promoted the benefits of prairie living. Potential immigrants read leaflets information painted Canada as a veritable garden of Eden and downplayed the need for agricultural expertise. Ads in "The Nor'-West Farmer" by the Commissioner of Immigration implied that western land was blessed with water, wood, gold, silver, iron, copper, and cheap coal for fuel, all of which were readily at hand. Reality was far harsher, especially for the first arrivals who lived in sod houses. However eastern money poured in and by 1913, long term mortgage loans to Saskatchewan farmers had reached $65 million.

The dominant groups comprised British settlers from eastern Canada and Britain, who comprised about half of the population during the late 19th and early 20th centuries. They played the leading role in establishing the basic institutions of plains society, economy and government.

Gender roles were sharply defined. Men were primarily responsible for breaking the land; planting and harvesting; building the house; buying, operating and repairing machinery; and handling finances. At first, there were many single men on the prairie, or husbands whose wives were still back east, but they had a hard time. They realized the need for a wife. In 1901, there were 19,200 families, but this surged to 150,300 families only 15 years later. Wives played a central role in settlement of the prairie region. Their labor, skills, and ability to adapt to the harsh environment proved decisive in meeting the challenges. They prepared bannock, beans and bacon, mended clothes, raised children, cleaned, tended the garden, helped at harvest time and nursed everyone back to health. While prevailing patriarchal attitudes, legislation, and economic principles obscured women's contributions, the flexibility exhibited by farm women in performing productive and nonproductive labor was critical to the survival of family farms, and thus to the success of the wheat economy.

On September 1, 1905, Saskatchewan became a province, with inauguration day held September 4. Its political leaders at the time proclaimed its destiny was to become Canada's most powerful province. Saskatchewan embarked on an ambitious province-building program based on its Anglo-Canadian culture and wheat production for the export market. Population quintupled from 91,000 in 1901 to 492,000 to 1911, thanks to heavy immigration of farmers from the Ukraine, U.S., Germany and Scandinavia. Efforts were made to assimilate the newcomers to British Canadian culture and values.

In the 1905 provincial elections, Liberals won 16 of 25 seats in Saskatchewan. The Saskatchewan government bought out Bell Telephone Company in 1909, with the government owning the long-distance lines and left local service to small companies organized at the municipal level. Premier Walter Scott preferred government assistance to outright ownership because he thought enterprises worked better if citizens had a stake in running them; he set up the Saskatchewan Cooperative Elevator Company in 1911. Despite pressure from farm groups for direct government involvement in the grain handling business, the Scott government opted to loan money to a farmer-owned elevator company. Saskatchewan in 1909 provided bond guarantees to railway companies for the construction of branch lines, alleviating the concerns of farmers who had trouble getting their wheat to market by wagon. The Saskatchewan Grain Growers Association, was the dominant political force in the province until the 1920s; it had close ties with the governing Liberal party. In 1913, the Saskatchewan Stock Growers Association was established with three goals: to watch over legislation; to forward the interests of the stock growers in every honourable and legitimate way; and to suggest to parliament legislation to meet changing conditions and requirements.

Immigration peaked in 1910, and in spite of the initial difficulties of frontier life – distance from towns, sod homes, and backbreaking labour – new settlers established a European-Canadian style of prosperous agrarian society. The long-term prosperity of the province depended on the world price of grain, which headed steadily upward from the 1880s to 1920, then plunged down. Wheat output was increased by new strains, such as the "Marquis wheat" strain which matured 8 days sooner and yielded 7 more bushels per acre (0.72 m/ha) than the previous standard, "Red Fife". The national output of wheat soared from in 1896, to in 1901, reaching by 1921.

Urban reform movements in Regina were based on support from business and professional groups. City planning, reform of local government, and municipal ownership of utilities were more widely supported by these two groups, often through such organizations as the Board of Trade. Church-related and other altruistic organizations generally supported social welfare and housing reforms; these groups were generally less successful in getting their own reforms enacted.

The province responded to the First World War in 1914 with patriotic enthusiasm and enjoyed the resultant economic boom for farms and cities alike. Emotional and intellectual support for the war emerged from the politics of Canadian national identity, the rural myth, and social gospel progressivism The Church of England was especially supportive. However, there was strong hostility toward German-Canadian farmers. Recent Ukrainian immigrants were enemy aliens because of their citizenship in the Austro-Hungarian Empire. A small fraction were taken to internment camps. Most of the internees were unskilled unemployed labourers who were imprisoned "because they were destitute, not because they were disloyal".

The price of wheat tripled and acreage seeded doubled. The wartime spirit of sacrifice intensified social reform movements that had predated the war and now came to fruition. Saskatchewan gave women the right to vote in 1916 and at the end of 1916 passed a referendum to prohibit the sale of alcohol.

In the late 1920s, the Ku Klux Klan, imported from the United States and Ontario, gained brief popularity in nativist circles in Saskatchewan and Alberta. The Klan, briefly allied with the provincial Conservative party because of their mutual dislike for Premier James G. "Jimmy" Gardiner and his Liberals (who ferociously fought the Klan), enjoyed about two years of prominence. It declined and disappeared, subject to widespread political and media opposition, plus internal scandals involving the use of the organization's funds.

In 1970, the first annual Canadian Western Agribition was held in Regina. This farm-industry trade show, with its strong emphasis on livestock, is rated as one of the five top livestock shows in North America, along with those in Houston, Denver, Louisville and Toronto.

The province celebrated the 75th anniversary of its establishment in 1980, with Princess Margaret, Countess of Snowdon, presiding over the official ceremonies. In 2005, 25 years later, her sister, Queen Elizabeth II, attended the events held to mark Saskatchewan's centennial.

Since the late 20th century, First Nations have become more politically active in seeking justice for past inequities, especially related to the taking of indigenous lands by various governments. The federal and provincial governments have negotiated on numerous land claims, and developed a program of "Treaty Land Entitlement", enabling First Nations to buy land to be taken into reserves with money from settlements of claims.
"In 1992, the federal and provincial governments signed a historic land claim agreement with Saskatchewan First Nations. Under the Agreement, the First Nations received money to buy land on the open market. As a result, about 761,000 acres have been turned into reserve land and many First Nations continue to invest their settlement dollars in urban areas", including Saskatoon. The money from such settlements has enabled First Nations to invest in businesses and other economic infrastructure.

According to the Canada 2011 Census, the largest ethnic group in Saskatchewan is German (28.6%), followed by English (24.9%), Scottish (18.9%), Canadian (18.8%), Irish (15.5%), Ukrainian (13.5%), French (Fransaskois) (12.2%), First Nations (12.1%), Norwegian (6.9%), and Polish (5.8%).

Historically, Saskatchewan's economy was primarily associated with agriculture, with wheat being the precious symbol on the province's flag. Increasing diversification has resulted in agriculture, forestry, fishing, and hunting only making up 8.9% of the province's GDP in 2018. Saskatchewan grows a large portion of Canada's grain. In 2017, the production of canola surpassed the production of wheat, which is Saskatchewan's most familiar crop and the one most often associated with the province. Total net income from farming was $3.3 billion in 2017, which was $0.9 billion less than the income in 2016. Other grains such as flax, rye, oats, peas, lentils, canary seed, and barley are also produced in the province. Saskatchewan is the world's largest exporter of mustard seed. Beef cattle production by a Canadian province is only exceeded by Alberta. In the northern part of the province, forestry is also a significant industry. 

Mining is a major industry in the province, with Saskatchewan being the world's largest exporter of potash and uranium. Oil and natural gas production is also a very important part of Saskatchewan's economy, although the oil industry is larger. Among Canadian provinces, only Alberta exceeds Saskatchewan in overall oil production. Heavy crude is extracted in the Lloydminster-Kerrobert-Kindersley areas. Light crude is found in the Kindersley-Swift Current areas as well as the Weyburn-Estevan fields. Natural gas is found almost entirely in the western part of Saskatchewan, from the Primrose Lake area through Lloydminster, Unity, Kindersley, Leader, and around Maple Creek areas.

A list of the companies includes The Potash Corporation of Saskatchewan (defunct in December 2017), Federated Cooperatives Ltd. and IPSCO.

Major Saskatchewan-based Crown corporations are Saskatchewan Government Insurance (SGI), SaskTel, SaskEnergy (the province's main supplier of natural gas), and SaskPower. Bombardier runs the NATO Flying Training Centre at 15 Wing, near Moose Jaw. Bombardier was awarded a long-term contract in the late 1990s for $2.8 billion from the federal government for the purchase of military aircraft and the running of the training facility. SaskPower since 1929 has been the principal supplier of electricity in Saskatchewan, serving more than 451,000 customers and managing $4.5 billion in assets. SaskPower is a major employer in the province with almost 2,500 permanent full-time staff located in 71 communities.

The Tabulated Data covers each fiscal year (e.g. 2015–2016 covers April 1, 2015 – March 31, 2016). All data is in $1,000s.

"Source: Government of Saskatchewan."

Publicly funded elementary and secondary schools in the province are administered by the Saskatchewan Ministry of Education. Public elementary and secondary schools either operate as secular or as a separate schools. Nearly all school divisions, except one operate as an English first language school board. The Division scolaire francophone No. 310 is the only school division that operates French first language schools. In addition to elementary and secondary schools, the province is also home to several post-secondary institutions.

The first education on the prairies took place within the family groups of the First Nation and early fur trading settlers. There were only a few missionary or trading post schools established in Rupert's Land – later known as the North West Territories. The first 76 North-West Territories school districts and the first Board of Education meeting formed in 1886. The pioneering boom formed ethnic bloc settlements. Communities were seeking education for their children similar to the schools of their home land. Log cabins, and dwellings were constructed for the assembly of the community, school, church, dances and meetings.

The prosperity of the Roaring Twenties and the success of farmers in proving up on their homesteads helped provide funding to standardize education. Textbooks, normal schools for educating teachers, formal school curricula and state of the art school house architectural plans provided continuity throughout the province. English as the school language helped to provide economic stability because one community could communicate with another and goods could be traded and sold in a common language. The number of one-room schoolhouse districts across Saskatchewan totalled approximately 5,000 at the height of this system of education in the late 1940s.

Following World War II, the transition from many one-room schoolhouses to fewer and larger consolidated modern technological town and city schools occurred as a means of ensuring technical education. School buses, highways, and family vehicles create ease and accessibility of a population shift to larger towns and cities. Combines and tractors mean the farmer could manage more than a quarter section of land, so there was a shift from family farms and subsistence crops to cash crops grown on many sections of land. School vouchers have been newly proposed as a means of allowing competition between rural schools and making the operation of co-operative schools practicable in rural areas.

Saskatchewan's Ministry of Health is responsible for policy direction, sets and monitors standards, and provides funding for regional health authorities and provincial health services. Saskatchewan's medical health system is widely and inaccurately characterized as "socialized medicine": medical practitioners in Saskatchewan, as in other Canadian provinces, are not civil servants but remit their accounts to the publicly funded Saskatchewan Medical Care Insurance Plan rather than to patients (i.e. a single-payer system).

Saskatchewan medical health system has faced criticism due to a lack of accessibility to the midwifery program. According to Leanne Smith, the director for maternal services in the Saskatoon Health Region declared half of the women who apply for the midwifery program are turned away. Ministry of Health data shows midwives saw 1,233 clients in the 2012–13 fiscal year (which runs April to March). But in that fourth quarter, 359 women were still on waiting lists for immediate or future care. The provincial Health Ministry received 47 letters about midwifery services in 2012, most of which asked for more midwives. As a continuing problem in the Saskatchewan health care system, more pressure has been placed to recruit more midwives for the province.

Saskatchewan has the same form of government as the other Canadian provinces with a lieutenant-governor (who is the representative of the Queen in Right of Saskatchewan), premier, and a unicameral legislature.

During the 20th century, Saskatchewan was one of Canada's more left-wing provinces, reflecting the slant of its many rural citizens which distrusted the distant capital government and which favored a strong local government to attend to their issues. In 1944 Tommy Douglas became premier of the first avowedly socialist regional government in North America. Most of his Members of the Legislative Assembly (MLAs) represented rural and small-town ridings. Under his Cooperative Commonwealth Federation government, Saskatchewan became the first province to have Medicare. In 1961, Douglas left provincial politics to become the first leader of the federal New Democratic Party. In the 21st century, Saskatchewan began to drift to the right-wing, generally attributed to the province's economy shifting toward oil and gas production. In the 2015 federal election, the Conservative Party of Canada won ten of the province's fourteen seats, followed by the New Democratic Party with three and the Liberal Party of Canada with one; in the 2019 election, the Conservatives won in all of Saskatchewan's 14 seats, sweeping their competition.

Provincial politics in Saskatchewan is dominated by the social-democratic Saskatchewan New Democratic Party and the centre-right Saskatchewan Party, with the latter holding the majority in the Legislative Assembly of Saskatchewan since 2007. The current Premier of Saskatchewan is Scott Moe, who took over the leadership of the Saskatchewan Party in 2018 following the resignation of Brad Wall. Numerous smaller political parties also run candidates in provincial elections, including the Green Party of Saskatchewan, Liberal Party of Saskatchewan, and the Progressive Conservative Party of Saskatchewan, but none is currently represented in the Legislative Assembly (federal Conservatives and Liberals generally favour the Saskatchewan Party in provincial elections).

No Prime Minister of Canada has been born in Saskatchewan, but two (William Lyon Mackenzie King and John Diefenbaker) represented the province in the House of Commons of Canada during their tenures as head of government.

Transportation in Saskatchewan includes an infrastructure system of roads, highways, freeways, airports, ferries, pipelines, trails, waterways and railway systems serving a population of approximately 1,003,299 (according to 2007 estimates) inhabitants year-round. It is funded primarily with local and federal government funds. The Saskatchewan Department of Highways and Transportation estimates 80% of traffic is carried on the 5,031-kilometre principal system of highways.

The Ministry of Highways and Infrastructure operates over of highways and divided highways. There are also municipal roads which comprise different surfaces. Asphalt concrete pavements comprise almost , granular pavement almost , non structural or thin membrane surface TMS are close to and finally gravel highways make up over through the province. In the northern sector, ice roads which can only be navigated in the winter months comprise another approximately of travel.

Saskatchewan has over 250,000 kilometres (150,000 mi) of roads and highways, the highest length of road surface of any Canadian province. The major highways in Saskatchewan are the Trans Canada expressway, Yellowhead Highway northern Trans Canada route, Louis Riel Trail, CanAm Highway, Red Coat Trail, Northern Woods and Water route, and Saskota travel route.

The first Canadian transcontinental railway was constructed by the Canadian Pacific Railway between 1881 and 1885. After the great east-west transcontinental railway was built, north-south connector branch lines were established. The 1920s saw the largest rise in rail line track as the CPR and CNR fell into competition to provide rail service within ten kilometres. In the 1960s there were applications for abandonment of branch lines. Today the only two passenger rail services in the province are "The Canadian" and Winnipeg–Churchill train, both operated by Via Rail. "The Canadian" is a transcontinental service linking Toronto with Vancouver.

The main Saskatchewan waterways are the North Saskatchewan River or South Saskatchewan River routes. In total, there are 3,050 bridges maintained by the Department of Highways in Saskatchewan. There are currently twelve ferry services operating in the province, all under the jurisdiction of the Department of Highways.

The Saskatoon Airport (YXE) was initially established as part of the Royal Canadian Air Force training program during World War II. It was renamed the "John G. Diefenbaker Airport" in the official ceremony, June 23, 1993. "Roland J. Groome Airfield" is the official designation for the Regina International Airport (YQR) as of August 3, 2005; the airport was established in 1930. Under the British Commonwealth Air Training Plan (BCATP), twenty Service Flying Training Schools (RAF) were established at various Saskatchewan locations in World War II. 15 Wing Moose Jaw is home to the Canadian Forces formation aerobatics team, the "Snowbirds".

Airlines offering service to Saskatchewan are Air Canada, WestJet Airlines, United Airlines, Delta Air Lines, Transwest Air, Sunwing Airlines, Norcanair Airlines, La Ronge Aviation Services Ltd, La Loche Airways, Osprey Wings Ltd, Buffalo Narrows Airways Ltd, Île-à-la-Crosse Airways Ltd, Voyage Air, Pronto Airways, Venture Air Ltd, Pelican Narrows Air Service, Jackson Air Services Ltd, and Northern Dene Airways Ltd.

The Government of Canada has agreed to contribute $20 million for two new interchanges in Saskatoon. One of them being at the Sk Hwy 219/Lorne Ave intersection with Circle Drive, the other at the Senator Sid Buckwold Bridge (Idylwyld Freeway) and Circle Drive. This is part of the Asia-Pacific Gateway and Corridor Initiative to improve access to the Canadian National Railway's intermodal freight terminal thereby increasing Asia-Pacific trade. Also, the Government of Canada will contribute $27 million to Regina to construct a Canadian Pacific Railway CPR intermodal facility and improve infrastructure transportation to the facility from both national highway networks, Sk Hwy 1, the TransCanada Highway and Sk Hwy 11, Louis Riel Trail. This also is part of the Asia-Pacific Gateway and Corridor Initiative to improve access to the CPR terminal and increase Asia-Pacific trade.

Saskatchewan is home to a number of museums. The Royal Saskatchewan Museum serves as the provincial museum of the province. Other museums include Diefenbaker House, Evolution of Education Museum, Museum of Antiquities, the RCMP Heritage Centre, Rotary Museum of Police and Corrections, Saskatchewan Science Centre, Saskatchewan Western Development Museum, and the T.rex Discovery Centre.

The province is home to several art galleries, including MacKenzie Art Gallery, and Remai Modern. The province is also home to several performing arts centres including the Conexus Arts Centre in Regina, and TCU Place in Saskatoon. PAVED Arts, a new media artist-run space, is also located in Saskatoon. The province is presently home to several concert orchestras, the Regina Symphony Orchestra, the Saskatoon Symphony Orchestra, and the Saskatoon Youth Orchestra. The Regina Symphony Orchestra is at the Conexus Arts Centre, while the Saskatoon perform at TCU Place.

The Saskatchewan Roughriders Canadian football team is the province's professional football franchise (playing in the Canadian Football League), and are extremely popular across Saskatchewan. The team's fans are also found to congregate on game days throughout Canada, and collectively they are known as "Rider Nation". The province's other major sport franchise is the Saskatchewan Rush of the National Lacrosse League. In their first year of competition, 2016, the Rush won both their Division Title and the League Championship.

Hockey is the most popular sport in the province. More than 490 NHL players have been born in Saskatchewan, the highest per capita output of any Canadian province, U.S. state, or European country. Notable NHL figures born in Saskatchewan include Keith Allen, Gordie Howe, Bryan Trottier, Bernie Federko, Clark Gillies, Fern Flaman, Bert Olmstead, Harry Watson, Elmer Lach, Max Bentley, Sid Abel, Doug Bentley, Eddie Shore, Clint Smith, Bryan Hextall, Johnny Bower, Emile Francis, Glenn Hall, Chuck Rayner, Brad McCrimmon, Patrick Marleau, Dave Manson, Theo Fleury, Terry Harper, Wade Redden, Brian Propp, Scott Hartnell, Ryan Getzlaf, and Chris Kunitz. Saskatchewan does not have an NHL or minor professional franchise, but five teams in the junior Western Hockey League are located in the province: the Moose Jaw Warriors, Prince Albert Raiders, Regina Pats, Saskatoon Blades and Swift Current Broncos.

In 2015, Budweiser honoured Saskatchewan for their abundance of hockey players by sculpting a 12-foot-tall hockey player monument in ice for Saskatchewan's capital city of Regina. The company then filmed this frozen monument for a national television commercial, thanking the province for creating so many goal scorers throughout hockey's history. Budweiser also gifted the “hockey player” province a trophy made of white birch—Saskatchewan's provincial tree—which bears the name of every pro player in history. Sitting atop the trophy was a golden Budweiser Red Light, synched to every current Saskatchewan player in the pros. This trophy can currently be seen at Victoria Bar in Regina.

Historically, Saskatchewan has been one of the strongest curling provinces. Teams from Saskatchewan have finished in the top three places at 38 briers and Saskatchewan has more women's championships than any other province with 11. Notable curlers from Saskatchewan include Sandra Schmirler, Ernie Richardson, and Vera Pezer. In a 2019 TSN poll, experts ranked Schmirler's Saskatchewan team, which won a gold medal at the 1998 Olympics, as the greatest women's team in Canada's history.

The flag of Saskatchewan was officially adopted on September 22, 1969. The flag features the provincial shield in the upper quarter nearest the staff, with the floral emblem, the Prairie Lily, in the fly. The upper green (in forest green) half of the flag represents the northern Saskatchewan forest lands, while the golden lower half of the flag symbolizes the southern wheat fields and prairies. A province-wide competition was held to design the flag, and drew over 4,000 entries. The winning design was by Anthony Drake, then living in Hodgeville.

In 2005, Saskatchewan Environment held a province-wide vote to recognize Saskatchewan's centennial year, receiving more than 10,000 online and mail-in votes from the public. The walleye was the overwhelming favourite of the six native fish species nominated for the designation, receiving more than half the votes cast. Other species in the running were the lake sturgeon, lake trout, lake whitefish, northern pike and yellow perch.

Saskatchewan's other symbols include the tartan, the license plate, and the provincial flower. Saskatchewan's official tartan was registered with the Court of Lord Lyon King of Arms in Scotland in 1961. It has seven colours: gold, brown, green, red, yellow, white and black. The provincial licence plates display the slogan "Land of Living Skies". The provincial flower of Saskatchewan is the Western Red Lily.

In 2005, Saskatchewan celebrated its centennial. To honour it, the Royal Canadian Mint issued a commemorative five-dollar coin depicting Canada's wheat fields as well as a circulation 25-cent coin of a similar design. Queen Elizabeth II and Prince Philip visited Regina, Saskatoon, and Lumsden, and the Saskatchewan-reared Joni Mitchell issued an album in Saskatchewan's honour.






</doc>
<doc id="26841" url="https://en.wikipedia.org/wiki?curid=26841" title="Summer solstice (disambiguation)">
Summer solstice (disambiguation)

Summer solstice is the astronomical phenomenon that occurs on the longest day of the year.

Summer solstice may also refer to:



</doc>
<doc id="26842" url="https://en.wikipedia.org/wiki?curid=26842" title="Salting">
Salting

Salting or Salted may refer to:





</doc>
<doc id="26847" url="https://en.wikipedia.org/wiki?curid=26847" title="Socialism">
Socialism

Socialism is a political, social and economic philosophy encompassing a range of economic and social systems characterised by social ownership of the means of production and workers' self-management of enterprises. It includes the political theories and movements associated with such systems. Social ownership can be public, collective, cooperative or of equity. While no single definition encapsulates many types of socialism, social ownership is the one common element. 

Socialist systems are divided into non-market and market forms. Non-market socialism substitutes factor markets and money with integrated economic planning and engineering or technical criteria based on calculation performed in-kind, thereby producing a different economic mechanism that functions according to different economic laws and dynamics than those of capitalism. A non-market socialist system eliminates the inefficiencies and crises traditionally associated with capital accumulation and the profit system in capitalism. The socialist calculation debate, originated by the economic calculation problem, concerns the feasibility and methods of resource allocation for a planned socialist system. By contrast, market socialism retains the use of monetary prices, factor markets and in some cases the profit motive, with respect to the operation of socially owned enterprises and the allocation of capital goods between them. Profits generated by these firms would be controlled directly by the workforce of each firm or accrue to society at large in the form of a social dividend.

Socialist politics has been both internationalist and nationalist in orientation; organised through political parties and opposed to party politics; at times overlapping with trade unions and at other times independent and critical of them; and present in both industrialised and developing nations. Social democracy originated within the socialist movement, supporting economic and social interventions to promote social justice. While retaining socialism as a long-term goal, since the post-war period it has come to embrace a Keynesian mixed economy within a predominantly developed capitalist market economy and liberal democratic polity that expands state intervention to include income redistribution, regulation and a welfare state. Economic democracy proposes a sort of market socialism, with more democratic control of companies, currencies, investments and natural resources.

The socialist political movement includes a set of political philosophies that originated in the revolutionary movements of the mid-to-late 18th century and out of concern for the social problems that were associated with capitalism. By the late 19th century, after the work of Karl Marx and his collaborator Friedrich Engels, socialism had come to signify opposition to capitalism and advocacy for a post-capitalist system based on some form of social ownership of the means of production. By the 1920s, communism and social democracy had become the two dominant political tendencies within the international socialist movement, with socialism itself becoming the most influential secular movement of the 20th century. Socialist parties and ideas remain a political force with varying degrees of power and influence on all continents, heading national governments in many countries around the world. Today, many socialists have also adopted the causes of other social movements such as environmentalism, feminism and progressivism.

While the emergence of the Soviet Union as the world's first nominally socialist state led to socialism's widespread association with the Soviet economic model, some economists and intellectuals argued that in practice the model functioned as a form of state capitalism or a non-planned administrative or command economy. Academics, political commentators and other scholars tend to distinguish between authoritarian socialist and democratic socialist states, with the first representing the Soviet Bloc and the latter representing Western Bloc countries which have been democratically governed by socialist parties such as Britain, France, Sweden and Western social-democracies in general, among others.
For Andrew Vincent, "[t]he word 'socialism' finds its root in the Latin "sociare", which means to combine or to share. The related, more technical term in Roman and then medieval law was "societas". This latter word could mean companionship and fellowship as well as the more legalistic idea of a consensual contract between freemen".
"Socialism" was coined by Henri de Saint-Simon, one of the founders of what would later be labelled utopian socialism. Simon contrasted it to the liberal doctrine of individualism that emphasized the moral worth of the individual whilst stressing that people act or should act as if they are in isolation from one another. The original utopian socialists condemned this doctrine of individualism for failing to address social concerns during the Industrial Revolution, including poverty, oppression and vast inequalities in wealth. They viewed their society as harming community life by basing society on competition. They presented socialism as an alternative to liberal individualism based on the shared ownership of resources. Saint-Simon proposed economic planning, scientific administration and the application of scientific understanding to the organisation of society. By contrast, Robert Owen proposed to organise production and ownership via cooperatives. "Socialism" is also attributed in France to Pierre Leroux and Marie Roch Louis Reybaud while in Britain it is associated to Owen, who became one of the fathers of the cooperative movement.

The definition and usage of "socialism" settled by the 1860s, replacing "associationist", "co-operative" and "mutualist" that had been used as synonyms while "communism" fell out of use during this period. An early distinction between "communism" and "socialism" was that the latter aimed to only socialise production while the former aimed to socialise both production and consumption (in the form of free access to final goods). By 1888, Marxists employed "socialism" in place of "communism" as the latter had come to be considered an old-fashion synonym for "socialism". It was not until after the Bolshevik Revolution that "socialism" was appropriated by Vladimir Lenin to mean a stage between capitalism and communism. He used it to defend the Bolshevik program from Marxist criticism that Russia's productive forces were not sufficiently developed for communism. The distinction between "communism" and "socialism" became salient in 1918 after the Russian Social Democratic Labour Party renamed itself to the All-Russian Communist Party, interpreting "communism" specifically to mean socialists who supported the politics and theories of Bolshevism, Leninism and later that of Marxism–Leninism, although communist parties continued to describe themselves as socialists dedicated to socialism. According to "The Oxford Handbook of Karl Marx", "Marx used many terms to refer to a post-capitalist society—positive humanism, socialism, Communism, realm of free individuality, free association of producers, etc. He used these terms completely interchangeably. The notion that 'socialism' and 'Communism' are distinct historical stages is alien to his work and only entered the lexicon of Marxism after his death".

In Christian Europe, communists were believed to have adopted atheism. In Protestant England, "communism" was too close to the Roman Catholic communion rite, hence "socialist" was the preferred term. Engels argued that in 1848, when "The Communist Manifesto" was published, socialism was respectable in Europe while communism was not. The Owenites in England and the Fourierists in France were considered respectable socialists while working-class movements that "proclaimed the necessity of total social change" denoted themselves "communists". This branch of socialism produced the communist work of Étienne Cabet in France and Wilhelm Weitling in Germany. British moral philosopher John Stuart Mill discussed a form of economic socialism within a liberal context that would later be known as liberal socialism. In later editions of his "Principles of Political Economy" (1848), Mill further argued that "as far as economic theory was concerned, there is nothing in principle in economic theory that precludes an economic order based on socialist policies" and promoted substituting capitalist businesses with worker cooperatives. While democrats looked to the Revolutions of 1848 as a democratic revolution which in the long run ensured liberty, equality and fraternity, Marxists denounced it as a betrayal of working-class ideals by a bourgeoisie indifferent to the proletariat.

Socialist models and ideas espousing common or public ownership have existed since antiquity. The economy of the 3rd century BCE Mauryan Empire of India was described as "a socialized monarchy" and "a sort of state socialism". Elements of socialist thought were discerned in the politics of classical Greek philosophers Plato and Aristotle. Mazdak the Younger (died c. 524 or 528 CE), a Persian communal proto-socialist, instituted communal possessions and advocated the public good. Abu Dharr al-Ghifari, a Companion of Muhammad, is credited by multiple authors as a principal antecedent of Islamic socialism. The teachings of Jesus are frequently described as socialist, especially by Christian socialists. records that in the early church in Jerusalem "[n]o one claimed that any of their possessions was their own", although the pattern soon disappears from church history except within monasticism. Christian socialism was one of the founding threads of the British Labour Party and is claimed to begin with the uprising of Wat Tyler and John Ball in the 14th century CE. After the French Revolution, activists and theorists such as François-Noël Babeuf, Étienne-Gabriel Morelly, Philippe Buonarroti and Auguste Blanqui influenced the early French labour and socialist movements. In Britain, Thomas Paine proposed a detailed plan to tax property owners to pay for the needs of the poor in "Agrarian Justice" while Charles Hall wrote "The Effects of Civilization on the People in European States", denouncing capitalism's effects on the poor of his time. This work influenced the utopian schemes of Thomas Spence.

The first self-conscious socialist movements developed in the 1820s and 1830s. Fourierists, Owenites and Saint-Simonians and provided a series of analyses and interpretations of society. Especially the Owenites overlapped with other working-class movements such as the Chartists in the United Kingdom. The Chartists gathered significant numbers around the People's Charter of 1838 which sought democratic reforms focused on the extension of suffrage to all male adults. Leaders in the movement called for a more equitable distribution of income and better living conditions for the working classes. The first trade unions and consumer cooperative societies followed the Chartist movement. Pierre-Joseph Proudhon proposed his philosophy of mutualism in which "everyone had an equal claim, either alone or as part of a small cooperative, to possess and use land and other resources as needed to make a living". Other currents inspired Christian socialism "often in Britain and then usually coming out of left liberal politics and a romantic anti-industrialism" which produced theorists such as Edward Bellamy, Charles Kingsley and Frederick Denison Maurice.

The first advocates of socialism favoured social levelling in order to create a meritocratic or technocratic society based on individual talent. Henri de Saint-Simon was fascinated by the potential of science and technology and advocated a socialist society that would eliminate the disorderly aspects of capitalism based on equal opportunities. He sought a society in which each person was ranked according to his or her capacities and rewarded according to his or her work. His key focus was on administrative efficiency and industrialism and a belief that science was essential to progress. This was accompanied by a desire for a rationally organised economy based on planning and geared towards large-scale scientific and material progress. Other early socialist thinkers such as Charles Hall and Thomas Hodgkin based their ideas on David Ricardo's economic theories. They reasoned that the equilibrium value of commodities approximated prices charged by the producer when those commodities were in elastic supply and that these producer prices corresponded to the embodied labour—the cost of the labour (essentially the wages paid) that was required to produce the commodities. The Ricardian socialists viewed profit, interest and rent as deductions from this exchange-value.

West European social critics, including Louis Blanc, Charles Fourier, Charles Hall, Robert Owen, Pierre-Joseph Proudhon and Saint-Simon were the first modern socialists who criticised the poverty and inequality of the Industrial Revolution. They advocated reform, Owen advocating the transformation of society to small communities without private property. Owen's contribution to modern socialism was his claim that individual actions and characteristics were largely determined by their social environment. On the other hand, Fourier advocated Phalanstères (communities that respected individual desires, including sexual preferences), affinities and creativity and saw that work has to be made enjoyable for people. Owen and Fourier's ideas were practiced in intentional communities around Europe and North America in the mid-19th century.

The Paris Commune was a government that ruled Paris from 18 March (formally, from 28 March) to 28 May 1871. The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War. The Commune elections were held on 26 March. They elected a Commune council of 92 members, one member for each 20,000 residents. Despite internal differences, the council began to organise public services. It reached a consensus on certain policies that tended towards a progressive, secular and highly democratic social democracy.

Because the Commune was able to meet on fewer than 60 days in total, only a few decrees were actually implemented. These included the separation of church and state; the remission of rents owed for the period of the siege (during which payment had been suspended); the abolition of night work in the hundreds of Paris bakeries; the granting of pensions to the unmarried companions and children of National Guards killed on active service; and the free return of all workmen's tools and household items valued up to 20 francs that had been pledged during the siege. The Commune was concerned that skilled workers had been forced to pawn their tools during the war; the postponement of commercial debt obligations and the abolition of interest on the debts; and the right of employees to take over and run an enterprise if it were deserted by its owner. The Commune nonetheless recognised the previous owner's right to compensation.

In 1864, the First International was founded in London. It united diverse revolutionary currents, including socialists such as the French followers of Proudhon, Blanquists, Philadelphes, English trade unionists and social democrats. In 1865 and 1866, it held a preliminary conference and had its first congress in Geneva, respectively. Due to their wide variety of philosophies, conflict immediately erupted. The first objections to Marx came from the mutualists who opposed state socialism. Shortly after Mikhail Bakunin and his followers joined in 1868, the First International became polarised into camps headed by Marx and Bakunin. The clearest differences between the groups emerged over their proposed strategies for achieving their visions. The First International became the first major international forum for the promulgation of socialist ideas.

Bakunin's followers were called collectivists and sought to collectivise ownership of the means of production while retaining payment proportional to the amount and kind of labour of each individual. Like Proudhonists, they asserted the right of each individual to the product of his labour and to be remunerated for his particular contribution to production. By contrast, anarcho-communists sought collective ownership of both the means and the products of labour. As Errico Malatesta put it, "instead of running the risk of making a confusion in trying to distinguish what you and I each do, let us all work and put everything in common. In this way each will give to society all that his strength permits until enough is produced for every one; and each will take all that he needs, limiting his needs only in those things of which there is not yet plenty for every one". Anarcho-communism as a coherent economic-political philosophy was first formulated in the Italian section of the First International by Malatesta, Carlo Cafiero, Emilio Covelli, Andrea Costa and other ex-Mazzinian republicans. Out of respect for Bakunin, they did not make their differences with collectivist anarchism explicit until after his death.

Syndicalism emerged in France inspired in part by Proudhon and later by Pelloutier and Georges Sorel. It developed at the end of the 19th century out of the French trade-union movement ("syndicat" is the French word for trade union). It was a significant force in Italy and Spain in the early 20th century until it was crushed by the fascist regimes in those countries. In the United States, syndicalism appeared in the guise of the Industrial Workers of the World, or "Wobblies", founded in 1905. Syndicalism is an economic system that organises industries into confederations (syndicates) and the economy is managed by negotiation between specialists and worker representatives of each field, comprising multiple non-competitive categorised units. Syndicalism is a form of communism and economic corporatism, but also refers to the political movement and tactics used to bring about this type of system. An influential anarchist movement based on syndicalist ideas is anarcho-syndicalism. The International Workers Association is an international anarcho-syndicalist federation of various labour unions.

The Fabian Society is a British socialist organisation established to advance socialism via gradualist and reformist means. The society laid many foundations of the Labour Party and subsequently affected the policies of states emerging from the decolonisation of the British Empire, most notably India and Singapore. Originally, the Fabian Society was committed to the establishment of a socialist economy, alongside a commitment to British imperialism as a progressive and modernising force. Later, the society functioned primarily as a think tank and is one of fifteen socialist societies affiliated with the Labour Party. Similar societies exist in Australia (the Australian Fabian Society), in Canada (the Douglas-Coldwell Foundation and the now disbanded League for Social Reconstruction) and in New Zealand.

Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public". It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. Inspired by medieval guilds, theorists such as Samuel George Hobson and G. D. H. Cole advocated the public ownership of industries and their workforces' organisation into guilds, each of which under the democratic control of its trade union. Guild socialists were less inclined than Fabians to invest power in a state. At some point, like the American Knights of Labor, guild socialism wanted to abolish the wage system.

As the ideas of Marx and Engels gained acceptance, particularly in central Europe, socialists sought to unite in an international organisation. In 1889 (the centennial of the French Revolution), the Second International was founded, with 384 delegates from twenty countries representing about 300 labour and socialist organisations. It was termed the Socialist International and Engels was elected honorary president at the third congress in 1893. Anarchists were banned, mainly due to pressure from Marxists. It has been argued that at some point the Second International turned "into a battleground over the issue of libertarian versus authoritarian socialism. Not only did they effectively present themselves as champions of minority rights; they also provoked the German Marxists into demonstrating a dictatorial intolerance which was a factor in preventing the British labour movement from following the Marxist direction indicated by such leaders as H. M. Hyndman".

Reformism arose as an alternative to revolution. Eduard Bernstein was a leading social democrat in Germany who proposed the concept of evolutionary socialism. Revolutionary socialists quickly targeted reformism: Rosa Luxemburg condemned Bernstein's "Evolutionary Socialism" in her 1900 essay "Social Reform or Revolution?" Revolutionary socialism encompasses multiple social and political movements that may define "revolution" differently. The Social Democratic Party of Germany (SPD) became the largest and most powerful socialist party in Europe, despite working illegally until the anti-socialist laws were dropped in 1890. In the 1893 elections, it gained 1,787,000 votes, a quarter of the total votes cast, according to Engels. In 1895, the year of his death, Engels emphasised "The Communist Manifesto"'s emphasis on winning, as a first step, the "battle of democracy".

In Argentina, the Socialist Party of Argentina was established in the 1890s led by Juan B. Justo and Nicolás Repetto, among others. It was the first mass party in the country and in Latin America. The party affiliated itself with the Second International. Between 1924 and 1940, it was a member of the Labour and Socialist International.

In 1904, Australians elected Chris Watson as the first Australian Labor Party Prime Minister, becoming the first democratically elected socialist. In 1909, the first Kibbutz was established in Palestine by Russian Jewish Immigrants. The Kibbutz Movement expanded through the 20th century following a doctrine of Zionist socialism. The British Labour Party first won seats in the House of Commons in 1902. The International Socialist Commission (ISC, also known as Berne International) was formed in February 1919 at a meeting in Bern by parties that wanted to resurrect the Second International.

By 1917, the patriotism of World War I changed into political radicalism in Australia, most of Europe and the United States. Other socialist parties from around the world who were beginning to gain importance in their national politics in the early 20th century included the Italian Socialist Party, the French Section of the Workers' International, the Spanish Socialist Workers' Party, the Swedish Social Democratic Party, the Russian Social Democratic Labour Party and the Socialist Party in Argentina, the Socialist Workers' Party in Chile and the Socialist Party of America in the United States.

In February 1917, revolution exploded in Russia. Workers, soldiers and peasants established soviets (councils), the monarchy fell and a provisional government convened pending the election of a constituent assembly. In April of that year, Vladimir Lenin, leader of the Bolshevik faction of socialists in Russia and known for his profound and controversial expansions of Marxism, was allowed to cross Germany to return from exile in Switzerland.

Lenin had published essays on his analysis of imperialism, the monopoly and globalisation phase of capitalism, as well as analyses on social conditions. He observed that as capitalism had further developed in Europe and America, the workers remained unable to gain class consciousness so long as they were too busy working to pay their expenses. He therefore proposed that the social revolution would require the leadership of a vanguard party of class-conscious revolutionaries from the educated and politically active part of the population.

Upon arriving in Petrograd, Lenin declared that the revolution in Russia had only begun, and that the next step was for the workers' soviets to take full authority. He issued a thesis outlining the Bolshevik programme, including rejection of any legitimacy in the provisional government and advocacy for state power to be administered through the soviets. The Bolsheviks became the most influential force. On 7 November, the capitol of the provisional government was stormed by Bolshevik Red Guards in what afterwards became known as the Great October Socialist Revolution. The provisional government ended and the Russian Socialist Federative Soviet Republic—the world's first constitutionally socialist state—was established. On 25 January 1918, Lenin declared "Long live the world socialist revolution!" at the Petrograd Soviet and proposed an immediate armistice on all fronts and transferred the land of the landed proprietors, the crown and the monasteries to the peasant committees without compensation.

The day after assuming executive power on 25 January, Lenin wrote "Draft Regulations on Workers' Control", which granted workers control of businesses with more than five workers and office employees and access to all books, documents and stocks and whose decisions were to be "binding upon the owners of the enterprises". Governing through the elected soviets and in alliance with the peasant-based Left Socialist-Revolutionaries, the Bolshevik government began nationalising banks and industry; and disavowed the national debts of the deposed Romanov royal régime. It sued for peace, withdrawing from World War I and convoked a Constituent Assembly in which the peasant Socialist-Revolutionary Party (SR) won a majority.

The Constituent Assembly elected SR leader Victor Chernov President of a Russian republic, but rejected the Bolshevik proposal that it endorse the Soviet decrees on land, peace and workers' control and acknowledge the power of the Soviets of Workers', Soldiers' and Peasants' Deputies. The next day, the Bolsheviks declared that the assembly was elected on outdated party lists and the All-Russian Central Executive Committee of the Soviets dissolved it. In March 1919, world communist parties formed Comintern (also known as the Third International) at a meeting in Moscow.

Parties which did not want to be a part of the resurrected Second International (ISC) or Comintern formed the International Working Union of Socialist Parties (IWUSP, also known as Vienna International/Vienna Union/Two-and-a-Half International) on 27 February 1921 at a conference in Vienna. The ISC and the IWUSP joined to form the Labour and Socialist International (LSI) in May 1923 at a meeting in Hamburg Left-wing groups which did not agree to the centralisation and abandonment of the soviets by the Bolshevik Party led left-wing uprisings against the Bolsheviks—such groups included Socialist Revolutionaries, Left Socialist Revolutionaries, Mensheviks and anarchists.

Within this left-wing discontent, the most large-scale events were the worker's Kronstadt rebellion and the anarchist led Revolutionary Insurrectionary Army of Ukraine uprising which controlled an area known as the Free Territory.

The Bolshevik Russian Revolution of January 1918 launched communist parties in many countries and concomitant revolutions from 1917–1923. Few communists doubted that the Russian experience depended on successful, working-class socialist revolutions in developed capitalist countries. In 1919, Lenin and Trotsky organised the world's communist parties into an international association of workers—the Communist International (Comintern), also called the Third International.

The Russian Revolution influenced uprisings in other countries. The German Revolution of 1918–1919 replaced Germany's imperial government with a republic. The revolution lasted from November 1918 until the establishment of the Weimar Republic in August 1919. It included an episode known as the Bavarian Soviet Republic and the Spartacist uprising. In Italy, the events known as the "Biennio Rosso" were characterised by mass strikes, worker demonstrations and self-management experiments through land and factory occupations. In Turin and Milan, workers' councils were formed and many factory occupations took place led by anarcho-syndicalists organised around the Unione Sindacale Italiana.

By 1920, the Red Army under Trotsky had largely defeated the royalist White Armies. In 1921, War Communism was ended and under the New Economic Policy (NEP) private ownership was allowed for small and medium peasant enterprises. While industry remained largely state-controlled, Lenin acknowledged that the NEP was a necessary capitalist measure for a country unready for socialism. Profiteering returned in the form of "NEP men" and rich peasants (kulaks) gained power. Trotsky's role was questioned by other socialists, including ex-Trotskyists. In the United States, Dwight Macdonald broke with Trotsky and left the Trotskyist Socialist Workers Party by noting the Kronstadt rebellion, which Trotsky and the other Bolsheviks had brutally repressed. He then moved towards democratic socialism and anarchism.

A similar critique of Trotsky's role in the Kronstadt rebellion was raised by American anarchist Emma Goldman. In her essay "Trotsky Protests Too Much", she states, "I admit, the dictatorship under Stalin's rule has become monstrous. That does not, however, lessen the guilt of Leon Trotsky as one of the actors in the revolutionary drama of which Kronstadt was one of the bloodiest scenes".

In 1922, the fourth congress of the Communist International took up the policy of the United Front. It urged communists to work with rank and file social democrats while remaining critical of their leaders. They criticised those leaders for betraying the working class by supporting the capitalists' war efforts. The social democrats pointed to the dislocation caused by revolution and later the growing authoritarianism of the communist parties. The Labour Party rejected the Communist Party of Great Britain's application to affiliate to them in 1920.

On seeing the Soviet State's growing coercive power in 1923, a dying Lenin said Russia had reverted to "a bourgeois tsarist machine [...] barely varnished with socialism". After Lenin's death in January 1924, the Communist Party of the Soviet Union—then increasingly under the control of Joseph Stalin—rejected the theory that socialism could not be built solely in the Soviet Union in favour of the concept of socialism in one country. Despite the marginalised Left Opposition's demand for the restoration of Soviet democracy, Stalin developed a bureaucratic, authoritarian government that was condemned by democratic socialists, anarchists and Trotskyists for undermining the Revolution's ideals.

In 1924, the Mongolian People's Republic was established and was ruled by the Mongolian People's Party. The Russian Revolution and its aftermath motivated national communist parties elsewhere that gained political and social influence, in France, the US, Italy, China, Mexico, the Brazil, Chile and Indonesia.

In Spain in 1936, the national anarcho-syndicalist trade union Confederación Nacional del Trabajo (CNT) initially refused to join a popular front electoral alliance. Their abstention led to a right-wing election victory. In 1936, the CNT changed its policy and anarchist votes helped return the popular front to power. Months later, the former ruling class attempted a coup, sparking the Spanish Civil War (1936–1939).

In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land. The Spanish Revolution was a workers' social revolution that began with the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles in some areas for two to three years, primarily Catalonia, Aragon, Andalusia and parts of Levante.

Much of Spain's economy came under worker control. In anarchist strongholds like Catalonia the figure was as high as 75%, but lower in areas with heavy Communist Party influence, which actively resisted attempts at collectivisation. Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes. Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or indirectly in the Spanish Revolution.

Trotsky's Fourth International was established in France in 1938 when Trotskyists argued that the Comintern or Third International had become irretrievably "lost to Stalinism" and thus incapable of leading the working class to power. The rise of Nazism and the start of World War II led to the dissolution of the LSI in 1940. After the War, the Socialist International was formed in Frankfurt in July 1951 as its successor.

After World War II, social democratic governments introduced social reform and wealth redistribution via welfare and taxation. Social democratic parties dominated post-war politics in countries such as France, Italy, Czechoslovakia, Belgium and Norway. At one point, France claimed to be the world's most state-controlled capitalist country. It nationalised public utilities including Charbonnages de France (CDF), Électricité de France (EDF), Gaz de France (GDF), Air France, Banque de France and Régie Nationale des Usines Renault.

In 1945, the British Labour Party led by Clement Attlee was elected based on a radical socialist programme. The Labour government nationalised industries including mines, gas, coal, electricity, rail, iron, steel and the Bank of England. British Petroleum was officially nationalised in 1951. Anthony Crosland said that in 1956 25% of British industry was nationalised and that public employees, including those in nationalised industries, constituted a similar proportion of the country's workers. The Labour Governments of 1964–1970 and 1974–1979 intervened further. It re-nationalised British Steel (1967) after the Conservatives had denationalised it and nationalised British Leyland (1976). The National Health Service provided taxpayer-funded health care to everyone, free at the point of service. Working-class housing was provided in council housing estates and university education became available via a school grant system.

During most of the post-war era, Sweden was governed by the Swedish Social Democratic Party largely in cooperation with trade unions and industry. In Sweden, the Swedish Social Democratic Party held power from 1936 to 1976, 1982 to 1991, 1994 to 2006 and 2014 through 2023, most recently in a minority coalition. Tage Erlander was the first leader of the Swedish Social Democratic Party (SSDP). He led the government from 1946 to 1969, the longest uninterrupted parliamentary government. These governments substantially expanded the welfare state. Swedish Prime Minister Olof Palme identified as a "democratic socialist" and was described as a "revolutionary reformist".

The Norwegian Labour Party was established in 1887 and was largely a trade union federation. The party did not proclaim a socialist agenda, elevating universal suffrage and dissolution of the union with Sweden as its top priorities. In 1899, the Norwegian Confederation of Trade Unions separated from the Labour Party. Around the time of the Russian Revolution, the Labour Party moved to the left and joined the Communist International from 1919 through 1923. Thereafter, the party still regarded itself as revolutionary, but the party's left-wing broke away and established the Communist Party of Norway while the Labour Party gradually adopted a reformist line around 1930. In 1935, Johan Nygaardsvold established a coalition that lasted until 1945.

From 1946 to 1962, the Norwegian Labour Party held an absolute majority in the parliament led by Einar Gerhardsen, who remained Prime Minister for seventeen years. Although the party abandoned most of its pre-war socialist ideas, the welfare state was expanded under Gerhardsen to ensure the universal provision of basic human rights and stabilise the economy. In the 1945 Norwegian parliamentary election, the Communist Party took 12% of the votes, but it largely vanished during the Cold War. In the 1950s, popular socialism emerged in Nordic countries. It placed itself between communism and social democracy. In the early 1960s, the Socialist Left Party challenged the Labour Party from the left. Also in the 1960s, Gerhardsen established a planning agency and tried to establish a planned economy. In the 1970s, a more radical socialist party, the Worker's Communist Party (AKP), broke from the Socialist Left Party and had notable influence in student associations and some trade unions. The AKP identified with Communist China and Albania rather than the Soviet Union.
In countries such as Sweden, the Rehn–Meidner model allowed capitalists owning productive and efficient firms to retain profits at the expense of the firms' workers, exacerbating inequality and causing workers to agitate for a share of the profits in the 1970s. At that time, women working in the state sector began to demand better wages. Rudolf Meidner established a study committee that came up with a 1976 proposal to transfer excess profits into worker-controlled investment funds, with the intention that firms would create jobs and pay higher wages rather than reward company owners and managers. Capitalists immediately labeled this proposal as socialism and launched an unprecedented opposition—including calling off the class compromise established in the 1938 Saltsjöbaden Agreement. Social democratic parties are some of the oldest such parties and operate in all Nordic countries. Countries or political systems that have long been dominated by social democratic parties are often labelled social democratic. Those countries fit the social democratic type of "high socialism" which is described as favouring "a high level of decommodification and a low degree of stratification".

The Nordic model is a form of economic-political system common to the Nordic countries (Denmark, Finland, Iceland, Norway and Sweden). It has three main ingredients, namely peaceful, institutionalised negotiation between employers and trade unions; active, predictable and measured macroeconomic policy; and universal welfare and free education. The welfare system is governmental in Norway and Sweden whereas trade unions play a greater role in Denmark, Finland and Iceland. The Nordic model is often labelled social democratic and contrasted with the conservative continental model and the liberal Anglo-American model. Major reforms in the Nordic countries are the results of consensus and compromise across the political spectrum. Key reforms were implemented under social democratic cabinets in Denmark, Norway and Sweden while centre-right parties dominated during the implementation of the model in Finland and Iceland. Since World War II, Nordic countries have largely maintained a social democratic mixed economy, characterised by labour force participation, gender equality, egalitarian and universal benefits, redistribution of wealth and expansionary fiscal policy.

In Norway, the first mandatory social insurances were introduced by conservative cabinets in 1895 (Francis Hagerups's cabinet) and 1911 (Konow's Cabinet). During the 1930s, the Labour Party adopted the conservatives' welfare state project. After World War II, all political parties agreed that the welfare state should be expanded. Universal social security ("Folketrygden") was introduced by the conservative Borten's Cabinet. Norway's economy is open to the international or European market for most products and services, joining the European Union's internal market in 1994 through European Economic Area. Some of the mixed economy institutions from the post-war period were relaxed by the conservative cabinet of the 1980s and the finance market was deregulated. Within the "Varieties of Capitalism"-framework, Finland, Norway and Sweden are identified as coordinated market economies.

The Soviet era saw some of the most significant technological achievements of the 20th century, including the world's first spacecraft and the first astronaut. The Soviet economy was the modern world's first centrally planned economy. It adopted state ownership of industry managed through Gosplan (the State Planning Commission), Gosbank (the State Bank) and the Gossnab (State Commission for Materials and Equipment Supply).

Economic planning was conducted through serial Five-Year Plans. The emphasis was on development of heavy industry. The nation became one of the world's top manufacturers of basic and heavy industrial products, while deemphasizing light industrial production and consumer durables. Modernisation brought about a general increase in the standard of living.

The Eastern Bloc was the group of Communist states of Central and Eastern Europe, including the Soviet Union and the countries of the Warsaw Pact, including Poland, the German Democratic Republic, the Hungary, Bulgaria, Czechoslovakia, Romania, Albania and Yugoslavia. The Hungarian Revolution of 1956 was a spontaneous nationwide revolt against the Communist government, lasting from 23 October until 10 November 1956. Soviet leader Nikita Khrushchev's denunciation of the excesses of Stalin's regime during the Twentieth Communist Party Congress in 1956 as well as the Hungarian revolt, produced disunity within Western European communist and socialist parties.

In the post-war years, socialism became increasingly influential in many then-developing countries. Embracing Third World socialism, countries in Africa, Asia and Latin America often nationalised industries.

The Chinese Revolution was the second stage in the Chinese Civil War, which ended with the establishment of the People's Republic of China led by the Chinese Communist Party. The then-Chinese Kuomintang Party in the 1920s incorporated Chinese socialism as part of its ideology.

The emergence of this new political entity in the frame of the Cold War was complex and painful. Several tentative efforts were made to organise newly independent states in order to establish a common front to limit the United States' and the Soviet Union's influence on them. This led to the Sino-Soviet split. The Non-Aligned Movement gathered around the figures of Jawaharlal Nehru of India, Sukarno of Indonesia, Josip Broz Tito of Yugoslavia and Gamal Abdel Nasser of Egypt. After the 1954 Geneva Conference which ended the French war in Vietnam, the 1955 Bandung Conference gathered Nasser, Nehru, Tito, Sukarno and Chinese Premier Zhou Enlai. As many African countries gained independence during the 1960s, some of them rejected capitalism in favour of African socialism as defined by Julius Nyerere of Tanzania, Léopold Senghor of Senegal, Kwame Nkrumah of Ghana and Sékou Touré of Guinea.

The Cuban Revolution (1953–1959) was an armed revolt conducted by Fidel Castro's 26th of July Movement and its allies against the government of Fulgencio Batista. Castro's government eventually adopted communism, becoming the Communist Party of Cuba in October 1965.

In Indonesia, a right-wing military regime led by Suharto killed between 500,000 and one million people in 1965 and 1966, mainly to crush the growing influence of the Communist Party and other leftist groups, with support from the United States which provided kill lists containing thousands of names of suspected high-ranking Communists.

The New Left was a term used mainly in the United Kingdom and United States in reference to activists, educators, agitators and others in the 1960s and 1970s who sought to implement a broad range of reforms on issues such as gay rights, abortion, gender roles and drugs in contrast to earlier leftist or Marxist movements that had taken a more vanguardist approach to social justice and focused mostly on labour unionisation and questions of social class. The New Left rejected involvement with the labour movement and Marxism's historical theory of class struggle.

In the United States, the New Left was associated with the Hippie movement and anti-war college campus protest movements as well as the black liberation movements such as the Black Panther Party. While initially formed in opposition to the "Old Left" Democratic Party, groups composing the New Left gradually became central players in the Democratic coalition.

The protests of 1968 represented a worldwide escalation of social conflicts, predominantly characterised by popular rebellions against military, capitalist and bureaucratic elites who responded with an escalation of political repression. These protests marked a turning point for the civil rights movement in the United States which produced revolutionary movements like the Black Panther Party. The prominent civil rights leader Martin Luther King Jr. organised the "Poor People's Campaign" to address issues of economic justice, while personally showing sympathy with democratic socialism. In reaction to the Tet Offensive, protests also sparked a broad movement in opposition to the Vietnam War all over the United States and even into London, Paris, Berlin and Rome. In 1968, the International of Anarchist Federations was founded during an international anarchist conference held in Carrara by the three existing European federations of France, the Italian and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile.

Mass socialist or communist movements grew not only in the United States, but also in most European countries. The most spectacular manifestation of this were the May 1968 protests in France in which students linked up with strikes of up to ten million workers and for a few days the movement seemed capable of overthrowing the government.

In many other capitalist countries, struggles against dictatorships, state repression and colonisation were also marked by protests in 1968, such as the beginning of the Troubles in Northern Ireland, the Tlatelolco massacre in Mexico City and the escalation of guerrilla warfare against the military dictatorship in Brazil. Countries governed by communist parties had protests against bureaucratic and military elites. In Eastern Europe there were widespread protests that escalated particularly in the Prague Spring in Czechoslovakia. In response, Soviet Union occupied Czechoslovakia, but the occupation was denounced by the Italian and French communist parties and the Communist Party of Finland. Few western European political leaders defended the occupation, among them the Portuguese communist secretary-general Álvaro Cunhal. along with the Luxembourg party and conservative factions of the Communist Party of Greece.

In the Chinese Cultural Revolution, a social-political youth movement mobilised against "bourgeois" elements which were seen to be infiltrating the government and society at large, aiming to restore capitalism. This movement motivated Maoism-inspired movements around the world in the context of the Sino-Soviet split.

In the 1960s, a socialist tendency within the Latin American Catholic church appeared and was known as liberation theology It motivated the Colombian priest Camilo Torres Restrepo to enter the ELN guerrilla. In Chile, Salvador Allende, a physician and candidate for the Socialist Party of Chile, was elected president in 1970. In 1973, his government was ousted by the United States-backed military dictatorship of Augusto Pinochet, which lasted until the late 1980s. Pinochet's regime was a leader of Operation Condor, a U.S.-backed campaign of repression and state terrorism carried out by the intelligence services of the Southern Cone countries of Latin America to eliminate suspected Communist subversion. In Jamaica, the democratic socialist Michael Manley served as the fourth Prime Minister of Jamaica from 1972 to 1980 and from 1989 to 1992. According to opinion polls, he remains one of Jamaica's most popular Prime Ministers since independence. The Nicaraguan Revolution encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–1979, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990 and the socialist measures which included wide-scale agrarian reform and educational programs. The People's Revolutionary Government was proclaimed on 13 March 1979 in Grenada which was overthrown by armed forces of the United States in 1983. The Salvadoran Civil War (1979–1992) was a conflict between the military-led government of El Salvador and the Farabundo Martí National Liberation Front (FMLN), a coalition or umbrella organisation of five socialist guerrilla groups. A coup on 15 October 1979 led to the killings of anti-coup protesters by the government as well as anti-disorder protesters by the guerrillas, and is widely seen as the tipping point towards the civil war.

In Italy, Autonomia Operaia was a leftist movement particularly active from 1976 to 1978. It took an important role in the autonomist movement in the 1970s, aside earlier organisations such as Potere Operaio (created after May 1968) and Lotta Continua. This experience prompted the contemporary socialist radical movement autonomism. In 1982, the newly elected French socialist government of François Mitterrand made nationalisations in a few key industries, including banks and insurance companies. Eurocommunism was a trend in the 1970s and 1980s in various Western European communist parties to develop a theory and practice of social transformation that was more relevant for a Western European country and less aligned to the influence or control of the Communist Party of the Soviet Union. Outside Western Europe, it is sometimes called neocommunism. Some communist parties with strong popular support, notably the Italian Communist Party (PCI) and the Communist Party of Spain (PCE) adopted Eurocommunism most enthusiastically and the Communist Party of Finland was dominated by Eurocommunists. The French Communist Party (PCF) and many smaller parties strongly opposed Eurocommunism and stayed aligned with the Communist Party of the Soviet Union until the end of the Soviet Union.

In the late 1970s and in the 1980s, the Socialist International (SI) had extensive contacts and discussion with the two powers of the Cold War, the United States and the Soviet Union, about east–west relations and arms control. Since then, the SI has admitted as member parties the Nicaraguan FSLN, the left-wing Puerto Rican Independence Party, as well as former communist parties such as the Democratic Party of the Left of Italy and the Front for the Liberation of Mozambique (FRELIMO). The SI aided social democratic parties in re-establishing themselves when dictatorship gave way to democracy in Portugal (1974) and Spain (1975). Until its 1976 Geneva Congress, the SI had few members outside Europe and no formal involvement with Latin America.
After Mao's death in 1976 and the arrest of the faction known as the Gang of Four, who were blamed for the excesses of the Cultural Revolution, Deng Xiaoping took power and led the People's Republic of China to significant economic reforms. The Communist Party of China loosened governmental control over citizens' personal lives and the communes were disbanded in favour of private land leases, thus China's transition from a planned economy to a mixed economy named as "socialism with Chinese characteristics" which maintained state ownership rights over land, state or cooperative ownership of much of the heavy industrial and manufacturing sectors and state influence in the banking and financial sectors. China adopted its current constitution on 4 December 1982. President Jiang Zemin and Premier Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. At the Sixth National Congress of the Communist Party of Vietnam in December 1986, reformist politicians replaced the "old guard" government with new leadership. The reformers were led by 71-year-old Nguyen Van Linh, who became the party's new general secretary. Linh and the reformers implemented a series of free market reforms—known as "" ("Renovation")—which carefully managed the transition from a planned economy to a "socialist-oriented market economy". Mikhail Gorbachev wished to move the Soviet Union towards of Nordic-style social democracy, calling it "a socialist beacon for all mankind". Prior to its dissolution in 1991, the economy of the Soviet Union was the second largest in the world after the United States. With the collapse of the Soviet Union, the economic integration of the Soviet republics was dissolved and overall industrial activity declined substantially. A lasting legacy remains in the physical infrastructure created during decades of combined industrial production practices, and widespread environmental destruction. The transition to capitalism in the former Soviet Union and Eastern Bloc, which was accompanied by Washington Consensus-inspired "shock therapy", resulted in a steep fall in the standard of living. The region experienced rising economic inequality and poverty a surge in excess mortality and a decline in life expectancy, which was accompanied by the entrenchment of a newly established business oligarchy in the former. The average post-communist country had returned to 1989 levels of per-capita GDP by 2005, although some are still far behind that. These developments led to increased nationalist sentiment and nostalgia for the Communist era.

Many social democratic parties, particularly after the Cold War, adopted neoliberal market policies including privatisation, deregulation and financialisation. They abandoned their pursuit of moderate socialism in favour of economic liberalism. By the 1980s, with the rise of conservative neoliberal politicians such as Ronald Reagan in the United States, Margaret Thatcher in Britain, Brian Mulroney in Canada and Augusto Pinochet in Chile, the Western welfare state was attacked from within, but state support for the corporate sector was maintained. Monetarists and neoliberals attacked social welfare systems as impediments to private entrepreneurship. In the United Kingdom, Labour Party leader Neil Kinnock made a public attack against the entryist group Militant at the 1985 Labour Party conference. Labour ruled that Militant was ineligible for affiliation with the party and it gradually expelled Militant supporters. The Kinnock leadership had refused to support the 1984–1985 miner's strike over pit closures, a decision that the party's left wing and the National Union of Mineworkers blamed for the strike's eventual defeat. In 1989, the 18th Congress of the Socialist International adopted a new Declaration of Principles, stating:
Democratic socialism is an international movement for freedom, social justice, and solidarity. Its goal is to achieve a peaceful world where these basic values can be enhanced and where each individual can live a meaningful life with the full development of his or her personality and talents, and with the guarantee of human and civil rights in a democratic framework of society.

In the 1990s, the British Labour Party under Tony Blair enacted policies based on the free market economy to deliver public services via the private finance initiative. Influential in these policies was the idea of a "Third Way" which called for a re-evaluation of welfare state policies. In 1995, the Labour Party re-defined its stance on socialism by re-wording Clause IV of its constitution, defining socialism in ethical terms and removing all references to public, direct worker or municipal ownership of the means of production. The Labour Party stated: "The Labour Party is a democratic socialist party. It believes that, by the strength of our common endeavour we achieve more than we achieve alone, so as to create, for each of us, the means to realise our true potential, and, for all of us, a community in which power, wealth, and opportunity are in the hands of the many, not the few".

African socialism has been and continues to be a major ideology around the continent. Julius Nyerere was inspired by Fabian socialist ideals. He was a firm believer in rural Africans and their traditions and ujamaa, a system of collectivisation that according to Nyerere was present before European imperialism. Essentially he believed Africans were already socialists. Other African socialists include Jomo Kenyatta, Kenneth Kaunda, Nelson Mandela and Kwame Nkrumah. Fela Kuti was inspired by socialism and called for a democratic African republic. In South Africa the African National Congress (ANC) abandoned its partial socialist allegiances after taking power and followed a standard neoliberal route. From 2005 through to 2007, the country was wracked by many thousands of protests from poor communities. One of these gave rise to a mass movement of shack dwellers, Abahlali baseMjondolo that despite major police suppression continues to work for popular people's planning and against the creation of a market economy in land and housing.

In Asia, states with socialist economies—such as the People's Republic of China, North Korea, Laos and Vietnam—have largely moved away from centralised economic planning in the 21st century, placing a greater emphasis on markets. Forms include the Chinese socialist market economy and the Vietnamese socialist-oriented market economy. They use state-owned corporate management models as opposed to modelling socialist enterprise on traditional management styles employed by government agencies. In China living standards continued to improve rapidly despite the late-2000s recession, but centralised political control remained tight. Brian Reynolds Myers in his book "The Cleanest Race", later supported by other academics, dismisses the idea that "Juche" is North Korea's leading ideology, regarding its public exaltation as designed to deceive foreigners and that it exists to be praised and not actually read, pointing out that North Korea's constitution of 2009 omits all mention of communism.

Although the authority of the state remained unchallenged under "Đổi Mới", the government of Vietnam encourages private ownership of farms and factories, economic deregulation and foreign investment, while maintaining control over strategic industries. The Vietnamese economy subsequently achieved strong growth in agricultural and industrial production, construction, exports and foreign investment. However, these reforms have also caused a rise in income inequality and gender disparities.

Elsewhere in Asia, some elected socialist parties and communist parties remain prominent, particularly in India and Nepal. The Communist Party of Nepal in particular calls for multi-party democracy, social equality and economic prosperity. In Singapore, a majority of the GDP is still generated from the state sector comprising government-linked companies. In Japan, there has been a resurgent interest in the Japanese Communist Party among workers and youth. In Malaysia, the Socialist Party of Malaysia got its first Member of Parliament, Dr. Jeyakumar Devaraj, after the 2008 general election. In 2010, there were 270 kibbutzim in Israel. Their factories and farms account for 9% of Israel's industrial output, worth US$8 billion and 40% of its agricultural output, worth over $1.7 billion. Some Kibbutzim had also developed substantial high-tech and military industries. Also in 2010, Kibbutz Sasa, containing some 200 members, generated $850 million in annual revenue from its military-plastics industry.

The United Nations "World Happiness Report 2013" shows that the happiest nations are concentrated in Northern Europe, where the Nordic model is employed, with Denmark topping the list. This is at times attributed to the success of the Nordic model in the region that has been labelled social democratic in contrast with the conservative continental model and the liberal Anglo-American model. The Nordic countries ranked highest on the metrics of real GDP per capita, healthy life expectancy, having someone to count on, perceived freedom to make life choices, generosity and freedom from corruption.

The objectives of the Party of European Socialists, the European Parliament's socialist and social democratic bloc, are now "to pursue international aims in respect of the principles on which the European Union is based, namely principles of freedom, equality, solidarity, democracy, respect of Human Rights and Fundamental Freedoms, and respect for the Rule of Law". As a result, today the rallying cry of the French Revolution—"Liberté, égalité, fraternité"—is promoted as essential socialist values. To the left of the PES at the European level is the Party of the European Left (PEL), also commonly abbreviated "European Left"), which is a political party at the European level and an association of democratic socialist, socialist and communist political parties in the European Union and other European countries. It was formed in January 2004 for the purposes of running in the 2004 European Parliament elections. PEL was founded on 8–9 May 2004 in Rome. Elected MEPs from member parties of the European Left sit in the European United Left–Nordic Green Left (GUE/NGL) group in the European parliament.
The socialist Left Party in Germany grew in popularity due to dissatisfaction with the increasingly neoliberal policies of the SPD, becoming the fourth biggest party in parliament in the general election on 27 September 2009. Communist candidate Dimitris Christofias won a crucial presidential runoff in Cyprus, defeating his conservative rival with a majority of 53%. In Ireland, in the 2009 European election Joe Higgins of the Socialist Party took one of three seats in the capital Dublin European constituency.

In Denmark, the Socialist People's Party (SF) more than doubled its parliamentary representation to 23 seats from 11, making it the fourth largest party. In 2011, the Social Democrats, Socialist People's Party and the Danish Social Liberal Party formed government, after a slight victory over the main rival political coalition. They were led by Helle Thorning-Schmidt, and had the Red-Green Alliance as a supporting party.

In Norway, the Red-Green Coalition consists of the Labour Party (Ap), the Socialist Left Party (SV) and the Centre Party (Sp) and governed the country as a majority government from the 2005 general election until 2013.

In the Greek legislative election of January 2015, the Coalition of the Radical Left (SYRIZA) led by Alexis Tsipras won a legislative election for the first time while the Communist Party of Greece won 15 seats in parliament. SYRIZA has been characterised as an anti-establishment party, whose success has sent "shock-waves across the EU".

In the United Kingdom, the National Union of Rail, Maritime and Transport Workers put forward a slate of candidates in the 2009 European Parliament elections under the banner of No to EU – Yes to Democracy, a broad left-wing alter-globalisation coalition involving socialist groups such as the Socialist Party, aiming to offer an alternative to the "anti-foreigner" and pro-business policies of the UK Independence Party. In the following May 2010 United Kingdom general election, the Trade Unionist and Socialist Coalition, launched in January 2010 and backed by Bob Crow, the leader of the National Union of Rail, Maritime and Transport Workers union (RMT), other union leaders and the Socialist Party among other socialist groups, stood against Labour in 40 constituencies. The Trade Unionist and Socialist Coalition contested the 2011 local elections, having gained the endorsement of the RMT June 2010 conference, but gained no seats. Left Unity was also founded in 2013 after the film director Ken Loach appealed for a new party of the left to replace the Labour Party, which he claimed had failed to oppose austerity and had shifted towards neoliberalism. In 2015, following a defeat at the 2015 United Kingdom general election, self-described socialist Jeremy Corbyn took over from Ed Miliband as leader of the Labour Party.

In France, Olivier Besancenot, the Revolutionary Communist League (LCR) candidate in the 2007 presidential election, received 1,498,581 votes, 4.08%, double that of the communist candidate. The LCR abolished itself in 2009 to initiate a broad anti-capitalist party, the New Anticapitalist Party, whose stated aim is to "build a new socialist, democratic perspective for the twenty-first century".

On 25 May 2014, the Spanish left-wing party Podemos entered candidates for the 2014 European parliamentary elections, some of which were unemployed. In a surprise result, it polled 7.98% of the vote and thus was awarded five seats out of 54 while the older United Left was the third largest overall force obtaining 10.03% and 5 seats, 4 more than the previous elections.

The government of Portugal established on 26 November 2015 was a Socialist Party (PS) minority government led by prime minister António Costa, who succeeded in securing support for a Socialist minority government by the Left Bloc (B.E.), the Portuguese Communist Party (PCP) and the Ecologist Party "The Greens" (PEV).

All around Europe and in some places of Latin America there exists a social centre and squatting movement mainly inspired by autonomist and anarchist ideas.

According to a 2013 article in "The Guardian", "[c]ontrary to popular belief, Americans don't have an innate allergy to socialism. Milwaukee has had several socialist mayors (Frank Zeidler, Emil Seidel and Daniel Hoan), and there is currently an independent socialist in the US Senate, Bernie Sanders of Vermont". Sanders, once mayor of Vermont's largest city, Burlington, has described himself as a democratic socialist and has praised Scandinavian-style social democracy. In 2016, Sanders made a bid for the Democratic Party presidential candidate, thereby gaining considerable popular support, particularly among the younger generation, but lost the nomination to Hillary Clinton. As of 2019, the Democratic Socialists of America have two members in Congress, and various members in state legislatures and city councils. According to a 2018 Gallup poll, 37% of American adults have a positive view of socialism, including 57% of Democrat-leaning voters and 16% of Republican-leaning voters. A 2019 YouGov poll found that 7 out of 10 millennials would vote for a socialist presidential candidate, and 36% had a favorable view of communism. An earlier 2019 Harris Poll found that socialism is more popular with women than men, with 55% of women between the ages of 18 and 54 preferring to live in a socialist society while a majority of men surveyed in the poll chose capitalism over socialism.

Anti-capitalism, anarchism and the anti-globalisation movement rose to prominence through events such as protests against the World Trade Organization Ministerial Conference of 1999 in Seattle. Socialist-inspired groups played an important role in these movements, which nevertheless embraced much broader layers of the population and were championed by figures such as Noam Chomsky. In Canada, the Co-operative Commonwealth Federation (CCF), the precursor to the social democratic New Democratic Party (NDP), had significant success in provincial politics. In 1944, the Saskatchewan CCF formed the first socialist government in North America. At the federal level, the NDP was the Official Opposition, from 2011 through 2015.

In their "Johnson" linguistics column, "The Economist" opines that in the 21st century United States, the term "socialism", without clear definition, has become a pejorative used by conservatives to attack liberal and progressive policies, proposals, and public figures.

For the "Encyclopedia Britannica", "the attempt by Salvador Allende to unite Marxists and other reformers in a socialist reconstruction of Chile is most representative of the direction that Latin American socialists have taken since the late 20th century. [...] Several socialist (or socialist-leaning) leaders have followed Allende's example in winning election to office in Latin American countries". The success of the Workers' Party () of Brazil, formed in 1980 and governing Brazil from 2003 to 2016, was the first major breakthrough for this trend. 
Foro de São Paulo is a conference of leftist political parties and other organisations from Latin America and the Caribbean. It was launched by the Workers' Party in 1990 in the city of São Paulo, after the PT approached other parties and social movements of Latin America and the Caribbean with the objective of debating the new international scenario after the fall of the Berlin Wall and the consequences of the implementation of what were taken as neoliberal policies adopted at the time by contemporary right-leaning governments in the region, the stated main objective of the conference being to argue for alternatives to neoliberalism. Among its members have been socialist and social-democratic parties in government in the region such as Bolivia's Movement for Socialism, the Communist Party of Cuba, Ecuador's PAIS Alliance, the United Socialist Party of Venezuela, the Socialist Party of Chile, Uruguay's Broad Front, Nicaragua's Sandinista National Liberation Front, El Salvador's Farabundo Martí National Liberation Front and members of Argentina's Frente de Todos.

In the first decade of the 21st century, Venezuelan President Hugo Chávez, Nicaraguan President Daniel Ortega, Bolivian President Evo Morales and Ecuadorian president Rafael Correa referred to their political programmes as socialist, and Chávez adopted the term "socialism of the 21st century". After winning re-election in December 2006, Chávez said: "Now more than ever, I am obliged to move Venezuela's path towards socialism". Chávez was also reelected in October 2012 for his third six-year term as president, but he died in March 2013 from cancer. After Chávez's death on 5 March 2013, Vice President from Chavez's party Nicolás Maduro assumed the powers and responsibilities of the President. A special election was held on 14 April of the same year to elect a new president, which Maduro won by a tight margin as the candidate of the United Socialist Party of Venezuela and he was formally inaugurated on 19 April. "Pink tide" is a term used in political analysis, in the media and elsewhere to describe the perception that leftist ideology in general and left-wing politics in particular were increasingly influential in Latin America in the 2000s. Some of the "pink tide" governments were criticised for turning from socialism to populism and authoritarianism. The "pink tide" was followed in the 2010s by a "conservative wave" as right-wing governments came to power in Argentina, Brazil and Chile, and Venezuela and Nicaragua experienced political crises. However, socialism saw a resurgence in 2018–19 after successive electoral victories of left-wing and centre-left candidates in Mexico, Panama, and Argentina.

Australia saw an increase in interest of socialism in the early 21st century, especially amongst youth. It is strongest in Victoria, where three socialist parties have merged into the Victorian Socialists, who aim to address problems in housing and public transportation.

In New Zealand, socialism emerged within the budding trade union movement during the late 19th century and early 20th century. In July 1916, several left-wing political organisations and trade unions merged to form the New Zealand Labour Party. While Labour traditionally had a socialist orientation, the party shifted towards a more social democratic orientation during the 1920s and 1930s. Following the 1935 general election, the First Labour Government pursued socialist policies such as nationalising industry, broadcasting, transportation, and implementing a Keynesian welfare state. However, the party did not seek to abolish capitalism, instead opting for a mixed economy. Labour's welfare state and mixed economy were not challenged until the 1980s. During the 1980s, the Fourth Labour Government implemented a raft of neoliberal economic reforms known as Rogernomics which saw New Zealand society and the economy shift towards a more free market model. Labour's abandonment of its traditional values fractured the party. Successive Labour governments have since pursued centre-left social and economic policies while maintaining a free-market economy. The current Prime Minister of New Zealand Jacinda Ardern formerly served as President of the International Union of Socialist Youth. Ardern is a self-described social democrat who has criticized capitalism as a "blatant failure" due to high levels of homelessness and low wages. New Zealand still has a small socialist scene, mainly dominated by Trotskyist groups.

Melanesian socialism developed in the 1980s, inspired by African socialism. It aims to achieve full independence from Britain and France in Melanesian territories and creation of a Melanesian federal union. It is very popular with the New Caledonia independence movement.

The Progressive Alliance is a political international founded on 22 May 2013 by political parties, the majority of whom are current or former members of the Socialist International. The organisation states the aim of becoming the global network of "the progressive, democratic, social-democratic, socialist and labour movement".

Early socialist thought took influences from a diverse range of philosophies such as civic republicanism, Enlightenment rationalism, romanticism, forms of materialism, Christianity (both Catholic and Protestant), natural law and natural rights theory, utilitarianism and liberal political economy. Another philosophical basis for a lot of early socialism was the emergence of positivism during the European Enlightenment. Positivism held that both the natural and social worlds could be understood through scientific knowledge and be analysed using scientific methods. This core outlook influenced early social scientists and different types of socialists ranging from anarchists like Peter Kropotkin to technocrats like Saint Simon.
The fundamental objective of socialism is to attain an advanced level of material production and therefore greater productivity, efficiency and rationality as compared to capitalism and all previous systems, under the view that an expansion of human productive capability is the basis for the extension of freedom and equality in society. Many forms of socialist theory hold that human behaviour is largely shaped by the social environment. In particular, socialism holds that social mores, values, cultural traits and economic practices are social creations and not the result of an immutable natural law. The object of their critique is thus not human avarice or human consciousness, but the material conditions and man-made social systems (i.e. the economic structure of society) that gives rise to observed social problems and inefficiencies. Bertrand Russell, often considered to be the father of analytic philosophy, identified as a socialist. Russell opposed the class struggle aspects of Marxism, viewing socialism solely as an adjustment of economic relations to accommodate modern machine production to benefit all of humanity through the progressive reduction of necessary work time.

Socialists view creativity as an essential aspect of human nature and define freedom as a state of being where individuals are able to express their creativity unhindered by constraints of both material scarcity and coercive social institutions. The socialist concept of individuality is intertwined with the concept of individual creative expression. Karl Marx believed that expansion of the productive forces and technology was the basis for the expansion of human freedom and that socialism, being a system that is consistent with modern developments in technology, would enable the flourishing of "free individualities" through the progressive reduction of necessary labour time. The reduction of necessary labour time to a minimum would grant individuals the opportunity to pursue the development of their true individuality and creativity.

Socialists argue that the accumulation of capital generates waste through externalities that require costly corrective regulatory measures. They also point out that this process generates wasteful industries and practices that exist only to generate sufficient demand for products such as high-pressure advertisement to be sold at a profit, thereby creating rather than satisfying economic demand.

Socialists argue that capitalism consists of irrational activity, such as the purchasing of commodities only to sell at a later time when their price appreciates, rather than for consumption, even if the commodity cannot be sold at a profit to individuals in need and therefore a crucial criticism often made by socialists is that "making money", or accumulation of capital, does not correspond to the satisfaction of demand (the production of use-values). The fundamental criterion for economic activity in capitalism is the accumulation of capital for reinvestment in production, but this spurs the development of new, non-productive industries that do not produce use-value and only exist to keep the accumulation process afloat (otherwise the system goes into crisis), such as the spread of the financial industry, contributing to the formation of economic bubbles.

Socialists view private property relations as limiting the potential of productive forces in the economy. According to socialists, private property becomes obsolete when it concentrates into centralised, socialised institutions based on private appropriation of revenue"—"but based on cooperative work and internal planning in allocation of inputs—until the role of the capitalist becomes redundant. With no need for capital accumulation and a class of owners, private property in the means of production is perceived as being an outdated form of economic organisation that should be replaced by a free association of individuals based on public or common ownership of these socialised assets. Private ownership imposes constraints on planning, leading to uncoordinated economic decisions that result in business fluctuations, unemployment and a tremendous waste of material resources during crisis of overproduction.

Excessive disparities in income distribution lead to social instability and require costly corrective measures in the form of redistributive taxation, which incurs heavy administrative costs while weakening the incentive to work, inviting dishonesty and increasing the likelihood of tax evasion while (the corrective measures) reduce the overall efficiency of the market economy. These corrective policies limit the incentive system of the market by providing things such as minimum wages, unemployment insurance, taxing profits and reducing the reserve army of labour, resulting in reduced incentives for capitalists to invest in more production. In essence, social welfare policies cripple capitalism and its incentive system and are thus unsustainable in the long-run. Marxists argue that the establishment of a socialist mode of production is the only way to overcome these deficiencies. Socialists and specifically Marxian socialists argue that the inherent conflict of interests between the working class and capital prevent optimal use of available human resources and leads to contradictory interest groups (labour and business) striving to influence the state to intervene in the economy in their favour at the expense of overall economic efficiency.

Early socialists (utopian socialists and Ricardian socialists) criticised capitalism for concentrating power and wealth within a small segment of society. In addition, they complained that capitalism does not use available technology and resources to their maximum potential in the interests of the public.

Karl Marx and Friedrich Engels argued that socialism would emerge from historical necessity as capitalism rendered itself obsolete and unsustainable from increasing internal contradictions emerging from the development of the productive forces and technology. It was these advances in the productive forces combined with the old social relations of production of capitalism that would generate contradictions, leading to working-class consciousness.
Marx and Engels held the view that the consciousness of those who earn a wage or salary (the working class in the broadest Marxist sense) would be moulded by their conditions of wage slavery, leading to a tendency to seek their freedom or emancipation by overthrowing ownership of the means of production by capitalists and consequently, overthrowing the state that upheld this economic order. For Marx and Engels, conditions determine consciousness and ending the role of the capitalist class leads eventually to a classless society in which the state would wither away. The Marxist conception of socialism is that of a specific historical phase that would displace capitalism and precede communism. The major characteristics of socialism (particularly as conceived by Marx and Engels after the Paris Commune of 1871) are that the proletariat would control the means of production through a workers' state erected by the workers in their interests. Economic activity would still be organised through the use of incentive systems and social classes would still exist, but to a lesser and diminishing extent than under capitalism.

For orthodox Marxists, socialism is the lower stage of communism based on the principle of "from each according to his ability, to each according to his contribution" while upper stage communism is based on the principle of "from each according to his ability, to each according to his need", the upper stage becoming possible only after the socialist stage further develops economic efficiency and the automation of production has led to a superabundance of goods and services. Marx argued that the material productive forces (in industry and commerce) brought into existence by capitalism predicated a cooperative society since production had become a mass social, collective activity of the working class to create commodities but with private ownership (the relations of production or property relations). This conflict between collective effort in large factories and private ownership would bring about a conscious desire in the working class to establish collective ownership commensurate with the collective efforts their daily experience.

Socialists have taken different perspectives on the state and the role it should play in revolutionary struggles, in constructing socialism and within an established socialist economy.

In the 19th century, the philosophy of state socialism was first explicitly expounded by the German political philosopher Ferdinand Lassalle. In contrast to Karl Marx's perspective of the state, Lassalle rejected the concept of the state as a class-based power structure whose main function was to preserve existing class structures. Lassalle also rejected the Marxist view that the state was destined to "wither away". Lassalle considered the state to be an entity independent of class allegiances and an instrument of justice that would therefore be essential for achieving socialism.

Preceding the Bolshevik-led revolution in Russia, many socialists including reformists, orthodox Marxist currents such as council communism, anarchists and libertarian socialists criticised the idea of using the state to conduct central planning and own the means of production as a way to establish socialism. Following the victory of Leninism in Russia, the idea of "state socialism" spread rapidly throughout the socialist movement and eventually state socialism came to be identified with the Soviet economic model.

Joseph Schumpeter rejected the association of socialism and social ownership with state ownership over the means of production because the state as it exists in its current form is a product of capitalist society and cannot be transplanted to a different institutional framework. Schumpeter argued that there would be different institutions within socialism than those that exist within modern capitalism, just as feudalism had its own distinct and unique institutional forms. The state, along with concepts like property and taxation, were concepts exclusive to commercial society (capitalism) and attempting to place them within the context of a future socialist society would amount to a distortion of these concepts by using them out of context.

Utopian socialism is a term used to define the first currents of modern socialist thought as exemplified by the work of Henri de Saint-Simon, Charles Fourier and Robert Owen which inspired Karl Marx and other early socialists. However, visions of imaginary ideal societies, which competed with revolutionary social democratic movements, were viewed as not being grounded in the material conditions of society and as reactionary. Although it is technically possible for any set of ideas or any person living at any time in history to be a utopian socialist, the term is most often applied to those socialists who lived in the first quarter of the 19th century who were ascribed the label "utopian" by later socialists as a negative term in order to imply naivete and dismiss their ideas as fanciful or unrealistic.

Religious sects whose members live communally such as the Hutterites are not usually called "utopian socialists", although their way of living is a prime example. They have been categorised as religious socialists by some. Similarly, modern intentional communities based on socialist ideas could also be categorised as "utopian socialist".

For Marxists, the development of capitalism in Western Europe provided a material basis for the possibility of bringing about socialism because according to "The Communist Manifesto" "[w]hat the bourgeoisie produces above all is its own grave diggers", namely the working class, which must become conscious of the historical objectives set it by society.

Revolutionary socialists believe that a social revolution is necessary to effect structural changes to the socioeconomic structure of society. Among revolutionary socialists there are differences in strategy, theory and the definition of "revolution". Orthodox Marxists and left communists take an impossibilist stance, believing that revolution should be spontaneous as a result of contradictions in society due to technological changes in the productive forces. Lenin theorised that under capitalism the workers cannot achieve class consciousness beyond organising into trade unions and making demands of the capitalists. Therefore, Leninists advocate that it is historically necessary for a vanguard of class conscious revolutionaries to take a central role in coordinating the social revolution to overthrow the capitalist state and eventually the institution of the state altogether. "Revolution" is not necessarily defined by revolutionary socialists as violent insurrection, but as a complete dismantling and rapid transformation of all areas of class society led by the majority of the masses: the working class.

Reformism is generally associated with social democracy and gradualist democratic socialism. Reformism is the belief that socialists should stand in parliamentary elections within capitalist society and if elected use the machinery of government to pass political and social reforms for the purposes of ameliorating the instabilities and inequities of capitalism. Within socialism, "reformism" is used in two different ways. One has no intention of bringing about socialism or fundamental economic change to society and is used to oppose such structural changes. The other is based on the assumption that while reforms are not socialist in themselves, they can help rally supporters to the cause of revolution by popularizing the cause of socialism to the working class.

The debate on the ability for social democratic reformism to lead to a socialist transformation of society is over a century old. Reformism is criticized for being paradoxical as it seeks to overcome the existing economic system of capitalism while trying to improve the conditions of capitalism, thereby making it appear more tolerable to society. According to Rosa Luxemburg, capitalism is not overthrown, "but is on the contrary strengthened by the development of social reforms". In a similar vein, Stan Parker of the Socialist Party of Great Britain argues that reforms are a diversion of energy for socialists and are limited because they must adhere to the logic of capitalism. French social theorist Andre Gorz criticized reformism by advocating a third alternative to reformism and social revolution that he called "non-reformist reforms", specifically focused on structural changes to capitalism as opposed to reforms to improve living conditions within capitalism or to prop it up through economic interventions.

Socialist economics starts from the premise that "individuals do not live or work in isolation but live in cooperation with one another. Furthermore, everything that people produce is in some sense a social product, and everyone who contributes to the production of a good is entitled to a share in it. Society as whole, therefore, should own or at least control property for the benefit of all its members".

The original conception of socialism was an economic system whereby production was organised in a way to directly produce goods and services for their utility (or use-value in classical and Marxian economics), with the direct allocation of resources in terms of physical units as opposed to financial calculation and the economic laws of capitalism (see law of value), often entailing the end of capitalistic economic categories such as rent, interest, profit and money. In a fully developed socialist economy, production and balancing factor inputs with outputs becomes a technical process to be undertaken by engineers.

Market socialism refers to an array of different economic theories and systems that use the market mechanism to organise production and to allocate factor inputs among socially owned enterprises, with the economic surplus (profits) accruing to society in a social dividend as opposed to private capital owners. Variations of market socialism include libertarian proposals such as mutualism, based on classical economics, and neoclassical economic models such as the Lange Model. However, some economists such as Joseph Stiglitz, Mancur Olson and others not specifically advancing anti-socialists positions have shown that prevailing economic models upon which such democratic or market socialism models might be based have logical flaws or unworkable presuppositions.

The ownership of the means of production can be based on direct ownership by the users of the productive property through worker cooperative; or commonly owned by all of society with management and control delegated to those who operate/use the means of production; or public ownership by a state apparatus. Public ownership may refer to the creation of state-owned enterprises, nationalisation, municipalisation or autonomous collective institutions. Some socialists feel that in a socialist economy, at least the "" of the economy must be publicly owned. However, economic liberals and right libertarians view private ownership of the means of production and the market exchange as natural entities or moral rights which are central to their conceptions of freedom and liberty and view the economic dynamics of capitalism as immutable and absolute, therefore they perceive public ownership of the means of production, cooperatives and economic planning as infringements upon liberty.

Management and control over the activities of enterprises are based on self-management and self-governance, with equal power-relations in the workplace to maximise occupational autonomy. A socialist form of organisation would eliminate controlling hierarchies so that only a hierarchy based on technical knowledge in the workplace remains. Every member would have decision-making power in the firm and would be able to participate in establishing its overall policy objectives. The policies/goals would be carried out by the technical specialists that form the coordinating hierarchy of the firm, who would establish plans or directives for the work community to accomplish these goals.

The role and use of money in a hypothetical socialist economy is a contested issue. According to the Austrian school economist Ludwig von Mises, an economic system that does not use money, financial calculation and market pricing would be unable to effectively value capital goods and coordinate production and therefore these types of socialism are impossible because they lack the necessary information to perform economic calculation in the first place. Socialists including Karl Marx, Robert Owen, Pierre-Joseph Proudhon and John Stuart Mill advocated various forms of labour vouchers or labour credits, which like money would be used to acquire articles of consumption, but unlike money they are unable to become capital and would not be used to allocate resources within the production process. Bolshevik revolutionary Leon Trotsky argued that money could not be arbitrarily abolished following a socialist revolution. Money had to exhaust its "historic mission", meaning it would have to be used until its function became redundant, eventually being transformed into bookkeeping receipts for statisticians and only in the more distant future would money not be required for even that role.

A planned economy is a type of economy consisting of a mixture of public ownership of the means of production and the coordination of production and distribution through economic planning. A planned economy can be either decentralised or centralised. Enrico Barone provided a comprehensive theoretical framework for a planned socialist economy. In his model, assuming perfect computation techniques, simultaneous equations relating inputs and outputs to ratios of equivalence would provide appropriate valuations in order to balance supply and demand.

The most prominent example of a planned economy was the economic system of the Soviet Union and as such the centralised-planned economic model is usually associated with the communist states of the 20th century, where it was combined with a single-party political system. In a centrally planned economy, decisions regarding the quantity of goods and services to be produced are planned in advance by a planning agency (see also the analysis of Soviet-type economic planning). The economic systems of the Soviet Union and the Eastern Bloc are further classified as "command economies", which are defined as systems where economic coordination is undertaken by commands, directives and production targets. Studies by economists of various political persuasions on the actual functioning of the Soviet economy indicate that it was not actually a planned economy. Instead of conscious planning, the Soviet economy was based on a process whereby the plan was modified by localised agents and the original plans went largely unfulfilled. Planning agencies, ministries and enterprises all adapted and bargained with each other during the formulation of the plan as opposed to following a plan passed down from a higher authority, leading some economists to suggest that planning did not actually take place within the Soviet economy and that a better description would be an "administered" or "managed" economy.

Although central planning was largely supported by Marxist–Leninists, some factions within the Soviet Union before the rise of Stalinism held positions contrary to central planning. Leon Trotsky rejected central planning in favour of decentralised planning. He argued that central planners, regardless of their intellectual capacity, would be unable to coordinate effectively all economic activity within an economy because they operated without the input and tacit knowledge embodied by the participation of the millions of people in the economy. As a result, central planners would be unable to respond to local economic conditions. State socialism is unfeasible in this view because information cannot be aggregated by a central body and effectively used to formulate a plan for an entire economy, because doing so would result in distorted or absent price signals.

A self-managed, decentralised economy is based on autonomous self-regulating economic units and a decentralised mechanism of resource allocation and decision-making. This model has found support in notable classical and neoclassical economists including Alfred Marshall, John Stuart Mill and Jaroslav Vanek. There are numerous variations of self-management, including labour-managed firms and worker-managed firms. The goals of self-management are to eliminate exploitation and reduce alienation. Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public". It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. It was strongly associated with G. D. H. Cole and influenced by the ideas of William Morris.

One such system is the cooperative economy, a largely free market economy in which workers manage the firms and democratically determine remuneration levels and labour divisions. Productive resources would be legally owned by the cooperative and rented to the workers, who would enjoy usufruct rights. Another form of decentralised planning is the use of cybernetics, or the use of computers to manage the allocation of economic inputs. The socialist-run government of Salvador Allende in Chile experimented with Project Cybersyn, a real-time information bridge between the government, state enterprises and consumers. Another, more recent variant is participatory economics, wherein the economy is planned by decentralised councils of workers and consumers. Workers would be remunerated solely according to effort and sacrifice, so that those engaged in dangerous, uncomfortable and strenuous work would receive the highest incomes and could thereby work less. A contemporary model for a self-managed, non-market socialism is Pat Devine's model of negotiated coordination. Negotiated coordination is based upon social ownership by those affected by the use of the assets involved, with decisions made by those at the most localised level of production.

Michel Bauwens identifies the emergence of the open software movement and peer-to-peer production as a new alternative mode of production to the capitalist economy and centrally planned economy that is based on collaborative self-management, common ownership of resources and the production of use-values through the free cooperation of producers who have access to distributed capital.

Anarcho-communism is a theory of anarchism which advocates the abolition of the state, private property and capitalism in favour of common ownership of the means of production. Anarcho-syndicalism was practised in Catalonia and other places in the Spanish Revolution during the Spanish Civil War. Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution.

The economy of the former Socialist Federal Republic of Yugoslavia established a system based on market-based allocation, social ownership of the means of production and self-management within firms. This system substituted Yugoslavia's Soviet-type central planning with a decentralised, self-managed system after reforms in 1953.

The Marxian economist Richard D. Wolff argues that "re-organising production so that workers become collectively self-directed at their work-sites" not only moves society beyond both capitalism and state socialism of the last century, but would also mark another milestone in human history, similar to earlier transitions out of slavery and feudalism. As an example, Wolff claims that Mondragon is "a stunningly successful alternative to the capitalist organisation of production".

State socialism can be used to classify any variety of socialist philosophies that advocates the ownership of the means of production by the state apparatus, either as a transitional stage between capitalism and socialism, or as an end-goal in itself. Typically, it refers to a form of technocratic management, whereby technical specialists administer or manage economic enterprises on behalf of society and the public interest instead of workers' councils or workplace democracy.

A state-directed economy may refer to a type of mixed economy consisting of public ownership over large industries, as promoted by various Social democratic political parties during the 20th century. This ideology influenced the policies of the British Labour Party during Clement Attlee's administration. In the biography of the 1945 United Kingdom Labour Party Prime Minister Clement Attlee, Francis Beckett states: "[T]he government [...] wanted what would become known as a mixed economy".

Nationalisation in the United Kingdom was achieved through compulsory purchase of the industry (i.e. with compensation). British Aerospace was a combination of major aircraft companies British Aircraft Corporation, Hawker Siddeley and others. British Shipbuilders was a combination of the major shipbuilding companies including Cammell Laird, Govan Shipbuilders, Swan Hunter and Yarrow Shipbuilders, whereas the nationalisation of the coal mines in 1947 created a coal board charged with running the coal industry commercially so as to be able to meet the interest payable on the bonds which the former mine owners' shares had been converted into.

Market socialism consists of publicly owned or cooperatively owned enterprises operating in a market economy. It is a system that uses the market and monetary prices for the allocation and accounting of the means of production, thereby retaining the process of capital accumulation. The profit generated would be used to directly remunerate employees, collectively sustain the enterprise or finance public institutions. In state-oriented forms of market socialism, in which state enterprises attempt to maximise profit, the profits can be used to fund government programs and services through a social dividend, eliminating or greatly diminishing the need for various forms of taxation that exist in capitalist systems. Neoclassical economist Léon Walras believed that a socialist economy based on state ownership of land and natural resources would provide a means of public finance to make income taxes unnecessary. Yugoslavia implemented a market socialist economy based on cooperatives and worker self-management.
Mutualism is an economic theory and anarchist school of thought that advocates a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labour in the free market. Integral to the scheme was the establishment of a mutual-credit bank that would lend to producers at a minimal interest rate, just high enough to cover administration. Mutualism is based on a labour theory of value that holds that when labour or its product is sold, in exchange it ought to receive goods or services embodying "the amount of labour necessary to produce an article of exactly similar and equal utility".

The current economic system in China is formally referred to as a socialist market economy with Chinese characteristics. It combines a large state sector that comprises the commanding heights of the economy, which are guaranteed their public ownership status by law, with a private sector mainly engaged in commodity production and light industry responsible from anywhere between 33% to over 70% of GDP generated in 2005. Although there has been a rapid expansion of private-sector activity since the 1980s, privatisation of state assets was virtually halted and were partially reversed in 2005. The current Chinese economy consists of 150 corporatised state-owned enterprises that report directly to China's central government. By 2008, these state-owned corporations had become increasingly dynamic and generated large increases in revenue for the state, resulting in a state-sector led recovery during the 2009 financial crises while accounting for most of China's economic growth. However, the Chinese economic model is widely cited as a contemporary form of state capitalism, the major difference between Western capitalism and the Chinese model being the degree of state-ownership of shares in publicly listed corporations.

The Socialist Republic of Vietnam has adopted a similar model after the Doi Moi economic renovation, but slightly differs from the Chinese model in that the Vietnamese government retains firm control over the state sector and strategic industries, but allows for private-sector activity in commodity production.

The major socialist political movements are described below. Independent socialist theorists, utopian socialist authors and academic supporters of socialism may not be represented in these movements. Some political groups have called themselves socialist while holding views that some consider antithetical to socialism. The term "socialist" has also been used by some politicians on the political right as an epithet against certain individuals who do not consider themselves to be socialists and against policies that are not considered socialist by their proponents. There are many variations of socialism and as such there is no single definition encapsulating all of socialism. However, there have been common elements identified by scholars.

In his "Dictionary of Socialism" (1924), Angelo S. Rappoport analysed forty definitions of socialism to conclude that common elements of socialism include general criticism of the social effects of private ownership and control of capital—as being the cause of poverty, low wages, unemployment, economic and social inequality and a lack of economic security; a general view that the solution to these problems is a form of collective control over the means of production, distribution and exchange (the degree and means of control vary amongst socialist movements); an agreement that the outcome of this collective control should be a society based upon social justice, including social equality, economic protection of people and should provide a more satisfying life for most people.

In "The Concepts of Socialism" (1975), Bhikhu Parekh identifies four core principles of socialism and particularly socialist society, namely sociality, social responsibility, cooperation and planning. In his study "Ideologies and Political Theory" (1996), Michael Freeden states that all socialists share five themes: the first is that socialism posits that society is more than a mere collection of individuals; second, that it considers human welfare a desirable objective; third, that it considers humans by nature to be active and productive; fourth, it holds the belief of human equality; and fifth, that history is progressive and will create positive change on the condition that humans work to achieve such change.

Anarchism advocates stateless societies often defined as self-governed voluntary institutions, but that several authors have defined as more specific institutions based on non-hierarchical free associations. While anarchism holds the state to be undesirable, unnecessary or harmful, it is not the central aspect. Anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including the state system. Mutualists support market socialism, collectivist anarchists favour workers cooperatives and salaries based on the amount of time contributed to production, anarcho-communists advocate a direct transition from capitalism to libertarian communism and a gift economy and anarcho-syndicalists prefer workers' direct action and the general strike.

The authoritarian–libertarian struggles and disputes within the socialist movement go back to the First International and the expulsion in 1872 of the anarchists, who went on to lead the Anti-authoritarian International and then founded their own libertarian international, the Anarchist St. Imier International. In 1888, the individualist anarchist Benjamin Tucker, who proclaimed himself to be an anarchistic socialist and libertarian socialist in opposition to the authoritarian state socialism and the compulsory communism, included the full text of a "Socialistic Letter" by Ernest Lesigne in his essay on "State Socialism and Anarchism". According to Lesigne, there are two types of socialism: "One is dictatorial, the other libertarian". Tucker's two socialisms were the authoritarian state socialism which he associated to the Marxist school and the libertarian anarchist socialism, or simply anarchism, that he advocated. Tucker noted that the fact that the authoritarian "State Socialism has overshadowed other forms of Socialism gives it no right to a monopoly of the Socialistic idea". According to Tucker, what those two schools of socialism had in common was the labor theory of value and the ends, by which anarchism pursued different means.

According to anarchists such as the authors of "An Anarchist FAQ", anarchism is one of the many traditions of socialism. For anarchists and other anti-authoritarian socialists, socialism "can only mean a classless and anti-authoritarian (i.e. libertarian) society in which people manage their own affairs, either as individuals or as part of a group (depending on the situation). In other words, it implies self-management in all aspects of life", including at the workplace. Michael Newman includes anarchism as one of many socialist traditions. Peter Marshall argues that "[i]n general anarchism is closer to socialism than liberalism. [...] Anarchism finds itself largely in the socialist camp, but it also has outriders in liberalism. It cannot be reduced to socialism, and is best seen as a separate and distinctive doctrine".

Democratic socialism represents any socialist movement that seeks to establish an economy based on economic democracy by and for the working class. Democratic socialism is difficult to define and groups of scholars have radically different definitions for the term. Some definitions simply refer to all forms of socialism that follow an electoral, reformist or evolutionary path to socialism rather than a revolutionary one. According to Christopher Pierson, "[i]f the contrast which 1989 highlights is not that between socialism in the East and liberal democracy in the West, the latter must be recognized to have been shaped, reformed and compromised by a century of social democratic pressure". Pierson further claims that "social democratic and socialist parties within the constitutional arena in the West have almost always been involved in a politics of compromise with existing capitalist institutions (to whatever far distant prize its eyes might from time to time have been lifted)". For Pierson, "if advocates of the death of socialism accept that social democrats belong within the socialist camp, as I think they must, then the contrast between socialism (in all its variants) and liberal democracy must collapse. For "actually existing" liberal democracy is, in substantial part, a product of socialist (social democratic) forces".

Social democracy is a socialist tradition of political thought. Many social democrats refer to themselves as socialists or democratic socialists and some such as Tony Blair employ these terms interchangeably. Others found "clear differences" between the three terms and prefer to describe their own political beliefs by using the term "social democracy". The two main directions were to establish democratic socialism or to build first a welfare state within the capitalist system. The first variant advances democratic socialism through reformist and gradualist methods. In the second variant, social democracy is a policy regime involving a welfare state, collective bargaining schemes, support for publicly financed public services and a mixed economy. It is often used in this manner to refer to Western and Northern Europe during the later half of the 20th century. It was described by Jerry Mander as "hybrid economics", an active collaboration of capitalist and socialist visions. Numerous studies and surveys indicate that people tend to live happier lives in social democratic societies rather than neoliberal ones.
Social democrats advocate for a peaceful, evolutionary transition of the economy to socialism through progressive social reform. It asserts that the only acceptable constitutional form of government is representative democracy under the rule of law. It promotes extending democratic decision-making beyond political democracy to include economic democracy to guarantee employees and other economic stakeholders sufficient rights of co-determination. It supports a mixed economy that opposes inequality, poverty and oppression while rejecting both a totally unregulated market economy or a fully planned economy. Common social democratic policies include universal social rights and universally accessible public services such as education, health care, workers' compensation and other services, including child care and elder care. Social democracy supports the trade union labour movement and supports collective bargaining rights for workers. Most social democratic parties are affiliated with the Socialist International.

Modern democratic socialism is a broad political movement that seeks to promote the ideals of socialism within the context of a democratic system. Some democratic socialists support social democracy as a temporary measure to reform the current system while others reject reformism in favour of more revolutionary methods. Modern social democracy emphasises a program of gradual legislative modification of capitalism in order to make it more equitable and humane while the theoretical end goal of building a socialist society is relegated to the indefinite future. According to Sheri Berman, Marxism is loosely held to be valuable for its emphasis on changing the world for a more just, better future.

The two movements are widely similar both in terminology and in ideology, although there are a few key differences. The major difference between social democracy and democratic socialism is the object of their politics in that contemporary social democrats support a welfare state and unemployment insurance as well as other practical, progressive reforms of capitalism and are more concerned to administrate and humanise it. On the other hand, democratic socialists seek to replace capitalism with a socialist economic system, arguing that any attempt to humanise capitalism through regulations and welfare policies would distort the market and create economic contradictions.

Ethical socialism appeals to socialism on ethical and moral grounds as opposed to economic, egoistic and consumeristic grounds. It emphasizes the need for a morally conscious economy based upon the principles of altruism, cooperation and social justice while opposing possessive individualism. Ethical socialism has been the official philosophy of mainstream socialist parties.

Liberal socialism incorporates liberal principles to socialism. It has been compared to post-war social democracy for its support of a mixed economy that includes both public and private capital goods. While democratic socialism and social democracy are anti-capitalist positions insofar as criticism of capitalism is linked to the private ownership of the means of production, liberal socialism identifies artificial and legalistic monopolies to be the fault of capitalism and opposes an entirely unregulated market economy. It considers both liberty and social equality to be compatible and mutually dependent.

Principles that can be described as ethical or liberal socialist have been based upon or developed by philosophers such as John Stuart Mill, Eduard Bernstein, John Dewey, Carlo Rosselli, Norberto Bobbio and Chantal Mouffe. Other important liberal socialist figures include Guido Calogero, Piero Gobetti, Leonard Trelawny Hobhouse, John Maynard Keynes and R. H. Tawney. Liberal socialism has been particularly prominent in British and Italian politics.

Blanquism is a conception of revolution named for Louis Auguste Blanqui. It holds that socialist revolution should be carried out by a relatively small group of highly organised and secretive conspirators. Upon seizing power, the revolutionaries introduce socialism. Rosa Luxemburg and Eduard Bernstein criticised Lenin, stating that his conception of revolution was elitist and Blanquist. Marxism–Leninism combines Marx's scientific socialist concepts and Lenin's anti-imperialism, democratic centralism and party-building principles.

Hal Draper defined socialism from above as the philosophy which employs an elite administration to run the socialist state. The other side of socialism is a more democratic socialism from below. The idea of socialism from above is much more frequently discussed in elite circles than socialism from below—even if that is the Marxist ideal—because it is more practical. Draper viewed socialism from below as being the purer, more Marxist version of socialism. According to Draper, Karl Marx and Friedrich Engels were devoutly opposed to any socialist institution that was "conducive to superstitious authoritarianism". Draper makes the argument that this division echoes the division between "reformist or revolutionary, peaceful or violent, democratic or authoritarian, etc." and further identifies six major varieties of socialism from above, among them "Philanthropism", "Elitism", "Pannism", "Communism", "Permeationism" and "Socialism-from-Outside".

According to Arthur Lipow, Marx and Engels were "the founders of modern revolutionary democratic socialism", described as a form of "socialism from below" that is "based on a mass working-class movement, fighting from below for the extension of democracy and human freedom". This type of socialism is contrasted to that of the "authoritarian, antidemocratic creed" and "the various totalitarian collectivist ideologies which claim the title of socialism" as well as "the many varieties of 'socialism from above' which have led in the twentieth century to movements and state forms in which a despotic 'new class' rules over a statified economy in the name of socialism", a division that "runs through the history of the socialist movement". Lipow identifies Bellamyism and Stalinism as two prominent authoritarian socialist currents within the history of the socialist movement.

Libertarian socialism, sometimes called left-libertarianism, social anarchism and socialist libertarianism, is an anti-authoritarian, anti-statist and libertarian tradition within socialism that rejects centralised state ownership and control including criticism of wage labour relationships (wage slavery) as well as the state itself. It emphasises workers' self-management and decentralised structures of political organisation. Libertarian socialism asserts that a society based on freedom and equality can be achieved through abolishing authoritarian institutions that control production. Libertarian socialists generally prefer direct democracy and federal or confederal associations such as libertarian municipalism, citizens' assemblies, trade unions and workers' councils.

Anarcho-syndicalist Gaston Leval explained: "We therefore foresee a Society in which all activities will be coordinated, a structure that has, at the same time, sufficient flexibility to permit the greatest possible autonomy for social life, or for the life of each enterprise, and enough cohesiveness to prevent all disorder. [...] In a well-organised society, all of these things must be systematically accomplished by means of parallel federations, vertically united at the highest levels, constituting one vast organism in which all economic functions will be performed in solidarity with all others and that will permanently preserve the necessary cohesion". All of this is generally done within a general call for libertarian and voluntary free associations through the identification, criticism and practical dismantling of illegitimate authority in all aspects of human life.

As part of the larger socialist movement, it seeks to distinguish itself from Bolshevism, Leninism and Marxism–Leninism as well as social democracy. Past and present political philosophies and movements commonly described as libertarian socialist include anarchism (anarcho-communism, anarcho-syndicalism collectivist anarchism, individualist anarchism and mutualism), autonomism, Communalism, participism, libertarian Marxism (council communism and Luxemburgism), revolutionary syndicalism and utopian socialism (Fourierism).

Christian socialism is a broad concept involving an intertwining of Christian religion with socialism.

Islamic socialism is a more spiritual form of socialism. Muslim socialists believe that the teachings of the Qur'an and Muhammad are compatible with principles of equality and public ownership, drawing inspiration from the early Medina welfare state he established. Muslim socialists are more conservative than their Western contemporaries and find their roots in anti-imperialism, anti-colonialism and Arab nationalism. Islamic socialists believe in deriving legitimacy from political mandate as opposed to religious texts.

Socialist feminism is a branch of feminism that argues that liberation can only be achieved by working to end both economic and cultural sources of women's oppression. Marxist feminism's foundation was laid by Engels in "The Origin of the Family, Private Property, and the State" (1884). August Bebel's "Woman under Socialism" (1879), is the "single work dealing with sexuality most widely read by rank-and-file members of the Social Democratic Party of Germany (SPD)". In the late 19th and early 20th centuries, both Clara Zetkin and Eleanor Marx were against the demonisation of men and supported a proletariat revolution that would overcome as many male-female inequalities as possible. As their movement already had the most radical demands in women's equality, most Marxist leaders, including Clara Zetkin and Alexandra Kollontai, counterposed Marxism against liberal feminism rather than trying to combine them. Anarcha-feminism began with late 19th and early 20th century authors and theorists such as anarchist feminists Goldman and Voltairine de Cleyre In the Spanish Civil War, an anarcha-feminist group, ("Free Women") linked to the , organised to defend both anarchist and feminist ideas. In 1972, the Chicago Women's Liberation Union published "Socialist Feminism: A Strategy for the Women's Movement", which is believed to be the first published use of the term "socialist feminism".
Many socialists were early advocates for LGBT rights. For early socialist Charles Fourier, true freedom could only occur without suppressing passions, as the suppression of passions is not only destructive to the individual, but to society as a whole. Writing before the advent of the term "homosexuality", Fourier recognised that both men and women have a wide range of sexual needs and preferences which may change throughout their lives, including same-sex sexuality and "androgénité". He argued that all sexual expressions should be enjoyed as long as people are not abused and that "affirming one's difference" can actually enhance social integration. In Oscar Wilde's "The Soul of Man Under Socialism", he advocates for an egalitarian society where wealth is shared by all, while warning of the dangers of social systems that crush individuality. Edward Carpenter actively campaigned for homosexual rights. His work "The Intermediate Sex: A Study of Some Transitional Types of Men and Women" was a 1908 book arguing for gay liberation. who was an influential personality in the foundation of the Fabian Society and the Labour Party. After the Russian Revolution under the leadership of Lenin and Trotsky, the Soviet Union abolished previous laws against homosexuality. Harry Hay was an early leader in the American LGBT rights movement as well as a member of the Communist Party USA. He is known for his roles in helping to found gay organisations, including the Mattachine Society, the first sustained gay rights group in the United States which in its early days reflected a strong Marxist influence. The "Encyclopedia of Homosexuality" reports that "[a]s Marxists the founders of the group believed that the injustice and oppression which they suffered stemmed from relationships deeply embedded in the structure of American society". Emerging from events such as the May 1968 insurrection in France, the anti-Vietnam war movement in the US and the Stonewall riots of 1969, militant gay liberation organisations began to spring up around the world. Many sprang from left radicalism more than established homophile groups, although the Gay Liberation Front took an anti-capitalist stance and attacked the nuclear family and traditional gender roles.

Eco-socialism is a political strain merging aspects of socialism, Marxism or libertarian socialism with green politics, ecology and alter-globalisation. Eco-socialists generally claim that the expansion of the capitalist system is the cause of social exclusion, poverty, war and environmental degradation through globalisation and imperialism under the supervision of repressive states and transnational structures. Contrary to the depiction of Karl Marx by some environmentalists, social ecologists and fellow socialists as a productivist who favoured the domination of nature, eco-socialists revisited Marx's writings and believe that he "was a main originator of the ecological world-view". Marx discussed a "metabolic rift" between man and nature, stating that "private ownership of the globe by single individuals will appear quite absurd as private ownership of one man by another" and his observation that a society must "hand it [the planet] down to succeeding generations in an improved condition". English socialist William Morris is credited with developing principles of what was later called eco-socialism. During the 1880s and 1890s, Morris promoted his ideas within the Social Democratic Federation and Socialist League. Green anarchism blends anarchism with environmental issues. An important early influence was Henry David Thoreau and his book "Walden" as well as Élisée Reclus.

In the late 19th century, anarcho-naturism fused anarchism and naturist philosophies within individualist anarchist circles in France, Spain, Cuba and Portugal. Murray Bookchin's first book "Our Synthetic Environment" was followed by his essay "Ecology and Revolutionary Thought" which introduced ecology as a concept in radical politics. In the 1970s, Barry Commoner, claimed that capitalist technologies were chiefly responsible for environmental degradation as opposed to population pressures. In the 1990s socialist/feminists Mary Mellor and Ariel Salleh adopt an eco-socialist paradigm. An "environmentalism of the poor" combining ecological awareness and social justice has also become prominent. Pepper critiqued the current approach of many within green politics, particularly deep ecologists.

Many green parties around the world, such as the Dutch Green Left Party (GroenLinks), employ eco-socialist elements. Radical red-green alliances have been formed in many countries by eco-socialists, radical greens and other radical left groups. In Denmark, the Red-Green Alliance was formed as a coalition of numerous radical parties. Within the European Parliament, a number of leftist parties from Northern Europe have organised themselves into the Nordic Green Left Alliance.

Syndicalism operates through industrial trade unions. It rejects state socialism and the use of establishment politics. Syndicalists reject state power in favour of strategies such as the general strike. Syndicalists advocate a socialist economy based on federated unions or syndicates of workers who own and manage the means of production. Some Marxist currents advocate syndicalism, such as De Leonism. Anarcho-syndicalism views syndicalism as a method for workers in capitalist society to gain control of an economy. The Spanish Revolution was largely orchestrated by the anarcho-syndicalist trade union CNT. The International Workers' Association is an international federation of anarcho-syndicalist labour unions and initiatives.

Socialism is criticized in terms of its models of economic organization as well as its political and social implications. Other critiques are directed at the socialist movement, parties or existing states. Some criticisms occupy theoretical grounds (such as in the economic calculation problem and the socialist calculation debate) while others support their criticism by examining historical attempts to establish socialist societies. Because of socialism's many varieties, most critiques focused on a specific approach. Proponents of one approach typically criticize others.





</doc>
<doc id="26849" url="https://en.wikipedia.org/wiki?curid=26849" title="Sabine River (Texas–Louisiana)">
Sabine River (Texas–Louisiana)

The Sabine River () is a river, long, in the Southern U.S. states of Texas and Louisiana. From the 32nd parallel north and downstream, it serves as part of the boundary between the two states and empties into Sabine Lake, an estuary of the Gulf of Mexico. Over the first half of the 19th century, the river formed part of the Spanish–American, Mexican–American, and Texan–American international boundaries. The upper reaches of the river flow through the prairie country of northeast Texas. Along much of its lower reaches, it flows through the pine forests along the Texas–Louisiana border, and the bayou country near the Gulf Coast.

The river drains an area of , of which are in Texas and in Louisiana. It flows through an area of abundant rainfall and discharges the largest volume of any river in Texas. The name Sabine (Sp: "Río de Sabinas") comes from the Spanish word for cypress, in reference to the extensive growth of bald cypresses along the lower river. The river flows through an important petroleum-producing region, and the lower river near the Gulf is among the most industrialized areas of the southeastern United States. The river was often described as the dividing line between the Old South and the New Southwest.

The Sabine rises in northeast Texas by the union of three branches: the Cowleech Fork, Caddo Fork, and South Fork. The Cowleech Fork rises in northwestern Hunt County and flows southeast for . The Caddo Fork, shown as "Caddo Creek" on federal maps, rises in two tributary forks, the East Caddo Fork and the West Caddo Fork, in northwestern Hunt County. The South Fork rises in the southwestern corner of Hunt County and flows east for , joining the Caddo Fork and Cowleech Fork in southeastern Hunt County. The confluence of the forks is now submerged in the Lake Tawakoni reservoir. The combined river flows southeast across northeast Texas and is joined by a fourth branch, Lake Fork Creek, downstream from the reservoir.

In northeast Texas, the river flows past Mineola, Gladewater, Big Sandy, and Longview, the largest city on the river, to southwest of Shreveport at the 32nd parallel north, where it establishes the Texas-Louisiana boundary. It flows south, forming the state line for the remainder of its course. It is impounded west of Leesville, Louisiana, to form the Toledo Bend Reservoir, with the Sabine National Forest along its western bank. South of the reservoir, it passes through the bayou country, surrounded by wetlands, as well as widespread industrial areas near the Gulf Coast. Approximately south of Orange, it meets the Neches River from the west to form the and Sabine Lake, which drains through Sabine Pass to the Gulf of Mexico. The city of Port Arthur, Texas, sits along the western shore of Sabine Lake

Archeological evidence indicates the valley of the river has been inhabited for as long as 12,000 years by indigenous peoples. Starting in the eighth century, the Caddo inhabited the area, building extensive earthwork mounds in complexes expressing their cosmology. The Caddo culture flourished until the late 13th century. Descendants of the Caddo were living along the river when the first European explorers arrived in the 16th century.

The river was named in 1716 by Spanish explorer Domingo Ramón, and appeared as "Río de Sabinas" on a 1721 map. The river was used by French traders, and at various times, the river was claimed by both Spain and France. After the acquisition by Spain of the French territory of Louisiana in 1763, following France's defeat by Great Britain in the Seven Years' War, the capital of the Spanish province of Texas was established on the east side of the river, near present-day Robeline, Louisiana.

After acquiring the French territory west of the Mississippi River in the 1803 Louisiana Purchase, the United States started to exert control in this area. It was at war with Native Americans in Louisiana along the Sabine River from 1836 to 1837, in the period when it was trying to remove the Indians to Indian Territory from the Southeast.

The Sabine River was too deep to ford, and proved to be navigable. Early travelers and settlers would have to swim the river on horseback and cattle would have to be driven into the river to swim across. Ferries were later put into service. By the 1840s, steamboats were travelling from Logansport to Sabine Lake.

Recorded ferry use began 1794, when Louis Chabinan (Sharben), his wife Margarite LaFleur, and their four children settled on the east bank of the Sabine River on land purchased from Vicinte Michele. Chabinan built a ferry landing on the river called "Paso del Chaland." Louisiana State Highway 6 (La 6) and Texas State Highway 21 now meet near here, at the site of the present-day Pendleton Bridge. In 1796, Chabinan was drowned after being kicked by a horse and falling into the Sabine.

Michel Crow married his widow and ran the ferry, until he sold it to James Gaines "circa" 1819; it was renamed Gaines Ferry. This ferry was in service until 1937, when it was replaced by the Pendleton Bridge, built during the Great Depression. Crow also operated a ferry he had started upriver, a 120-foot crossing started in 1796. It linked what became known as Carter's Ferry Road, now Texas FM 276. Carter's ferry was 25 miles from San Augustine and 15 miles from Many, Louisiana. Crow sold the ferry to Carter, who became the namesake. Farther north, and just above Bayou Lanan, was Williamson Ferry. 
Other ferries on the Sabine River:
The main Sabine River crossings were the El Camino Real (King's Highway) from Natchitoches, or "Upper Route" from Shreveport; and the "Lower" Route, from Opelousas called "The Old Beef Trail". It was used to drive thousands of cattle from Texas to Alexandria, Louisiana, for shipment to cities such as New Orleans. Hickman Ferry was a shipping point for areas as far west as Burkeville. Sabine River ports from Sabine Pass in river mileage were "Belgrade", 171 miles; "Stark's Landing" 191 miles; "Loftin Ferry", and "Bayou Lanacoco" 220 miles; "Hickman's Ferry" 252 miles; "Burnham's Landing" 261 miles; and "Burr's Ferry" 281 miles.

The area's geography remained one of the least understood in the region. Various Spanish maps had errors in the naming of the Sabine and Neches, and sometimes showed them flowing independently into the Gulf of Mexico. After the Louisiana Purchase by the United States in 1803, a dispute over the boundary between the U.S. and Spain led to an agreement on November 6, 1806, negotiated by Gen.James Wilkinson and Lt. Col. Simón de Herrera, to establish a neutral territory on both sides of the river. Neither country would put military troops or civil police there.

The indefinite boundary was resolved by the Adams-Onis Treaty of 1819, which established the Sabine River as the boundary from the Gulf to the 32nd parallel. The Spanish delay in the ratification of the treaty, and Mexico gaining independence in 1821, reignited the boundary dispute. The United States, at the insistence of Anthony Butler, claimed for a while that the names of the Sabine and Neches had been reversed, thus they claimed that the treaty established the boundary at the Neches. The first Anglo-American settlers began arriving in the region in the 1820s, soon outnumbering the Mexicans by ten to one. After the independence of the Republic of Texas from Mexico in 1836, the boundary between the U.S. and Texas was firmly established at the Sabine in accordance with the Adams-Onis Treaty. The river served as the western boundary of the United States until it annexed Texas in 1845.

In 1843, Capt. John Clemmons made the first trip up the Sabine in the steamboat "Sabine." Steamboats carried passengers, as well as commodities such as cotton, from as far north as Logansport, Louisiana, down to Sabine Pass.

The pirate Jean Lafitte made many trips up the Sabine and reportedly started the colony of Shacklefoot on the Texas side of the Sabine River, south of Carter's ferry up Bayou Patroon.

During the American Civil War, on September 8, 1863, a small Confederate force thwarted a Union invasion of Texas at the Second Battle of Sabine Pass, fought at the mouth of the river.

In the late 19th and early 20th centuries, the middle course of the river was an area of widespread logging. The discovery of petroleum at nearby Spindletop led to the river basin becoming the scene of widespread oil drilling. The lower river became heavily industrialized, developed with many oil refineries and chemical plants. Such alteration to the wetlands resulted in a degradation of the water quality. Since the late 20th century, there have been federal, state, and local efforts to restore the quality of the river. In addition, draining of wetlands and dredging of bayous has caused decline in the acreage of wetlands, resulting in coastal erosion, and making the area much more vulnerable to hurricane damage.

The lower river, south of Orange to Sabine Lake, forms part of the Intracoastal Waterway, carrying barge traffic and some pleasure boats.

As a young man, Captain Bill McDonald of the Texas Rangers operated a small store at Brown's Bluff (modern-day Elderville) on the Sabine in Gregg County, Texas.

Hadden's Ferry was the site of the ground-breaking ceremony held on October 5, 1961, for the 181,600-acre Toledo Bend Reservoir. Dedicated October 11, 1969, the reservoir is the largest man-made lake in the South. Flooding of lands along the Sabine River behind the dam inundated all the ferry sites within its boundary.

The 1970 Louisiana Legislature passed Acts 90 and 117, creating the Sabine River Diversion Canal, for the purpose of supplying fresh river water to businesses in Lake Charles, Sulphur, Westlake, and what was Mossville (now the Sasol complex), as well as to farmers along the canal, with a total capacity of a day. The canal was completed by the Louisiana Department of Public Works in 1981. The canal is long, with about of underground pipe, and begins on the Old Sabine River north of Niblett's Bluff. Pump station #1 is located 2 miles east of the river. The canal continues running east, piped under roadways such as Louisiana Highway 109 north of Vinton, the Edgerly Big Woods road, and Highway 388, which runs to Dequincy.

Just east of Louisiana Highway 27, the canal forks to the south, running around southern Sulphur. The canal is piped under Louisiana Highway 108, at pumping station #4, providing river water to the business area known as City Service in Westlake, and companies such as Equistar, which has a daily contract for 734,400 gallons a day. Other customers and their gallons of use per day are the city of Westlake (8,640,000 gallons), Air Liquide (129,600), Air Products (1,728,000), CITGO (20,160,000), Phillips 66 (3,600,000), The Axiall subsidiary Eagle US 2 LLC (20,160,000), Entergy (21,600,000), Lake Charles Co-Gen (14,400,000), Louisiana Pigment (3,038,400) that produces Titanium White, another LyondellBasell company (720,000), and Matheson Tri-Gas (175,680).

The main canal continues east, crossing under Highway 27 and joined by the Houston River canal at pumping station #2, continuing to old Mossville. There it tees to the left, providing water to the Krause and Managan canal supplying the Nelson Industrial Steam Company (Nisco), which supplies steam and electricity to area businesses. The right tee of the canal terminates at pumping station #3 on what was 8th street in Mossville, now the Sasol complex, providing 46,080,000 gallons of river water for a total daily contract use of 141,166,000 gallons of river water a day.


Up to 450,000 gallons (about 11,000 bls) of crude oil spilled over the Sabine River when the tanker "Eagle Otome", which was carrying the shipment, struck two chemical-carrying barges due to loss of engine power on January 24, 2010, at 10 am local time.

Severe flooding during the first week of March 2016 was the result of record rainfalls in northern Louisiana and the Sabine River basin, of 18 to more than 24 inches. Toledo Bend Reservoir is considered at "full pool" at 172 ft; before the rains started, it was at 171.5 ft. On March 10, the level reached a record 174.36 ft, and 9 of the 11 gates were opened to 22 ft (two gates were out of commission for repairs). Lake Tawakoni, east of Dallas on the Sabine River, was 2 feet above full pool and Lake Fork Reservoir was 1 1/2 feet above full pool.

When the reservoir level dropped to 173.69 ft, 9 gates were in operation at 20 ft. The previous record level of 173.93 ft was on May 18, 1989. At that time, the spillway gates were opened to 9 ft. The maximum height is 28 ft and with nine 9 gates open, the discharge rate is over 190,000 ft per second, which is equivalent to the flow over Niagara Falls.
The peak water flow from the dam was nearly 208,000 ft per second for 31 hours, equating to 1.5 million gallons per second. Catastrophic flooding was predicted to be from 2 to 5 ft above record floods of 1884 and 1889.

During peak flooding, Deweyville, Texas was surrounded by water, accessible only by air or boat. The flood stage is 24 ft, but reached 33.24 ft on March 10, 2016, which was 9.24 ft above flood stage.

A group of Texas residents who suffered damage in the flooding met March 17, 2016, to discuss a class-action suit against the Sabine River Authority (SRA), based on their belief that it had mismanaged water release. The issue is under review by counsel.

According to local ABC affiliate KBMT-TV, SRA spokesperson Ann Galassi stated that the SRA has guidelines it has to follow and those cannot be altered based on weather forecasts. She said that the guidelines are designed to protect the infrastructure of the dam. After the record flood event, the regulatory commission could possibly review the guidelines, and she said that the SRA would welcome that. The SRA of Texas states, "The Authority was created as a conservation and reclamation district with responsibilities to control, store, preserve, and distribute the waters of the Sabine River and its tributary streams for useful purposes." The site also states, "Toledo Bend Project-since its inception and original development over 50 years ago-has never been a flood-control facility. Rather, the project is regulated, as set forth in the project license, to accommodate a number of public benefits, including water supply, recreation, and hydropower production.".





</doc>
<doc id="26859" url="https://en.wikipedia.org/wiki?curid=26859" title="Synergy">
Synergy

Synergy is the creation of a whole that is greater than the simple sum of its parts. The term "synergy" comes from the Attic Greek word συνεργία ' from ', , meaning "working together".

The words "synergy" and "synergetic" have been used in the field of physiology since at least the middle of the 19th century:

SYN'ERGY, "Synergi'a", "Synenergi'a", (F.) "Synergie"; from "συν", 'with', and "εργον", 'work'. A correlation or concourse of action between different organs in health; and, according to some, in disease.

In 1896, applied the term "synergy" to social psychology by writing "La synergie sociale", in which he argued that Darwinian theory failed to account of "social synergy" or "social love", a collective evolutionary drive. The highest civilizations were the work not only of the elite but of the masses too; those masses must be led, however, because the crowd, a feminine and unconscious force, cannot distinguish between good and evil.

In 1909, Lester Frank Ward defined synergy as the universal constructive principle of nature:

I have characterized the social struggle as centrifugal and social solidarity as centripetal. Either alone is productive of evil consequences. Struggle is essentially destructive of the social order, while communism removes individual initiative. The one leads to disorder, the other to degeneracy. What is not seen—the truth that has no expounders—is that the wholesome, constructive movement consists in the properly ordered combination and interaction of both these principles. This is "social synergy", which is a form of cosmic synergy, the universal constructive principle of nature.
In the natural world, synergistic phenomena are ubiquitous, ranging from physics (for example, the different combinations of quarks that produce protons and neutrons) to chemistry (a popular example is water, a compound of hydrogen and oxygen), to the cooperative interactions among the genes in genomes, the division of labor in bacterial colonies, the synergies of scale in multi-cellular organisms, as well as the many different kinds of synergies produced by socially-organized groups, from honeybee colonies to wolf packs and human societies: compare stigmergy, a mechanism of indirect coordination between agents or actions that results in the self-assembly of complex systems. Even the tools and technologies that are widespread in the natural world represent important sources of synergistic effects. The tools that enabled early hominins to become systematic big-game hunters is a primordial human example.
In the context of organizational behavior, following the view that a cohesive group is more than the sum of its parts, synergy is the ability of a group to outperform even its best individual member. These conclusions are derived from the studies conducted by Jay Hall on a number of laboratory-based group ranking and prediction tasks. He found that effective groups actively looked for the points in which they disagreed and in consequence encouraged conflicts amongst the participants in the early stages of the discussion. In contrast, the ineffective groups felt a need to establish a common view quickly, used simple decision making methods such as averaging, and focused on completing the task rather than on finding solutions they could agree on.
In a technical context, its meaning is a construct or collection of different elements working together to produce results not obtainable by any of the elements alone. The elements, or parts, can include people, hardware, software, facilities, policies, documents: all things required to produce system-level results. The value added by the system as a whole, beyond that contributed independently by the parts, is created primarily by the relationship among the parts, that is, how they are interconnected. In essence, a system constitutes a set of interrelated components working together with a common objective: fulfilling some designated need.

If used in a business application, synergy means that teamwork will produce an overall better result than if each person within the group were working toward the same goal individually. However, the concept of group cohesion needs to be considered. Group cohesion is that property that is inferred from the number and strength of mutual positive attitudes among members of the group. As the group becomes more cohesive, its functioning is affected in a number of ways. First, the interactions and communication between members increase. Common goals, interests and small size all contribute to this. In addition, group member satisfaction increases as the group provides friendship and support against outside threats.

There are negative aspects of group cohesion that have an effect on group decision-making and hence on group effectiveness. There are two issues arising. The risky shift phenomenon is the tendency of a group to make decisions that are riskier than those that the group would have recommended individually. Group Polarisation is when individuals in a group begin by taking a moderate stance on an issue regarding a common value and, after having discussed it, end up taking a more extreme stance.

A second, potential negative consequence of group cohesion is group think. Group think is a mode of thinking that people engage in when they are deeply involved in cohesive group, when the members' striving for unanimity overrides their motivation to appraise realistically the alternative courses of action. Studying the events of several American policy "disasters" such as the failure to anticipate the Japanese attack on Pearl Harbor (1941) and the Bay of Pigs Invasion fiasco (1961), Irving Janis argued that they were due to the cohesive nature of the committees that made the relevant decisions.

That decisions made by committees lead to failure in a simple system is noted by Dr. Chris Elliot. His case study looked at IEEE-488, an international standard set by the leading US standards body; it led to a failure of small automation systems using the IEEE-488 standard (which codified a proprietary communications standard HP-IB). But the external devices used for communication were made by two different companies, and the incompatibility between the external devices led to a financial loss for the company. He argues that systems will be safe only if they are designed, not if they emerge by chance.
The idea of a systemic approach is endorsed by the United Kingdom Health and Safety Executive. The successful performance of the health and safety management depends upon the analyzing the causes of incidents and accidents and learning correct lessons from them. The idea is that all events (not just those causing injuries) represent failures in control, and present an opportunity for learning and improvement. UK Health and Safety Executive, "Successful health and safety management" (1997): this book describes the principles and management practices, which provide the basis of effective health and safety management. It sets out the issues that need to be addressed, and can be used for developing improvement programs, self-audit, or self-assessment. Its message is that organizations must manage health and safety with the same degree of expertise and to the same standards as other core business activities, if they are to effectively control risks and prevent harm to people.

The term synergy was refined by R. Buckminster Fuller, who analyzed some of its implications more fully and coined the term synergetics.


Synergy of various kinds has been advanced by Peter Corning as a causal agency that can explain the progressive evolution of complexity in living systems over the course of time. According to the Synergism Hypothesis, synergistic effects have been the drivers of cooperative relationships of all kinds and at all levels in living systems. The thesis, in a nutshell, is that synergistic effects have often provided functional advantages (economic benefits) in relation to survival and reproduction that have been favored by natural selection. The cooperating parts, elements, or individuals become, in effect, functional “units” of selection in evolutionary change. Similarly, environmental systems may react in a non-linear way to perturbations, such as climate change, so that the outcome may be greater than the sum of the individual component alterations. Synergistic responses are a complicating factor in environmental modeling.

Pest synergy would occur in a biological host organism population, where, for example, the introduction of parasite A may cause 10% fatalities, and parasite B may also cause 10% loss. When both parasites are present, the losses would normally be expected to total less than 20%, yet, in some cases, losses are significantly greater. In such cases, it is said that the parasites in combination have a synergistic effect.

Mechanisms that may be involved in the development of synergistic effects include:

More mechanisms are described in an exhaustive 2009 review.

Toxicological synergy is of concern to the public and regulatory agencies because chemicals individually considered safe might pose unacceptable health or ecological risk in combination. Articles in scientific and lay journals include many definitions of chemical or toxicological synergy, often vague or in conflict with each other. Because toxic interactions are defined relative to the expectation under "no interaction", a determination of synergy (or antagonism) depends on what is meant by "no interaction". The United States Environmental Protection Agency has one of the more detailed and precise definitions of toxic interaction, designed to facilitate risk assessment. In their guidance documents, the no-interaction default assumption is dose addition, so synergy means a mixture response that exceeds that predicted from dose addition. The EPA emphasizes that synergy does not always make a mixture dangerous, nor does antagonism always make the mixture safe; each depends on the predicted risk under dose addition.

For example, a consequence of pesticide use is the risk of health effects. During the registration of pesticides in the United States exhaustive tests are performed to discern health effects on humans at various exposure levels. A regulatory upper limit of presence in foods is then placed on this pesticide. As long as residues in the food stay below this regulatory level, health effects are deemed highly unlikely and the food is considered safe to consume.

However, in normal agricultural practice, it is rare to use only a single pesticide. During the production of a crop, several different materials may be used. Each of them has had determined a regulatory level at which they would be considered individually safe. In many cases, a commercial pesticide is itself a combination of several chemical agents, and thus the safe levels actually represent levels of the mixture. In contrast, a combination created by the end user, such as a farmer, has rarely been tested in that combination. The potential for synergy is then unknown or estimated from data on similar combinations. This lack of information also applies to many of the chemical combinations to which humans are exposed, including residues in food, indoor air contaminants, and occupational exposures to chemicals. Some groups think that the rising rates of cancer, asthma, and other health problems may be caused by these combination exposures; others have alternative explanations. This question will likely be answered only after years of exposure by the population in general and research on chemical toxicity, usually performed on animals. Examples of pesticide synergists include Piperonyl butoxide and MGK 264.

Human synergy relates to human interaction and teamwork. For example, say person A alone is too short to reach an apple on a tree and person B is too short as well. Once person B sits on the shoulders of person A, they are tall enough to reach the apple. In this example, the product of their synergy would be one apple. Another case would be two politicians. If each is able to gather one million votes on their own, but together they were able to appeal to 2.5 million voters, their synergy would have produced 500,000 more votes than had they each worked independently. A song is also a good example of human synergy, taking more than one musical part and putting them together to create a song that has a much more dramatic effect than each of the parts when played individually.

A third form of human synergy is when one person is able to complete two separate tasks by doing one action, for example, if a person were asked by a teacher and his boss at work to write an essay on how he could improve his work. A more visual example of this synergy is a drummer using four separate rhythms to create one drum beat.

Synergy usually arises when two persons with different complementary skills cooperate. In business, cooperation of people with organizational and technical skills happens very often. In general, the most common reason why people cooperate is that it brings a synergy. On the other hand, people tend to specialize just to be able to form groups with high synergy (see also division of labor and teamwork).

Example: Two teams in System Administration working together to combine technical and organizational skills in order to better the client experience, thus creating synergy. Counter-examples can be found in books like The Mythical Man-Month, in which the addition of additional team members is shown to have negative effects on productivity.

Organismic computing is an approach to improving group efficacy by increasing synergy in human groups via technological means.

When synergy occurs in the work place, the individuals involved get to work in a positive and supportive working environment. When individuals get to work in environments such as these, the company reaps the benefits. The authors of "Creating the Best Workplace on Earth" Rob Goffee and Gareth Jones, state that "highly engaged employees are, on average, 50% more likely to exceed expectations that the least-engaged workers. And companies with highly engaged people outperform firms with the most disengaged folks- by 54% in employee retention, by 89% in customer satisfaction, and by fourfold in revenue growth (Goffee & Jones, pg. 100)." Also, those that are able to be open about their views on the company, and have confidence that they will be heard, are likely to be a more organized employee who helps his/ her fellow team members succeed.

Corporate synergy occurs when corporations interact congruently. A corporate synergy refers to a financial benefit that a corporation expects to realize when it merges with or acquires another corporation. This type of synergy is a nearly ubiquitous feature of a corporate acquisition and is a negotiating point between the buyer and seller that impacts the final price both parties agree to. There are distinct types of corporate synergies, as follows.

A marketing synergy refers to the use of information campaigns, studies, and scientific discovery or experimentation for research and development. This promotes the sale of products for varied use or off-market sales as well as development of marketing tools and in several cases exaggeration of effects. It is also often a meaningless buzzword used by corporate leaders.

A revenue synergy refers to the opportunity of a combined corporate entity to generate more revenue than its two predecessor stand-alone companies would be able to generate. For example, if company A sells product X through its sales force, company B sells product Y, and company A decides to buy company B, then the new company could use each salesperson to sell products X and Y, thereby increasing the revenue that each salesperson generates for the company.

In media revenue, synergy is the promotion and sale of a product throughout the various subsidiaries of a media conglomerate, e.g. films, soundtracks, or video games.

Financial synergy gained by the combined firm is a result of number of benefits which flow to the entity as a consequence of acquisition and merger. These benefits may be:

This is when a firm having a number of cash extensive projects acquires a firm which is cash-rich, thus enabling the new combined firm to enjoy the profits from investing the cash of one firm in the projects of the other.

If two firms have no or little capacity to carry debt before individually, it is possible for them to join and gain the capacity to carry the debt through decreased gearing (leverage). This creates value for the firm, as debt is thought to be a cheaper source of finance.

It is possible for one firm to have unused tax benefits which might be offset against the profits of another after combination, thus resulting in less tax being paid. However this greatly depends on the tax law of the country.

Synergy in management and in relation to teamwork refers to the combined effort of individuals as participants of the team. The condition that exists when the organization's parts interact to produce a joint effect that is greater than the sum of the parts acting alone. Positive or negative synergies can exist. In these cases, positive synergy has positive effects such as improved efficiency in operations, greater exploitation of opportunities, and improved utilization of resources. Negative synergy on the other hand has negative effects such as: reduced efficiency of operations, decrease in quality, underutilization of resources and disequilibrium with the external environment.

A cost synergy refers to the opportunity of a combined corporate entity to reduce or eliminate expenses associated with running a business. Cost synergies are realized by eliminating positions that are viewed as duplicate within the merged entity. Examples include the headquarters office of one of the predecessor companies, certain executives, the human resources department, or other employees of the predecessor companies. This is related to the economic concept of economies of scale.

The synergistic action of the economic players lies within the economic phenomenon's profundity. The synergistic action gives different dimensions to competitiveness, strategy and network identity becoming an unconventional "weapon" which belongs to those who exploit the economic systems’ potential in depth.

The synergistic gravity equation (SYNGEq), according to its complex “title”, represents a synthesis of the endogenous and exogenous factors which determine the private and non-private economic decision makers to call to actions of synergistic exploitation of the economic network in which they operate. That is to say, SYNGEq constitutes a big picture of the factors/motivations which determine the entrepreneurs to contour an active synergistic network. SYNGEq includes both factors which character is changing over time (such as the competitive conditions), as well as classics factors, such as the imperative of the access to resources of the collaboration and the quick answers. The synergistic gravity equation (SINGEq) comes to be represented by the formula:

∑SYN.Act = ∑R-*I(CRed+COOP++A)*V(Cust.+Info.)*cc

where:

The synergistic network represents an integrated part of the economic system which, through the coordination and control functions (of the undertaken economic actions), agrees synergies. The networks which promote synergistic actions can be divided in horizontal synergistic networks and vertical synergistic networks.

The synergy effects are difficult (even impossible) to imitate by competitors and difficult to reproduce by their authors because these effects depend on the combination of factors with time-varying characteristics. The synergy effects are often called "synergistic benefits", representing the direct and implied result of the developed/adopted synergistic actions.

Synergy can also be defined as the combination of human strengths and computer strengths, such as advanced chess. Computers can process data much more quickly than humans, but lack the ability to respond meaningfully to arbitrary stimuli.

Etymologically, the "synergy" term was first used around 1600, deriving from the Greek word “synergos”, which means “to work together” or “to cooperate”. If during this period the synergy concept was mainly used in the theological field (describing “the cooperation of human effort with divine will”), in the 19th and 20th centuries, "synergy" was promoted in physics and biochemistry, being implemented in the study of the open economic systems only in the 1960 and 1970s.

In 1938, J. R. R. Tolkien wrote an essay titled "On Fairy Stores", delivered at an Andrew Lang Lecture, and reprinted in his book, "The Tolkien Reader", published in 1966. In it, he made two references to synergy, although he did not use that term. He wrote:
Faerie cannot be caught in a net of words; for it is one of its qualities to be indescribable, though not imperceptible. It has many ingredients, but analysis will not necessarily discover the secret of the whole.
And more succinctly, in a footnote, about the "part of producing the web of an intricate story", he wrote:
It is indeed easier to unravel a single "thread" — an incident, a name, a motive — than to trace the history of any "picture" defined by many threads. For with the picture in the tapestry a new element has come in: the picture is greater than, and not explained by, the sum of the component threads.
Synergy, a book: DION, Eric (2017), "Synergy; A Theoretical Model of Canada's Comprehensive Approach", iUniverse, 308 pp.

The informational synergies which can be applied also in media involve a compression of transmission, access and use of information’s time, the flows, circuits and means of handling information being based on a complementary, integrated, transparent and coordinated use of knowledge.

In media economics, synergy is the promotion and sale of a product (and all its versions) throughout the various subsidiaries of a media conglomerate, e.g. films, soundtracks or video games. Walt Disney pioneered synergistic marketing techniques in the 1930s by granting dozens of firms the right to use his Mickey Mouse character in products and ads, and continued to market Disney media through licensing arrangements. These products can help advertise the film itself and thus help to increase the film's sales. For example, the Spider-Man films had toys of webshooters and figures of the characters made, as well as posters and games. The NBC sitcom 30 Rock often shows the power of synergy, while also poking fun at the use of the term in the corporate world. There are also different forms of synergy in popular card games like , Yu-Gi-Oh!, Cardfight!! Vanguard, and Future Card Buddyfight.

When multiple sources of information taken together provide more information than the sum of the information provided by each source alone, there is said to be a synergy in the sources. This in contrast to the case in which the sources provide less information, in which case there is said to be a redundancy in the sources.



</doc>
<doc id="26860" url="https://en.wikipedia.org/wiki?curid=26860" title="Syntax">
Syntax

In linguistics, syntax () is the set of rules, principles, and processes that govern the structure of sentences (sentence structure) in a given language, usually including word order. The term "syntax" is also used to refer to the study of such principles and processes. The goal of many syntacticians is to discover the syntactic rules common to all languages.

The word "syntax" comes from Ancient Greek: "coordination", which consists of "syn", "together", and "táxis", "an ordering".

One basic description of a language's syntax is the sequence in which the subject (S), verb (V), and object (O) usually appear in sentences. Over 85% of languages usually place the subject first, either in the sequence SVO or the sequence SOV. The other possible sequences are VSO, VOS, OVS, and OSV, the last three of which are rare. In most generative theories of syntax, these surface differences arise from a more complex clausal phrase structure, and each order may be compatible with multiple derivations.

The "Aṣṭādhyāyī" of Pāṇini (c. 4th century BC in Ancient India), is often cited as an example of a premodern work that approaches the sophistication of a modern syntactic theory (as works on grammar were written long before modern syntax came about). In the West, the school of thought that came to be known as "traditional grammar" began with the work of Dionysius Thrax.

For centuries, a framework known as (first expounded in 1660 by Antoine Arnauld in a book of the same title) dominated work in syntax: as its basic premise the assumption that language is a direct reflection of thought processes and therefore there is a single, most natural way to express a thought.

However, in the 19th century, with the development of historical-comparative linguistics, linguists began to realize the sheer diversity of human language and to question fundamental assumptions about the relationship between language and logic. It became apparent that there was no such thing as the most natural way to express a thought, and therefore logic could no longer be relied upon as a basis for studying the structure of language.

The Port-Royal grammar modeled the study of syntax upon that of logic. (Indeed, large parts of the Port-Royal Logic were copied or adapted from the "Grammaire générale".) Syntactic categories were identified with logical ones, and all sentences were analyzed in terms of "subject – copula – predicate". Initially, this view was adopted even by the early comparative linguists such as Franz Bopp.

The central role of syntax within theoretical linguistics became clear only in the 20th century, which could reasonably be called the "century of syntactic theory" as far as linguistics is concerned. (For a detailed and critical survey of the history of syntax in the last two centuries, see the monumental work by Giorgio Graffi (2001).)

There are a number of theoretical approaches to the discipline of syntax. One school of thought, founded in the works of Derek Bickerton, sees syntax as a branch of biology, since it conceives of syntax as the study of linguistic knowledge as embodied in the human mind. Other linguists (e.g., Gerald Gazdar) take a more Platonistic view, since they regard syntax to be the study of an abstract formal system. Yet others (e.g., Joseph Greenberg) consider syntax a taxonomical device to reach broad generalizations across languages.

Dependency grammar is an approach to sentence structure where syntactic units are arranged according to the dependency relation, as opposed to the constituency relation of phrase structure grammars. Dependencies are directed links between words. The (finite) verb is seen as the root of all clause structure and all the other words in the clause are either directly or indirectly dependent on this root. Some prominent dependency-based theories of syntax are:


Lucien Tesnière (1893–1954) is widely seen as the father of modern dependency-based theories of syntax and grammar. He argued vehemently against the binary division of the clause into subject and predicate that is associated with the grammars of his day (S → NP VP) and which remains at the core of most phrase structure grammars. In the place of this division, he positioned the verb as the root of all clause structure.

Categorial grammar is an approach that attributes the syntactic structure not to rules of grammar, but to the properties of the syntactic categories themselves. For example, rather than asserting that sentences are constructed by a rule that combines a noun phrase (NP) and a verb phrase (VP) (e.g., the phrase structure rule S → NP VP), in categorial grammar, such principles are embedded in the category of the head word itself. So the syntactic category for an intransitive verb is a complex formula representing the fact that the verb acts as a function word requiring an NP as an input and produces a sentence level structure as an output. This complex category is notated as (NP\S) instead of V. NP\S is read as "a category that searches to the left (indicated by \) for an NP (the element on the left) and outputs a sentence (the element on the right)." The category of transitive verb is defined as an element that requires two NPs (its subject and its direct object) to form a sentence. This is notated as (NP/(NP\S)) which means "a category that searches to the right (indicated by /) for an NP (the object), and generates a function (equivalent to the VP) which is (NP\S), which in turn represents a function that searches to the left for an NP and produces a sentence."

Tree-adjoining grammar is a categorial grammar that adds in partial tree structures to the categories.

Theoretical approaches to syntax that are based upon probability theory are known as stochastic grammars. One common implementation of such an approach makes use of a neural network or connectionism.

Functionalist models of grammar study the form–function interaction by performing a structural and a functional analysis.


The hypothesis of generative grammar is that language is a biological structure. The difference between structural–functional and generative models is that, in generative grammar, the object is placed into the verb phrase. Generative grammar is meant to be used to describe all human language and to predict whether any given utterance in a hypothetical language would sound correct to a speaker of that language (versus constructions which no human language would use). This approach to language was pioneered by Noam Chomsky. Most generative theories (although not all of them) assume that syntax is based upon the constituent structure of sentences. Generative grammars are among the theories that focus primarily on the form of a sentence, rather than its communicative function.

Among the many generative theories of linguistics, the Chomskyan theories are:

Other theories that find their origin in the generative paradigm are:

The Cognitive Linguistics framework stems from generative grammar, but adheres to evolutionary rather than Chomskyan linguistics. Cognitive models often recognise the generative assumption that the object belongs to the verb phrase. Cognitive frameworks include:







</doc>
<doc id="26861" url="https://en.wikipedia.org/wiki?curid=26861" title="Shamanism">
Shamanism

Shamanism is a religious practice that involves a practitioner, a shaman, who is believed to interact with a spirit world through altered states of consciousness, such as trance. The goal of this is usually to direct these spirits or spiritual energies into the physical world, for healing or some other purpose.

Beliefs and practices that have been categorized as "shamanic" have attracted the interest of scholars from a wide variety of disciplines, including anthropologists, archaeologists, historians, religious studies scholars, philosophers and psychologists. Hundreds of books and academic papers on the subject have been produced, with a peer-reviewed academic journal being devoted to the study of shamanism. In the 20th century, many Westerners involved in counter-cultural movements have created modern magico-religious practices influenced by their ideas of Indigenous religions from across the world, creating what has been termed "neoshamanism" or the neoshamanic movement. It has affected the development of many neopagan practices, as well as faced a backlash and accusations of cultural appropriation, exploitation and misrepresentation when outside observers have tried to represent cultures to which they do not belong.

The word "shamanism" probably derives from the Manchu-Tungus word , meaning "one who knows". The word "shaman" may also have originated from the Evenki word "šamán", most likely from the southwestern dialect spoken by the Sym Evenki peoples. The Tungusic term was subsequently adopted by Russians interacting with the Indigenous peoples in Siberia. It is found in the memoirs of the exiled Russian churchman Avvakum.

The word was brought to Western Europe in the late 17th century by the Dutch traveler Nicolaes Witsen, who reported his stay and journeys among the Tungusic- and Samoyedic-speaking Indigenous peoples of Siberia in his book "Noord en Oost Tataryen" (1692). Adam Brand, a merchant from Lübeck, published in 1698 his account of a Russian embassy to China; a translation of his book, published the same year, introduced the word "shaman" to English speakers.

The etymology of the Evenki word is sometimes connected to a Tungus root "ša-" "to know". This has been questioned on linguistic grounds: "The possibility cannot be completely rejected, but neither should it be accepted without reservation since the assumed derivational relationship is phonologically irregular (note especially the vowel quantities)." Other scholars assert that the word comes directly from the Manchu language, and as such would be the only commonly used English word that is a loan from this language.

However, Mircea Eliade noted that the Sanskrit word "śramaṇa", designating a wandering monastic or holy figure, has spread to many Central Asian languages along with Buddhism and could be the ultimate origin of the Tungusic word. This proposal has been thoroughly critiqued since 1917. Ethnolinguist Juha Janhunen regards it as an "anachronism" and an "impossibility" that is nothing more than a "far-fetched etymology".

Twenty-first-century anthropologist and archeologist Silvia Tomaskova argues that by the mid-1600s, many Europeans applied the Arabic term "shaitan" (meaning "devil") to the non-Christian practices and beliefs of Indigenous peoples beyond the Ural Mountains. She suggests that "shaman" may have entered the various Tungus dialects as a corruption of this term, and then been told to Christian missionaries, explorers, soldiers and colonial administrators with whom the people had increasing contact for centuries.

A (female shaman) is sometimes called a ', which is not an actual Tungus term but simply "shaman" plus the Russian suffix ' (for feminine nouns).

There is no single agreed-upon definition for the word "shamanism" among anthropologists. The English historian Ronald Hutton noted that by the dawn of the 21st century, there were four separate definitions of the term which appeared to be in use. The first of these uses the term to refer to "anybody who contacts a spirit world while in an altered state of consciousness." The second definition limits the term to refer to those who contact a spirit world while in an altered state of consciousness at the behest of others. The third definition attempts to distinguish shamans from other magico-religious specialists who are believed to contact spirits, such as "mediums", "witch doctors", "spiritual healers" or "prophets," by claiming that shamans undertake some particular technique not used by the others. Problematically, scholars advocating the third view have failed to agree on what the defining technique should be. The fourth definition identified by Hutton uses "shamanism" to refer to the Indigenous religions of Siberia and neighboring parts of Asia. According to the Golomt Center for Shamanic Studies, a Mongolian organisation of shamans, the Evenk word "shaman" would more accurately be translated as "priest".

According to the Oxford English Dictionary, a shaman ( , or ) is someone who is regarded as having access to, and influence in, the world of benevolent and malevolent spirits, who typically enters into a trance state during a ritual, and practices divination and healing. The word "shaman" probably originates from the Tungusic Evenki language of North Asia. According to ethnolinguist Juha Janhunen, "the word is attested in all of the Tungusic idioms" such as Negidal, Lamut, Udehe/Orochi, Nanai, Ilcha, Orok, Manchu and Ulcha, and "nothing seems to contradict the assumption that the meaning 'shaman' also derives from Proto-Tungusic" and may have roots that extend back in time at least two millennia. The term was introduced to the west after Russian forces conquered the shamanistic Khanate of Kazan in 1552.

The term "shamanism" was first applied by Western anthropologists as outside observers of the ancient religion of the Turks and Mongols, as well as those of the neighbouring Tungusic- and Samoyedic-speaking peoples. Upon observing more religious traditions across the world, some Western anthropologists began to also use the term in a very broad sense. The term was used to describe unrelated magico-religious practices found within the ethnic religions of other parts of Asia, Africa, Australasia and even completely unrelated parts of the Americas, as they believed these practices to be similar to one another. While the term has been incorrectly applied by cultural outsiders to many Indigenous spiritual practices, the words “shaman” and “shamanism” do not accurately describe the variety and complexity that is Indigenous spirituality. Each Nation and tribe has its own way of life, and uses terms in their own languages.

Mircea Eliade writes, "A first definition of this complex phenomenon, and perhaps the least hazardous, will be: shamanism = 'technique of religious ecstasy'." Shamanism encompasses the premise that shamans are intermediaries or messengers between the human world and the spirit worlds. Shamans are said to treat ailments and illnesses by mending the soul. Alleviating traumas affecting the soul or spirit are believed to restore the physical body of the individual to balance and wholeness. Shamans also claim to enter supernatural realms or dimensions to obtain solutions to problems afflicting the community. Shamans claim to visit other worlds or dimensions to bring guidance to misguided souls and to ameliorate illnesses of the human soul caused by foreign elements. Shamans operate primarily within the spiritual world, which, they believe, in turn affects the human world. The restoration of balance is said to result in the elimination of the ailment.

Shamanism is a system of religious practice. Historically, it is often associated with Indigenous and tribal societies, and involves belief that shamans, with a connection to the otherworld, have the power to heal the sick, communicate with spirits, and escort souls of the dead to the afterlife. Shamanism is especially associated with the Native Peoples of Siberia in northern Asia, where shamanic practice has been noted for centuries by Asian and Western visitors. It is an ideology that used to be widely practiced in Europe, Asia, Tibet, North and South America, and Africa. It centered on the belief in supernatural phenomenon such as the world of gods, demons, and ancestral spirits.

Despite structural implications of colonialism and imperialism that have limited the ability of Indigenous Peoples to practice traditional spiritualities, many communities are undergoing resurgence through self-determination and the reclamation of dynamic traditions. Other groups have been able to avoid some of these structural impediments by virtue of their isolation, such as the nomadic Tuvan (with an estimated population of 3000 people surviving from this tribe). Tuva is one of the most isolated tribes in Russia where the art of shamanism has been preserved until today due to its isolated existence, allowing it to be free from the influences of other major religions.

Shamans often claim to have been called through dreams or signs. However, some say their powers are inherited. In traditional societies shamanic training varies in length, but generally takes years.

Turner and colleagues mention a phenomenon called "shamanistic initiatory crisis", a rite of passage for shamans-to-be, commonly involving physical illness or psychological crisis. The significant role of initiatory illnesses in the calling of a shaman can be found in the detailed case history of Chuonnasuan, who was the last master shaman among the Tungus peoples in Northeast China.

The wounded healer is an archetype for a shamanic trial and journey. This process is important to young shamans. They undergo a type of sickness that pushes them to the brink of death. This is said to happen for two reasons:

Though the importance of spiritual roles in many cultures cannot be overlooked, the degree to which such roles are comparable (and even classifiable under one term) is questionable. In fact, scholars have argued that such universalist classifications paint Indigenous societies as primitive while exemplifying the civility of Western societies. That being said, shamans have been conceptualized as those who are able to gain knowledge and power to heal in the spiritual world or dimension. Most shamans have dreams or visions that convey certain messages. Shamans may claim to have or have acquired many spirit guides, who they believe guide and direct them in their travels in the spirit world. These spirit guides are always thought to be present within the shaman, although others are said to encounter them only when the shaman is in a trance. The spirit guide energizes the shamans, enabling them to enter the spiritual dimension. Shamans claim to heal within the communities and the spiritual dimension by returning lost parts of the human soul from wherever they have gone. Shamans also claim to cleanse excess negative energies, which are said to confuse or pollute the soul. 
Shamans act as mediators in their cultures. Shamans claim to communicate with the spirits on behalf of the community, including the spirits of the deceased. Shamans believe they can communicate with both living and dead to alleviate unrest, unsettled issues, and to deliver gifts to the spirits.

Among the Selkups, the sea duck is a spirit animal. Ducks fly in the air and dive in the water and are thus believed to belong to both the upper world and the world below. Among other Siberian peoples, these characteristics are attributed to waterfowl in general. The upper world is the afterlife primarily associated with deceased humans and is believed to be accessed by soul journeying through a portal in the sky. The lower world or "world below" is the afterlife primarily associated with animals and is believed to be accessed by soul journeying through a portal in the earth. In shamanic cultures, many animals are regarded as spirit animals.

Shamans perform a variety of functions depending upon their respective cultures; healing, leading a sacrifice, preserving traditions by storytelling and songs, fortune-telling, and acting as a psychopomp ("guide of souls"). A single shaman may fulfill several of these functions.

The functions of a shaman may include either guiding to their proper abode the souls of the dead (which may be guided either one-at-a-time or in a group, depending on the culture), and the curing of ailments. The ailments may be either purely physical afflictions—such as disease, which are claimed to be cured by gifting, flattering, threatening, or wrestling the disease-spirit (sometimes trying all these, sequentially), and which may be completed by displaying a supposedly extracted token of the disease-spirit (displaying this, even if "fraudulent", is supposed to impress the disease-spirit that it has been, or is in the process of being, defeated so that it will retreat and stay out of the patient's body), or else mental (including psychosomatic) afflictions—such as persistent terror, which is likewise believed to be cured by similar methods. In most languages a different term other than the one translated "shaman" is usually applied to a religious official leading sacrificial rites ("priest"), or to a raconteur ("sage") of traditional lore; there may be more of an overlap in functions (with that of a shaman), however, in the case of an interpreter of omens or of dreams.

There are distinct types of shamans who perform more specialized functions. For example, among the Nani people, a distinct kind of shaman acts as a psychopomp. Other specialized shamans may be distinguished according to the type of spirits, or realms of the spirit world, with which the shaman most commonly interacts. These roles vary among the Nenets, Enets, and Selkup shamans.

The assistant of an Oroqen shaman (called "jardalanin", or "second spirit") knows many things about the associated beliefs. He or she accompanies the rituals and interprets the behaviors of the shaman. Despite these functions, the "jardalanin" is not a shaman. For this interpretative assistant, it would be unwelcome to fall into a trance.

Among the Tucano people, a sophisticated system exists for environmental resources management and for avoiding resource depletion through overhunting. This system is conceptualized mythologically and symbolically by the belief that breaking hunting restrictions may cause illness. As the primary teacher of tribal symbolism, the shaman may have a leading role in this ecological management, actively restricting hunting and fishing. The shaman is able to "release" game animals, or their souls, from their hidden abodes. The Piaroa people have ecological concerns related to shamanism. Among the Inuit, shamans fetch the souls of game from remote places, or soul travel to ask for game from mythological beings like the Sea Woman.

The way shamans get sustenance and take part in everyday life varies across cultures. In many Inuit groups, they provide services for the community and get a "due payment", and believe the payment is given to the helping spirits. An account states that the gifts and payments that a shaman receives are given by his partner spirit. Since it obliges the shaman to use his gift and to work regularly in this capacity, the spirit rewards him with the goods that it receives. These goods, however, are only "welcome addenda". They are not enough to enable a full-time shaman. Shamans live like any other member of the group, as a hunter or housewife. Due to the popularity of ayahuasca tourism in South America, there are practitioners in areas frequented by backpackers who make a living from leading ceremonies.

There are many variations of shamanism throughout the world, but several common beliefs are shared by all forms of shamanism. Common beliefs identified by Eliade (1972) are the following:

As Alice Kehoe notes, Eliade's conceptualization of shamans produces a universalist image of Indigenous cultures, which perpetuates notions of the dead (or dying) Indian as well as the noble savage.

Shamanism is based on the premise that the visible world is pervaded by invisible forces or spirits which affect the lives of the living. Although the causes of disease lie in the spiritual realm, inspired by malicious spirits, both spiritual and physical methods are used to heal. Commonly, a shaman "enters the body" of the patient to confront the spiritual infirmity and heals by banishing the infectious spirit.

Many shamans have expert knowledge of medicinal plants native to their area, and an herbal treatment is often prescribed. In many places shamans learn directly from the plants, harnessing their effects and healing properties, after obtaining permission from the indwelling or patron spirits. In the Peruvian Amazon Basin, shamans and "curanderos" use medicine songs called "icaros" to evoke spirits. Before a spirit can be summoned it must teach the shaman its song. The use of totemic items such as rocks with special powers and an animating spirit is common.

Such practices are presumably very ancient. Plato wrote in his "Phaedrus" that the "first prophecies were the words of an oak", and that those who lived at that time found it rewarding enough to "listen to an oak or a stone, so long as it was telling the truth".

Belief in witchcraft and sorcery, known as "brujería" in Latin America, exists in many societies. Other societies assert all shamans have the power to both cure and kill. Those with shamanic knowledge usually enjoy great power and prestige in the community, but they may also be regarded suspiciously or fearfully as potentially harmful to others.

By engaging in their work, a shaman is exposed to significant personal risk as shamanic plant materials can be toxic or fatal if misused. Spells are commonly used in an attempt to protect against these dangers, and the use of more dangerous plants is often very highly ritualized.






Generally, shamans traverse the axis mundi and enter the "spirit world" by effecting a transition of consciousness, entering into an ecstatic trance, either autohypnotically or through the use of entheogens or ritual performances. The methods employed are diverse, and are often used together.

An entheogen ("generating the divine within") is a psychoactive substance used in a religious, shamanic, or spiritual context. Entheogens have been used in a ritualized context for thousands of years; their religious significance is well established in anthropological and modern evidences. Examples of traditional entheogens include: peyote, psilocybin and Amanita muscaria (fly agaric) mushrooms, uncured tobacco, cannabis, ayahuasca, "Salvia divinorum", iboga, and Mexican morning glory.

Some shamans observe dietary or customary restrictions particular to their tradition. These restrictions are more than just cultural. For example, the diet followed by shamans and apprentices prior to participating in an ayahuasca ceremony includes foods rich in tryptophan (a biosynthetic precursor to serotonin) as well as avoiding foods rich in tyramine, which could induce hypertensive crisis if ingested with MAOIs such as are found in ayahuasca brews as well as abstinence from alcohol or sex.

Entheogens have a substantial history of commodification, especially in the realm of spiritual tourism. For instance, countries such as Brazil and Peru have faced an influx of tourists since the psychedelic era beginning in the late 1960s, initiating what has been termed "ayahuasca tourism."

Just like shamanism itself, music and songs related to it in various cultures are diverse. In several instances, songs related to shamanism are intended to imitate natural sounds, via onomatopoeia.

Sound mimesis in various cultures may serve other functions not necessarily related to shamanism: practical goals such as luring game in the hunt; or entertainment (Inuit throat singing).


Shamans may employ varying materials in spiritual practice in different cultures. 

There are two major frameworks among cognitive and evolutionary scientists for explaining shamanism. The first, proposed by anthropologist Michael Winkelman, is known as the "neurotheological theory". According to Winkelman, shamanism develops reliably in human societies because it provides valuable benefits to the practitioner, their group, and individual clients. In particular, the trance states induced by dancing, hallucinogens, and other triggers are hypothesized to have an "integrative" effect on cognition, allowing communication among mental systems that specialize in theory of mind, social intelligence, and natural history. With this cognitive integration, the shaman can better predict the movement of animals, resolve group conflicts, plan migrations, and provide other useful services.

The neurotheological theory contrasts with the "by-product" or "subjective" model of shamanism developed by Harvard anthropologist Manvir Singh. According to Singh, shamanism is a cultural technology that adapts to (or hacks) our psychological biases to convince us that a specialist can influence important but uncontrollable outcomes. Citing work on the psychology of magic and superstition, Singh argues that humans search for ways of influencing uncertain events, such as healing illness, controlling rain, or attracting animals. As specialists compete to help their clients control these outcomes, they drive the evolution of psychologically compelling magic, producing traditions adapted to people's cognitive biases. Shamanism, Singh argues, is the culmination of this cultural evolutionary process—a psychologically appealing method for controlling uncertainty. For example, some shamanic practices exploit our intuitions about humanness: Practitioners use trance and dramatic initiations to seemingly become entities distinct from normal humans and thus more apparently capable of interacting with the invisible forces believed to oversee important outcomes. Influential cognitive and anthropological scientists such as Pascal Boyer and Nicholas Humphrey have endorsed Singh's approach, although other researchers have criticized Singh's dismissal of individual- and group-level benefits.

David Lewis-Williams explains the origins of shamanic practice, and some of its precise forms, through aspects of human consciousness evinced in cave art and LSD experiments alike.

Gerardo Reichel-Dolmatoff relates these concepts to developments in the ways that modern science (systems theory, ecology, new approaches in anthropology and archeology) treats causality in a less linear fashion. He also suggests a cooperation of modern science and Indigenous lore.

Shamanic practices may originate as early as the Paleolithic, predating all organized religions, and certainly as early as the Neolithic period. The earliest known undisputed burial of a shaman (and by extension the earliest undisputed evidence of shamans and shamanic practices) dates back to the early Upper Paleolithic era (c. 30,000 BP) in what is now the Czech Republic.

Sanskrit scholar and comparative mythologist Michael Witzel proposes that all of the world's mythologies, and also the concepts and practices of shamans, can be traced to the migrations of two prehistoric populations: the "Gondwana" type (of circa 65,000 years ago) and the "Laurasian" type (of circa 40,000 years ago).

In November 2008, researchers from the Hebrew University of Jerusalem announced the discovery of a 12,000-year-old site in Israel that is perceived as one of the earliest-known shaman burials. The elderly woman had been arranged on her side, with her legs apart and folded inward at the knee. Ten large stones were placed on the head, pelvis, and arms. Among her unusual grave goods were 50 complete tortoise shells, a human foot, and certain body parts from animals such as a cow tail and eagle wings. Other animal remains came from a boar, leopard, and two martens. "It seems that the woman … was perceived as being in a close relationship with these animal spirits", researchers noted. The grave was one of at least 28 graves at the site, located in a cave in lower Galilee and belonging to the Natufian culture, but is said to be unlike any other among the Epipaleolithic Natufians or in the Paleolithic period.

A debated etymology of the word "shaman" is "one who knows", implying, among other things, that the shaman is an expert in keeping together the multiple codes of the society, and that to be effective, shamans must maintain a comprehensive view in their mind which gives them certainty of knowledge. According to this view, the shaman uses (and the audience understands) multiple codes, expressing meanings in many ways: verbally, musically, artistically, and in dance. Meanings may be manifested in objects such as amulets. If the shaman knows the culture of their community well, and acts accordingly, their audience will know the used symbols and meanings and therefore trust the shamanic worker.

There are also semiotic, theoretical approaches to shamanism, and examples of "mutually opposing symbols" in academic studies of Siberian lore, distinguishing a "white" shaman who contacts sky spirits for good aims by day, from a "black" shaman who contacts evil spirits for bad aims by night. (Series of such opposing symbols referred to a world-view behind them. Analogously to the way grammar arranges words to express meanings and convey a world, also this formed a cognitive map). Shaman's lore is rooted in the folklore of the community, which provides a "mythological mental map". Juha Pentikäinen uses the concept ""grammar of mind"".

Armin Geertz coined and introduced the hermeneutics, or "ethnohermeneutics", interpretation. Hoppál extended the term to include not only the interpretation of oral and written texts, but that of "visual texts as well (including motions, gestures and more complex rituals, and ceremonies performed, for instance, by shamans)". Revealing the animistic views in shamanism, but also their relevance to the contemporary world, where ecological problems have validated paradigms of balance and protection.

Shamanism is believed to be declining around the world, possibly due to other organized religious influences, like Christianity, that want people who practice shamanism to convert to their own system and doctrine. Another reason is Western views of shamanism as primitive, superstitious, backward and outdated. Whalers who frequently interact with Inuit tribes are one source of this decline in that region.

In many areas, former shamans ceased to fulfill the functions in the community they used to, as they felt mocked by their own community, or regarded their own past as deprecated and were unwilling to talk about it to ethnographers.

Moreover, besides personal communications of former shamans, folklore texts may narrate directly about a deterioration process. For example, a Buryat epic text details the wonderful deeds of the ancient "first shaman" Kara-Gürgän: he could even compete with God, create life, steal back the soul of the sick from God without his consent. A subsequent text laments that shamans of older times were stronger, possessing capabilities like omnividence, fortune-telling even for decades in the future, moving as fast as a bullet.

In most affected areas, shamanic practices ceased to exist, with authentic shamans dying and their personal experiences dying with them. The loss of memories is not always lessened by the fact the shaman is not always the only person in a community who knows the beliefs and motives related to the local shaman-hood. Although the shaman is often believed and trusted precisely because he or she "accommodates" to the beliefs of the community, several parts of the knowledge related to the local shamanhood consist of personal experiences of the shaman, or root in his or her family life, thus, those are lost with his or her death. Besides that, in many cultures, the entire traditional belief system has become endangered (often together with a partial or total language shift), with the other people of the community remembering the associated beliefs and practices (or the language at all) grew old or died, many folklore memories songs, and texts were forgotten—which may threaten even such peoples who could preserve their isolation until the middle of the 20th century, like the Nganasan.

Some areas could enjoy a prolonged resistance due to their remoteness.

After exemplifying the general decline even in the most remote areas, there are revitalizations or tradition-preserving efforts as a response. Besides collecting the memories, there are also tradition-preserving and even revitalization efforts, led by authentic former shamans (for example among the Sakha people and Tuvans). However, according to Richard L. Allen, research and policy analyst for the Cherokee Nation, they are overwhelmed with fraudulent shamans ("plastic medicine people"). "One may assume that anyone claiming to be a Cherokee 'shaman, spiritual healer, or pipe-carrier', is equivalent to a modern day medicine show and snake-oil vendor." One indicator of a plastic shaman might be someone who discusses "Native American spirituality" but does not mention any specific Native American tribe.

Besides tradition-preserving efforts, there are also neoshamanistic movements, these may differ from many traditional shamanistic practice and beliefs in several points. Admittedly, several traditional beliefs systems indeed have ecological considerations (for example, many Inuit peoples), and among Tucano people, the shaman indeed has direct resource-protecting roles.

Today, shamanism survives primarily among Indigenous peoples. Shamanic practices continue today in the tundras, jungles, deserts, and other rural areas, and even in cities, towns, suburbs, and shantytowns all over the world. This is especially true for Africa and South America, where "mestizo shamanism" is widespread.

The anthropologist Alice Kehoe criticizes the term "shaman" in her book "Shamans and Religion: An Anthropological Exploration in Critical Thinking". Part of this criticism involves the notion of cultural appropriation. This includes criticism of New Age and modern Western forms of shamanism, which, according to Kehoe, misrepresent or dilute Indigenous practices. Kehoe also believes that the term reinforces racist ideas such as the noble savage.

Kehoe is highly critical of Mircea Eliade's work on shamanism as an invention synthesized from various sources unsupported by more direct research. To Kehoe, citing that ritualistic practices (most notably drumming, trance, chanting, entheogens and hallucinogens, spirit communication and healing) as being definitive of shamanism is poor practice. Such citations ignore the fact that those practices exist outside of what is defined as shamanism and play similar roles even in non-shamanic cultures (such as the role of chanting in Judeo-Christian and Islamic rituals) and that in their expression are unique to each culture that uses them. Such practices cannot be generalized easily, accurately, or usefully into a global religion of shamanism. Because of this, Kehoe is also highly critical of the hypothesis that shamanism is an ancient, unchanged, and surviving religion from the Paleolithic period.

The term has been criticized for its colonial roots and as a tool to perpetuate contemporary linguistic colonialism. By Western scholars, the term "shamanism" is used to refer to a variety of different cultures and practices around the world, and differ greatly in different Indigenous cultures. Author and award-winning scholar from the Driftpile Cree Nation in Canada Billy-Ray Belcourt argues that using language with the intention of simplifying culture that is diverse, such as Shamanism, as it is prevalent in communities around the world and is made up of many complex components, works to conceal the complexities of the social and political violence that Indigenous communities have experienced at the hands of settlers. Belcourt argues that language used to imply “simplicity” in regards to Indigenous culture, is a tool used to belittle Indigenous cultures, as it views Indigenous communities solely as a result of a history embroiled in violence, that leaves Indigenous communities only capable of simplicity and plainness.

Anthropologist Mihály Hoppál also discusses whether the term "shamanism" is appropriate. He notes that for many readers, "-ism" implies a particular dogma, like Buddhism or Judaism. He recommends using the term "shamanhood" or "shamanship" (a term used in old Russian and German ethnographic reports at the beginning of the 20th century) for stressing the diversity and the specific features of the discussed cultures. He believes that this places more stress on the local variations and emphasizes that shamanism is not a religion of sacred dogmas, but linked to the everyday life in a practical way. Following similar thoughts, he also conjectures a contemporary paradigm shift. Piers Vitebsky also mentions that, despite really astonishing similarities, there is no unity in shamanism. The various, fragmented shamanistic practices and beliefs coexist with other beliefs everywhere. There is no record of pure shamanistic societies (although their existence is not impossible). Norwegian social anthropologist Hakan Rydving has likewise argued for the abandonment of the terms "shaman" and "shamanism" as "scientific illusions."

Dulam Bumochir has affirmed the above critiques of "shamanism" as a Western construct created for comparative purposes and, in an extensive article, has documented the role of Mongols themselves, particularly "the partnership of scholars and shamans in the reconstruction of shamanism" in post-1990/post-communist Mongolia. This process has also been documented by Swiss anthropologist Judith Hangartner in her landmark study of Darhad shamans in Mongolia. Historian Karena Kollmar-Polenz argues that the social construction and reification of shamanism as a religious "other" actually began with the 18th-century writings of Tibetan Buddhist monks in Mongolia and later "probably influenced the formation of European discourse on Shamanism".




</doc>
<doc id="26862" url="https://en.wikipedia.org/wiki?curid=26862" title="Sexology">
Sexology

Sexology is the scientific study of human sexuality, including human sexual interests, behaviors, and functions. The term "sexology" does not generally refer to the non-scientific study of sexuality, such as political science or social criticism.

Sexologists apply tools from several academic fields, such as biology, medicine, psychology, epidemiology, sociology, and criminology. Topics of study include sexual development (puberty), sexual orientation, gender identity, sexual relationships, sexual activities, paraphilias, and atypical sexual interests. It also includes the study of sexuality across the lifespan, including child sexuality, puberty, adolescent sexuality, and sexuality among the elderly. Sexology also spans sexuality among the mentally and/or physically disabled. The sexological study of sexual dysfunctions and disorders, including erectile dysfunction, anorgasmia, and pedophilia, are also mainstays.

Sexual manuals have existed since antiquity, such as Ovid's "Ars Amatoria", the "Kama Sutra" of Vatsyayana, the "Ananga Ranga" and "The Perfumed Garden for the Soul's Recreation". ("Prostitution in the City of Paris"), an early 1830s study on 3,558 registered prostitutes in Paris, published by Alexander Jean Baptiste Parent-Duchatelet (and published in 1837, a year after he died), has been called the first work of modern sex research.

The scientific study of sexual behavior in human beings began in the 19th century. Shifts in Europe's national borders at that time brought into conflict laws that were sexually liberal and laws that criminalized behaviors such as homosexual activity.

Despite the prevailing social attitude of sexual repression in the Victorian era, the movement towards sexual emancipation began towards the end of the nineteenth century in England and Germany. In 1886, Richard Freiherr von Krafft-Ebing published "Psychopathia Sexualis." That work is considered as having established sexology as a scientific discipline.

In England, the founding father of sexology was the doctor and sexologist Havelock Ellis who challenged the sexual taboos of his era regarding masturbation and homosexuality and revolutionized the conception of sex in his time. His seminal work was the 1897 "Sexual Inversion", which describes the sexual relations of homosexual males, including men with boys. Ellis wrote the first objective study of homosexuality (the term was coined by Karl-Maria Kertbeny), as he did not characterize it as a disease, immoral, or a crime. The work assumes that same-sex love transcended age taboos as well as gender taboos. Seven of his twenty-one case studies are of inter-generational relationships. He also developed other important psychological concepts, such as autoerotism and narcissism, both of which were later developed further by Sigmund Freud.

Ellis pioneered transgender phenomena alongside the German Magnus Hirschfeld. He established it as new category that was separate and distinct from homosexuality. Aware of Hirschfeld's studies of transvestism, but disagreeing with his terminology, in 1913 Ellis proposed the term "sexo-aesthetic inversion" to describe the phenomenon.

In 1908, the first scholarly journal of the field, "Journal of Sexology" (Zeitschrift für Sexualwissenschaft), began publication and was published monthly for one year. Those issues contained articles by Freud, Alfred Adler, and Wilhelm Stekel. In 1913, the first academic association was founded: the "Society for Sexology".

Freud developed a theory of sexuality. These stages of development include: Oral, Anal, Phallic, Latency and Genital. These stages run from infancy to puberty and onwards. based on his studies of his clients, between the late 19th and early 20th centuries. Wilhelm Reich and Otto Gross, were disciples of Freud, but rejected by his theories because of their emphasis on the role of sexuality in the revolutionary struggle for the emancipation of mankind.
Pre-Nazi Germany, under the sexually liberal Napoleonic code, organized and resisted the anti-sexual, Victorian cultural influences. The momentum from those groups led them to coordinate sex research across traditional academic disciplines, bringing Germany to the leadership of sexology. Physician Magnus Hirschfeld was an outspoken advocate for sexual minorities, founding the Scientific Humanitarian Committee, the first advocacy for homosexual and transgender rights.

Hirschfeld also set up the first Institut für Sexualwissenschaft (Institute for Sexology) in Berlin in 1919. Its library housed over 20,000 volumes, 35,000 photographs, a large collection of art and other objects. People from around Europe visited the Institute to gain a clearer understanding of their sexuality and to be treated for their sexual concerns and dysfunctions.

Hirschfeld developed a system which identified numerous actual or hypothetical types of sexual intermediary between heterosexual male and female to represent the potential diversity of human sexuality, and is credited with identifying a group of people that today are referred to as transsexual or transgender as separate from the categories of homosexuality, he referred to these people as 'transvestiten' (transvestites). Germany's dominance in sexual behavior research ended with the Nazi regime. The Institute and its library were destroyed by the Nazis less than three months after they took power, May 8, 1933. The institute was shut down and Hirschfeld's books were burned.

Other sexologists in the early gay rights movement included Ernst Burchard and Benedict Friedlaender. Ernst Gräfenberg, after whom the G-spot is named, published the initial research developing the intrauterine device (IUD).

After World War II, sexology experienced a renaissance, both in the United States and Europe. Large scale studies of sexual behavior, sexual function, and sexual dysfunction gave rise to the development of sex therapy. Post-WWII sexology in the U.S. was influenced by the influx of European refugees escaping the Nazi regime and the popularity of the Kinsey studies. Until that time, American sexology consisted primarily of groups working to end prostitution and to educate youth about sexually transmitted diseases. Alfred Kinsey founded the Institute for Sex Research at Indiana University at Bloomington in 1947. This is now called the Kinsey Institute for Research in Sex, Gender and Reproduction. He wrote in his 1948 book that more was scientifically known about the sexual behavior of farm animals than of humans.

Psychologist and sexologist John Money developed theories on sexual identity and gender identity in the 1950s. His work, notably on the David Reimer case has since been regarded as controversial, even while the case was key to the development of treatment protocols for intersex infants and children.

Kurt Freund developed the penile plethysmograph in Czechoslovakia in the 1950s. The device was designed to provide an objective measurement of sexual arousal in males and is currently used in the assessment of pedophilia and hebephilia. This tool has since been used with sex offenders.

In 1966 and 1970, Masters and Johnson released their works "Human Sexual Response" and "Human Sexual Inadequacy," respectively. Those volumes sold well, and they were founders of what became known as the Masters & Johnson Institute in 1978.

Vern Bullough was a historian of sexology during this era, as well as being a researcher in the field.

The emergence of HIV/AIDS in the 1980s caused a dramatic shift in sexological research efforts towards understanding and controlling the spread of the disease.

Technological advances have permitted sexological questions to be addressed with studies using behavioral genetics, neuroimaging, and large-scale Internet-based surveys.

This is a list of sexologists and notable contributors to the field of sexology, by year of birth:




</doc>
<doc id="26865" url="https://en.wikipedia.org/wiki?curid=26865" title="List of leaders of the Soviet Union">
List of leaders of the Soviet Union

During its sixty-nine-year history, the Soviet Union usually had a "de facto" leader who would not necessarily be head of state, but would lead while holding an office such as Premier or General Secretary. Under the 1977 Constitution, the Chairman of the Council of Ministers, or Premier, was the head of government and the Chairman of the Presidium of the Supreme Soviet was the head of state. The office of the Chairman of the Council of Ministers was comparable to a prime minister in the First World whereas the office of the Chairman of the Presidium was comparable to a president. In the ideology of Vladimir Lenin, the head of the Soviet state was a collegiate body of the vanguard party (see "What Is To Be Done?"). 

Following Joseph Stalin's consolidation of power in the 1920s, the post of the General Secretary of the Central Committee of the Communist Party became synonymous with leader of the Soviet Union, because the post controlled both the Communist Party and the Soviet government both indirectly via party membership and via the tradition of a single person holding two highest posts in the party and in the government. The post of the General Secretary was abolished in 1952 under Stalin and later re-established by Nikita Khrushchev under the name of First Secretary. In 1966, Leonid Brezhnev reverted the office title to its former name. Being the head of the Communist Party of the Soviet Union, the office of the General Secretary was the highest in the Soviet Union until 1990. The post of General Secretary lacked clear guidelines of succession, so after the death or removal of a Soviet leader the successor usually needed the support of the Politburo, the Central Committee, or another government or party apparatus to both take and stay in power. The President of the Soviet Union, an office created in March 1990, replaced the General Secretary as the highest Soviet political office.

Contemporaneously to establishment of the office of the President, representatives of the Congress of People's Deputies voted to remove Article 6 from the Soviet Constitution which stated that the Soviet Union was a one-party state controlled by the Communist Party which in turn played the leading role in society. This vote weakened the party and its hegemony over the Soviet Union and its people. Upon death, resignation, or removal from office of an incumbent President, the Vice President of the Soviet Union would assume the office, though the Soviet Union dissolved before this was actually tested. After the failed August 1991 coup, the Vice President was replaced by an elected member of the State Council of the Soviet Union.

Vladimir Lenin was voted the Chairman of the Council of People's Commissars of the Soviet Union (Sovnarkom) on 30 December 1922 by the Congress of Soviets. At the age of 53, his health declined from effects of two bullet wounds, later aggravated by three strokes which culminated with his death in 1924. Irrespective of his health status in his final days, Lenin was already losing much of his power to Joseph Stalin. Alexei Rykov succeeded Lenin as Chairman of the Sovnarkom and although he was "de jure" the most powerful person in the country, but in fact all power was concentrated in the hands of the "troika" - the union of three influential party figures: Grigory Zinoviev, Joseph Stalin and Lev Kamenev. Stalin continued to increase his influence in the party, and by the end of the 1920s he became the sole dictator of the USSR, defeating all his political opponents. The post of General Secretary of the party, which was held by Stalin, became the most important post in the Soviet hierarchy.

Stalin's early policies pushed for rapid industrialisation, nationalisation of private industry and the collectivisation of private plots created under Lenin's New Economic Policy. As leader of the Politburo, Stalin consolidated near-absolute power by 1938 after the Great Purge, a series of campaigns of political murder, repression and persecution. Nazi German troops invaded the Soviet Union in June 1941, but by December the Soviet Army managed to stop the attack just shy of Moscow. On Stalin's orders, the Soviet Union launched a counter-attack on Nazi Germany which finally succeeded in 1945. Stalin died in March 1953 and his death triggered a power struggle in which Nikita Khrushchev after several years emerged victorious against Georgy Malenkov.

Khrushchev denounced Stalin on two occasions, first in 1956 and then in 1962. His policy of de-Stalinisation earned him many enemies within the party, especially from old Stalinist appointees. Many saw this approach as destructive and destabilising. A group known as Anti-Party Group tried to oust Khrushchev from office in 1957, but it failed. As Khrushchev grew older, his erratic behavior became worse, usually making decisions without discussing or confirming them with the Politburo. Leonid Brezhnev, a close companion of Khrushchev, was elected First Secretary the same day of Khrushchev's removal from power. Alexei Kosygin became the new Premier and Anastas Mikoyan kept his office as Chairman of the Presidium of the Supreme Soviet. On the orders of the Politburo, Mikoyan was forced to retire in 1965 and Nikolai Podgorny took over the office of Chairman of the Presidium. The Soviet Union in the post-Khrushchev 1960s was governed by a collective leadership. Henry A. Kissinger, the American National Security Advisor, mistakenly believed that Kosygin was the leader of the Soviet Union and that he was at the helm of Soviet foreign policy because he represented the Soviet Union at the 1967 Glassboro Summit Conference. The "Era of Stagnation", a derogatory term coined by Mikhail Gorbachev, was a period marked by low socio-economic efficiency in the country and a gerontocracy ruling the country. Yuri Andropov (aged 68 at the time) succeeded Brezhnev in his post as General Secretary in 1982. In 1983, Andropov was hospitalised and rarely met up at work to chair the politburo meetings due to his declining health. Nikolai Tikhonov usually chaired the meetings in his place. Following Andropov's death fifteen months after his appointment, an even older leader, 72 year old Konstantin Chernenko, was elected to the General Secretariat. His rule lasted for little more than a year until his death thirteen months later on 10 March 1985.

At the age of 54, Mikhail Gorbachev was elected to the General Secretariat by the Politburo on 11 March 1985. In May 1985, Gorbachev publicly admitted the slowing down of the economic development and inadequate living standards, being the first Soviet leader to do so while also beginning a series of fundamental reforms. From 1986 to around 1988, he dismantled central planning, allowed state enterprises to set their own outputs, enabled private investment in businesses not previously permitted to be privately owned and allowed foreign investment, among other measures. He also opened up the management of and decision-making within the Soviet Union and allowed greater public discussion and criticism, along with a warming of relationships with the West. These twin policies were known as "perestroika" (literally meaning "reconstruction", though it varies) and "glasnost" ("openness" and "transparency"), respectively. The dismantling of the principal defining features of Soviet Communism in 1988 and 1989 in the Soviet Union led to the unintended consequence of the Soviet Union breaking up after the failed August 1991 coup led by Gennady Yanayev.

The following list includes persons who held the top leadership position of the Soviet Union from its founding in 1922 until its 1991 dissolution. Note that † denotes leaders who died in office.

On four occasionsthe 2–3 year period between Vladimir Lenin's incapacitation and Joseph Stalin's leadership; the three months following Stalin's death; the interval between Nikita Khrushchev's fall and Leonid Brezhnev's consolidation of power; and the ailing Konstantin Chernenko's tenure as General Secretarya form of oligarchy known as a troika ("triumvirate") governed the Soviet Union, with no individual holding complete control over its policies.

The youngest leader of the USSR in 1924 was Joseph Stalin (45 years old). The oldest at the time of taking office was Konstantin Chernenko (72 years old). The oldest at the time of the loss of power is Leonid Brezhnev (75 years old). The shortest life was lived by Vladimir Lenin (53 years old). Mikhail Gorbachev (89 years old, living) lived the longest. Stalin ruled the longest (29 years). Georgy Malenkov spent the shortest time in power (183 days).




</doc>
<doc id="26866" url="https://en.wikipedia.org/wiki?curid=26866" title="Seafood">
Seafood

Seafood is any form of sea life regarded as food by humans, prominently including fish and shellfish. Shellfish include various species of molluscs (e.g. bivalve molluscs such as clams, oysters, and mussels and cephalopods such as octopus and squid), crustaceans (e.g. shrimp, crabs, and lobster), and echinoderms (e.g. sea cucumbers and sea urchins). Historically, marine mammals such as cetaceans (whales and dolphins) as well as seals have been eaten as food, though that happens to a lesser extent in modern times. Edible sea plants such as some seaweeds and microalgae are widely eaten as around the world, especially in Asia. In the United States, although not generally in the United Kingdom, the term "seafood" is extended to fresh water organisms eaten by humans, so all edible aquatic life may be referred to as "seafood".

The harvesting of wild seafood is usually known as fishing or hunting, while the cultivation and farming of seafood is known as aquaculture or fish farming (in the case of fish). Seafood is often colloquially distinguished from meat, although it is still animal in nature and is excluded from a vegetarian diet, as decided by groups like the Vegetarian Society after confusion surrounding pescetarianism. Seafood is an important source of (animal) protein in many diets around the world, especially in coastal areas.

Most of the seafood harvest is consumed by humans, but a significant proportion is used as fish food to farm other fish or rear farm animals. Some seafoods (i.e. kelp) are used as food for other plants (a fertilizer). In these ways, seafoods are used to produce further food for human consumption. Also, products such as fish oil and spirulina tablets are extracted from seafoods. Some seafood is fed to aquarium fish, or used to feed domestic pets such as cats. A small proportion is used in medicine, or is used industrially for nonfood purposes (e.g. leather).

The harvesting, processing, and consuming of seafoods are ancient practices with archaeological evidence dating back well into the Paleolithic. Findings in a sea cave at Pinnacle Point in South Africa indicate "Homo sapiens" (modern humans) harvested marine life as early as 165,000 years ago, while the Neanderthals, an extinct human species contemporary with early "Homo sapiens", appear to have been eating seafood at sites along the Mediterranean coast beginning around the same time. Isotopic analysis of the skeletal remains of Tianyuan man, a 40,000-year-old anatomically modern human from eastern Asia, has shown that he regularly consumed freshwater fish. Archaeology features such as shell middens, discarded fish bones and cave paintings show that sea foods were important for survival and consumed in significant quantities. During this period, most people lived a hunter-gatherer lifestyle and were, of necessity, constantly on the move. However, early examples of permanent settlements (though not necessarily permanently occupied), such as those at Lepenski Vir, were almost always associated with fishing as a major source of food.

The ancient river Nile was full of fish; fresh and dried fish were a staple food for much of the population. The Egyptians had implements and methods for fishing and these are illustrated in tomb scenes, drawings, and papyrus documents. Some representations hint at fishing being pursued as a pastime.

Fishing scenes are rarely represented in ancient Greek culture, a reflection of the low social status of fishing. However, Oppian of Corycus, a Greek author wrote a major treatise on sea fishing, the "Halieulica" or "Halieutika", composed between 177 and 180. This is the earliest such work to have survived to the modern day. The consumption of fish varied in accordance with the wealth and location of the household. In the Greek islands and on the coast, fresh fish and seafood (squid, octopus, and shellfish) were common. They were eaten locally but more often transported inland. Sardines and anchovies were regular fare for the citizens of Athens. They were sometimes sold fresh, but more frequently salted. A stele of the late 3rd century BCE from the small Boeotian city of Akraiphia, on Lake Copais, provides us with a list of fish prices. The cheapest was "skaren" (probably parrotfish) whereas Atlantic bluefin tuna was three times as expensive. Common salt water fish were yellowfin tuna, red mullet, ray, swordfish or sturgeon, a delicacy which was eaten salted. Lake Copais itself was famous in all Greece for its eels, celebrated by the hero of "The Acharnians". Other fresh water fish were pike-fish, carp and the less appreciated catfish.

Pictorial evidence of Roman fishing comes from mosaics. At a certain time the goatfish was considered the epitome of luxury, above all because its scales exhibit a bright red color when it dies out of water. For this reason these fish were occasionally allowed to die slowly at the table. There even was a recipe where this would take place "in garo", in the sauce. At the beginning of the Imperial era, however, this custom suddenly came to an end, which is why "mullus" in the feast of Trimalchio (see "the Satyricon") could be shown as a characteristic of the "parvenu", who bores his guests with an unfashionable display of dying fish.

In medieval times, seafood was less prestigious than other animal meats, and often seen as merely an alternative to meat on fast days. Still, seafood was the mainstay of many coastal populations. Kippers made from herring caught in the North Sea could be found in markets as far away as Constantinople. While large quantities of fish were eaten fresh, a large proportion was salted, dried, and, to a lesser extent, smoked. Stockfish, cod that was split down the middle, fixed to a pole and dried, was very common, though preparation could be time-consuming, and meant beating the dried fish with a mallet before soaking it in water. A wide range of mollusks including oysters, mussels and scallops were eaten by coastal and river-dwelling populations, and freshwater crayfish were seen as a desirable alternative to meat during fish days. Compared to meat, fish was much more expensive for inland populations, especially in Central Europe, and therefore not an option for most.

Modern knowledge of the reproductive cycles of aquatic species has led to the development of hatcheries and improved techniques of fish farming and aquaculture. Better understanding of the hazards of eating raw and undercooked fish and shellfish has led to improved preservation methods and processing.

The following table is based on the ISSCAAP classification (International Standard Statistical Classification of Aquatic Animals and Plants) used by the FAO for the purposes of collecting and compiling fishery statistics. The production figures have been extracted from the FAO FishStat database, and include both capture from wild fisheries and aquaculture production.

Fish is a highly perishable product: the "fishy" smell of dead fish is due to the breakdown of amino acids into biogenic amines and ammonia.

Live food fish are often transported in tanks at high expense for an international market that prefers its seafood killed immediately before it is cooked. Delivery of live fish without water is also being explored. While some seafood restaurants keep live fish in aquaria for display purposes or for cultural beliefs, the majority of live fish are kept for dining customers. The live food fish trade in Hong Kong, for example, is estimated to have driven imports of live food fish to more than 15,000 tonnes in 2000. Worldwide sales that year were estimated at US$400 million, according to the World Resources Institute.

If the cool chain has not been adhered to correctly, food products generally decay and become harmful before the validity date printed on the package. As the potential harm for a consumer when eating rotten fish is much larger than for example with dairy products, the U.S. Food and Drug Administration (FDA) has introduced regulation in the USA requiring the use of a time temperature indicator on certain fresh chilled seafood products.

Fresh fish is a highly perishable food product, so it must be eaten promptly or discarded; it can be kept for only a short time. In many countries, fresh fish are filleted and displayed for sale on a bed of crushed ice or refrigerated. Fresh fish is most commonly found near bodies of water, but the advent of refrigerated train and truck transportation has made fresh fish more widely available inland.

Long term preservation of fish is accomplished in a variety of ways. The oldest and still most widely used techniques are drying and salting. Desiccation (complete drying) is commonly used to preserve fish such as cod. Partial drying and salting is popular for the preservation of fish like herring and mackerel. Fish such as salmon, tuna, and herring are cooked and canned. Most fish are filleted prior to canning, but some small fish (e.g. sardines) are only decapitated and gutted prior to canning.

Seafood is consumed all over the world; it provides the world's prime source of high-quality protein: 14–16% of the animal protein consumed worldwide; over one billion people rely on seafood as their primary source of animal protein. Fish is among the most common food allergens.

Iceland, Japan, and Portugal are the greatest consumers of seafood per capita in the world.

The UK Food Standards Agency recommends that at least two portions of seafood should be consumed each week, one of which should be oil-rich. There are over 100 different types of seafood available around the coast of the UK.

Oil-rich fish such as mackerel or herring are rich in long chain Omega-3 oils. These oils are found in every cell of the human body, and are required for human biological functions such as brain functionality.

Whitefish such as haddock and cod are very low in fat and calories which, combined with oily fish rich in Omega-3 such as mackerel, sardines, fresh tuna, salmon and trout, can help to protect against coronary heart disease, as well as helping to develop strong bones and teeth.

Shellfish are particularly rich in zinc, which is essential for healthy skin and muscles as well as fertility. Casanova reputedly ate 50 oysters a day.

Over 33,000 species of fish and many more marine invertebrate species have been described. Bromophenols, which are produced by marine algae, gives marine animals an odor and taste that is absent from freshwater fish and invertebrates. Also, a chemical substance called dimethylsulfoniopropionate (DMSP) that is found in red and green algae is transferred to animals in the marine food chain. When broken down, dimethyl sulfide (DMS) is produced, and is often released during food preparation when fresh fish and shellfish are heated. In small quantities it creates a specific smell one associates with the ocean, but which in larger quantities gives the impression of rotten seaweed and old fish. Another molecule known as TMAO occurs in fishes and give them a distinct smell. It also exists in freshwater species, but becomes more numerous in the cells of an animal the deeper it lives, so that fish from the deeper parts of the ocean has a stronger taste than species who lives in shallow water. Eggs from seaweed contains sex pheromones called dictyopterenes, which are meant to attract the sperm. These pheromones are also found in edible seaweeds, which contributes to their aroma. However, only a small number of species are commonly eaten by humans.

There is broad scientific consensus that docosahexaenoic acid (DHA) and eicosapentaenoic acid (EPA) found in seafood are beneficial to neurodevelopment and cognition, especially at young ages. The United Nations Food and Agriculture Organization has described fish as "nature's super food." Seafood consumption is associated with improved neurologic development during gestation and early childhood and more tenuously linked to reduced mortality from coronary heart disease.

The parts of fish containing essential fats and micronutrients, often cited as primary health benefits for eating seafood, are frequently discarded in the developed world. Micronutrients including calcium, potassium, selenium, zinc, and iodine are found in their highest concentrations in the head, intestines, bones, and scales.

There is some debate over the particular health benefits of fish, especially regarding the relationship between seafood consumption and cardiovascular health. However, government recommendations promoting limited seafood consumption are relatively unified. The US Food and Drug Administration recommends moderate (4 oz for children and 8 - 12 oz for adults, weekly) consumption of fish as part of a healthy and balanced diet. The UK National Health Service gives similar advice, recommending at least 2 portions (about 10 oz) of fish weekly. The Chinese National Health Commission recommends slightly more, advising 10 - 20 oz of fish weekly.

There are numerous factors to consider when evaluating health hazards in seafood. These concerns include marine toxins, microbes, foodborne illness, radionuclide contamination, and man-made pollutants. Shellfish are among the more common food allergens. Most of these dangers can be mitigated or avoided with accurate knowledge of when and where seafood is caught. However, consumers have limited access to relevant and actionable information in this regard and the seafood industry's systemic problems with mislabelling make decisions about what is safe even more fraught.

Ciguatera fish poisoning (CFP) is an illness resulting from consuming toxins produced by dinoflagellates which bioaccumulate in the liver, roe, head, and intestines of reef fish. It is the most common disease associated with seafood consumption and poses the greatest risk to consumers. The population of plankton which produces these toxins varies significantly over time and location, as seen in red tides. Evaluating the risk of ciguatera in any given fish requires specific knowledge of its origin and life history, information which is often inaccurate or unavailable. While ciguatera is relatively widespread compared to other seafood-related health hazards (up to 50,000 people suffer from ciguatera every year), mortality is very low.

Fish and shellfish have a natural tendency to concentrate inorganic and organic toxins and pollutants in their bodies, including methylmercury, a highly toxic organic compound of mercury, polychlorinated biphenyls (PCBs), and microplastics. Species of fish that are high on the food chain, such as shark, swordfish, king mackerel, albacore tuna, and tilefish contain higher concentrations of these bioaccumulants. This is because bioaccumulants are stored in the muscle tissues of fish, and when a predatory fish eats another fish, it assumes the entire body burden of bioaccumulants in the consumed fish. Thus species that are high on the food chain amass body burdens of bioaccumulants that can be ten times higher than the species they consume. This process is called biomagnification.

Man-made disasters can cause localized hazards in seafood which may spread widely via piscine food chains. The first occurrence of widespread mercury poisoning in humans occurred this way in the 1950s in Minamata, Japan. Wastewater from a nearby chemical factory released methylmercury that accumulated in fish which were consumed by humans. Severe mercury poisoning is now known as Minamata disease. The 2011 Fukushima Daiichi Nuclear Power Plant disaster and 1947 - 1991 Marshall Islands nuclear bomb testing led to dangerous radionuclide contamination of local sea life which, in the latter case, remained as of 2008.

A widely cited study in JAMA which synthesized government and MEDLINE reports, and meta-analyses to evaluate risks from methylmercury, dioxins, and polychlorinated biphenyls to cardiovascular health and links between fish consumption and neurologic outcomes concluded that: "The benefits of modest fish consumption (1-2 servings/wk) outweigh the risks among adults and, excepting a few selected fish species, among women of childbearing age. Avoidance of modest fish consumption due to confusion regarding risks and benefits could result in thousands of excess CHD [congenital heart disease] deaths annually and suboptimal neurodevelopment in children." 

Due to the wide array of options in the seafood marketplace, seafood is far more susceptible to mislabeling than terrestrial food. There are more than 1,700 species of seafood in the United States' consumer marketplace, 80 - 90% of which are imported and less than 1% of which is tested for fraud. Estimates of mislabelled seafood in the United States range from 33% in general up to 86% for particular species.

Byzantine supply chains, frequent bycatch, brand naming, species substitution, and inaccurate ecolabels all contribute to confusion for the consumer. A 2013 study by Oceana found that one third of seafood sampled from the United States was incorrectly labelled. Snapper and tuna were particularly susceptible to mislabelling, and seafood substitution was the most common type of fraud. Another type of mislabelling is short-weighting, where practices such as overglazing or soaking can misleadingly increase the apparent weight of the fish. For supermarket shoppers, many seafood products are unrecognizable fillets. Without sophisticated DNA testing, there is no foolproof method to identify a fish species without their head, skin, and fins. This creates easy opportunities to substitute cheap products for expensive ones, a form of economic fraud.

Beyond financial concerns, significant health risks arise from hidden pollutants and marine toxins in an already fraught marketplace. Seafood fraud has led to widespread keriorrhea due to mislabeled escolar, mercury poisoning from products marketed as safe for pregnant women, and hospitalization and neurological damage due to mislabeled pufferfish. For example, a 2014 study published in PLOS One found that 15% of MSC certified Patagonian toothfish originated from uncertified and mercury polluted fisheries. These fishery-stock substitutions had 100% more mercury than their genuine counterparts, "vastly exceeding" limits in Canada, New Zealand, and Australia.

Research into population trends of various species of seafood is pointing to a global collapse of seafood species by 2048. Such a collapse would occur due to pollution and overfishing, threatening oceanic ecosystems, according to some researchers.

A major international scientific study released in November 2006 in the journal "Science" found that about one-third of all fishing stocks worldwide have collapsed (with a collapse being defined as a decline to less than 10% of their maximum observed abundance), and that if current trends continue all fish stocks worldwide will collapse within fifty years. In July 2009, Boris Worm of Dalhousie University, the author of the November 2006 study in "Science", co-authored an update on the state of the world's fisheries with one of the original study's critics, Ray Hilborn of the University of Washington at Seattle. The new study found that through good fisheries management techniques even depleted fish stocks can be revived and made commercially viable again.

The FAO State of World Fisheries and Aquaculture 2004 report estimates that in 2003, of the main fish stocks or groups of resources for which assessment information is available, "approximately one-quarter were overexploited, depleted or recovering from depletion (16%, 7% and 1% respectively) and needed rebuilding."

The National Fisheries Institute, a trade advocacy group representing the United States seafood industry, disagree. They claim that currently observed declines in fish population are due to natural fluctuations and that enhanced technologies will eventually alleviate whatever impact humanity is having on oceanic life.

For the most part Islamic dietary laws allow the eating of seafood, though the Hanbali forbid eels, the Shafi forbid frogs and crocodiles, and the Hanafi forbid bottom feeders such as shellfish and carp. The Jewish laws of Kashrut forbid the eating of shellfish and eels. In the Old Testament, the Mosaic Covenant allowed the Israelites to eat finfish, but shellfish and eels were an abomination and not allowed. In ancient and medieval times, the Catholic Church forbade the practice of eating meat, eggs and dairy products during Lent. Thomas Aquinas argued that these "afford greater pleasure as food [than fish], and greater nourishment to the human body, so that from their consumption there results a greater surplus available for seminal matter, which when abundant becomes a great incentive to lust." In the United States, the Catholic practice of abstaining from meat on Fridays during Lent has popularized the Friday fish fry, and parishes often sponsor a fish fry during Lent. In predominantly Roman Catholic areas, restaurants may adjust their menus during Lent by adding seafood items to the menu.





</doc>
<doc id="26872" url="https://en.wikipedia.org/wiki?curid=26872" title="SI base unit">
SI base unit

The SI base units are the standard units of measurement defined by the International System of Units (SI) for the seven base quantities of what is now known as the International System of Quantities: they are notably a basic set from which all other SI units can be derived. The units and their physical quantities are the second for time, the metre for measurement of length, the kilogram for mass, the ampere for electric current, the kelvin for temperature, the mole for amount of substance, and the candela for luminous intensity. The SI base units are a fundamental part of modern metrology, and thus part of the foundation of modern science and technology.

The SI base units form a set of mutually independent dimensions as required by dimensional analysis commonly employed in science and technology. 

The names and symbols of SI base units are written in lowercase, except the symbols of those named after a person, which are written with an initial capital letter. For example, the "metre" (US English: "meter") has the symbol m, but the "kelvin" has symbol K, because it is named after Lord Kelvin and the "ampere" with symbol A is named after André-Marie Ampère.

A number of other units, such as the litre (US English: "liter"), astronomical unit and electronvolt, are not formally part of the SI, but are accepted for use with SI.

On 20 May 2019, as the final act of the 2019 redefinition of the SI base units, the BIPM officially introduced the following new definitions, replacing the preceding definitions of the SI base units. 

New definitions of the base units were approved on 16 November 2018, and took effect 20 May 2019. The definitions of the base units have been modified several times since the Metre Convention in 1875, and new additions of base units have occurred. Since the redefinition of the metre in 1960, the kilogram had been the only base unit still defined directly in terms of a physical artefact, rather than a property of nature. This led to a number of the other SI base units being defined indirectly in terms of the mass of the same artefact; the mole, the ampere, and the candela were linked through their definitions to the mass of the International Prototype of the Kilogram, a roughly golfball-sized platinum–iridium cylinder stored in a vault near Paris.

It has long been an objective in metrology to define the kilogram in terms of a fundamental constant, in the same way that the metre is now defined in terms of the speed of light. The 21st General Conference on Weights and Measures (CGPM, 1999) placed these efforts on an official footing, and recommended "that national laboratories continue their efforts to refine experiments that link the unit of mass to fundamental or atomic constants with a view to a future redefinition of the kilogram". Two possibilities attracted particular attention: the Planck constant and the Avogadro constant.

In 2005, the International Committee for Weights and Measures (CIPM) approved preparation of new definitions for the kilogram, the ampere, and the kelvin and it noted the possibility of a new definition of the mole based on the Avogadro constant. The 23rd CGPM (2007) decided to postpone any formal change until the next General Conference in 2011.

In a note to the CIPM in October 2009, Ian Mills, the President of the CIPM "Consultative Committee – Units" (CCU) catalogued the uncertainties of the fundamental constants of physics according to the current definitions and their values under the proposed new definition. He urged the CIPM to accept the proposed changes in the definition of the "kilogram", "ampere", "kelvin", and "mole" so that they are referenced to the values of the fundamental constants, namely the Planck constant ("h"), the electron charge ("e"), the Boltzmann constant ("k"), and the Avogadro constant ("N"). This approach was approved in 2018, only after measurements of these constants were achieved with sufficient accuracy.




</doc>
<doc id="26873" url="https://en.wikipedia.org/wiki?curid=26873" title="Second">
Second

The second (symbol: s, abbreviation: sec) is the base unit of time in the International System of Units (SI) (French: Système International d’unités), commonly understood and historically defined as of a day – this factor derived from the division of the day first into 24 hours, then to 60 minutes and finally to 60 seconds each. Analog clocks and watches often have sixty tick marks on their faces, representing seconds (and minutes), and a "second hand" to mark the passage of time in seconds. Digital clocks and watches often have a two-digit seconds counter. The second is also part of several other units of measurement like meters per second for velocity, meters per second per second for acceleration, and cycles per second for frequency.

Although the historical definition of the unit was based on this division of the Earth's rotation cycle, the formal definition in the International System of Units (SI) is a much steadier timekeeper: it is defined by taking the fixed numerical value of the caesium frequency ∆"ν", the unperturbed ground-state hyperfine transition frequency of the caesium-133 atom, to be when expressed in the unit Hz, which is equal to s.
Because the Earth's rotation varies and is also slowing ever so slightly, a leap second is periodically added to clock time to keep clocks in sync with Earth's rotation.

Multiples of seconds are usually counted in hours and minutes. Fractions of a second are usually counted in tenths or hundredths. In scientific work, small fractions of a second are counted in milliseconds (thousandths), microseconds (millionths), nanoseconds (billionths), and sometimes smaller units of a second. An everyday experience with small fractions of a second is a 1-gigahertz microprocessor which has a cycle time of 1 nanosecond. Camera shutter speeds are often expressed in fractions of a second, such as second or second.

Sexagesimal divisions of the day from a calendar based on astronomical observation have existed since the third millennium BC, though they were not seconds as we know them today. Small divisions of time could not be measured back then, so such divisions were mathematically derived. The first timekeepers that could count seconds accurately were pendulum clocks invented in the 17th century. Starting in the 1950s, atomic clocks became better timekeepers than earth's rotation, and they continue to set the standard today.

A mechanical clock, one which does not depend on measuring the relative rotational position of the earth, keeps uniform time called "mean time", within whatever accuracy is intrinsic to it. That means that every second, minute and every other division of time counted by the clock will be the same duration as any other identical division of time. But a sundial which measures the relative position of the sun in the sky called "apparent time", does not keep uniform time. The time kept by a sundial varies by time of year, meaning that seconds, minutes and every other division of time is a different duration at different times of the year. The time of day measured with mean time versus apparent time may differ by as much as 15 minutes, but a single day will differ from the next by only a small amount; 15 minutes is a cumulative difference over a part of the year. The effect is due chiefly to the obliqueness of earth's axis with respect to its orbit around the sun.

The difference between apparent solar time and mean time was recognized by astronomers since antiquity, but prior to the invention of accurate mechanical clocks in the mid-17th century, sundials were the only reliable timepieces, and apparent solar time was the generally accepted standard.

Fractions of a second are usually denoted in decimal notation, for example 2.01 seconds, or two and one hundredth seconds. Multiples of seconds are usually expressed as minutes and seconds, or hours, minutes and seconds of clock time, separated by colons, such as 11:23:24, or 45:23 (the latter notation can give rise to ambiguity, because the same notation is used to denote hours and minutes). It rarely makes sense to express longer periods of time like hours or days in seconds, because they are awkwardly large numbers. For the metric unit of second, there are decimal prefixes representing 10 to 10 seconds.

Some common units of time in seconds are: a minute is 60 seconds; an hour is 3,600 seconds; a day is 86,400 seconds; a week is 604,800 seconds; a year (other than leap years) is 31,536,000 seconds; and a (Gregorian) century averages 3,155,695,200 seconds; with all of the above excluding any possible leap seconds.

Some common events in seconds are: a stone falls about 4.9 meters from rest in one second; a pendulum of length about one meter has a swing of one second, so pendulum clocks have pendulums about a meter long; the fastest human sprinters run 10 meters in a second; an ocean wave in deep water travels about 23 meters in one second; sound travels about 343 meters in one second in air; light takes 1.3 seconds to reach Earth from the surface of the Moon, a distance of 384,400 kilometers.

A second is part of other units, such as frequency measured in hertz (inverse seconds or second), speed (meters per second) and acceleration (meters per second squared). The metric system unit becquerel, a measure of radioactive decay, is measured in inverse seconds. The meter is defined in terms of the speed of light and the second; definitions of the metric base units kilogram, ampere, kelvin, and candela also depend on the second. The only base unit whose definition does not depend on the second is the mole. Of the 22 named derived units of the SI, only two (radian and steradian), do not depend on the second. Many derivative units for everyday things are reported in terms of larger units of time, not seconds, such as clock time in hours and minutes, velocity of a car in kilometers per hour or miles per hour, kilowatt hours of electricity usage, and speed of a turntable in rotations per minute.

A set of atomic clocks throughout the world keeps time by consensus: the clocks "vote" on the correct time, and all voting clocks are steered to agree with the consensus, which is called International Atomic Time (TAI). TAI "ticks" atomic seconds.

Civil time is defined to agree with the rotation of the earth. The international standard for timekeeping is Coordinated Universal Time (UTC). This time scale "ticks" the same atomic seconds as TAI, but inserts or omits leap seconds as necessary to correct for variations in the rate of rotation of the earth.

A time scale in which the seconds are not exactly equal to atomic seconds is UT1, a form of universal time. UT1 is defined by the rotation of the earth with respect to the sun, and does not contain any leap seconds. UT1 always differs from UTC by less than a second.

While they are not yet part of any timekeeping standard, optical lattice clocks with frequencies in the visible light spectrum now exist and are the most accurate timekeepers of all. A strontium clock with frequency 430 THz, in the red range of visible light, now holds the accuracy record: it will gain or lose less than a second in 15 billion years, which is longer than the estimated age of the universe. Such a clock can measure a change in its elevation of as little as 2 cm by the change in its rate due to gravitational time dilation.

There have only ever been three definitions of the second: as a fraction of the day, as a fraction of an extrapolated year, and as the microwave frequency of a caesium atomic clock, and they have realized a sexagesimal division of the day from ancient astronomical calendars.

Civilizations in the classic period and earlier created divisions of the calendar as well as arcs using a sexagesimal system of counting, so at that time the second was a sexagesimal subdivision of the day (ancient second=), not of the hour like the modern second (=). Sundials and water clocks were among the earliest timekeeping devices, and units of time were measured in degrees of arc. Conceptual units of time smaller than realizable on sundials were also used.

There are references to 'second' as part of a lunar month in the writings of natural philosophers of the Middle Ages, which were mathematical subdivisions that could not be measured mechanically.

The earliest mechanical clocks which appeared starting in the 14th century had displays that divided the hour into halves, thirds, quarters and sometimes even 12 parts, but never by 60. In fact, the hour was not commonly divided in 60 minutes as it was not uniform in duration. It was not practical for timekeepers to consider minutes until the first mechanical clocks that displayed minutes appeared near the end of the 16th century. Mechanical clocks kept the mean time, as opposed to the apparent time displayed by sundials. 
By that time, sexagesimal divisions of time were well established in Europe.

The earliest clocks to display seconds appeared during the last half of the 16th century. The second became accurately measurable with the development of mechanical clocks. The earliest spring-driven timepiece with a second hand which marked seconds is an unsigned clock depicting Orpheus in the Fremersdorf collection, dated between 1560 and During the 3rd quarter of the 16th century, Taqi al-Din built a clock with marks every 1/5 minute.
In 1579, Jost Bürgi built a clock for William of Hesse that marked seconds. In 1581, Tycho Brahe redesigned clocks that had displayed only minutes at his observatory so they also displayed seconds, even though those seconds were not accurate. In 1587, Tycho complained that his four clocks disagreed by plus or minus four seconds.

In 1656, Dutch scientist Christiaan Huygens invented the first pendulum clock. It had a pendulum length of just under a meter which gave it a swing of one second, and an escapement that ticked every second. It was the first clock that could accurately keep time in seconds. By the 1730s, 80 years later, John Harrison's maritime chronometers could keep time accurate to within one second in 100 days.

In 1832, Gauss proposed using the second as the base unit of time in his millimeter-milligram-second system of units. The British Association for the Advancement of Science (BAAS) in 1862 stated that "All men of science are agreed to use the second of mean solar time as the unit of time." BAAS formally proposed the CGS system in 1874, although this system was gradually replaced over the next 70 years by MKS units. Both the CGS and MKS systems used the same second as their base unit of time. MKS was adopted internationally during the 1940s, defining the second as of a mean solar day.

Some time in the late 1940s, quartz crystal oscillator clocks with an operating frequency of ~100 kHz advanced to keep time with accuracy better than 1 part in 10 over an operating period of a day. It became apparent that a consensus of such clocks kept better time than the rotation of the Earth. Metrologists also knew that Earth's orbit around the Sun (a year) was much more stable than earth's rotation. This led to proposals as early as 1950 to define the second as a fraction of a year.

The Earth's motion was described in Newcomb's "Tables of the Sun" (1895), which provided a formula for estimating the motion of the Sun relative to the epoch 1900 based on astronomical observations made between 1750 and 1892. This resulted in adoption of an ephemeris time scale expressed in units of the sidereal year at that epoch by the IAU in 1952. This extrapolated timescale brings the observed positions of the celestial bodies into accord with Newtonian dynamical theories of their motion. In 1955, the tropical year, considered more fundamental than the sidereal year, was chosen by the IAU as the unit of time. The tropical year in the definition was not measured but calculated from a formula describing a mean tropical year that decreased linearly over time.

In 1956, the second was redefined in terms of a year relative to that epoch. The second was thus defined as "the fraction of the tropical year for 1900 January 0 at 12 hours ephemeris time". This definition was adopted as part of the International System of Units in 1960.

But even the best mechanical, electric motorized and quartz crystal-based clocks develop discrepancies, and virtually none are good enough to realize an ephemeris second. Far better for timekeeping is the natural and exact "vibration" in an energized atom. The frequency of vibration (i.e., radiation) is very specific depending on the type of atom and how it is excited. Since 1967, the second has been defined as exactly "the duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium-133 atom" (at a temperature of 0 K). This length of a second was selected to correspond exactly to the length of the ephemeris second previously defined. Atomic clocks use such a frequency to measure seconds by counting cycles per second at that frequency. Radiation of this kind is one of the most stable and reproducible phenomena of nature. The current generation of atomic clocks is accurate to within one second in a few hundred million years.

Atomic clocks now set the length of a second and the time standard for the world.

SI prefixes are commonly used for times shorter than one second, but rarely for multiples of a second. Instead, certain non-SI units are permitted for use in SI: minutes, hours, days, and in astronomy Julian years.




</doc>
<doc id="26874" url="https://en.wikipedia.org/wiki?curid=26874" title="Metric prefix">
Metric prefix

A metric prefix is a unit prefix that precedes a basic unit of measure to indicate a multiple or submultiple of the unit. All metric prefixes used today are decadic. Each prefix has a unique symbol that is prepended to any unit symbol. The prefix "kilo-", for example, may be added to "gram" to indicate "multiplication" by one thousand: one kilogram is equal to one thousand grams. The prefix "milli-", likewise, may be added to "metre" to indicate "division" by one thousand; one millimetre is equal to one thousandth of a metre.

Decimal multiplicative prefixes have been a feature of all forms of the metric system, with six of these dating back to the system's introduction in the 1790s. Metric prefixes have also been used with some non-metric units. The SI prefixes are metric prefixes that were standardized for use in the International System of Units (SI) by the International Bureau of Weights and Measures (BIPM) in resolutions dating from 1960 to 1991. Since 2009, they have formed part of the International System of Quantities. They are also used in the Unified Code for Units of Measure (UCUM)

The BIPM specifies twenty prefixes for the International System of Units (SI).

Each prefix name has a symbol that is used in combination with the symbols for units of measure. For example, the symbol for "kilo-" is k, and is used to produce km, kg, and kW, which are the SI symbols for kilometre, kilogram, and kilowatt, respectively. Except for the early prefixes of "kilo-", "hecto-", and "deca-", the symbols for the multiplicative prefixes are uppercase letters, and those for the fractional prefixes are lowercase letters. There is a Unicode symbol for MICRO "µ" for use if the Greek letter "μ" is unavailable. When both are unavailable, the visually similar lowercase Latin letter "u" is commonly used instead. SI unit symbols are never italicised.

Prefixes corresponding to an integer power of one thousand are generally preferred. Hence 100 m is preferred over 1 hm (hectometre) or 10 dam (decametres). The prefixes "deci-", and "centi-", and less frequently "hecto-" and "deca-", are commonly used for everyday purposes, and the centimetre (cm) is especially common. Some modern building codes require that the millimetre be used in preference to the centimetre, because "use of centimetres leads to extensive usage of decimal points and confusion".

Prefixes may not be used in combination. This also applies to mass, for which the SI base unit (kilogram) already contains a prefix. For example, milligram (mg) is used instead of microkilogram (μkg).

In the arithmetic of measurements having units, the units are treated as multiplicative factors to values. If they have prefixes, all but one of the prefixes must be expanded to their numeric multiplier, except when combining values with identical units. Hence:

When powers of units occur, for example, squared or cubed, the multiplicative prefix must be considered part of the unit, and thus included in the exponentiation:


The use of prefixes can be traced back to the introduction of the metric system in the 1790s, long before the 1960 introduction of the SI. The prefixes, including those introduced after 1960, are used with any metric unit, whether officially included in the SI or not (e.g., millidynes and milligauss). Metric prefixes may also be used with non-metric units.

The choice of prefixes with a given unit is usually dictated by convenience of use. Unit prefixes for amounts that are much larger or smaller than those actually encountered are seldom used.

The units kilogram, gram, milligram, microgram, and smaller are commonly used for measurement of mass. However, megagram, gigagram, and larger are rarely used; tonnes (and kilotonnes, megatonnes, etc.) or scientific notation are used instead. Megagram and teragram are occasionally used to disambiguate the metric tonne from other units with the name "ton".

The kilogram is the only base unit of the International System of Units that includes a metric prefix.

The litre (equal to a cubic decimetre), millilitre (equal to a cubic centimetre), microlitre, and smaller are common. In Europe, the centilitre is often used for liquids, and the decilitre is used less frequently. Bulk agricultural products, such as grain, beer and wine, are often measured in hectolitres (each 100 litres in size).

Larger volumes are usually denoted in kilolitres, megalitres or gigalitres, or else in cubic metres (1 cubic metre = 1 kilolitre) or cubic kilometres (1 cubic kilometre = 1 teralitre). For scientific purposes, the cubic metre is usually used.

The kilometre, metre, centimetre, millimetre, and smaller are common. (However, the decimetre is rarely used.) The micrometre is often still referred to by the older non-SI term "micron". In some fields, such as chemistry, the ångström (equal to 0.1 nm) was historically used instead of the nanometre. The femtometre, used mainly in particle physics, is sometimes called a fermi. For large scales, megametre, gigametre, and larger are rarely used. Instead, ad hoc non-metric units are used, such as the solar radius, astronomical units, light years, and parsecs; the astronomical unit is mentioned in the SI standards as an accepted non-SI unit.

Prefixes for the SI standard unit second are most commonly encountered for quantities less than one second. For larger quantities, the system of minutes (60 seconds), hours (60 minutes) and days (24 hours) is accepted for use with the SI and more commonly used. When speaking of spans of time, the length of the day is usually standardized to seconds so as not to create issues with the irregular leap second.

Larger multiples of the second such as kiloseconds and megaseconds are occasionally encountered in scientific contexts, but are seldom used in common parlance. For long-scale scientific work, particularly in astronomy, the Julian year or "annum" is a standardized variant of the year, equal to exactly SI seconds (365 days, 6 hours). The unit is so named because it was the average length of a year in the Julian calendar. Long time periods are then expressed by using metric prefixes with the annum, such as megaannum or gigaannum.

The SI unit of angle is the radian, but degrees, minutes, and seconds see some scientific use.

Official policy also varies from common practice for the degree Celsius (°C). NIST states: "Prefix symbols may be used with the unit symbol °C and prefix names may be used with the unit name "degree Celsius". For example, 12 m°C (12 millidegrees Celsius) is acceptable." In practice, it is more common for prefixes to be used with the kelvin when it is desirable to denote extremely large or small absolute temperatures or temperature differences. Thus, temperatures of star interiors may be given in units of MK (megakelvins), and molecular cooling may be described in mK (millikelvins).

In use the joule and kilojoule are common, with larger multiples seen in limited contexts. In addition, the kilowatt hour, a composite unit formed from the kilowatt and hour, is often used for electrical energy; other multiples can be formed by modifying the prefix of watt (e.g. terawatt hour).

There exist a number of definitions for the non-SI unit, the calorie. There are gram calories and kilogram calories. One kilogram calorie, which equals one thousand gram calories, often appears capitalized and without a prefix (i.e. "Cal") when referring to "dietary calories" in food. It is common to apply metric prefixes to the gram calorie, but not to the kilogram calorie: thus, 1 kcal = 1000 cal = 1 Cal.

Metric prefixes are widely used outside the metric SI system. Common examples include the megabyte and the decibel. Metric prefixes rarely appear with imperial or US units except in some special cases (e.g., microinch, kilofoot, kilopound). They are also used with other specialized units used in particular fields (e.g., megaelectronvolt, gigaparsec, millibarn). They are also occasionally used with currency units (e.g., gigadollar), mainly by people who are familiar with the prefixes from scientific usage. In astronomy, geology, and paleontology, the year, with symbol a (from the Latin "annus"), is commonly used with metric prefixes: ka, Ma, and Ga.

Official policies about the use of SI prefixes with non-SI units vary slightly between the International Bureau of Weights and Measures (BIPM) and the American National Institute of Standards and Technology (NIST). For instance, the NIST advises that 'to avoid confusion, prefix symbols (and prefix names) are not used with the time-related unit symbols (names) min (minute), h (hour), d (day); nor with the angle-related symbols (names) ° (degree), ′ (minute), and ″ (second), whereas the BIPM adds information about the use of prefixes with the symbol "as" for arcsecond when they state: "However astronomers use milliarcsecond, which they denote mas, and microarcsecond, μas, which they use as units for measuring very small angles."

An advantage of the SI system decimal prefixes is that they make for simplicity of calculation and conversion involving units of different sizes; consider for example the simplicity of buying 13 items of 390 g weight at €12.34 per kilogram, compared with items of  oz at $4.79 per pound (or, worse, with old non-metric currency: £4/15/9½). In the units used in the US, combining of units that are not decimal multiples of each other is often avoided by not mixing the units used, e.g., using inches, feet or miles only: 89  inches rather than 7 feet 5 inches (or 2 yards, 1 foot 5 inches).

When a metric prefix is affixed to a root word, the prefix carries the stress, while the root drops its stress but retains a full vowel in the syllable that is stressed when the root word stands alone. For example, "kilobyte" is , with stress on the first syllable. However, units in common use outside the scientific community may be stressed idiosyncratically. In English-speaking countries, "kilometre" is the most conspicuous example. It is often pronounced , with reduced vowels on both syllables of "metre". This stress is not applied to other multiples or sub-multiples of metre, or to other units prefixed with "kilo-".

The prefix "giga" is usually pronounced in English as , with hard ⟨g⟩ as in "get", but sometimes , with soft ⟨g⟩ as in "gin".

The LaTeX typesetting system features an "SIunitx" package in which the units of measurement are spelled out, for example, codice_1 formats as "3 THz".

Some of the prefixes formerly used in the metric system have fallen into disuse and were not adopted into the SI. The decimal prefix for ten thousand, "myria-" (sometimes also written as "myrio-"), and the binary prefixes "double-" (2×) and "demi-" (×) were parts of the original metric system adopted by France in 1795, but were not retained when the SI prefixes were internationally adopted by the 11th CGPM conference in 1960.

Other metric prefixes used historically include hebdo- (10) and micri- (10).

Double prefixes have been used in the past, such as "micromillimetres" or "millimicrons" (now nanometres), "micromicrofarads" (μμF; now picofarads, pF), "kilomegatons" (now gigatons), "hectokilometres" (now 100 kilometres) and the derived adjective "hectokilometric" (typically used for qualifying the fuel consumption measures). These are not compatible with the SI.

Other obsolete double prefixes included "decimilli-" (10), which was contracted to "dimi-" and standardized in France up to 1961.

In written English, the symbol "K" is often used informally to indicate a multiple of thousand in many contexts. For example, one may talk of a "40K salary" (), or call the Year 2000 problem the "Y2K problem". In these cases, an uppercase K is often used with an implied unit (although it could then be confused with the symbol for the kelvin temperature unit if the context is unclear). This informal postfix is read or spoken as "thousand" or "grand", or just "k".

The financial and general news media mostly use m or M, b or B, and t or T as abbreviations for million, billion (10) and trillion (10), respectively, for large quantities, typically currency and population.

The medical and automotive fields in the United States use the abbreviations "cc" or "ccm" for cubic centimetres. 1 cubic centimetre is equivalent to 1 millilitre.

For nearly a century, engineers used the abbreviation "MCM" to designate a "thousand circular mils" in specifying the cross-sectional area of large electrical cables. Since the mid-1990s, "kcmil" has been adopted as the official designation of a thousand circular mils, but the designation "MCM" still remains in wide use. A similar system is used in natural gas sales in the United States: "m" (or "M") for thousands and "mm" (or "MM") for millions of British thermal units or therms, and in the oil industry, where "MMbbl" is the symbol for "millions of barrels". This usage of the capital letter "M" for "thousand" is from Roman numerals, in which "M" means 1000.

In some fields of information technology, it has been common to designate non-decimal multiples based on powers of 1024, rather than 1000, for some SI prefixes ("kilo-", "mega-", "giga-"), contrary to the definitions in the International System of Units (SI). This practice was once sanctioned by some industry associations, including JEDEC. The International Electrotechnical Commission (IEC) standardized the system of binary prefixes ("kibi-", "mebi-", "gibi-", etc.) for this purpose.



</doc>
<doc id="26876" url="https://en.wikipedia.org/wiki?curid=26876" title="SI derived unit">
SI derived unit

SI derived units are units of measurement derived from the
seven base units specified by the International System of Units (SI). They are either dimensionless or can be expressed as a product of one or more of the base units, possibly scaled by an appropriate power of exponentiation. 

The SI has special names for 22 of these derived units (for example, hertz, the SI unit of measurement of frequency), but the rest merely reflect their derivation: for example, the square metre (m), the SI derived unit of area; and the kilogram per cubic metre (kg/m or kg⋅m), the SI derived unit of density. 

The names of SI derived units, when written in full, are always in lowercase. However, the symbols for units named after persons are written with an uppercase initial letter. For example, the symbol for hertz is "Hz", but the symbol for metre is "m".

The International System of Units assigns special names to 22 derived units, which includes two dimensionless derived units, the radian (rad) and the steradian (sr). 
Some other units such as the hour, litre, tonne, bar and electronvolt are not SI units, but are widely used in conjunction with SI units.

Until 1995, the SI classified the radian and the steradian as "supplementary units", but this designation was abandoned and the units were grouped as derived units.



</doc>
<doc id="26882" url="https://en.wikipedia.org/wiki?curid=26882" title="Split (poker)">
Split (poker)

In poker it is sometimes necessary to split, or divide the pot among two or more players rather than awarding it all to a single player. This can happen because of ties, and also by playing intentional split-pot poker variants (the most typical of these is high-low split poker, where the high hand and low hand split the pot).

To split a pot, one player uses both hands to take the chips from the pot and make stacks, placing them side by side to compare height (and therefore value). Equal stacks are placed aside. If there is more than one denomination of chip in the pot, the largest value chip is done first, and then progressively smaller value chips. If there is an odd number of larger chips, smaller chips from the pot can be used to equalize stacks or make change as necessary. Pots are always split down to the lowest denomination of chip used in the game. Three-way ties or further splits can also be done this way.

After fully dividing a pot, there may be a single odd lowest-denomination chip remaining (or two odd chips if splitting three ways, etc.). Odd chips can be awarded in several ways, agreed upon before the beginning of the game. The following rules are common:


Sometimes it is necessary to further split a half pot into quarters, or even smaller portions. This is especially common in community card high-low split games such as Omaha hold'em, where one player has the high hand and two or more players have tied low hands. Unfortunate players receiving such a fractional pot call it being "quartered". When this happens, an exception to the odd chip rules above can be made: if the high hand wins its half of the pot alone, and the low half is going to be quartered, the odd chip (if any) from the first split should be placed in the low half, rather than being awarded to the high hand.


</doc>
<doc id="26884" url="https://en.wikipedia.org/wiki?curid=26884" title="Superconductivity">
Superconductivity

Superconductivity is a set of physical properties observed in certain materials where electrical resistance vanishes and magnetic flux fields are expelled from the material. Any material exhibiting these properties is a superconductor. Unlike an ordinary metallic conductor, whose resistance decreases gradually as its temperature is lowered even down to near absolute zero, a superconductor has a characteristic critical temperature below which the resistance drops abruptly to zero. An electric current through a loop of superconducting wire can persist indefinitely with no power source.

The superconductivity phenomenon was discovered in 1911 by Dutch physicist Heike Kamerlingh Onnes. Like ferromagnetism and atomic spectral lines, superconductivity is a phenomenon which can only be explained by quantum mechanics. It is characterized by the Meissner effect, the complete ejection of magnetic field lines from the interior of the superconductor during its transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of "perfect conductivity" in classical physics.

In 1986, it was discovered that some cuprate-perovskite ceramic materials have a critical temperature above . Such a high transition temperature is theoretically impossible for a conventional superconductor, leading the materials to be termed high-temperature superconductors. The cheaply available coolant liquid nitrogen boils at 77 K, and thus the existence of superconductivity at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.

There are many criteria by which superconductors are classified. The most common are:

A superconductor can be "Type I", meaning it has a single critical field, above which all superconductivity is lost and below which the magnetic field is completely expelled from the superconductor; or "Type II", meaning it has two critical fields, between which it allows partial penetration of the magnetic field through isolated points. These points are called vortices. Furthermore, in multicomponent superconductors it is possible to have a combination of the two behaviours. In that case the superconductor is of Type-1.5.

It is "conventional" if it can be explained by the BCS theory or its derivatives, or "unconventional", otherwise.

A superconductor is generally considered "high-temperature" if it reaches a superconducting state above a temperature of 30 K (−243.15 °C); as in the initial discovery by Georg Bednorz and K. Alex Müller. It may also reference materials that transition to superconductivity when cooled using liquid nitrogen – that is, at only "T" > 77 K, although this is generally used only to emphasize that liquid nitrogen coolant is sufficient. Low temperature superconductors refer to materials with a critical temperature below 30 K. One exception to this rule is the iron pnictide group of superconductors which display behaviour and properties typical of high-temperature superconductors, yet some of the group have critical temperatures below 30 K.

Superconductor material classes include chemical elements (e.g. mercury or lead), alloys (such as niobium–titanium, germanium–niobium, and niobium nitride), ceramics (YBCO and magnesium diboride), superconducting pnictides (like fluorine-doped LaOFeAs) or organic superconductors (fullerenes and carbon nanotubes; though perhaps these examples should be included among the chemical elements, as they are composed entirely of carbon).

Most of the physical properties of superconductors vary from material to material, such as the heat capacity and the critical temperature, critical field, and critical current density at which superconductivity is destroyed.

On the other hand, there is a class of properties that are independent of the underlying material. For instance, all superconductors have "exactly" zero resistivity to low applied currents when there is no magnetic field present or if the applied field does not exceed a critical value. The existence of these "universal" properties implies that superconductivity is a thermodynamic phase, and thus possesses certain distinguishing properties which are largely independent of microscopic details.

The simplest method to measure the electrical resistance of a sample of some material is to place it in an electrical circuit in series with a current source "I" and measure the resulting voltage "V" across the sample. The resistance of the sample is given by Ohm's law as "R = V / I". If the voltage is zero, this means that the resistance is zero.

Superconductors are also able to maintain a current with no applied voltage whatsoever, a property exploited in superconducting electromagnets such as those found in MRI machines. Experiments have demonstrated that currents in superconducting coils can persist for years without any measurable degradation. Experimental evidence points to a current lifetime of at least 100,000 years. Theoretical estimates for the lifetime of a persistent current can exceed the estimated lifetime of the universe, depending on the wire geometry and the temperature. In practice, currents injected in superconducting coils have persisted for more than 25 years (as on August 4, 2020) in superconducting gravimeters. In such instruments, the measurement principle is based on the monitoring of the levitation of a superconducting niobium sphere with a mass of 4 grams.

In a normal conductor, an electric current may be visualized as a fluid of electrons moving across a heavy ionic lattice. The electrons are constantly colliding with the ions in the lattice, and during each collision some of the energy carried by the current is absorbed by the lattice and converted into heat, which is essentially the vibrational kinetic energy of the lattice ions. As a result, the energy carried by the current is constantly being dissipated. This is the phenomenon of electrical resistance and Joule heating.

The situation is different in a superconductor. In a conventional superconductor, the electronic fluid cannot be resolved into individual electrons. Instead, it consists of bound "pairs" of electrons known as Cooper pairs. This pairing is caused by an attractive force between electrons from the exchange of phonons. Due to quantum mechanics, the energy spectrum of this Cooper pair fluid possesses an "energy gap", meaning there is a minimum amount of energy Δ"E" that must be supplied in order to excite the fluid. Therefore, if Δ"E" is larger than the thermal energy of the lattice, given by "kT", where "k" is Boltzmann's constant and "T" is the temperature, the fluid will not be scattered by the lattice. The Cooper pair fluid is thus a superfluid, meaning it can flow without energy dissipation.

In a class of superconductors known as type II superconductors, including all known high-temperature superconductors, an extremely low but nonzero resistivity appears at temperatures not too far below the nominal superconducting transition when an electric current is applied in conjunction with a strong magnetic field, which may be caused by the electric current. This is due to the motion of magnetic vortices in the electronic superfluid, which dissipates some of the energy carried by the current. If the current is sufficiently small, the vortices are stationary, and the resistivity vanishes. The resistance due to this effect is tiny compared with that of non-superconducting materials, but must be taken into account in sensitive experiments. However, as the temperature decreases far enough below the nominal superconducting transition, these vortices can become frozen into a disordered but stationary phase known as a "vortex glass". Below this vortex glass transition temperature, the resistance of the material becomes truly zero.

In superconducting materials, the characteristics of superconductivity appear when the temperature "T" is lowered below a critical temperature "T". The value of this critical temperature varies from material to material. Conventional superconductors usually have critical temperatures ranging from around 20 K to less than 1 K. Solid mercury, for example, has a critical temperature of 4.2 K. As of 2015, the highest critical temperature found for a conventional superconductor is 203K for HS, although high pressures of approximately 90 gigapascals were required. Cuprate superconductors can have much higher critical temperatures: YBaCuO, one of the first cuprate superconductors to be discovered, has a critical temperature above 90 K, and mercury-based cuprates have been found with critical temperatures in excess of 130 K. The basic physical mechanism responsible for the high critical temperature is not yet clear. However, it is clear that a two-electron pairing is involved, although the nature of the pairing (formula_1 wave vs. formula_2 wave) remains controversial.

Similarly, at a fixed temperature below the critical temperature, superconducting materials cease to superconduct when an external magnetic field is applied which is greater than the "critical magnetic field". This is because the Gibbs free energy of the superconducting phase increases quadratically with the magnetic field while the free energy of the normal phase is roughly independent of the magnetic field. If the material superconducts in the absence of a field, then the superconducting phase free energy is lower than that of the normal phase and so for some finite value of the magnetic field (proportional to the square root of the difference of the free energies at zero magnetic field) the two free energies will be equal and a phase transition to the normal phase will occur. More generally, a higher temperature and a stronger magnetic field lead to a smaller fraction of electrons that are superconducting and consequently to a longer London penetration depth of external magnetic fields and currents. The penetration depth becomes infinite at the phase transition.

The onset of superconductivity is accompanied by abrupt changes in various physical properties, which is the hallmark of a phase transition. For example, the electronic heat capacity is proportional to the temperature in the normal (non-superconducting) regime. At the superconducting transition, it suffers a discontinuous jump and thereafter ceases to be linear. At low temperatures, it varies instead as "e" for some constant, α. This exponential behavior is one of the pieces of evidence for the existence of the energy gap.

The order of the superconducting phase transition was long a matter of debate. Experiments indicate that the transition is second-order, meaning there is no latent heat. However, in the presence of an external magnetic field there is latent heat, because the superconducting phase has a lower entropy below the critical temperature than the normal phase. It has been experimentally demonstrated that, as a consequence, when the magnetic field is increased beyond the critical field, the resulting phase transition leads to a decrease in the temperature of the superconducting material.

Calculations in the 1970s suggested that it may actually be weakly first-order due to the effect of long-range fluctuations in the electromagnetic field. In the 1980s it was shown theoretically with the help of a disorder field theory, in which the vortex lines of the superconductor play a major role, that the transition is of second order within the type II regime and of first order (i.e., latent heat) within the type I regime, and that the two regions are separated by a tricritical point. The results were strongly supported by Monte Carlo computer simulations.

When a superconductor is placed in a weak external magnetic field H, and cooled below its transition temperature, the magnetic field is ejected. The Meissner effect does not cause the field to be completely ejected but instead the field penetrates the superconductor but only to a very small distance, characterized by a parameter "λ", called the London penetration depth, decaying exponentially to zero within the bulk of the material. The Meissner effect is a defining characteristic of superconductivity. For most superconductors, the London penetration depth is on the order of 100 nm.

The Meissner effect is sometimes confused with the kind of diamagnetism one would expect in a perfect electrical conductor: according to Lenz's law, when a "changing" magnetic field is applied to a conductor, it will induce an electric current in the conductor that creates an opposing magnetic field. In a perfect conductor, an arbitrarily large current can be induced, and the resulting magnetic field exactly cancels the applied field.

The Meissner effect is distinct from this—it is the spontaneous expulsion which occurs during transition to superconductivity. Suppose we have a material in its normal state, containing a constant internal magnetic field. When the material is cooled below the critical temperature, we would observe the abrupt expulsion of the internal magnetic field, which we would not expect based on Lenz's law.

The Meissner effect was given a phenomenological explanation by the brothers Fritz and Heinz London, who showed that the electromagnetic free energy in a superconductor is minimized provided

where H is the magnetic field and λ is the London penetration depth.

This equation, which is known as the London equation, predicts that the magnetic field in a superconductor decays exponentially from whatever value it possesses at the surface.

A superconductor with little or no magnetic field within it is said to be in the Meissner state. The Meissner state breaks down when the applied magnetic field is too large. Superconductors can be divided into two classes according to how this breakdown occurs. In Type I superconductors, superconductivity is abruptly destroyed when the strength of the applied field rises above a critical value "H". Depending on the geometry of the sample, one may obtain an intermediate state consisting of a baroque pattern of regions of normal material carrying a magnetic field mixed with regions of superconducting material containing no field. In Type II superconductors, raising the applied field past a critical value "H" leads to a mixed state (also known as the vortex state) in which an increasing amount of magnetic flux penetrates the material, but there remains no resistance to the flow of electric current as long as the current is not too large. At a second critical field strength "H", superconductivity is destroyed. The mixed state is actually caused by vortices in the electronic superfluid, sometimes called fluxons because the flux carried by these vortices is quantized. Most pure elemental superconductors, except niobium and carbon nanotubes, are Type I, while almost all impure and compound superconductors are Type II.

Conversely, a spinning superconductor generates a magnetic field, precisely aligned with the spin axis. The effect, the London moment, was put to good use in Gravity Probe B. This experiment measured the magnetic fields of four superconducting gyroscopes to determine their spin axes. This was critical to the experiment since it is one of the few ways to accurately determine the spin axis of an otherwise featureless sphere.

Superconductivity was discovered on April 8, 1911 by Heike Kamerlingh Onnes, who was studying the resistance of solid mercury at cryogenic temperatures using the recently produced liquid helium as a refrigerant. At the temperature of 4.2 K, he observed that the resistance abruptly disappeared. In the same experiment, he also observed the superfluid transition of helium at 2.2 K, without recognizing its significance. The precise date and circumstances of the discovery were only reconstructed a century later, when Onnes's notebook was found. In subsequent decades, superconductivity was observed in several other materials. In 1913, lead was found to superconduct at 7 K, and in 1941 niobium nitride was found to superconduct at 16 K.

Great efforts have been devoted to finding out how and why superconductivity works; the important step occurred in 1933, when Meissner and Ochsenfeld discovered that superconductors expelled applied magnetic fields, a phenomenon which has come to be known as the Meissner effect. In 1935, Fritz and Heinz London showed that the Meissner effect was a consequence of the minimization of the electromagnetic free energy carried by superconducting current.

The theoretical model that was first conceived in human history for superconductivity was completely classical: it is summarized by London constitutive equations. 
It was put forward by the brothers Fritz and Heinz London in 1935, shortly after the discovery that magnetic fields are expelled from superconductors. A major triumph of the equations of this theory is their ability to explain the Meissner effect, wherein a material exponentially expels all internal magnetic fields as it crosses the superconducting threshold. By using the London equation, one can obtain the dependence of the magnetic field inside the superconductor on the distance to the surface.

The two constitutive equations for a superconductor by London are:
The first equation follows from Newton's second law for superconducting electrons.

During the 1950s, theoretical condensed matter physicists arrived at an understanding of "conventional" superconductivity, through a pair of remarkable and important theories: the phenomenological Ginzburg–Landau theory (1950) and the microscopic BCS theory (1957).

In 1950, the phenomenological Ginzburg–Landau theory of superconductivity was devised by Landau and Ginzburg. This theory, which combined Landau's theory of second-order phase transitions with a Schrödinger-like wave equation, had great success in explaining the macroscopic properties of superconductors. In particular, Abrikosov showed that Ginzburg–Landau theory predicts the division of superconductors into the two categories now referred to as Type I and Type II. Abrikosov and Ginzburg were awarded the 2003 Nobel Prize for their work (Landau had received the 1962 Nobel Prize for other work, and died in 1968). The four-dimensional extension of the Ginzburg–Landau theory, the Coleman-Weinberg model, is important in quantum field theory and cosmology.

Also in 1950, Maxwell and Reynolds "et al." found that the critical temperature of a superconductor depends on the isotopic mass of the constituent element. This important discovery pointed to the electron-phonon interaction as the microscopic mechanism responsible for superconductivity.

The complete microscopic theory of superconductivity was finally proposed in 1957 by Bardeen, Cooper and Schrieffer. This BCS theory explained the superconducting current as a superfluid of Cooper pairs, pairs of electrons interacting through the exchange of phonons. For this work, the authors were awarded the Nobel Prize in 1972.

The BCS theory was set on a firmer footing in 1958, when N. N. Bogolyubov showed that the BCS wavefunction, which had originally been derived from a variational argument, could be obtained using a canonical transformation of the electronic Hamiltonian. In 1959, Lev Gor'kov showed that the BCS theory reduced to the Ginzburg–Landau theory close to the critical temperature.

Generalizations of BCS theory for conventional superconductors form the basis for understanding of the phenomenon of superfluidity, because they fall into the lambda transition universality class. The extent to which such generalizations can be applied to unconventional superconductors is still controversial.

The first practical application of superconductivity was developed in 1954 with Dudley Allen Buck's invention of the cryotron. Two superconductors with greatly different values of critical magnetic field are combined to produce a fast, simple switch for computer elements.

Soon after discovering superconductivity in 1911, Kamerlingh Onnes attempted to make an electromagnet with superconducting windings but found that relatively low magnetic fields destroyed superconductivity in the materials he investigated. Much later, in 1955, G. B. Yntema succeeded in constructing a small 0.7-tesla iron-core electromagnet with superconducting niobium wire windings. Then, in 1961, J. E. Kunzler, E. Buehler, F. S. L. Hsu, and J. H. Wernick made the startling discovery that, at 4.2 kelvin niobium–tin, a compound consisting of three parts niobium and one part tin, was capable of supporting a current density of more than 100,000 amperes per square centimeter in a magnetic field of 8.8 tesla. Despite being brittle and difficult to fabricate, niobium–tin has since proved extremely useful in supermagnets generating magnetic fields as high as 20 tesla. In 1962 T. G. Berlincourt and R. R. Hake discovered that more ductile alloys of niobium and titanium are suitable for applications up to 10 tesla.
Promptly thereafter, commercial production of niobium–titanium supermagnet wire commenced at Westinghouse Electric Corporation and at Wah Chang Corporation. Although niobium–titanium boasts less-impressive superconducting properties than those of niobium–tin, niobium–titanium has, nevertheless, become the most widely used "workhorse" supermagnet material, in large measure a consequence of its very high ductility and ease of fabrication. However, both niobium–tin and niobium–titanium find wide application in MRI medical imagers, bending and focusing magnets for enormous high-energy-particle accelerators, and a host of other applications. Conectus, a European superconductivity consortium, estimated that in 2014, global economic activity for which superconductivity was indispensable amounted to about five billion euros, with MRI systems accounting for about 80% of that total.

In 1962, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum "Φ" = "h"/(2"e"), where "h" is the Planck constant. Coupled with the quantum Hall resistivity, this leads to a precise measurement of the Planck constant. Josephson was awarded the Nobel Prize for this work in 1973.

In 2008, it was proposed that the same mechanism that produces superconductivity could produce a superinsulator state in some materials, with almost infinite electrical resistance.

Until 1986, physicists had believed that BCS theory forbade superconductivity at temperatures above about 30 K. In that year, Bednorz and Müller discovered superconductivity in lanthanum barium copper oxide (LBCO), a lanthanum-based cuprate perovskite material, which had a transition temperature of 35 K (Nobel Prize in Physics, 1987). It was soon found that replacing the lanthanum with yttrium (i.e., making YBCO) raised the critical temperature above 90 K.

This temperature jump is particularly significant, since it allows liquid nitrogen as a refrigerant, replacing liquid helium.
This can be important commercially because liquid nitrogen can be produced relatively cheaply, even on-site. Also, the higher temperatures help avoid some of the problems that arise at liquid helium temperatures, such as the formation of plugs of frozen air that can block cryogenic lines and cause unanticipated and potentially hazardous pressure buildup.

Many other cuprate superconductors have since been discovered, and the theory of superconductivity in these materials is one of the major outstanding challenges of theoretical condensed matter physics.
There are currently two main hypotheses – the resonating-valence-bond theory, and spin fluctuation which has the most support in the research community. The second hypothesis proposed that electron pairing in high-temperature superconductors is mediated by short-range spin waves known as paramagnons.

In 2008, holographic superconductivity, which uses holographic duality or AdS/CFT correspondence theory, was proposed by Gubser, Hartnoll, Herzog, and Horowitz, as a possible explanation of high-temperature superconductivity in certain materials.

Since about 1993, the highest-temperature superconductor has been a ceramic material consisting of mercury, barium, calcium, copper and oxygen (HgBaCaCuO) with "T" = 133–138 K. The latter experiment (138 K) still awaits experimental confirmation, however.

In February 2008, an iron-based family of high-temperature superconductors was discovered. Hideo Hosono, of the Tokyo Institute of Technology, and colleagues found lanthanum oxygen fluorine iron arsenide (LaOFFeAs), an oxypnictide that superconducts below 26 K. Replacing the lanthanum in LaOFFeAs with samarium leads to superconductors that work at 55 K.

In 2014 and 2015, hydrogen sulfide () at extremely high pressures (around 150 gigapascals) was first predicted and then confirmed to be a high-temperature superconductor with a transition temperature of 80 K. Additionally, in 2019 it was discovered that lanthanum hydride () becomes a superconductor at 250 K under a pressure of 170 gigapascals. This is currently the highest temperature at which any material has shown superconductivity.

In 2018, a research team from the Department of Physics, Massachusetts Institute of Technology, discovered superconductivity in bilayer graphene with one layer twisted at an angle of approximately 1.1 degrees with cooling and applying a small electric charge. Even if the experiments were not carried out in a high-temperature environment, the results are correlated less to classical but high temperature superconductors, given that no foreign atoms need to be introduced.

Superconducting magnets are some of the most powerful electromagnets known. They are used in MRI/NMR machines, mass spectrometers, the beam-steering magnets used in particle accelerators and plasma confining magnets in some tokamaks. They can also be used for magnetic separation, where weakly magnetic particles are extracted from a background of less or non-magnetic particles, as in the pigment industries. They can also be used in large wind turbines to overcome the restrictions imposed by high electrical currents, with an industrial grade 3.6 megawatt superconducting windmill generator having been tested successfully in Denmark.

In the 1950s and 1960s, superconductors were used to build experimental digital computers using cryotron switches. More recently, superconductors have been used to make digital circuits based on rapid single flux quantum technology and RF and microwave filters for mobile phone base stations.

Superconductors are used to build Josephson junctions which are the building blocks of SQUIDs (superconducting quantum interference devices), the most sensitive magnetometers known. SQUIDs are used in scanning SQUID microscopes and magnetoencephalography. Series of Josephson devices are used to realize the SI volt. Depending on the particular mode of operation, a superconductor–insulator–superconductor Josephson junction can be used as a photon detector or as a mixer. The large resistance change at the transition from the normal- to the superconducting state is used to build thermometers in cryogenic micro-calorimeter photon detectors. The same effect is used in ultrasensitive bolometers made from superconducting materials.

Other early markets are arising where the relative efficiency, size and weight advantages of devices based on high-temperature superconductivity outweigh the additional costs involved. For example, in wind turbines the lower weight and volume of superconducting generators could lead to savings in construction and tower costs, offsetting the higher costs for the generator and lowering the total levelized cost of electricity (LCOE).

Promising future applications include high-performance smart grid, electric power transmission, transformers, power storage devices, electric motors (e.g. for vehicle propulsion, as in vactrains or maglev trains), magnetic levitation devices, fault current limiters, enhancing spintronic devices with superconducting materials, and superconducting magnetic refrigeration. However, superconductivity is sensitive to moving magnetic fields, so applications that use alternating current (e.g. transformers) will be more difficult to develop than those that rely upon direct current. Compared to traditional power lines, superconducting transmission lines are more efficient and require only a fraction of the space, which would not only lead to a better environmental performance but could also improve public acceptance for expansion of the electric grid.






</doc>
<doc id="26886" url="https://en.wikipedia.org/wiki?curid=26886" title="Siam (disambiguation)">
Siam (disambiguation)

Siam is the former name of Thailand, and is used to refer to the historical region of Central Thailand, usually including Southern Thailand.

Siam or SIAM may also refer to:







</doc>
<doc id="26889" url="https://en.wikipedia.org/wiki?curid=26889" title="Geography of Sweden">
Geography of Sweden

Sweden is a country in Northern Europe on the Scandinavian Peninsula. It borders Norway to the west; Finland to the northeast; and the Baltic Sea and Gulf of Bothnia to the south and east. At , Sweden is the largest country in Northern Europe, the fifth largest in Europe, and the 55th largest country in the world.

Sweden has a long coastline on its east, and the Scandinavian mountain chain (Scanderna) on its western border, separating it from Norway. It has maritime borders with Denmark, Germany, Poland, Russia, Lithuania, Latvia and Estonia, and it is also linked to Denmark (southwest) by the Öresund bridge. It has an Exclusive Economic Zone of .

Much of Sweden is heavily forested, with 69% of the country being forest and woodland, while farmland constitutes only 8% of land use. Sweden consists of 39,960 km of water area, constituting around 95,700 lakes. The lakes are sometimes used for water power plants, especially the large northern rivers and lakes.

Most of northern and western central Sweden consists of vast tracts of hilly and mountainous land called the Norrland terrain. From the south the transition to the Norrland terrain is not only seen in the relief but also in the wide and contiguous boreal forests that extend north of it with till and peat being the overwhelmingly most common soil types.

South of the Norrland terrain lies the Central Swedish lowland which forms a broad east-west trending belt from Gothenburg to Stockholm. This is the traditional heartland of Sweden due to its large population and agricultural resources. The region forms a belt of fertile soils suitable for agriculture that interrupts the forested and till-coated lands to the north and south. Before the expansion of agriculture, these fertile soils were covered by a broad-leaved tree forest where maples, oaks, ashes, small-leaved lime and common hazel grew. The Central Swedish lowland does however also contain soils of poor quality, particularly in hills where Scots pine and Norway spruce grow on top of thin till soils. Agriculture aside, the region benefits also from the proximity of hydropower, forest and bergslagen's mineral resources. Sweden's four largest lakes, Vänern, Vättern, Mälaren and Hjälmaren, lie within the lowlands.

To the south of the Central Swedish lowland lies the South Swedish highlands which except for a lack of deep valleys is similar to the Norrland terrain found further north in Sweden. The highest point of the highlands lies at 377 m. Poor soil conditions have posed significant difficulties for agriculture in the highlands, meaning that over time small industries became relatively important in local economies. 

Southernmost Sweden contains a varied landscape with both plains and hilly terrain. A characteristic chain of elongated hills runs across Scania from northwest to southeast. These hills are horsts located along the Tornquist Zone. Some of the horst are Hallandsåsen, Römelåsen and Söderåsen. The plains of Scania and Halland make up 10% of Sweden's cultivated lands and are the country's main agricultural landscape. Productivity is high relative to the rest of Sweden and more akin to that of more southern European countries. The natural vegetation is made up of broadleaf forest although conifer plantations are common. Southern Sweden has Sweden's greatest animal and plant diversity.

The two largest islands are Gotland and Öland in the southeast. They differ from the rest of Sweden by being made up of limestone and marl with an alvar vegetation adapted to the island's calcareous soils. Gotland and Öland have landforms that are rare or absent in mainland Sweden. These include active cliffs seen in segments of their western coasts, sea stacks called "rauks" and large cave systems. 

Sweden has 25 provinces or "landskap" ("landscapes"), based on culture, geography and history: Bohuslän, Blekinge, Dalarna, Dalsland, Gotland, Gästrikland, Halland, Hälsingland, Härjedalen, Jämtland, Lapland, Medelpad, Norrbotten, Närke, Skåne, Småland, Södermanland, Uppland, Värmland, Västmanland, Västerbotten, Västergötland, Ångermanland, Öland and Östergötland.

While these provinces serve no political or administrative purpose, they play an important role for people's self-identification. The provinces are usually grouped together in three large lands ("landsdelar"): the northern Norrland, the central Svealand and southern Götaland. The sparsely populated Norrland encompasses almost 60% of the country.

Administratively, Sweden is divided into 21 counties, or "län". In each county there is a County Administrative Board, or "länsstyrelse", which is appointed by the national government.

In each county there is also a separate County Council, or "landsting", which is the municipal representation appointed by the county electorate.


The letters shown were on the vehicle registration plates until 1973.

Each county is further divided into municipalities or "kommuner", ranging from only one (in Gotland County) to forty-nine (in Västra Götaland County). The total number of municipalities is 290.

The northern municipalities are often large in size, but have small populations – the largest municipality is Kiruna with an area as large as the three southern provinces in Sweden (Scania, Blekinge and Halland) combined, but it only has a population of 25,000, and its density is about 1 / km.

Sweden has a population of 10 million as of January 2017. The mountainous north is considerably less populated than the southern and central regions, partly because the summer period lasts longer in the south, and this is where the more successful agricultural industries were originally established. Another historical reason is said to be the desired proximity to key trade routes and partners in continental Europe, e.g. Germany. As a result, all seven urban areas in Sweden with a population of 100,000 or more, are located in the southern half of the country.

Cities and towns in Sweden are neither political nor administrative entities; rather they are localities or urban areas, independent of municipal subdivisions.
The largest city, in terms of population, is the capital Stockholm, in the east, the dominant city for culture and media, with a population of 1,250,000. The second largest city is Gothenburg, with 510,500, in the west. The third largest is Malmö in the south, with 258,000. The largest city in the north is Umeå with 76,000 inhabitants.

Sweden's natural resources include copper, gold, hydropower, iron ore, lead, silver, timber, uranium, and zinc.

Acid rain has become an issue because it is damaging soils and lakes and polluting the North Sea and the Baltic Sea. The HBV hydrology transport model has been used to analyze nutrient discharge to the Baltic from tributary watersheds.

The extreme points of Sweden include the coordinates that are farthest north, south, east and west in Sweden, and the ones that are at the highest and the lowest elevations in the country. Unlike Norway and Denmark, Sweden has no external territories that can be considered either inside or outside the country depending on definition, meaning that the extreme points of Sweden are unambiguous.

The latitude and longitude are expressed in , in which a positive latitude value refers to the Northern Hemisphere, and a negative value refers to the Southern Hemisphere. Additionally, a negative elevation value refers to land below sea level. The coordinates used in this article are sourced from Google Earth, which makes use of the World Geodetic System (WGS) 84, a geodetic reference system.

Sweden's northernmost point is Treriksröset, in the Lapland province, where the borders of Sweden, Norway, and Finland meet. The closest Swedish city to the area is Kiruna, which is Sweden's northernmost city. Sweden's southernmost point is in the harbour of the fishing village Smygehuk, near the city of Trelleborg, which borders the Baltic Sea. At the pier of the harbour, a signpost displays the exact position of the point, as well as the distance to Treriksröset, Stockholm, Berlin, Paris, and Moscow.

Sweden's westernmost point is on Stora Drammen, an islet in Skagerrak outside the coast of Bohuslän. Seabirds and harbor seals have colonies on the islet, but it is uninhabited by humans. Sweden's easternmost point is on Kataja, an islet south of Haparanda in the Bothnian Bay. The islet is divided between Sweden and Finland. The border was established in 1809, after the Finnish War, between what was previously two islets, a Swedish one called Kataja and a smaller Finnish one called Inakari. Since 1809, post-glacial rebound has caused the sea level in the region to drop relative to land level, joining the two islets. If counting the mainland only, Stensvik in Strömstad is Sweden's westernmost point, and Sundholmen in Haparanda is the easternmost point.

The highest point in Sweden is Kebnekaise, which stands at (August 2018). It is in the Scandinavian Mountains chain, in the province of Lapland. The mountain has two peaks, of which the glaciated southern one is the highest at . The northern peak, which stands at , is free of ice. Although the south top is traditionally said to be high, new measurements have shown that the glacier has shrunk fairly fast; therefore the summit is not as high as earlier. It was in 2008. Other points of comparable height in the vicinity of Kebnekaise include Sarektjåkka at , and Kaskasatjåkka at . If the summers of 2016 and 2017 get as warm as the previous years, the northern peak will become the highest.

Sweden's lowest point, which is below sea level, is in the Kristianstads Vattenrike Biosphere Reserve in the city of Kristianstad. The point is at the bottom of what was once Nosabyviken, a bay on the lake of Hammarsjön. The bay was drained in the 1860s by John Nun Milner, an engineer, to get more arable land for Kristianstad.
Only public transportation.




</doc>
<doc id="26890" url="https://en.wikipedia.org/wiki?curid=26890" title="Demographics of Sweden">
Demographics of Sweden

The total resident population of Sweden was 10,343,403 in March 2020. The population exceeded 10 million for the first time on Friday 20 January 2017. The three largest cities are Stockholm, Gothenburg and Malmö. Sweden's population has become much more ethnically, religiously and linguistically diverse over the past 70 years as a result of global immigration. Every fourth (24.9%) resident in the country has immigrant background and every third (32.3%) has at least one parent born abroad.

Demographic statistics according to the World Population Review.


Demographic statistics according to the CIA World Factbook, unless otherwise indicated.

















The demography of Sweden is monitored by Statistics Sweden (SCB).

The 2005 Swedish census showed an increase of 475,322 compared to the 1990 census, an average increase of 31,680 annually. During the 1990s, birth rate increased by more than 100,000 children per year while death rates fell and immigration surged. In the early 2000s, birth rate declined as immigration increased further, with the context of unrest in the Middle East, upholding steady population growth.

In 1950 Sweden had fewer people aged 10–20 with more people ages 20–30 and 0–10. In 2017 the ratio of male to female remains steady at about 50–50. As a whole, the graph broadens with people appearing to live longer. In 2050 it is predicted that all ages will increase from below 300,000 males and females to above 300,000 males and females. With about 50,000 people living to the ages of 90–100. In 2100 the graph is shaped as a rectangle with people of all ages and genders remaining steady. It narrows slightly at the top of the graph with about 250,000/300,000 males and females living to be 90–100 years old.
Statistics Sweden projects the following population development in Sweden:
Eurostat projects a population in Sweden reaching 11,994,364 people in 2040 and 14,388,478 in 2080.

The population density is just over 25 people per km² (65 per square mile), with 1 437 persons per km² in localities (continuous settlement with at least 200 inhabitants). 87% of the population live in urban areas, which cover 1.5% of the entire land area. 63% of Swedes are in large urban areas. The population density is substantially higher in the south than in the north. The capital city Stockholm has a municipal population of about 950,000 (with 1.5 million in the urban area and 2.3 million in the metropolitan area). The second- and third-largest cities are Gothenburg and Malmö. Greater Gothenburg counts just over a million inhabitants and the same goes for the western part of Scania, along the Öresund. The Öresund Region, the Danish-Swedish cross-border region around the Öresund that Malmö is part of, has a population of 4 million. Outside of major cities, areas with notably higher population density include the agricultural part of Östergötland, the western coast, the area around Lake Mälaren and the agricultural area around Uppsala.

Norrland, which covers approximately 60% of the Swedish territory, has a very low population density (below 5 people per square kilometer). The mountains and most of the remote coastal areas are almost unpopulated. Low population density exists also in large parts of western Svealand, as well as southern and central Småland. An area known as "Finnveden", which is located in the south-west of Småland, and mainly below the 57th parallel, can also be considered as almost empty of people.

The majority of the population are ethnic Swedes, or people who can trace their ethnicity to Swedish stock going back at least 12 generations, however this is estimated to change by 2060. The Sweden Finns are a large ethnic minority comprising approximately 50,000 along the Swedish-Finnish border, and 450,000 first and second-generation immigrated ethnic Finns, mainly living in the Mälaren Valley region. Meänkieli Finnish has official status in parts of northern Sweden near the Finnish border. In addition, Sweden's indigenous population groups include the Sami people, who have a history of practicing hunting and gathering and gradually adopting a largely semi-nomadic reindeer herding lifestyle. They have been present in Fenno-Scandinavia from at earliest 5000 years to at latest around 2650 years . Today, the Sami language holds the status of official minority language in four municipalities in the Norrbotten county.

In addition to the Sami, Tornedalers, and Sweden Finns, Jewish and Roma people have national minority status in Sweden.

There are no official statistics on ethnicity, but according to Statistics Sweden, around 3,311,312 (32.3%) inhabitants of Sweden were of a foreign background in 2018, defined as being born abroad or born in Sweden with at least one parent born abroad. The most common countries of origin were Syria (1.82%), Finland (1.45%), Iraq (1.41%), Poland (0.91%), Iran (0.76%) and Somalia (0.67%). Sweden subsequently has one of the oldest populations in the world, with the average age of 41.1 years.

The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire period. Sources: Our World In Data and Gapminder Foundation.

Data according to Statistics Sweden, which collects the official statistics for Sweden.

Number of births :

Number of deaths :

Natural increase :

Sources: Our World In Data and the United Nations.

Source: "UN World Population Prospects"

Prior to World War II, emigrants generally outnumbered immigrants. Since then, net migration has been positive with many immigrants coming to Sweden from the 1970s through today.

Between 1820 and 1930, approximately 1.3 million Swedes, a third of the country's population at the time, emigrated to North America, and most of them to the United States. There are more than 4.4 million Swedish Americans according to a 2006 US Census Bureau estimate. In Canada, the community of Swedish ancestry is 330,000 strong.

The demographic profile of Sweden has altered considerably due to immigration patterns since the 1970s. As of 2017, Statistics Sweden reported that around 2,439,007 or 24.1% of the inhabitants of Sweden were from a foreign background: that is, each such person either had been born abroad or had been born in Sweden to two parents who themselves had both been born abroad. Also taking into account people with only one parent born abroad, this number increases to almost a third in 2017.

Additionally, the birth rate among immigrant women after arriving in Sweden is somewhat higher than among ethnic Swedes. Taking into account the fact that immigrant women have on average fewer children than Swedish women of comparable age, however, the difference in total birth rate is only 0.1 children more if the woman is foreign born – with the disclaimer that some women may have children not immigrating to and not reported in Sweden, who are thus not included in the statistics.

Immigration increased markedly with World War II. Historically, the most numerous of foreign born nationalities are ethnic Germans from Germany and other Scandinavians from Denmark and Norway. In short order, 70,000 war children were evacuated from Finland, of which 15,000 remained in Sweden. Also, many of Denmark's nearly 7,000 Jews who were evacuated to Sweden decided to remain there.

A sizable community from the Baltic States (Estonia, Latvia and Lithuania) arrived during the Second World War.

During the 1950s and 1960s, the recruitment of immigrant labour was an important factor of immigration. The Nordic countries signed a trade agreement in 1952, establishing a common labour market and free movement across borders. This migration within the Nordic countries, especially from Finland to Scandinavia, was essential to create the tax-base required for the expansion of the strong public sector now characteristic of Scandinavia.
This continued until 1967, when the labour market became saturated, and Sweden introduced new immigration controls.

On a smaller scale, Sweden took in political refugees from Hungary and the former Czechoslovakia after their countries were invaded by the Soviet Union in 1956 and 1968, respectively.

Since the early 1970s, immigration to Sweden has been mostly due to refugee migration and family reunification from countries in the Middle East and Latin America.
According to Eurostat, in 2010, there were 1.33 million foreign-born residents in Sweden, corresponding to 14.3% of the total population. Of these, 859,000 (64.3%) were born outside the EU and 477,000 (35.7%) were born in another EU Member State. By comparison, the Swedish civil registry reports, for 2018, that nearly 1.96 million residents are foreign-born, a 47% increase from 2010. There are 8.27 million Swedish-born residents, giving a total population of 10.23 million, and a 19.1% foreign-born population.
The first group of Assyrians/Syriacs moved to Sweden from Lebanon in 1967. Many of them live in Södertälje (Stockholm). There are also around 40,000 Roma in Sweden. Some Roma people have long historical roots in Sweden, while others are more recent migrants from elsewhere in Europe.

Immigrants from Western Asia have been a rapidly growing share of Sweden's population. According to the government agency Statistics Sweden, the number of immigrants born in all of Asia (including the Middle East) rose from just 1,000 in 1950 to 295,000 in 2003. Most of those immigrants came from Iraq, Iran, Lebanon and Syria, according to Statistics Sweden.

Immigration of Iraqis increased dramatically during the Iraq War, beginning in 2003. A total of 8,951 Iraqis came to Sweden in 2006, accounting for 45% of the entire Iraqi migration to Europe. By 2007, the community of Iraqis in Sweden numbered above 70,000. In 2008, Sweden introduced tighter rules on asylum seekers.

A significant number of Syrian Christians have also settled in Sweden. There have also been immigrants from South-Central Asia such as Afghanistan and Pakistan. Since the European migrant crisis, Syrians became the second-largest group of foreign-born persons in the Swedish civil registry in 2017 with 158,443 people (after former Yugoslavia).

Note that the table below lists the citizenship the person had when arriving in Sweden, and therefore there are no registered Eritreans, Russians or Bosnians from 1990, they were recorded as Ethiopians, Soviets and Yugoslavs. The nationality of Yugoslavs below is therefore people who came to Sweden from the Socialist Federal Republic of Yugoslavia before 1991 and people who came from today's Montenegro and Serbia before 2003, then called the Federal Republic of Yugoslavia. Counting all people who came from Slovenia, Croatia, Bosnia and Herzegovina, Serbia, Montenegro, Kosovo, Macedonia, Serbia and Montenegro, the Federal Republic of Yugoslavia and the Socialist Federal Republic of Yugoslavia, there were 176,033 people from there in 2018.

The twenty-five largest groups of foreign-born persons in the Swedish civil registry as of autumn 2018 were:



The ten most common countries of birth among immigrants registered in Sweden during 2016 (including asylum seekers who came in 2015) were the following:

The Swedish language is by far the dominating language in Sweden, and is used by the government administration. English is also widely spoken and is taught in public schools.

Since 1999, Sweden has five officially recognised minority languages: Sami, Meänkieli, Standard Finnish, Romani chib and Yiddish.

The Sami language, spoken by about 7,000 people in Sweden, may be used in government agencies, courts, preschools and nursing homes in the municipalities of Arjeplog, Gällivare, Jokkmokk and Kiruna and 

Similarly, Finnish and Meänkieli can be used in the municipalities of Gällivare, Haparanda, Kiruna, Pajala and Övertorneå and its immediate neighbourhood.
Finnish is also official language, along with Swedish, in the city of Eskilstuna.
During the mid to late 20th century, immigrant communities brought other languages, among others being Persian, Serbo-Croatian, Arabic and Neo-Aramaic.

The majority (56.4%) of the population belongs to the Church of Sweden, the Lutheran church that was disestablished in 2000. This is because until 1996, those who had family members in the church automatically became members at birth. Other Christian denominations in Sweden include the Roman Catholic Church (see Catholic Church of Sweden), several Orthodox churches in diaspora, Baptist, Pentecostal, Neo-pietistic ("nyevangeliska") and other evangelical Christian churches ("frikyrkor" = 'free churches'). Shamanism persisted among the Sami people up until the 18th century, but no longer exists in its traditional form as most Sami today belong to the Lutheran church.

Jews were permitted to practice their religion in five Swedish cities in 1782, and have enjoyed full rights as citizens since 1870. The new Freedom of Religion Bill was passed in 1951, and former obstacles against Non-Lutherans working in schools and hospitals were removed. Further, that bill made it legal to leave any religious denomination, without entering another. There are also many Muslims, as well as a number of Buddhists and Bahá'í in Sweden, mainly as a result of 20th and 21st century immigration. There is also a small Zoroastrian community in Sweden.




</doc>
<doc id="26893" url="https://en.wikipedia.org/wiki?curid=26893" title="Telecommunications in Sweden">
Telecommunications in Sweden

This article covers telecommunications in Sweden.

Sweden liberalized its telecommunications industry starting in 1980s and being formally liberalized in 1993. This was three years ahead of USA and five years before the European common policy introduced in January 1998 allowed for an open and competitive telecommunication market. The Swedes, most of who are computer literate, enjoy a continuous growth in the Internet market and the availability of technologies such as Metro Ethernet, fiber, satellite, WAN access technologies and even the availability of 3G services. Statistically, 6.447 (2004) million telephone main lines are in use, 8.0436 (2005) million mobile cellular telephones are in use and 6.7 million Swedes are regular internet users.

This abundance of telecommunication technology is a result of promoting a competitive industry that was made possible by deregulation. Since Sweden was the first to take on this arduous task the government had to come up with “a regulatory framework of its own”. The processes that went about resulting in the liberalization of the telecommunications’ industry can be structured into three phases: “Phase 1 of monopoly to Phase 2 with a mix of monopoly and competition to a “mature” Phase 3 with extensive competition”.




During the period of 1993-2000 there is rise in competition with legislation of the regulatory body being changed several times. In the case of the POTS, Telia in 2000 still held monopoly in the fixed-line access market. Whereas, mobile phone and Internet penetration in the household market ended up being one of the highest in the world with more than 50 percent of the revenue coming from these two industries. There were three major organizations providing GSM services and 120 internet service providers. One of the major causes that lead competitions thrive in areas that did not have a history of monopoly was the light handed approach taken towards the interconnection issue by the regulatory body initially. Telia held very high interconnection charges, making it very difficult for new entrants to enter. But what it did do was push the new entrants to enter other markets. Tele2 did just that by taking out a massive marketing campaign to attract a huge number of customers to its internet access service. This campaign was successful enough to bring back Telia to the negotiation table over the interconnection issue . This process eventually lead to the abolition of the light handed regulatory approach towards interconnection and put more power in the hands of the regulatory body. The intensity of regulation kept increasing around 1999 in areas other than POTS, especially the mobile market.





In 2009, the Riksdag passed new legislation regulating the National Defence Radio Establishment (FRA), enabling them to collect information from both wireless and cable bound signals passing the Swedish border. Since most communications in Sweden pass through its borders at one point or another, this monitoring in practice affects most traffic within Sweden as well.







</doc>
<doc id="26894" url="https://en.wikipedia.org/wiki?curid=26894" title="Transport in Sweden">
Transport in Sweden

Transportation in Sweden is carried out by car, bus, train, tram, boat or aeroplane.

Rail transport is operated by SJ, DSBFirst, Green Cargo, Vy Tåg and more. Most counties have companies that do ticketing, marketing and financing of local passenger rail, but the actual operation are done by the above-mentioned companies.


Stockholm Metro (Stockholms Tunnelbana) is the only metro system in Sweden.

Cities with light rail (trams);

Stockholm previously had a large tram network, but this was discontinued in favour of bus and metro; a revival of the tram network was seen in the construction of Tvärbanan in the late 1990s and early 2000s.


Sweden has right-hand traffic today like all its neighbours.

Sweden had left-hand traffic ("Vänstertrafik" in Swedish) from approximately 1736 and continued to do so until 1967. Despite this virtually all cars in Sweden were actually left-hand drive and the neighbouring Nordic countries already drove on the right, leading to mistakes by visitors. The Swedish voters rejected a change to driving on the right in a referendum held in 1955.

Nevertheless, in 1963 the Riksdag passed legislation ordering the switch to right-hand traffic. The changeover took place on a Sunday morning at 5am on September 3, 1967, which was known in Swedish as "Dagen H" (H-Day), the 'H' standing for "Högertrafik" or right-hand traffic.

Since Swedish cars were left-hand drive, experts had suggested that changing to driving on the right would reduce accidents, because drivers would have a better view of the road ahead. Indeed, fatal car-to-car and car-to-pedestrian accidents did drop sharply as a result. This was likely due to drivers initially being more careful and because of the initially very low speed limits, since accident rates soon returned to nearly the same as earlier.

Total roadways: 572,900 km, as of 2009.

Motorways run through Sweden, Denmark and over the Öresund Bridge to Stockholm, Gothenburg, Uppsala and Uddevalla. The system of motorways is still being extended. The longest continuous motorways are Värnamo-Gävle (E4; 585 km) and Rabbalshede-Vellinge (E6; 412 km; will by 2013 be extended so the motorway between Trelleborg and Oslo in Norway will be completed).











</doc>
<doc id="26895" url="https://en.wikipedia.org/wiki?curid=26895" title="Swedish Armed Forces">
Swedish Armed Forces

The Swedish Armed Forces (, literally “the Defense Force”) is the government agency that forms the military forces of Sweden, and which is tasked with defense of the country, as well as promoting Sweden's wider interests, supporting international peacekeeping efforts, and providing humanitarian aid. 
It consists of the Swedish Army, the Swedish Air Force and the Swedish Navy, as well as a military reserve force, the Home Guard. Since 1994, all Swedish military branches are organized within a single unified government agency, headed by the Supreme Commander, even though the individual services maintain their distinct identities. King Carl XVI Gustaf of Sweden is traditionally considered Honorary General and Admiral "à la suite".

The Swedish Armed Forces consist of a mix of volunteers and conscripts. About 4,000 men and women are called up for service every year.

Units from the Swedish Armed Forces are currently on deployment in several international operations either actively or as military observers, including Afghanistan as part of the Resolute Support Mission and in Kosovo (as part of Kosovo Force). Moreover, Swedish Armed Forces contribute as the lead nation for an EU Battle Group approximately once every three years through the Nordic Battlegroup. Sweden has close relations with NATO and NATO members, and participates in training exercises like the Admiral Pitka Recon Challenge, and Exercise Trident Juncture 2018. Sweden also has a strong cooperation with its closest allies of the Nordic countries being part of the Nordic Defence Cooperation NORDEFCO and joint exercises such as Exercise Northern Wind 2019. In total, about 10,000 people participate in Northern Wind, of which approximately 7,000 come from prioritized cooperation states: Finland, Norway, the US and the UK.

Sweden has not participated in an officially declared war since the 1814 Swedish–Norwegian War, although e.g. Swedish aircraft took part in the NATO-led 2011 military intervention in Libya. Swedish foreign policy has managed to keep Sweden out of war through a policy of neutrality.

Sweden also provides information to its citizens in case of an emergency being part of the concept of total defense with pamphlets being sent home to all households. The publication contains information about how to act in a situation of national crisis and most notably, nuclear war. The pamphlets (titled "If the war comes") were distributed to all households from 1943 to 1961; after 1961 some of the information from the pamphlet was printed in every phone book until 1991, the end of the Cold War. In 2018 the pamphlet was renewed and distributed under the title "If the crisis or the war comes" (Swedish: Om krisen eller kriget kommer). The new pamphlet includes the well-known quote from the older ones (in case of enemy invasion): "Every statement that the resistance has ceased is false. Resistance shall be made all the time and in every situation. It depends on You - Your efforts, Your determination, Your will to survive."

After a period of enhanced readiness during World War I, the Swedish Armed Forces were subject to severe downsizing during the interwar years. When World War II started, a large rearmament program was launched to once again guard Swedish neutrality, relying on mass conscription to fill the ranks.

After World War II, Sweden considered building nuclear weapons to deter a Soviet invasion. From 1945 to 1972 the Swedish government ran a clandestine nuclear weapons program under the guise of civilian defense research at the Swedish National Defence Research Institute. By the late 1950s the work had reached the point where underground testing was feasible. However, at this time the Riksdag prohibited research and development of nuclear weapons, pledging that research should be done only for the purpose of defense against nuclear attack. They reserved the right to continue development of nuclear weapons in the future. The option to continue development of weapons was abandoned in 1966, and Sweden's subsequent signing of the Non-Proliferation Treaty in 1968 began the wind-down of the program, which finally concluded in 1972.

During the Cold War, the wartime mass conscription system was kept in place to act as a deterrent to the Soviet Union, seen as the greatest military threat to Sweden. The end of the Cold War and the collapse of the Soviet Union meant that the perceived threat lessened and the armed forces were downsized, with conscription taking in less and less recruits until it was deactivated in 2010 (and then reactivated in 2017).

After twenty years of cooperation with NATO, starting with the Partnership for Peace back in 1994, Sweden was one of five partners granted enhanced opportunities for dialogue and cooperation at the Wales Summit in 2014. The status of Enhanced Opportunities Partner provided a platform for developing a more flexible and individualized relationship, in addition to other partner formats. It coincided with Russia's illegal annexation of Crimea and military intervention in Eastern Ukraine, and also with NATO defense bill for 2016-2020. Both the need to review NATO's own defense policy and the dramatic signal that a European country was prepared to violate the existing security order using military might, gave momentum to the new partner platform. Conscription was reintroduced in 2017 to supplement the insufficient number of volunteers signing up for service.

The Swedish Armed Forces have four main tasks:

Sweden aims to have the option of remaining neutral in case of proximate war. However, Sweden cooperates militarily with a number of foreign countries. As a member of the European Union, Sweden is acting as the lead nation for EU Battlegroups and also has a close cooperation, including joint exercises, with NATO through its membership in Partnership for Peace and Euro-Atlantic Partnership Council. In 2008 a partnership was initiated between the Nordic countries to, among other things, increase the capability of joint action, and this led to the creation of NORDEFCO. As a response to the expanded military cooperation the defence proposition of 2009 stated that Sweden will not remain passive if a Nordic country or a member of the European Union were attacked.

Recent political decisions have strongly emphasized the capability to participate in international operations, to the point where this has become the main short-term goal of training and equipment acquisition. However, after the 2008 South Ossetia war territorial defense was once again emphasized. Until then most units could not be mobilized within one year. In 2009 the Minister for Defence stated that in the future all of the armed forces must capable of fully mobilizing within one week.

In 2013, after Russian air exercises in close proximity to the Swedish border were widely reported, only six percent of Swedes expressed confidence in the ability of the nation to defend itself.

The Supreme Commander () is a four-star general or flag officer that is the agency head of the Swedish Armed Forces, and is the highest ranking professional officer on active duty. The Supreme Commander in turn reports, normally through the Minister for Defence, directly to the Government of Sweden, which in turn answers to the Riksdag.

The King of Sweden was, before the enactment of the 1974 Instrument of Government, the de jure commander in chief (), but currently only has a strictly ceremonial and representative role with respect to the Armed Forces.

The Swedish Armed Forces consists of three service branches; the Army, the Air Force and the Navy, with addition of the military reserve force Home Guard. Since 1994, the first three service branches are organized within a single unified government agency, headed by the Supreme Commander, while the Home Guard reports directly to the Supreme Commander. However, the services maintain their separate identities through the use of different uniforms, ranks, and other service specific traditions.

The Armed Forces Headquarters is the highest level of command in the Swedish Armed Forces. It is led by the Supreme Commander with a civilian Director General as his deputy, with functional directorates having different responsibilities (e.g. the Military Intelligence and Security Service). Overall, the Armed Forces Headquarters have about 1000 employees, including civilian personnel.

Some of the schools listed below answer to other units, listed under the various branches of the Armed Forces:



The Nordic Battle Group is a cooperative formation of the Swedish Armed Forces alongside mainly the other Nordic countries but also some of the Baltic countries as well as Ireland, tasked as one of the EU Battle Groups. The headquarter garrison for this group is currently situated in Enköping Sweden.

Currently, Sweden has military forces deployed in Afghanistan with the NATO-led Resolute Support Mission. Swedish forces were part of the previous International Security Assistance Force (2002–2014) in Afghanistan. Sweden is also part of the multinational Kosovo Force and has a naval force deployed to the gulf of Aden as a part of Operation Atalanta. Military observers from Sweden have been sent to a large number of countries, including Georgia, Lebanon, Israel and Sri Lanka and Sweden also participates with staff officers to missions in Sudan and Chad. Sweden has been one of the Peacekeeping nations of the Neutral Nations Supervisory Commission that is tasked with overseeing the truce in the Korean Demilitarized Zone since the Korean war ended in 1953.

A battalion and other units were deployed with the NATO-led peacekeeping SFOR in Bosnia and Herzegovina (1996–2000), following the Bosnian War.

Swedish air and ground forces saw combat during the Congo Crisis, as part of the United Nations Operation in the Congo force. 9 army battalions were sent in all, and their mission lasted 1960–1964.

In mid-1995, with the national service system based on universal military training, the Swedish Army consisted of 15 maneuver brigades and, in addition, 100 battalions of various sorts (artillery, engineers, rangers, air defense, amphibious, security, surveillance etc.) with a mobilization-time of between one and two days. When national service was replaced by a selective service system, fewer and fewer young men were drafted due to the reduction in size of the armed forces. By 2010 the Swedish Army had two battalions that could be mobilized within 90 days. When the volunteer system has been fully implemented by 2019, the army will consist of 7 maneuver battalions and 14 battalions of various sorts with a readiness of one week. The Home Guard will be reduced in size to 22,000 soldiers. In 2019 the Swedish armed forces, now with a restared national service system combained with volunteer forces, aimed to reach 3 brigades as maneuver units by 2025.

After having ended the universal male conscription system in 2010, as well as deactivating conscription in peacetime, the conscription system was re-activated in 2017. Since 2018 both women and men are conscripted on equal terms. The motivation behind reactivating conscription was the need for personnel, as volunteer numbers proved to be insufficient to maintain the armed forces.

Military personnel of the Swedish Armed Forces consists of:


K = Continuously

T = Part-time

P = Conscript, for personnel drafted under the Swedish law of comprehensive defense duty

Annual recruitment of GSS is assumed to be about 4,000 persons.

Source:

In 2008, professor Mats Alvesson of the University of Lund and Karl Ydén of the University of Göteborg claimed in an op-ed, based on Ydén's doctoral dissertation, that a large part of the officer corps of the Swedish Armed Forces was preoccupied with administrative tasks instead of training soldiers or partaking in international operations. They claimed that Swedish officers were mainly focused on climbing the ranks and thereby increasing their wages and that the main way of doing this is to take more training courses, which decreases the number of officers that are specialized in their field. Therefore, the authors claimed, the Swedish Armed Forces was poorly prepared for its mission.

Major changes have been made to the officer system since then.

The transformation of the old invasion defence-oriented armed forces to the new smaller and more mobile force has also been criticized. According to the Supreme Commander of the Swedish Armed Forces the present defence budget will not be enough to implement the new defence structure by 2019. And that even when finished the armed forces will only be able to fight for a week at most.

During 2013 several Russian Air Force exercises over the Baltic Sea aimed at Swedish Military targets have made 
the future of the Swedish Armed Forces a hot topic and several political parties now want to increase defense funding. In August 2019, the government announced a bank tax to fund the military spending.

When an army based on national service (conscription) was introduced in 1901 all commissioned officers had ranks that were senior of the warrant officers ("underofficerare") and non-commissioned officers ("underbefäl"). In a reform 1926 the relative rank of the then senior warrant officer, fanjunkare, was increased to be equal with the junior officer rank "underlöjtnant" and above the most junior officer rank "fänrik". In 1960 the relative rank of the warrant officers were elevated further so that


In 1972 the personnel structure changed, reflecting increased responsibilities of warrant and non-commissioned officers, renaming the "underofficerare" as "kompaniofficerare", giving them the same ranks as company grade officers ("fänrik", "löjtnant", "kapten"). "Underbefäl" was renamed "plutonsofficerare" and given the rank titles of sergeant and "fanjunkare", although their relative rank were now placed below "fänrik". The commissioned officers were renamed "regementsofficerare", beginning with "löjtnant". The three-track career system was maintained, as well as three separate messes.

A major change in the personnel structure in 1983 (NBO 1983), merged the three professional corps of platoon officers, company officers, and regimental officers into a one-track career system within a single corps called professional officers ("yrkesofficerare"). The three messes were also merged to one.

In 2008 the Riksdag decided to create a two-track career system with a category called "specialistofficerare". When implementing the parliamentary resolution the Supreme Commander decided that some ranks in this category should, like the old "underofficerare" ranks in 1960–1972, have a relative rank higher than the most junior officers.




Manpower-numbers are taken from CIA – The World Factbook



</doc>
<doc id="26896" url="https://en.wikipedia.org/wiki?curid=26896" title="Foreign relations of Sweden">
Foreign relations of Sweden

The foreign policy of Sweden is based on the premise that national security is best served by staying free of alliances in peacetime in order to remain a neutral country in the event of war. In 2002, Sweden revised its security doctrine. The security doctrine still states that "Sweden pursues a policy of non-participation in military alliances," but permits cooperation in response to threats against peace and security. The government also seeks to maintain Sweden's high standard of living. These two objectives require heavy expenditures for social welfare, defense spending at rates considered low by Western European standards (currently around 1.2% of GNP), and close attention to foreign trade opportunities and world economic cooperation.

Sweden has been a member of the United Nations since November 19, 1946, and participates actively in the activities of the organization, including as an elected member of the Security Council (1957–1958, 1975–1976, 1997–1998 and 2017–2018), providing Dag Hammarskjöld as the second elected Secretary-General of the UN, etc. The strong interest of the Swedish Government and people in international cooperation and peacemaking has been supplemented in the early 1980s by renewed attention to Nordic and European security questions.

Sweden decided not to sign the Treaty on the Prohibition of Nuclear Weapons.

After the then Prime Minister Ingvar Carlsson had submitted Sweden's application in July 1991 the negotiations began in February 1993. Finally, on January 1, 1995, Sweden became a member of the European Union. While some argued that it went against Sweden's historic policy of neutrality, where Sweden had not joined during the Cold War because it was seen as incompatible with neutrality, others viewed the move as a natural extension of the economic cooperation that had been going on since 1972 with the EU. Sweden addressed this controversy by reserving the right not to participate in any future EU defense alliance. In membership negotiations in 1993–1994, Sweden also had reserved the right to make the final decision on whether to join the third stage of the EMU "in light of continued developments." In a nationwide referendum in November 1994, 52.3 percent of participants voted in favour of EU membership. Voter turnout was high, 83.3 percent of the eligible voters voted. The main Swedish concerns included winning popular support for EU cooperation, EU enlargement, and strengthening the EU in areas such as economic growth, job promotion, and environmental issues.

In polls taken a few years after the referendum, many Swedes indicated that they were unhappy with Sweden's membership in the EU. However, after Sweden successfully hosted its first presidency of the EU in the first half of 2001, most Swedes today have a more positive attitude towards the EU. The government, with the support of the Center Party, decided in spring 1997 to remain outside of the EMU, at least until 2002. A referendum was held on September 14, 2003. The results were 55.9% for "no", 42.0% "yes" and 2.1% giving no answer ("blank vote").

Swedish foreign policy has been the result of a wide consensus. Sweden cooperates closely with its Nordic neighbors, formally in economic and social matters through the Nordic Council of Ministers and informally in political matters through direct consultation.

Swedish neutrality and nonalignment policy in peacetime may partly explain how the country could stay out of wars since 1814. Swedish governments have not defined nonalignment as precluding outspoken positions in international affairs. Government leaders have favored national liberation movements that enjoy broad support among developing world countries, with notable attention to Africa. During the Cold War, Sweden was suspicious of the superpowers, which it saw as making decisions affecting small countries without always consulting those countries. With the end of the Cold War, that suspicion has lessened somewhat, although Sweden still chooses to remain nonaligned. Sweden has devoted particular attention to issues of disarmament, arms control, and nuclear nonproliferation and has contributed importantly to UN and other international peacekeeping efforts, including the NATO-led peacekeeping forces in the Balkans. It sat as an observer in the Western European Union from 1995 to 2011, but it is not an active member of NATO's Partnership for Peace and the Euro-Atlantic Partnership Council.

Sweden's engagement with NATO was especially strengthened during the term of Anders Fogh Rasmussen.

Sweden's nonalignment policy has led it to serve as the protecting power for a number of nations who don't have formal diplomatic relations with each other for various reasons. It currently represents the United States, Canada, and several Western European nations in North Korea for consular matters. On several occasions when the United Kingdom broke off relations with Iran (including the 1979 Iranian Revolution, the Salman Rushdie affair, and the 2012 storming of the British embassy in Tehran), Sweden served as the protecting power for the UK.

Sweden has employed its military on numerous occasions since the end of the Cold War, from Bosnia and Congo to Afghanistan and Libya. According to one study, "this military activism is driven both by the Swedish internationalist tradition of "doing good" in the world, but also for instrumental purposes. These include a desire for political influence in international institutions, an interest in collective milieu shaping, and a concern to improve the interoperability and effectiveness of the Swedish military."









</doc>
<doc id="26897" url="https://en.wikipedia.org/wiki?curid=26897" title="Spice">
Spice

A spice is a seed, fruit, root, bark, or other plant substance primarily used for flavoring or coloring food. Spices are distinguished from herbs, which are the leaves, flowers, or stems of plants used for flavoring or as a garnish. Spices are sometimes used in medicine, religious rituals, cosmetics or perfume production.

The spice trade developed throughout the Indian subcontinent by at earliest 2000 BCE with cinnamon and black pepper, and in East Asia with herbs and pepper. The Egyptians used herbs for mummification and their demand for exotic spices and herbs helped stimulate world trade. The word "spice" comes from the Old French word "espice", which became "epice", and which came from the Latin root "spec", the noun referring to "appearance, sort, kind": "species" has the same root. By 1000 BCE, medical systems based upon herbs could be found in China, Korea, and India. Early uses were connected with magic, medicine, religion, tradition, and preservation.

Cloves were used in Mesopotamia by 1700 BCE. The ancient Indian epic Ramayana mentions cloves. The Romans had cloves in the 1st century CE, as Pliny the Elder wrote about them.

The earliest written records of spices come from ancient Egyptian, Chinese, and Indian cultures. The Ebers Papyrus from Early Egyptians that dates from 1550 describes some eight hundred different medicinal remedies and numerous medicinal procedures.

Historians believe that nutmeg, which originates from the Banda Islands in Southeast Asia, was introduced to Europe in the 6th century BCE.

Indonesian merchants traveled around China, India, the Middle East, and the east coast of Africa. Arab merchants facilitated the routes through the Middle East and India. This resulted in the Egyptian port city of Alexandria being the main trading center for spices. The most important discovery prior to the European spice trade were the monsoon winds (40 CE). Sailing from Eastern spice cultivators to Western European consumers gradually replaced the land-locked spice routes once facilitated by the Middle East Arab caravans.

In the story of Genesis, Joseph was sold into slavery by his brothers to spice merchants. In the biblical poem Song of Solomon, the male speaker compares his beloved to many forms of spices.

Spices were among the most demanded and expensive products available in Europe in the Middle Ages, the most common being black pepper, cinnamon (and the cheaper alternative cassia), cumin, nutmeg, ginger and cloves. Given medieval medicine's main theory of humorism, spices and herbs were indispensable to balance "humors" in food, a daily basis for good health at a time of recurrent pandemics. In addition to being desired by those using medieval medicine, the European elite also craved spices in the Middle Ages. An example of the European aristocracy's demand for spice comes from the King of Aragon, who invested substantial resources into bringing back spices to Spain in the 12th century. He was specifically looking for spices to put in wine, and was not alone among European monarchs at the time to have such a desire for spice.

Spices were all imported from plantations in Asia and Africa, which made them expensive. From the 8th until the 15th century, the Republic of Venice had the monopoly on spice trade with the Middle East, and along with it the neighboring Italian maritime republics and city-states. The trade made the region rich. It has been estimated that around 1,000 tons of pepper and 1,000 tons of the other common spices were imported into Western Europe each year during the Late Middle Ages. The value of these goods was the equivalent of a yearly supply of grain for 1.5 million people. The most exclusive was saffron, used as much for its vivid yellow-red color as for its flavor. Spices that have now fallen into obscurity in European cuisine include grains of paradise, a relative of cardamom which mostly replaced pepper in late medieval north French cooking, long pepper, mace, spikenard, galangal and cubeb.

Spain and Portugal were interested in seeking new routes to trade in spices and other valuable products from Asia. The control of trade routes and the spice-producing regions were the main reasons that Portuguese navigator Vasco da Gama sailed to India in 1499. When da Gama discovered the pepper market in India, he was able to secure peppers for a much cheaper price than the ones demanded by Venice. At around the same time, Christopher Columbus returned from the New World. He described to investors new spices available there.

Another source of competition in the spice trade during the 15th and 16th century was the Ragusans from the maritime republic of Dubrovnik in southern Croatia.

The military prowess of Afonso de Albuquerque (1453–1515) allowed the Portuguese to take control of the sea routes to India. In 1506, he took the island of Socotra in the mouth of the Red Sea and, in 1507, Ormuz in the Persian Gulf. Since becoming the viceroy of the Indies, he took Goa in India in 1510, and Malacca on the Malay peninsula in 1511. The Portuguese could now trade directly with Siam, China, and the Maluku Islands.

With the discovery of the New World came new spices, including allspice, chili peppers, vanilla, and chocolate. This development kept the spice trade, with America as a late comer with its new seasonings, profitable well into the 19th century.

Spices are primarily used as food flavoring. They are also used to perfume cosmetics and incense. At various periods, many spices have been believed to have medicinal value. Finally, since they are expensive, rare, and exotic commodities, their conspicuous consumption has often been a symbol of wealth and social class.
It is often claimed that spices were used either as food preservatives or to mask the taste of spoiled meat, especially in the Middle Ages. This is false. In fact, spices are rather ineffective as preservatives as compared to salting, smoking, pickling, or drying, and are ineffective in covering the taste of spoiled meat. Moreover, spices have always been comparatively expensive: in 15th century Oxford, a whole pig cost about the same as a pound of the cheapest spice, pepper. There is also no evidence of such use from contemporary cookbooks: "Old cookbooks make it clear that spices weren't used as a preservative. They typically suggest adding spices toward the end of the cooking process, where they could have no preservative effect whatsoever." In fact, Cristoforo di Messisbugo suggested in the 16th century that pepper may speed up spoilage.

Though some spices have antimicrobial properties in vitro, pepper—by far the most common spice—is relatively ineffective, and in any case, salt, which is far cheaper, is also far more effective.

A spice may be available in several forms: fresh, whole dried, or pre-ground dried. Generally, spices are dried. Spices may be ground into a powder for convenience. A whole dried spice has the longest shelf life, so it can be purchased and stored in larger amounts, making it cheaper on a per-serving basis. A fresh spice, such as ginger, is usually more flavorful than its dried form, but fresh spices are more expensive and have a much shorter shelf life. Some spices are not always available either fresh or whole, for example turmeric, and often must be purchased in ground form. Small seeds, such as fennel and mustard seeds, are often used both whole and in powder form.
To grind a whole spice, the classic tool is mortar and pestle. Less labor-intensive tools are more common now: a microplane or fine grater can be used to grind small amounts; a coffee grinder is useful for larger amounts. A frequently used spice such as black pepper may merit storage in its own hand grinder or mill.

The flavor of a spice is derived in part from compounds (volatile oils) that oxidize or evaporate when exposed to air. Grinding a spice greatly increases its surface area and so increases the rates of oxidation and evaporation. Thus, flavor is maximized by storing a spice whole and grinding when needed. The shelf life of a whole dry spice is roughly two years; of a ground spice roughly six months. The "flavor life" of a ground spice can be much shorter. Ground spices are better stored away from light.

Some flavor elements in spices are soluble in water; many are soluble in oil or fat. As a general rule, the flavors from a spice take time to infuse into the food so spices are added early in preparation. This contrasts to herbs which are usually added late in preparation.
A study by the Food and Drug Administration of shipments of spices to the United States during fiscal years 2007-2009 showed about 7% of the shipments were contaminated by Salmonella bacteria, some of it antibiotic-resistant. As most spices are cooked before being served salmonella contamination often has no effect, but some spices, particularly pepper, are often eaten raw and present at table for convenient use. Shipments from Mexico and India, a major producer, were the most frequently contaminated. However, with newly developed radiation sterilization methods, the risk of Salmonella contamination is now lower.

Because they tend to have strong flavors and are used in small quantities, spices tend to add few calories to food, even though many spices, especially those made from seeds, contain high portions of fat, protein, and carbohydrate by weight. However, when used in larger quantity, spices can also contribute a substantial amount of minerals and other micronutrients, including iron, magnesium, calcium, and many others, to the diet. For example, a teaspoon of paprika contains about 1133 IU of Vitamin A, which is over 20% of the recommended daily allowance specified by the US FDA.

Most herbs and spices have substantial antioxidant activity, owing primarily to phenolic compounds, especially flavonoids, which influence nutrition through many pathways, including affecting the absorption of other nutrients. One study found cumin and fresh ginger to be highest in antioxidant activity.

India contributes 75% of global spice production.

The International Organization for Standardization addresses spices and condiments, along with related food additives, as part of the International Classification for Standards 67.220 series.

The Indian Institute of Spices Research in Kozhikode, Kerala, is devoted exclusively to conducting research for ten spice crops: black pepper, cardamom, cinnamon, clove, garcinia, ginger, nutmeg, paprika, turmeric, and vanilla.


Books
Articles



</doc>
<doc id="26898" url="https://en.wikipedia.org/wiki?curid=26898" title="Sect">
Sect

A sect is a subgroup of a religious, political, or philosophical belief system, usually an offshoot of a larger group. Although the term was originally a classification for religious separated groups, it can now refer to any organization that breaks away from a larger one to follow a different set of rules and principles.

In an Indian context, sect refers to an organized tradition.

The word "sect" comes from the Latin noun "secta" (a feminine form of a variant past participle of the verb "", to follow), meaning "a way, road", and figuratively a (prescribed) way, mode, or manner, and hence metonymously, a discipline or school of thought as defined by a set of methods and doctrines. The present gamut of meanings of "sect" has been influenced by confusion with the homonymous (but etymologically unrelated) Latin word "secta" (the feminine form of the past participle of the verb "", to cut).

There are several different sociological definitions and descriptions for the term. Among the first to define them were Max Weber and Ernst Troeltsch (1912). In the church-sect typology they are described as newly formed religious groups that form to protest elements of their parent religion (generally a denomination). Their motivation tends to be situated in accusations of apostasy or heresy in the parent denomination; they are often decrying liberal trends in denominational development and advocating a return to true religion. The American sociologists Rodney Stark and William Sims Bainbridge assert that "sects claim to be authentic purged, refurbished version of the faith from which they split". They further assert that sects have, in contrast to churches, a high degree of tension with the surrounding society. Other sociologists of religion such as Fred Kniss have asserted that sectarianism is best described with regard to what a sect is in tension with. Some religious groups exist in tension only with co-religious groups of different ethnicities, or exist in tension with the whole of society rather than the church which the sect originated from.

Sectarianism is sometimes defined in the sociology of religion as a worldview that emphasizes the unique legitimacy of believers' creed and practices and that heightens tension with the larger society by engaging in boundary-maintaining practices.

The English sociologist Roy Wallis argues that a sect is characterized by "epistemological authoritarianism": sects possess some authoritative locus for the legitimate attribution of heresy. According to Wallis, "sects lay a claim to possess unique and privileged access to the truth or salvation" and "their committed adherents typically regard all those outside the confines of the collectivity as 'in error'". He contrasts this with a cult that he described as characterized by "epistemological individualism" by which he means that "the cult has no clear locus of final authority beyond the individual member."

The corresponding words for "sect" in European languages other than English – "Sekte" (German), "secte" (French), "secta" (Spanish, Catalan), "sectă" (Romanian), "setta" (Italian), "seita" (Portuguese, Galician), "sekta" (Polish, Czech, Slovak, Bosnian, Croatian, Serbian, Slovenian, Latvian, Lithuanian), "sekt" (Danish, Estonian, Norwegian, Swedish), "sekte" (Dutch), "szekta" (Hungarian), "секта" (Russian, Serbian, Bulgarian), σέχτα (Greek) – refer to a harmful religious sect and translate into English as "cult". In France, since the 1970s, "secte" has a specific meaning which is very different from the English word.

The "Macmillan Encyclopedia of Religion" distinguishes three types of classification of Buddhism, separated into "Movements", "Nikāyas" and "Doctrinal schools":


While the historical usage of the term "sect" in Christendom has had pejorative connotations, referring to a group or movement with heretical beliefs or practices that deviate from those of groups considered orthodox, its primary meaning is to indicate a community which has separated itself from the larger body from which its members came.

There are many groups outside the Roman Catholic Church which regard themselves as Catholic, such as the Community of the Lady of All Nations, the Palmarian Catholic Church, the Philippine Independent Church, the Brazilian Catholic Apostolic Church, the Movement for the Restoration of the Ten Commandments of God, Most Holy Family Monastery, and others.

The Indologist Axel Michaels writes in his book about Hinduism that in an Indian context the word "sect does not denote a split or excluded community, but rather an organized tradition, usually established by founder with ascetic practices." According to Michaels, "Indian sects do not focus on heresy, since the lack of a center or a compulsory center makes this impossible – instead, the focus is on adherents and followers."

The ancient schools of fiqh or sharia in Islam are known as "madhhabs." In the beginning Islam was classically divided into three major sects. These political divisions are well known as Sunni Islam, Shia Islam and Khariji Islam. Each sect developed several distinct jurisprudence systems reflecting their own understanding of the Islamic law during the course of the history of Islam.

For instance, Sunnis are separated into five sub-sects, namely, Hanafi, Maliki, Shafi'i, Hanbali and Ẓāhirī.

The Shia, on the other hand, first developed Kaysanism, which in turn divided into three major groupings known as Fivers, Seveners and Twelvers. The Zaydis separated first. The non-Zaydis are initially called as "Rafida Groups." These Rafidis were later divided into two sub-groups known as Imamiyyah and Batiniyyah.


The Hanafi, Maliki, Shafi'i and Hanbali Sunnis, the Twelver groups, the Ismā'īlī groups, the Zaydis, the Ibadis, and the Ẓāhirīs continue to exist. In addition, new sects like Black Muslim movements, Quranists, Salafis, Wahhabis, and Zikris have been emerged independently.

An Islamic convention held in Jordan in July 2005, which brought 200 Muslim scholars from over 50 countries together, announced the official recognition of eight schools of Islamic jurisprudence and the varying schools of Islamic theology. The eight recognized Islamic schools and branches are:




</doc>
<doc id="26899" url="https://en.wikipedia.org/wiki?curid=26899" title="Spearmint">
Spearmint

Spearmint, also known as garden mint, common mint, lamb mint and mackerel mint, is a species of mint, Mentha spicata, native to Europe and southern temperate Asia, extending from Ireland in the west to southern China in the east. It is naturalized in many other temperate parts of the world, including northern and southern Africa, North America and South America. It is used as a flavouring in food and herbal teas. The aromatic oil, called oil of spearmint, is also used as a flavouring and sometimes as a scent.

The species and its subspecies have many synonyms, including "Mentha crispa", "Mentha crispata" and "Mentha viridis".

Spearmint is a perennial herbaceous plant. It is tall, with variably hairless to hairy stems and foliage, and a wide-spreading fleshy underground rhizome from which it grows. The leaves are long and broad, with a serrated margin. The stem is square-shaped, a defining characteristic of the mint family of herbs. Spearmint produces flowers in slender spikes, each flower pink or white in colour, long, and broad. Spearmint flowers in the summer (from July to September in the northern hemisphere), and has relatively large seeds, which measure . The name 'spear' mint derives from the pointed leaf tips.

"Mentha spicata" varies considerably in leaf blade dimensions, the prominence of leaf veins, and pubescence.

"Mentha spicata" was first described scientifically by Carl Linnaeus in 1753. The epithet "spicata" means 'bearing a spike'. The species has two accepted subspecies, each of which has acquired a large number of synonyms:

The plant is a tetraploid species (2"n" = 48), which could be a result of hybridization and chromosome doubling. "Mentha longifolia" and "Mentha suaveolens" (2"n" = 24) are likely to be the contributing diploid species.

"Mentha spicata" hybridizes with other "Mentha" species, forming hybrids such as:


Mention of spearmint dates back to at least the 1st century AD, with references from naturalist Pliny and mentions in the Bible. Further records show descriptions of mint in ancient mythology. Findings of early versions of toothpaste using mint in the 14th century suggest widespread domestication by this point. It was introduced into England through the Romans by the 5th century, and the “Father of British Botany”, of the surname Turner, mentions mint as being good for the stomach. John Gerard's "Herbal" (1597) states that: "It is good against watering eyes and all manner of break outs on the head and sores. It is applied with salt to the biting of mad dogs," and that "They lay it on the stinging of wasps and bees with good success." He also mentions that "the smell rejoice the heart of man", for which cause they used to strew it in chambers and places of recreation, pleasure and repose, where feasts and banquets are made."

Spearmint is documented as being an important cash crop in Connecticut during the period of the American Revolution, at which time mint teas were noted as being a popular drink due to them not being taxed.

Spearmint can readily adapt to grow in various types of soil. Spearmint tends to thrive with plenty of organic material in full sun to part shade. The plant is also known to be found in moist habitats such as swamps or creeks, where the soil is sand or clay.

Spearmint ideally thrives in soils that are deep and well drained, moist, rich in nutrients and organic matter, and have a crumbly texture. pH range should be between 6.0 and 7.5.

Fungal diseases are common diseases in spearmint. Two main diseases are rust and leaf spot. "Puccinia menthae" is a fungus that causes the disease called “rust”. Rust affects the leaves of spearmint by producing pustules inducing the leaves to fall off. Leaf spot is a fungal disease that occurs when "Alternaria alernata" is present on the spearmint leaves. The infection looks like circular dark spot on the top side of the leaf. Other fungi that cause disease in spearmint are "Rhizoctonia solani", "Verticillium dahliae", "Phoma strasseri", and "Erysiphe cischoracearum".

Some nematode diseases in spearmint include root knot and root lesions. Nematode species that cause root knots in this plant are various "Meloidogyne" species. The other nematode species are "Pratylenchus" which cause root lesions.

Spearmint can be infected by tobacco ringspot virus. This virus can lead to stunted plant growth and deformation of the leaves in this plant. In China, spearmint have been seen with mosaic symptoms and deformed leaves. This is an indication that the plant can also be infected by the viruses, cucumber mosaic and tomato aspermy.

Spearmint grows well in nearly all temperate climates. Gardeners often grow it in pots or planters due to its invasive, spreading rhizomes.

Spearmint leaves can be used fresh, dried, or frozen. They can also be preserved in salt, sugar, sugar syrup, alcohol, or oil. The leaves lose their aromatic appeal after the plant flowers. It can be dried by cutting just before, or right (at peak) as the flowers open, about one-half to three-quarters the way down the stalk (leaving smaller shoots room to grow). Some dispute exists as to what drying method works best; some prefer different materials (such as plastic or cloth) and different lighting conditions (such as darkness or sunlight).

Spearmint is used for its aromatic oil, called oil of spearmint. The most abundant compound in spearmint oil is "R"-(–)-carvone, which gives spearmint its distinctive smell. Spearmint oil also contains significant amounts of limonene, dihydrocarvone, and 1,8-cineol. Unlike oil of peppermint, oil of spearmint contains minimal amounts of menthol and menthone. It is used as a flavouring for toothpaste and confectionery, and is sometimes added to shampoos and soaps.

Spearmint has been used traditionally as medicines for minor ailments such as fevers, and digestive disorders. There is research on spearmint extracts in the treatment of gout and as an antiemetic.

Spearmint essential oil has had success as a larvicide against mosquitoes. Using spearmint as a larvicide would be a greener alternative to synthetic insecticides due to their toxicity and negative effect to the environment.

Used as a fumigant, spearmint essential oil is an effective insecticide against adult moths.

The main chemical component of spearmint is the terpenoid carvone, which has been shown to aid in the inhibition of tumors. Perillyl alcohol, an additional terpenoid found in lower concentrations in spearmint, positively effects the regulation of various cell substances involved in cell growth and differentiation.

Studies on spearmint have shown varying results on the antioxidant effects of the plant and its extracts. Results have ranged from spearmint essential oil displaying considerable free radical scavenging activity to no antioxidant activity in spearmint essential oil, but strong activity in spearmint methanolic extract. Antioxidant activity has been shown to be significantly higher in spearmint that is dried at lower temperatures rather than high. It is suggested this is due to the degradation of phenolics at high temperatures. In experiments demonstrating antioxidant properties in spearmint oil, the major component, carvone, alone showed lower antioxidant activity.

Spearmint has been historically used for its antimicrobial activity, which is likely due to the high concentration of carvone. Its in vitro antibacterial activity has been compared to, and is even said to surpass, that of amoxicillin, penicillin, and streptomycin. Spearmint oil is found to have higher activity against Gram-positive bacteria compared to Gram-negative bacteria, which may be due to differing sensitivities to oils. The degree of antimicrobial activity varies with the type of microorganism tested.

Studies have found significant antiandrogen effects in spearmint, specifically following routine spearmint herbal tea ingestion. Antispasmodic effects have been displayed in spearmint oil and carvone, the main chemical component of spearmint.

Spearmint leaves are infused in water to make spearmint tea. Spearmint is an ingredient of Maghrebi mint tea. Grown in the mountainous regions of Morocco, this variety of mint possesses a clear, pungent, but mild aroma. Spearmint is an ingredient in several mixed drinks, such as the mojito and mint julep. Sweet tea, iced and flavoured with spearmint, is a summer tradition in the Southern United States.



</doc>
<doc id="26902" url="https://en.wikipedia.org/wiki?curid=26902" title="Satureja">
Satureja

Satureja is a genus of aromatic plants of the family Lamiaceae, related to rosemary and thyme. It is native to North Africa, southern and southeastern Europe, the Middle East, and Central Asia. A few New World species were formerly included in "Satureja", but they have all been moved to other genera. Several species are cultivated as culinary herbs called savory, and they have become established in the wild in a few places.

"Satureja" species may be annual or perennial. They are low-growing herbs and subshrubs, reaching heights of .

The leaves are long, with flowers forming in whorls on the stem, white to pale pink-violet.

Satureja is more commonly known as bean herb.

"Satureja" species are food plants for the larva of some Lepidoptera (butterflies and moths). Caterpillars of the moth "Coleophora bifrondella" feed exclusively on winter savory ("S. montana").

Savory may be grown purely for ornamental purposes; members of the genus need sun and well-drained soil.

Both summer savory ("Satureja hortensis") and winter savory ("Satureja montana") are used to flavor food. The former is preferred by cooks but as an annual is only available in summer; winter savory is an evergreen perennial.

Savory plays an important part in Armenian, Georgian, Bulgarian and Italian cuisine, particularly when cooking beans. It is also used to season the traditional Acadian stew known as "". Savory is also a key ingredient in "sarmale", a stuffed cabbage dish in traditional Romanian cuisine. The modern spice mixture Herbes de Provence has savory as one of the principal ingredients.

In Azerbaijan, savory is often incorporated as a flavoring in black tea.



The etymology of the Latin word "satureia" is unclear. Speculation that it is related to "saturare", to "satyr", or to za'atar is not well supported. The ancient Hebrew name is Tzatrah צתרה.


</doc>
<doc id="26903" url="https://en.wikipedia.org/wiki?curid=26903" title="Solar System">
Solar System

The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight planets, with the remainder being smaller objects, the dwarf planets and small Solar System bodies. Of the objects that orbit the Sun indirectly—the moons—two are larger than the smallest planet, Mercury.

The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant interstellar molecular cloud. The vast majority of the system's mass is in the Sun, with the majority of the remaining mass contained in Jupiter. The four smaller inner planets, Mercury, Venus, Earth and Mars, are terrestrial planets, being primarily composed of rock and metal. The four outer planets are giant planets, being substantially more massive than the terrestrials. The two largest planets, Jupiter and Saturn, are gas giants, being composed mainly of hydrogen and helium; the two outermost planets, Uranus and Neptune, are ice giants, being composed mostly of substances with relatively high melting points compared with hydrogen and helium, called volatiles, such as water, ammonia and methane. All eight planets have almost circular orbits that lie within a nearly flat disc called the ecliptic.

The Solar System also contains smaller objects. The asteroid belt, which lies between the orbits of Mars and Jupiter, mostly contains objects composed, like the terrestrial planets, of rock and metal. Beyond Neptune's orbit lie the Kuiper belt and scattered disc, which are populations of trans-Neptunian objects composed mostly of ices, and beyond them a newly discovered population of sednoids. Within these populations, some objects are large enough to have rounded under their own gravity, though there is considerable debate as to how many there will prove to be. Such objects are categorized as dwarf planets. The only certain dwarf planet is Pluto, with another trans-Neptunian object, Eris, expected to be, and the asteroid Ceres at least close to being a dwarf planet. In addition to these two regions, various other small-body populations, including comets, centaurs and interplanetary dust clouds, freely travel between regions. Six of the planets, the six largest possible dwarf planets, and many of the smaller bodies are orbited by natural satellites, usually termed "moons" after the Moon. Each of the outer planets is encircled by planetary rings of dust and other small objects.

The solar wind, a stream of charged particles flowing outwards from the Sun, creates a bubble-like region in the interstellar medium known as the heliosphere. The heliopause is the point at which pressure from the solar wind is equal to the opposing pressure of the interstellar medium; it extends out to the edge of the scattered disc. The Oort cloud, which is thought to be the source for long-period comets, may also exist at a distance roughly a thousand times further than the heliosphere. The Solar System is located in the Orion Arm, 26,000 light-years from the center of the Milky Way galaxy.

For most of history, humanity did not recognize or understand the concept of the Solar System. Most people up to the Late Middle Ages–Renaissance believed Earth to be stationary at the centre of the universe and categorically different from the divine or ethereal objects that moved through the sky. Although the Greek philosopher Aristarchus of Samos had speculated on a heliocentric reordering of the cosmos, Nicolaus Copernicus was the first to develop a mathematically predictive heliocentric system.

In the 17th century, Galileo discovered that the Sun was marked with sunspots, and that Jupiter had four satellites in orbit around it. Christiaan Huygens followed on from Galileo's discoveries by discovering Saturn's moon Titan and the shape of the rings of Saturn. Edmond Halley realised in 1705 that repeated sightings of a comet were recording the same object, returning regularly once every 75–76 years. This was the first evidence that anything other than the planets orbited the Sun. Around this time (1704), the term "Solar System" first appeared in English. In 1838, Friedrich Bessel successfully measured a stellar parallax, an apparent shift in the position of a star created by Earth's motion around the Sun, providing the first direct, experimental proof of heliocentrism. Improvements in observational astronomy and the use of unmanned spacecraft have since enabled the detailed investigation of other bodies orbiting the Sun.
The principal component of the Solar System is the Sun, a G2 main-sequence star that contains 99.86% of the system's known mass and dominates it gravitationally. The Sun's four largest orbiting bodies, the giant planets, account for 99% of the remaining mass, with Jupiter and Saturn together comprising more than 90%. The remaining objects of the Solar System (including the four terrestrial planets, the dwarf planets, moons, asteroids, and comets) together comprise less than 0.002% of the Solar System's total mass.

Most large objects in orbit around the Sun lie near the plane of Earth's orbit, known as the ecliptic. The planets are very close to the ecliptic, whereas comets and Kuiper belt objects are frequently at significantly greater angles to it. As a result of the formation of the Solar System planets, and most other objects, orbit the Sun in the same direction that the Sun is rotating (counter-clockwise, as viewed from above Earth's north pole). There are exceptions, such as Halley's Comet. Most of the larger moons orbit their planets in this "prograde" direction (with Triton being the largest "retrograde" exception) and most larger objects rotate themselves in the same direction (with Venus being a notable "retrograde" exception).

The overall structure of the charted regions of the Solar System consists of the Sun, four relatively small inner planets surrounded by a belt of mostly rocky asteroids, and four giant planets surrounded by the Kuiper belt of mostly icy objects. Astronomers sometimes informally divide this structure into separate regions. The inner Solar System includes the four terrestrial planets and the asteroid belt. The outer Solar System is beyond the asteroids, including the four giant planets. Since the discovery of the Kuiper belt, the outermost parts of the Solar System are considered a distinct region consisting of the objects beyond Neptune.

Most of the planets in the Solar System have secondary systems of their own, being orbited by planetary objects called natural satellites, or moons (two of which, Titan and Ganymede, are larger than the planet Mercury), and, in the case of the four giant planets, by planetary rings, thin bands of tiny particles that orbit them in unison. Most of the largest natural satellites are in synchronous rotation, with one face permanently turned toward their parent.

Kepler's laws of planetary motion describe the orbits of objects about the Sun. Following Kepler's laws, each object travels along an ellipse with the Sun at one focus. Objects closer to the Sun (with smaller semi-major axes) travel more quickly because they are more affected by the Sun's gravity. On an elliptical orbit, a body's distance from the Sun varies over the course of its year. A body's closest approach to the Sun is called its "perihelion", whereas its most distant point from the Sun is called its "aphelion". The orbits of the planets are nearly circular, but many comets, asteroids, and Kuiper belt objects follow highly elliptical orbits. The positions of the bodies in the Solar System can be predicted using numerical models.

Although the Sun dominates the system by mass, it accounts for only about 2% of the angular momentum. The planets, dominated by Jupiter, account for most of the rest of the angular momentum due to the combination of their mass, orbit, and distance from the Sun, with a possibly significant contribution from comets.

The Sun, which comprises nearly all the matter in the Solar System, is composed of roughly 98% hydrogen and helium. Jupiter and Saturn, which comprise nearly all the remaining matter, are also primarily composed of hydrogen and helium. A composition gradient exists in the Solar System, created by heat and light pressure from the Sun; those objects closer to the Sun, which are more affected by heat and light pressure, are composed of elements with high melting points. Objects farther from the Sun are composed largely of materials with lower melting points. The boundary in the Solar System beyond which those volatile substances could condense is known as the frost line, and it lies at roughly 5 AU from the Sun.

The objects of the inner Solar System are composed mostly of rock, the collective name for compounds with high melting points, such as silicates, iron or nickel, that remained solid under almost all conditions in the protoplanetary nebula. Jupiter and Saturn are composed mainly of gases, the astronomical term for materials with extremely low melting points and high vapour pressure, such as hydrogen, helium, and neon, which were always in the gaseous phase in the nebula. Ices, like water, methane, ammonia, hydrogen sulfide, and carbon dioxide, have melting points up to a few hundred kelvins. They can be found as ices, liquids, or gases in various places in the Solar System, whereas in the nebula they were either in the solid or gaseous phase. Icy substances comprise the majority of the satellites of the giant planets, as well as most of Uranus and Neptune (the so-called "ice giants") and the numerous small objects that lie beyond Neptune's orbit. Together, gases and ices are referred to as "volatiles".

The distance from Earth to the Sun is . For comparison, the radius of the Sun is . Thus, the Sun occupies 0.00001% (10 %) of the volume of a sphere with a radius the size of Earth's orbit, whereas Earth's volume is roughly one millionth (10) that of the Sun. Jupiter, the largest planet, is from the Sun and has a radius of , whereas the most distant planet, Neptune, is from the Sun.

With a few exceptions, the farther a planet or belt is from the Sun, the larger the distance between its orbit and the orbit of the next nearer object to the Sun. For example, Venus is approximately 0.33 AU farther out from the Sun than Mercury, whereas Saturn is 4.3 AU out from Jupiter, and Neptune lies 10.5 AU out from Uranus. Attempts have been made to determine a relationship between these orbital distances (for example, the Titius–Bode law), but no such theory has been accepted. The images at the beginning of this section show the orbits of the various constituents of the Solar System on different scales.

Some Solar System models attempt to convey the relative scales involved in the Solar System on human terms. Some are small in scale (and may be mechanical—called orreries)—whereas others extend across cities or regional areas. The largest such scale model, the Sweden Solar System, uses the 110-metre (361 ft) Ericsson Globe in Stockholm as its substitute Sun, and, following the scale, Jupiter is a 7.5-metre (25-foot) sphere at Stockholm Arlanda Airport, 40 km (25 mi) away, whereas the farthest current object, Sedna, is a 10 cm (4 in) sphere in Luleå, 912 km (567 mi) away.

If the Sun–Neptune distance is scaled to 100 metres, then the Sun would be about 3 cm in diameter (roughly two-thirds the diameter of a golf ball), the giant planets would be all smaller than about 3 mm, and Earth's diameter along with that of the other terrestrial planets would be smaller than a flea (0.3 mm) at this scale.

The Solar System formed 4.568 billion years ago from the gravitational collapse of a region within a large molecular cloud. This initial cloud was likely several light-years across and probably birthed several stars. As is typical of molecular clouds, this one consisted mostly of hydrogen, with some helium, and small amounts of heavier elements fused by previous generations of stars. As the region that would become the Solar System, known as the pre-solar nebula, collapsed, conservation of angular momentum caused it to rotate faster. The centre, where most of the mass collected, became increasingly hotter than the surrounding disc. As the contracting nebula rotated faster, it began to flatten into a protoplanetary disc with a diameter of roughly 200 AU and a hot, dense protostar at the centre. The planets formed by accretion from this disc, in which dust and gas gravitationally attracted each other, coalescing to form ever larger bodies. Hundreds of protoplanets may have existed in the early Solar System, but they either merged or were destroyed, leaving the planets, dwarf planets, and leftover minor bodies.
Due to their higher boiling points, only metals and silicates could exist in solid form in the warm inner Solar System close to the Sun, and these would eventually form the rocky planets of Mercury, Venus, Earth, and Mars. Because metallic elements only comprised a very small fraction of the solar nebula, the terrestrial planets could not grow very large. The giant planets (Jupiter, Saturn, Uranus, and Neptune) formed further out, beyond the frost line, the point between the orbits of Mars and Jupiter where material is cool enough for volatile icy compounds to remain solid. The ices that formed these planets were more plentiful than the metals and silicates that formed the terrestrial inner planets, allowing them to grow massive enough to capture large atmospheres of hydrogen and helium, the lightest and most abundant elements. Leftover debris that never became planets congregated in regions such as the asteroid belt, Kuiper belt, and Oort cloud. The Nice model is an explanation for the creation of these regions and how the outer planets could have formed in different positions and migrated to their current orbits through various gravitational interactions.

Within 50 million years, the pressure and density of hydrogen in the centre of the protostar became great enough for it to begin thermonuclear fusion. The temperature, reaction rate, pressure, and density increased until hydrostatic equilibrium was achieved: the thermal pressure equalled the force of gravity. At this point, the Sun became a main-sequence star. The main-sequence phase, from beginning to end, will last about 10 billion years for the Sun compared to around two billion years for all other phases of the Sun's pre-remnant life combined. Solar wind from the Sun created the heliosphere and swept away the remaining gas and dust from the protoplanetary disc into interstellar space, ending the planetary formation process. The Sun is growing brighter; early in its main-sequence life its brightness was 70% that of what it is today.

The Solar System will remain roughly as we know it today until the hydrogen in the core of the Sun has been entirely converted to helium, which will occur roughly 5 billion years from now. This will mark the end of the Sun's main-sequence life. At this time, the core of the Sun will contract with hydrogen fusion occurring along a shell surrounding the inert helium, and the energy output will be much greater than at present. The outer layers of the Sun will expand to roughly 260 times its current diameter, and the Sun will become a red giant. Because of its vastly increased surface area, the surface of the Sun will be considerably cooler (2,600 K at its coolest) than it is on the main sequence. The expanding Sun is expected to vaporize Mercury and render Earth uninhabitable. Eventually, the core will be hot enough for helium fusion; the Sun will burn helium for a fraction of the time it burned hydrogen in the core. The Sun is not massive enough to commence the fusion of heavier elements, and nuclear reactions in the core will dwindle. Its outer layers will move away into space, leaving a white dwarf, an extraordinarily dense object, half the original mass of the Sun but only the size of Earth. The ejected outer layers will form what is known as a planetary nebula, returning some of the material that formed the Sun—but now enriched with heavier elements like carbon—to the interstellar medium.

The Sun is the Solar System's star and by far its most massive component. Its large mass (332,900 Earth masses), which comprises 99.86% of all the mass in the Solar System, produces temperatures and densities in its core high enough to sustain nuclear fusion of hydrogen into helium, making it a main-sequence star. This releases an enormous amount of energy, mostly radiated into space as electromagnetic radiation peaking in visible light.

The Sun is a G2-type main-sequence star. Hotter main-sequence stars are more luminous. The Sun's temperature is intermediate between that of the hottest stars and that of the coolest stars. Stars brighter and hotter than the Sun are rare, whereas substantially dimmer and cooler stars, known as red dwarfs, make up 85% of the stars in the Milky Way.

The Sun is a population I star; it has a higher abundance of elements heavier than hydrogen and helium ("metals" in astronomical parlance) than the older population II stars. Elements heavier than hydrogen and helium were formed in the cores of ancient and exploding stars, so the first generation of stars had to die before the Universe could be enriched with these atoms. The oldest stars contain few metals, whereas stars born later have more. This high metallicity is thought to have been crucial to the Sun's development of a planetary system because the planets form from the accretion of "metals".

The vast majority of the Solar System consists of a near-vacuum known as the interplanetary medium. Along with light, the Sun radiates a continuous stream of charged particles (a plasma) known as the solar wind. This stream of particles spreads outwards at roughly 1.5 million kilometres per hour, creating a tenuous atmosphere that permeates the interplanetary medium out to at least 100 AU "(see )". Activity on the Sun's surface, such as solar flares and coronal mass ejections, disturbs the heliosphere, creating space weather and causing geomagnetic storms. The largest structure within the heliosphere is the heliospheric current sheet, a spiral form created by the actions of the Sun's rotating magnetic field on the interplanetary medium.

Earth's magnetic field stops its atmosphere from being stripped away by the solar wind. Venus and Mars do not have magnetic fields, and as a result the solar wind is causing their atmospheres to gradually bleed away into space. Coronal mass ejections and similar events blow a magnetic field and huge quantities of material from the surface of the Sun. The interaction of this magnetic field and material with Earth's magnetic field funnels charged particles into Earth's upper atmosphere, where its interactions create aurorae seen near the magnetic poles.

The heliosphere and planetary magnetic fields (for those planets that have them) partially shield the Solar System from high-energy interstellar particles called cosmic rays. The density of cosmic rays in the interstellar medium and the strength of the Sun's magnetic field change on very long timescales, so the level of cosmic-ray penetration in the Solar System varies, though by how much is unknown.

The interplanetary medium is home to at least two disc-like regions of cosmic dust. The first, the zodiacal dust cloud, lies in the inner Solar System and causes the zodiacal light. It was likely formed by collisions within the asteroid belt brought on by gravitational interactions with the planets. The second dust cloud extends from about 10 AU to about 40 AU, and was probably created by similar collisions within the Kuiper belt.

The inner Solar System is the region comprising the terrestrial planets and the asteroid belt. Composed mainly of silicates and metals, the objects of the inner Solar System are relatively close to the Sun; the radius of this entire region is less than the distance between the orbits of Jupiter and Saturn. This region is also within the frost line, which is a little less than 5 AU (about 700 million km) from the Sun.

The four terrestrial or inner planets have dense, rocky compositions, few or no moons, and no ring systems. They are composed largely of refractory minerals, such as the silicateswhich form their crusts and mantlesand metals, such as iron and nickel, which form their cores. Three of the four inner planets (Venus, Earth and Mars) have atmospheres substantial enough to generate weather; all have impact craters and tectonic surface features, such as rift valleys and volcanoes. The term "inner planet" should not be confused with "inferior planet", which designates those planets that are closer to the Sun than Earth is (i.e. Mercury and Venus).

Mercury ( from the Sun) is the closest planet to the Sun and on average, all seven other planets. The smallest planet in the Solar System (), Mercury has no natural satellites. Besides impact craters, its only known geological features are lobed ridges or rupes that were probably produced by a period of contraction early in its history. Mercury's very tenuous atmosphere consists of atoms blasted off its surface by the solar wind. Its relatively large iron core and thin mantle have not yet been adequately explained. Hypotheses include that its outer layers were stripped off by a giant impact, or that it was prevented from fully accreting by the young Sun's energy.

Venus (0.7 AU from the Sun) is close in size to Earth () and, like Earth, has a thick silicate mantle around an iron core, a substantial atmosphere, and evidence of internal geological activity. It is much drier than Earth, and its atmosphere is ninety times as dense. Venus has no natural satellites. It is the hottest planet, with surface temperatures over , most likely due to the amount of greenhouse gases in the atmosphere. No definitive evidence of current geological activity has been detected on Venus, but it has no magnetic field that would prevent depletion of its substantial atmosphere, which suggests that its atmosphere is being replenished by volcanic eruptions.

Earth (1 AU from the Sun) is the largest and densest of the inner planets, the only one known to have current geological activity, and the only place where life is known to exist. Its liquid hydrosphere is unique among the terrestrial planets, and it is the only planet where plate tectonics has been observed. Earth's atmosphere is radically different from those of the other planets, having been altered by the presence of life to contain 21% free oxygen. It has one natural satellite, the Moon, the only large satellite of a terrestrial planet in the Solar System.

Mars (1.5 AU from the Sun) is smaller than Earth and Venus (). It has an atmosphere of mostly carbon dioxide with a surface pressure of 6.1 millibars (roughly 0.6% of that of Earth). Its surface, peppered with vast volcanoes, such as Olympus Mons, and rift valleys, such as Valles Marineris, shows geological activity that may have persisted until as recently as 2 million years ago. Its red colour comes from iron oxide (rust) in its soil. Mars has two tiny natural satellites (Deimos and Phobos) thought to be either captured asteroids, or ejected debris from a massive impact early in Mars's history.

Asteroids except for the largest, Ceres, are classified as small Solar System bodies and are composed mainly of refractory rocky and metallic minerals, with some ice. They range from a few metres to hundreds of kilometres in size. Asteroids smaller than one meter are usually called meteoroids and micrometeoroids (grain-sized), depending on different, somewhat arbitrary definitions.

The asteroid belt occupies the orbit between Mars and Jupiter, between from the Sun. It is thought to be remnants from the Solar System's formation that failed to coalesce because of the gravitational interference of Jupiter. The asteroid belt contains tens of thousands, possibly millions, of objects over one kilometre in diameter. Despite this, the total mass of the asteroid belt is unlikely to be more than a thousandth of that of Earth. The asteroid belt is very sparsely populated; spacecraft routinely pass through without incident.

Ceres (2.77 AU) is the largest asteroid, a protoplanet, and a dwarf planet. It has a diameter of slightly under , and a mass large enough for its own gravity to pull it into a spherical shape. Ceres was considered a planet when it was discovered in 1801, and was reclassified to asteroid in the 1850s as further observations revealed additional asteroids. It was classified as a dwarf planet in 2006 when the definition of a planet was created.
Asteroids in the asteroid belt are divided into asteroid groups and families based on their orbital characteristics. Asteroid moons are asteroids that orbit larger asteroids. They are not as clearly distinguished as planetary moons, sometimes being almost as large as their partners. The asteroid belt also contains main-belt comets, which may have been the source of Earth's water.

Jupiter trojans are located in either of Jupiter's L or L points (gravitationally stable regions leading and trailing a planet in its orbit); the term is also used for small bodies in any other planetary or satellite Lagrange point. Hilda asteroids are in a 2:3 resonance with Jupiter; that is, they go around the Sun three times for every two Jupiter orbits.

The inner Solar System also contains near-Earth asteroids, many of which cross the orbits of the inner planets. Some of them are potentially hazardous objects.

The outer region of the Solar System is home to the giant planets and their large moons. The centaurs and many short-period comets also orbit in this region. Due to their greater distance from the Sun, the solid objects in the outer Solar System contain a higher proportion of volatiles, such as water, ammonia, and methane than those of the inner Solar System because the lower temperatures allow these compounds to remain solid.

The four outer planets, or giant planets (sometimes called Jovian planets), collectively make up 99% of the mass known to orbit the Sun. Jupiter and Saturn are together more than 400 times the mass of Earth and consist overwhelmingly of hydrogen and helium. Uranus and Neptune are far less massiveless than 20 Earth masses () eachand are composed primarily of ices. For these reasons, some astronomers suggest they belong in their own category, ice giants. All four giant planets have rings, although only Saturn's ring system is easily observed from Earth. The term "superior planet" designates planets outside Earth's orbit and thus includes both the outer planets and Mars.

Jupiter (5.2 AU), at , is 2.5 times the mass of all the other planets put together. It is composed largely of hydrogen and helium. Jupiter's strong internal heat creates semi-permanent features in its atmosphere, such as cloud bands and the Great Red Spot. Jupiter has 79 known satellites. The four largest, Ganymede, Callisto, Io, and Europa, show similarities to the terrestrial planets, such as volcanism and internal heating. Ganymede, the largest satellite in the Solar System, is larger than Mercury.

Saturn (9.5 AU), distinguished by its extensive ring system, has several similarities to Jupiter, such as its atmospheric composition and magnetosphere. Although Saturn has 60% of Jupiter's volume, it is less than a third as massive, at . Saturn is the only planet of the Solar System that is less dense than water. The rings of Saturn are made up of small ice and rock particles. Saturn has 82 confirmed satellites composed largely of ice. Two of these, Titan and Enceladus, show signs of geological activity. Titan, the second-largest moon in the Solar System, is larger than Mercury and the only satellite in the Solar System with a substantial atmosphere.

Uranus (19.2 AU), at , is the lightest of the outer planets. Uniquely among the planets, it orbits the Sun on its side; its axial tilt is over ninety degrees to the ecliptic. It has a much colder core than the other giant planets and radiates very little heat into space. Uranus has 27 known satellites, the largest ones being Titania, Oberon, Umbriel, Ariel, and Miranda.

Neptune (), though slightly smaller than Uranus, is more massive () and hence more dense. It radiates more internal heat, but not as much as Jupiter or Saturn. Neptune has 14 known satellites. The largest, Triton, is geologically active, with geysers of liquid nitrogen. Triton is the only large satellite with a retrograde orbit. Neptune is accompanied in its orbit by several minor planets, termed Neptune trojans, that are in 1:1 resonance with it.

The centaurs are icy comet-like bodies whose orbits have semi-major axes greater than Jupiter's (5.5 AU) and less than Neptune's (30 AU). The largest known centaur, 10199 Chariklo, has a diameter of about 250 km. The first centaur discovered, 2060 Chiron, has also been classified as a comet (95P) because it develops a coma just as comets do when they approach the Sun.

Comets are small Solar System bodies, typically only a few kilometres across, composed largely of volatile ices. They have highly eccentric orbits, generally a perihelion within the orbits of the inner planets and an aphelion far beyond Pluto. When a comet enters the inner Solar System, its proximity to the Sun causes its icy surface to sublimate and ionise, creating a coma: a long tail of gas and dust often visible to the naked eye.

Short-period comets have orbits lasting less than two hundred years. Long-period comets have orbits lasting thousands of years. Short-period comets are thought to originate in the Kuiper belt, whereas long-period comets, such as Hale–Bopp, are thought to originate in the Oort cloud. Many comet groups, such as the Kreutz Sungrazers, formed from the breakup of a single parent. Some comets with hyperbolic orbits may originate outside the Solar System, but determining their precise orbits is difficult. Old comets whose volatiles have mostly been driven out by solar warming are often categorised as asteroids.

Beyond the orbit of Neptune lies the area of the "trans-Neptunian region", with the doughnut-shaped Kuiper belt, home of Pluto and several other dwarf planets, and an overlapping disc of scattered objects, which is tilted toward the plane of the Solar System and reaches much further out than the Kuiper belt. The entire region is still largely unexplored. It appears to consist overwhelmingly of many thousands of small worlds—the largest having a diameter only a fifth that of Earth and a mass far smaller than that of the Moon—composed mainly of rock and ice. This region is sometimes described as the "third zone of the Solar System", enclosing the inner and the outer Solar System.

The Kuiper belt is a great ring of debris similar to the asteroid belt, but consisting mainly of objects composed primarily of ice. It extends between 30 and 50 AU from the Sun. Though it is estimated to contain anything from dozens to thousands of dwarf planets, it is composed mainly of small Solar System bodies. Many of the larger Kuiper belt objects, such as Quaoar, Varuna, and Orcus, may prove to be dwarf planets with further data. There are estimated to be over 100,000 Kuiper belt objects with a diameter greater than 50 km, but the total mass of the Kuiper belt is thought to be only a tenth or even a hundredth the mass of Earth. Many Kuiper belt objects have multiple satellites, and most have orbits that take them outside the plane of the ecliptic.

The Kuiper belt can be roughly divided into the "classical" belt and the resonances. Resonances are orbits linked to that of Neptune (e.g. twice for every three Neptune orbits, or once for every two). The first resonance begins within the orbit of Neptune itself. The classical belt consists of objects having no resonance with Neptune, and extends from roughly 39.4 AU to 47.7 AU. Members of the classical Kuiper belt are classified as cubewanos, after the first of their kind to be discovered, 15760 Albion (which previously had the provisional designation 1992 QB), and are still in near primordial, low-eccentricity orbits.

The dwarf planet Pluto (39 AU average) is the largest known object in the Kuiper belt. When discovered in 1930, it was considered to be the ninth planet; this changed in 2006 with the adoption of a formal definition of planet. Pluto has a relatively eccentric orbit inclined 17 degrees to the ecliptic plane and ranging from 29.7 AU from the Sun at perihelion (within the orbit of Neptune) to 49.5 AU at aphelion. Pluto has a 3:2 resonance with Neptune, meaning that Pluto orbits twice round the Sun for every three Neptunian orbits. Kuiper belt objects whose orbits share this resonance are called plutinos.

Charon, the largest of Pluto's moons, is sometimes described as part of a binary system with Pluto, as the two bodies orbit a barycentre of gravity above their surfaces (i.e. they appear to "orbit each other"). Beyond Charon, four much smaller moons, Styx, Nix, Kerberos, and Hydra, orbit within the system.

Makemake (45.79 AU average), although smaller than Pluto, is the largest known object in the "classical" Kuiper belt (that is, a Kuiper belt object not in a confirmed resonance with Neptune). Makemake is the brightest object in the Kuiper belt after Pluto. It was assigned a naming committee under the expectation that it would prove to be a dwarf planet in 2008. Its orbit is far more inclined than Pluto's, at 29°.

Haumea (43.13 AU average) is in an orbit similar to Makemake, except that it is in a temporary 7:12 orbital resonance with Neptune.
It was named under the same expectation that it would prove to be a dwarf planet, though subsequent observations have indicated that it may not be a dwarf planet after all.

The scattered disc, which overlaps the Kuiper belt but extends out to about 200 AU, is thought to be the source of short-period comets. Scattered-disc objects are thought to have been ejected into erratic orbits by the gravitational influence of Neptune's early outward migration. Most scattered disc objects (SDOs) have perihelia within the Kuiper belt but aphelia far beyond it (some more than 150 AU from the Sun). SDOs' orbits are also highly inclined to the ecliptic plane and are often almost perpendicular to it. Some astronomers consider the scattered disc to be merely another region of the Kuiper belt and describe scattered disc objects as "scattered Kuiper belt objects". Some astronomers also classify centaurs as inward-scattered Kuiper belt objects along with the outward-scattered residents of the scattered disc.

Eris (68 AU average) is the largest known scattered disc object, and caused a debate about what constitutes a planet, because it is 25% more massive than Pluto and about the same diameter. It is the most massive of the known dwarf planets. It has one known moon, Dysnomia. Like Pluto, its orbit is highly eccentric, with a perihelion of 38.2 AU (roughly Pluto's distance from the Sun) and an aphelion of 97.6 AU, and steeply inclined to the ecliptic plane.

The point at which the Solar System ends and interstellar space begins is not precisely defined because its outer boundaries are shaped by two forces, the solar wind and the Sun's gravity. The limit of the solar wind's influence is roughly four times Pluto's distance from the Sun; this "heliopause", the outer boundary of the heliosphere, is considered the beginning of the interstellar medium. The Sun's Hill sphere, the effective range of its gravitational dominance, is thought to extend up to a thousand times farther and encompasses the hypothetical Oort cloud.

The heliosphere is a stellar-wind bubble, a region of space dominated by the Sun, which radiates at roughly 400 km/s its solar wind, a stream of charged particles, until it collides with the wind of the interstellar medium.

The collision occurs at the "termination shock", which is roughly 80–100 AU from the Sun upwind of the interstellar medium and roughly 200 AU from the Sun downwind. Here the wind slows dramatically, condenses and becomes more turbulent, forming a great oval structure known as the "heliosheath". This structure is thought to look and behave very much like a comet's tail, extending outward for a further 40 AU on the upwind side but tailing many times that distance downwind; evidence from "Cassini" and Interstellar Boundary Explorer spacecraft has suggested that it is forced into a bubble shape by the constraining action of the interstellar magnetic field.

The outer boundary of the heliosphere, the "heliopause", is the point at which the solar wind finally terminates and is the beginning of interstellar space. "Voyager 1" and "Voyager 2" are reported to have passed the termination shock and entered the heliosheath, at 94 and 84 AU from the Sun, respectively. "Voyager 1" is reported to have crossed the heliopause in August 2012.

The shape and form of the outer edge of the heliosphere is likely affected by the fluid dynamics of interactions with the interstellar medium as well as solar magnetic fields prevailing to the south, e.g. it is bluntly shaped with the northern hemisphere extending 9 AU farther than the southern hemisphere. Beyond the heliopause, at around 230 AU, lies the bow shock, a plasma "wake" left by the Sun as it travels through the Milky Way.
Due to a lack of data, conditions in local interstellar space are not known for certain. It is expected that NASA's Voyager spacecraft, as they pass the heliopause, will transmit valuable data on radiation levels and solar wind to Earth. How well the heliosphere shields the Solar System from cosmic rays is poorly understood. A NASA-funded team has developed a concept of a "Vision Mission" dedicated to sending a probe to the heliosphere.

90377 Sedna (520 AU average) is a large, reddish object with a gigantic, highly elliptical orbit that takes it from about 76 AU at perihelion to 940 AU at aphelion and takes 11,400 years to complete. Mike Brown, who discovered the object in 2003, asserts that it cannot be part of the scattered disc or the Kuiper belt because its perihelion is too distant to have been affected by Neptune's migration. He and other astronomers consider it to be the first in an entirely new population, sometimes termed "distant detached objects" (DDOs), which also may include the object , which has a perihelion of 45 AU, an aphelion of 415 AU, and an orbital period of 3,420 years. Brown terms this population the "inner Oort cloud" because it may have formed through a similar process, although it is far closer to the Sun. Sedna is very likely a dwarf planet, though its shape has yet to be determined. The second unequivocally detached object, with a perihelion farther than Sedna's at roughly 81 AU, is , discovered in 2012. Its aphelion is only half that of Sedna's, at 400–500 AU.

The Oort cloud is a hypothetical spherical cloud of up to a trillion icy objects that is thought to be the source for all long-period comets and to surround the Solar System at roughly 50,000 AU (around 1 light-year (ly)), and possibly to as far as 100,000 AU (1.87 ly). It is thought to be composed of comets that were ejected from the inner Solar System by gravitational interactions with the outer planets. Oort cloud objects move very slowly, and can be perturbed by infrequent events, such as collisions, the gravitational effects of a passing star, or the galactic tide, the tidal force exerted by the Milky Way.

Much of the Solar System is still unknown. The Sun's gravitational field is estimated to dominate the gravitational forces of surrounding stars out to about two light years (125,000 AU). Lower estimates for the radius of the Oort cloud, by contrast, do not place it farther than 50,000 AU. Despite discoveries such as Sedna, the region between the Kuiper belt and the Oort cloud, an area tens of thousands of AU in radius, is still virtually unmapped. There are also ongoing studies of the region between Mercury and the Sun. Objects may yet be discovered in the Solar System's uncharted regions.

Currently, the furthest known objects, such as Comet West, have aphelia around 70,000 AU from the Sun, but as the Oort cloud becomes better known, this may change.

The Solar System is located in the Milky Way, a barred spiral galaxy with a diameter of about 100,000 light-years containing more than 100 billion stars. The Sun resides in one of the Milky Way's outer spiral arms, known as the Orion–Cygnus Arm or Local Spur. The Sun lies between 25,000 and 28,000 light-years from the Galactic Centre, and its speed within the Milky Way is about 220 km/s, so that it completes one revolution every 225–250 million years. This revolution is known as the Solar System's galactic year. The solar apex, the direction of the Sun's path through interstellar space, is near the constellation Hercules in the direction of the current location of the bright star Vega. The plane of the ecliptic lies at an angle of about 60° to the galactic plane.

The Solar System's location in the Milky Way is a factor in the evolutionary history of life on Earth. Its orbit is close to circular, and orbits near the Sun are at roughly the same speed as that of the spiral arms. Therefore, the Sun passes through arms only rarely. Because spiral arms are home to a far larger concentration of supernovae, gravitational instabilities, and radiation that could disrupt the Solar System, this has given Earth long periods of stability for life to evolve. The Solar System also lies well outside the star-crowded environs of the galactic centre. Near the centre, gravitational tugs from nearby stars could perturb bodies in the Oort cloud and send many comets into the inner Solar System, producing collisions with potentially catastrophic implications for life on Earth. The intense radiation of the galactic centre could also interfere with the development of complex life. Even at the Solar System's current location, some scientists have speculated that recent supernovae may have adversely affected life in the last 35,000 years, by flinging pieces of expelled stellar core towards the Sun, as radioactive dust grains and larger, comet-like bodies.

The Solar System is in the Local Interstellar Cloud or Local Fluff. It is thought to be near the neighbouring G-Cloud but it is not known if the Solar System is embedded in the Local Interstellar Cloud, or if it is in the region where the Local Interstellar Cloud and G-Cloud are interacting. The Local Interstellar Cloud is an area of denser cloud in an otherwise sparse region known as the Local Bubble, an hourglass-shaped cavity in the interstellar medium roughly 300 light-years (ly) across. The bubble is suffused with high-temperature plasma, that suggests it is the product of several recent supernovae.

There are relatively few stars within ten light-years of the Sun. The closest is the triple star system Alpha Centauri, which is about 4.4 light-years away. Alpha Centauri A and B are a closely tied pair of Sun-like stars, whereas the small red dwarf, Proxima Centauri, orbits the pair at a distance of 0.2 light-year. In 2016, a potentially habitable exoplanet was confirmed to be orbiting Proxima Centauri, called Proxima Centauri b, the closest confirmed exoplanet to the Sun. The stars next closest to the Sun are the red dwarfs Barnard's Star (at 5.9 ly), Wolf 359 (7.8 ly), and Lalande 21185 (8.3 ly).

The largest nearby star is Sirius, a bright main-sequence star roughly 8.6 light-years away and roughly twice the Sun's mass and that is orbited by a white dwarf, Sirius B. The nearest brown dwarfs are the binary Luhman 16 system at 6.6 light-years. Other systems within ten light-years are the binary red-dwarf system Luyten 726-8 (8.7 ly) and the solitary red dwarf Ross 154 (9.7 ly). The closest solitary Sun-like star to the Solar System is Tau Ceti at 11.9 light-years. It has roughly 80% of the Sun's mass but only 60% of its luminosity. The closest known free-floating planetary-mass object to the Sun is WISE 0855−0714, an object with a mass less than 10 Jupiter masses roughly 7 light-years away.
Compared to many other planetary systems, the Solar System stands out in lacking planets interior to the orbit of Mercury. The known Solar System also lacks super-Earths (Planet Nine could be a super-Earth beyond the known Solar System). Uncommonly, it has only small rocky planets and large gas giants; elsewhere planets of intermediate size are typical—both rocky and gas—so there is no "gap" as seen between the size of Earth and of Neptune (with a radius 3.8 times as large). Also, these super-Earths have closer orbits than Mercury. This led to the hypothesis that all planetary systems start with many close-in planets, and that typically a sequence of their collisions causes consolidation of mass into few larger planets, but in case of the Solar System the collisions caused their destruction and ejection.

The orbits of Solar System planets are nearly circular. Compared to other systems, they have smaller orbital eccentricity. Although there are attempts to explain it partly with a bias in the radial-velocity detection method and partly with long interactions of a quite high number of planets, the exact causes remain undetermined.

This section is a sampling of Solar System bodies, selected for size and quality of imagery, and sorted by volume. Some omitted objects are larger than the ones included here, notably Eris, because these have not been imaged in high quality.



</doc>
<doc id="26904" url="https://en.wikipedia.org/wiki?curid=26904" title="Silurian">
Silurian

The Silurian ( ) is a geologic period and system spanning 24.6 million years from the end of the Ordovician Period, at million years ago (Mya), to the beginning of the Devonian Period, Mya. The Silurian is the shortest period of the Paleozoic Era. As with other geologic periods, the rock beds that define the period's start and end are well identified, but the exact dates are uncertain by a few million years. The base of the Silurian is set at a series of major Ordovician–Silurian extinction events when up to 60% of marine genera were wiped out.

A significant evolutionary milestone during the Silurian was the diversification of jawed fish and bony fish. Multi-cellular life also began to appear on land in the form of small, bryophyte-like and vascular plants that grew beside lakes, streams, and coastlines, and terrestrial arthropods are also first found on land during the Silurian. However, terrestrial life would not greatly diversify and affect the landscape until the Devonian.

The Silurian system was first identified by British geologist Roderick Murchison, who was examining fossil-bearing sedimentary rock strata in south Wales in the early 1830s. He named the sequences for a Celtic tribe of Wales, the Silures, inspired by his friend Adam Sedgwick, who had named the period of his study the Cambrian, from the Latin name for Wales. This naming does not indicate any correlation between the occurrence of the Silurian rocks and the land inhabited by the Silures (. , ). In 1835 the two men presented a joint paper, under the title "On the Silurian and Cambrian Systems, Exhibiting the Order in which the Older Sedimentary Strata Succeed each other in England and Wales," which was the germ of the modern geological time scale. As it was first identified, the "Silurian" series when traced farther afield quickly came to overlap Sedgwick's "Cambrian" sequence, however, provoking furious disagreements that ended the friendship.

Charles Lapworth resolved the conflict by defining a new Ordovician system including the contested beds. An alternative name for the Silurian was ""Gotlandian"" after the strata of the Baltic island of Gotland.

The French geologist Joachim Barrande, building on Murchison's work, used the term "Silurian" in a more comprehensive sense than was justified by subsequent knowledge. He divided the Silurian rocks of Bohemia into eight stages. His interpretation was questioned in 1854 by Edward Forbes, and the later stages of Barrande; F, G and H have since been shown to be Devonian. Despite these modifications in the original groupings of the strata, it is recognized that Barrande established Bohemia as a classic ground for the study of the earliest Silurian fossils.

The Llandovery Epoch lasted from mya, and is subdivided into three stages: the , lasting until , the , lasting to , and the . The epoch is named for the town of Llandovery in Carmarthenshire, Wales.

The Wenlock, which lasted from mya, is subdivided into the (to ) and ages. It is named after Wenlock Edge in Shropshire, England. During the Wenlock, the oldest-known tracheophytes of the genus "Cooksonia", appear. The complexity of slightly later Gondwana plants like "Baragwanathia", which resembled a modern clubmoss, indicates a much longer history for vascular plants, extending into the early Silurian or even Ordovician. The first terrestrial animals also appear in the Wenlock, represented by air-breathing millipedes from Scotland.

The Ludlow lasted from mya. It is named for the town of Ludlow in Shropshire, England. The Ludlow comprises the stage (lasting until ) and the stage (named for Ludford, also in Shropshire).

The Přídolí epoch, lasting from mya, is the final and shortest epoch of the Silurian. It is named after one locality at the "Homolka a Přídolí" nature reserve near the Prague suburb Slivenec in the Czech Republic. "Přídolí" is the old name of a cadastral field area.

In North America a different suite of regional stages is sometimes used:

In Estonia the following suite of regional stages is used:

With the supercontinent Gondwana covering the equator and much of the southern hemisphere, a large ocean occupied most of the northern half of the globe. The high sea levels of the Silurian and the relatively flat land (with few significant mountain belts) resulted in a number of island chains, and thus a rich diversity of environmental settings.

During the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian icecaps were less extensive than those of the late-Ordovician glaciation. The southern continents remained united during this period. The melting of icecaps and glaciers contributed to a rise in sea level, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. The continents of Avalonia, Baltica, and Laurentia drifted together near the equator, starting the formation of a second supercontinent known as Euramerica.

When the proto-Europe collided with North America, the collision folded coastal sediments that had been accumulating since the Cambrian off the east coast of North America and the west coast of Europe. This event is the Caledonian orogeny, a spate of mountain building that stretched from New York State through conjoined Europe and Greenland to Norway. At the end of the Silurian, sea levels dropped again, leaving telltale basins of evaporites extending from Michigan to West Virginia, and the new mountain ranges were rapidly eroded. The Teays River, flowing into the shallow mid-continental sea, eroded Ordovician Period strata, forming deposits of Silurian strata in northern Ohio and Indiana.

The vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include two phases of the Tethys, the Proto-Tethys and Paleo-Tethys, the Rheic Ocean, the Iapetus Ocean (a narrow seaway between Avalonia and Laurentia), and the newly formed Ural Ocean.

The Silurian period enjoyed relatively stable and warm temperatures, in contrast with the extreme glaciations of the Ordovician before it, and the extreme heat of the ensuing Devonian. Sea levels rose from their Hirnantian low throughout the first half of the Silurian; they subsequently fell throughout the rest of the period, although smaller scale patterns are superimposed on this general trend; fifteen high-stands (periods when sea levels were above the edge of the continental shelf) can be identified, and the highest Silurian sea level was probably around 140 m higher than the lowest level reached.

During this period, the Earth entered a long, warm greenhouse phase, supported by high CO levels of 4500 ppm, and warm shallow seas covered much of the equatorial land masses. Early in the Silurian, glaciers retreated back into the South Pole until they almost disappeared in the middle of Silurian. The period witnessed a relative stabilization of the Earth's general climate, ending the previous pattern of erratic climatic fluctuations. Layers of broken shells (called coquina) provide strong evidence of a climate dominated by violent storms generated then as now by warm sea surfaces. Later in the Silurian, the climate cooled slightly, but closer to the Silurian-Devonian boundary, the climate became warmer.

The climate and carbon cycle appear to be rather unsettled during the Silurian, which had a higher concentration of isotopic excursions than any other period. The Ireviken event, Mulde event and Lau event each represent isotopic excursions following a minor mass extinction and associated with rapid sea-level change, in addition to the larger extinction at the end of the Silurian. Each one leaves a similar signature in the geological record, both geochemically and biologically; pelagic (free-swimming) organisms were particularly hard hit, as were brachiopods, corals and trilobites, and extinctions rarely occur in a rapid series of fast bursts.

The Silurian was the first period to see megafossils of extensive terrestrial biota, in the form of moss-like miniature forests along lakes and streams. However, the land fauna did not have a major impact on the Earth until it diversified in the Devonian.

The first fossil records of vascular plants, that is, land plants with tissues that carry water and food, appeared in the second half of the Silurian period. The earliest-known representatives of this group are "Cooksonia". Most of the sediments containing "Cooksonia" are marine in nature. Preferred habitats were likely along rivers and streams. "Baragwanathia" appears to be almost as old, dating to the early Ludlow (420 million years) and has branching stems and needle-like leaves of 10–20 cm. The plant shows a high degree of development in relation to the age of its fossil remains. Fossils of this plant have been recorded in Australia, Canada and China. "Eohostimella heathana" is an early, probably terrestrial, "plant" known from compression fossils of early Silurian (Llandovery) age. The chemistry of its fossils is similar to that of fossilised vascular plants, rather than algae.

The first bony fish, the Osteichthyes, appeared, represented by the Acanthodians covered with bony scales; fish reached considerable diversity and developed movable jaws, adapted from the supports of the front two or three gill arches. A diverse fauna of eurypterids (sea scorpions)—some of them several meters in length—prowled the shallow Silurian seas of North America; many of their fossils have been found in New York state. Leeches also made their appearance during the Silurian Period. Brachiopods, bryozoa, molluscs, hederelloids, tentaculitoids, crinoids and trilobites were abundant and diverse. Endobiotic symbionts were common in the corals and stromatoporoids.

Reef abundance was patchy; sometimes fossils are frequent but at other points are virtually absent from the rock record.

The earliest-known animals fully adapted to terrestrial conditions appear during the Mid-Silurian, including the millipede "Pneumodesmus". Some evidence also suggests the presence of predatory trigonotarbid arachnoids and myriapods in Late Silurian facies. Predatory invertebrates would indicate that simple food webs were in place that included non-predatory prey animals. Extrapolating back from Early Devonian biota, Andrew Jeram "et al." in 1990 suggested a food web based on as-yet-undiscovered detritivores and grazers on micro-organisms.




</doc>
