<doc id="29212" url="https://en.wikipedia.org/wiki?curid=29212" title="Supersessionism">
Supersessionism

Supersessionism, also called replacement theology, is a Christian doctrine which asserts that the New Covenant through Jesus Christ supersedes the Old Covenant, which was made exclusively with the Jewish people.

In Christianity, supersessionism is a theological view on the current status of the church in relation to the Jewish people and Judaism. It holds that the Christian Church has succeeded the Israelites as the definitive people of God or that the New Covenant has replaced or superseded the Mosaic covenant. From a supersessionist's "point of view, just by continuing to exist [outside the Church], the Jews dissent". This view directly contrasts with dual-covenant theology which holds that the Mosaic covenant remains valid for Jews.

Supersessionism has formed a core tenet of the Christian Churches for the majority of their existence. Christian traditions that have traditionally championed dual-covenant theology (including the Roman Catholic, Reformed and Methodist teachings of this doctrine), have taught that the moral law continues to stand.

Subsequent to and because of the Holocaust, some mainstream Christian theologians and denominations have rejected supersessionism.

The Islamic tradition views Islam as the final and most authentic expression of Abrahamic prophetic monotheism, superseding both Jewish and Christian teachings. The doctrine of "tahrif" teaches that earlier monotheistic scriptures or their interpretations have been corrupted, while the Quran presents a pure version of the divine message that they originally contained.

The word "supersessionism" comes from the English verb "to supersede", from the Latin verb "sedeo, sedere, sedi, sessum", "to sit", plus "super", "upon". It thus signifies one thing being replaced or supplanted by another.

The word "supersession" is used by Sydney Thelwall in the title of chapter three of his 1870 translation of Tertullian's "Adversus Iudaeos". (Tertullian wrote between 198 and 208 AD.) The title is provided by Thelwall; it is not in the original Latin (which means ""Against the Jews"").

Many Christian theologians saw the New Covenant in Christ as a replacement for the Mosaic Covenant. Historically, statements on behalf of the Roman Catholic Church have claimed its ecclesiastical structures to be a fulfillment and replacement of Jewish ecclesiastical structures (see also Jerusalem as an allegory for the Church). As recently as 1965 Vatican Council II affirmed, "the Church is the new people of God," without intending to make "Israel according to the flesh", the Jewish people, irrelevant in terms of eschatology (see "Roman Catholicism," below). Modern Protestants hold to a range of positions on the relationship between the Church and the Jewish people with the primary Protestant alternative to Supersessionism being Dispensationalism.

In the wake of the Holocaust, mainstream Christian communities began to re-examine supersessionism.

In the New Testament, Jesus and others repeatedly give Jews priority in their mission, as in Jesus' expression of him coming to the Jews rather than to Gentiles and in Paul's formula "first for the Jew, then for the Gentile." Yet after the death of Jesus, the inclusion of the Gentiles as equals in this burgeoning sect of Judaism also caused problems, particularly when it came to Gentiles keeping the Mosaic Law, which was both a major issue at the Council of Jerusalem and a theme of Paul's Epistle to the Galatians, though the relationship of Paul of Tarsus and Judaism is still disputed today.

Paul's views on "the Jews" are complex, but he is generally regarded as the first person to make the claim that by not accepting claims of Jesus' divinity, known as high Christology, Jews disqualified themselves from salvation. Paul himself was born a Jew, but after a conversion experience he came to accept Jesus' divinity later in his life. In the opinion of Roman Catholic ex-priest James Carroll, accepting Jesus' divinity, for Paul, was dichotomous with being a Jew. His personal conversion and his understanding of the dichotomy between being Jewish and accepting Jesus' divinity, was the religious philosophy he wanted to see adopted among other Jews of his time. However, New Testament scholar N.T. Wright argues that Paul saw his faith in Jesus as precisely the fulfillment of his Judaism, not that there was any tension between being Jewish and Christian. Christians quickly adopted Paul's views.

For most of Christian history, supersessionism has been the mainstream interpretation of the New Testament of all three major historical traditions within Christianity – Orthodox, Roman Catholic and Protestant. The text most often quoted in favor of the supersessionist view is Hebrews 8:13: "In speaking of 'a new covenant' [Jer. 31.31-32] he has made the first one obsolete."

Many Early Christian commentators taught that the Old Covenant was fulfilled and replaced (superseded) by the New Covenant in Christ, for instance:

Augustine (354–430) follows these views of the earlier Church Fathers, but he emphasizes the importance to Christianity of the continued existence of the Jewish people: "The Jews ... are thus by their own Scriptures a testimony to us that we have not forged the prophecies about Christ." The Catholic church built its system of eschatology on his theology, where Christ rules the earth spiritually through his triumphant church. Like his anti-Jewish teacher, St. Ambrose of Milan, he defined Jews as a special subset of those damned to hell, calling them "Witness People": "Not by bodily death, shall the ungodly race of carnal Jews perish. ...Scatter them abroad, take away their strength. And bring them down O Lord." Augustine mentioned to "love" the Jews but as a means to convert them to Christianity. Jeremy Cohen, followed by John Y. B. Hood and James Carroll, sees this as having had decisive social consequences, with Carroll saying, "It is not too much to say that, at this juncture, Christianity 'permitted' Judaism to endure because of Augustine."

Supersessionism is not the name of any official Roman Catholic doctrine and the word appears in no Church documents, but official Catholic teaching has reflected varying levels of supersessionist thought throughout its history, especially prior to the mid-twentieth century. Supersessionist theology is extensive in Catholic liturgy and literature. The Second Vatican Council (1962–65) marked a shift in emphasis of official Catholic teaching about Judaism, a shift which may be described as a move from "hard" to "soft" supersessionism, to use the terminology of David Novak (below).
Prior to Vatican II, Catholic doctrine on the matter was characterized by "displacement" or "substitution" theologies, according to which the Church and its New Covenant took the place of Judaism and its "Old Covenant", the latter being rendered void by the coming of Jesus. The nullification of the Old Covenant was often explained in terms of the "deicide charge" that Jews forfeited their covenantal relationship with God by executing the divine Christ. As recently as 1943, Pope Pius XII stated in his encyclical "Mystici corporis Christi":

At the Second Vatican Council, convened within two decades of the Holocaust, there emerged a different framework for thinking about the status of the Jewish covenant. The declaration "Nostra aetate", promulgated in 1965, made several statements which signaled a shift away from "hard supersessionist" replacement thinking which posited that the Jews’ covenant was no longer acknowledged by God. Retrieving Paul's language in chapter 11 of his Epistle to the Romans, the declaration states, "God holds the Jews most dear for the sake of their Fathers; He does not repent of the gifts He makes or of the calls He issues. …Although the Church is the new people of God, the Jews should not be presented as rejected or accursed by God, as if this followed from the Holy Scriptures." Notably, a draft of the declaration contained a passage which originally called for "the entry of that [Jewish] people into the fullness of the people of God established by Christ;" however, at the suggestion of Catholic priest (and convert from Judaism) John M. Oesterreicher, it was replaced in the final promulgated version with the following language: “the Church awaits that day, known to God alone, on which all peoples will address the Lord in a single voice and ‘serve him shoulder to shoulder’ (Zeph 3:9).”
Further developments in Catholic thinking on the covenantal status of Jews were led by Pope John Paul II. Among his most noteworthy statements on the matter is that which occurred during his historic visit to the synagogue in Mainz (1980), where he called Jews the "people of God of the Old Covenant, which has never been abrogated by God (cf. Romans 11:29, "for the gifts and the calling of God are irrevocable" [NRSV])." In 1997, John Paul II again affirmed the Jews’ covenantal status: “This people continues in spite of everything to be the people of the covenant and, despite human infidelity, the Lord is faithful to his covenant.”

The post-Vatican II shift toward acknowledging the Jews as a covenanted people has led to heated discussions in the Catholic Church over the issue of missionary activity directed toward Jews, with some Catholics theologians reasoning that "if Christ is the redeemer of the world, every tongue should confess him", while others vehemently oppose "targeting Jews for conversion". Weighing in on this matter, Cardinal Walter Kasper, then President of the Pontifical Commission for Religious Relations with the Jews, reaffirmed the validity of the Jews’ covenant and then continued: Recently, in his apostolic exhortation "Evangelii gaudium" (2013), Pope Francis’s own teaching emphasized the communal heritage and mutual respect for each other. Similarly, the words of Cardinal Kasper, "God's grace, which is the grace of Jesus Christ according to our faith, is available to all. Therefore, the Church believes that Judaism, [as] the faithful response of the Jewish people to God's irrevocable covenant, is salvific for them, because God is faithful to his promises," highlight the covenantal relationship of God with the Jewish people, but differs from Pope Francis in calling the Jewish faith salvific. In 2011, Kasper specifically repudiated the notion of "displacement" theology, clarifying that the "New Covenant for Christians is not the replacement (substitution), but the fulfillment of the Old Covenant."

These statements from Catholic officials signal a remaining point of debate, wherein some adhere to a movement away from supersessionism, and others remain with a "soft" notion of supersessionism. Fringe Catholic groups, such as the Society of St. Pius X, strongly oppose the theological developments concerning Judaism made at Vatican II and retain "hard" supersessionist views. Even among mainstream Catholic groups and official Catholic teaching, elements of "soft" supersessionism remain:


Protestant opinions on supersessionism vary. These differences arise from dissimilar literal versus figurative approaches to understanding the relationships between the covenants of the Bible, particularly the relationship between the covenants of the Old Testament and the New Covenant. In consequence, there is a range of viewpoints, including:


Three prominent Protestant views on this relationship are covenant theology, New Covenant theology, and dispensationalism. Extensive discussion is found in Christian views on the Old Covenant and in the respective articles for each of these viewpoints: for example, there is a section within Dispensationalism detailing that perspective's concept of Israel. Differing approaches influence how the land promise in Genesis 12, 15 and 17 is understood, whether it is interpreted literally or figuratively, both with regard to the land and the identity of people who inherit it.

Adherents to these various views are not restricted to a single denomination though some traditions teach a certain view. Classical covenant theology is taught within the Presbyterian and Continental Reformed traditions. Methodist hermeneutics traditionally use a variation of this, known as Wesleyan covenant theology, which is consistent with Arminian soteriology. In the United States, a difference of approach has been perceived between the Presbyterian Church and the Episcopal Church, the Evangelical Lutheran Church in America, and the United Methodist Church which have worked to develop a non-supersessionist theology.

Paul van Buren developed a thoroughly nonsupersessionist position, in contrast to Karl Barth, his mentor. He wrote, "The reality of the Jewish people, fixed in history by the reality of their election, in their faithfulness in spite of their unfaithfulness, is as solid and sure as that of the gentile church."

Mormonism rejects supersessionism.

Judaism rejects supersessionism, only discussing the topic as an idea upheld by Christian and Muslim theologians. While some modern Jews are offended by the traditional Christian belief in supersessionism, a different viewpoint has been offered by Rabbi and Jewish theologian David Novak, who has stated that "Christian supersessionism need not denigrate Judaism" and that some subsets of Christian supersessionism "can affirm that God has not annulled his everlasting covenant with the Jewish people, neither past nor present nor future."

In its canonical form, the Islamic doctrine of tahrif teaches that Jewish and Christian scriptures or their interpretations have been corrupted, which has obscured the divine message that they originally contained. According to this doctrine, the Quran both points out and corrects these supposed errors introduced by previous corruption of monotheistic scriptures, which makes it the final and most pure divine revelation.

Sandra Toenis Keiting argues that Islam was supersessionist from its inception, advocating the view that the Quranic revelations would "replace the corrupted scriptures possessed by other communities", and that early Islamic scriptures display a "clear theology of revelation that is concerned with establishing the credibility of the nascent community" viz-a-viz other religions. In contrast, Abdulaziz Sachedina has argued that Islamic supersessionism stems not from the Quran or hadith, but rather from the work of Muslim jurists who reinterpreted the Quranic message about "islam" (in its literal meaning of "submission") being "the only true religion with God" into an argument about the religion of Islam being superior to other faiths, thereby providing theoretical justification for Muslim political dominance and a wider interpretation of the notion of jihad.

Both Christian and Jewish theologians have identified different types of supersessionism in the Christian reading of the Bible.

R. Kendall Soulen notes three categories of supersessionism identified by Christian theologians: punitive, economic, and structural:

These three views are neither mutually exclusive, nor logically dependent, and it is possible to hold all of them or any one with or without the others. The work of Matthew Tapie attempts a further clarification of the language of supersessionism in modern theology that Peter Ochs has called "the clearest teaching on supersessionism in modern scholarship." Tapie argued that Soulen's view of economic supersessionism shares important similarities with those of Jules Isaac's thought (the French-Jewish historian well known for his identification of "the teaching of contempt" in the Christian tradition) and can ultimately be traced to the medieval concept of the "cessation of the law" – the idea that Jewish observance of the ceremonial law (Sabbath, circumcision, and dietary laws) ceases to have a positive significance for Jews after the passion of Christ. According to Soulen, Christians today often repudiate supersessionism but they do not always carefully examine just what that is supposed to mean. Soulen thinks Tapie's work is a remedy to this situation.





</doc>
<doc id="29213" url="https://en.wikipedia.org/wiki?curid=29213" title="Software cracking">
Software cracking

Software cracking (known as "breaking" in the 1980s) is the modification of software to remove or disable features which are considered undesirable by the person cracking the software, especially copy protection features (including protection against the manipulation of software, serial number, hardware key, date checks and disc check) or software annoyances like nag screens and adware.

A crack refers to the means of achieving, for example a stolen serial number or a tool that performs that act of cracking. Some of these tools are called keygen, patch, or loader. A keygen is a handmade product serial number generator that often offers the ability to generate working serial numbers in your own name. A patch is a small computer program that modifies the machine code of another program. This has the advantage for a cracker to not include a large executable in a release when only a few bytes are changed. A loader modifies the startup flow of a program and does not remove the protection but circumvents it. A well-known example of a loader is a trainer used to cheat in games. Fairlight pointed out in one of their .nfo files that these type of cracks are not allowed for warez scene game releases. A nukewar has shown that the protection may not kick in at any point for it to be a valid crack.

The distribution of cracked copies is illegal in most countries. There have been lawsuits over cracking software. It might be legal to use cracked software in certain circumstances. Educational resources for reverse engineering and software cracking are, however, legal and available in the form of Crackme programs.

The first software copy protection was applied to software for the Apple II, Atari 800, and Commodore 64 computers.. Software publishers have implemented increasingly complex methods in an effort to stop unauthorized copying of software.

On the Apple II, unlike modern computers that use standardized device drivers to manage device communications, the operating system directly controlled the step motor that moves the floppy drive head, and also directly interpreted the raw data, called "nibbles", read from each track to identify the data sectors. This allowed complex disk-based software copy protection, by storing data on half tracks (0, 1, 2.5, 3.5, 5, 6...), quarter tracks (0, 1, 2.25, 3.75, 5, 6...), and any combination thereof. In addition, tracks did not need to be perfect rings, but could be sectioned so that sectors could be staggered across overlapping offset tracks, the most extreme version being known as spiral tracking. It was also discovered that many floppy drives did not have a fixed upper limit to head movement, and it was sometimes possible to write an additional 36th track above the normal 35 tracks. The standard Apple II copy programs could not read such protected floppy disks, since the standard DOS assumed that all disks had a uniform 35-track, 13- or 16-sector layout. Special nibble-copy programs such as Locksmith and Copy II Plus could sometimes duplicate these disks by using a reference library of known protection methods; when protected programs were cracked they would be completely stripped of the copy protection system, and transferred onto a standard format disk that any normal Apple II copy program could read.

One of the primary routes to hacking these early copy protections was to run a program that simulates the normal CPU operation. The CPU simulator provides a number of extra features to the hacker, such as the ability to single-step through each processor instruction and to examine the CPU registers and modified memory spaces as the simulation runs (any modern disassembler/debugger can do this). The Apple II provided a built-in opcode disassembler, allowing raw memory to be decoded into CPU opcodes, and this would be utilized to examine what the copy-protection was about to do next. Generally there was little to no defense available to the copy protection system, since all its secrets are made visible through the simulation. However, because the simulation itself must run on the original CPU, in addition to the software being hacked, the simulation would often run extremely slowly even at maximum speed.

On Atari 8-bit computers, the most common protection method was via "bad sectors". These were sectors on the disk that were intentionally unreadable by the disk drive. The software would look for these sectors when the program was loading and would stop loading if an error code was not returned when accessing these sectors. Special copy programs were available that would copy the disk and remember any bad sectors. The user could then use an application to spin the drive by constantly reading a single sector and display the drive RPM. With the disk drive top removed a small screwdriver could be used to slow the drive RPM below a certain point. Once the drive was slowed down the application could then go and write "bad sectors" where needed. When done the drive RPM was sped up back to normal and an uncracked copy was made. Of course cracking the software to expect good sectors made for readily copied disks without the need to meddle with the disk drive. As time went on more sophisticated methods were developed, but almost all involved some form of malformed disk data, such as a sector that might return different data on separate accesses due to bad data alignment. Products became available (from companies such as Happy Computers) which replaced the controller BIOS in Atari's "smart" drives. These upgraded drives allowed the user to make exact copies of the original program with copy protections in place on the new disk.

On the Commodore 64, several methods were used to protect software. For software distributed on ROM cartridges, subroutines were included which attempted to write over the program code. If the software was on ROM, nothing would happen, but if the software had been moved to RAM, the software would be disabled. Because of the operation of Commodore floppy drives, one write protection scheme would cause the floppy drive head to bang against the end of its rail, which could cause the drive head to become misaligned. In some cases, cracked versions of software were desirable to avoid this result. A misaligned drive head was rare usually fixing itself by smashing against the rail stops. Another brutal protection scheme was grinding from track 1 to 40 and back a few times.

Most of the early software crackers were computer hobbyists who often formed groups that competed against each other in the cracking and spreading of software. Breaking a new copy protection scheme as quickly as possible was often regarded as an opportunity to demonstrate one's technical superiority rather than a possibility of money-making. Some low skilled hobbyists would take already cracked software and edit various unencrypted strings of text in it to change messages a game would tell a game player, often something considered vulgar. Uploading the altered copies on file sharing networks provided a source of laughs for adult users. The cracker groups of the 1980s started to advertise themselves and their skills by attaching animated screens known as crack intros in the software programs they cracked and released. Once the technical competition had expanded from the challenges of cracking to the challenges of creating visually stunning intros, the foundations for a new subculture known as demoscene were established. Demoscene started to separate itself from the illegal "warez scene" during the 1990s and is now regarded as a completely different subculture. Many software crackers have later grown into extremely capable software reverse engineers; the deep knowledge of assembly required in order to crack protections enables them to reverse engineer drivers in order to port them from binary-only drivers for Windows to drivers with source code for Linux and other free operating systems. Also because music and game intro was such an integral part of gaming the music format and graphics became very popular when hardware became affordable for the home user.

With the rise of the Internet, software crackers developed secretive online organizations. In the latter half of the nineties, one of the most respected sources of information about "software protection reversing" was Fravia's website.

Most of the well-known or "elite" cracking groups make software cracks entirely for respect in the "Scene", not profit. From there, the cracks are eventually leaked onto public Internet sites by people/crackers who use well-protected/secure FTP release archives, which are made into full copies and sometimes sold illegally by other parties.

The Scene today is formed of small groups of skilled people, who informally compete to have the best crackers, methods of cracking, and reverse engineering.

The "High Cracking University" (+HCU), was founded by Old Red Cracker (+ORC), considered a genius of reverse engineering and a legendary figure in RCE, to advance research into Reverse Code Engineering (RCE). He had also taught and authored many papers on the subject, and his texts are considered classics in the field and are mandatory reading for students of RCE.

The addition of the "+" sign in front of the nickname of a reverser signified membership in the +HCU. Amongst the students of +HCU were the top of the elite Windows reversers worldwide. +HCU published a new reverse engineering problem annually and a small number of respondents with the best replies qualified for an undergraduate position at the university.

+Fravia was a professor at +HCU. Fravia's website was known as "+Fravia's Pages of Reverse Engineering" and he used it to challenge programmers as well as the wider society to "reverse engineer" the "brainwashing of a corrupt and rampant materialism". In its heyday, his website received millions of visitors per year and its influence was "widespread".

Nowadays most of the graduates of +HCU have migrated to Linux and few have remained as Windows reversers. The information at the university has been rediscovered by a new generation of researchers and practitioners of RCE who have started new research projects in the field.

The most common software crack is the modification of an application's binary to cause or prevent a specific key branch in the program's execution. This is accomplished by reverse engineering the compiled program code using a debugger such as SoftICE, x64dbg, OllyDbg, GDB, or MacsBug until the software cracker reaches the subroutine that contains the primary method of protecting the software (or by disassembling an executable file with a program such as IDA). The binary is then modified using the debugger or a hex editor or monitor in a manner that replaces a prior branching opcode with its complement or a NOP opcode so the key branch will either always execute a specific subroutine or skip over it. Almost all common software cracks are a variation of this type. Proprietary software developers are constantly developing techniques such as code obfuscation, encryption, and self-modifying code to make this modification increasingly difficult. Even with these measures being taken, developers struggle to combat software cracking. This is because it is very common for a professional to publicly release a simple cracked EXE or Retrium Installer for public download, eliminating the need for inexperienced users to crack the software themselves.

A specific example of this technique is a crack that removes the expiration period from a time-limited trial of an application. These cracks are usually programs that alter the program executable and sometimes the .dll or .so linked to the application. Similar cracks are available for software that requires a hardware dongle. A company can also break the copy protection of programs that they have legally purchased but that are licensed to particular hardware, so that there is no risk of downtime due to hardware failure (and, of course, no need to restrict oneself to running the software on bought hardware only).

Another method is the use of special software such as CloneCD to scan for the use of a commercial copy protection application. After discovering the software used to protect the application, another tool may be used to remove the copy protection from the software on the CD or DVD. This may enable another program such as Alcohol 120%, CloneDVD, Game Jackal, or Daemon Tools to copy the protected software to a user's hard disk. Popular commercial copy protection applications which may be scanned for include SafeDisc and StarForce.

In other cases, it might be possible to decompile a program in order to get access to the original source code or code on a level higher than machine code. This is often possible with scripting languages and languages utilizing JIT compilation. An example is cracking (or debugging) on the .NET platform where one might consider manipulating CIL to achieve one's needs. Java's bytecode also works in a similar fashion in which there is an intermediate language before the program is compiled to run on the platform dependent machine code.

Advanced reverse engineering for protections such as SecuROM, SafeDisc, StarForce, or Denuvo requires a cracker, or many crackers to spend much time studying the protection, eventually finding every flaw within the protection code, and then coding their own tools to "unwrap" the protection automatically from executable (.EXE) and library (.DLL) files.

There are a number of sites on the Internet that let users download cracks produced by warez groups for popular games and applications (although at the danger of acquiring malicious software that is sometimes distributed via such sites). Although these cracks are used by legal buyers of software, they can also be used by people who have downloaded or otherwise obtained unauthorized copies (often through P2P networks).

Many commercial programs that can be downloaded from the Internet have a trial period (often 30 days) and must be registered (i.e. be bought) after its expiration if the user wants to continue to use them. To reset the trial period, registry entries and/or hidden files that contain information about the trial period are modified and/or deleted. For this purpose, crackers develop ""trial resetters"" for a particular program or sometimes also for a group of programs by the same manufacturer.
<br>
A method to make trial resets less attractive is the limitation of the software during the trial period (e.g., some features are only available in the registered version; pictures/videos/hardcopies created with the program get a watermark; the program runs for only 10–20 minutes and then closes automatically). Some programs have an unlimited trial period, but are limited until their registration.



</doc>
<doc id="29215" url="https://en.wikipedia.org/wiki?curid=29215" title="SOAP">
SOAP

SOAP (abbreviation for Simple Object Access Protocol) is a messaging protocol specification for exchanging structured information in the implementation of web services in computer networks. Its purpose is to provide extensibility, neutrality, verbosity and independence. It uses XML Information Set for its message format, and relies on application layer protocols, most often Hypertext Transfer Protocol (HTTP), although some legacy systems communicate over Simple Mail Transfer Protocol (SMTP), for message negotiation and transmission.

SOAP allows developers to invoke processes running on disparate operating systems (such as Windows, macOS, and Linux) to authenticate, authorize, and communicate using Extensible Markup Language (XML). Since Web protocols like HTTP are installed and running on all operating systems, SOAP allows clients to invoke web services and receive responses independent of language and platforms.

SOAP provides the Messaging Protocol layer of a web services protocol stack for web services. It is an XML-based protocol consisting of three parts: 

SOAP has three major characteristics:


As an example of what SOAP procedures can do, an application can send a SOAP request to a server that has web services enabled—such as a real-estate price database—with the parameters for a search. The server then returns a SOAP response (an XML-formatted document with the resulting data), e.g., prices, location, features. Since the generated data comes in a standardized machine-parsable format, the requesting application can then integrate it directly.

The SOAP architecture consists of several layers of specifications for:


SOAP evolved as a successor of XML-RPC, though it borrows its transport and interaction neutrality from Web Service Addressing and the envelope/header/body from elsewhere (probably from WDDX).

SOAP was designed as an object-access protocol in 1998 by Dave Winer, Don Box, Bob Atkinson, and Mohsen Al-Ghosein for Microsoft, where Atkinson and Al-Ghosein were working. The specification was not made available until it was submitted to IETF 13 September 1999. According to Don Box, this was due to politics within Microsoft. Because of Microsoft's hesitation, Dave Winer shipped XML-RPC in 1998.

The submitted Internet Draft did not reach RFC status and is therefore not considered a "standard" as such. Version 1.1 of the specification was published as a W3C Note on 8 May 2000. Since version 1.1 did not reach W3C Recommendation status, it can not be considered a "standard" either. Version 1.2 of the specification, however, became a W3C recommendation on June 24, 2003.

The SOAP specification was maintained by the XML Protocol Working Group of the World Wide Web Consortium until the group was closed 10 July 2009. "SOAP" originally stood for "Simple Object Access Protocol" but version 1.2 of the standard dropped this acronym.

After SOAP was first introduced, it became the underlying layer of a more complex set of web services, based on Web Services Description Language (WSDL), XML schema and Universal Description Discovery and Integration (UDDI). These different services, especially UDDI, have proved to be of far less interest, but an appreciation of them gives a complete understanding of the expected role of SOAP compared to how web services have actually evolved.

SOAP specification can be broadly defined to be consisting of the following 3 conceptual components: protocol concepts, encapsulation concepts and network concepts.




The SOAP specification defines the messaging framework, which consists of:

A SOAP message is an ordinary XML document containing the following elements:

Both SMTP and HTTP are valid application layer protocols used as transport for SOAP, but HTTP has gained wider acceptance as it works well with today's internet infrastructure; specifically, HTTP works well with network firewalls. SOAP may also be used over HTTPS (which is the same protocol as HTTP at the application level, but uses an encrypted transport protocol underneath) with either simple or mutual authentication; this is the advocated WS-I method to provide web service security as stated in the WS-I Basic Profile 1.1.

This is a major advantage over other distributed protocols like GIOP/IIOP or DCOM, which are normally filtered by firewalls. SOAP over AMQP is yet another possibility that some implementations support. SOAP also has an advantage over DCOM that it is unaffected by security rights configured on the machines that require knowledge of both transmitting and receiving nodes. This lets SOAP be loosely coupled in a way that is not possible with DCOM. There is also the SOAP-over-UDP OASIS standard.

XML Information Set was chosen as the standard message format because of its widespread use by major corporations and open source development efforts. Typically, XML Information Set is serialized as XML. A wide variety of freely available tools significantly eases the transition to a SOAP-based implementation. The somewhat lengthy syntax of XML can be both a benefit and a drawback. While it promotes readability for humans, facilitates error detection, and avoids interoperability problems such as byte-order (endianness), it can slow processing speed and can be cumbersome. For example, CORBA, GIOP, ICE, and DCOM use much shorter, binary message formats. On the other hand, hardware appliances are available to accelerate processing of XML messages. Binary XML is also being explored as a means for streamlining the throughput requirements of XML.
XML messages by their self-documenting nature usually have more 'overhead' (e.g., headers, nested tags, delimiters) than actual data in contrast to earlier protocols where the overhead was usually a relatively small percentage of the overall message.

In financial messaging SOAP was found to result in a 2–4 times larger message than previous protocols FIX (Financial Information Exchange) and CDR (Common Data Representation).

XML Information Set does not have to be serialized in XML. For instance, CSV and JSON XML-infoset representations exist. There is also no need to specify a generic transformation framework. The concept of SOAP bindings allows for specific bindings for a specific application. The drawback is that both the senders and receivers have to support this newly defined binding.

The message below is requesting a stock price for AT&T (stock ticker symbol "T").
POST /InStock HTTP/1.1
Host: www.example.org
Content-Type: application/soap+xml; charset=utf-8
Content-Length: 299
SOAPAction: "http://www.w3.org/2003/05/soap-envelope"

<?xml version="1.0"?>
<soap:Envelope xmlns:soap="http://www.w3.org/2003/05/soap-envelope" xmlns:m="http://www.example.org">
</soap:Envelope>





</doc>
<doc id="29218" url="https://en.wikipedia.org/wiki?curid=29218" title="Sodium thiopental">
Sodium thiopental

Sodium thiopental, also known as Sodium Pentothal (a trademark of Abbott Laboratories), thiopental, thiopentone, or Trapanal (also a trademark), is a rapid-onset short-acting barbiturate general anesthetic. It is the thiobarbiturate analog of pentobarbital, and an analog of thiobarbital. Sodium thiopental was a core medicine in the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system, but was supplanted by propofol. Despite this thiopental is still listed as an acceptable alternative to propofol, depending on local availability and cost of these agents. It was previously the first of three drugs administered during most lethal injections in the United States, but the US manufacturer Hospira stopped manufacturing the drug and the EU banned the export of the drug for this purpose. Although thiopental abuse carries a dependency risk, its recreational use is rare.

Sodium thiopental is an ultra-short-acting barbiturate and has been used commonly in the induction phase of general anesthesia. Its use has been largely replaced with that of propofol, but retains popularity as an induction agent for rapid-sequence intubation and in obstetrics. Following intravenous injection, the drug rapidly reaches the brain and causes unconsciousness within 30–45 seconds. At one minute, the drug attains a peak concentration of about 60% of the total dose in the brain. Thereafter, the drug distributes to the rest of the body, and in about 5–10 minutes the concentration is low enough in the brain that consciousness returns.

A normal dose of sodium thiopental (usually 4–6 mg/kg) given to a pregnant woman for operative delivery (caesarian section) rapidly makes her unconscious, but the baby in her uterus remains conscious. However, larger or repeated doses can depress the baby.

Sodium thiopental is not used to maintain anesthesia in surgical procedures because, in infusion, it displays zero-order elimination pharmacokinetics, leading to a long period before consciousness is regained. Instead, anesthesia is usually maintained with an inhaled anesthetic (gas) agent. Inhaled anesthetics are eliminated relatively quickly, so that stopping the inhaled anesthetic will allow rapid return of consciousness. Sodium thiopental would have to be given in large amounts to maintain an anesthetic plane, and because of its 11.5- to 26-hour half-life, consciousness would take a long time to return.

In veterinary medicine, sodium thiopental is used to induce anesthesia in animals. Since it is redistributed to fat, certain lean breeds of dogs such as sighthounds will have prolonged recoveries from sodium thiopental due to their lack of body fat and their lean body mass. Conversely, obese animals will have rapid recoveries, but it will be some time before it is entirely removed (metabolized) from their bodies. Sodium thiopental is always administered intravenously, as it can be fairly irritating; severe tissue necrosis and sloughing can occur if it is injected incorrectly into the tissue around a vein.

In addition to anesthesia induction, sodium thiopental was historically used to induce medical comas. It has now been superseded by drugs such as propofol because their effects wear off more quickly than thiopental.
Patients with brain swelling, causing elevation of intracranial pressure, either secondary to trauma or following surgery, may benefit from this drug. Sodium thiopental, and the barbiturate class of drugs, decrease neuronal activity thereby decreasing cerebral metabolic rate of oxygen consumption (CMRO2), decrease intracranial vascular response to carbon dioxide (CO2), which in turn decreases intracranial pressure. Patients with refractory elevated intracranial pressure (RICH) due to traumatic brain injury (TBI) may have improved long term outcome when barbiturate coma is added to their neurointensive care treatment. Reportedly, thiopental has been shown to be superior to pentobarbital in reducing intracranial pressure. This phenomenon is also called a reverse steal effect.

In refractory status epilepticus, thiopental may be used to terminate a seizure.

Sodium thiopental is used intravenously for the purposes of euthanasia. In both Belgium and the Netherlands, where active euthanasia is allowed by law, the standard protocol recommends sodium thiopental as the ideal agent to induce coma, followed by pancuronium bromide to paralyze muscles and stop breathing.

Intravenous administration is the most reliable and rapid way to accomplish euthanasia. Death is quick. A coma is first induced by intravenous administration of 20 mg/kg thiopental sodium (Nesdonal) in a small volume (10 ml physiological saline). Then, a triple dose of a non-depolarizing neuromuscular blocking drug is given, such as 20 mg pancuronium bromide (Pavulon) or 20 mg vecuronium bromide (Norcuron). The muscle relaxant should be given intravenously to ensure optimal availability but pancuronium bromide may be administered intramuscularly at an increased dosage level of 40 mg.

Along with pancuronium bromide and potassium chloride, thiopental is used in 34 states of the United States to execute prisoners by lethal injection. A very large dose is given to ensure rapid loss of consciousness. Although death usually occurs within ten minutes of the beginning of the injection process, some have been known to take longer. The use of sodium thiopental in execution protocols was challenged in court after a study in the medical journal "The Lancet" reported autopsies of executed inmates showed the level of thiopental in their bloodstream was insufficient to cause unconsciousness.

On December 8, 2009, Ohio became the first state to use a single dose of sodium thiopental for its capital execution, following the failed use of the standard three-drug cocktail during a recent execution, due to inability to locate suitable veins. Kenneth Biros was executed using the single-drug method.

Washington became the second state in the US to use the single-dose sodium thiopental injections for executions. On September 10, 2010, the execution of Cal Coburn Brown was the first in the state to use a single-dose, single-drug injection. His death was pronounced approximately one and a half minutes after the intravenous administration of five grams of the drug.

After its use for the execution of Jeffrey Landrigan in the US, the UK introduced a ban on the export of sodium thiopental in December 2010, after it was established that no European supplies to the US were being used for any other purpose. The restrictions were based on "the European Union Torture Regulation (including licensing of drugs used in execution by lethal injection)". From 21 December 2011 the European Union extended trade restrictions to prevent the export of certain medicinal products for capital punishment, stating that "the Union disapproves of capital punishment in all circumstances and works towards its universal abolition".

Thiopental (Pentothal) is still used in some places as a truth serum to weaken the resolve of a subject and make them more compliant to pressure. The barbiturates as a class decrease higher cortical brain functioning, and also due to the loss of inhibition produced by barbiturates. Some psychiatrists hypothesize that because lying is more complex than telling the truth, suppression of the higher cortical functions may lead to the uncovering of the truth. The drug tends to make subjects loquacious and cooperative with interrogators; however, the reliability of confessions made under thiopental is questionable.

Psychiatrists have used thiopental to desensitize patients with phobias and to "facilitate the recall of painful repressed memories." One psychiatrist who worked with thiopental is the Dutch Professor , who used this procedure to help relieve trauma in surviving victims of the Holocaust.

Sodium thiopental is a member of the barbiturate class of drugs, which are relatively non-selective compounds that bind to an entire superfamily of ligand-gated ion channels, of which the GABA receptor channel is one of several representatives. This superfamily of ion channels includes the neuronal nAChR channel, the 5HT3R channel, the GlyR channel and others. Surprisingly, while GABA receptor currents are increased by barbiturates (and other general anesthetics), ligand-gated ion channels that are predominantly permeable for cationic ions are blocked by these compounds. For example, neuronal nAChR channels are blocked by clinically relevant anesthetic concentrations of both sodium thiopental and pentobarbital. Such findings implicate (non-GABA-ergic) ligand-gated ion channels, e.g. the neuronal nAChR channel, in mediating some of the (side) effects of barbiturates. The GABA receptor is an inhibitory channel that decreases neuronal activity, and barbiturates enhance the inhibitory action of the GABA receptor.

Following a shortage that led a court to delay an execution in California, a company spokesman for Hospira, the sole American manufacturer of the drug, objected to the use of thiopental in lethal injection. "Hospira manufactures this product because it improves or saves lives, and the company markets it solely for use as indicated on the product labeling. The drug is not indicated for capital punishment and Hospira does not support its use in this procedure." On January 21, 2011, the company announced that it would stop production of sodium thiopental from its plant in Italy because Italian authorities couldn't guarantee that exported quantities of the drug would not be used in executions. Italy was the only viable place where the company could produce sodium thiopental, leaving the United States without a supplier.

Thiopental rapidly and easily crosses the blood brain barrier as it is a lipophilic molecule. As with all lipid-soluble anaesthetic drugs, the short duration of action of sodium thiopental is due almost entirely to its redistribution away from central circulation towards muscle and fat tissue, due to its very high fat:water partition coefficient (approximately 10), leading to sequestration in fat tissue. Once redistributed, the free fraction in the blood is metabolized in the liver. Sodium thiopental is mainly metabolized to pentobarbital, 5-ethyl-5-(1'-methyl-3'-hydroxybutyl)-2-thiobarbituric acid, and 5-ethyl-5-(1'-methyl-3'-carboxypropyl)-2-thiobarbituric acid.

The usual dose range for induction of anesthesia using thiopental is from 3 to 6 mg/kg; however, there are many factors that can alter this. Premedication with sedatives such as benzodiazepines or clonidine will reduce requirements, as do specific disease states and other patient factors. Among patient factors are: age, sex, and lean body mass. Specific disease conditions that can alter the dose requirements of thiopentone and for that matter any other intravenous anaesthetic are: hypovolemia, burns, azotemia, liver failure, hypoproteinemia, etc.

As with nearly all anesthetic drugs, thiopental causes cardiovascular and respiratory depression resulting in hypotension, apnea, and airway obstruction. For these reasons, only suitably trained medical personnel should give thiopental in an environment suitably equipped to deal with these effects. Side effects include headache, agitated emergence, prolonged somnolence, and nausea. Intravenous administration of sodium thiopental is followed instantly by an odor and/or taste sensation, sometimes described as being similar to rotting onions, or to garlic. The hangover from the side effects may last up to 36 hours.

Although each molecule of thiopental contains one sulfur atom, it is not a sulfonamide, and does not show allergic reactions of sulfa/sulpha drugs.

Thiopental should be used with caution in cases of liver disease, Addison's disease, myxedema, severe heart disease, severe hypotension, a severe breathing disorder, or a family history of porphyria.

Co-administration of pentoxifylline and thiopental causes death by acute pulmonary edema in rats. This pulmonary edema was not mediated by cardiac failure or by pulmonary hypertension but was due to increased pulmonary vascular permeability.

Sodium thiopental was discovered in the early 1930s by Ernest H. Volwiler and Donalee L. Tabern, working for Abbott Laboratories. It was first used in human beings on March 8, 1934, by Dr. Ralph M. Waters in an investigation of its properties, which were short-term anesthesia and surprisingly little analgesia. Three months later, Dr. John S. Lundy started a clinical trial of thiopental at the Mayo Clinic at the request of Abbott. Abbott continued to make the drug until 2004, when it spun off its hospital-products division as Hospira.

Thiopental is famously associated with a number of anesthetic deaths in victims of the attack on Pearl Harbor. These deaths, relatively soon after the drug's introduction, were said to be due to excessive doses given to shocked trauma patients. However, recent evidence available through freedom of information legislation was reviewed in the "British Journal of Anaesthesia", which has suggested that this story was grossly exaggerated. Of the 344 wounded that were admitted to the Tripler Army Hospital, only 13 did not survive, and it is unlikely that thiopentone overdose was responsible for more than a few of these.




</doc>
<doc id="29219" url="https://en.wikipedia.org/wiki?curid=29219" title="Stone Age">
Stone Age

The Stone Age was a broad prehistoric period during which stone was widely used to make tools with an edge, a point, or a percussion surface. The period lasted for roughly 3.4 million years, and ended between 8700 BCE and 2000 BCE , with the advent of metalworking. Though some simple metalworking of malleable metals, particularly the use of gold and copper for purposes of ornamentation, was known in the Stone Age, it is the melting and smelting of copper that marks the end of the Stone Age. In western Asia this occurred by about 3000 BCE, when bronze became widespread. The term Bronze Age is used to describe the period that followed the Stone Age, as well as to describe cultures that had developed techniques and technologies for working copper into tools, supplanting stone in many uses.

Stone Age artifacts that have been discovered include tools used by modern humans, by their predecessor species in the genus "Homo", and possibly by the earlier partly contemporaneous genera "Australopithecus" and "Paranthropus". Bone tools have been discovered that were used during this period as well but these are rarely preserved in the archaeological record. The Stone Age is further subdivided by the types of stone tools in use.

The Stone Age is the first period in the three-age system frequently used in archaeology to divide the timeline of human technological prehistory into functional periods:
The Stone Age is contemporaneous with the evolution of the genus "Homo", with the possible exception of the early Stone Age, when species prior to "Homo" may have manufactured tools. According to the age and location of the current evidence, the cradle of the genus is the East African Rift System, especially toward the north in Ethiopia, where it is bordered by grasslands. The closest relative among the other living primates, the genus "Pan", represents a branch that continued on in the deep forest, where the primates evolved. The rift served as a conduit for movement into southern Africa and also north down the Nile into North Africa and through the continuation of the rift in the Levant to the vast grasslands of Asia.

Starting from about 4 million years ago (mya) a single biome established itself from South Africa through the rift, North Africa, and across Asia to modern China. This has been called "transcontinental 'savannahstan'" recently. Starting in the grasslands of the rift, "Homo erectus", the predecessor of modern humans, found an ecological niche as a tool-maker and developed a dependence on it, becoming a "tool equipped savanna dweller".

The oldest indirect evidence found of stone tool use is fossilised animal bones with tool marks; these are 3.4 million years old and were found in the Lower Awash Valley in Ethiopia. Archaeological discoveries in Kenya in 2015, identifying what may be the oldest evidence of hominin use of tools known to date, have indicated that "Kenyanthropus platyops" (a 3.2 to 3.5-million-year-old Pliocene hominin fossil discovered in Lake Turkana, Kenya in 1999) may have been the earliest tool-users known.

The oldest stone tools were excavated from the site of Lomekwi 3 in West Turkana, northwestern Kenya, and date to 3.3 million years old. Prior to the discovery of these "Lomekwian" tools, the oldest known stone tools had been found at several sites at Gona, Ethiopia, on sediments of the paleo-Awash River, which serve to date them. All the tools come from the Busidama Formation, which lies above a disconformity, or missing layer, which would have been from 2.9 to 2.7 mya. The oldest sites discovered to contain tools are dated to 2.6–2.55 mya. One of the most striking circumstances about these sites is that they are from the Late Pliocene, where prior to their discovery tools were thought to have evolved only in the Pleistocene. Excavators at the locality point out that:

The species who made the Pliocene tools remains unknown. Fragments of "Australopithecus garhi", "Australopithecus aethiopicus", and "Homo", possibly "Homo habilis", have been found in sites near the age of the Gona tools.

In July 2018, scientists reported the discovery in China of the known oldest stone tools outside Africa, estimated at 2.12 million years old.

Innovation of the technique of smelting ore is regarded as ending the Stone Age and beginning the Bronze Age. The first highly significant metal manufactured was bronze, an alloy of copper and tin or arsenic, each of which was smelted separately. The transition from the Stone Age to the Bronze Age was a period during which modern people could smelt copper, but did not yet manufacture bronze, a time known as the Copper Age (or more technically the Chalcolithic or Eneolithic, both meaning 'copper–stone'). The Chalcolithic by convention is the initial period of the Bronze Age. The Bronze Age was followed by the Iron Age.

The transition out of the Stone Age occurred between 6000 and 2500 BCE for much of humanity living in North Africa and Eurasia. The first evidence of human metallurgy dates to between the 6th and 5th millennia BCE in the archaeological sites of Majdanpek, Yarmovac, and Pločnik in modern-day Serbia (including a copper axe from 5500 BCE belonging to the Vinca culture); though not conventionally considered part of the Chalcolithic, this provides the earliest known example of copper metallurgy. Note the Rudna Glava mine in Serbia. Ötzi the Iceman, a mummy from about 3300 BCE, carried with him a copper axe and a flint knife.

In some regions, such as Sub-Saharan Africa, the Stone Age was followed directly by the Iron Age. The Middle East and Southeast Asian regions progressed past Stone Age technology around 6000 BCE. Europe, and the rest of Asia became post-Stone Age societies by about 4000 BCE. The proto-Inca cultures of South America continued at a Stone Age level until around 2000 BCE, when gold, copper, and silver made their entrance. The peoples of the Americas notably did not develop a widespread behavior of smelting bronze or iron after the Stone Age period, although the technology existed. Stone-tool manufacture continued even after the Stone Age ended in a given area. In Europe and North America, millstones were in use until well into the 20th century, and still are in many parts of the world.

The terms "Stone Age", "Bronze Age", and "Iron Age" are not intended to suggest that advancements and time periods in prehistory are only measured by the type of tool material, rather than, for example, social organization, food sources exploited, adaptation to climate, adoption of agriculture, cooking, settlement, and religion. Like pottery, the typology of the stone tools combined with the relative sequence of the types in various regions provide a chronological framework for the evolution of humanity and society. They serve as diagnostics of date, rather than characterizing the people or the society.

Lithic analysis is a major and specialised form of archaeological investigation. It involves measurement of stone tools to determine their typology, function and technologies involved. It includes scientific study of the lithic reduction of the raw materials and methods used to make the prehistoric artifacts that are discovered. Much of this study takes place in the laboratory in the presence of various specialists. In experimental archaeology, researchers attempt to create replica tools, to understand how they were made. Flintknappers are craftsmen who use sharp tools to reduce flintstone to flint tool.

In addition to lithic analysis, field prehistorians utilize a wide range of techniques derived from multiple fields. The work of archaeologists in determining the paleocontext and relative sequence of the layers is supplemented by the efforts of geologic specialists in identifying layers of rock developed or deposited over geologic time; of paleontological specialists in identifying bones and animals; of palynologists in discovering and identifying pollen, spores and plant species; of physicists and chemists in laboratories determining ages of materials by carbon-14, potassium-argon and other methods. Study of the Stone Age has never been limited to stone tools and archaeology, even though they are important forms of evidence. The chief focus of study has always been on the society and the living people who belonged to it.

Useful as it has been, the concept of the Stone Age has its limitations. The date range of this period is ambiguous, disputed, and variable, depending upon the region in question. While it is possible to speak of a general 'stone age' period for the whole of humanity, some groups never developed metal-smelting technology, and so remained in the so-called 'stone age' until they encountered technologically developed cultures. The term was innovated to describe the archaeological cultures of Europe. It may not always be the best in relation to regions such as some parts of the Indies and Oceania, where farmers or hunter-gatherers used stone for tools until European colonisation began.

Archaeologists of the late 19th and early 20th centuries CE, who adapted the three-age system to their ideas, hoped to combine cultural anthropology and archaeology in such a way that a specific contemporaneous tribe can be used to illustrate the way of life and beliefs of the people exercising a particular Stone-Age technology. As a description of people living today, the term "stone age" is controversial. The Association of Social Anthropologists discourages this use, asserting:To describe any living group as 'primitive' or 'Stone Age' inevitably implies that they are living representatives of some earlier stage of human development that the majority of humankind has left behind.

In the 1920s, South African archaeologists organizing the stone tool collections of that country observed that they did not fit the newly detailed Three-Age System. In the words of J. Desmond Clark,

It was early realized that the threefold division of culture into Stone, Bronze and Iron Ages adopted in the nineteenth century for Europe had no validity in Africa outside the Nile valley.

Consequently, they proposed a new system for Africa, the Three-stage System. Clark regarded the Three-age System as valid for North Africa; in sub-Saharan Africa, the Three-stage System was best. In practice, the failure of African archaeologists either to keep this distinction in mind, or to explain which one they mean, contributes to the considerable equivocation already present in the literature. There are in effect two Stone Ages, one part of the Three-age and the other constituting the Three-stage. They refer to one and the same artifacts and the same technologies, but vary by locality and time.

The three-stage system was proposed in 1929 by Astley John Hilary Goodwin, a professional archaeologist, and Clarence van Riet Lowe, a civil engineer and amateur archaeologist, in an article titled "Stone Age Cultures of South Africa" in the journal "Annals of the South African Museum". By then, the dates of the Early Stone Age, or Paleolithic, and Late Stone Age, or Neolithic ("neo" = new), were fairly solid and were regarded by Goodwin as absolute. He therefore proposed a relative chronology of periods with floating dates, to be called the Earlier and Later Stone Age. The Middle Stone Age would not change its name, but it would not mean Mesolithic.

The duo thus reinvented the Stone Age. In Sub-Saharan Africa, however, iron-working technologies were either invented independently or came across the Sahara from the north (see "iron metallurgy in Africa"). The Neolithic was characterized primarily by herding societies rather than large agricultural societies, and although there was copper metallurgy in Africa as well as bronze smelting, archaeologists do not currently recognize a separate Copper Age or Bronze Age. Moreover, the technologies included in those 'stages', as Goodwin called them, were not exactly the same. Since then, the original relative terms have become identified with the technologies of the Paleolithic and Mesolithic, so that they are no longer relative. Moreover, there has been a tendency to drop the comparative degree in favor of the positive: resulting in two sets of Early, Middle and Late Stone Ages of quite different content and chronologies.

By voluntary agreement, archaeologists respect the decisions of the Pan-African Congress on Prehistory, which meets every four years to resolve archaeological business brought before it. Delegates are actually international; the organization takes its name from the topic. Louis Leakey hosted the first one in Nairobi in 1947. It adopted Goodwin and Lowe's 3-stage system at that time, the stages to be called Early, Middle and Later.

The problem of the transitions in archaeology is a branch of the general philosophic continuity problem, which examines how discrete objects of any sort that are contiguous in any way can be presumed to have a relationship of any sort. In archaeology, the relationship is one of causality. If Period B can be presumed to descend from Period A, there must be a boundary between A and B, the A–B boundary. The problem is in the nature of this boundary. If there is no distinct boundary, then the population of A suddenly stopped using the customs characteristic of A and suddenly started using those of B, an unlikely scenario in the process of evolution. More realistically, a distinct border period, the A/B transition, existed, in which the customs of A were gradually dropped and those of B acquired. If transitions do not exist, then there is no proof of any continuity between A and B.

The Stone Age of Europe is characteristically in deficit of known transitions. The 19th and early 20th-century innovators of the modern three-age system recognized the problem of the initial transition, the "gap" between the Paleolithic and the Neolithic. Louis Leakey provided something of an answer by proving that man evolved in Africa. The Stone Age must have begun there to be carried repeatedly to Europe by migrant populations. The different phases of the Stone Age thus could appear there without transitions. The burden on African archaeologists became all the greater, because now they must find the missing transitions in Africa. The problem is difficult and ongoing.

After its adoption by the First Pan African Congress in 1947, the Three-Stage Chronology was amended by the Third Congress in 1955 to include a First Intermediate Period between Early and Middle, to encompass the Fauresmith and Sangoan technologies, and the Second Intermediate Period between Middle and Later, to encompass the Magosian technology and others. The chronologic basis for definition was entirely relative. With the arrival of scientific means of finding an absolute chronology, the two intermediates turned out to be will-of-the-wisps. They were in fact Middle and Lower Paleolithic. Fauresmith is now considered to be a facies of Acheulean, while Sangoan is a facies of Lupemban. Magosian is "an artificial mix of two different periods".

Once seriously questioned, the intermediates did not wait for the next Pan African Congress two years hence, but were officially rejected in 1965 (again on an advisory basis) by Burg Wartenstein Conference #29, "Systematic Investigation of the African Later Tertiary and Quaternary", a conference in anthropology held by the Wenner-Gren Foundation, at Burg Wartenstein Castle, which it then owned in Austria, attended by the same scholars that attended the Pan African Congress, including Louis Leakey and Mary Leakey, who was delivering a pilot presentation of her typological analysis of Early Stone Age tools, to be included in her 1971 contribution to "Olduvai Gorge", "Excavations in Beds I and II, 1960–1963."

However, although the intermediate periods were gone, the search for the transitions continued.

In 1859 Jens Jacob Worsaae first proposed a division of the Stone Age into older and younger parts based on his work with Danish kitchen middens that began in 1851. In the subsequent decades this simple distinction developed into the archaeological periods of today. The major subdivisions of the Three-age Stone Age cross two epoch boundaries on the geologic time scale:
The succession of these phases varies enormously from one region (and culture) to another.

The Paleolithic or Palaeolithic (from Greek: παλαιός, "palaios", "old"; and λίθος, "lithos", "stone" lit. "old stone", coined by archaeologist John Lubbock and published in 1865) is the earliest division of the Stone Age. It covers the greatest portion of humanity's time (roughly 99% of "human technological history", where "human" and "humanity" are interpreted to mean the genus "Homo"), extending from 2.5 or 2.6 million years ago, with the first documented use of stone tools by hominans such as "Homo habilis", to the end of the Pleistocene around 10,000 BCE. The Paleolithic era ended with the Mesolithic, or in areas with an early neolithisation, the Epipaleolithic.

At sites dating from the Lower Paleolithic Period (about 2,500,000 to 200,000 years ago), simple pebble tools have been found in association with the remains of what may have been the earliest human ancestors. A somewhat more sophisticated Lower Paleolithic tradition, known as the Chopper chopping-tool industry, is widely distributed in the Eastern Hemisphere. This tradition is thought to have been the work of the hominin species named Homo erectus. Although no such fossil tools have yet been found, it is believed that H. erectus probably made tools of wood and bone as well as stone.
About 700,000 years ago, a new Lower Paleolithic tool, the hand ax, appeared. The earliest European hand axes are assigned to the Abbevillian industry, which developed in northern France in the valley of the Somme River; a later, more refined hand-axe tradition is seen in the Acheulian industry, evidence of which has been found in Europe, Africa, the Middle East, and Asia. Some of the earliest known hand axes were found at Olduvai Gorge (Tanzania) in association with remains of H. erectus. Alongside the hand-axe tradition there developed a distinct and very different stone-tool industry, based on flakes of stone: special tools were made from worked (carefully shaped) flakes of flint. In Europe, the Clactonian industry is one example of a flake tradition. The early flake industries probably contributed to the development of the Middle Paleolithic flake tools of the Mousterian industry, which is associated with the remains of Neanderthal man.

The earliest documented stone tools have been found in eastern Africa, manufacturers unknown, at the 3.3 million year old site of Lomekwi 3 in Kenya. Better known are the later tools belonging to an industry known as Oldowan, after the type site of Olduvai Gorge in Tanzania.

The tools were formed by knocking pieces off a river pebble, or stones like it, with a hammerstone to obtain large and small pieces with one or more sharp edges. The original stone is called a core; the resultant pieces, flakes. Typically, but not necessarily, small pieces are detached from a larger piece, in which case the larger piece may be called the core and the smaller pieces the flakes. The prevalent usage, however, is to call all the results flakes, which can be confusing. A split in half is called bipolar flaking.

Consequently, the method is often called "core-and-flake". More recently, the tradition has been called "small flake" since the flakes were small compared to subsequent Acheulean tools.The essence of the Oldowan is the making and often immediate use of small flakes.

Another naming scheme is "Pebble Core Technology (PBC)":Pebble cores are ... artifacts that have been shaped by varying amounts of hard-hammer percussion.

Various refinements in the shape have been called choppers, discoids, polyhedrons, subspheroid, etc. To date no reasons for the variants have been ascertained:From a functional standpoint, pebble cores seem designed for no specific purpose.

However, they would not have been manufactured for no purpose:Pebble cores can be useful in many cutting, scraping or chopping tasks, but ... they are not particularly more efficient in such tasks than a sharp-edged rock.

The whole point of their utility is that each is a "sharp-edged rock" in locations where nature has not provided any. There is additional evidence that Oldowan, or Mode 1, tools were utilized in "percussion technology"; that is, they were designed to be gripped at the blunt end and strike something with the edge, from which use they were given the name of choppers. Modern science has been able to detect mammalian blood cells on Mode 1 tools at Sterkfontein, Member 5 East, in South Africa. As the blood must have come from a fresh kill, the tool users are likely to have done the killing and used the tools for butchering. Plant residues bonded to the silicon of some tools confirm the use to chop plants.

Although the exact species authoring the tools remains unknown, Mode 1 tools in Africa were manufactured and used predominantly by "Homo habilis". They cannot be said to have developed these tools or to have contributed the tradition to technology. They continued a tradition of yet unknown origin. As chimpanzees sometimes naturally use percussion to extract or prepare food in the wild, and may use either unmodified stones or stones that they have split, creating an Oldowan tool, the tradition may well be far older than its current record.

Towards the end of Oldowan in Africa a new species appeared over the range of "Homo habilis": "Homo erectus". The earliest "unambiguous" evidence is a whole cranium, KNM-ER 3733 (a find identifier) from Koobi Fora in Kenya, dated to 1.78 mya. An early skull fragment, KNM-ER 2598, dated to 1.9 mya, is considered a good candidate also. Transitions in paleoanthropology are always hard to find, if not impossible, but based on the "long-legged" limb morphology shared by "H. habilis" and "H. rudolfensis" in East Africa, an evolution from one of those two has been suggested.

The most immediate cause of the new adjustments appears to have been an increasing aridity in the region and consequent contraction of parkland savanna, interspersed with trees and groves, in favor of open grassland, dated 1.8–1.7 mya. During that transitional period the percentage of grazers among the fossil species increased from 15–25% to 45%, dispersing the food supply and requiring a facility among the hunters to travel longer distances comfortably, which "H. erectus" obviously had. The ultimate proof is the "dispersal" of "H. erectus" "across much of Africa and Asia, substantially before the development of the Mode 2 technology and use of fire ..." "H. erectus" carried Mode 1 tools over Eurasia.

According to the current evidence (which may change at any time) Mode 1 tools are documented from about 2.6 mya to about 1.5 mya in Africa, and to 0.5 mya outside of it. The genus Homo is known from "H. habilis" and "H. rudolfensis" from 2.3 to 2.0 mya, with the latest habilis being an upper jaw from Koobi Fora, Kenya, from 1.4 mya. "H. erectus" is dated 1.8–0.6 mya.

According to this chronology Mode 1 was inherited by "Homo" from unknown Hominans, probably "Australopithecus" and "Paranthropus", who must have continued on with Mode 1 and then with Mode 2 until their extinction no later than 1.1 mya. Meanwhile, living contemporaneously in the same regions "H. habilis" inherited the tools around 2.3 mya. At about 1.9 mya "H. erectus" came on stage and lived contemporaneously with the others. Mode 1 was now being shared by a number of Hominans over the same ranges, presumably subsisting in different niches, but the archaeology is not precise enough to say which.

Tools of the Oldowan tradition first came to archaeological attention in Europe, where, being intrusive and not well defined, compared to the Acheulean, they were puzzling to archaeologists. The mystery would be elucidated by African archaeology at Olduvai, but meanwhile, in the early 20th century, the term "Pre-Acheulean" came into use in climatology. C.E.P, Brooks, a British climatologist working in the United States, used the term to describe a "chalky boulder clay" underlying a layer of gravel at Hoxne, central England, where Acheulean tools had been found. Whether any tools would be found in it and what type was not known. Hugo Obermaier, a contemporary German archaeologist working in Spain, quipped:Unfortunately, the stage of human industry which corresponds to these deposits cannot be positively identified. All we can say is that it is pre-Acheulean. This uncertainty was clarified by the subsequent excavations at Olduvai; nevertheless, the term is still in use for pre-Acheulean contexts, mainly across Eurasia, that are yet unspecified or uncertain but with the understanding that they are or will turn out to be pebble-tool.

There are ample associations of Mode 2 with "H. erectus" in Eurasia. "H. erectus" – Mode 1 associations are scantier but they do exist, especially in the Far East. One strong piece of evidence prevents the conclusion that only "H. erectus" reached Eurasia: at Yiron, Israel, Mode 1 tools have been found dating to 2.4 mya, about 0.5 my earlier than the known "H. erectus" finds. If the date is correct, either another Hominan preceded "H. erectus" out of Africa or the earliest "H. erectus" has yet to be found.

After the initial appearance at Gona in Ethiopia at 2.7 mya, pebble tools date from 2.0 mya at Sterkfontein, Member 5, South Africa, and from 1.8 mya at El Kherba, Algeria, North Africa. The manufacturers had already left pebble tools at Yiron, Israel, at 2.4 mya, Riwat, Pakistan, at 2.0 mya, and Renzidong, South China, at over 2 mya. The identification of a fossil skull at Mojokerta, Pernung Peninsula on Java, dated to 1.8 mya, as "H. erectus", suggests that the African finds are not the earliest to be found in Africa, or that, in fact, erectus did not originate in Africa after all but on the plains of Asia. The outcome of the issue waits for more substantial evidence. Erectus was found also at Dmanisi, Georgia, from 1.75 mya in association with pebble tools.

Pebble tools are found the latest first in southern Europe and then in northern. They begin in the open areas of Italy and Spain, the earliest dated to 1.6 mya at Pirro Nord, Italy. The mountains of Italy are rising at a rapid rate in the framework of geologic time; at 1.6 mya they were lower and covered with grassland (as much of the highlands still are). Europe was otherwise mountainous and covered over with dense forest, a formidable terrain for warm-weather savanna dwellers. Similarly there is no evidence that the Mediterranean was passable at Gibraltar or anywhere else to "H. erectus" or earlier hominans. They might have reached Italy and Spain along the coasts.

In northern Europe pebble tools are found earliest at Happisburgh, United Kingdom, from 0.8 mya. The last traces are from Kent's Cavern, dated 0.5 mya. By that time "H. erectus" is regarded as having been extinct; however, a more modern version apparently had evolved, "Homo heidelbergensis", who must have inherited the tools. He also explains the last of the Acheulean in Germany at 0.4 mya.

In the late 19th and early 20th centuries archaeologists worked on the assumptions that a succession of Hominans and cultures prevailed, that one replaced another. Today the presence of multiple hominans living contemporaneously near each other for long periods is accepted as proved true; moreover, by the time the previously assumed "earliest" culture arrived in northern Europe, the rest of Africa and Eurasia had progressed to the Middle and Upper Palaeolithic, so that across the earth all three were for a time contemporaneous. In any given region there was a progression from Oldowan to Acheulean, Lower to Upper, no doubt.

The end of Oldowan in Africa was brought on by the appearance of Acheulean, or Mode 2, stone tools. The earliest known instances are in the 1.7–1.6 mya layer at Kokiselei, West Turkana, Kenya. At Sterkfontein, South Africa, they are in Member 5 West, 1.7–1.4 mya. The 1.7 is a fairly certain, fairly standard date. Mode 2 is often found in association with "H. erectus". It makes sense that the most advanced tools should have been innovated by the most advanced Hominan; consequently, they are typically given credit for the innovation.

A Mode 2 tool is a biface consisting of two concave surfaces intersecting to form a cutting edge all the way around, except in the case of tools intended to feature a point. More work and planning go into the manufacture of a Mode 2 tool. The manufacturer hits a slab off a larger rock to use as a blank. Then large flakes are struck off the blank and worked into bifaces by hard-hammer percussion on an anvil stone. Finally the edge is retouched: small flakes are hit off with a bone or wood soft hammer to sharpen or resharpen it. The core can be either the blank or another flake. Blanks are ported for manufacturing supply in places where nature has provided no suitable stone.

Although most Mode 2 tools are easily distinguished from Mode 1, there is a close similarity of some Oldowan and some Acheulean, which can lead to confusion. Some Oldowan tools are more carefully prepared to form a more regular edge. One distinguishing criterion is the size of the flakes. In contrast to the Oldowan "small flake" tradition, Acheulean is "large flake:" "The primary technological distinction remaining between Oldowan and the Acheulean is the preference for large flakes (>10 cm) as blanks for making large cutting tools (handaxes and cleavers) in the Acheulean." "Large Cutting Tool (LCT)" has become part of the standard terminology as well.

In North Africa, the presence of Mode 2 remains a mystery, as the oldest finds are from Thomas Quarry in Morocco at 0.9 mya. Archaeological attention, however, shifts to the Jordan Rift Valley, an extension of the East African Rift Valley (the east bank of the Jordan is slowly sliding northward as East Africa is thrust away from Africa). Evidence of use of the Nile Valley is in deficit, but Hominans could easily have reached the palaeo-Jordan river from Ethiopia along the shores of the Red Sea, one side or the other. A crossing would not have been necessary, but it is more likely there than over a theoretical but unproven land bridge through either Gibraltar or Sicily.

Meanwhile, Acheulean went on in Africa past the 1.0 mya mark and also past the extinction of "H. erectus" there. The last Acheulean in East Africa is at Olorgesailie, Kenya, dated to about 0.9 mya. Its owner was still "H. erectus", but in South Africa, Acheulean at Elandsfontein, 1.0–0.6 mya, is associated with Saldanha man, classified as "H. heidelbergensis", a more advanced, but not yet modern, descendant most likely of "H. erectus". The Thoman Quarry Hominans in Morocco similarly are most likely Homo rhodesiensis, in the same evolutionary status as "H. heidelbergensis".
Mode 2 is first known out of Africa at 'Ubeidiya, Israel, a site now on the Jordan River, then frequented over the long term (hundreds of thousands of years) by Homo on the shore of a variable-level palaeo-lake, long since vanished. The geology was created by successive "transgression and regression" of the lake resulting in four cycles of layers. The tools are located in the first two, Cycles Li (Limnic Inferior) and Fi (Fluviatile Inferior), but mostly in Fi. The cycles represent different ecologies and therefore different cross-sections of fauna, which makes it possible to date them. They appear to be the same faunal assemblages as the Ferenta Faunal Unit in Italy, known from excavations at Selvella and Pieterfitta, dated to 1.6–1.2 mya.

At 'Ubeidiya the marks on the bones of the animal species found there indicate that the manufacturers of the tools butchered the kills of large predators, an activity that has been termed "scavenging". There are no living floors, nor did they process bones to obtain the marrow. These activities cannot be understood therefore as the only or even the typical economic activity of Hominans. Their interests were selective: they were primarily harvesting the meat of Cervids, which is estimated to have been available without spoiling for up to four days after the kill.

The majority of the animals at the site were of "Palaearctic biogeographic origin". However, these overlapped in range on 30–60% of "African biogeographic origin". The biome was Mediterranean, not savanna. The animals were not passing through; there was simply an overlap of normal ranges. Of the Hominans, "H. erectus" left several cranial fragments. Teeth of undetermined species may have been "H. ergaster". The tools are classified as "Lower Acheulean" and "Developed Oldowan". The latter is a disputed classification created by Mary Leakey to describe an Acheulean-like tradition in Bed II at Olduvai. It is dated 1.53–1.27 mya. The date of the tools therefore probably does not exceed 1.5 mya; 1.4 is often given as a date. This chronology, which is definitely later than in Kenya, supports the "out of Africa" hypothesis for Acheulean, if not for the Hominans.

From Southwest Asia, as the Levant is now called, the Acheulean extended itself more slowly eastward, arriving at Isampur, India, about 1.2 mya. It does not appear in China and Korea until after 1mya and not at all in Indonesia. There is a discernible boundary marking the furthest extent of the Acheulean eastward before 1 mya, called the Movius Line, after its proposer, Hallam L. Movius. On the east side of the line the small flake tradition continues, but the tools are additionally worked Mode 1, with flaking down the sides. In Athirampakkam at Chennai in Tamil Nadu the Acheulean age started at 1.51 mya and it is also prior than North India and Europe.

The cause of the Movius Line remains speculative, whether it represents a real change in technology or a limitation of archeology, but after 1 mya evidence not available to Movius indicates the prevalence of Acheulean. For example, the Acheulean site at Bose, China, is dated 0.803±3K mya. The authors of this chronologically later East Asian Acheulean remain unknown, as does whether it evolved in the region or was brought in.

There is no named boundary line between Mode 1 and Mode 2 on the west; nevertheless, Mode 2 is equally late in Europe as it is in the Far East. The earliest comes from a rock shelter at Estrecho de Quípar in Spain, dated to greater than 0.9 mya. Teeth from an undetermined Hominan were found there also. The last Mode 2 in Southern Europe is from a deposit at Fontana Ranuccio near Anagni in Italy dated to 0.45 mya, which is generally linked to "Homo cepranensis", a "late variant of "H. erectus"", a fragment of whose skull was found at Ceprano nearby, dated 0.46 mya.

This period is best known as the era during which the Neanderthals lived in Europe and the Near East (c. 300,000–28,000 years ago). Their technology is mainly the Mousterian, but Neanderthal physical characteristics have been found also in ambiguous association with the more recent Châtelperronian archeological culture in Western Europe and several local industries like the Szeletian in Eastern Europe/Eurasia. There is no evidence for Neanderthals in Africa, Australia or the Americas.

Neanderthals nursed their elderly and practised ritual burial indicating an organised society. The earliest evidence (Mungo Man) of settlement in Australia dates to around 40,000 years ago when modern humans likely crossed from Asia by island-hopping. Evidence for symbolic behavior such as body ornamentation and burial is ambiguous for the Middle Paleolithic and still subject to debate. The Bhimbetka rock shelters exhibit the earliest traces of human life in India, some of which are approximately 30,000 years old.

From 50,000 to 10,000 years ago in Europe, the Upper Paleolithic ends with the end of the Pleistocene and onset of the Holocene era (the end of the last ice age). Modern humans spread out further across the Earth during the period known as the Upper Paleolithic.

The Upper Paleolithic is marked by a relatively rapid succession of often complex stone artifact technologies and a large increase in the creation of art and personal ornaments. During period between 35 and 10 kya evolved: from 38 to 30 kya Châtelperronian, 40–28 Aurignacian, 28–22 Gravettian, 22–17 Solutrean, and 18–10 Magdalenian. All of these industries except the Châtelperronian are associated with anatomically modern humans. Authorship of the Châtelperronian is still the subject of much debate.

Most scholars date the arrival of humans in Australia at 40,000 to 50,000 years ago, with a possible range of up to 125,000 years ago. The earliest anatomically modern human remains found in Australia (and outside of Africa) are those of Mungo Man; they have been dated at 42,000 years old.

The Americas were colonised via the Bering land bridge which was exposed during this period by lower sea levels. These people are called the Paleo-Indians, and the earliest accepted dates are those of the Clovis culture sites, some 13,500 years ago. Globally, societies were hunter-gatherers but evidence of regional identities begins to appear in the wide variety of stone tool types being developed to suit very different environments.

The period starting from the end of the last ice age, 10,000 years ago, to around 6,000 years ago was characterized by rising sea levels and a need to adapt to a changing environment and find new food sources. The development of Mode 5 (microlith) tools began in response to these changes. They were derived from the previous Paleolithic tools, hence the term Epipaleolithic, or were intermediate between the Paleolithic and the Neolithic, hence the term Mesolithic (Middle Stone Age), used for parts of Eurasia, but not outside it. The choice of a word depends on exact circumstances and the inclination of the archaeologists excavating the site. Microliths were used in the manufacture of more efficient composite tools, resulting in an intensification of hunting and fishing and with increasing social activity the development of more complex settlements, such as Lepenski Vir. Domestication of the dog as a hunting companion probably dates to this period.

The earliest known battle occurred during the Mesolithic period at a site in Egypt known as Cemetery 117.

The Neolithic, or New Stone Age, was approximately characterized by the adoption of agriculture. The shift from food gathering to food producing, in itself one of the most revolutionary changes in human history, was accompanied by the so-called Neolithic Revolution: the development of pottery, polished stone tools, and construction of more complex, larger settlements such as Göbekli Tepe and Çatal Hüyük. Some of these features began in certain localities even earlier, in the transitional Mesolithic. The first Neolithic cultures started around 7000 BCE in the fertile crescent and spread concentrically to other areas of the world; however, the Near East was probably not the only nucleus of agriculture, the cultivation of maize in Meso-America and of rice in the Far East being others.

Due to the increased need to harvest and process plants, ground stone and polished stone artifacts became much more widespread, including tools for grinding, cutting, and chopping. Skara Brae located in Orkney off Scotland is one of Europe's best examples of a Neolithic village. The community contains stone beds, shelves and even an indoor toilet linked to a stream. The first large-scale constructions were built, including settlement towers and walls, e.g., Jericho (Tell es-Sultan) and ceremonial sites, e.g.: Stonehenge. The Ġgantija temples of Gozo in the Maltese archipelago are the oldest surviving free standing structures in the world, erected c. 3600–2500 BCE. The earliest evidence for established trade exists in the Neolithic with newly settled people importing exotic goods over distances of many hundreds of miles.

These facts show that there were sufficient resources and co-operation to enable large groups to work on these projects. To what extent this was a basis for the development of elites and social hierarchies is a matter of ongoing debate. Although some late Neolithic societies formed complex stratified chiefdoms similar to Polynesian societies such as the Ancient Hawaiians, based on the societies of modern tribesmen at an equivalent technological level, most Neolithic societies were relatively simple and egalitarian. A comparison of art in the two ages leads some theorists to conclude that Neolithic cultures were noticeably more hierarchical than the Paleolithic cultures that preceded them.

The Early Stone Age in Africa is not to be identified with "Old Stone Age", a translation of Paleolithic, or with Paleolithic, or with the "Earlier Stone Age" that originally meant what became the Paleolithic and Mesolithic. In the initial decades of its definition by the Pan-African Congress of Prehistory, it was parallel in Africa to the Upper and Middle Paleolithic. However, since then Radiocarbon dating has shown that the Middle Stone Age is in fact contemporaneous with the Middle Paleolithic. The Early Stone Age therefore is contemporaneous with the Lower Paleolithic and happens to include the same main technologies, Oldowan and Acheulean, which produced Mode 1 and Mode 2 stone tools respectively. A distinct regional term is warranted, however, by the location and chronology of the sites and the exact typology.

The Middle Stone Age was a period of African prehistory between Early Stone Age and Late Stone Age. It began around 300,000 years ago and ended around 50,000 years ago. It is considered as an equivalent of European Middle Paleolithic. It is associated with anatomically modern or almost modern "Homo sapiens". Early physical evidence comes from Omo and Herto, both in Ethiopia and dated respectively at c. 195 ka and at c. 160 ka.

The Later Stone Age (LSA, sometimes also called the Late Stone Age) refers to a period in African prehistory. Its beginnings are roughly contemporaneous with the European Upper Paleolithic. It lasts until historical times and this includes cultures corresponding to Mesolithic and Neolithic in other regions.

Stone tools were made from a variety of stones. For example, flint and chert were shaped (or "chipped") for use as cutting tools and weapons, while basalt and sandstone were used for ground stone tools, such as quern-stones. Wood, bone, shell, antler (deer) and other materials were widely used, as well. During the most recent part of the period, sediments (such as clay) were used to make pottery. Agriculture was developed and certain animals were domesticated as well.

Some species of non-primates are able to use stone tools, such as the sea otter, which breaks abalone shells with them. Primates can both use and manufacture stone tools. This combination of abilities is more marked in apes and men, but only men, or more generally Hominans, depend on tool use for survival. The key anatomical and behavioral features required for tool manufacture, which are possessed only by Hominans, are the larger thumb and the ability to hold by means of an assortment of grips.

Food sources of the Palaeolithic hunter-gatherers were wild plants and animals harvested from the environment. They liked animal organ meats, including the livers, kidneys and brains. Large seeded legumes were part of the human diet long before the agricultural revolution, as is evident from archaeobotanical finds from the Mousterian layers of Kebara Cave, in Israel.<ref name="doi10.1016/j.jas.2004.11.006"></ref> Moreover, recent evidence indicates that humans processed and consumed wild cereal grains as far back as 23,000 years ago in the Upper Paleolithic.

Near the end of the Wisconsin glaciation, 15,000 to 9,000 years ago, mass extinction of Megafauna such as the woolly mammoth occurred in Asia, Europe, North America and Australia. This was the first Holocene extinction event. It possibly forced modification in the dietary habits of the humans of that age and with the emergence of agricultural practices, plant-based foods also became a regular part of the diet. A number of factors have been suggested for the extinction: certainly over-hunting, but also deforestation and climate change. The net effect was to fragment the vast ranges required by the large animals and extinguish them piecemeal in each fragment.

Around 2 million years ago, "Homo habilis" is believed to have constructed the first man-made structure in East Africa, consisting of simple arrangements of stones to hold branches of trees in position. A similar stone circular arrangement believed to be around 380,000 years old was discovered at Terra Amata, near Nice, France. (Concerns about the dating have been raised, see Terra Amata). Several human habitats dating back to the Stone Age have been discovered around the globe, including:

Prehistoric art is visible in the artifacts. Prehistoric music is inferred from found instruments, while parietal art can be found on rocks of any kind. The latter are petroglyphs and rock paintings. The art may or may not have had a religious function.

Petroglyphs appeared in the Neolithic. A Petroglyph is an intaglio abstract or symbolic image engraved on natural stone by various methods, usually by prehistoric peoples. They were a dominant form of pre-writing symbols. Petroglyphs have been discovered in different parts of the world, including Australia (Sydney rock engravings), Asia (Bhimbetka, India), North America (Death Valley National Park), South America (Cumbe Mayo, Peru), and Europe (Finnmark, Norway).

In paleolithic times, mostly animals were painted, in theory ones that were used as food or represented strength, such as the rhinoceros or large cats (as in the Chauvet Cave). Signs such as dots were sometimes drawn. Rare human representations include handprints and half-human/half-animal figures. The Cave of Chauvet in the Ardèche "département", France, contains the most important cave paintings of the paleolithic era, dating from about 36,000 BCE. The Altamira cave paintings in Spain were done 14,000 to 12,000 BCE and show, among others, bisons. The hall of bulls in Lascaux, Dordogne, France, dates from about 15,000 to 10,000 BCE.

The meaning of many of these paintings remains unknown. They may have been used for seasonal rituals. The animals are accompanied by signs that suggest a possible magic use. Arrow-like symbols in Lascaux are sometimes interpreted as calendar or almanac use, but the evidence remains interpretative.

Some scenes of the Mesolithic, however, can be typed and therefore, judging from their various modifications, are fairly clear. One of these is the battle scene between organized bands of archers. For example, "the marching Warriors", a rock painting at Cingle de la Mola, Castellón in Spain, dated to about 7,000–4,000 BCE, depicts about 50 bowmen in two groups marching or running in step toward each other, each man carrying a bow in one hand and a fistful of arrows in the other. A file of five men leads one band, one of whom is a figure with a "high crowned hat".

In other scenes elsewhere, the men wear head-dresses and knee ornaments but otherwise fight nude. Some scenes depict the dead and wounded, bristling with arrows. One is reminded of Ötzi the Iceman, a Copper Age mummy revealed by an Alpine melting glacier, who collapsed from loss of blood due to an arrow wound in the back.

Modern studies and the in-depth analysis of finds dating from the Stone Age indicate certain rituals and beliefs of the people in those prehistoric times. It is now believed that activities of the Stone Age humans went beyond the immediate requirements of procuring food, body coverings, and shelters. Specific rites relating to death and burial were practiced, though certainly differing in style and execution between cultures. 

The image of the caveman is commonly associated with the Stone Age. For example, a 2003 documentary series showing the evolution of humans through the Stone Age was called "Walking with Cavemen", but only the last programme showed humans living in caves. While the idea that human beings and dinosaurs coexisted is sometimes portrayed in popular culture in cartoons, films and computer games, such as "The Flintstones", "One Million Years B.C." and "Chuck Rock", the notion of hominids and non-avian dinosaurs co-existing is not supported by any scientific evidence.

Other depictions of the Stone Age include the best-selling "Earth's Children" series of books by Jean M. Auel, which are set in the Paleolithic and are loosely based on archaeological and anthropological findings.

The 1981 film "Quest for Fire" by Jean-Jacques Annaud tells the story of a group of early homo sapiens searching for their lost fire. A 21st-century series, "Chronicles of Ancient Darkness" by Michelle Paver tells of two New Stone Age children fighting to fulfil a prophecy and save their clan.



</doc>
<doc id="29222" url="https://en.wikipedia.org/wiki?curid=29222" title="Sam Loyd">
Sam Loyd

Samuel Loyd (January 30, 1841 – April 10, 1911), born in Philadelphia and raised in New York City, was an American chess player, chess composer, puzzle author, and recreational mathematician.

As a chess composer, he authored a number of chess problems, often with interesting themes. At his peak, Loyd was one of the best chess players in the US, and was ranked 15th in the world, according to chessmetrics.com.

He played in the strong Paris 1867 chess tournament (won by Ignatz von Kolisch) with little success, placing near the bottom of the field.

Following his death, his book "Cyclopedia of 5000 Puzzles" was published (1914) by his son. His son, named after his father, dropped the "Jr" from his name and started publishing reprints of his father's puzzles.
Loyd (senior) was inducted into the US Chess Hall of Fame in 1987.

Loyd is widely acknowledged as one of America's great puzzle-writers and popularizers, often mentioned as "the" greatest. Martin Gardner featured Loyd in his August 1957 Mathematical Games column in Scientific American and called him "America's greatest puzzler". In 1898 "The Strand" dubbed him "the prince of puzzlers". As a chess problemist, his composing style is distinguished by wit and humour.

However, he is also known for lies and self-promotion, and criticized on these grounds—Martin Gardner's assessment continues "but also obviously a hustler". Canadian puzzler Mel Stover called Loyd "an old reprobate", and Matthew Costello called him "puzzledom's greatest celebrity ... popularizer, genius", but also a "huckster" and "fast-talking snake oil salesman".

He collaborated with puzzler Henry Dudeney for a while, but Dudeney broke off the correspondence and accused Loyd of stealing his puzzles and publishing them under his own name. Dudeney despised Loyd so intensely he equated him with the Devil.

Loyd claimed from 1891 until his death in 1911 that he invented the 15 puzzle, for example writing in the "Cyclopedia of Puzzles" (published 1914), p. 235: "The older inhabitants of Puzzleland will remember how in the early seventies I drove the entire world crazy over a little box of movable pieces which became known as the '14–15 Puzzle'." This is false as Loyd had nothing to do with the invention or popularity of the puzzle, and the craze was in the early 1880s, not the early 1870s. The craze had ended by July 1880 and Loyd's first article on the subject was not published until 1896. Loyd first claimed in 1891 that he had invented the puzzle, and continued to do so until his death. The actual inventor was Noyes Chapman, who applied for a patent in March 1880.

An enthusiast of Tangram puzzles, Loyd popularised them with "The Eighth Book Of Tan", a book of seven hundred unique Tangram designs and a fanciful history of the origin of the Tangram, claiming that the puzzle was invented 4,000 years ago by a god named Tan. This was presented as true and has been described as "Sam Loyd's Most Successful Hoax".

One of his best known chess problems is the following, called "Excelsior" by Loyd after the poem by Henry Wadsworth Longfellow. White is to move and checkmate black in five moves against any defense:

Loyd bet a friend that he could not pick a piece that "didn't" give mate in the main line, and when it was published in 1861 it was with the stipulation that white mates with "the least likely piece or pawn".
One of the most famous chess problems by Loyd. He wrote on this problem: "The originality of the problem is due to the White King being placed in absolute safety, and yet coming out on a reckless career, with no immediate threat and in the face of innumerable checks".

This problem was originally published in 1859. The story involves an incident during the siege of Charles XII of Sweden by the Turks at Bender in 1713. "Charles beguiled this period by means of drills and chess, and used frequently to play with his minister, Christian Albert Grosthusen, some of the contests being mentioned by Voltaire. One day while so engaged, the game had advanced to this stage, and Charles (White) had just announced mate in three."

"Scarcely had he uttered the words, when a Turkish bullet, shattering the window, dashed the White knight off of the board in fragments. Grothusen started violently, but Charles, with utmost coolness, begged him to put back the other knight and work out the mate, observing that it was pretty enough. But another glance at the board made Charles smile. We do not need the knight. I can give it to you and still mate in four!" 

Who would believe it, he had scarcely spoken when another bullet flew across the room, and the pawn at h2 shared the fate of the knight. Grothusen turned pale. "You have our good friends the Turks with you," said the king unconcerned, "it can scarcely be expected that I should contend against such odds; but let me see if I can dispense with that unlucky pawn. I have it!" he shouted with a tremendous laugh, "I have great pleasure in informing you that there is undoubtedly a mate in 5." 

In 1900, Friedrich Amelung pointed out that in the original position, if the first bullet had struck the rook instead of the knight, Charles would still have a mate in six.

In 2003, ChessBase posted a fifth variation, attributed to Brian Stewart. After the first bullet took out the knight, if the second had removed the g-pawn rather than the h-pawn, Charles would be able to mate in ten.

One of Loyd's notable puzzles was the "Trick Donkeys". It was based on a similar puzzle involving dogs published in 1857. In the problem, the solver must cut the drawing along the dotted lines and rearrange the three pieces so that the riders appear to be riding the donkeys.
This is one of Sam Loyd's most famous puzzles, first printed in the "New York Journal and Advertiser", April 24, 1898 (as far as available evidence indicates). Loyd's original instructions were to: Start from that heart in the center and go three steps in a straight line in any one of the eight directions, north, south, east or west, or on the bias, as the ladies say, northeast, northwest, southeast or southwest. When you have gone three steps in a straight line, you will reach a square with a number on it, which indicates the second day's journey, as many steps as it tells, in a straight line in any of the eight directions. From this new point when reached, march on again according to the number indicated, and continue on, following the requirements of the numbers reached, until you come upon a square with a number which will carry you just one step beyond the border, when you are supposed to be out of the woods and can holler all you want, as you will have solved the puzzle.




Chess

Interactive puzzle

Books


</doc>
<doc id="29228" url="https://en.wikipedia.org/wiki?curid=29228" title="Shiba Inu">
Shiba Inu

The is a Japanese breed of hunting dog. A small-to-medium breed, it is the smallest of the six original and distinct spitz breeds of dog native to Japan. 

A small, alert and agile dog that copes very well with mountainous terrain and hiking trails, the Shiba Inu was originally bred for hunting. It looks similar to and is often mistaken for other Japanese dog breeds like the Akita Inu or Hokkaido, but the Shiba Inu is a different breed with a distinct blood line, temperament, and smaller size than other Japanese dog breeds.

The Shiba's frame is compact with well-developed muscles. Males are at the withers. Females are . The preferred size is the middle of the range for each sex. Average weight at preferred size is approximately for males, for females. Bones are moderate.

The Shiba is double coated, with the outer coat being stiff and straight and the undercoat soft and thick. Fur is short and even on the fox-like face, ears, and legs. Guard hairs stand off the body and are about long at the withers. The purpose of the guard hairs is to protect their underlying skin and to repel rain or snow. Tail hair is slightly longer and stands open in a brush. Their tails are a defining characteristic and makes them stand apart from other dog breeds. Their tails help to protect them from the harsh winter weather. When they sleep, Shiba Inus curl up and use their tails to shield their face and nose in order to protect their sensitive areas from the cold. Shibas may be red, orange, yellow, black and tan, or sesame (red with black-tipped hairs), with a cream, buff, or grey undercoat. They may also be white (cream), though this color is considered a "major fault" by the American Kennel Club and should never be intentionally bred in a show dog, as the required markings known as are not visible; "Urajiro" literally translates to "underside white". Conversely, a white (cream) coat is perfectly acceptable according to the British Kennel Club breed standard.

The "urajiro" (cream to white ventral color) is required in the following areas on all coat colors: on the sides of the muzzle, on the cheeks, inside the ears, on the underjaw and upper throat inside of legs, on the abdomen, around the vent and the ventral side of the tail. On reds: commonly on the throat, forechest, and chest. On blacks and sesames: commonly as a triangular mark on both sides of the forechest.

Shibas tend to exhibit an independent nature. From the Japanese breed standard:

The dog has a spirited boldness and is fiercely proud with a good nature and a feeling of artlessness. The Shiba is able to move quickly with nimble, elastic steps.

The terms , , and have subtle interpretations that have been the subject of much commentary.

The Shiba is a relatively fastidious breed and feels the need to maintain itself in a clean state. They can often be seen licking their paws and legs, much as cats do. They generally go out of their way to keep their coats clean. Because of their fastidious and proud nature, Shiba puppies are easy to housebreak and in many cases will housebreak themselves. Having their owner simply place them outside after meal times and naps is generally enough to teach the Shiba the appropriate method of toileting.

A distinguishing characteristic of the breed is the so-called "shiba scream". When sufficiently provoked or unhappy, the dog will produce a loud, high-pitched scream. This can occur when attempting to handle the dog in a way that it deems unacceptable. The animal may also emit a very similar sound during periods of great joy, such as the return of the owner after an extended absence, or the arrival of a favored human guest.

The Shiba Inu has been identified as a basal breed that predates the emergence of the modern breeds in the 19th century.

Originally, the Shiba Inu was bred to hunt and flush small game, such as birds and rabbit. Shiba lived in the mountainous areas of the Chūbu region. During the Meiji Restoration, western dog breeds were imported and crosses between these and native Japanese breeds became popular. From 1912 to 1926, almost no pure Shiba remained. From around 1928, hunters and intellectuals began to show interest in the protection of the remaining pure Shiba; however, despite efforts to preserve the breed, the Shiba nearly became extinct during World War II due to a combination of food shortage and a post-war distemper epidemic. All subsequent dogs were bred from the only three surviving bloodlines. These bloodlines were the Shinshu Shiba from Nagano Prefecture, the Mino Shiba from the former Mino Province in the south of present-day Gifu Prefecture, and the San'in Shiba from Tottori and Shimane Prefectures.

The Shinshu Shibas possessed a solid undercoat, with a dense layer of guard-hairs, and were small and red in color. The Mino Shibas tended to have thick, prick ears, and possessed a sickle tail, rather than the common curled tail found on most modern Shibas. The San'in Shibas were larger than most modern shibas, and tended to be black, without the common tan and white accents found on modern black-and-tan shibas. When the study of Japanese dogs was formalized in the early and mid-20th century, these three strains were combined into one overall breed, the Shiba Inu. The first Japanese breed standard for the Shiba, the Nippo Standard, was published in 1934. In December 1936, the Shiba Inu was recognized as a Natural Monument of Japan through the Cultural Properties Act, largely due to the efforts of Nippo (Nihon Ken Hozonkai), the Association for the Preservation of the Japanese Dog.

In 1954, an armed service family brought the first Shiba Inu to the United States. In 1979, the first recorded litter was born in the United States. The Shiba was recognized by the American Kennel Club in 1992 and added to the AKC Non-Sporting Group in 1993. It is now primarily kept as a pet both in Japan and abroad. According to the American Kennel Club, the Shiba Inu is the number one companion dog in Japan. In the United States, the growing popularity of the Shiba Inu is evident as the American Kennel Club Registration Statistics ranked the breed in 44th place in 2016; a rise from 50th place in 2012.

Overall, the Shiba Inu is a healthy dog breed. Health conditions known to affect this breed are allergies, glaucoma, cataracts, hip dysplasia, entropion, and luxating patella. Periodic joint examinations are recommended throughout the dog's life. Eye tests should be performed yearly as eye problems can develop over time. By two years of age, Shiba Inus may be considered fully free from joint problems, if none have been discovered, since at this age the skeleton is fully developed.

As with most dog breeds, Shibas should be walked or otherwise exercised daily.

Their average life expectancy is from 12 to 15 years. Exercise, especially daily walks, is preferred for this breed to live a long and healthy life. The oldest known Shiba, Pusuke, died at age 26 in early December 2011. Pusuke was the oldest dog alive at the time and lived three years less than the world record for longest living dog.

These dogs are very clean, so grooming needs will likely be minimal. They naturally tend to hate to be wet or bathed, thus, it is very important to start accustomed when they are young. A Shiba Inu's coat is coarse; short to medium length with the outer coat being long, and is naturally waterproof so there is little need for regular bathing. They also have a thick undercoat that can protect them from temperatures well below freezing. However, shedding, also known as blowing coat, can be a nuisance. Shedding is heaviest during the seasonal change and particularly during the summer season, but daily brushing can temper this problem. It is recommended that owners never shave or cut the coat of a Shiba Inu, as the coat is needed to protect them from both cold and hot temperatures.


</doc>
<doc id="29229" url="https://en.wikipedia.org/wiki?curid=29229" title="Slot machine">
Slot machine

A slot machine (American English), known variously as a fruit machine (British English), puggy (Scottish English), the slots (Canadian English and American English), poker machine/pokies (Australian English and New Zealand English), fruities (British English) or slots (American English), is a gambling machine that creates a game of chance for its customers. Slot machines are also known pejoratively as one-armed bandits because of the large mechanical levers affixed to the sides of early mechanical machines and the games' ability to empty players' pockets and wallets as thieves would.

A slot machine's standard layout features a screen displaying three or more reels that "spin" when the game is activated. Some modern slot machines still include a lever as a skeuomorphic design trait to trigger play. However, the mechanics of early machines have been superseded by random number generators, and most are now operated using buttons and touchscreens.

Slot machines include one or more currency detectors that validate the form of payment, whether coin, cash, voucher, or token. The machine pays out according to the pattern of symbols displayed when the reels stop "spinning". Slot machines are the most popular gambling method in casinos and constitute about 70% of the average U.S. casino's income.

Digital technology has resulted in variations on the original slot machine concept. As the player is essentially playing a video game, manufacturers are able to offer more interactive elements, such as advanced bonus rounds and more varied video graphics.

The "slot machine" term derives from the slots on the machine for inserting and retrieving coins. "Fruit machine" comes from the traditional fruit images on the spinning reels such as lemons and cherries.

Sittman and Pitt of Brooklyn, New York developed a gambling machine in 1891 that was a precursor to the modern slot machine. It contained five drums holding a total of 50 card faces and was based on poker. The machine proved extremely popular, and soon many bars in the city had one or more of them. Players would insert a nickel and pull a lever, which would spin the drums and the cards that they held, the player hoping for a good poker hand. There was no direct payout mechanism, so a pair of kings might get the player a free beer, whereas a royal flush could pay out cigars or drinks; the prizes were wholly dependent upon what the establishment would offer. To improve the odds for the house, two cards were typically removed from the deck, the ten of spades and the jack of hearts, doubling the odds against winning a royal flush. The drums could also be rearranged to further reduce a player's chance of winning.

Because of the vast number of possible wins in the original poker-based game, it proved practically impossible to make a machine capable of awarding an automatic payout for all possible winning combinations. At some time between 1887 and 1895, Charles Fey of San Francisco, California devised a much simpler automatic mechanism with three spinning reels containing a total of five symbols: horseshoes, diamonds, spades, hearts and a Liberty Bell; the bell gave the machine its name. By replacing ten cards with five symbols and using three reels instead of five drums, the complexity of reading a win was considerably reduced, allowing Fey to design an effective automatic payout mechanism. Three bells in a row produced the biggest payoff, ten nickels (50¢). Liberty Bell was a huge success and spawned a thriving mechanical gaming device industry. After a few years, the devices were banned in California, but Fey still could not keep up with demand for them from elsewhere. The Liberty Bell machine was so popular that it was copied by many slot-machine manufacturers. The first of these, also called the "Liberty Bell", was produced by the manufacturer Herbert Mills in 1907. By 1908, many "bell" machines had been installed in most cigar stores, saloons, bowling alleys, brothels and barber shops. Early machines, including an 1899 Liberty Bell, are now part of the Nevada State Museum's Fey Collection.

The first Liberty Bell machines produced by Mills used the same symbols on the reels as did Charles Fey's original. Soon afterward, another version was produced with patriotic symbols, such as flags and wreaths, on the wheels. Later, a similar machine called the Operator's Bell was produced that included the option of adding a gum-vending attachment. As the gum offered was fruit-flavored, fruit symbols were placed on the reels: lemons, cherries, oranges and plums. A bell was retained, and a picture of a stick of Bell-Fruit Gum, the origin of the bar symbol, was also present. This set of symbols proved highly popular and was used by other companies that began to make their own slot machines: Caille, Watling, Jennings and Pace.

A commonly used technique to avoid gambling laws in a number of states was to award food prizes. For this reason, a number of gumball and other vending machines were regarded with mistrust by the courts. The two Iowa cases of "State v. Ellis" and "State v. Striggles" are both used in criminal law classes to illustrate the concept of reliance upon authority as it relates to the axiomatic "ignorantia juris non excusat" ("ignorance of the law is no excuse"). In these cases, a mint vending machine was declared to be a gambling device because the machine would, by internally manufactured chance, occasionally give the next user a number of tokens exchangeable for more candy. Despite the display of the result of the next use on the machine, the courts ruled that "[t]he machine appealed to the player's propensity to gamble, and that is [a] vice."

In 1963, Bally developed the first fully electromechanical slot machine called Money Honey (although earlier machines such as Bally's High Hand draw-poker machine had exhibited the basics of electromechanical construction as early as 1940). Its electromechanical workings made Money Honey the first slot machine with a bottomless hopper and automatic payout of up to 500 coins without the help of an attendant. The popularity of this machine led to the increasing predominance of electronic games, with the side lever soon becoming vestigial.

The first video slot machine was developed in 1976 in Kearny Mesa, California by the Las Vegas–based Fortune Coin Co. This machine used a modified Sony Trinitron color receiver for the display and logic boards for all slot-machine functions. The prototype was mounted in a full-size, show-ready slot-machine cabinet. The first production units went on trial at the Las Vegas Hilton Hotel. After some modifications to defeat cheating attempts, the video slot machine was approved by the Nevada State Gaming Commission and eventually found popularity on the Las Vegas Strip and in downtown casinos. Fortune Coin Co. and its video slot-machine technology were purchased by IGT (International Gaming Technology) in 1978.

The first American video slot machine to offer a "second screen" bonus round was Reel ’Em In, developed by WMS Industries in 1996. This type of machine had appeared in Australia from at least 1994 with the Three Bags Full game. With this type of machine, the display changes to provide a different game in which an additional payout may awarded.

Depending on the machine, the player can insert cash or, in "ticket-in, ticket-out" machines, a paper ticket with a barcode, into a designated slot on the machine. The machine is then activated by means of a lever or button (either physical or on a touchscreen), which activates reels that spin and stop to rearrange the symbols. If a player matches a winning combination of symbols, the player earns credits based on the paytable. Symbols vary depending on the theme of the machine. Classic symbols include objects such as fruits, bells, and stylized lucky sevens. Most slot games have a theme, such as a specific aesthetic, location, or character. Symbols and other bonus features of the game are typically aligned with the theme. Some themes are licensed from popular media franchises, including films, television series (including game shows such as "Wheel of Fortune"), entertainers, and musicians.

Multi-line slot machines have become more popular since the 1990s. These machines have more than one payline, meaning that visible symbols that are not aligned on the main horizontal may be considered as winning combinations. Traditional three-reel slot machines commonly have one, three, or five paylines while video slot machines may have 9, 15, 25, or as many as 1024 different paylines. Most accept variable numbers of credits to play, with 1 to 15 credits per line being typical. The higher the amount bet, the higher the payout will be if the player wins.

One of the main differences between video slot machines and reel machines is in the way payouts are calculated. With reel machines, the only way to win the maximum jackpot is to play the maximum number of coins (usually three, sometimes four or even five coins per spin). With video machines, the fixed payout values are multiplied by the number of coins per line that is being bet. In other words: on a reel machine, the odds are more favorable if the gambler plays with the maximum number of coins available. However, depending on the structure of the game and its bonus features, some video slots may still include features that improve chances at payouts by making increased wagers.

"Multi-way" games eschew fixed paylines in favor of allowing symbols to pay anywhere, as long as there is at least one in at least three consecutive reels from left to right. Multi-way games may be configured to allow players to bet by-reel: for example, on a game with a 3x5 pattern (often referred to as a 243-way game), playing one reel allows all three symbols in the first reel to potentially pay, but only the center row pays on the remaining reels (often designated by darkening the unused portions of the reels). Other multi-way games use a 4x5 or 5x5 pattern, where there are up to five symbols in each reel, allowing for up to 1,024 and 3,125 ways to win respectively. The Australian manufacturer Aristocrat Leisure brands games featuring this system as "Reel Power", "Xtra Reel Power" and "Super Reel Power" respectively. A variation involves patterns where symbols pay adjacent to one another. Most of these games have a hexagonal reel formation, and much like multi-way games, any patterns not played are darkened out of use.

Denominations can range from 1 cent ("penny slots") all the way up to $100.00 or more per credit. The latter are typically known as "high limit" machines, and machines configured to allow for such wagers are often located in dedicated areas (which may have a separate team of attendants to cater to the needs of those who play there). The machine automatically calculates the number of credits the player receives in exchange for the cash inserted. Newer machines often allow players to choose from a selection of denominations on a splash screen or menu.

A bonus is a special feature of the particular game theme, which is activated when certain symbols appear in a winning combination. Bonuses and the number of bonus features vary depending upon the game. Some bonus rounds are a special session of free spins (the number of which is often based on the winning combination that triggers the bonus), often with a different or modified set of winning combinations as the main game and/or other multipliers or increased frequencies of symbols, or a "hold and re-spin" mechanic in which specific symbols (usually marked with values of credits or other prizes) are collected and locked in place over a finite number of spins. In other bonus rounds, the player is presented with several items on a screen from which to choose. As the player chooses items, a number of credits is revealed and awarded. Some bonuses use a mechanical device, such as a spinning wheel, that works in conjunction with the bonus to display the amount won. 

A candle is a light on top of the slot machine. It flashes to alert the operator that change is needed, hand pay is requested or a potential problem with the machine. It can be lit by the player by pressing the "service" or "help" button.

Carousel refers to a grouping of slot machines, usually in a circle or oval formation.

A coin hopper is a container where the coins that are immediately available for payouts are held. The hopper is a mechanical device that rotates coins into the coin tray when a player collects credits/coins (by pressing a "Cash Out" button). When a certain preset coin capacity is reached, a coin diverter automatically redirects, or "drops", excess coins into a "drop bucket" or "drop box". (Unused coin hoppers can still be found even on games that exclusively employ Ticket-In, Ticket-Out technology, as a vestige.)

The credit meter is a display of the amount of money or number of credits on the machine. On mechanical slot machines, this is usually a seven-segment display, but video slot machines typically use stylized text that suits the game's theme and user interface.

The drop bucket or drop box is a container located in a slot machine's base where excess coins are diverted from the hopper. Typically, a drop bucket is used for low-denomination slot machines and a drop box is used for high-denomination slot machines. A drop box contains a hinged lid with one or more locks whereas a drop bucket does not contain a lid. The contents of drop buckets and drop boxes are collected and counted by the casino on a scheduled basis.

EGM is short for "Electronic Gaming Machine".

Free spins are a common form of bonus, where a series of spins are automatically played at no charge at the player's current wager. Free spins are usually triggered via a scatter of at least three designated symbols (with the number of spins dependent on the number of symbols that land). Some games allow the free spins bonus to "retrigger", which adds additional spins on top of those already awarded. There is no theoretical limit to the number of free spins obtainable. Some games may have other features that can also trigger over the course of free spins.

A hand pay refers to a payout made by an attendant or at an exchange point ("cage"), rather than by the slot machine itself. A hand pay occurs when the amount of the payout exceeds the maximum amount that was preset by the slot machine's operator. Usually, the maximum amount is set at the level where the operator must begin to deduct taxes. A hand pay could also be necessary as a result of a short pay.

Hopper fill slip is a document used to record the replenishment of the coin in the coin hopper after it becomes depleted as a result of making payouts to players. The slip indicates the amount of coin placed into the hoppers, as well as the signatures of the employees involved in the transaction, the slot machine number and the location and the date.

MEAL book (Machine entry authorization log) is a log of the employee's entries into the machine.

Low-level or slant-top slot machines include a stool so the player may sit down. Stand-up or upright slot machines are played while standing.

Optimal play is a payback percentage based on a gambler using the optimal strategy in a skill-based slot machine game.

Payline is a line that crosses through one symbol on each reel, along which a winning combination is evaluated. Classic spinning reel machines usually have up to nine paylines, while video slot machines may have as many as one hundred. Paylines could be of various shapes (horizontal, vertical, oblique, triangular, zigzag, etc.)

Persistent state refers to passive features on some slot machines, some of which able to trigger bonus payouts or other special features if certain conditions are met over time by players on that machine.

Roll-up is the process of dramatizing a win by playing sounds while the meters count up to the amount that has been won.

Short pay refers to a partial payout made by a slot machine, which is less than the amount due to the player. This occurs if the coin hopper has been depleted as a result of making earlier payouts to players. The remaining amount due to the player is either paid as a hand pay or an attendant will come and refill the machine.

A scatter is a pay combination based on occurrences of a designated symbol landing anywhere on the reels, rather than falling in sequence on the same payline. A scatter pay usually requires a minimum of three symbols to land, and the machine may offer increased prizes or jackpots depending on the number that land. Scatters are frequently used to trigger bonus games, such as free spins (with the number of spins multiplying based on the number of scatter symbols that land). The scatter symbol usually cannot be matched using wilds, and some games may require the scatter symbols to appear on consecutive reels in order to pay. On some multiway games, scatter symbols still pay in unused areas.

Taste is a reference to the small amount often paid out to keep a player seated and continuously betting. Only rarely will machines fail to pay even the minimum out over the course of several pulls.
Tilt is a term derived from electromechanical slot machines' "tilt switches", which would make or break a circuit when they were tilted or otherwise tampered with that triggered an alarm. While modern machines no longer have tilt switches, any kind of technical fault (door switch in the wrong state, reel motor failure, out of paper, etc.) is still called a "tilt".

A theoretical hold worksheet is a document provided by the manufacturer for every slot machine that indicates the theoretical percentage the machine should hold based on the amount paid in. The worksheet also indicates the reel strip settings, number of coins that may be played, the payout schedule, the number of reels and other information descriptive of the particular type of slot machine.

Volatility or variance refers to the measure of risk associated with playing a slot machine. A low-volatility slot machine has regular but smaller wins, while a high-variance slot machine has fewer but bigger wins.

Weight count is an American term referring to the total value of coins or tokens removed from a slot machine's drop bucket or drop box for counting by the casino's hard count team through the use of a weigh scale.

Wild symbols substitute for most other symbols in the game (similarly to a joker card), usually excluding scatter and jackpot symbols (or offering a lower prize on non-natural combinations that include wilds). How jokers behave are dependent on the specific game and whether the player is in a bonus or free games mode. Sometimes wild symbols may only appear on certain reels, or have a chance to "stack" across the entire reel.

Each machine has a table that lists the number of credits the player will receive if the symbols listed on the pay table line up on the pay line of the machine. Some symbols are wild and can represent many, or all, of the other symbols to complete a winning line. Especially on older machines, the pay table is listed on the face of the machine, usually above and below the area containing the wheels. On video slot machines, they are usually contained within a help menu, along with information on other features.

Historically, all slot machines used revolving mechanical reels to display and determine results. Although the original slot machine used five reels, simpler, and therefore more reliable, three reel machines quickly became the standard.

A problem with three reel machines is that the number of combinations is only cubic – the original slot machine with three physical reels and 10 symbols on each reel had only 10 = 1,000 possible combinations. This limited the manufacturer's ability to offer large jackpots since even the rarest event had a likelihood of 0.1%. The maximum theoretical payout, assuming 100% return to player would be 1000 times the bet, but that would leave no room for other pays, making the machine very high risk, and also quite boring.

Although the number of symbols eventually increased to about 22, allowing 10,648 combinations, this still limited jackpot sizes as well as the number of possible outcomes.

In the 1980s, however, slot machine manufacturers incorporated electronics into their products and programmed them to weight particular symbols. Thus the odds of losing symbols appearing on the payline became disproportionate to their actual frequency on the physical reel. A symbol would only appear once on the reel displayed to the player, but could, in fact, occupy several stops on the multiple reel.

In 1984 Inge Telnaes received a patent for a device titled, "Electronic Gaming Device Utilizing a Random Number Generator for Selecting the Reel Stop Positions" (US Patent 4448419), which states: "It is important to make a machine that is perceived to present greater chances of payoff than it actually has within the legal limitations that games of chance must operate." The patent was later bought by International Game Technology and has since expired.

A virtual reel that has 256 virtual stops per reel would allow up to 256 = 16,777,216 final positions. The manufacturer could choose to offer a $1 million dollar jackpot on a $1 bet, confident that it will only happen, over the long term, once every 16.8 million plays.

With microprocessors now ubiquitous, the computers inside modern slot machines allow manufacturers to assign a different probability to every symbol on every reel. To the player it might appear that a winning symbol was "so close", whereas in fact the probability is much lower.

In the 1980s in the U.K., machines embodying microprocessors became common. These used a number of features to ensure the payout was controlled within the limits of the gambling legislation. As a coin was inserted into the machine, it could go either directly into the cashbox for the benefit of the owner or into a channel that formed the payout reservoir, with the microprocessor monitoring the number of coins in this channel. The drums themselves were driven by stepper motors, controlled by the processor and with proximity sensors monitoring the position of the drums. A "look-up table" within the software allows the processor to know what symbols were being displayed on the drums to the gambler. This allowed the system to control the level of payout by stopping the drums at positions it had determined. If the payout channel had filled up, the payout became more generous; if nearly empty, the payout became less so (thus giving good control of the odds).

Video slot machines do not use mechanical reels, instead of using graphical reels on a computerized display. As there are no mechanical constraints on the design of video slot machines, games often use at least five reels, and may also use non-standard layouts. This greatly expands the number of possibilities: a machine can have 50 or more symbols on a reel, giving odds as high as 300 million to 1 against – enough for even the largest jackpot. As there are so many combinations possible with five reels, manufacturers do not need to weight the payout symbols (although some may still do so). Instead, higher paying symbols will typically appear only once or twice on each reel, while more common symbols earning a more frequent payout will appear many times. Video slot machines usually make more extensive use of multimedia, and can feature more elaborate minigames as bonuses. Modern cabinets typically use flat-panel displays, but cabinets using larger curved screens (which can provide a more immersive experience for the player) are not uncommon.

Video slot machines typically encourage the player to play multiple "lines": rather than simply taking the middle of the three symbols displayed on each reel, a line could go from top left to the bottom right or any other pattern specified by the manufacturer. As each symbol is equally likely, there is no difficulty for the manufacturer in allowing the player to take as many of the possible lines on offer as desire – the long-term return to the player will be the same. The difference for the player is that the more lines they play, the more likely they are to get paid on a given spin (because they are betting more).

To avoid seeming as if the player's money is simply ebbing away (whereas a payout of 100 credits on a single-line machine would be 100 bets and the player would feel they had made a substantial win, on a 20-line machine, it would only be five bets and not seem as significant), manufacturers commonly offer bonus games, which can return many times their bet. The player is encouraged to keep playing to reach the bonus: even if he is losing, the bonus game could allow then to win back their losses.

All modern machines are designed using pseudorandom number generators ("PRNGs"), which are constantly generating a sequence of simulated random numbers, at a rate of hundreds or perhaps thousands per second. As soon as the "Play" button is pressed, the most recent random number is used to determine the result. This means that the result varies depending on exactly when the game is played. A fraction of a second earlier or later and the result would be different.

It is important that the machine contains a high-quality RNG implementation. Because all PRNGs must eventually repeat their number sequence and, if the period is short or the PRNG is otherwise flawed, an advanced player may be able to "predict" the next result. Having access to the PRNG code and seed values, Ronald Dale Harris, a former slot machine programmer, discovered equations for specific gambling games like Keno that allowed him to predict what the next set of selected numbers would be based on the previous games played.

Most machines are designed to defeat this by generating numbers even when the machine is not being played so the player cannot tell where in the sequence they are, even if they know how the machine was programmed.

Slot machines are typically programmed to pay out as winnings 0% to 99% of the money that is wagered by players. This is known as the "theoretical payout percentage" or RTP, "return to player". The minimum theoretical payout percentage varies among jurisdictions and is typically established by law or regulation. For example, the minimum payout in Nevada is 75%, in New Jersey 83%, and in Mississippi 80%. The winning patterns on slot machines – the amounts they pay and the frequencies of those payouts – are carefully selected to yield a certain fraction of the money paid to the "house" (the operator of the slot machine) while returning the rest to the players during play. Suppose that a certain slot machine costs $1 per spin and has a return to player (RTP) of 95%. It can be calculated that, over a sufficiently long period such as 1,000,000 spins, the machine will return an average of $950,000 to its players, who have inserted $1,000,000 during that time. In this (simplified) example, the slot machine is said to pay out 95%. The operator keeps the remaining $50,000. Within some EGM development organizations this concept is referred to simply as "par". "Par" also manifests itself to gamblers as promotional techniques: "Our 'Loose Slots' have a 93% payback! Play now!"

A slot machine's theoretical payout percentage is set at the factory when the software is written. Changing the payout percentage after a slot machine has been placed on the gaming floor requires a physical swap of the software or "firmware", which is usually stored on an EPROM but may be loaded onto non-volatile random access memory (NVRAM) or even stored on CD-ROM or DVD, depending on the capabilities of the machine and the applicable regulations. In certain jurisdictions, such as New Jersey, the EPROM has a tamper-evident seal and can only be changed in the presence of Gaming Control Board officials. Other jurisdictions, including Nevada, randomly audit slot machines to ensure that they contain only approved software.

Historically, many casinos, both online and offline, have been unwilling to publish individual game RTP figures, making it impossible for the player to know whether they are playing a "loose" or a "tight" game. Since the turn of the century some information regarding these figures has started to come into the public domain either through various casinos releasing them—primarily this applies to online casinos—or through studies by independent gambling authorities.

The "return to player" is not the only statistic that is of interest. The probabilities of every payout on the pay table is also critical. For example, consider a hypothetical slot machine with a dozen different values on the pay table. However, the probabilities of getting all the payouts are zero except the largest one. If the payout is 4,000 times the input amount, and it happens every 4,000 times on average, the "return to player" is exactly 100%, but the game would be dull to play. Also, most people would not win anything, and having entries on the paytable that have a return of zero would be deceptive. As these individual probabilities are closely guarded secrets, it is possible that the advertised machines with high return to player simply increase the probabilities of these jackpots. The casino could legally place machines of a similar style payout and advertise that some machines have 100% return to player. The added advantage is that these large jackpots increase the excitement of the other players.

The table of probabilities for a specific machine is called the Probability and Accounting Report or PAR sheet, also PARS commonly understood as Paytable and Reel Strips. Mathematician Michael Shackleford revealed the PARS for one commercial slot machine, an original International Gaming Technology "Red White and Blue" machine. This game, in its original form, is obsolete, so these specific probabilities do not apply. He only published the odds after a fan of his sent him some information provided on a slot machine that was posted on a machine in the Netherlands. The psychology of the machine design is quickly revealed. There are 13 possible payouts ranging from 1:1 to 2,400:1. The 1:1 payout comes every 8 plays. The 5:1 payout comes every 33 plays, whereas the 2:1 payout comes every 600 plays. Most players assume the likelihood increases proportionate to the payout. The one mid-size payout that is designed to give the player a thrill is the 80:1 payout. It is programmed to occur an average of once every 219 plays. The 80:1 payout is high enough to create excitement, but not high enough that it makes it likely that the player will take his winnings and abandon the game. More than likely the player began the game with at least 80 times his bet (for instance there are 80 quarters in $20). In contrast the 150:1 payout occurs only on average of once every 6,241 plays. The highest payout of 2,400:1 occurs only on average of once every 64 = 262,144 plays since the machine has 64 virtual stops. The player who continues to feed the machine is likely to have several mid-size payouts, but unlikely to have a large payout. He quits after he is bored or has exhausted his bankroll.

Despite their confidentiality, occasionally a PAR sheet is posted on a website. They have limited value to the player, because usually a machine will have 8 to 12 different possible programs with varying payouts. In addition, slight variations of each machine (e.g., with "double jackpots" or "five times play") are always being developed. The casino operator can choose which EPROM chip to install in any particular machine to select the payout desired. The result is that there is not really such a thing as a high payback type of machine, since every machine potentially has multiple settings. From October 2001 to February 2002, columnist Michael Shackleford obtained PAR sheets for five different nickel machines; four IGT games "Austin Powers", "Fortune Cookie", "Leopard Spots" and "Wheel of Fortune" and one game manufactured by WMS; "Reel 'em In". Without revealing the proprietary information, he developed a program that would allow him to determine with usually less than a dozen plays on each machine which EPROM chip was installed. Then he did a survey of over 400 machines in 70 different casinos in Las Vegas. He averaged the data, and assigned an average payback percentage to the machines in each casino. The resultant list was widely publicized for marketing purposes (especially by the Palms casino which had the top ranking).

One reason that the slot machine is so profitable to a casino is that the player must play the "high house edge and high payout" wagers along with the "low house edge and low payout" wagers. In a more traditional wagering game like craps, the player knows that certain wagers have almost a 50/50 chance of winning or losing, but they only pay a limited multiple of the original bet (usually no higher than three times). Other bets have a higher house edge, but the player is rewarded with a bigger win (up to thirty times in craps). The player can choose what kind of wager he wants to make. A slot machine does not afford such an opportunity. Theoretically, the operator could make these probabilities available, or allow the player to choose which one so that the player is free to make a choice. However, no operator has ever enacted this strategy. Different machines have different maximum payouts, but without knowing the odds of getting the jackpot, there is no rational way to differentiate.

In many markets where central monitoring and control systems are used to link machines for auditing and security purposes, usually in wide area networks of multiple venues and thousands of machines, player return must usually be changed from a central computer rather than at each machine. A range of percentages is set in the game software and selected remotely.

In 2006, the Nevada Gaming Commission began working with Las Vegas casinos on technology that would allow the casino's management to change the game, the odds, and the payouts remotely. The change cannot be done instantaneously, but only after the selected machine has been idle for at least four minutes. After the change is made, the machine must be locked to new players for four minutes and display an on-screen message informing potential players that a change is being made.

Some varieties of slot machines can be linked together in a setup sometimes known as a "community" game. The most basic form of this setup involves progressive jackpots that are shared between the bank of machines, but may include multiplayer bonuses and other features.

In some cases multiple machines are linked across multiple casinos. In these cases, the machines may be owned by the manufacturer, who is responsible for paying the jackpot. The casinos lease the machines rather than owning them outright. Casinos in New Jersey, Nevada, and South Dakota now offer multi-state progressive jackpots, which now offer bigger jackpot pools.

Mechanical slot machines and their coin acceptors were sometimes susceptible to cheating devices and other scams. One historical example involved spinning a coin with a short length of plastic wire. The weight and size of the coin would be accepted by the machine and credits would be granted. However, the spin created by the plastic wire would cause the coin to exit through the reject chute into the payout tray. This particular scam has become obsolete due to improvements in newer slot machines. Another obsolete method of defeating slot machines was to use a light source to confuse the optical sensor used to count coins during payout.

Modern slot machines are controlled by EPROM computer chips and, in large casinos, coin acceptors have become obsolete in favor of bill acceptors. These machines and their bill acceptors are designed with advanced anti-cheating and anti-counterfeiting measures and are difficult to defraud. Early computerized slot machines were sometimes defrauded through the use of cheating devices, such as the "slider" or "monkey paw". Computerized slot machines are fully deterministic and thus outcomes can be sometimes successfully predicted.

Malfunctioning electronic slot machines are capable of indicating jackpot winnings far in excess of those advertised. Two such cases occurred in casinos in Colorado in 2010, where software errors led to indicated jackpots of $11 million and $42 million. Analysis of machine records by the state Gaming Commission revealed faults, with the true jackpot being substantially smaller. State gaming laws do not require a casino to honour payouts.

In the United States, the public and private availability of slot machines is highly regulated by state governments. Many states have established gaming control boards to regulate the possession and use of slot machines and other form of gaming.

Nevada is the only state that has no significant restrictions against slot machines both for public and private use. In New Jersey, slot machines are only allowed in hotel casinos operated in Atlantic City. Several states (Indiana, Louisiana and Missouri) allow slot machines (as well as any casino-style gambling) only on licensed riverboats or permanently anchored barges. Since Hurricane Katrina, Mississippi has removed the requirement that casinos on the Gulf Coast operate on barges and now allows them on land along the shoreline. Delaware allows slot machines at three horse tracks; they are regulated by the state lottery commission. In Wisconsin, bars and taverns are allowed to have up to five machines. These machines usually allow a player to either take a payout, or gamble it on a double-or-nothing "side game". 

The territory of Puerto Rico places significant restrictions on slot machine ownership, but the law is widely flouted and slot machines are common in bars and coffeeshops.

In regards to tribal casinos located on Native American reservations, slot machines played against the house and operating independently from a centralized computer system are classified as "Class III" gaming by the Indian Gaming Regulatory Act (IGRA), and sometimes promoted as "Vegas-style" slot machines. In order to offer Class III gaming, tribes must enter into a compact (agreement) with the state that is approved by the Department of the Interior, which may contain restrictions on the types and quantity of such games. As a workaround, some casinos may operate slot machines as "Class II" games—a category that includes games where players play exclusively against at least one other opponent and not the house, such as bingo or any related games (such as pull-tabs). In these cases, the reels are an entertainment display with a pre-determined outcome based on a centralized game played against other players. Under the IGRA, Class II games are regulated by individual tribes and the National Indian Gaming Commission, and do not require any additional approval if the state already permits tribal gaming. 

Some historical race wagering terminals operate in a similar manner, with the machines using slots as an entertainment display for outcomes paid using the parimutuel betting system, based on results of randomly-selected, previously-held horse races (with the player able to view selected details about the race and adjust their picks before playing the credit, or otherwise use an auto-bet system).

Alaska, Arizona, Arkansas, Kentucky, Maine, Minnesota, Nevada, Ohio, Rhode Island, Texas, Utah, Virginia, and West Virginia place no restrictions on private ownership of slot machines. Conversely, in Connecticut, Hawaii, Nebraska, South Carolina, and Tennessee, private ownership of any slot machine is completely prohibited. The remaining states allow slot machines of a certain age (typically 25–30 years) or slot machines manufactured before a specific date. For a detailed list of state-by-state regulations on private slot machine ownership, see U.S. state slot machine ownership regulations.

The Government of Canada has minimal involvement in gambling beyond the Canadian Criminal Code. In essence, the term "lottery scheme" used in the code means slot machines, bingo and table games normally associated with a casino. These fall under the jurisdiction of the province or territory without reference to the federal government; in practice, all Canadian provinces operate gaming boards that oversee lotteries, casinos and video lottery terminals under their jurisdiction.

OLG piloted a classification system for slot machines at the Grand River Raceway developed by University of Waterloo professor Kevin Harrigan, as part of its PlaySmart initiative for responsible gambling. Inspired by nutrition labels on foods, they displayed metrics such as volatility and frequency of payouts.

In Australia "Poker Machines" or "pokies" are officially termed "gaming machines". In Australia, gaming machines are a matter for state governments, so laws vary between states. Gaming machines are found in casinos (approximately one in each major city), pubs and clubs in some states (usually sports, social, or RSL clubs). The first Australian state to legalize this style of gambling was New South Wales, when in 1956 they were made legal in all registered clubs in the state. There are suggestions that the proliferation of poker machines has led to increased levels of problem gambling; however, the precise nature of this link is still open to research.

In 1999 the Australian Productivity Commission reported that nearly half Australia's gaming machines were in New South Wales. At the time, 21% of all the gambling machines in the world were operating in Australia and, on a per capita basis, Australia had roughly five times as many gaming machines as the United States. Australia ranks 8th in total number of gaming machines after Japan, U.S.A., Italy, U.K., Spain and Germany. This primarily is because gaming machines have been legal in the state of New South Wales since 1956; over time, the number of machines has grown to 97,103 (at December 2010, including the Australian Capital Territory). By way of comparison, the U.S. State of Nevada, which legalised gaming including slots several decades before N.S.W., had 190,135 slots operating.

Revenue from gaming machines in pubs and clubs accounts for more than half of the $4 billion in gambling revenue collected by state governments in fiscal year 2002–03.

In Queensland, gaming machines in pubs and clubs must provide a return rate of 85%, while machines located in casinos must provide a return rate of 90%. Most other states have similar provisions. In Victoria, gaming machines must provide a minimum return rate of 87% (including jackpot contribution), including machines in Crown Casino. As of December 1, 2007, Victoria banned gaming machines that accepted $100 notes; all gaming machines made since 2003 comply with this rule. This new law also banned machines with an automatic play option. One exception exists in Crown Casino for any player with a VIP loyalty card: they can still insert $100 notes and use an autoplay feature (whereby the machine will automatically play until credit is exhausted or the player intervenes). All gaming machines in Victoria have an information screen accessible to the user by pressing the "i key" button, showing the game rules, paytable, return to player percentage, and the top and bottom five combinations with their odds. These combinations are stated to be played on a minimum bet (usually 1 credit per line, with 1 line or reel played, although some newer machines do not have an option to play 1 line; some machines may only allow maximum lines to be played), excluding feature wins.

Western Australia has the most restrictive regulations on electronic gaming machines in general, with the Crown Perth casino resort being the only venue allowed to operate them, and banning slot machines with spinning reels entirely. This policy had an extensive political history, reaffirmed by the 1974 Royal Commission into Gambling:

While Western Australian gaming machines are similar to the other states', they do not have spinning reels. Therefore different animations are used in place of the spinning reels in order to display each game result.

Nick Xenophon was elected on an independent No Pokies ticket in the South Australian Legislative Council at the 1997 South Australian state election on 2.9 percent, re-elected at the 2006 election on 20.5 percent, and elected to the Australian Senate at the 2007 federal election on 14.8 percent. Independent candidate Andrew Wilkie, an anti-pokies campaigner, was elected to the Australian House of Representatives seat of Denison at the 2010 federal election. Wilkie was one of four crossbenchers who supported the Gillard Labor government following the hung parliament result. Wilkie immediately began forging ties with Xenophon as soon as it was apparent that he was elected. In exchange for Wilkie's support, the Labor government are attempting to implement precommitment technology for high-bet/high-intensity poker machines, against opposition from the Tony Abbott Coalition and Clubs Australia.

During the Covid-19 pandemic of 2020, every establishment in the country that facilitated poker machines was shut down, in an attempt to curb the spread of the virus. Bringing Australia's usage of poker machines effectively to zero. 

In Russia, "slot clubs" appeared quite late, only in 1992. Before 1992, slot machines were only in casinos and small shops, but later slot clubs began appearing all over the country. The most popular and numerous were "Vulcan 777" and "Taj Mahal". Since 2009 when gambling establishments were banned, almost all slot clubs disappeared and are found only in a specially authorized gambling zones.

Slot machines are covered by the Gambling Act 2005, which superseded the Gaming Act 1968.

Slot machines in the U.K. are categorised by definitions produced by the Gambling Commission as part of the Gambling Act of 2005.

Casinos built under the provisions of the 1968 Act are allowed to house either up to twenty machines of categories B–D or any number of C–D machines. As defined by the 2005 Act, large casinos can have a maximum of one hundred and fifty machines in any combination of categories B–D (subject to a machine-to-table ratio of 5:1); small casinos can have a maximum of eighty machines in any combination of categories B–D (subject to a machine-to-table ratio of 2:1).

Category A games were defined in preparation for the planned "Super Casinos". Despite a lengthy bidding process with Manchester being chosen as the single planned location, the development was cancelled soon after Gordon Brown became Prime Minister of the United Kingdom. As a result, there are no lawful Category A games in the U.K.

Category B games are divided into subcategories. The differences between B1, B3 and B4 games are mainly the stake and prizes as defined in the above table. Category B2 games – Fixed odds betting terminals (FOBTs) – have quite different stake and prize rules: FOBTs are mainly found in licensed betting shops, or bookmakers, usually in the form of electronic roulette.

The games are based on a random number generator; thus each game's probability of getting the jackpot is independent of any other game: probabilities are all equal. If a pseudorandom number generator is used instead of a truly random one, probabilities are not independent since each number is determined at least in part by the one generated before it.

Category C games are often referred to as fruit machines, one-armed bandits and AWP (amusement with prize). Fruit machines are commonly found in pubs, clubs, and arcades. Machines commonly have three but can be found with four or five reels, each with 16–24 symbols printed around them. The reels are spun each play, from which the appearance of particular combinations of symbols result in payment of their associated winnings by the machine (or alternatively initiation of a subgame). These games often have many extra features, trails and subgames with opportunities to win money; usually more than can be won from just the payouts on the reel combinations.

Fruit machines in the U.K. almost universally have the following features, generally selected at random using a pseudorandom number generator:

It is known for machines to pay out multiple jackpots, one after the other (this is known as a streak or rave) but each jackpot requires a new game to be played so as not to violate the law about the maximum payout on a single play. Typically this involves the player only pressing the Start button for which a single credit is taken, regardless of whether this causes the reels to spin or not. The minimum payout percentage is 70%, with pubs often setting the payout at around 78%.

Japanese slot machines, known as or pachislot (portmanteaus of the words "pachinko" and "slot machine"), are a descendant of the traditional Japanese pachinko game. Slot machines are a fairly new phenomenon and they can be found mostly in pachinko parlors and the adult sections of amusement arcades, known as game centers.

The machines are regulated with integrated circuits, and have six different levels changing the odds of a 777. The levels provide a rough outcome of between 90% to 160% (200% for skilled players). Japanese slot machines are "beatable". Parlor operators naturally set most machines to simply collect money, but intentionally place a few paying machines on the floor so that there will be at least someone winning, encouraging players on the losing machines to keep gambling, using the psychology of the gambler's fallacy.

Despite the many varieties of pachislot machines, there are certain rules and regulations put forward by the , an affiliate of the National Police Agency. For example, there must be three reels. All reels must be accompanied by buttons which allow players to manually stop them, reels may not spin faster than 80 RPM, and reels must stop within 0.19 seconds of a button press. In practice, this means that machines cannot let reels slip more than 4 symbols. Other rules include a 15 coin payout cap, a 50 credit cap on machines, a 3 coin maximum bet, and other such regulations.

Although a 15 coin payout may seem quite low, regulations allow "Big Bonus" (c. 400–711 coins) and "Regular Bonus" modes (c. 110 coins) where these 15 coin payouts occur nearly continuously until the bonus mode is finished. While the machine is in bonus mode, the player is entertained with special winning scenes on the LCD display, and energizing music is heard, payout after payout.

Three other unique features of Pachisuro machines are "stock", "renchan", and . On many machines, when enough money to afford a bonus is taken in, the bonus is not immediately awarded. Typically the game merely stops making the reels slip off the bonus symbols for a few games. If the player fails to hit the bonus during these "standby games", it is added to the "stock" for later collection. Many current games, after finishing a bonus round, set the probability to release additional stock (gained from earlier players failing to get a bonus last time the machine stopped making the reels slip for a bit) very high for the first few games. As a result, a lucky player may get to play several bonus rounds in a row (a "renchan"), making payouts of 5,000 or even 10,000 coins possible. The lure of "stock" waiting in the machine, and the possibility of "renchan" tease the gambler to keep feeding the machine. To tease them further, there is a "tenjō" (ceiling), a maximum limit on the number of games between "stock" release. For example, if the "tenjō" is 1,500, and the number of games played since the last bonus is 1,490, the player is guaranteed to release a bonus within just 10 games.

Because of the "stock", "renchan", and "tenjō" systems, it is possible to make money by simply playing machines on which someone has just lost a huge amount of money. This is called being a "hyena". They are easy to recognize, roaming the aisles for a "kamo" ("sucker" in English) to leave his machine.

In short, the regulations allowing "stock", "renchan", and "tenjō" transformed the pachisuro from a low-stakes form of entertainment just a few years back to hardcore gambling. Many people may be gambling more than they can afford, and the big payouts also lure unsavory "hyena" types into the gambling halls.

To address these social issues, a new regulation (Version 5.0) was adopted in 2006 which caps the maximum amount of "stock" a machine can hold to around 2,000–3,000 coins' worth of bonus games. Moreover, all pachisuro machines must be re-evaluated for regulation compliance every three years. Version 4.0 came out in 2004, so that means all those machines with the up to 10,000 coin payouts will be removed from service by 2007.

Natasha Dow Schüll, associate professor in New York University's Department of Media, Culture, and Communication, uses the term "machine zone" to describe the state of immersion that users of slot machines experience during gambling, in which they lose a sense of time, space, bodily awareness, and monetary value.

Mike Dixon, PhD, professor of psychology at the University of Waterloo, studies the relationship between slot players and slot machines. Slot players were observed experiencing heightened arousal from the sensory stimulus coming from the machines. They "sought to show that these 'losses disguised as wins' (LDWs) would be as arousing as wins, and more arousing than regular losses."

Psychologists Robert Breen and Marc Zimmerman found that players of video slot machines reach a debilitating level of involvement with gambling three times as rapidly as those who play traditional casino games, even if they have gambled regularly on other forms of gambling in the past without a problem.

Eye tracking research in local bookkeepers offices in the UK suggested that, in slots games, the slot-reels dominated players' visual attention and problem gamblers looked more frequently at amount-won messages than those without gambling problems. 

The 2011 "60 Minutes" report "Slot Machines: The Big Gamble" focused on the link between slot machines and gambling addiction.

Skill stop buttons predated the Bally electromechanical slot machines of the 1960s and 70s. They appeared on mechanical slot machines manufactured by Mills Novelty Co. as early as the mid 1920s. These machines had modified reel-stop arms, which allowed them to be released from the timing bar, earlier than in a normal play, simply by pressing the buttons on the front of the machine, located between each reel.

"Skill stop" buttons were added to some slot machines by Zacharias Anthony in the early 1970s. These enabled the player to stop each reel, allowing a degree of "skill" so as to satisfy the New Jersey gaming laws of the day which required that players were able to control the game in some way. The original conversion was applied to approximately 50 late-model Bally slot machines. Because the typical machine stopped the reels automatically in less than 10 seconds, weights were added to the mechanical timers to prolong the automatic stopping of the reels. By the time the New Jersey Alcoholic Beverages Commission (ABC) had approved the conversion for use in New Jersey arcades, the word was out and every other distributor began adding skill stops. The machines were a huge hit on the Jersey Shore and the remaining unconverted Bally machines were destroyed as they had become instantly obsolete.





</doc>
<doc id="29230" url="https://en.wikipedia.org/wiki?curid=29230" title="Sneaker Pimps">
Sneaker Pimps

Sneaker Pimps are a British electronic music band formed in Hartlepool, England in 1994. They are best known for their debut album, "Becoming X" (1996), and its singles "6 Underground" and "Spin Spin Sugar". The band takes its name from an article the Beastie Boys published in their "Grand Royal" magazine about a man they hired to track down classic sneakers.

The band was founded by electronic musician Liam Howe and guitarist Chris Corner. They later recruited Kelli Ali (then known as Kelli Dayton) as lead singer, plus guitarist Joe Wilson and drummer Dave Westlake as backup musicians. After "Becoming X", Ali left the band and Corner took over on vocals. Wilson and Westlake departed in 2002. After a lengthy hiatus, Howe and Corner revived the group in 2016.

Chris Corner and Liam Howe met as teenagers in the 1980s, both taking an interest in recording and studio experimentation. They banded together under the name F.R.I.S.K. and produced the "Soul of Indiscretion" EP, an early example of what became known as trip hop. The mix of beats and acoustic folk sounds was further explored on two more instrumental EPs: "F.R.I.S.K." and "World as a Cone". They were signed to Clean Up Records. The duo also worked as DJs and producers under the name Line of Flight.

Howe and Corner launched Sneaker Pimps as a recording group in 1994. The following year they recruited Ian Pickering to help write lyrics for what would become Sneaker Pimps' debut album, "Becoming X". Corner recorded vocals for several demo tracks, but the band decided the kind of music they were writing would better suit a female voice. At their manager's suggestion, they saw Kelli Ali (then known as Kelli Dayton) performing in a pub with her band The Lumieres and invited her to sing on some demos, including an early version of "6 Underground". She soon joined the band, and the demos won the group a contract with Virgin Records. The group was presented as a trio featuring Howe, Corner, and Ali; while bassist Joe Wilson and drummer Dave Westlake were added as supporting musicians. 

Released in 1996, "Becoming X" sold over one million copies. The band toured for two years to support the album, including gigs alongside Aphex Twin. A "grueling" tour of the US strained relations within the band, and Howe left the tour prematurely. A remix album, "Becoming Remixed", followed in 1998.

Howe and Corner then developed their own studio, also called Line of Flight after their earlier production work, and began sessions for the second Sneaker Pimps album. Kelli Ali had taken a break after the "Becoming X" tour and was away traveling, so Corner sang on the new demos. When Ali returned, she was told by Howe and Corner that her voice was no longer considered suitable for their new music, and that Corner's voice was a better fit. Due to other ongoing personality conflicts and the band's concern about being stereotyped as a faddish female-fronted trip-hop act, Ali was fired and Corner took over on lead vocals.

This significant lineup change caused Virgin Records to drop the band. Their second album "Splinter" was released in the UK on Clean Up Records in 1999, and failed to match the commercial success of "Becoming X". New songs were premiered during a 2001 European tour opening for Placebo. Their third album "Bloodsport" was released on Tommy Boy Records in 2002. Howe and Corner also gained notice by writing and producing for other artists, including Natalie Imbruglia, and for remixing songs under the name Line of Flight. 

In 2002, Joe Wilson and Dave Westlake left Sneaker Pimps. In 2003, a fourth Sneaker Pimps album was demoed but shelved. The album, which started as the soundtrack for an abandoned indie film project called "Blind Michael", is referred to in fan circles as "SP4". Corner then launched the solo project IAMX, which included several songs from the "SP4" project. After some additional cancelled projects, in 2006 Howe and Corner recorded some new demo tracks with an unidentified female singer that turned up on a MiniDisc found in a bar in Russia. The tracks were leaked online and were later confirmed to be legitimate new Sneaker Pimps songs. They have never been officially released.

After several years of side projects, Howe hinted in 2015 that Sneaker Pimps may reform. Corner confirmed the reunion in 2016, and as of early 2019 they were reportedly working on a new album. 

In the studio, the band regularly swapped instruments. As Corner explained during the recording sessions for "Bloodsport", "we tend towards jobs, but generally we can mix and match. If we get bored of one aspect, someone else jumps in the seat. Gone are the days where it’s like 'You’re the drummer, I’m the synth player." When playing live, however, their roles were more fixed:








</doc>
<doc id="29234" url="https://en.wikipedia.org/wiki?curid=29234" title="Spear">
Spear

A spear is a pole weapon consisting of a shaft, usually of wood, with a pointed head. The head may be simply the sharpened end of the shaft itself, as is the case with fire hardened spears, or it may be made of a more durable material fastened to the shaft, such as bone, flint, obsidian, iron, steel or bronze. The most common design for hunting or combat spears since ancient times has incorporated a metal spearhead shaped like a triangle, lozenge, or leaf. The heads of fishing spears usually feature barbs or serrated edges. 

The word "spear" comes from the Old English "spere", from the Proto-Germanic "speri", from a Proto-Indo-European root "*sper-" "spear, pole".
Spears can be divided into two broad categories: those designed for thrusting in melee combat and those designed for throwing (usually referred to as javelins).

The spear has been used throughout human history both as a hunting and fishing tool and as a weapon. Along with the axe, knife, and club; it is one of the earliest and most important tools developed by early humans. As a weapon, it may be wielded with either one or two hands. It was used in virtually every conflict up until the modern era, where even then it continues on in the form of the fixed bayonet, and is probably the most commonly used weapon in history.

Spear manufacture and use is not confined to humans. It is also practiced by the western chimpanzee. Chimpanzees near Kédougou, Senegal have been observed to create spears by breaking straight limbs off trees, stripping them of their bark and side branches, and sharpening one end with their teeth. They then used the weapons to hunt galagos sleeping in hollows.

Archaeological evidence found in present-day Germany documents that wooden spears have been used for hunting since at least 400,000 years ago, and a 2012 study from the site of Kathu Pan in South Africa suggests that hominids, possibly "Homo heidelbergensis", may have developed the technology of hafted stone-tipped spears in Africa about 500,000 years ago. Wood does not preserve well, however, and Craig Stanford, a primatologist and professor of anthropology at the University of Southern California, has suggested that the discovery of spear use by chimpanzees probably means that early humans used wooden spears as well, perhaps, five million years ago.

Neanderthals were constructing stone spear heads from as early as 300,000 BP and by 250,000 years ago, wooden spears were made with fire-hardened points.

From circa 200,000 BCE onwards, Middle Paleolithic humans began to make complex stone blades with flaked edges which were used as spear heads. These stone heads could be fixed to the spear shaft by gum or resin or by bindings made of animal sinew, leather strips or vegetable matter. During this period, a clear difference remained between spears designed to be thrown and those designed to be used in hand-to-hand combat. By the Magdalenian period (c. 15,000–9500 BCE), spear-throwers similar to the later atlatl were in use.

The spear is the main weapon of the warriors of Homer's "Iliad". The use of both a single thrusting spear and two throwing spears are mentioned. It has been suggested that two styles of combat are being described; an early style, with thrusting spears, dating to the Mycenaean period in which the Iliad is set, and, anachronistically, a later style, with throwing spears, from Homer's own Archaic period.

In the 7th century BCE, the Greeks evolved a new close-order infantry formation, the phalanx. The key to this formation was the hoplite, who was equipped with a large, circular, bronze-faced shield (aspis) and a spear with an iron head and bronze butt-spike (doru). The hoplite phalanx dominated warfare among the Greek City States from the 7th into the 4th century BCE.

The 4th century saw major changes. One was the greater use of peltasts, light infantry armed with spear and javelins. The other was the development of the sarissa, a two-handed pike in length, by the Macedonians under Phillip of Macedon and Alexander the Great. The pike phalanx, supported by peltasts and cavalry, became the dominant mode of warfare among the Greeks from the late 4th century onward until Greek military systems were supplanted by the Roman legions.

In the pre-Marian Roman armies, the first two lines of battle, the "hastati" and "principes", often fought with a sword called a "gladius" and "pila", heavy javelins that were specifically designed to be thrown at an enemy to pierce and foul a target's shield. Originally the "principes" were armed with a short spear called a "hasta", but these gradually fell out of use, eventually being replaced by the gladius. The third line, the "triarii", continued to use the "hasta".

From the late 2nd century BCE, all legionaries were equipped with the "pilum". The "pilum" continued to be the standard legionary spear until the end of the 2nd century CE. "Auxilia", however, were equipped with a simple hasta and, perhaps, throwing spears. During the 3rd century CE, although the "pilum" continued to be used, legionaries usually were equipped with other forms of throwing and thrusting spear, similar to "auxilia" of the previous century. By the 4th century, the "pilum" had effectively disappeared from common use.

In the late period of the Roman Empire, the spear became more often used because of its anti-cavalry capacities as the barbarian invasions were often conducted by people with a developed culture of cavalry in warfare.

Muslim warriors used a spear that was called an "az-zaġāyah". Berbers pronounced it "zaġāya", but the English term, derived from the Old French via Berber, is "assegai". It is a pole weapon used for throwing or hurling, usually a light spear or javelin made of hard wood and pointed with a forged iron tip.The "az-zaġāyah" played an important role during the Islamic conquest as well as during later periods, well into the 20th century. A longer pole "az-zaġāyah" was being used as a hunting weapon from horseback. The "az-zaġāyah" was widely used. It existed in various forms in areas stretching from Southern Africa to the Indian subcontinent, although these places already had their own variants of the spear. This javelin was the weapon of choice during the "Fulani jihad" as well as during the Mahdist War in Sudan. It is still being used by Sikh "Nihang" in the Punjab as well as certain wandering Sufi ascetics "(Derwishes)".

After the fall of the Western Roman Empire, the spear and shield continued to be used by nearly all Western European cultures. Since a medieval spear required only a small amount of steel along the sharpened edges (most of the spear-tip was wrought iron), it was an economical weapon. Quick to manufacture, and needing less smithing skill than a sword, it remained the main weapon of the common soldier. The Vikings, for instance, although often portrayed with axe or sword in hand, were armed mostly with spears, as were their Anglo-Saxon, Irish, or continental contemporaries.

Broadly speaking, spears were either designed to be used in melee, or to be thrown. Within this simple classification, there was a remarkable range of types. For example, M. J. Swanton identified thirty different spearhead categories and sub-categories in early Saxon England. Most medieval spearheads were generally leaf-shaped. Notable types of early medieval spears include the "angon", a throwing spear with a long head similar to the Roman "pilum", used by the Franks and Anglo-Saxons, and the winged (or lugged) spear, which had two prominent wings at the base of the spearhead, either to prevent the spear penetrating too far into an enemy or to aid in spear fencing. Originally a Frankish weapon, the winged spear also was popular with the Vikings. It would become the ancestor of later medieval polearms, such as the partisan and spetum.

The thrusting spear also has the advantage of reach, being considerably longer than other weapon types. Exact spear lengths are hard to deduce as few spear shafts survive archaeologically but would seem to have been the norm. Some nations were noted for their long spears, including the Scots and the Flemish. Spears usually were used in tightly ordered formations, such as the shield wall or the schiltron. To resist cavalry, spear shafts could be planted against the ground. William Wallace drew up his schiltrons in a circle at the Battle of Falkirk in 1298 to deter charging cavalry; this was a widespread tactic sometimes known as the "crown" formation.

Throwing spears became rarer as the Middle Ages drew on, but survived in the hands of specialists such as the Catalan Almogavars. They were commonly used in Ireland until the end of the 16th century.

Spears began to lose fashion among the infantry during the 14th century, being replaced by pole weapons that combined the thrusting properties of the spear with the cutting properties of the axe, such as the halberd. Where spears were retained they grew in length, eventually evolving into pikes, which would be a dominant infantry weapon in the 16th and 17th centuries.

Cavalry spears were originally the same as infantry spears and were often used with two hands or held with one hand overhead. In the 12th century, after the adoption of stirrups and a high-cantled saddle, the spear became a decidedly more powerful weapon. A mounted knight would secure the lance by holding it with one hand and tucking it under the armpit (the "couched lance" technique) This allowed all the momentum of the horse and knight to be focused on the weapon's tip, whilst still retaining accuracy and control. This use of the spear spurred the development of the lance as a distinct weapon that was perfected in the medieval sport of jousting.

In the 14th century, tactical developments meant that knights and men-at-arms often fought on foot. This led to the practice of shortening the lance to about .) to make it more manageable. As dismounting became commonplace, specialist pole weapons such as the pollaxe were adopted by knights and this practice ceased.

Spears were used first as hunting weapons amongst the ancient Chinese. They became popular as infantry weapons during the Warring States and Qin era, when spearmen were used as especially highly disciplined soldiers in organized group attacks. When used in formation fighting, spearmen would line up their large rectangular or circular shields in a shieldwall manner. The Qin also employed long spears (more akin to a pike) in formations similar to Swiss pikemen in order to ward off cavalry. The Han Empire would use similar tactics as its Qin predecessors. Halberds, polearms, and dagger axes were also common weapons during this time.

Spears were also common weaponry for Warring States, Qin, and Han era cavalry units. During these eras, the spear would develop into a longer lance-like weapon used for cavalry charges.

There are many words in Chinese that would be classified as a spear in English. The "Mao" is the predecessor of the "Qiang". The first bronze "Mao" appeared in the Shang dynasty. This weapon was less prominent on the battlefield than the "ge" (dagger-axe). In some archaeological examples two tiny holes or ears can be found in the blade of the spearhead near the socket, these holes were presumably used to attach tassels, much like modern day wushu spears.

In the early Shang, the "Mao" appeared to have a relatively short shaft as well as a relatively narrow shaft as opposed to "Mao" in the later Shang and Western Zhou period. Some "Mao" from this era are heavily decorated as is evidenced by a Warring States period "Mao" from the Ba Shu area.

In the Han dynasty the "Mao" and the "Ji" (戟 "Ji" can be loosely defined as a halberd) rose to prominence in the military. Interesting to note is that the amount of iron Mao-heads found exceeds the number of bronze heads. By the end of the Han dynasty (Eastern Han) the process of replacement of the iron "Mao" had been completed and the bronze "Mao" had been rendered completely obsolete. After the Han dynasty toward the Sui and Tang dynasties the "Mao" used by cavalry were fitted with much longer shafts, as is mentioned above. During this era, the use of the "Shuo" (矟) was widespread among the footmen. The "Shuo" can be likened to a pike or simply a long spear.

After the Tang dynasty, the popularity of the "Mao" declined and was replaced by the "Qiang" (枪). The Tang dynasty divided the "Qiang" in four categories: "一曰漆枪， 二曰木枪， 三曰白杆枪， 四曰扑头枪。” Roughly translated the four categories are: Qi (a kind of wood) Spears, Wooden Spears, Bai Gan (A kind of wood) Spears and Pu Tou Qiang. The Qiang that were produced in the Song and Ming dynasties consisted of four major parts: Spearhead, Shaft, End Spike and Tassel. The types of Qiang that exist are many. Among the types there are cavalry Qiang that were the length of one "zhang" (eleven feet and nine inches or 3.58 m), Litte-Flower Spears (Xiao Hua Qiang 小花枪) that are the length of one person and their arm extended above his head, double hooked spears, single hooked spears, ringed spears and many more.

There is some confusion as to how to distinguish the "Qiang" from the "Mao", as they are obviously very similar. Some people say that a "Mao" is longer than a "Qiang", others say that the main difference is between the stiffness of the shaft, where the "Qiang" would be flexible and the "Mao" would be stiff. Scholars seem to lean toward the latter explanation more than the former. Because of the difference in the construction of the "Mao" and the "Qiang", the usage is also different, though there is no definitive answer as to what exactly the differences are between the "Mao" and the "Qiang".

Spears in the Indian society were used both in missile and non-missile form, both by cavalry and foot-soldiers. Mounted spear-fighting was practiced using with a ten-foot, ball-tipped wooden lance called a "bothati", the end of which was covered in dye so that hits may be confirmed. Spears were constructed from a variety of materials such as the "sang" made completely of steel, and the "ballam" which had a bamboo shaft.

The Arab presence in Sindh and the Mameluks of Delhi introduced the Middle Eastern javelin into India.

The Rajputs wielded a type of spear for infantrymen which had a club integrated into the spearhead, and a pointed butt end. Other spears had forked blades, several spear-points, and numerous other innovations. One particular spear unique to India was the "vita" or corded lance. 

Used by the Maratha army, it had a rope connecting the spear with the user's wrist, allowing the weapon to be thrown and pulled back. The "Vel" is a type of spear or lance, originated in Southern India, primarily used by Tamils.

Sikh Nihangs sometimes carry a spear even today. Spears were used in conflicts and training by armed paramilitary units such as the razakars of Nizams of Hyderabad State as late as the second half of the 20th century. Tribal made spears are used in conflicts and rioting in the Northeastern states of India, such as Assam, Arunachal Pradesh, Nagaland, Mizoram and Tripura.

The hoko spear was used in ancient Japan sometime between the Yayoi period and the Heian period, but it became unpopular as early samurai often acted as horseback archers. Medieval Japan employed spears again for infantrymen to use, but it was not until the 11th century in that samurai began to prefer spears over bows. Several polearms were used in the Japanese theatres; the naginata was a glaive-like weapon with a long, curved blade popularly among the samurai and the Buddhist warrior-monks, often used against cavalry; the yari was a longer polearm, with a straight-bladed spearhead, which became the weapon of choice of both the samurai and the ashigaru (footmen) during the Warring States Era; the horseback samurai used shorter yari for his single-armed combat; on the other hand, ashigaru infantries used long yari (similar with European pike) for their massed combat formation.

Filipino spears (sibat) were used as both a weapon and a tool throughout the Philippines. It is also called a "bangkaw" (after the Bankaw Revolt.), "sumbling" or "palupad" in the islands of Visayas and Mindanao. Sibat are typically made from rattan, either with a sharpened tip or a head made from metal. These heads may either be single-edged, double-edged or barbed. Styles vary according to function and origin. For example, a sibat designed for fishing may not be the same as those used for hunting.

The spear was used as the primary weapon in expeditions and battles against neighbouring island kingdoms and it became famous during the 1521 Battle of Mactan, where the chieftain Lapu Lapu of Cebu fought against Spanish forces led by Ferdinand Magellan who was subsequently killed.

As advanced metallurgy was largely unknown in pre-Columbian America outside of Western Mexico and South America, most weapons in Meso-America were made of wood or obsidian. This did not mean that they were less lethal, as obsidian may be sharpened to become many times sharper than steel. Meso-American spears varied greatly in shape and size. While the Aztecs preferred the sword-like macuahuitl for fighting, the advantage of a far-reaching thrusting weapon was recognised, and a large portion of the army would carry the tepoztopilli into battle. The tepoztopilli was a pole-arm, and to judge from depictions in various Aztec codices, it was roughly the height of a man, with a broad wooden head about twice the length of the users' palm or shorter, edged with razor-sharp obsidian blades which were deeply set in grooves carved into the head, and cemented in place with bitumen or plant resin as an adhesive. The tepoztopilli was able both to thrust and slash effectively.

Throwing spears also were used extensively in Meso-American warfare, usually with the help of an atlatl. Throwing spears were typically shorter and more stream-lined than the tepoztopilli, and some had obsidian edges for greater penetration.

Typically, most spears made by Native Americans were created with materials surrounded by their communities. Usually, the shaft of the spear was made with a wooden stick while the head of the spear was fashioned from arrowheads, pieces of metal such as copper, or a bone that had been sharpened. Spears were a preferred weapon by many since it was inexpensive to create, could more easily be taught to others, and could be made quickly and in large quantities.

Native Americans used the Buffalo Pound method to kill buffalo, which required a hunter to dress as a buffalo and lure one into a ravine where other hunters were hiding. Once the buffalo appeared, the other hunters would kill him with spears. A variation of this technique, called the Buffalo Jump, was when a runner would lead the animals towards a cliff. As the buffalo got close to the cliff, other members of the tribe would jump out from behind rocks or trees and scare the buffalo over the cliff. Other hunters would be waiting at the bottom of the cliff to spear the animal to death.

The use of various types of the assegai (a light spear or javelin made of wood and pointed with iron or fire-hardened tip) was widespread all over Africa and it was the most common weapon used before the introduction of firearms. The Zulu, Xhosa and other Nguni tribes of South Africa were renowned for their use of the assegai.

Shaka of the Zulu invented a shorter-style spear with a two-foot shaft and which had a larger, broader blade one foot long. This weapon is otherwise known as the "iklwa" or "ixwa", after the sound that was heard as it was withdrawn from the victim's wound. It was used as a stabbing weapon. The traditional spear was not abandoned, but was used to soften range attack enemy formations before closing in for close quarters battle with the iklwa. This tactical combination originated during Shaka's military reforms. This weapon was typically used with one hand while the off hand held a cow hide shield for protection.

The development of both the long, two-handed pike and gunpowder in Renaissance Europe saw an ever-increasing focus on integrated infantry tactics. Those infantry not armed with these weapons carried variations on the pole-arm, including the halberd and the bill. Ultimately, the spear proper was rendered obsolete on the battlefield. Its last flowering was the half-pike or spontoon, a shortened version of the pike carried by officers and NCOs. While originally a weapon, this came to be seen more as a badge of office, or "leading staff" by which troops were directed. The half-pike, sometimes known as a boarding pike, was also used as a weapon on board ships until the late 19th century.

At the start of the Renaissance, cavalry remained predominantly lance-armed; gendarmes with the heavy knightly lance and lighter cavalry with a variety of lighter lances. By the 1540s, however, pistol-armed cavalry called reiters were beginning to make their mark. Cavalry armed with pistols and other lighter firearms, along with a sword, had virtually replaced lance armed cavalry in Western Europe by the beginning of the 17th century.

One of the earliest forms of killing prey for humans, hunting game with a spear and spear fishing continues to this day as both a means of catching food and as a cultural activity. Some of the most common prey for early humans were mega fauna such as mammoths which were hunted with various kinds of spear. One theory for the Quaternary extinction event was that most of these animals were hunted to extinction by humans with spears. Even after the invention of other hunting weapons such as the bow the spear continued to be used, either as a projectile weapon or used in the hand as was common in boar hunting.


Spear hunting fell out of favour in most of Europe in the 18th century, but continued in Germany, enjoying a revival in the 1930s. Spear hunting is still practiced in the United States. Animals taken are primarily wild boar and deer, although trophy animals such as cats and big game as large as a Cape Buffalo are hunted with spears. Alligator are hunted in Florida with a type of harpoon.

Like many weapons, a spear may also be a symbol of power. In the Chinese martial arts community, the Chinese spear (Qiang 槍) is popularly known as the "king of weapons".

The Celts would symbolically destroy a dead warrior's spear either to prevent its use by another or as a sacrificial offering.

In classical Greek mythology Zeus' bolts of lightning may be interpreted as a symbolic spear. Some would carry that interpretation to the spear that frequently is associated with Athena, interpreting her spear as a symbolic connection to some of Zeus' power beyond the Aegis once he rose to replacing other deities in the pantheon. Athena was depicted with a spear prior to that change in myths, however. Chiron's wedding-gift to Peleus when he married the nymph Thetis in classical Greek mythology, was an ashen spear as the nature of ashwood with its straight grain made it an ideal choice of wood for a spear.

The Romans and their early enemies would force prisoners to walk underneath a 'yoke of spears', which humiliated them. The yoke would consist of three spears, two upright with a third tied between them at a height which made the prisoners stoop. It has been suggested that the arrangement has a magical origin, a way to trap evil spirits. The word "subjugate" has its origins in this practice (from Latin "sub" = under, "jugum" = yoke).
In Norse mythology, the god Odin's spear (named Gungnir) was made by the sons of Ivaldi. It had the special property that it never missed its mark. During the War with the Vanir, Odin symbolically threw Gungnir into the Vanir host. This practice of symbolically casting a spear into the enemy ranks at the start of a fight was sometimes used in historic clashes, to seek Odin's support in the coming battle. In Wagner's opera "Siegfried", the haft of Gungnir is said to be from the "World-Tree" Yggdrasil.

Other spears of religious significance are the Holy Lance and the Lúin of Celtchar, believed by some to have vast mystical powers.

Sir James George Frazer in "The Golden Bough" noted the phallic nature of the spear and suggested that in the Arthurian legends the spear or lance functioned as a symbol of male fertility, paired with the Grail (as a symbol of female fertility).

The Hindu god of war Murugan is worshipped by Tamils in the form of the spear called "Vel", which is his primary weapon.

The term "spear" is also used (in a somewhat archaic manner) to describe the male line of a family, as opposed to the distaff or female line.



Related weapons:


</doc>
<doc id="29236" url="https://en.wikipedia.org/wiki?curid=29236" title="Sigrid Undset">
Sigrid Undset

Sigrid Undset (20 May 1882 – 10 June 1949) was a Norwegian novelist who was awarded the Nobel Prize for Literature in 1928.

Undset was born in Kalundborg, Denmark, but her family moved to Norway when she was two years old. In 1924, she converted to Catholicism. She fled Norway for the United States in 1940 because of her opposition to Nazi Germany and the German invasion and occupation of Norway, but returned after World War II ended in 1945.

Her best-known work is "Kristin Lavransdatter", a trilogy about life in Norway in the Middle Ages, portrayed through the experiences of a woman from birth until death. Its three volumes were published between 1920 and 1922.

Sigrid Undset was born on 20 May 1882 in the small town of Kalundborg, Denmark, at the childhood home of her mother, Charlotte Undset (1855–1939, née Anna Maria Charlotte Gyth). Undset was the eldest of three daughters. She and her family moved to Norway when she was two.

She grew up in the Norwegian capital, Oslo (or Kristiania, as it was known until 1925). When she was only 11 years old, her father, the Norwegian archaeologist Ingvald Martin Undset (1853–1893), died at the age of 40 after a long illness.

The family's economic situation meant that Undset had to give up hope of a university education and after a one-year secretarial course she obtained work at the age of 16 as a secretary with an engineering company in Kristiania, a post she was to hold for 10 years.

She joined the Norwegian Authors' Union in 1907 and from 1933 through 1935 headed its Literary Council, eventually serving as the union's chairman from 1936 until 1940.

While employed at office work, Undset wrote and studied. She was 16 years old when she made her first attempt at writing a novel set in the Nordic Middle Ages. The manuscript, a historical novel set in medieval Denmark, was ready by the time she was 22. It was turned down by the publishing house.

Nonetheless, two years later, she completed another manuscript, much less voluminous than the first at only 80 pages. She had put aside the Middle Ages and had instead produced a realistic description of a woman with a middle-class background in contemporary Kristiania. This book was also refused by the publishers at first but it was subsequently accepted. The title was "Fru Marta Oulie", and the opening sentence (the words of the book's main character) scandalised readers: "I have been unfaithful to my husband".

Thus, at the age of 25, Undset made her literary debut with a short realistic novel on adultery, set against a contemporary background. It created a stir, and she found herself ranked as a promising young author in Norway. During the years up to 1919, Undset published a number of novels set in contemporary Kristiania. Her contemporary novels of the period 1907–1918 are about the city and its inhabitants. They are stories of working people, of trivial family destinies, of the relationship between parents and children. Her main subjects are women and their love. Or, as she herself put it—in her typically curt and ironic manner—"the immoral kind" (of love).

This realistic period culminated in the novels "Jenny" (1911) and "Vaaren" (Spring) (1914). The first is about a woman painter who, as a result of romantic crises, believes that she is wasting her life, and, in the end, commits suicide. The other tells of a woman who succeeds in saving both herself and her love from a serious matrimonial crisis, finally creating a secure family. These books placed Undset apart from the incipient women's emancipation movement in Europe.

Undset's books sold well from the start, and, after the publication of her third book, she left her office job and prepared to live on her income as a writer. Having been granted a writer's scholarship, she set out on a lengthy journey in Europe. After short stops in Denmark and Germany, she continued to Italy, arriving in Rome in December 1909, where she remained for nine months. Undset's parents had had a close relationship with Rome, and, during her stay there, she followed in their footsteps. The encounter with Southern Europe meant a great deal to her; she made friends within the circle of Scandinavian artists and writers in Rome.

In Rome, Undset met Anders Castus Svarstad, a Norwegian painter, whom she married almost three years later. She was 30; Svarstad was nine years older, married, and had a wife and three children in Norway. It was nearly three years before Svarstad got his divorce from his first wife.

Undset and Svarstad were married in 1912 and went to stay in London for six months. From London, they returned to Rome, where their first child was born in January 1913. A boy, he was named after his father. In the years up to 1919, she had another child, and the household also took in Svarstad's three children from his first marriage. These were difficult years: her second child, a girl, was mentally handicapped, as was one of Svarstad's sons by his first wife.

She continued writing, finishing her last realistic novels and collections of short stories. She also entered the public debate on topical themes: women's emancipation and other ethical and moral issues. She had considerable polemical gifts, and was critical of emancipation as it was developing, and of the moral and ethical decline she felt was threatening in the wake of the First World War.

In 1919, she moved to Lillehammer, a small town in the Gudbrand Valley in southeast Norway, taking her two children with her. She was then expecting her third child. The intention was that she should take a rest at Lillehammer and move back to Kristiania as soon as Svarstad had their new house in order. However, the marriage broke down and a divorce followed. In August 1919, she gave birth to her third child, at Lillehammer. She decided to make Lillehammer her home, and within two years, Bjerkebæk, a large house of traditional Norwegian timber architecture, was completed, along with a large fenced garden with views of the town and the villages around. Here she was able to retreat and concentrate on her writing.

After the birth of her third child, and with a secure roof over her head, Undset started a major project: "Kristin Lavransdatter". She was at home in the subject matter, having written a short novel at an earlier stage about a period in Norwegian history closer to the Pre-Christian era. She had also published a Norwegian retelling of the Arthurian legends. She had studied Old Norse manuscripts and Medieval chronicles and visited and examined Medieval churches and monasteries, both at home and abroad. She was now an authority on the period she was portraying and a very different person from the 22-year-old who had written her first novel about the Middle Ages. 

It was only after the end of her marriage that Undset grew mature enough to write her masterpiece. In the years between 1920 and 1927, she first published the three-volume "Kristin", and then the 4-volume "Olav" (Audunssøn), swiftly translated into English as "The Master of Hestviken". Simultaneously with this creative process, she was engaged in trying to find meaning in her own life, finding the answer in God.

Undset experimented with modernist tropes such as stream of consciousness in her novel, although the original English translation by Charles Archer excised many of these passages. In 1997, the first volume of Tiina Nunnally's new translation of the work won the PEN/Faulkner Award for Fiction in the category of translation. The names of each volume were translated by Archer as "The Bridal Wreath", "The Mistress of Husaby", and "The Cross", and by Nunnally as "The Wreath", "The Wife", and "The Cross".

Both Undset's parents were atheists and, although, in accord with the norm of the day, she and her two younger sisters were baptised and with their mother regularly attended the local Lutheran church, the milieu in which they were raised was a thoroughly secular one. Undset spent much of her life as an agnostic, but marriage and the outbreak of the First World War were to change her attitudes. During those difficult years she experienced a crisis of faith, almost imperceptible at first, then increasingly strong. The crisis led her from clear agnostic skepticism, by way of painful uneasiness about the ethical decline of the age, towards Christianity.

In all her writing, one senses an observant eye for the mystery of life and for that which cannot be explained by reason or the human intellect. At the back of her sober, almost brutal realism, there is always an inkling of something unanswerable. At any rate, this crisis radically changed her views and ideology. Whereas she had once believed that man created God, she eventually came to believe that God created man.

However, she did not turn to the established Lutheran Church of Norway, where she had been nominally reared. She was received into the Catholic Church in November 1924, after thorough instruction from the Catholic priest in her local parish. She was 42 years old. She subsequently became a lay Dominican.

It is noteworthy that "The Master of Hestviken", written immediately after Undset's conversion, takes place in a historical period when Norway was Catholic, that it has very religious themes of the main character's relations with God and his deep feeling of sin, and that the Medieval Catholic Church is presented in a favorable light, with virtually all clergy and monks in the series being positive characters.

In Norway, Undset's conversion to Catholicism was not only considered sensational; it was scandalous. It was also noted abroad, where her name was becoming known through the international success of "Kristin Lavransdatter". At the time, there were very few practicing Catholics in Norway, which was an almost exclusively Lutheran country. Anti-Catholicism was widespread not only among the Lutheran clergy, but through large sections of the population. Likewise, there was just as much anti-Catholic scorn among the Norwegian intelligentsia, many of whom were adherents of socialism and communism The attacks against her faith and character were quite vicious at times, with the result that Undset's literary gifts were aroused in response. For many years, she participated in the public debate, going out of her way to defend the Catholic Church. In response, she was swiftly dubbed "The Mistress of Bjerkebæk" and "The Catholic Lady".

At the end of this creative eruption, Undset entered calmer waters. After 1929, she completed a series of novels set in contemporary Oslo, with a strong Catholic element. She selected her themes from the small Catholic community in Norway. But here also, the main theme is love. She also published a number of weighty historical works which put the history of Norway into a sober perspective. In addition, she translated several Icelandic sagas into Modern Norwegian and published a number of literary essays, mainly on English literature, of which a long essay on the Brontë sisters, and one on D. H. Lawrence, are especially worth mentioning.

In 1934, she published "Eleven Years Old", an autobiographical work. With a minimum of camouflage, it tells the story of her own childhood in Kristiania, of her home, rich in intellectual values and love, and of her sick father. 

At the end of the 1930s, she commenced work on a new historical novel set in 18th century Scandinavia. Only the first volume, "Madame Dorthea", was published, in 1939. The Second World War broke out that same year and proceeded to break her, both as a writer and as a woman. She never completed her new novel. When Joseph Stalin's invasion of Finland touched off the Winter War, Undset supported the Finnish war effort by donating her Nobel Prize on 25 January 1940.

When Germany invaded Norway in April 1940, Undset was forced to flee. She had strongly criticised Hitler since the early 1930s, and, from an early date, her books were banned in Nazi Germany. She had no wish to become a target of the Gestapo and fled to neutral Sweden. Her eldest son, Second Lieutenant Anders Svarstad of the Norwegian Army, was killed in action at the age of 27, on 27 April 1940, in an engagement with German troops at Segalstad Bridge in Gausdal.

Undset's sick daughter had died shortly before the outbreak of the war. Bjerkebæk was requisitioned by the Wehrmacht, and used as officers' quarters throughout the Occupation of Norway.

In 1940, Undset and her younger son left neutral Sweden for the United States. There, she untiringly pleaded her occupied country's cause and that of Europe's Jews in writings, speeches and interviews. She lived in Brooklyn Heights, New York. She was active in St. Ansgar's Scandinavian Catholic League and wrote several articles for its bulletin. She also traveled to Florida, where she became close friends with novelist Marjorie Kinnan Rawlings.

Following the German execution of the Danish Lutheran pastor Kaj Munk on 4 January 1944, the Danish resistance newspaper "De frie Danske" printed condemning articles from influential Scandinavians, including Undset.

Undset returned to Norway after the liberation in 1945. She lived another four years but never published another word. Undset died at 67 in Lillehammer, Norway, where she had lived from 1919 through 1940. She was buried in the village of Mesnali, 15 kilometers east of Lillehammer, where also her daughter and the son who died in battle are remembered. The grave is recognizable by three black crosses.







</doc>
<doc id="29238" url="https://en.wikipedia.org/wiki?curid=29238" title="Systems theory">
Systems theory

Systems theory is the interdisciplinary study of systems. A system is a cohesive conglomeration of interrelated and interdependent parts which can be natural or human-made. Every system is bounded by space and time, influenced by its environment, defined by its structure and purpose, and expressed through its functioning. A system may be more than the sum of its parts if it expresses synergy or emergent behavior.

Changing one part of a system may affect other parts or the whole system. It may be possible to predict these changes in patterns of behavior. For systems that learn and adapt, the growth and the degree of adaptation depend upon how well the system is engaged with its environment. Some systems support other systems, maintaining the other system to prevent failure. The goals of systems theory are to model a system's dynamics, constraints, conditions, and to elucidate principles (such as purpose, measure, methods, tools) that can be discerned and applied to other systems at every level of nesting, and in a wide range of fields for achieving optimized equifinality.

General systems theory is about developing broadly applicable concepts and principles, as opposed to concepts and principles specific to one domain of knowledge. It distinguishes dynamic or active systems from static or passive systems. Active systems are activity structures or components that interact in behaviours and processes. Passive systems are structures and components that are being processed. For example, a program is passive when it is a disc file and active when it runs in memory. The field is related to systems thinking, machine logic, and systems engineering.


The term "general systems theory" originates from Bertalanffy's general systems theory (GST). His ideas were adopted by others including Kenneth E. Boulding, William Ross Ashby and Anatol Rapoport working in mathematics, psychology, biology, game theory, and social network analysis.

In sociology, systems thinking started earlier, in the 20th century. Stichweh states: "... Since its beginnings the social sciences were an important part of the establishment of systems theory... the two most influential suggestions were the comprehensive sociological versions of systems theory which were proposed by Talcott Parsons since the 1950s and by Niklas Luhmann since the 1970s." References include Parsons' action theory and Luhmann's social systems theory.

Elements of systems thinking can also be seen in the work of James Clerk Maxwell, in particular control theory.

Systems theory is manifest in the work of practitioners in many disciplines, for example the works of biologist Ludwig von Bertalanffy, linguist Béla H. Bánáthy, sociologist Talcott Parsons, and in the study of ecological systems by Howard T. Odum, Eugene Odum and is Fritjof Capra's study of organizational theory, and in the study of management by Peter Senge, in interdisciplinary areas such as Human Resource Development in the works of Richard A. Swanson, and in the works of educators Debora Hammond and Alfonso Montuori.

As a transdisciplinary, interdisciplinary, and multiperspectival endeavor, systems theory brings together principles and concepts from ontology, the philosophy of science, physics, computer science, biology and engineering as well as geography, sociology, political science, psychotherapy (especially family systems therapy), and economics. Systems theory promotes dialogue between autonomous areas of study as well as within systems science itself.

In this respect, with the possibility of misinterpretations, von Bertalanffy believed a general theory of systems "should be an important regulative device in science", to guard against superficial analogies that "are useless in science and harmful in their practical consequences". Others remain closer to the direct systems concepts developed by the original theorists. For example, Ilya Prigogine, of the Center for Complex Quantum Systems at the University of Texas, Austin, has studied emergent properties, suggesting that they offer analogues for living systems. The theories of autopoiesis of Francisco Varela and Humberto Maturana represent further developments in this field. Important names in contemporary systems science include Russell Ackoff, Ruzena Bajcsy, Béla H. Bánáthy, Gregory Bateson, Anthony Stafford Beer, Peter Checkland, Barbara Grosz, Brian Wilson, Robert L. Flood, Allenna Leonard, Radhika Nagpal, Fritjof Capra, Warren McCulloch, Kathleen Carley, Michael C. Jackson, Katia Sycara, and Edgar Morin among others.

With the modern foundations for a general theory of systems following World War I, Ervin Laszlo, in the preface for Bertalanffy's book: "Perspectives on General System Theory", points out that the translation of "general system theory" from German into English has "wrought a certain amount of havoc":

A system in this frame of reference can contain regularly interacting or interrelating groups of activities. For example, in noting the influence in organizational psychology as the field evolved from "an individually oriented industrial psychology to a systems and developmentally oriented organizational psychology", some theorists recognize that organizations have complex social systems; separating the parts from the whole reduces the overall effectiveness of organizations. This difference, from conventional models that center on individuals, structures, departments and units, separates in part from the whole, instead of recognizing the interdependence between groups of individuals, structures and processes that enable an organization to function. Laszlo explains that the new systems view of organized complexity went "one step beyond the Newtonian view of organized simplicity" which reduced the parts from the whole, or understood the whole without relation to the parts. The relationship between organisations and their environments can be seen as the foremost source of complexity and interdependence. In most cases, the whole has properties that cannot be known from analysis of the constituent elements in isolation. Béla H. Bánáthy, who argued—along with the founders of the systems society—that "the benefit of humankind" is the purpose of science, has made significant and far-reaching contributions to the area of systems theory. For the Primer Group at ISSS, Bánáthy defines a perspective that iterates this view:

Similar ideas are found in learning theories that developed from the same fundamental concepts, emphasising how understanding results from knowing concepts both in part and as a whole. In fact, Bertalanffy's organismic psychology paralleled the learning theory of Jean Piaget. Some consider interdisciplinary perspectives critical in breaking away from industrial age models and thinking, wherein history represents history and math represents math, while the arts and sciences specialization remain separate and many treat teaching as behaviorist conditioning. The contemporary work of Peter Senge provides detailed discussion of the commonplace critique of educational systems grounded in conventional assumptions about learning, including the problems with fragmented knowledge and lack of holistic learning from the "machine-age thinking" that became a "model of school separated from daily life". In this way some systems theorists attempt to provide alternatives to, and evolved ideation from orthodox theories which have grounds in classical assumptions, including individuals such as Max Weber and Émile Durkheim in sociology and Frederick Winslow Taylor in scientific management. The theorists sought holistic methods by developing systems concepts that could integrate with different areas.

Some may view the contradiction of reductionism in conventional theory (which has as its subject a single part) as simply an example of changing assumptions. The emphasis with systems theory shifts from parts to the organization of parts, recognizing interactions of the parts as not static and constant but dynamic processes. Some questioned the conventional closed systems with the development of open systems perspectives. The shift originated from absolute and universal authoritative principles and knowledge to relative and general conceptual and perceptual knowledge and still remains in the tradition of theorists that sought to provide means to organize human life. In other words, theorists rethought the preceding history of ideas; they did not lose them. Mechanistic thinking was particularly critiqued, especially the industrial-age mechanistic metaphor for the mind from interpretations of Newtonian mechanics by Enlightenment philosophers and later psychologists that laid the foundations of modern organizational theory and management by the late 19th century.

System dynamics is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, and time delays.

Systems biology is a movement that draws on several trends in bioscience research. Proponents describe systems biology as a biology-based inter-disciplinary study field that focuses on complex interactions in biological systems, claiming that it uses a new perspective (holism instead of reduction). Particularly from the year 2000 onwards, the biosciences use the term widely and in a variety of contexts. An often stated ambition of systems biology is the modelling and discovery of emergent properties which represents properties of a system whose theoretical description requires the only possible useful techniques to fall under the remit of systems biology. It is thought that Ludwig von Bertalanffy may have created the term systems biology in 1928.

Systems chemistry is the science of studying networks of interacting molecules, to create new functions from a set (or library) of molecules with different hierarchical levels and emergent properties. Systems chemistry is also related to the origin of life (abiogenesis).

Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems. Systems ecology can be seen as an application of general systems theory to ecology. Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties. Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions. It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Systems engineering is an interdisciplinary approach and means for enabling the realisation and deployment of successful systems. It can be viewed as the application of engineering techniques to the engineering of systems, as well as the application of a systems approach to engineering efforts. Systems engineering integrates other disciplines and specialty groups into a team effort, forming a structured development process that proceeds from concept to production to operation and disposal. Systems engineering considers both the business and the technical needs of all customers, with the goal of providing a quality product that meets the user's needs.

Systems psychology is a branch of psychology that studies human behaviour and experience in complex systems. It received inspiration from systems theory and systems thinking, as well as the basics of theoretical work from Roger Barker, Gregory Bateson, Humberto Maturana and others. It makes an approach in psychology in which groups and individuals receive consideration as systems in homeostasis. Systems psychology "includes the domain of engineering psychology, but in addition seems more concerned with societal systems and with the study of motivational, affective, cognitive and group behavior that holds the name engineering psychology." In systems psychology, "characteristics of organizational behaviour, for example individual needs, rewards, expectations, and attributes of the people interacting with the systems, considers this process in order to create an effective system".

Whether considering the first systems of written communication with Sumerian cuneiform to Mayan numerals, or the feats of engineering with the Egyptian pyramids, systems thinking can date back to antiquity. Differentiated from Western rationalist traditions of philosophy, C. West Churchman often identified with the I Ching as a systems approach sharing a frame of reference similar to pre-Socratic philosophy and Heraclitus. Von Bertalanffy traced systems concepts to the philosophy of G.W. Leibniz and Nicholas of Cusa's "coincidentia oppositorum". While modern systems can seem considerably more complicated, today's systems may embed themselves in history.

Figures like James Joule and Sadi Carnot represent an important step to introduce the "systems approach" into the (rationalist) hard sciences of the 19th century, also known as the energy transformation. Then, the thermodynamics of this century, by Rudolf Clausius, Josiah Gibbs and others, established the "system" reference model as a formal scientific object.

The Society for General Systems Research specifically catalyzed systems theory as an area of study, which developed following the World Wars from the work of Ludwig von Bertalanffy, Anatol Rapoport, Kenneth E. Boulding, William Ross Ashby, Margaret Mead, Gregory Bateson, C. West Churchman and others in the 1950s. Cognizant of advances in science that questioned classical assumptions in the organizational sciences, Bertalanffy's idea to develop a theory of systems began as early as the interwar period, publishing "An Outline for General Systems Theory" in the "British Journal for the Philosophy of Science", Vol 1, No. 2, by 1950. Where assumptions in Western science from Greek thought with Plato and Aristotle to Newton's "Principia" have historically influenced all areas from the hard to social sciences (see David Easton's seminal development of the "political system" as an analytical construct), the original theorists explored the implications of twentieth century advances in terms of systems.

People have studied subjects like complexity, self-organization, connectionism and adaptive systems in the 1940s and 1950s. In fields like cybernetics, researchers such as Norbert Wiener, William Ross Ashby, John von Neumann and Heinz von Foerster, examined complex systems mathematically. John von Neumann discovered cellular automata and self-reproducing systems, again with only pencil and paper. Aleksandr Lyapunov and Jules Henri Poincaré worked on the foundations of chaos theory without any computer at all. At the same time Howard T. Odum, known as a radiation ecologist, recognized that the study of general systems required a language that could depict energetics, thermodynamics and kinetics at any system scale. Odum developed a general system, or universal language, based on the circuit language of electronics, to fulfill this role, known as the Energy Systems Language. Between 1929-1951, Robert Maynard Hutchins at the University of Chicago had undertaken efforts to encourage innovation and interdisciplinary research in the social sciences, aided by the Ford Foundation with the interdisciplinary Division of the Social Sciences established in 1931. Numerous scholars had actively engaged in these ideas before (Tectology by Alexander Bogdanov, published in 1912-1917, is a remarkable example), but in 1937, von Bertalanffy presented the general theory of systems at a conference at the University of Chicago.

The systems view was based on several fundamental ideas. First, all phenomena can be viewed as a web of relationships among elements, or a system. Second, all systems, whether electrical, biological, or social, have common patterns, behaviors, and properties that the observer can analyze and use to develop greater insight into the behavior of complex phenomena and to move closer toward a unity of the sciences. System philosophy, methodology and application are complementary to this science. By 1956, theorists established the Society for General Systems Research, which they renamed the International Society for Systems Science in 1988. The Cold War affected the research project for systems theory in ways that sorely disappointed many of the seminal theorists. Some began to recognize that theories defined in association with systems theory had deviated from the initial General Systems Theory (GST) view. The economist Kenneth Boulding, an early researcher in systems theory, had concerns over the manipulation of systems concepts. Boulding concluded from the effects of the Cold War that abuses of power always prove consequential and that systems theory might address such issues. Since the end of the Cold War, a renewed interest in systems theory emerged, combined with efforts to strengthen an ethical view on the subject.

Many early systems theorists aimed at finding a general systems theory that could explain all systems in all fields of science. The term goes back to Bertalanffy's book titled "General System Theory: Foundations, Development, Applications" from 1968. He developed the "allgemeine Systemlehre" (general systems theory) first via lectures beginning in 1937 and then via publications beginning in 1946.

Von Bertalanffy's objective was to bring together under one heading the organismic science he had observed in his work as a biologist. His desire was to use the word "system" for those principles that are common to systems in general. In GST, he writes:

Ervin Laszlo in the preface of von Bertalanffy's book "Perspectives on General System Theory":

Ludwig von Bertalanffy outlines systems inquiry into three major domains: Philosophy, Science, and Technology. In his work with the Primer Group, Béla H. Bánáthy generalized the domains into four integratable domains of systemic inquiry:

These operate in a recursive relationship, he explained. Integrating Philosophy and Theory as Knowledge, and Method and Application as action, Systems Inquiry then is knowledgeable action.

Cybernetics is the study of the communication and control of regulatory feedback both in living and lifeless systems (organisms, organizations, machines), and in combinations of those. Its focus is how anything (digital, mechanical or biological) controls its behavior, processes information, reacts to information, and changes or can be changed to better accomplish those three primary tasks.

The terms "systems theory" and "cybernetics" have been widely used as synonyms. Some authors use the term "cybernetic" systems to denote a proper subset of the class of general systems, namely those systems that include feedback loops. However Gordon Pask's differences of eternal interacting actor loops (that produce finite products) makes general systems a proper subset of cybernetics. According to Jackson (2000), von Bertalanffy promoted an embryonic form of general system theory (GST) as early as the 1920s and 1930s but it was not until the early 1950s it became more widely known in scientific circles.

Threads of cybernetics began in the late 1800s that led toward the publishing of seminal works (e.g., Wiener's "Cybernetics" in 1948 and von Bertalanffy's "General Systems Theory" in 1968). Cybernetics arose more from engineering fields and GST from biology. If anything it appears that although the two probably mutually influenced each other, cybernetics had the greater influence. Von Bertalanffy (1969) specifically makes the point of distinguishing between the areas in noting the influence of cybernetics: "Systems theory is frequently identified with cybernetics and control theory. This again is incorrect. Cybernetics as the theory of control mechanisms in technology and nature is founded on the concepts of information and feedback, but as part of a general theory of systems;" then reiterates: "the model is of wide application but should not be identified with 'systems theory' in general", and that "warning is necessary against its incautious expansion to fields for which its concepts are not made." (17-23). Jackson (2000) also claims von Bertalanffy was informed by Alexander Bogdanov's three volume "Tectology" that was published in Russia between 1912 and 1917, and was translated into German in 1928. He also states it is clear to Gorelik (1975) that the "conceptual part" of general system theory (GST) had first been put in place by Bogdanov. The similar position is held by Mattessich (1978) and Capra (1996). Ludwig von Bertalanffy never even mentioned Bogdanov in his works, which Capra (1996) finds "surprising".

Cybernetics, catastrophe theory, chaos theory and complexity theory have the common goal to explain complex systems that consist of a large number of mutually interacting and interrelated parts in terms of those interactions. Cellular automata (CA), neural networks (NN), artificial intelligence (AI), and artificial life (ALife) are related fields, but they do not try to describe general (universal) complex (singular) systems. The best context to compare the different "C"-Theories about complex systems is historical, which emphasizes different tools and methodologies, from pure mathematics in the beginning to pure computer science now. Since the beginning of chaos theory when Edward Lorenz accidentally discovered a strange attractor with his computer, computers have become an indispensable source of information. One could not imagine the study of complex systems without the use of computers today.

Complex adaptive systems (CAS) are special cases of complex systems. They are "complex" in that they are diverse and composed of multiple, interconnected elements; they are "adaptive" in that they have the capacity to change and learn from experience. In contrast to control systems in which negative feedback dampens and reverses disequilibria, CAS are often subject to positive feedback, which magnifies and perpetuates changes, converting local irregularities into global features. Another mechanism, Dual-phase evolution arises when connections between elements repeatedly change, shifting the system between phases of variation and selection that reshape the system. Differently from Stafford Beer’s Management Cybernetics, Cultural Agency Theory (CAT) provides a modelling approach to explore predefined contexts and can be adapted to reflect those contexts.

The term "complex adaptive system" was coined at the interdisciplinary Santa Fe Institute (SFI), by John H. Holland, Murray Gell-Mann and others. An alternative conception of complex adaptive (and learning) systems, methodologically at the interface between natural and social science, has been presented by Kristo Ivanov in terms of hypersystems. This concept intends to offer a theoretical basis for understanding and implementing participation of "users", decisions makers, designers and affected actors, in the development or maintenance of self-learning systems.



Organizations


</doc>
<doc id="29240" url="https://en.wikipedia.org/wiki?curid=29240" title="Lists of stars">
Lists of stars

The following are lists of stars. These are astronomical objects that spend some portion of their existence generating energy through thermonuclear fusion.







The following is a list of particularly notable actual or hypothetical stars that have their own articles in Wikipedia, but are not included in the lists above.





</doc>
<doc id="29247" url="https://en.wikipedia.org/wiki?curid=29247" title="Sulfuric acid">
Sulfuric acid

Sulfuric acid (American spelling) or sulphuric acid (British spelling), also known as oil of vitriol, is a mineral acid composed of the elements sulfur, oxygen and hydrogen, with molecular formula HSO. It is a colourless, odourless, and viscous liquid that is soluble in water and is synthesized in reactions that are highly exothermic.

Its corrosiveness can be mainly ascribed to its strong acidic nature, and, if at a high concentration, its dehydrating properties. It is also hygroscopic, readily absorbing water vapor from the air. Upon contact, sulfuric acid can cause severe chemical burns and even secondary thermal burns; it is very dangerous even at lower concentrations.

Sulfuric acid is a very important commodity chemical, and a nation's sulfuric acid production is a good indicator of its industrial strength. It is widely produced with different methods, such as contact process, wet sulfuric acid process, lead chamber process and some other methods.

Sulfuric acid is also a key substance in the chemical industry. It is most commonly used in fertilizer manufacture, but is also important in mineral processing, oil refining, wastewater processing, and chemical synthesis. It has a wide range of end applications including in domestic acidic drain cleaners, as an electrolyte in lead-acid batteries, in dehydrating a compound, and in various cleaning agents.

Although nearly 100% sulfuric acid solutions can be made, the subsequent loss of at the boiling point brings the concentration to 98.3% acid. The 98.3% grade is more stable in storage, and is the usual form of what is described as "concentrated sulfuric acid". Other concentrations are used for different purposes. Some common concentrations are:
"Chamber acid" and "tower acid" were the two concentrations of sulfuric acid produced by the lead chamber process, chamber acid being the acid produced in the lead chamber itself (<70% to avoid contamination with nitrosylsulfuric acid) and tower acid being the acid recovered from the bottom of the Glover tower. They are now obsolete as commercial concentrations of sulfuric acid, although they may be prepared in the laboratory from concentrated sulfuric acid if needed. In particular, "10M" sulfuric acid (the modern equivalent of chamber acid, used in many titrations) is prepared by slowly adding 98% sulfuric acid to an equal volume of water, with good stirring: the temperature of the mixture can rise to 80 °C (176 °F) or higher.

Sulfuric acid reacts with its anhydride, , to form , called "pyrosulfuric acid", "fuming sulfuric acid", "Disulfuric acid" or "oleum" or, less commonly, "Nordhausen acid". Concentrations of oleum are either expressed in terms of % (called % oleum) or as % (the amount made if were added); common concentrations are 40% oleum (109% ) and 65% oleum (114.6% ). Pure is a solid with melting point of 36 °C.

Pure sulfuric acid has a vapor pressure of <0.001 mmHg at 25 °C and 1 mmHg at 145.8 °C, and 98% sulfuric acid has a <1 mmHg vapor pressure at 40 °C.

Pure sulfuric acid is a viscous clear liquid, like oil, and this explains the old name of the acid ('oil of vitriol').

Commercial sulfuric acid is sold in several different purity grades. Technical grade is impure and often colored, but is suitable for making fertilizer. Pure grades, such as United States Pharmacopeia (USP) grade, are used for making pharmaceuticals and dyestuffs. Analytical grades are also available.

Nine hydrates are known, but four of them were confirmed to be tetrahydrate (HSO·4HO), hemihexahydrate (HSO·HO) and octahydrate (HSO·8HO).

Anhydrous is a very polar liquid, having a dielectric constant of around 100. It has a high electrical conductivity, caused by dissociation through protonating itself, a process known as autoprotolysis.
The equilibrium constant for the autoprotolysis is

The comparable equilibrium constant for water, "K" is 10, a factor of 10 (10 billion) smaller.

In spite of the viscosity of the acid, the effective conductivities of the and ions are high due to an intramolecular proton-switch mechanism (analogous to the Grotthuss mechanism in water), making sulfuric acid a good conductor of electricity. It is also an excellent solvent for many reactions.

Because the hydration reaction of sulfuric acid is highly exothermic, dilution should always be performed by adding the acid to the water rather than the water to the acid. Because the reaction is in an equilibrium that favors the rapid protonation of water, addition of acid to the water ensures that the "acid" is the limiting reagent. This reaction is best thought of as the formation of hydronium ions:

Because the hydration of sulfuric acid is thermodynamically favorable and the affinity of it for water is sufficiently strong, sulfuric acid is an excellent dehydrating agent. Concentrated sulfuric acid has a very powerful dehydrating property, removing water (HO) from other chemical compounds including sugar and other carbohydrates and producing carbon, heat, and steam.

In the laboratory, this is often demonstrated by mixing table sugar (sucrose) into sulfuric acid. The sugar changes from white to dark brown and then to black as carbon is formed. A rigid column of black, porous carbon will emerge as well. The carbon will smell strongly of caramel due to the heat generated.

Similarly, mixing starch into concentrated sulfuric acid will give elemental carbon and water as absorbed by the sulfuric acid (which becomes slightly diluted). The effect of this can be seen when concentrated sulfuric acid is spilled on paper which is composed of cellulose; the cellulose reacts to give a burnt appearance, the carbon appears much as soot would in a fire.
Although less dramatic, the action of the acid on cotton, even in diluted form, will destroy the fabric.

The reaction with copper(II) sulfate can also demonstrate the dehydration property of sulfuric acid. The blue crystal is changed into white powder as water is removed.

As an acid, sulfuric acid reacts with most bases to give the corresponding sulfate. For example, the blue copper salt copper(II) sulfate, commonly used for electroplating and as a fungicide, is prepared by the reaction of copper(II) oxide with sulfuric acid:

Sulfuric acid can also be used to displace weaker acids from their salts. Reaction with sodium acetate, for example, displaces acetic acid, , and forms sodium bisulfate:

Similarly, reacting sulfuric acid with potassium nitrate can be used to produce nitric acid and a precipitate of potassium bisulfate. When combined with nitric acid, sulfuric acid acts both as an acid and a dehydrating agent, forming the nitronium ion , which is important in nitration reactions involving electrophilic aromatic substitution. This type of reaction, where protonation occurs on an oxygen atom, is important in many organic chemistry reactions, such as Fischer esterification and dehydration of alcohols.

When allowed to react with superacids, sulfuric acid can act as a base and be protonated, forming the [HSO] ion. Salt of [HSO] have been prepared using the following reaction in liquid HF:

The above reaction is thermodynamically favored due to the high bond enthalpy of the Si–F bond in the side product. Protonation using simply HF/SbF, however, have met with failure, as pure sulfuric acid undergoes self-ionization to give [HO] ions, which prevents the conversion of HSO to [HSO] by the HF/SbF system:

Even dilute sulfuric acid reacts with many metals via a single displacement reaction as with other typical acids, producing hydrogen gas and salts (the metal sulfate). It attacks reactive metals (metals at positions above copper in the reactivity series) such as iron, aluminium, zinc, manganese, magnesium, and nickel.

Concentrated sulfuric acid can serve as an oxidizing agent, releasing sulfur dioxide:

Lead and tungsten, however, are resistant to sulfuric acid.

Hot concentrated sulfuric acid oxidizes carbon (as bituminous coal) and sulfur.

It reacts with sodium chloride, and gives hydrogen chloride gas and sodium bisulfate:

Benzene undergoes electrophilic aromatic substitution with sulfuric acid to give the corresponding sulfonic acids:

Pure sulfuric acid is not encountered naturally on Earth in anhydrous form, due to its great affinity for water. Dilute sulfuric acid is a constituent of acid rain, which is formed by atmospheric oxidation of sulfur dioxide in the presence of water – i.e., oxidation of sulfurous acid. When sulfur-containing fuels such as coal or oil are burned, sulfur dioxide is the main byproduct (besides the chief products carbon oxides and water).

Sulfuric acid is formed naturally by the oxidation of sulfide minerals, such as iron sulfide. The resulting water can be highly acidic and is called acid mine drainage (AMD) or acid rock drainage (ARD). This acidic water is capable of dissolving metals present in sulfide ores, which results in brightly colored, toxic solutions. The oxidation of pyrite (iron sulfide) by molecular oxygen produces iron(II), or :

The can be further oxidized to :

The produced can be precipitated as the hydroxide or hydrous iron oxide:

The iron(III) ion ("ferric iron") can also oxidize pyrite:

When iron(III) oxidation of pyrite occurs, the process can become rapid. pH values below zero have been measured in ARD produced by this process.

ARD can also produce sulfuric acid at a slower rate, so that the acid neutralizing capacity (ANC) of the aquifer can neutralize the produced acid. In such cases, the total dissolved solids (TDS) concentration of the water can be increased from the dissolution of minerals from the acid-neutralization reaction with the minerals.

Sulfuric acid is used as a defense by certain marine species, for example, the phaeophyte alga "Desmarestia munda" (order Desmarestiales) concentrates sulfuric acid in cell vacuoles.

In the stratosphere, the atmosphere's second layer that is generally between 10 and 50 km above Earth's surface, sulfuric acid is formed by the oxidation of volcanic sulfur dioxide by the hydroxyl radical:

Because sulfuric acid reaches supersaturation in the stratosphere, it can nucleate aerosol particles and provide a surface for aerosol growth via condensation and coagulation with other water-sulfuric acid aerosols. This results in the stratospheric aerosol layer.

The permanent Venusian clouds produce a concentrated acid rain, as the clouds in the atmosphere of Earth produce water rain. Jupiter's moon Europa is also thought to have an atmosphere containing sulfuric acid hydrates.

Sulfuric acid is produced from sulfur, oxygen and water via the conventional contact process (DCDA) or the wet sulfuric acid process (WSA).

In the first step, sulfur is burned to produce sulfur dioxide.

The sulfur dioxide is oxidized to sulfur trioxide by oxygen in the presence of a vanadium(V) oxide catalyst. This reaction is reversible and the formation of the sulfur trioxide is exothermic.

The sulfur trioxide is absorbed into 97–98% to form oleum (), also known as fuming sulfuric acid. The oleum is then diluted with water to form concentrated sulfuric acid.

Directly dissolving in water is not practiced.

In the first step, sulfur is burned to produce sulfur dioxide:

or, alternatively, hydrogen sulfide () gas is incinerated to gas:
The sulfur dioxide then oxidized to sulfur trioxide using oxygen with vanadium(V) oxide as catalyst.

The sulfur trioxide is hydrated into sulfuric acid :

The last step is the condensation of the sulfuric acid to liquid 97–98% :

Another method is the less well-known metabisulfite method, in which metabisulfite is placed at the bottom of a beaker, and 12.6 molar concentration hydrochloric acid is added. The resulting gas is bubbled through nitric acid, which will release brown/red vapors. The completion of the reaction is indicated by the ceasing of the fumes. This method does not produce an inseparable mist, which is quite convenient.

In principle, sulfuric acid can be produced in the laboratory by burning sulfur in air followed by dissolving the resulting sulfur dioxide in a hydrogen peroxide solution.

Alternatively, dissolving sulfur dioxide in an aqueous solution of a certain oxidizing metal salt such as copper (II) or iron (III) chloride:

Two less well-known laboratory methods of producing sulfuric acid, albeit in dilute form and require some extra effort in purification. A solution of copper (II) sulfate can be electrolyzed with a copper cathode and platinum/graphite anode to give spongy copper at cathode and evolution of oxygen gas at anode, the solution diluted sulfuric acid that indicates completed reaction when it turns from blue to clear (production of hydrogen at cathode is another sign):

More costly, dangerous, and troublesome yet novel is the electrobromine method, which employs a mixture of sulfur, water, and hydrobromic acid as the electrolytic solution. The sulfur pushed to bottom of container under the acid solution, then copper cathode and platinum/graphite anode are used with cathode near surface and anode at bottom of electrolyte to apply the current. This may take longer and emits toxic bromine/sulfur bromide vapors, but reactant acid is recyclable, overall only sulfur and water converted to sulfuric acid (omitting losses of acid as vapors):

Prior to 1900, most sulfuric acid was manufactured by the lead chamber process. As late as 1940, up to 50% of sulfuric acid manufactured in the United States was produced by chamber process plants.

In early to mid nineteenth century "vitriol" plants existed, among other places, in Prestonpans in Scotland, Shropshire and the Lagan Valley in County Antrim Ireland where it was used as a bleach for linen. Early bleaching of linen was done using lactic acid from sour milk but this was a slow process and the use of vitriol sped up the bleaching process.

Sulfuric acid is a very important commodity chemical, and indeed, a nation's sulfuric acid production is a good indicator of its industrial strength. World production in the year 2004 was about 180 million tonnes, with the following geographic distribution: Asia 35%, North America (including Mexico) 24%, Africa 11%, Western Europe 10%, Eastern Europe and Russia 10%, Australia and Oceania 7%, South America 7%. Most of this amount (≈60%) is consumed for fertilizers, particularly superphosphates, ammonium phosphate and ammonium sulfates. About 20% is used in chemical industry for production of detergents, synthetic resins, dyestuffs, pharmaceuticals, petroleum catalysts, insecticides and antifreeze, as well as in various processes such as oil well acidicizing, aluminium reduction, paper sizing, water treatment. About 6% of uses are related to pigments and include paints, enamels, printing inks, coated fabrics and paper, and the rest is dispersed into a multitude of applications such as production of explosives, cellophane, acetate and viscose textiles, lubricants, non-ferrous metals, and batteries.

The major use for sulfuric acid is in the "wet method" for the production of phosphoric acid, used for manufacture of phosphate fertilizers. In this method, phosphate rock is used, and more than 100 million tonnes are processed annually. This raw material is shown below as fluorapatite, though the exact composition may vary. This is treated with 93% sulfuric acid to produce calcium sulfate, hydrogen fluoride (HF) and phosphoric acid. The HF is removed as hydrofluoric acid. The overall process can be represented as:

Ammonium sulfate, an important nitrogen fertilizer, is most commonly produced as a byproduct from coking plants supplying the iron and steel making plants. Reacting the ammonia produced in the thermal decomposition of coal with waste sulfuric acid allows the ammonia to be crystallized out as a salt (often brown because of iron contamination) and sold into the agro-chemicals industry.

Another important use for sulfuric acid is for the manufacture of aluminium sulfate, also known as paper maker's alum. This can react with small amounts of soap on paper pulp fibers to give gelatinous aluminium carboxylates, which help to coagulate the pulp fibers into a hard paper surface. It is also used for making aluminium hydroxide, which is used at water treatment plants to filter out impurities, as well as to improve the taste of the water. Aluminium sulfate is made by reacting bauxite with sulfuric acid:

Sulfuric acid is also important in the manufacture of dyestuffs solutions.

The sulfur–iodine cycle is a series of thermo-chemical processes possibly usable to produce hydrogen from water. It consists of three chemical reactions whose net reactant is water and whose net products are hydrogen and oxygen.

The compounds of sulfur and iodine are recovered and reused, hence the consideration of the process as a cycle. This process is endothermic and must occur at high temperatures, so energy in the form of heat has to be supplied.

The sulfur–iodine cycle has been proposed as a way to supply hydrogen for a hydrogen-based economy. It is an alternative to electrolysis, and does not require hydrocarbons like current methods of steam reforming. But note that all of the available energy in the hydrogen so produced is supplied by the heat used to make it.

The sulfur–iodine cycle is currently being researched as a feasible method of obtaining hydrogen, but the concentrated, corrosive acid at high temperatures poses currently insurmountable safety hazards if the process were built on a large scale.

Sulfuric acid is used in large quantities by the iron and steelmaking industry to remove oxidation, rust, and scaling from rolled sheet and billets prior to sale to the automobile and major appliances industry. Used acid is often recycled using a spent acid regeneration (SAR) plant. These plants combust spent acid with natural gas, refinery gas, fuel oil or other fuel sources. This combustion process produces gaseous sulfur dioxide () and sulfur trioxide () which are then used to manufacture "new" sulfuric acid. SAR plants are common additions to metal smelting plants, oil refineries, and other industries where sulfuric acid is consumed in bulk, as operating a SAR plant is much cheaper than the recurring costs of spent acid disposal and new acid purchases.

Hydrogen peroxide () can be added to sulfuric acid to produce piranha solution, a powerful but very toxic cleaning solution with which substrate surfaces can be cleaned. Piranha solution is typically used in the microelectronics industry, and also in laboratory settings to clean glassware.

Sulfuric acid is used for a variety of other purposes in the chemical industry. For example, it is the usual acid catalyst for the conversion of cyclohexanone oxime to caprolactam, used for making nylon. It is used for making hydrochloric acid from salt via the Mannheim process. Much is used in petroleum refining, for example as a catalyst for the reaction of isobutane with isobutylene to give isooctane, a compound that raises the octane rating of gasoline (petrol). Sulfuric acid is also often used as a dehydrating or oxidising agent in industrial reactions, such as the dehydration of various sugars to form solid carbon.

Sulfuric acid acts as the electrolyte in lead–acid batteries (lead-acid accumulator):

At anode:

At cathode:

Overall:

Sulfuric acid at high concentrations is frequently the major ingredient in acidic drain cleaners which are used to remove grease, hair, tissue paper, etc. Similar to their alkaline versions, such drain openers can dissolve fats and proteins via hydrolysis. Moreover, as concentrated sulfuric acid has a strong dehydrating property, it can remove tissue paper via dehydrating process as well. Since the acid may react with water vigorously, such acidic drain openers should be added slowly into the pipe to be cleaned.

The study of vitriol, a category of glassy minerals from which the acid can be derived, began in ancient times. Sumerians had a list of types of vitriol that they classified according to the substances' color. Some of the earliest discussions on the origin and properties of vitriol is in the works of the Greek physician Dioscorides (first century AD) and the Roman naturalist Pliny the Elder (23–79 AD). Galen also discussed its medical use. Metallurgical uses for vitriolic substances were recorded in the Hellenistic alchemical works of Zosimos of Panopolis, in the treatise "Phisica et Mystica", and the Leyden papyrus X.

Medieval Islamic era alchemists, Jābir ibn Hayyān (c. 721 – c. 815 AD, also known as Geber), Muhammad ibn Zakariya al-Razi (865 – 925 AD), and Jamal Din al-Watwat (d. 1318, wrote the book "Mabāhij al-fikar wa-manāhij al-'ibar"), included vitriol in their mineral classification lists. Ibn Sina focused on its medical uses and different varieties of vitriol. Razi is credited with being the first to produce sulfuric acid.

Sulfuric acid was called "oil of vitriol" by medieval European alchemists because it was prepared by roasting "green vitriol" (iron(II) sulfate) in an iron retort. There are references to it in the works of Vincent of Beauvais and in the "Compositum de Compositis" ascribed to Saint Albertus Magnus. A passage from Pseudo-Geber's "Summa Perfectionis" was long considered to be a recipe for sulfuric acid, but this was a misinterpretation.

In the seventeenth century, the German-Dutch chemist Johann Glauber prepared sulfuric acid by burning sulfur together with saltpeter (potassium nitrate, ), in the presence of steam. As saltpeter decomposes, it oxidizes the sulfur to , which combines with water to produce sulfuric acid. In 1736, Joshua Ward, a London pharmacist, used this method to begin the first large-scale production of sulfuric acid.

In 1746 in Birmingham, John Roebuck adapted this method to produce sulfuric acid in lead-lined chambers, which were stronger, less expensive, and could be made larger than the previously used glass containers. This process allowed the effective industrialization of sulfuric acid production. After several refinements, this method, called the lead chamber process or "chamber process", remained the standard for sulfuric acid production for almost two centuries.

Sulfuric acid created by John Roebuck's process approached a 65% concentration. Later refinements to the lead chamber process by French chemist Joseph Louis Gay-Lussac and British chemist John Glover improved concentration to 78%. However, the manufacture of some dyes and other chemical processes require a more concentrated product. Throughout the 18th century, this could only be made by dry distilling minerals in a technique similar to the original alchemical processes. Pyrite (iron disulfide, ) was heated in air to yield iron(II) sulfate, , which was oxidized by further heating in air to form iron(III) sulfate, Fe(SO), which, when heated to 480 °C, decomposed to iron(III) oxide and sulfur trioxide, which could be passed through water to yield sulfuric acid in any concentration. However, the expense of this process prevented the large-scale use of concentrated sulfuric acid.

In 1831, British vinegar merchant Peregrine Phillips patented the contact process, which was a far more economical process for producing sulfur trioxide and concentrated sulfuric acid. Today, nearly all of the world's sulfuric acid is produced using this method.

Sulfuric acid is capable of causing very severe burns, especially when it is at high concentrations. In common with other corrosive acids and alkali, it readily decomposes proteins and lipids through amide and ester hydrolysis upon contact with living tissues, such as skin and flesh. In addition, it exhibits a strong dehydrating property on carbohydrates, liberating extra heat and causing secondary thermal burns. Accordingly, it rapidly attacks the cornea and can induce permanent blindness if splashed onto eyes. If ingested, it damages internal organs irreversibly and may even be fatal. Protective equipment should hence always be used when handling it. Moreover, its strong oxidizing property makes it highly corrosive to many metals and may extend its destruction on other materials. Because of such reasons, damage posed by sulfuric acid is potentially more severe than that by other comparable strong acids, such as hydrochloric acid and nitric acid.
Sulfuric acid must be stored carefully in containers made of nonreactive material (such as glass). Solutions equal to or stronger than 1.5 M are labeled "CORROSIVE", while solutions greater than 0.5 M but less than 1.5 M are labeled "IRRITANT". However, even the normal laboratory "dilute" grade (approximately 1 M, 10%) will char paper if left in contact for a sufficient time.

The standard first aid treatment for acid spills on the skin is, as for other corrosive agents, irrigation with large quantities of water. Washing is continued for at least ten to fifteen minutes to cool the tissue surrounding the acid burn and to prevent secondary damage. Contaminated clothing is removed immediately and the underlying skin washed thoroughly.

Preparation of the diluted acid can be dangerous due to the heat released in the dilution process. To avoid splattering, the concentrated acid is usually added to water and not the other way around. Water has a higher heat capacity than the acid, and so a vessel of cold water will absorb heat as acid is added.

Also, because the acid is denser than water, it sinks to the bottom. Heat is generated at the interface between acid and water, which is at the bottom of the vessel. Acid will not boil, because of its higher boiling point. Warm water near the interface rises due to convection, which cools the interface, and prevents boiling of either acid or water.

In contrast, addition of water to concentrated sulfuric acid results in a thin layer of water on top of the acid. Heat generated in this thin layer of water can boil, leading to the dispersal of a sulfuric acid aerosol or worse, an explosion.

Preparation of solutions greater than 6 M (35%) in concentration is most dangerous, because the heat produced may be sufficient to boil the diluted acid: efficient mechanical stirring and external cooling (such as an ice bath) are essential.

Reaction rates double for about every 10-degree Celsius increase in temperature. Therefore, the reaction will become more violent as dilution proceeds, unless the mixture is given time to cool. Adding acid to warm water will cause a violent reaction.

On a laboratory scale, sulfuric acid can be diluted by pouring concentrated acid onto crushed ice made from de-ionized water. The ice melts in an endothermic process while dissolving the acid. The amount of heat needed to melt the ice in this process is greater than the amount of heat evolved by dissolving the acid so the solution remains cold. After all the ice has melted, further dilution can take place using water.

Sulfuric acid is non-flammable.

The main occupational risks posed by this acid are skin contact leading to burns (see above) and the inhalation of aerosols. Exposure to aerosols at high concentrations leads to immediate and severe irritation of the eyes, respiratory tract and mucous membranes: this ceases rapidly after exposure, although there is a risk of subsequent pulmonary edema if tissue damage has been more severe. At lower concentrations, the most commonly reported symptom of chronic exposure to sulfuric acid aerosols is erosion of the teeth, found in virtually all studies: indications of possible chronic damage to the respiratory tract are inconclusive as of 1997. Repeated occupational exposure to sulfuric acid mists may increase the chance of lung cancer by up to 64 percent. In the United States, the permissible exposure limit (PEL) for sulfuric acid is fixed at 1 mg/m: limits in other countries are similar. There have been reports of sulfuric acid ingestion leading to vitamin B12 deficiency with subacute combined degeneration. The spinal cord is most often affected in such cases, but the optic nerves may show demyelination, loss of axons and gliosis.

International commerce of sulfuric acid is controlled under the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances, 1988, which lists sulfuric acid under Table II of the convention as a chemical frequently used in the illicit manufacture of narcotic drugs or psychotropic substances.




</doc>
<doc id="29248" url="https://en.wikipedia.org/wiki?curid=29248" title="Space colonization">
Space colonization

Space colonization (also called space settlement, or extraterrestrial colonization) is permanent human habitation and exploitation of natural resources off the planet Earth.

Many arguments have been made for and against space colonization. The two most common in favor of colonization are survival of human civilization and the biosphere in the event of a planetary-scale disaster (natural or man-made), and the availability of additional resources in space that could enable expansion of human society. The most common objections to colonization include concerns that the commodification of the cosmos may be likely to enhance the interests of the already powerful, including major economic and military institutions, and to exacerbate pre-existing detrimental processes such as wars, economic inequality, and environmental degradation.

No space colonies have been built so far. Currently, the building of a space colony would present a set of huge technological and economic challenges. Space settlements would have to provide for nearly all (or all) the material needs of hundreds or thousands of humans, in an environment out in space that is very hostile to human life. They would involve technologies, such as controlled ecological life-support systems, that have yet to be developed in any meaningful way. They would also have to deal with the as-yet unknown issue of how humans would behave and thrive in such places long-term. Because of the present cost of sending anything from the surface of the Earth into orbit (around $1400 per kg, or $640 per-pound, to low Earth orbit by Falcon Heavy), a space colony would currently be a massively expensive project.

There are yet no plans for building space colonies by any large-scale organization, either government or private. However, many proposals, speculations, and designs for space settlements have been made through the years, and a considerable number of space colonization advocates and groups are active. Several famous scientists, such as Freeman Dyson, have come out in favor of space settlement.

On the technological front, there is ongoing progress in making access to space cheaper (reusable launch systems could reach $20 per kg to orbit), and in creating automated manufacturing and construction techniques.

The primary argument calling for space colonization is the long-term survival of human civilization. By developing alternative locations off Earth, the planet's species, including humans, could live on in the event of natural or man-made disasters on our own planet.

On two occasions, theoretical physicist and cosmologist Stephen Hawking argued for space colonization as a means of saving humanity. In 2001, Hawking predicted that the human race would become extinct within the next thousand years, unless colonies could be established in space. In 2010, he stated that humanity faces two options: either we colonize space within the next two hundred years, or we will face the prospect of long-term extinction.

In 2005, then NASA Administrator Michael Griffin identified space colonization as the ultimate goal of current spaceflight programs, saying:
Louis J. Halle, formerly of the United States Department of State, wrote in "Foreign Affairs" (Summer 1980) that the colonization of space will protect humanity in the event of global nuclear warfare. The physicist Paul Davies also supports the view that if a planetary catastrophe threatens the survival of the human species on Earth, a self-sufficient colony could "reverse-colonize" Earth and restore human civilization. The author and journalist William E. Burrows and the biochemist Robert Shapiro proposed a private project, the Alliance to Rescue Civilization, with the goal of establishing an off-Earth "backup" of human civilization.

Based on his Copernican principle, J. Richard Gott has estimated that the human race could survive for another 7.8 million years, but it is not likely to ever colonize other planets. However, he expressed a hope to be proven wrong, because "colonizing other worlds is our best chance to hedge our bets and improve the survival prospects of our species".

In a theoretical study from 2019, a group of researchers have pondered the long-term trajectory of human civilization. It is argued that due to Earth's finitude as well as the limited duration of our solar system, mankind's survival into the far future will very likely require extensive space colonization. This 'astronomical trajectory' of mankind, as it is termed, could come about in four steps: First step, plenty of space colonies could be established at various habitable locations — be it in outer space or on celestial bodies away from planet earth — and allowed to remain dependent on support from earth for a start. Second step, these colonies could gradually become self-sufficient, enabling them to survive if or when the mother civilization on earth fails or dies. Third step, the colonies could develop and expand their habitation by themselves on their space stations or celestial bodies, for example via terraforming. Fourth step, the colonies could self-replicate and establish new colonies further into space, a process that could then repeat itself and continue at an exponential rate throughout cosmos. However, this astronomical trajectory may not be a lasting one, as it will most likely be interrupted and eventually decline due to resource depletion or straining competition between various human factions, bringing about some 'star wars' scenario. In the very far future, mankind is expected to become extinct in any case, as no civilization — whether human or alien — will ever outlive the limited duration of cosmos itself.

Resources in space, both in materials and energy, are enormous. The Solar System alone has, according to different estimates, enough material and energy to support anywhere from several thousand to over a billion times that of the current Earth-based human population. Outside the Solar System, several hundred billion other planets in the Milky Way alone provide opportunities for both colonization and resource collection, though travel to any of them is impossible on any practical time-scale without interstellar travel by use of generation ships or revolutionary new methods of travel, such as faster-than-light (FTL).

Asteroid mining will also be a key player in space colonization. Water and materials to make structures and shielding can be easily found in asteroids. Instead of resupplying on Earth, mining and fuel stations need to be established on asteroids to facilitate better space travel. Optical mining is the term NASA uses to describe extracting materials from asteroids. NASA believes by using propellant derived from asteroids for exploration to the moon, Mars, and beyond will save $100 billion. If funding and technology come sooner than estimated, asteroid mining might be possible within a decade.

All these planets and other bodies offer a virtually endless supply of resources providing limitless growth potential. Harnessing these resources can lead to much economic development.

Expansion of humans and technological progress has usually resulted in some form of environmental devastation, and destruction of ecosystems and their accompanying wildlife. In the past, expansion has often come at the expense of displacing many indigenous peoples, the resulting treatment of these peoples ranging anywhere from encroachment to genocide. Because space has no known life, this need not be a consequence, as some space settlement advocates have pointed out.

Another argument for space colonization is to mitigate the negative effects of overpopulation.
If the resources of space were opened to use and viable life-supporting habitats were built, Earth would no longer define the limitations of growth. Although many of Earth's resources are non-renewable, off-planet colonies could satisfy the majority of the planet's resource requirements. With the availability of extraterrestrial resources, demand on terrestrial ones would decline.

Additional goals cite the innate human drive to explore and discover, a quality recognized at the core of progress and thriving civilizations.

Nick Bostrom has argued that from a utilitarian perspective, space colonization should be a chief goal as it would enable a very large population to live for a very long period of time (possibly billions of years), which would produce an enormous amount of utility (or happiness). He claims that it is more important to reduce existential risks to increase the probability of eventual colonization than to accelerate technological development so that space colonization could happen sooner. In his paper, he assumes that the created lives will have positive ethical value despite the problem of suffering.

In a 2001 interview with Freeman Dyson, J. Richard Gott and Sid Goldstein, they were asked for reasons why some humans should live in space. Their answers were:

Although some items of the infrastructure requirements above can already be easily produced on Earth and would therefore not be very valuable as trade items (oxygen, water, base metal ores, silicates, etc.), other high value items are more abundant, more easily produced, of higher quality, or can only be produced in space. These would provide (over the long-term) a very high return on the initial investment in space infrastructure.

Some of these high-value trade goods include precious metals, gemstones, power, solar cells, ball bearings, semi-conductors, and pharmaceuticals.

The mining and extraction of metals from a small asteroid the size of 3554 Amun or (6178) 1986 DA, both small near-Earth asteroids, would be 30 times as much metal as humans have mined throughout history. A metal asteroid this size would be worth approximately US$20 trillion at 2001 market prices

Space colonization is seen as a long-term goal of some national space programs. Since the advent of the 21st-century commercialization of space, which saw greater cooperation between NASA and the private sector, several private companies have announced plans toward the colonization of Mars. Among entrepreneurs leading the call for space colonization are Elon Musk, Dennis Tito and Bas Lansdorp.

The main impediments to commercial exploitation of these resources are the very high cost of initial investment, the very long period required for the expected return on those investments ("The Eros Project" plans a 50-year development), and the fact that the venture has never been carried out before—the high-risk nature of the investment.

Major governments and well-funded corporations have announced plans for new categories of activities: space tourism and hotels, prototype space-based solar-power satellites, heavy-lift boosters and asteroid mining—that create needs and capabilities for humans to be present in space.

Building colonies in space would require access to water, food, space, people, construction materials, energy, transportation, communications, life support, simulated gravity, radiation protection and capital investment. It is likely the colonies would be located near the necessary physical resources. The practice of space architecture seeks to transform spaceflight from a heroic test of human endurance to a normality within the bounds of comfortable experience. As is true of other frontier-opening endeavors, the capital investment necessary for space colonization would probably come from governments, an argument made by John Hickman and Neil deGrasse Tyson.

Colonies on the Moon, Mars, or asteroids could extract local materials. The Moon is deficient in volatiles such as argon, helium and compounds of carbon, hydrogen and nitrogen. The LCROSS impacter was targeted at the Cabeus crater which was chosen as having a high concentration of water for the Moon. A plume of material erupted in which some water was detected. Mission chief scientist Anthony Colaprete estimated that the Cabeus crater contains material with 1% water or possibly more. Water ice should also be in other permanently shadowed craters near the lunar poles. Although helium is present only in low concentrations on the Moon, where it is deposited into regolith by the solar wind, an estimated million tons of He-3 exists over all. It also has industrially significant oxygen, silicon, and metals such as iron, aluminum, and titanium.

Launching materials from Earth is expensive, so bulk materials for colonies could come from the Moon, a near-Earth object (NEO), Phobos, or Deimos. The benefits of using such sources include: a lower gravitational force, no atmospheric drag on cargo vessels, and no biosphere to damage. Many NEOs contain substantial amounts of metals. Underneath a drier outer crust (much like oil shale), some other NEOs are inactive comets which include billions of tons of water ice and kerogen hydrocarbons, as well as some nitrogen compounds.

Farther out, Jupiter's Trojan asteroids are thought to be rich in water ice and other volatiles.

Recycling of some raw materials would almost certainly be necessary.

Solar energy in orbit is abundant, reliable, and is commonly used to power satellites today. There is no night in free space, and no clouds or atmosphere to block sunlight. Light intensity obeys an inverse-square law. So the solar energy available at distance "d" from the Sun is "E" = 1367/"d" W/m, where "d" is measured in astronomical units (AU) and 1367 watts/m is the energy available at the distance of Earth's orbit from the Sun, 1 AU.

In the weightlessness and vacuum of space, high temperatures for industrial processes can easily be achieved in solar ovens with huge parabolic reflectors made of metallic foil with very lightweight support structures. Flat mirrors to reflect sunlight around radiation shields into living areas (to avoid line-of-sight access for cosmic rays, or to make the Sun's image appear to move across their "sky") or onto crops are even lighter and easier to build.

Large solar power photovoltaic cell arrays or thermal power plants would be needed to meet the electrical power needs of the settlers' use. In developed parts of Earth, electrical consumption can average 1 kilowatt/person (or roughly 10 megawatt-hours per person per year.) These power plants could be at a short distance from the main structures if wires are used to transmit the power, or much farther away with wireless power transmission.

A major export of the initial space settlement designs was anticipated to be large solar power satellites (SPS) that would use wireless power transmission (phase-locked microwave beams or lasers emitting wavelengths that special solar cells convert with high efficiency) to send power to locations on Earth, or to colonies on the Moon or other locations in space. For locations on Earth, this method of getting power is extremely benign, with zero emissions and far less ground area required per watt than for conventional solar panels. Once these satellites are primarily built from lunar or asteroid-derived materials, the price of SPS electricity could be lower than energy from fossil fuel or nuclear energy; replacing these would have significant benefits such as the elimination of greenhouse gases and nuclear waste from electricity generation.

Transmitting solar energy wirelessly from the Earth to the Moon and back is also an idea proposed for the benefit of space colonization and energy resources. Physicist Dr. David Criswell, who worked for NASA during the Apollo missions, came up with the idea of using power beams to transfer energy from space. These beams, microwaves with a wavelength of about 12 cm, will be almost untouched as they travel through the atmosphere. They can also be aimed at more industrial areas to keep away from humans or animal activities. This will allow for safer and more reliable methods of transferring solar energy.

In 2008, scientists were able to send a 20 watt microwave signal from a mountain in Maui to the island of Hawaii. Since then JAXA and Mitsubishi has teamed up on a $21 billion project in order to place satellites in orbit which could generate up to 1 gigawatt of energy. These are the next advancements being done today in order to make energy be transmitted wirelessly for space-based solar energy.

However, the value of SPS power delivered wirelessly to other locations in space will typically be far higher than to Earth. Otherwise, the means of generating the power would need to be included with these projects and pay the heavy penalty of Earth launch costs. Therefore, other than proposed demonstration projects for power delivered to Earth, the first priority for SPS electricity is likely to be locations in space, such as communications satellites, fuel depots or "orbital tugboat" boosters transferring cargo and passengers between low Earth orbit (LEO) and other orbits such as geosynchronous orbit (GEO), lunar orbit or highly-eccentric Earth orbit (HEEO). The system will also rely on satellites and receiving stations on Earth to convert the energy into electricity. Because of this energy can be transmitted easily from dayside to nightside meaning power is reliable 24/7.

Nuclear power is sometimes proposed for colonies located on the Moon or on Mars, as the supply of solar energy is too discontinuous in these locations; the Moon has nights of two Earth weeks in duration. Mars has nights, relatively high gravity, and an atmosphere featuring large dust storms to cover and degrade solar panels. Also, Mars' greater distance from the Sun (1.5 astronomical units, AU) translates into "E/(1.5 = 2.25)" only ½–⅔ the solar energy of Earth orbit. Another method would be transmitting energy wirelessly to the lunar or Martian colonies from solar power satellites (SPSs) as described above; the difficulties of generating power in these locations make the relative advantages of SPSs much greater there than for power beamed to locations on Earth. In order to also be able to fulfill the requirements of a Moon base and energy to supply life support, maintenance, communications, and research, a combination of both nuclear and solar energy will be used in the first colonies.

For both solar thermal and nuclear power generation in airless environments, such as the Moon and space, and to a lesser extent the very thin Martian atmosphere, one of the main difficulties is dispersing the inevitable heat generated. This requires fairly large radiator areas.

In space settlements, a life support system must recycle or import all the nutrients without "crashing." The closest terrestrial analogue to space life support is possibly that of a nuclear submarine. Nuclear submarines use mechanical life support systems to support humans for months without surfacing, and this same basic technology could presumably be employed for space use. However, nuclear submarines run "open loop"—extracting oxygen from seawater, and typically dumping carbon dioxide overboard, although they recycle existing oxygen. Recycling of the carbon dioxide has been approached in the literature using the Sabatier process or the Bosch reaction.

Although a fully mechanistic life support system is conceivable, a closed ecological system is generally proposed for life support. The Biosphere 2 project in Arizona has shown that a complex, small, enclosed, man-made biosphere can support eight people for at least a year, although there were many problems. A year or so into the two-year mission oxygen had to be replenished, which strongly suggests that the mission failed.

The relationship between organisms, their habitat and the non-Earth environment can be:

A combination of the above technologies is also possible.

Cosmic rays and solar flares create a lethal radiation environment in space. In Earth orbit, the Van Allen belts make living above the Earth's atmosphere difficult. To protect life, settlements must be surrounded by sufficient mass to absorb most incoming radiation, unless magnetic or plasma radiation shields were developed.

Passive mass shielding of four metric tons per square meter of surface area will reduce radiation dosage to several mSv or less annually, well below the rate of some populated high natural background areas on Earth. This can be leftover material (slag) from processing lunar soil and asteroids into oxygen, metals, and other useful materials. However, it represents a significant obstacle to maneuvering vessels with such massive bulk (mobile spacecraft being particularly likely to use less massive active shielding). Inertia would necessitate powerful thrusters to start or stop rotation, or electric motors to spin two massive portions of a vessel in opposite senses. Shielding material can be stationary around a rotating interior. To protect from radiation they say to bundle up in the thickest clothes possible so that the cloth can absorb the radiation and prevent it from getting to your body.

Space manufacturing could enable self-replication. Some think it's the ultimate goal because it allows an exponential increase in colonies, while eliminating costs to and dependence on Earth. It could be argued that the establishment of such a colony would be Earth's first act of self-replication. Intermediate goals include colonies that expect only information from Earth (science, engineering, entertainment) and colonies that just require periodic supply of light weight objects, such as integrated circuits, medicines, genetic material and tools.

The monotony and loneliness that comes from a prolonged space mission can leave astronauts susceptible to cabin fever or having a psychotic break. Moreover, lack of sleep, fatigue, and work overload can affect an astronaut's ability to perform well in an environment such as space where every action is critical.

In 2002, the anthropologist John H. Moore estimated that a population of 150–180 would permit a stable society to exist for 60 to 80 generations—equivalent to 2000 years.

A much smaller initial population of as little as two women should be viable as long as human embryos are available from Earth. Use of a sperm bank from Earth also allows a smaller starting base with negligible inbreeding.

Researchers in conservation biology have tended to adopt the "50/500" rule of thumb initially advanced by Franklin and Soule. This rule says a short-term effective population size ("N") of 50 is needed to prevent an unacceptable rate of inbreeding, whereas a long‐term "N" of 500 is required to maintain overall genetic variability. The "N" = 50 prescription corresponds to an inbreeding rate of 1% per generation, approximately half the maximum rate tolerated by domestic animal breeders. The "N" = 500 value attempts to balance the rate of gain in genetic variation due to mutation with the rate of loss due to genetic drift.

Assuming a journey of 6,300 years, the astrophysicist Frédéric Marin and the particle physicist Camille Beluffi calculated that the minimum viable population for a generation ship to reach Proxima Centauri would be 98 settlers at the beginning of the mission (then the crew will breed until reaching a stable population of several hundred settlers within the ship) .

In 2020, Jean-Marc Salotti proposed a method to determine the minimum number of settlers to survive on an extraterrestrial world. It is based on the comparison between the required time to perform all activities and the working time of all human resources. For Mars, 110 individuals would be required.

Experts have debated on the possible usage of money and currencies in societies that will be established in space. The Quasi Universal Intergalactic Denomination, or QUID, is a physical currency made from a space-qualified polymer PTFE for inter-planetary travelers. QUID was designed for the foreign exchange company Travelex by scientists from Britain's National Space Centre and the University of Leicester.

Location is a frequent point of contention between space colonization advocates. The location of colonization can be on a physical body planet, dwarf planet, natural satellite, or asteroid or orbiting one. For colonies not on a body see also space habitat.

Due to its proximity and familiarity, Earth's Moon is discussed as a target for colonization. It has the benefits of proximity to Earth and lower escape velocity, allowing for easier exchange of goods and services. A drawback of the Moon is its low abundance of volatiles necessary for life such as hydrogen, nitrogen, and carbon. Water-ice deposits that exist in some polar craters could serve as a source for these elements. An alternative solution is to bring hydrogen from near-Earth asteroids and combine it with oxygen extracted from lunar rock.

The Moon's low surface gravity is also a concern, as it is unknown whether 1/6g is enough to maintain human health for long periods.

The Moon's lack of atmosphere provides no protection from space radiation or meteoroids. The early Moon colonies may shelter in ancient Lunar lava tubes to gain protection. The two-week day/night cycle makes use of solar power more difficult.

Another near-Earth possibility are the five Earth–Moon Lagrange points. Although they would generally also take a few days to reach with current technology, many of these points would have near-continuous solar power because their distance from Earth would result in only brief and infrequent eclipses of light from the Sun. However, the fact that the Earth–Moon Lagrange points and tend to collect dust and debris, whereas - require active station-keeping measures to maintain a stable position, make them somewhat less suitable places for habitation than was originally believed. Additionally, the orbit of – takes them out of the protection of the Earth's magnetosphere for approximately two-thirds of the time, exposing them to the health threat from cosmic rays.

The five Earth–Sun Lagrange points would totally eliminate eclipses, but only and would be reachable in a few days' time. The other three Earth–Sun points would require months to reach.

See space habitat

Colonizing Mercury would involve similar challenges as the Moon as there are few volatile elements, no atmosphere and the surface gravity is lower than Earth's. However, the planet also receives almost seven times the solar flux as the Earth/Moon system.

Geologist Stephen Gillett suggested in 1996 that this could make Mercury an ideal place to build and launch solar sail spacecraft, which could launch as folded up "chunks" by mass driver from Mercury's surface. Once in space the solar sails would deploy. Since Mercury's solar constant is 6.5 times higher than Earth's, energy for the mass driver should be easy to come by, and solar sails near Mercury would have 6.5 times the thrust they do near Earth. This could make Mercury an ideal place to acquire materials useful in building hardware to send to (and terraform) Venus. Vast solar collectors could also be built on or near Mercury to produce power for large scale engineering activities such as laser-pushed lightsails to nearby star systems.

Colonization of asteroids would require space habitats. The asteroid belt has significant overall material available, the largest object being Ceres, although it is thinly distributed as it covers a vast region of space. Uncrewed supply craft should be practical with little technological advance, even crossing 500 million kilometers of space. The colonists would have a strong interest in assuring their asteroid did not hit Earth or any other body of significant mass, but would have extreme difficulty in moving an asteroid of any size. The orbits of the Earth and most asteroids are very distant from each other in terms of delta-v and the asteroidal bodies have enormous momentum. Rockets or mass drivers can perhaps be installed on asteroids to direct their path into a safe course.

The Artemis Project designed a plan to colonize Europa, one of Jupiter's moons. Scientists were to inhabit igloos and drill down into the Europan ice crust, exploring any sub-surface ocean. This plan discusses possible use of "air pockets" for human habitation. Europa is considered one of the more habitable bodies in the Solar System and so merits investigation as a possible abode for life.

NASA performed a study called "HOPE" (Revolutionary Concepts for Human Outer Planet Exploration) regarding the future exploration of the Solar System. The target chosen was Callisto due to its distance from Jupiter, and thus the planet's harmful radiation. It could be possible to build a surface base that would produce fuel for further exploration of the Solar System.

Three of the Galilean moons (Europa, Ganymede, Callisto) have an abundance of volatiles that may support colonization efforts.

Titan is suggested as a target for colonization, because it is the only moon in the Solar System to have a dense atmosphere and is rich in carbon-bearing compounds. Titan has water ice and large methane oceans. Robert Zubrin identified Titan as possessing an abundance of all the elements necessary to support life, making Titan perhaps the most advantageous locale in the outer Solar System for colonization, and saying "In certain ways, Titan is the most hospitable extraterrestrial world within our solar system for human colonization".

Enceladus is a small, icy moon orbiting close to Saturn, notable for its extremely bright surface and the geyser-like plumes of ice and water vapor that erupt from its southern polar region. If Enceladus has liquid water, it joins Mars and Jupiter's moon Europa as one of the prime places in the Solar System to look for extraterrestrial life and possible future settlements.

Other large satellites: Rhea, Iapetus, Dione, Tethys, and Mimas, all have large quantities of volatiles, which can be used to support settlements.

The Kuiper belt is estimated to have 70,000 bodies of 100 km or larger.

Freeman Dyson has suggested that within a few centuries human civilization will have relocated to the Kuiper belt.

The Oort cloud is estimated to have up to a trillion comets.

Looking beyond the Solar System, there are up to several hundred billion potential stars with possible colonization targets. The main difficulty is the vast distances to other stars: roughly a hundred thousand times farther away than the planets in the Solar System. This means that some combination of very high speed (some more-than-fractional percentage of the speed of light), or travel times lasting centuries or millennia, would be required. These speeds are far beyond what current spacecraft propulsion systems can provide.

Space colonization technology could in principle allow human expansion at high, but sub-relativistic speeds, substantially less than the speed of light, "c".  An interstellar colony ship would be similar to a space habitat, with the addition of major propulsion capabilities and independent energy generation.

Hypothetical starship concepts proposed both by scientists and in hard science fiction include:

The above concepts which appear limited to high, but still sub-relativistic speeds, due to fundamental energy and reaction mass considerations, and all would entail trip times which might be enabled by space colonization technology, permitting self-contained habitats with lifetimes of decades to centuries. Yet human interstellar expansion at average speeds of even 0.1% of "c"  would permit settlement of the entire Galaxy in less than one half of the Sun's galactic orbital period of ~240,000,000 years, which is comparable to the timescale of other galactic processes. Thus, even if interstellar travel at near relativistic speeds is never feasible (which cannot be clearly determined at this time), the development of space colonization could allow human expansion beyond the Solar System without requiring technological advances that cannot yet be reasonably foreseen. This could greatly improve the chances for the survival of intelligent life over cosmic timescales, given the many natural and human-related hazards that have been widely noted.

If humanity does gain access to a large amount of energy, on the order of the mass-energy of entire planets, it may eventually become feasible to construct Alcubierre drives. These are one of the few methods of superluminal travel which may be possible under current physics. However it is probable that such a device could never exist, due to the fundamental challenges posed. For more on this see Difficulties of making and using an Alcubierre Drive.

Looking beyond the Milky Way, there are at least 2 trillion other galaxies in the observable universe. The distances between galaxies are on the order of a million times farther than those between the stars. Because of the speed of light limit on how fast any material objects can travel in space, intergalactic travel would either have to involve voyages lasting millions of years, or a possible faster than light propulsion method based on speculative physics, such as the Alcubierre drive. There are, however, no scientific reasons for stating that intergalactic travel is impossible in principle.

Uploaded human minds or AI may be transmitted to other galaxies in the hope some intelligence there would receive and activate them.

Space colonization can roughly be said to be possible when the necessary methods of space colonization become cheap enough (such as space access by cheaper launch systems) to meet the cumulative funds that have been gathered for the purpose, in addition to estimated profits from commercial use of space.

Although there are no immediate prospects for the large amounts of money required for space colonization to be available given traditional launch costs, there is some prospect of a radical reduction to launch costs in the 2010s, which would consequently lessen the cost of any efforts in that direction. With a published price of per launch of up to payload to low Earth orbit, SpaceX Falcon 9 rockets are already the "cheapest in the industry". Advancements currently being developed as part of the SpaceX reusable launch system development program to enable reusable Falcon 9s "could drop the price by an order of magnitude, sparking more space-based enterprise, which in turn would drop the cost of access to space still further through economies of scale." If SpaceX is successful in developing the reusable technology, it would be expected to "have a major impact on the cost of access to space", and change the increasingly competitive market in space launch services.

The President's Commission on Implementation of United States Space Exploration Policy suggested that an inducement prize should be established, perhaps by government, for the achievement of space colonization, for example by offering the prize to the first organization to place humans on the Moon and sustain them for a fixed period before they return to Earth.

The most famous attempt to build an analogue to a self-sufficient colony is Biosphere 2, which attempted to duplicate Earth's biosphere. BIOS-3 is another closed ecosystem, completed in 1972 in Krasnoyarsk, Siberia.

Many space agencies build testbeds for advanced life support systems, but these are designed for long duration human spaceflight, not permanent colonization.

Remote research stations in inhospitable climates, such as the Amundsen–Scott South Pole Station or Devon Island Mars Arctic Research Station, can also provide some practice for off-world outpost construction and operation. The Mars Desert Research Station has a habitat for similar reasons, but the surrounding climate is not strictly inhospitable.

The first known work on space colonization was "The Brick Moon", a work of fiction published in 1869 by Edward Everett Hale, about an inhabited artificial satellite.

The Russian schoolmaster and physicist Konstantin Tsiolkovsky foresaw elements of the space community in his book "Beyond Planet Earth" written about 1900. Tsiolkovsky had his space travelers building greenhouses and raising crops in space. Tsiolkovsky believed that going into space would help perfect human beings, leading to immortality and peace.

Others have also written about space colonies as Lasswitz in 1897 and Bernal, Oberth, Von Pirquet and Noordung in the 1920s. Wernher von Braun contributed his ideas in a 1952 "Colliers" article. In the 1950s and 1960s, Dandridge M. Cole published his ideas.

Another seminal book on the subject was the book "The High Frontier: Human Colonies in Space" by Gerard K. O'Neill in 1977 which was followed the same year by "Colonies in Space " by T. A. Heppenheimer.

M. Dyson wrote "Home on the Moon; Living on a Space Frontier" in 2003; Peter Eckart wrote "Lunar Base Handbook" in 2006 and then Harrison Schmitt's "Return to the Moon" written in 2007.

, Bigelow Aerospace was the only private commercial spaceflight company that had launched experimental space station modules, and they had launched two: Genesis I (2006) and Genesis II (2007), both into Earth-orbit. , they had indicated that their first production model of the space habitat, a much larger habitat () called the BA 330, could be launched as early as 2017. In the event, the larger habitat was never built, and Bigelow laid off all employees in March 2020.

Robotic spacecraft to Mars are required to be sterilized, to have at most 300,000 spores on the exterior of the craft—and more thoroughly sterilized if they contact "special regions" containing water, otherwise there is a risk of contaminating not only the life-detection experiments but possibly the planet itself.

It is impossible to sterilize human missions to this level, as humans are host to typically a hundred trillion microorganisms of thousands of species of the human microbiome, and these cannot be removed while preserving the life of the human. Containment seems the only option, but it is a major challenge in the event of a hard landing (i.e. crash). There have been several planetary workshops on this issue, but with no final guidelines for a way forward yet. Human explorers would also be vulnerable to back contamination to Earth if they become carriers of microorganisms.

A corollary to the Fermi paradox—"nobody else is doing it"—is the argument that, because no evidence of alien colonization technology exists, it is statistically unlikely to even be possible to use that same level of technology ourselves.

Colonizing space would require massive amounts of financial, physical, and human capital devoted to research, development, production, and deployment. Earth's natural resources do not increase to a noteworthy extent (which is in keeping with the "only one Earth" position of environmentalists). Thus, considerable efforts in colonizing places outside Earth would appear as a hazardous waste of the Earth's limited resources for an aim without a clear end.

The fundamental problem of public things, needed for survival, such as space programs, is the free-rider problem. Convincing the public to fund such programs would require additional self-interest arguments: If the objective of space colonization is to provide a "backup" in case everyone on Earth is killed, then why should someone on Earth pay for something that is only useful after they are dead? This assumes that space colonization is not widely acknowledged as a sufficiently valuable social goal.

Seen as a relief to the problem of overpopulation even as early as 1758, and listed as one of Stephen Hawking's reasons for pursuing space exploration, it has become apparent that space colonization in response to overpopulation is unwarranted. Indeed, the birth rates of many developed countries, specifically spacefaring ones, are at or below replacement rates, thus negating the need to use colonization as a means of population control.

Other objections include concerns that the forthcoming colonization and commodification of the cosmos may be likely to enhance the interests of the already powerful, including major economic and military institutions e.g. the large financial institutions, the major aerospace companies and the military–industrial complex, to lead to new wars, and to exacerbate pre-existing exploitation of workers and resources, economic inequality, poverty, social division and marginalization, environmental degradation, and other detrimental processes or institutions.

Additional concerns include creating a culture in which humans are no longer seen as human, but rather as material assets. The issues of human dignity, morality, philosophy, culture, bioethics, and the threat of megalomaniac leaders in these new "societies" would all have to be addressed in order for space colonization to meet the psychological and social needs of people living in isolated colonies.

As an alternative or addendum for the future of the human race, many science fiction writers have focused on the realm of the 'inner-space', that is the computer-aided exploration of the human mind and human consciousness—possibly en route developmentally to a Matrioshka Brain.

Robotic spacecraft are proposed as an alternative to gain many of the same scientific advantages without the limited mission duration and high cost of life support and return transportation involved in human missions. However, there are vast scientific domains that cannot be addressed with robots, especially biology in specific atmospheric and gravitational environments and human sciences in space.

Another concern is the potential to cause interplanetary contamination on planets that may harbor hypothetical extraterrestrial life.

Space colonization has been discussed as continuation of imperialism and colonialism. Questioning colonial decisionmaking and reasons for colonial labour and land exploitation with postcolonial critique. Seeing the need for inclusive and democratic participation and implementation of any space exploration, infrastructure or habitation.

The narrative of space exploration as a "New Frontier" has been criticized as unreflected continuation of settler colonialism and manifest destiny, continuing the narrative of colonial exploration as fundamental to the assumed human nature.
Also narratives of survival and arguments for space as a solution to global problems like pollution have been identified as imperialist.

The predominant perspective of territorial colonization in space has been called "surfacism", especially comparing advocacy for colonization of Mars opposed to Venus.

It has been argued that the present politico-legal regimes and their philosophic grounding advantage imperialist development of space.

The health of the humans who may participate in a colonization venture would be subject to increased physical, mental and emotional risks. NASA learned that – without gravity – bones lose minerals, causing osteoporosis. Bone density may decrease by 1% per month, which may lead to a greater risk of osteoporosis-related fractures later in life. Fluid shifts towards to the head may cause vision problems. NASA found that isolation in closed environments aboard the International Space Station led to depression, sleep disorders, and diminished personal interactions, likely due to confined spaces and the monotony and boredom of long space flight. Circadian rhythm may also be susceptible to the effects of space life due to the effects on sleep of disrupted timing of sunset and sunrise. This can lead to exhaustion, as well as other sleep problems such as insomnia, which can reduce their productivity and lead to mental health disorders. High-energy radiation is a health risk that colonizers would face, as radiation in deep space is deadlier than what astronauts face now in low Earth orbit. Metal shielding on space vehicles protects against only 25-30% of space radiation, possibly leaving colonizers exposed to the other 70% of radiation and its short and long-term health complications.

Although there are many physical, mental, and emotional health risks for future colonizers and pioneers, solutions have been proposed to correct these problems. Mars500, HI-SEAS, and SMART-OP represent efforts to help reduce the effects of loneliness and confinement for long periods of time. Keeping contact with family members, celebrating holidays, and maintaining cultural identities all had an impact on minimizing the deterioration of mental health. There are also health tools in development to help astronauts reduce anxiety, as well as helpful tips to reduce the spread of germs and bacteria in a closed environment. Radiation risk may be reduced for astronauts by frequent monitoring and focusing work away from the shielding on the shuttle. Future space agencies can also ensure that every colonizer would have a mandatory amount of daily exercise to prevent degradation of muscle.

Organizations that contribute to space colonization include:

Although established space colonies are a stock element in science fiction stories, fictional works that explore the themes, social or practical, of the settlement and occupation of a habitable world are much rarer.



</doc>
<doc id="29250" url="https://en.wikipedia.org/wiki?curid=29250" title="Second Council of Nicaea">
Second Council of Nicaea

The Second Council of Nicaea is recognized as the last of the first seven ecumenical councils by the Eastern Orthodox Church and the Catholic Church. In addition, it is also recognized as such by the Old Catholics and others. Protestant opinions on it are varied.

It met in AD 787 in Nicaea (site of the First Council of Nicaea; present-day İznik in Turkey) to restore the use and veneration of icons (or, holy images), which had been suppressed by imperial edict inside the Byzantine Empire during the reign of Leo III (717–741). His son, Constantine V (741–775), had held the Council of Hieria to make the suppression official.

The veneration of icons had been banned by Byzantine Emperor Constantine V and supported by his Council of Hieria (754 AD), which had described itself as the seventh ecumenical council. The Council of Hieria was overturned by the Second Council of Nicaea only 33 years later, and has also been rejected by Catholic and Orthodox churches, since none of the five major patriarchs were represented. The emperor's vigorous enforcement of the ban included persecution of those who venerated icons and of monks in general. There were also political overtones to the persecution—images of emperors were still allowed by Constantine, which some opponents saw as an attempt to give wider authority to imperial power than to the saints and bishops. Constantine's iconoclastic tendencies were shared by Constantine's son, Leo IV. After the latter's early death, his widow, Irene of Athens, as regent for her son, began its restoration for personal inclination and political considerations.

In 784 the imperial secretary Patriarch Tarasius was appointed successor to the Patriarch Paul IV—he accepted on the condition that intercommunion with the other churches should be reestablished; that is, that the images should be restored. However, a council, claiming to be ecumenical, had abolished the veneration of icons, so another ecumenical council was necessary for its restoration.

Pope Adrian I was invited to participate, and gladly accepted, sending an archbishop and an abbot as his legates.
In 786, the council met in the Church of the Holy Apostles in Constantinople. However, soldiers in collusion with the opposition entered the church, and broke up the assembly. As a result, the government resorted to a stratagem. Under the pretext of a campaign, the iconoclastic bodyguard was sent away from the capital – disarmed and disbanded.

The council was again summoned to meet, this time in Nicaea, since Constantinople was still distrusted. The council assembled on September 24, 787 at the church of Hagia Sophia. It numbered about 350 members; 308 bishops or their representatives signed. Tarasius presided, and seven sessions were held in Nicaea.

The distinction between the adoration offered to God and that accorded to the images was justified at the council not by asserting that images do not enjoy a status equal to the person represented (since "veneration proceeds through the image to the one represented") but by asserting that only God can be worshipped and that it is impossible to represent him in images. The status of images of Christ was left unclear (see Price, The Acts of the Second Council of Nicaea, 47-49). The twenty-two canons drawn up in Constantinople also served ecclesiastical reform. Careful maintenance of the ordinances of the earlier councils, knowledge of the scriptures on the part of the clergy, and care for Christian conduct are required, and the desire for a renewal of ecclesiastical life is awakened.

The council also decreed that every altar should contain a relic, which remains the case in modern Catholic and Orthodox regulations (Canon VII), and made a number of decrees on clerical discipline, especially for monks when mixing with women.

The papal legates voiced their approval of the restoration of the veneration of icons in no uncertain terms, and the patriarch sent a full account of the proceedings of the council to Pope Hadrian I, who had it translated (Pope Anastasius III later replaced the translation with a better one). The papacy did not, however, formally confirm the decrees of the council till 880. In the West, the Frankish clergy initially rejected the Council at a synod in 794, and Charlemagne, then King of the Franks, supported the composition of the "Libri Carolini" in response, which repudiated the teachings of both the Council and the iconoclasts. A copy of the "Libri" was sent to Pope Hadrian, who responded with a refutation of the Frankish arguments. The "Libri" would thereafter remain unpublished until the Reformation, and the Council is accepted as the Seventh Ecumenical Council by the Catholic Church.

The council is celebrated in the Eastern Orthodox Church, and Eastern Catholic Churches of Byzantine Rite as "The Sunday of the Triumph of Orthodoxy" each year on the first Sunday of Great Lent, the fast that leads up to Pascha (Easter), and again on the Sunday closest to October 11 (the Sunday on or after October 8). The former celebration commemorates the council as the culmination of the Church's battles against heresy, while the latter commemorates the council itself.

Many Protestants follow the French reformer John Calvin in rejecting the canons of the council, which they believe promoted idolatry. He rejected the distinction between veneration ("douleia", "proskynesis") and adoration ("latreia") as unbiblical "sophistry" and condemned even the decorative use of images. In subsequent editions of the "Institutes", he cited an influential Carolingian source, now ascribed to Theodulf of Orleans, which reacts negatively to a poor Latin translation of the council's acts. Calvin did not engage the apologetic arguments of John of Damascus or Theodore the Studite, apparently because he was unaware of them.


There are only a few translations of the above Acts in the modern languages:



There is no up-to-date English monograph on either the council or the iconoclast controversy in general. But see L. Brubaker and J. Haldon, Byzantium in the Iconoclast Era c. 680 to 850: A History (Cambridge 2011). 

]


</doc>
<doc id="29252" url="https://en.wikipedia.org/wiki?curid=29252" title="Sexual orientation">
Sexual orientation

Sexual orientation is an enduring pattern of romantic or sexual attraction (or a combination of these) to persons of the opposite sex or gender, the same sex or gender, or to both sexes or more than one gender. These attractions are generally subsumed under heterosexuality, homosexuality, and bisexuality, while asexuality (the lack of sexual attraction to others) is sometimes identified as the fourth category.

These categories are aspects of the more nuanced nature of sexual identity and terminology. For example, people may use other labels, such as "pansexual" or "polysexual", or none at all. According to the American Psychological Association, sexual orientation "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions". "Androphilia" and "gynephilia" are terms used in behavioral science to describe sexual orientation as an alternative to a gender binary conceptualization. "Androphilia" describes sexual attraction to masculinity; "gynephilia" describes the sexual attraction to femininity. The term "sexual preference" largely overlaps with sexual orientation, but is generally distinguished in psychological research. A person who identifies as bisexual, for example, may sexually prefer one sex over the other. "Sexual preference" may also suggest a degree of voluntary choice, whereas the scientific consensus is that sexual orientation is not a choice.

Scientists do not know the exact cause of sexual orientation, but they theorize that it is caused by a complex interplay of genetic, hormonal, and environmental influences. Although no single theory on the cause of sexual orientation has yet gained widespread support, scientists favor biologically-based theories. There is considerably more evidence supporting nonsocial, biological causes of sexual orientation than social ones, especially for males. There is no substantive evidence which suggests parenting or early childhood experiences play a role with regard to sexual orientation. Across cultures, most people are heterosexual, with a minority of people reporting a homosexual or bisexual sexual orientation. A person's sexual orientation can be anywhere on a continuum, from exclusive attraction to the opposite sex to exclusive attraction to the same sex.

Sexual orientation is studied primarily within biology, neuroscience, and psychology (including sexology), but it is also a subject area in sociology, history (including social constructionist perspectives), and law.

Sexual orientation is traditionally defined as including heterosexuality, bisexuality, and homosexuality, while asexuality is considered the fourth category of sexual orientation by some researchers and has been defined as the absence of a traditional sexual orientation. An asexual has little to no sexual attraction to people. It may be considered a lack of a sexual orientation, and there is significant debate over whether or not it is a sexual orientation.

Most definitions of sexual orientation include a psychological component, such as the direction of an individual's erotic desires, or a behavioral component, which focuses on the sex of the individual's sexual partner/s. Some people prefer simply to follow an individual's self-definition or identity. Scientific and professional understanding is that "the core attractions that form the basis for adult sexual orientation typically emerge between middle childhood and early adolescence". Sexual orientation differs from sexual identity in that it encompasses relationships with others, while sexual identity is a concept of self.

The American Psychological Association states that "[s]exual orientation refers to an enduring pattern of emotional, romantic, and/or sexual attractions to men, women, or both sexes" and that "[t]his range of behaviors and attractions has been described in various cultures and nations throughout the world. Many cultures use identity labels to describe people who express these attractions. In the United States, the most frequent labels are lesbians (women attracted to women), gay men (men attracted to men), and bisexual people (men or women attracted to both sexes). However, some people may use different labels or none at all". They additionally state that sexual orientation "is distinct from other components of sex and gender, including biological sex (the anatomical, physiological, and genetic characteristics associated with being male or female), gender identity (the psychological sense of being male or female), and social gender role (the cultural norms that define feminine and masculine behavior)".

Sexual identity and sexual behavior are closely related to sexual orientation, but they are distinguished, with sexual identity referring to an individual's conception of themselves, behavior referring to actual sexual acts performed by the individual, and orientation referring to "fantasies, attachments and longings." Individuals may or may not express their sexual orientation in their behaviors. People who have a non-heterosexual sexual orientation that does not align with their sexual identity are sometimes referred to as 'closeted'. The term may, however, reflect a certain cultural context and particular stage of transition in societies which are gradually dealing with integrating sexual minorities. In studies related to sexual orientation, when dealing with the degree to which a person's sexual attractions, behaviors and identity match, scientists usually use the terms "concordance" or "discordance." Thus, a woman who is attracted to other women, but calls herself heterosexual and only has sexual relations with men, can be said to experience discordance between her sexual orientation (homosexual or lesbian) and her sexual identity and behaviors (heterosexual).

"Sexual identity" may also be used to describe a person's perception of his or her own "sex", rather than sexual orientation. The term "sexual preference" has a similar meaning to "sexual orientation", and the two terms are often used interchangeably, but "sexual preference" suggests a degree of voluntary choice. The term has been listed by the American Psychological Association's Committee on Gay and Lesbian Concerns as a wording that advances a "heterosexual bias".

"Androphilia" and "gynephilia" (or "gynecophilia") are terms used in behavioral science to describe sexual attraction, as an alternative to a homosexual and heterosexual conceptualization. They are used for identifying a subject's object of attraction without attributing a sex assignment or gender identity to the subject. Related terms such as "pansexual" and "polysexual" do not make any such assignations to the subject. People may also use terms such as "queer", "pansensual," "polyfidelitous," "ambisexual," or personalized identities such as "byke" or "biphilic".

Using "androphilia" and "gynephilia" can avoid confusion and offense when describing people in non-western cultures, as well as when describing intersex and transgender people. Psychiatrist Anil Aggrawal explains that androphilia, along with gynephilia, "is needed to overcome immense difficulties in characterizing the sexual orientation of trans men and trans women. For instance, it is difficult to decide whether a trans man erotically attracted to males is a heterosexual female or a homosexual male; or a trans woman erotically attracted to females is a heterosexual male or a lesbian female. Any attempt to classify them may not only cause confusion but arouse offense among the affected subjects. In such cases, while defining sexual attraction, it is best to focus on the object of their attraction rather than on the sex or gender of the subject." Sexologist Milton Diamond writes, "The terms heterosexual, homosexual, and bisexual are better used as adjectives, not nouns, and are better applied to behaviors, not people. This usage is particularly advantageous when discussing the partners of transsexual or intersexed individuals. These newer terms also do not carry the social weight of the former ones."

Some researchers advocate use of the terminology to avoid bias inherent in Western conceptualizations of human sexuality. Writing about the Samoan fa'afafine demographic, sociologist Johanna Schmidt writes that in cultures where a third gender is recognized, a term like "homosexual transsexual" does not align with cultural categories.

"Same gender loving", or "SGL", is a term adopted by some African-Americans, meant as a culturally affirming homosexual identity.

Some researchers, such as Bruce Bagemihl, have criticized the labels "heterosexual" and "homosexual" as confusing and degrading. Bagemihl writes, "...the point of reference for 'heterosexual' or 'homosexual' orientation in this nomenclature is solely the individual's genetic sex prior to reassignment (see for example, Blanchard et al. 1987, Coleman and Bockting, 1988, Blanchard, 1989). These labels thereby ignore the individual's personal sense of gender identity taking precedence over biological sex, rather than the other way around." Bagemihl goes on to take issue with the way this terminology makes it easy to claim transsexuals are really homosexual males seeking to escape from stigma.

The earliest writers on sexual orientation usually understood it to be intrinsically linked to the subject's own sex. For example, it was thought that a typical female-bodied person who is attracted to female-bodied persons would have masculine attributes, and vice versa. This understanding was shared by most of the significant theorists of sexual orientation from the mid nineteenth to early twentieth century, such as Karl Heinrich Ulrichs, Richard von Krafft-Ebing, Magnus Hirschfeld, Havelock Ellis, Carl Jung, and Sigmund Freud, as well as many gender-variant homosexual people themselves. However, this understanding of homosexuality as sexual inversion was disputed at the time, and, through the second half of the twentieth century, gender identity came to be increasingly seen as a phenomenon distinct from sexual orientation. Transgender and cisgender people may be attracted to men, women, or both, although the prevalence of different sexual orientations is quite different in these two populations. An individual homosexual, heterosexual or bisexual person may be masculine, feminine, or androgynous, and in addition, many members and supporters of lesbian and gay communities now see the "gender-conforming heterosexual" and the "gender-nonconforming homosexual" as negative stereotypes. Nevertheless, studies by J. Michael Bailey and Kenneth Zucker found a majority of the gay men and lesbians sampled reporting various degrees of gender-nonconformity during their childhood years.

Transgender people today identify with the sexual orientation that corresponds with their gender; meaning that a trans woman who is solely attracted to women would often identify as a lesbian. A trans man solely attracted to women would be a straight man.

Sexual orientation sees greater intricacy when non-binary understandings of both sex (male, female, or intersex) and gender (man, woman, transgender, third gender, etc. are considered. Sociologist Paula Rodriguez Rust (2000) argues for a more multifaceted definition of sexual orientation:

Gay and lesbian people can have sexual relationships with someone of the opposite sex for a variety of reasons, including the desire for a perceived traditional family and concerns of discrimination and religious ostracism. While some LGBT people hide their respective orientations from their spouses, others develop positive gay and lesbian identities while maintaining successful heterosexual marriages. Coming out of the closet to oneself, a spouse of the opposite sex, and children can present challenges that are not faced by gay and lesbian people who are not married to people of the opposite sex or do not have children.

Often, sexual orientation and sexual orientation identity are not distinguished, which can impact accurately assessing sexual identity and whether or not sexual orientation is able to change; sexual orientation identity can change throughout an individual's life, and may or may not align with biological sex, sexual behavior or actual sexual orientation.<ref name="Concordance/discordance in SO"></ref> Sexual orientation is stable and unchanging for the vast majority of people, but some research indicates that some people may experience change in their sexual orientation, and this is more likely for women than for men. The American Psychological Association distinguishes between sexual orientation (an innate attraction) and sexual orientation identity (which may change at any point in a person's life).

The exact causes for the development of a particular sexual orientation have yet to be established. To date, a lot of research has been conducted to determine the influence of genetics, hormonal action, development dynamics, social and cultural influences—which has led many to think that biology and environment factors play a complex role in forming it. It was once thought that homosexuality was the result of faulty psychological development, resulting from childhood experiences and troubled relationships, including childhood sexual abuse. It has been found that this was based on prejudice and misinformation.

Research has identified several biological factors which may be related to the development of sexual orientation, including genes, prenatal hormones, and brain structure. No single controlling cause has been identified, and research is continuing in this area.

Although researchers generally believe that sexual orientation is not determined by any one factor but by a combination of genetic, hormonal, and environmental influences, with biological factors involving a complex interplay of genetic factors and the early uterine environment, they favor biological models for the cause. There is considerably more evidence supporting nonsocial, biological causes of sexual orientation than social ones, especially for males. Scientists do not believe that sexual orientation is a choice, and some of them believe that it is established at conception. Current scientific investigation usually seeks to find biological explanations for the adoption of a particular sexual orientation. Scientific studies have found a number of statistical biological differences between gay people and heterosexuals, which may result from the same underlying cause as sexual orientation itself.

Genes may be related to the development of sexual orientation. A twin study from 2001 appears to exclude genes as a major factor, while a twin study from 2010 found that homosexuality was explained by both genes and environmental factors. However, experimental design of the available twin studies has made their interpretation difficult.

In 2012, a large, comprehensive genome-wide linkage study of male sexual orientation was conducted by several independent groups of researchers. Significant linkage to homosexuality was found with genes on chromosome Xq28 and chromosome 8 in the pericentromeric region. The authors concluded that "our findings, taken in context with previous work, suggest that genetic variation in each of these regions contributes to development of the important psychological trait of male sexual orientation." It was the largest study of the genetic basis of homosexuality to date and was published online in November 2014.

The hormonal theory of sexuality holds that just as exposure to certain hormones plays a role in fetal sex differentiation, hormonal exposure also influences the sexual orientation that emerges later in the adult. Fetal hormones may be seen as either the primary influence upon adult sexual orientation or as a co-factor interacting with genes or environmental and social conditions.

For humans, the norm is that females possess two X sex chromosomes, while males have one X and one Y. The default developmental pathway for a human fetus being female, the Y chromosome is what induces the changes necessary to shift to the male developmental pathway. This differentiation process is driven by androgen hormones, mainly testosterone and dihydrotestosterone (DHT). The newly formed testicles in the fetus are responsible for the secretion of androgens, that will cooperate in driving the sexual differentiation of the developing fetus, including its brain. This results in sexual differences between males and females. This fact has led some scientists to test in various ways the result of modifying androgen exposure levels in mammals during fetus and early life.

A significant volume of research has demonstrated that the probability of a male growing up to be gay increases with each older brother he has from the same mother. Known as the "fraternal birth order" (FBO) effect, scientists attribute this to a prenatal biological mechanism – specifically a maternal immune response to male fetuses – since the effect is only present in men with older biological brothers, and not present among men with older step-brothers and adoptive brothers. This process, known as the "maternal immunization hypothesis" (MIH), would begin when cells from a male fetus enter the mother's circulation during pregnancy. These cells carry Y-proteins, which are thought to play a role in brain masculinisation (sex-differentiation) during fetal development. The mothers immune system builds antibodies to these Y-proteins. These antibodies are later released on future male fetuses and interfere with the masculinization role of Y-proteins. leaving regions of the brain responsible for sexual orientation in the 'default' female–typical arrangement, causing the exposed son to be more attracted to men over women. Biochemical evidence for this hypothesis was identified in 2017, finding that mothers with a gay son, especially those with older brothers, had significantly higher levels of anti-bodies to the NLGN4Y Y-protein than mothers with heterosexual sons. 

The effect becomes stronger with each successive male pregnancy, meaning the odds of the next son being gay increase by 38–48%. This does not mean that all or most sons will be gay after several male pregnancies, but rather, the odds of having a gay son increase from approximately 2% for the first born son, to 4% for the second, 6% for the third and so on. Scientists have estimated between 15% and 29% of gay men may owe their sexual orientation to this effect, but the number may be higher, as prior miscarriages and terminations of male pregnancies may have exposed their mothers to Y-linked antigens. The fraternal birth order effect would not likely apply to first born gay sons; instead, scientists say they may owe their orientation to genes, prenatal hormones and other maternal immune responses which also influence brain development. This effect is nullified if the man is left-handed. Ray Blanchard and Anthony Bogaert are credited with discovering the effect in the 1990s, and Blanchard describes it as "one of the most reliable epidemiological variables ever identified in the study of sexual orientation". J. Michael Bailey and Jacques Balthazart say the FBO effect demonstrates that sexual orientation is heavily influenced by prenatal biological mechanisms rather than unidentified factors in socialization.

In the field of genetics, any factor which is non-genetic is considered an "environmental influence". However, environmental influence does not automatically imply that the social environment influences or contributes to the development of sexual orientation. There is a vast non-social environment that is non-genetic yet still biological, such as prenatal development, that likely helps shape sexual orientation. There is no substantive evidence to support the suggestion that early childhood experiences, parenting, sexual abuse, or other adverse life events influence sexual orientation. Hypotheses for the impact of the post-natal social environment on sexual orientation are weak, especially for males. Parental attitudes may affect whether or not children openly identify with their sexual orientation.

The American Academy of Pediatrics in 2004 stated:
The American Psychological Association, the American Psychiatric Association, and the National Association of Social Workers in 2006 stated:
The Royal College of Psychiatrists in 2007 stated:
The American Psychiatric Association stated:
A legal brief dated September 26, 2007, and presented on behalf of the American Psychological Association, California Psychological Association, American Psychiatric Association, National Association of Social Workers, and National Association of Social Workers, California Chapter, stated:

Sexual orientation change efforts are methods that aim to change a same-sex sexual orientation. They may include behavioral techniques, cognitive behavioral therapy, reparative therapy, psychoanalytic techniques, medical approaches, and religious and spiritual approaches.

No major mental health professional organization sanctions efforts to change sexual orientation and virtually all of them have adopted policy statements cautioning the profession and the public about treatments that purport to change sexual orientation. These include the American Psychiatric Association, American Psychological Association, American Counseling Association, National Association of Social Workers in the US, the Royal College of Psychiatrists, and the Australian Psychological Society.

In 2009, the American Psychological Association Task Force on Appropriate Therapeutic Responses to Sexual Orientation conducted a systematic review of the peer-reviewed journal literature on sexual orientation change efforts (SOCE) and concluded:
Efforts to change sexual orientation are unlikely to be successful and involve some risk of harm, contrary to the claims of SOCE practitioners and advocates. Even though the research and clinical literature demonstrate that same-sex sexual and romantic attractions, feelings, and behaviors are normal and positive variations of human sexuality, regardless of sexual orientation identity, the task force concluded that the population that undergoes SOCE tends to have strongly conservative religious views that lead them to seek to change their sexual orientation. Thus, the appropriate application of affirmative therapeutic interventions for those who seek SOCE involves therapist acceptance, support, and understanding of clients and the facilitation of clients' active coping, social support, and identity exploration and development, without imposing a specific sexual orientation identity outcome.

In 2012, the Pan American Health Organization (the North and South American branch of the World Health Organization) released a statement cautioning against services that purport to "cure" people with non-heterosexual sexual orientations as they lack medical justification and represent a serious threat to the health and well-being of affected people, and noted that the global scientific and professional consensus is that homosexuality is a normal and natural variation of human sexuality and cannot be regarded as a pathological condition. The Pan American Health Organization further called on governments, academic institutions, professional associations and the media to expose these practices and to promote respect for diversity. The World Health Organization affiliate further noted that gay minors have sometimes been forced to attend these "therapies" involuntarily, being deprived of their liberty and sometimes kept in isolation for several months, and that these findings were reported by several United Nations bodies. Additionally, the Pan American Health Organization recommended that such malpractices be denounced and subject to sanctions and penalties under national legislation, as they constitute a violation of the ethical principles of health care and violate human rights that are protected by international and regional agreements.

The National Association for Research & Therapy of Homosexuality (NARTH), which described itself as a "professional, scientific organization that offers hope to those who struggle with unwanted homosexuality," disagreed with the mainstream mental health community's position on conversion therapy, both on its effectiveness and by describing sexual orientation not as a binary immutable quality, or as a disease, but as a continuum of intensities of sexual attractions and emotional affect. The American Psychological Association and the Royal College of Psychiatrists expressed concerns that the positions espoused by NARTH are not supported by the science and create an environment in which prejudice and discrimination can flourish.

Varying definitions and strong social norms about sexuality can make sexual orientation difficult to quantify.

One of the earliest sexual orientation classification schemes was proposed in the 1860s by Karl Heinrich Ulrichs in a series of pamphlets he published privately. The classification scheme, which was meant only to describe males, separated them into three basic categories: "dionings, urnings" and "uranodionings". An "urning" can be further categorized by degree of effeminacy. These categories directly correspond with the categories of sexual orientation used today: "heterosexual", "homosexual", and "bisexual". In the series of pamphlets, Ulrichs outlined a set of questions to determine if a man was an "urning". The definitions of each category of Ulrichs' classification scheme are as follows:

From at least the late nineteenth century in Europe, there was speculation that the range of human sexual response looked more like a continuum than two or three discrete categories. Berlin sexologist Magnus Hirschfeld published a scheme in 1896 that measured the strength of an individual's sexual desire on two independent 10-point scales, A (homosexual) and B (heterosexual). A heterosexual individual may be A0, B5; a homosexual individual may be A5, B0; an asexual would be A0, B0; and someone with an intense attraction to both sexes would be A9, B9.

The Kinsey scale, also called the Heterosexual-Homosexual Rating Scale, was first published in "Sexual Behavior in the Human Male" (1948) by Alfred Kinsey, Wardell Pomeroy, and Clyde Martin and also featured in "Sexual Behavior in the Human Female" (1953). The scale was developed to combat the assumption at the time that people are either heterosexual or homosexual and that these two types represent antitheses in the sexual world. Recognizing that a significant portion of the population is not completely heterosexual or homosexual and that such people can experience both heterosexual and homosexual behavior and psychic responses, Kinsey et al., stated:
The Kinsey scale provides a classification of sexual orientation based on the relative amounts of heterosexual and homosexual experience or psychic response in one's history at a given time. The classification scheme works such that individuals in the same category show the same balance between the heterosexual and homosexual elements in their histories. The position on the scale is based on the relation of heterosexuality to homosexuality in one's history, rather than the actual amount of overt experience or psychic response. An individual can be assigned a position on the scale in accordance with the following definitions of the points of the scale:

The Kinsey scale has been praised for dismissing the dichotomous classification of sexual orientation and allowing for a new perspective on human sexuality. Despite seven categories being able to provide a more accurate description of sexual orientation than a dichotomous scale, it is still difficult to determine which category individuals should be assigned to. In a major study comparing sexual response in homosexual males and females, Masters and Johnson discuss the difficulty of assigning the Kinsey ratings to participants. Particularly, they found it difficult to determine the relative amount heterosexual and homosexual experience and response in a person's history when using the scale. They report finding it difficult to assign ratings 2–4 for individuals with a large number of heterosexual and homosexual experiences. When there are a substantial number of heterosexual and homosexual experiences in one's history, it becomes difficult for that individual to be fully objective in assessing the relative amount of each.

Weinrich et al. (1993) and Weinberg et al. (1994) criticized the scale for lumping individuals who are different based on different dimensions of sexuality into the same categories. When applying the scale, Kinsey considered two dimensions of sexual orientation: overt sexual experience and psychosexual reactions. Valuable information was lost by collapsing the two values into one final score. A person who has only predominantly same sex reactions is different from someone with relatively little reaction but much same sex experience. It would have been quite simple for Kinsey to have measured the two dimensions separately and report scores independently to avoid loss of information. Furthermore, there are more than two dimensions of sexuality to be considered. Beyond behavior and reactions, one could also assess attraction, identification, lifestyle, etc. This is addressed by the Klein Sexual Orientation Grid.

A third concern with the Kinsey scale is that it inappropriately measures heterosexuality and homosexuality on the same scale, making one a tradeoff of the other. Research in the 1970s on masculinity and femininity found that concepts of masculinity and femininity are more appropriately measured as independent concepts on a separate scale rather than as a single continuum, with each end representing opposite extremes. When compared on the same scale, they act as tradeoffs such, whereby to be more feminine one had to be less masculine and vice versa. However, if they are considered as separate dimensions one can be simultaneously very masculine and very feminine. Similarly, considering heterosexuality and homosexuality on separate scales would allow one to be both very heterosexual and very homosexual or not very much of either. When they are measured independently, the degree of heterosexual and homosexual can be independently determined, rather than the balance between heterosexual and homosexual as determined using the Kinsey Scale.

In response to the criticism of the Kinsey scale only measuring two dimensions of sexual orientation, Fritz Klein developed the Klein sexual orientation grid (KSOG), a multidimensional scale for describing sexual orientation. Introduced in Klein's book "The Bisexual Option" (1978), the KSOG uses a 7-point scale to assess seven different dimensions of sexuality at three different points in an individual's life: past (from early adolescence up to one year ago), present (within the last 12 months), and ideal (what would you choose if it were completely your choice).

The Sell Assessment of Sexual Orientation (SASO) was developed to address the major concerns with the Kinsey Scale and Klein Sexual Orientation Grid and as such, measures sexual orientation on a continuum, considers various dimensions of sexual orientation, and considers homosexuality and heterosexuality separately. Rather than providing a final solution to the question of how to best measure sexual orientation, the SASO is meant to provoke discussion and debate about measurements of sexual orientation.

The SASO consists of 12 questions. Six of these questions assess sexual attraction, four assess sexual behavior, and two assess sexual orientation identity. For each question on the scale that measures homosexuality there is a corresponding question that measures heterosexuality giving six matching pairs of questions. Taken all together, the six pairs of questions and responses provide a profile of an individual's sexual orientation. However, results can be further simplified into four summaries that look specifically at responses that correspond to either homosexuality, heterosexuality, bisexuality or asexuality.

Of all the questions on the scale, Sell considered those assessing sexual attraction to be the most important as sexual attraction is a better reflection of the concept of sexual orientation which he defined as "extent of sexual attractions toward members of the other, same, both sexes or neither" than either sexual identity or sexual behavior. Identity and behavior are measured as supplemental information because they are both closely tied to sexual attraction and sexual orientation. Major criticisms of the SASO have not been established, but a concern is that the reliability and validity remains largely unexamined.

Research focusing on sexual orientation uses scales of assessment to identify who belongs in which sexual population group. It is assumed that these scales will be able to reliably identify and categorize people by their sexual orientation. However, it is difficult to determine an individual's sexual orientation through scales of assessment, due to ambiguity regarding the definition of sexual orientation. Generally, there are three components of sexual orientation used in assessment. Their definitions and examples of how they may be assessed are as follows:

Though sexual attraction, behavior, and identity are all components of sexual orientation, if a person defined by one of these dimensions were congruent with those defined by another dimension it would not matter which was used in assessing orientation, but this is not the case. There is "little coherent relationship between the amount and mix of homosexual and heterosexual behavior in a person's biography and that person's choice to label himself or herself as bisexual, homosexual, or heterosexual". Individuals typically experience diverse attractions and behaviors that may reflect curiosity, experimentation, social pressure and is not necessarily indicative of an underlying sexual orientation. For example, a woman may have fantasies or thoughts about sex with other women but never act on these thoughts and only have sex with opposite gender partners. If sexual orientation was being assessed based on one's sexual attraction then this individual would be considered homosexual, but her behavior indicates heterosexuality.

As there is no research indicating which of the three components is essential in defining sexual orientation, all three are used independently and provide different conclusions regarding sexual orientation. Savin Williams (2006) discusses this issue and notes that by basing findings regarding sexual orientation on a single component, researchers may not actually capture the intended population. For example, if homosexual is defined by same sex behavior, gay virgins are omitted, heterosexuals engaging in same sex behavior for other reasons than preferred sexual arousal are miscounted, and those with same sex attraction who only have opposite-sex relations are excluded. Because of the limited populations that each component captures, consumers of research should be cautious in generalizing these findings.

One of the uses for scales that assess sexual orientation is determining what the prevalence of different sexual orientations are within a population. Depending on subject's age, culture and sex, the prevalence rates of homosexuality vary depending on which component of sexual orientation is being assessed: sexual attraction, sexual behavior, or sexual identity. Assessing sexual attraction will yield the greatest prevalence of homosexuality in a population whereby the proportion of individuals indicating they are same sex attracted is two to three times greater than the proportion reporting same sex behavior or identify as gay, lesbian, or bisexual. Furthermore, reports of same sex behavior usually exceed those of gay, lesbian, or bisexual identification. The following chart demonstrates how widely the prevalence of homosexuality can vary depending on what age, location and component of sexual orientation is being assessed:

The variance in prevalence rates is reflected in people's inconsistent responses to the different components of sexual orientation within a study and the instability of their responses over time. Laumann et al., (1994) found that among U.S. adults 20% of those who would be considered homosexual on one component of orientation were homosexual on the other two dimensions and 70% responded in a way that was consistent with homosexuality on only one of the three dimensions. Furthermore, sexuality may be fluid; for example, a person's sexual orientation identity is not necessarily stable or consistent over time but is subject to change throughout life. Diamond (2003) found that over 7 years 2/3 of the women changed their sexual identity at least once, with many reporting that the label was not adequate in capturing the diversity of their sexual or romantic feelings. Furthermore, women who relinquished bisexual and lesbian identification did not relinquish same sex sexuality and acknowledged the possibility for future same sex attractions or behaviour. One woman stated "I'm mainly straight but I'm one of those people who, if the right circumstance came along, would change my viewpoint". Therefore, individuals classified as homosexual in one study might not be identified the same way in another depending on which components are assessed and when the assessment is made making it difficult to pin point who is homosexual and who is not and what the overall prevalence within a population may be.

Depending on which component of sexual orientation is being assessed and referenced, different conclusions can be drawn about the prevalence rate of homosexuality which has real world consequences. Knowing how much of the population is made up of homosexual individuals influences how this population may be seen or treated by the public and government bodies. For example, if homosexual individuals constitute only 1% of the general population they are politically easier to ignore or than if they are known to be a constituency that surpasses most ethnic and ad minority groups. If the number is relatively minor then it is difficult to argue for community based same sex programs and services, mass media inclusion of gay role models, or Gay/Straight Alliances in schools. For this reason, in the 1970s Bruce Voeller, the chair of the National Gay and Lesbian Task Force perpetuated a common myth that the prevalence of homosexuality is 10% for the whole population by averaging a 13% number for men and a 7% number for women. Voeller generalized this finding and used it as part of the modern gay rights movement to convince politicians and the public that "we [gays and lesbians] are everywhere".

In the paper "Who's Gay? Does It Matter?", psychologist Ritch Savin-Williams proposes two different approaches to assessing sexual orientation until well positioned and psychometrically sound and tested definitions are developed that would allow research to reliably identify the prevalence, causes, and consequences of homosexuality.
He first suggests that greater priority should be given to sexual arousal and attraction over behaviour and identity because it is less prone to self- and other-deception, social conditions and variable meanings. To measure attraction and arousal he proposed that biological measures should be developed and used. There are numerous biological/physiological measures that exist that can measure sexual orientation such as sexual arousal, brain scans, eye tracking, body odour preference, and anatomical variations such as digit-length ratio and right or left-handedness.
Secondly, Savin-Williams suggests that researchers should forsake the general notion of sexual orientation altogether and assess only those components that are relevant to the research question being investigated. For example:

Means typically used include surveys, interviews, cross-cultural studies, physical arousal measurements sexual behavior, sexual fantasy, or a pattern of erotic arousal. The most common is verbal self-reporting or self-labeling, which depend on respondents being accurate about themselves.

Studying human sexual arousal has proved a fruitful way of understanding how men and women differ as genders and in terms of sexual orientation. A clinical measurement may use penile or vaginal photoplethysmography, where genital engorgement with blood is measured in response to exposure to different erotic material.

Some researchers who study sexual orientation argue that the concept may "not" apply similarly to men and women. A study of sexual arousal patterns found that women, when viewing erotic films which show female-female, male-male and male-female sexual activity (oral sex or penetration), have patterns of arousal which do "not" match their declared sexual orientations as well as men's. That is, heterosexual and lesbian women's sexual arousal to erotic films do "not" differ significantly by the genders of the participants (male or female) or by the type of sexual activity (heterosexual or homosexual). On the contrary, men's sexual arousal patterns tend to be more in line with their stated orientations, with heterosexual men showing more penis arousal to female-female sexual activity and less arousal to female-male and male-male sexual stimuli, and homosexual and bisexual men being more aroused by films depicting male-male intercourse and less aroused by other stimuli.

Another study on men and women's patterns of sexual arousal confirmed that men and women have different patterns of arousal, independent of their sexual orientations. The study found that women's genitals become aroused to both human and nonhuman stimuli from movies showing humans of both genders having sex (heterosexual and homosexual) and from videos showing non-human primates (bonobos) having sex. Men did "not" show any sexual arousal to non-human visual stimuli, their arousal patterns being in line with their specific sexual interest (women for heterosexual men and men for homosexual men).

These studies suggest that men and women are different in terms of sexual arousal patterns and that this is also reflected in how their genitals react to sexual stimuli of both genders or even to non-human stimuli. Sexual orientation has many dimensions (attractions, behavior, identity), of which sexual arousal is the only product of sexual attractions which can be measured at present with some degree of physical precision. Thus, the fact that women are aroused by seeing non-human primates having sex does not mean that women's sexual orientation includes this type of sexual interest. Some researchers argue that women's sexual orientation depends less on their patterns of sexual arousal than men's and that other components of sexual orientation (like emotional attachment) must be taken into account when describing women's sexual orientations. In contrast, men's sexual orientations tend to be primarily focused on the physical component of attractions and, thus, their sexual feelings are more exclusively oriented according to sex.

More recently, scientists have started to focus on measuring changes in brain activity related to sexual arousal, by using brain-scanning techniques. A study on how heterosexual and homosexual men's brains react to seeing pictures of naked men and women has found that both hetero- and homosexual men react positively to seeing their preferred sex, using the same brain regions. The only significant group difference between these orientations was found in the amygdala, a brain region known to be involved in regulating fear.

Research suggests that sexual orientation is independent of cultural and other social influences, but that open identification of one's sexual orientation may be hindered by homophobic/heterosexist settings. Social systems such as religion, language and ethnic traditions can have a powerful impact on realization of sexual orientation. Influences of culture may complicate the process of measuring sexual orientation. The majority of empirical and clinical research on LGBT populations are done with largely white, middle-class, well-educated samples, however there are pockets of research that document various other cultural groups, although these are frequently limited in diversity of gender and sexual orientation of the subjects. Integration of sexual orientation with sociocultural identity may be a challenge for LGBT individuals. Individuals may or may not consider their sexual orientation to define their sexual identity, as they may experience various degrees of fluidity of sexuality, or may simply identify more strongly with another aspect of their identity such as family role. American culture puts a great emphasis on individual attributes, and views the self as unchangeable and constant. In contrast, East Asian cultures put a great emphasis on a person's social role within social hierarchies, and view the self as fluid and malleable. These differing cultural perspectives have many implications on cognition of the self, including perception of sexual orientation.

Translation is a major obstacle when comparing different cultures. Many English terms lack equivalents in other languages, while concepts and words from other languages fail to be reflected in the English language. Translation and vocabulary obstacles are not limited to the English language. Language can force individuals to identify with a label that may or may not accurately reflect their true sexual orientation. Language can also be used to signal sexual orientation to others. The meaning of words referencing categories of sexual orientation are negotiated in the mass media in relation to social organization. New words may be brought into use to describe new terms or better describe complex interpretations of sexual orientation. Other words may pick up new layers or meaning. For example, the heterosexual Spanish terms "marido" and "mujer" for "husband" and "wife", respectively, have recently been replaced in Spain by the gender-neutral terms "cónyuges" or "consortes" meaning "spouses".

One person may presume knowledge of another person's sexual orientation based upon perceived characteristics, such as appearance, clothing, voice (c.f. Gay male speech), and accompaniment by and behavior with other people. The attempt to detect sexual orientation in social situations is sometimes colloquially known as gaydar; some studies have found that guesses based on face photos perform better than chance. 2015 research suggests that "gaydar" is an alternate label for using LGBT stereotypes to infer orientation, and that face-shape is not an accurate indication of orientation.

Perceived sexual orientation may affect how a person is treated. For instance, in the United States, the FBI reported that 15.6% of hate crimes reported to police in 2004 were "because of a sexual-orientation bias". Under the UK Employment Equality (Sexual Orientation) Regulations 2003, as explained by Advisory, Conciliation and Arbitration Service, "workers or job applicants must not be treated less favourably because of their sexual orientation, their perceived sexual orientation or because they associate with someone of a particular sexual orientation".

In Euro-American cultures, sexual orientation is defined by the gender(s) of the people a person is romantically or sexually attracted to. Euro-American culture generally assumes heterosexuality, unless otherwise specified. Cultural norms, values, traditions and laws facilitate heterosexuality, including constructs of marriage and family. Efforts are being made to change these attitudes, and legislation is being passed to promote equality.
Some other cultures do not recognize a homosexual/heterosexual/bisexual distinction. It is common to distinguish a person's sexuality according to their sexual role (active/passive; insertive/penetrated). In this distinction, the passive role is typically associated with femininity or inferiority, while the active role is typically associated with masculinity or superiority. For example, an investigation of a small Brazilian fishing village revealed three sexual categories for men: men who have sex only with men (consistently in a passive role), men who have sex only with women, and men who have sex with women and men (consistently in an active role). While men who consistently occupied the passive role were recognized as a distinct group by locals, men who have sex with only women, and men who have sex with women and men, were not differentiated. Little is known about same-sex attracted females, or sexual behavior between females in these cultures.

In the United States, non-Caucasian LGBT individuals may find themselves in a double minority, where they are neither fully accepted or understood by mainly Caucasian LGBT communities, nor are they accepted by their own ethnic group. Many people experience racism in the dominant LGBT community where racial stereotypes merge with gender stereotypes, such that Asian-American LGBTs are viewed as more passive and feminine, while African-American LGBTs are viewed as more masculine and aggressive. There are a number of culturally specific support networks for LGBT individuals active in the United States. For example, "Ô-Môi" for Vietnamese American queer females.

Sexuality in the context of religion is often a controversial subject, especially that of sexual orientation. In the past, various sects have viewed homosexuality from a negative point of view and had punishments for same-sex relationships. In modern times, an increasing number of religions and religious denominations accept homosexuality. It is possible to integrate sexual identity and religious identity, depending on the interpretation of religious texts.

Some religious organizations object to the concept of sexual orientation entirely. In the 2014 revision of the code of ethics of the American Association of Christian Counselors, members are forbidden to "describe or reduce human identity and nature to sexual orientation or reference," even while counselors must acknowledge the client's fundamental right to self-determination.

The internet has influenced sexual orientation in two ways: it is a common mode of discourse on the subject of sexual orientation and sexual identity, and therefore shapes popular conceptions; and it allows anonymous attainment of sexual partners, as well as facilitates communication and connection between greater numbers of people.

Modern scientific surveys find that, across cultures, most people report a heterosexual orientation. Bisexuality comes in varying degrees of relative attraction to the same or opposite sex. Men are more likely to be exclusively homosexual than to be equally attracted to both sexes, while the opposite is true for women.

Surveys in Western cultures find, on average, that about 93% of men and 87% of women identify as completely heterosexual, 4% of men and 10% of women as mostly heterosexual, 0.5% of men and 1% of women as evenly bisexual, 0.5% of men and 0.5% of women as mostly homosexual, and 2% of men and 0.5% of women as completely homosexual. An analysis of 67 studies found that the lifetime prevalence of sex between men (regardless of orientation) was 3-5% for East Asia, 6-12% for South and South East Asia, 6-15% for Eastern Europe, and 6-20% for Latin America. The International HIV/AIDS Alliance estimates a worldwide prevalence of men who have sex with men between 3 and 16%.<ref name="International HIV/AIDS Alliance 2003"></ref>

The relative percentage of the population that reports a homosexual or bisexual orientation can vary with different methodologies and selection criteria. A 1998 report stated that these statistical findings are in the range of 2.8 to 9% for males, and 1 to 5% for females for the United States – this figure can be as high as 12% for some large cities and as low as 1% for rural areas.

A small percentage of people are not sexually attracted to anyone (asexuality). A study in 2004 placed the prevalence of asexuality at 1%.

In "Sexual Behavior in the Human Male" (1948) and "Sexual Behavior in the Human Female" (1953), by Alfred C. Kinsey et al., people were asked to rate themselves on a scale from completely heterosexual to completely homosexual. Kinsey reported that when the individuals' behavior, as well as their identity, are analyzed, a significant number of people appeared to be at least somewhat bisexual – i.e., they have some attraction to either sex, although usually one sex is preferred. However, only a small minority can be considered fully bisexual (with an equal attraction to both sexes). Kinsey's methods have been criticized as flawed, particularly with regard to the randomness of his sample population, which included prison inmates, male prostitutes and those who willingly participated in discussion of previously taboo sexual topics. Nevertheless, Paul Gebhard, subsequent director of the Kinsey Institute for Sex Research, reexamined the data in the Kinsey Reports and concluded that removing the prison inmates and prostitutes barely affected the results. More recent researchers believe that Kinsey overestimated the rate of same-sex attraction because of flaws in his sampling methods.

Because sexual orientation is complex, some academics and researchers, especially in queer studies, have argued that it is a historical and social construction. In 1976, philosopher and historian Michel Foucault argued in "The History of Sexuality" that homosexuality as an identity did not exist in the eighteenth century; that people instead spoke of "sodomy," which referred to sexual acts. Sodomy was a crime that was often ignored, but sometimes punished severely under sodomy laws. He wrote, "'Sexuality' is an invention of the modern state, the industrial revolution, and capitalism." Other scholars argue that there are significant continuities between ancient and modern homosexuality. The philosopher of science Michael Ruse has stated that the social constructionist approach, which is influenced by Foucault, is based on a selective reading of the historical record that confuses the existence of homosexual people with the way in which they are labelled or treated.

In much of the modern world, sexual identity is defined based on the sex of one's partner. In some parts of the world, however, sexuality is often socially defined based on sexual roles, whether one is a penetrator or is penetrated. In Western cultures, people speak meaningfully of gay, lesbian, and bisexual identities and communities. In some other cultures, homosexuality and heterosexual labels do not emphasize an entire social identity or indicate community affiliation based on sexual orientation.

Some historians and researchers argue that the emotional and affectionate activities associated with sexual-orientation terms such as "gay" and "heterosexual" change significantly over time and across cultural boundaries. For example, in many English-speaking nations, it is assumed that same-sex kissing, particularly between men, is a sign of homosexuality, whereas various types of same-sex kissing are common expressions of friendship in other nations. Also, many modern and historic cultures have formal ceremonies expressing long-term commitment between same-sex friends, even though homosexuality itself is taboo within the cultures.

Professor Michael King stated, "The conclusion reached by scientists who have investigated the origins and stability of sexual orientation is that it is a human characteristic that is formed early in life, and is resistant to change. Scientific evidence on the origins of homosexuality is considered relevant to theological and social debate because it undermines suggestions that sexual orientation is a choice."

In 1999, law professor David Cruz wrote that "sexual orientation (and the related concept homosexuality) might plausibly refer to a variety of different attributes, singly or in combination. What is not immediately clear is whether one conception is most suited to all social, legal, and constitutional purposes."




</doc>
<doc id="29253" url="https://en.wikipedia.org/wiki?curid=29253" title="Spandrel">
Spandrel

A spandrel is a triangular space, usually found in pairs, between the top of an arch and a rectangular frame; between the tops of two adjacent arches or one of the four spaces between a circle within a square. They are frequently filled with decorative elements.

There are four or five accepted and cognate meanings of the term "spandrel" in architectural and art history, mostly relating to the space between a curved figure and a rectangular boundary – such as the space between the curve of an arch and a rectilinear bounding moulding, or the wallspace bounded by adjacent arches in an arcade and the stringcourse or moulding above them, or the space between the central medallion of a carpet and its rectangular corners, or the space between the circular face of a clock and the corners of the square revealed by its hood. Also included is the space under a flight of stairs, if it is not occupied by another flight of stairs.

In a building with more than one floor, the term spandrel is also used to indicate the space between the top of the window in one story and the sill of the window in the story above. The term is typically employed when there is a sculpted panel or other decorative element in this space, or when the space between the windows is filled with opaque or translucent glass, in this case called "spandrel glass". In concrete or steel construction, an exterior beam extending from column to column usually carrying an exterior wall load is known as a "spandrel beam".

The spandrels over doorways in perpendicular work are generally richly decorated. At Magdalen College, Oxford, is one which is perforated. The spandrel of doors is sometimes ornamented in the Decorated Period, but seldom forms part of the composition of the doorway itself, being generally over the label.

Spandrels can also occur in the construction of domes and are typical in grand architecture from the medieval period onwards. Where a dome needed to rest on a square or rectangular base, the dome was raised above the level of the supporting pillars, with three-dimensional spandrels called pendentives taking the weight of the dome and concentrating it onto the pillars.




</doc>
<doc id="29257" url="https://en.wikipedia.org/wiki?curid=29257" title="SimpleText">
SimpleText

SimpleText is the native text editor for the Apple classic Mac OS. SimpleText allows text editing and text formatting (underline, italic, bold, etc.), fonts, and sizes. It was developed to integrate the features included in the different versions of TeachText that were created by various software development groups within Apple.

It can be considered similar to Windows' WordPad application. In later versions it also gained additional read only display capabilities for PICT files, as well as other Mac OS built-in formats like Quickdraw GX and QTIF, 3DMF and even QuickTime movies. SimpleText can even record short sound samples and, using Apple's PlainTalk speech system, read out text in English. Users who wanted to add sounds longer than 24 seconds, however, needed to use a separate program to create the sound and then paste the desired sound into the document using ResEdit.

SimpleText superseded TeachText, which was included in System Software up until it was replaced in 1994 (shipped with System Update 3.0 and System 7.1.2). The need for SimpleText arose after Apple stopped bundling MacWrite, to ensure that every user could open and read Readme documents.

The key improvement of SimpleText over TeachText was the addition of text styling. The underlying OS required by SimpleText implemented a standard styled text format, which meant that SimpleText could support multiple fonts and font sizes. Prior Macintosh OS versions lacked this feature, so TeachText supported only a single font per document. Adding text styling features made SimpleText WorldScript-savvy, meaning that it can use Simplified and Traditional Chinese characters. Like TeachText, SimpleText was also limited to only 32 kB of text in a document, although images could increase the total file size beyond this limit. SimpleText style information was stored in the file's resource fork in such a way that if the resource fork was stripped (such as by uploading to a non-Macintosh server), the text information would be retained.

In Mac OS X, SimpleText is replaced by the more powerful TextEdit application, which reads and writes more document formats as well as including word processor-like features such as a ruler and spell checking. TextEdit's styled text format is RTF, which is able to survive a single-forked file system intact.

Apple has released the source code for a Carbon version of SimpleText in the Mac OS X Developer Tools. If the Developer Tools are installed, it can be found at /Developer/Examples/Carbon/SimpleText.




</doc>
<doc id="29263" url="https://en.wikipedia.org/wiki?curid=29263" title="Statute of Westminster 1931">
Statute of Westminster 1931

The Statute of Westminster 1931 is an Act of the Parliament of the United Kingdom whose modified versions are now domestic law within Australia and Canada; it has been repealed in New Zealand and implicitly in former Dominions that are no longer Commonwealth realms. Passed on 11 December 1931, the act, either immediately or upon ratification, effectively both established the legislative independence of the self-governing Dominions of the British Empire from the United Kingdom and bound them all to seek each other's approval for changes to monarchical titles and the common line of succession. It thus became a statutory embodiment of the principles of equality and common allegiance to the Crown set out in the Balfour Declaration of 1926. As the statute removed nearly all of the British parliament's authority to legislate for the Dominions, it had the effect of making the Dominions largely sovereign nations in their own right. It was a crucial step in the development of the Dominions as separate states.

The Statute of Westminster's relevance today is that it sets the basis for the relationship between the Commonwealth realms and the Crown.

The Statute of Westminster gave effect to certain political resolutions passed by the Imperial Conferences of 1926 and 1930; in particular, the Balfour Declaration of 1926. The main effect was the removal of the ability of the British parliament to legislate for the Dominions, part of which also required the repeal of the Colonial Laws Validity Act 1865 in its application to the Dominions. King George V expressed his desire that the laws of royal succession be exempt from the Statute's provisions, but it was determined that this would be contrary to the principles of equality set out in the Balfour Declaration. Both Canada and the Irish Free State pushed for the ability to amend the succession laws themselves and section 2(2) (allowing a Dominion to amend or repeal laws of paramount force, such as the succession laws, insofar as they are part of the law of that Dominion) was included in the Statute of Westminster at Canada's insistence. After the Statute was passed, the British parliament could no longer make laws for the Dominions, other than with the request and consent of the government of that Dominion. Before then, the Dominions had legally been self-governing colonies of the United Kingdom. However, the statute had the effect of making them sovereign nations once they adopted it.

The Statute provides in section 4:
It also provides in section 2(1):
The whole Statute applied to the Dominion of Canada, the Irish Free State, and the Union of South Africa without the need for any acts of ratification; the governments of those countries gave their consent to the application of the law to their respective jurisdictions. Section 10 of the Statute provided that sections 2 to 6 would apply in the other three Dominions—Australia, New Zealand, and Newfoundland - only after the Parliament of that Dominion had legislated to adopt them.

Since 1931, over a dozen new Commonwealth realms have been created, all of which now hold the same powers as the United Kingdom, Canada, Australia, and New Zealand over matters of change to the monarchy, though the Statute of Westminster is not part of their laws. Ireland and South Africa are now republics and Newfoundland is now part of Canada as a province.

Australia adopted sections 2 to 6 of the Statute of Westminster with the Statute of Westminster Adoption Act 1942, in order to clarify the validity of certain Australian legislation relating to the Second World War; the adoption was backdated to 3 September 1939, the date that Britain and Australia joined the war.

Adopting section 2 of the Statute clarified that the Commonwealth Parliament was able to legislate inconsistently with British legislation, adopting section 3 clarified that it could legislate with extraterritorial effect. Adopting section 4 clarified that Britain could legislate with effect on Australia as a whole only with Australia's request and consent.

Nonetheless, under section 9 of the Statute, on matters not within Commonwealth power Britain could still legislate with effect in all or any of the Australian states, without the agreement of the Commonwealth although only to the extent of "the constitutional practice existing before the commencement" of the statute. However, this capacity had never been used. In particular, it was not used to implement the result of the 1933 Western Australian secession referendum, as it did not have the support of the Australian government.
All British power to legislate with effect in Australia ended with the Australia Act 1986, the British version of which says that it was passed with the request and consent of the Australian Parliament, which had obtained the concurrence of the Parliaments of the Australian states.

This Statute limited the legislative authority of the British parliament over Canada, effectively giving the country legal autonomy as a self-governing Dominion, though the British Parliament retained the power to amend Canada's constitution at the request of the Parliament of Canada. That authority remained in effect until the Constitution Act, 1982, which transferred it to Canada, the final step to achieving full sovereignty.
The British North America Acts—the written elements (in 1931) of the Canadian constitution—were excluded from the application of the statute because of disagreements between the Canadian provinces and the federal government over how the British North America Acts could be otherwise amended. These disagreements were resolved only in time for the passage of the Canada Act 1982, thus completing the patriation of the Canadian constitution to Canada. At that time, the Canadian parliament also repealed sections 4 and 7(1) of the Statute of Westminster. The Statute of Westminster remains a part of the constitution of Canada by virtue of section 52(2)(b) of the Constitution Act, 1982.

As a consequence of the Statute's adoption, the Parliament of Canada gained the ability to abolish appeals to the Judicial Committee of the Privy Council. Criminal appeals were abolished in 1933, while civil appeals continued until 1949. The passage of the Statute of Westminster meant that changes in British legislation governing the succession to the throne no longer automatically applied to Canada.

The Irish Free State never formally adopted the Statute of Westminster, its Executive Council (cabinet) taking the view that the Anglo-Irish Treaty of 1921 had already ended Westminster's right to legislate for the Irish Free State. The Free State's constitution gave the Oireachtas "sole and exclusive power of making laws". Hence, even before 1931, the Irish Free State did not arrest British Army and Royal Air Force deserters on its territory, even though the UK believed post-1922 British laws gave the Free State's Garda Síochána the power to do so. The UK's Irish Free State Constitution Act 1922 said, however, " in the [Free State] Constitution shall be construed as prejudicing the power of [the British] Parliament to make laws affecting the Irish Free State in any case where, in accordance with constitutional practice, Parliament would make laws affecting other self-governing Dominions".
Motions of approval of the Report of the Commonwealth Conference had been passed by the Dáil and Seanad in May 1931 and the final form of the Statute of Westminster included the Irish Free State among the Dominions the British Parliament could not legislate for without the Dominion's request and consent. Originally, the UK government had wanted to exclude from the Statute of Westminster the legislation underpinning the 1921 treaty, from which the Free State's constitution had emerged. Executive Council President (Prime Minister) W. T. Cosgrave objected, although he promised that the Executive Council would not amend the legislation unilaterally. The other Dominions backed Cosgrave and, when an amendment to similar effect was proposed at Westminster by John Gretton, parliament duly voted it down. When the Statute became law in the UK, Patrick McGilligan, the Free State Minister for External Affairs, stated: "It is a solemn declaration by the British people through their representatives in Parliament that the powers inherent in the Treaty position are what we have proclaimed them to be for the last ten years." He went on to present the Statute as largely the fruit of the Free State's efforts to secure for the other Dominions the same benefits it already enjoyed under the treaty. The Statute of Westminster had the effect of making the Irish Free State the first internationally recognised independent Irish state.

After Éamon de Valera led Fianna Fáil to victory in the Free State election of 1932, he began removing the monarchical elements of the Constitution, beginning with the Oath of Allegiance. De Valera initially considered invoking the Statute of Westminster in making these changes, but John J. Hearne advised him not to. Abolishing the Oath of Allegiance in effect abrogated the 1921 treaty. Generally, the British thought that this was morally objectionable but legally permitted by the Statute of Westminster. Robert Lyon Moore, a Southern Unionist from County Donegal, challenged the legality of the abolition in the Irish Free State's courts and then appealed to the Judicial Committee of the Privy Council (JCPC) in London. However, the Free State had also abolished the right of appeal to the JCPC. In 1935, the JCPC ruled that both abolitions were valid under the Statute of Westminster. The Free State, which in 1937 was renamed "Ireland", left the Commonwealth in 1949 upon the coming into force of its Republic of Ireland Act.

The Parliament of New Zealand adopted the Statute of Westminster by passing its Statute of Westminster Adoption Act 1947 in November 1947. The New Zealand Constitution Amendment Act, passed the same year, empowered the New Zealand Parliament to change the constitution, but did not remove the ability of the British Parliament to legislate regarding the New Zealand constitution. The remaining role of the British Parliament was removed by the New Zealand Constitution Act 1986 and the Statute of Westminster was repealed in its entirety.

The Dominion of Newfoundland never adopted the Statute of Westminster, especially because of financial troubles and corruption there. By request of the Dominion's government, the United Kingdom established the Commission of Government in 1934, resuming direct rule of Newfoundland. That arrangement remained until Newfoundland became a province of Canada in 1949 following referendums on the issue in 1948.

Although the Union of South Africa was not among the Dominions that needed to adopt the Statute of Westminster for it to take effect, two laws—the Status of the Union Act, 1934, and the Royal Executive Functions and Seals Act of 1934—were passed to confirm South Africa's status as a sovereign state.

The preamble to the Statute of Westminster sets out conventions which affect attempts to change the rules of succession to the Crown. The second paragraph of the preamble to the Statute reads:

This means, for example, that any change in any realm to the Act of Settlement's provisions barring Roman Catholics from the throne would require the unanimous assent of the Parliaments of all the other Commonwealth realms if the shared aspect of the Crown is to be retained. The preamble does not itself contain enforceable provisions, it merely expresses a constitutional convention, albeit one fundamental to the basis of the relationship between the Commonwealth realms. (As sovereign nations, each is free to withdraw from the arrangement, using their respective process for constitutional amendment.) Additionally, per section 4, if a realm wished for a British act amending the Act of Settlement in the UK to become part of that realm's laws, thereby amending the Act of Settlement in that realm, it would have to request and consent to the British act and the British act would have to state that such request and consent had been given. Section 4 of the Statute of Westminster has been repealed in a number of realms, however, and replaced by other constitutional clauses absolutely disallowing the British parliament from legislating for those realms.

This has raised some logistical concerns, as it would mean multiple Parliaments would all have to assent to any future changes in any realm to its line of succession, as with the Perth Agreement's proposals to abolish male-preference primogeniture.

During the abdication crisis in 1936, British Prime Minister Stanley Baldwin consulted the Commonwealth prime ministers at the request of King Edward VIII. The King wanted to marry Wallis Simpson, whom Baldwin and other British politicians considered unacceptable as Queen, as she was an American divorcée. Baldwin was able to get the then five Dominion prime ministers to agree with this and thus register their official disapproval at the King's planned marriage. The King later requested the Commonwealth prime ministers be consulted on a compromise plan, in which he would wed Simpson under a morganatic marriage pursuant to which she would not become queen. Under Baldwin's pressure, this plan was also rejected by the Dominions. All of these negotiations occurred at a diplomatic level and never went to the Commonwealth parliaments. However, the enabling legislation that allowed for the actual abdication (His Majesty's Declaration of Abdication Act 1936) did require the assent of each Dominion Parliament to be passed and the request and consent of the Dominion governments so as to allow it to be part of the law of each Dominion. For expediency and to avoid embarrassment, the British government had suggested the Dominion governments regard whoever is monarch of the UK to automatically be their monarch. However, the Dominions rejected this; Prime Minister of Canada William Lyon Mackenzie King pointed out that the Statute of Westminster required Canada's request and consent to any legislation passed by the British Parliament before it could become part of Canada's laws and affect the line of succession in Canada. The text of the British act states that Canada requested and consented (the only Dominion to formally do both) to the act applying in Canada under the Statute of Westminster, while Australia, New Zealand, and the Union of South Africa simply assented.

In February 1937, the South African Parliament formally gave its assent by passing His Majesty King Edward the Eighth's Abdication Act, 1937, which declared that Edward VIII had abdicated on 10 December 1936; that he and his descendants, if any, would have no right of succession to the throne; and that the Royal Marriages Act 1772 would not apply to him or his descendants, if any. The move was largely done for symbolic purposes, in an attempt by Prime Minister J. B. M. Hertzog to assert South Africa's independence from Britain. In Canada, the federal parliament passed the Succession to the Throne Act 1937, to assent to His Majesty's Declaration of Abdication Act and ratify the government's request and consent to it. In the Irish Free State, Prime Minister Éamon de Valera used the departure of Edward VIII as an opportunity to remove all explicit mention of the monarch from the Constitution of the Irish Free State, through the Constitution (Amendment No. 27) Act 1936, passed on 11 December 1936. The following day, the External Relations Act provided for the king to carry out certain diplomatic functions, if authorised by law; the same Act also brought Edward VIII's Instrument of Abdication into effect for the purposes of Irish law (s. 3(2)). A new Constitution of Ireland, with a president, was approved by Irish voters in 1937, with the Irish Free State becoming simply "Ireland", or, in the Irish language, "Éire". However, the head of state of Ireland remained unclear until 1949, when Ireland unambiguously became a republic outside the Commonwealth of Nations by enacting the Republic of Ireland Act 1948.

In some countries where the Statute of Westminster forms a part of the constitution, the anniversary of the date of the passage of the original British statute is commemorated as Statute of Westminster Day. In Canada, it is mandated that, on 11 December, the Royal Union Flag (as the Union Jack is called by law in Canada) is to be flown at properties owned by the federal Crown, where the requisite second flag pole is available.






</doc>
<doc id="29265" url="https://en.wikipedia.org/wiki?curid=29265" title="Serbia">
Serbia

Serbia (, ), officially the Republic of Serbia (, ), is a landlocked country situated at the crossroads of Central and Southeast Europe in the southern Pannonian Plain and the central Balkans. It borders Hungary to the north, Romania to the northeast, Bulgaria to the southeast, North Macedonia to the south, Croatia and Bosnia and Herzegovina to the west, and Montenegro to the southwest. The country claims a border with Albania through the disputed territory of Kosovo. Serbia's population numbers approximately seven million without Kosovo or 8.8 million if the territory is included. Its capital, Belgrade, ranks among the largest and oldest citiеs in southeastern Europe. 

Inhabited since the Paleolithic Age, the territory of modern-day Serbia faced Slavic migrations to Southeastern Europe in the 6th century, establishing several regional states in the early Middle Ages at times recognised as tributaries to the Byzantine, Frankish and Hungarian kingdoms. The Serbian Kingdom obtained recognition by the Holy See and Constantinople in 1217, reaching its territorial apex in 1346 as the relatively short-lived Serbian Empire. By the mid-16th century, the Ottomans annexed the entirety of modern-day Serbia; their rule was at times interrupted by the Habsburg Empire, which began expanding towards Central Serbia from the end of the 17th century while maintaining a foothold in Vojvodina. In the early 19th century, the Serbian Revolution established the nation-state as the region's first constitutional monarchy, which subsequently expanded its territory. Following disastrous casualties in World War I, and the subsequent unification of the former Habsburg crownland of Vojvodina (and other lands) with Serbia, the country co-founded Yugoslavia with other South Slavic nations, which would exist in various political formations until the Yugoslav Wars of the 1990s. During the breakup of Yugoslavia, Serbia formed a union with Montenegro, which was peacefully dissolved in 2006, restoring Serbia's independence as a sovereign state for the first time since 1918. In 2008, the parliament of the province of Kosovo unilaterally declared independence, with mixed responses from the international community. Serbia is one of the European countries with high numbers of registered national minorities, while the Autonomous Province of Vojvodina is recognizable for its multi-ethnic and multi-cultural identity.

A unitary parliamentary constitutional republic, Serbia is a member of the UN, CoE, OSCE, PfP, BSEC, CEFTA, and is acceding to the WTO. Since 2014, the country has been negotiating its EU accession with the perspective of joining the European Union by 2025. Since 2007, Serbia formally adheres to the policy of military neutrality. The country provides social security, universal health care system, and a free primary and secondary education to its citizens. An upper-middle-income economy with a dominant service sector, the country ranks relatively high on the Human Development Index (63rd) and Social Progress Index (45th) as well as the Global Peace Index (50th).

The origin of the name "Serbia" is unclear. Historically, authors have mentioned the Serbs ( / Срби) and the Sorbs of eastern Germany (Upper Sorbian: "Serbja"; Lower Sorbian: "Serby") in a variety of ways: Surbii, Suurbi, Serbloi, Zeriuani, Sorabi, Surben, Sarbi, Serbii, Serboi, Zirbi, Surbi, Sorben, etc. These authors used these names to refer to Serbs and Sorbs in areas where their historical (or current) presence was/is not disputed (notably in the Balkans and Lusatia). However, there are also sources that mention same or similar names in other parts of the World (most notably in the Asiatic Sarmatia in the Caucasus).
The Proto-Slavic root word *sъrbъ has been variously connected with Russian "paserb" (пасерб, "stepson"), Ukrainian "pryserbytysia" (присербитися, "join in"), Old Indic "sarbh-" ("fight, cut, kill"), Latin "sero" ("make up, constitute"), and Greek "siro" (ειρω, "repeat"). Polish linguist Stanisław Rospond (1906–1982) derived the Serbian language ethnonym "Srb" from "srbati" (cf. "sorbo", "absorbo"). Sorbian scholar H. Schuster-Šewc suggested a connection with the Proto-Slavic verb for "to slurp" *sьrb-, with cognates such as "сёрбать" (Russian), "сьорбати" (Ukrainian), "сёрбаць" (Belarusian), "srbati" (Slovak), "сърбам" (Bulgarian) and "серебати" (Old Russian).

In his book, De Administrando Imperio, Constantine VII Porphyrogenitus suggests that the Serbs originated from White Serbia on the far side of Turkey. He believed that the people split in two, with the half that became known as the Serbs coming down to settle Byzantine land. In this line of thinking, Serb is derived from the word Surbi which was used to describe people of the proto-country. 

From 1945 to 1963, the official name for Serbia was the People's Republic of Serbia, later renamed the Socialist Republic of Serbia from 1963 to 1990. Since 1990, the official name of the country has been the Republic of Serbia. From 1992 to 2006, however, the official names of the country Serbia was a part of were the Federal Republic of Yugoslavia and then the State Union of Serbia and Montenegro.

Archaeological evidence of Paleolithic settlements on the territory of present-day Serbia is scarce. A fragment of a human jaw was found in Sićevo (Mala Balanica) and is believed to be up to 525,000–397,000 years old.
Approximately around 6,500 years BC, during the Neolithic, the Starčevo, and Vinča cultures existed in the region of modern-day Belgrade. They dominated much of Southeastern Europe, (as well as parts of Central Europe and Asia Minor). Several important archaeological sites from this era, including Lepenski Vir and Vinča-Belo Brdo, still exist near the banks of the Danube.

During the Iron Age, local tribes of Triballi, Dardani, and Autariatae were encountered by the Ancient Greeks during their cultural and political expansion into the region, from the 5th up to the 2nd century BC. The Celtic tribe of Scordisci settled throughout the area in the 3rd century BC. It formed a tribal state, building several fortifications, including their capital at Singidunum (present-day Belgrade) and Naissos (present-day Niš).

The Romans conquered much of the territory in the 2nd century BC. In 167 BC the Roman province of Illyricum was established; the remainder was conquered around 75 BC, forming the Roman province of Moesia Superior; the modern-day Srem region was conquered in 9 BC; and Bačka and Banat in 106 AD after the Dacian Wars. As a result of this, contemporary Serbia extends fully or partially over several former Roman provinces, including Moesia, Pannonia, Praevalitana, Dalmatia, Dacia, and Macedonia.

The chief towns of Upper Moesia (and broader) were: Singidunum (Belgrade), Viminacium (now Old Kostolac), Remesiana (now Bela Palanka), Naissos (Niš), and Sirmium (now Sremska Mitrovica), the latter of which served as a Roman capital during the Tetrarchy. Seventeen Roman Emperors were born in the area of modern-day Serbia, second only to contemporary Italy. The most famous of these was Constantine the Great, the first Christian Emperor, who issued an edict ordering religious tolerance throughout the Empire.

When the Roman Empire was divided in 395, most of Serbia remained under the Eastern Roman Empire. At the same time, its northwestern parts were included in the Western Roman Empire. By the 6th century, South Slavs migrated into the European provinces of the Byzantine Empire in large numbers. They merged with the local Romanised population that was gradually assimilated.

White Serbs, an early Slavic tribe from White Serbia first settled in an area near Thessaloniki on the Balkans and in the 6th and early 7th century, established the Serbian Principality by the 8th century. It was said in 822 that the Serbs inhabited the more significant part of Roman Dalmatia, their territory spanning what is today southwestern Serbia and parts of neighbouring countries. Meanwhile, the Byzantine Empire and the Bulgarian Empire held other parts of the territory. The Serbian rulers adopted Christianity in ca. 870, and by the mid-10th-century the Serbian state stretched to the Adriatic Sea by the Neretva, the Sava, the Morava, and Skadar. Between 1166 and 1371 Serbia was ruled by the Nemanjić dynasty (whose legacy is especially cherished), under whom the state was elevated to a kingdom (and briefly an empire) and Serbian bishopric to an autocephalous archbishopric (through the effort of Sava, the country's patron saint). Monuments of the Nemanjić period survive in many monasteries (several being World Heritage sites) and fortifications. During these centuries the Serbian state (and influence) expanded significantly. The northern part, Vojvodina, was ruled by the Kingdom of Hungary. The period known as the Fall of the Serbian Empire saw the once-powerful state fragmented into duchies, culminating in the Battle of Kosovo (1389) against the rising Ottoman Empire. The Ottomans finally conquered the Serbian Despotate in 1459. The Ottoman threat and eventual conquest saw massive migrations of Serbs to the west and north.

In all Serbian lands conquered by the Ottomans, the native nobility was eliminated and the peasantry was enserfed to Ottoman rulers, while much of the clergy fled or were confined to the isolated monasteries. Under the Ottoman system, Serbs, as Christians, were considered an inferior class of people and subjected to heavy taxes, and a portion of the Serbian population experienced Islamization. Many Serbs were recruited during the devshirme system, a form of slavery in the Ottoman Empire, in which boys from Balkan Christian families were forcibly converted to Islam and trained for infantry units of the Ottoman army known as the Janissaries. The Serbian Patriarchate of Peć was extinguished in 1463, but reestablished in 1557, providing for limited continuation of Serbian cultural traditions within the Ottoman Empire, under the Millet system.
After the loss of statehood to the Ottoman Empire, Serbian resistance continued in northern regions (modern Vojvodina), under titular despots (until 1537), and popular leaders like Jovan Nenad (1526–1527). From 1521 to 1552, Ottomans conquered Belgrade and regions of Syrmia, Bačka, and Banat. Continuing wars and various rebellions constantly challenged Ottoman rule. One of the most significant was the Banat Uprising in 1594 and 1595, which was part of the Long War (1593–1606) between the Habsburgs and the Ottomans. The area of modern Vojvodina endured a century-long Ottoman occupation before being ceded to the Habsburg Empire, partially by the Treaty of Karlovci (1699), and fully by the Treaty of Požarevac (1718).

As the Great Serb Migrations depopulated most of southern Serbia, the Serbs sought refuge across the Danube River in Vojvodina to the north and the Military Frontier in the west, where they were granted rights by the Austrian crown under measures such as the "Statuta Wallachorum" of 1630. Much of central Serbia switched from Ottoman rule to Habsburg control (1686–91) during the Habsburg-Ottoman war (1683-1699). Following several petitions, Emperor Leopold I formally granted Serbs who wished to settle in the northern regions the right to their autonomous crown land. The ecclesiastical centre of the Serbs also moved northwards, to the Metropolitanate of Karlovci, and the Serbian Patriarchate of Peć was once-again abolished by the Ottomans in 1766. 

In 1718–39, the Habsburg Monarchy occupied much of Central Serbia and established the "Kingdom of Serbia" (1718–1739). Those gains were lost by the Treaty of Belgrade in 1739, when the Ottomans retook the region. Apart from territory of modern Vojvodina which remained under the Habsburg Empire, central regions of Serbia were occupied once again by the Habsburgs in 1788–1792.

The Serbian Revolution for independence from the Ottoman Empire lasted eleven years, from 1804 until 1815. The revolution comprised two separate uprisings which gained autonomy from the Ottoman Empire (1830) that eventually evolved towards full independence (1878). During the First Serbian Uprising (1804–1813), led by vožd Karađorđe Petrović, Serbia was independent for almost a decade before the Ottoman army was able to reoccupy the country. Shortly after this, the Second Serbian Uprising began in 1815. Led by Miloš Obrenović, it ended with a compromise between Serbian revolutionaries and Ottoman authorities. Likewise, Serbia was one of the first nations in the Balkans to abolish feudalism. The Akkerman Convention in 1826, the Treaty of Adrianople in 1829 and finally, the Hatt-i Sharif, recognised the suzerainty of Serbia. The First Serbian Constitution was adopted on 15 February 1835 (the anniversary of the outbreak of the First Serbian Uprising), making the country one of the first to adopt a democratic constitution in Europe. 15 February is now commemorated as Statehood Day, a public holiday.

Following the clashes between the Ottoman army and Serbs in Belgrade in 1862, and under pressure from the Great Powers, by 1867 the last Turkish soldiers left the Principality, making the country "de facto" independent. By enacting a new constitution in 1869, without consulting the Porte, Serbian diplomats confirmed the "de facto" independence of the country. In 1876, Serbia declared war on the Ottoman Empire, siding with the ongoing Christian uprisings in Bosnia-Herzegovina and Bulgaria.

The formal independence of the country was internationally recognised at the Congress of Berlin in 1878, which ended the Russo-Turkish War; this treaty, however, prohibited Serbia from uniting with other Serbian regions by placing Bosnia and Herzegovina under Austro-Hungarian occupation, alongside the occupation of the region of Raška. From 1815 to 1903, the Principality of Serbia was ruled by the House of Obrenović, save for the rule of Prince Aleksandar Karađorđević between 1842 and 1858. In 1882, Principality of Serbia became the Kingdom of Serbia, ruled by King Milan I. The House of Karađorđević, descendants of the revolutionary leader Karađorđe Petrović, assumed power in 1903 following the May Overthrow. 
In the north, the 1848 revolution in Austria led to the establishment of the autonomous territory of Serbian Vojvodina; by 1849, the region was transformed into the Voivodeship of Serbia and Banat of Temeschwar.

In the course of the First Balkan War in 1912, the Balkan League defeated the Ottoman Empire and captured its European territories, which enabled territorial expansion of the Kingdom of Serbia into regions of Raška, Kosovo, Metohija, and Vardarian Macedonia. The Second Balkan War soon ensued when Bulgaria turned on its former allies, but was defeated, resulting in the Treaty of Bucharest. In two years, Serbia enlarged its territory and its population by 50%; it also suffered high casualties on the eve of World War I, with more than 36,000 dead. Austria-Hungary became wary of the rising regional power on its borders and its potential to become an anchor for unification of Serbs and other South Slavs, and the relationship between the two countries became tense.
The assassination of Archduke Franz Ferdinand of Austria on 28 June 1914 in Sarajevo by Gavrilo Princip, a member of the Young Bosnia organisation, led to Austria-Hungary declaring war on Serbia, on 28 July. Local war escalated, when Germany declared war on Russia, and invaded France and Belgium, thus drawing Great Britain into the conflict, that became the First World War. Serbia won the first major battles of World War I, including the Battle of Cer, and the Battle of Kolubara, marking the first Allied victories against the Central Powers in World War I.

Despite initial success, it was eventually overpowered by the Central Powers in 1915. Most of its army and some people retreated through Albania to Greece and Corfu, suffering immense losses on the way. Serbia was occupied by the Central Powers. After the Central Powers military situation on other fronts worsened, the remains of the Serb army returned east and lead a final breakthrough through enemy lines on 15 September 1918, liberating Serbia and defeating Bulgaria and Austria-Hungary. Serbia, with its campaign, was a major Balkan Entente Power which contributed significantly to the Allied victory in the Balkans in November 1918, especially by helping France force Bulgaria's capitulation.

Serbia's casualties accounted for 8% of the total Entente military deaths; 58% (243,600) soldiers of the Serbian army perished in the war. The total number of casualties is placed around 700,000, more than 16% of Serbia's prewar size, and a majority (57%) of its overall male population. Serbia suffered the biggest casualty rate in World War I. 

As the Austro-Hungarian Empire collapsed, the territory of Syrmia united with Serbia on 24 November 1918. Just a day later on November 25, 1918 Grand National Assembly of Serbs, Bunjevci and other Slavs in Banat, Bačka and Baranja declared the unification of Banat, Bačka, and Baranja to the Kingdom of Serbia.

On 26 November 1918, the Podgorica Assembly deposed the House of Petrović-Njegoš and united Montenegro with Serbia. On 1 December 1918, in Belgrade, Serbian Prince Regent Alexander Karađorđević proclaimed the Kingdom of the Serbs, Croats, and Slovenes, under King Peter I of Serbia.
King Peter was succeeded by his son, Alexander, in August 1921. Serb centralists and Croat autonomists clashed in the parliament, and most governments were fragile and short-lived. Nikola Pašić, a conservative prime minister, headed or dominated most governments until his death. King Alexander established a dictatorship in 1929 with the aim of establishing the Yugoslav ideology and single Yugoslav nation, changed the name of the country to Yugoslavia and changed the internal divisions from the 33 oblasts to nine new banovinas. The effect of Alexander's dictatorship was to further alienate the non-Serbs living in Yugoslavia from the idea of unity.

Alexander was assassinated in Marseille, during an official visit in 1934 by Vlado Chernozemski, member of the IMRO. Alexander was succeeded by his eleven-year-old son Peter II and a regency council was headed by his cousin, Prince Paul. In August 1939 the Cvetković–Maček Agreement established an autonomous Banate of Croatia as a solution to Croatian concerns.

In 1941, in spite of Yugoslav attempts to remain neutral in the war, the Axis powers invaded Yugoslavia. The territory of modern Serbia was divided between Hungary, Bulgaria, the Independent State of Croatia and Italy (Greater Albania and Montenegro), while the remaining part of the occupied Serbia was placed under the military administration of the Nazi Germany, with Serbian puppet governments led by Milan Aćimović and Milan Nedić assisted by Dimitrije Ljotić's fascist organization Yugoslav National Movement (Zbor).
The Yugoslav territory was the scene of a civil war between royalist Chetniks commanded by Draža Mihailović and communist partisans commanded by Josip Broz Tito. Axis auxiliary units of the Serbian Volunteer Corps and the Serbian State Guard fought against both of these forces. Siege of Kraljevo was a major battle of the Uprising in Serbia, led by Chetnik forces against the Nazis. Several days after the battle began the German forces committed a massacre of approximately 2,000 civilians in an event known as the Kraljevo massacre, in a reprisal for the attack. Draginac and Loznica massacre of 2,950 villagers in Western Serbia in 1941 was the first large execution of civilians in occupied Serbia by Germans, with Kragujevac massacre and Novi Sad Raid of Jews and Serbs by Hungarian fascists being the most notorious, with over 3,000 victims in each case. After one year of occupation, around 16,000 Serbian Jews were murdered in the area, or around 90% of its pre-war Jewish population during The Holocaust in Serbia. Many concentration camps were established across the area. Banjica concentration camp was the largest concentration camp and jointly run by the German army and Nedić's regime, with primary victims being Serbian Jews, Roma, and Serb political prisoners.
During this period, hundreds of thousands of ethnic Serbs fled the Axis puppet state known as the Independent State of Croatia and sought refuge in German-occupied Serbia, seeking to escape the large-scale persecution and genocide of Serbs, Jews, and Roma being committed by the Ustaše regime.

According to Josip Broz Tito himself, Serbs made up the vast majority of Anti-fascist fighters and Yugoslav Partisans for the whole course of World War II. The Republic of Užice was a short-lived liberated territory established by the Partisans and the first liberated territory in World War II Europe, organised as a military mini-state that existed in the autumn of 1941 in the west of occupied Serbia. By late 1944, the Belgrade Offensive swung in favour of the partisans in the civil war; the partisans subsequently gained control of Yugoslavia. Following the Belgrade Offensive, the Syrmian Front was the last major military action of World War II in Serbia. A study by Vladimir Žerjavić estimates total war related deaths in Yugoslavia at 1,027,000, including 273,000 in Serbia. The Ustaše regime committed the Genocide of Serbs and systematically murdered approximately 300,000 to 500,000 Serbs.

The victory of the Communist Partisans resulted in the abolition of the monarchy and a subsequent constitutional referendum. A one-party state was soon established in Yugoslavia by the Communist Party of Yugoslavia. It is claimed between 60,000 and 70,000 people died in Serbia during the 1944–45 communist takeover and purge. All opposition was suppressed and people deemed to be promoting opposition to socialism or promoting separatism were imprisoned or executed for sedition. Serbia became a constituent republic within the SFRY known as the Socialist Republic of Serbia, and had a republic-branch of the federal communist party, the League of Communists of Serbia.

Serbia's most powerful and influential politician in Tito-era Yugoslavia was Aleksandar Ranković, one of the "big four" Yugoslav leaders, alongside Tito, Edvard Kardelj, and Milovan Đilas. Ranković was later removed from the office because of the disagreements regarding Kosovo's nomenklatura and the unity of Serbia. Ranković's dismissal was highly unpopular among Serbs. Pro-decentralisation reformers in Yugoslavia succeeded in the late 1960s in attaining substantial decentralisation of powers, creating substantial autonomy in Kosovo and Vojvodina, and recognising a distinctive "Muslim" nationality. As a result of these reforms, there was a massive overhaul of Kosovo's nomenklatura and police, that shifted from being Serb-dominated to ethnic Albanian-dominated through firing Serbs on a large scale. Further concessions were made to the ethnic Albanians of Kosovo in response to unrest, including the creation of the University of Pristina as an Albanian language institution. These changes created widespread fear among Serbs of being treated as second-class citizens.

Belgrade, the capital of SFR Yugoslavia and SR Serbia, hosted the first Non-Aligned Movement Summit in September 1961, as well as the first major gathering of the Organization for Security and Co-operation in Europe (OSCE) with the aim of implementing the Helsinki Accords from October 1977 to March 1978. The 1972 smallpox outbreak in SAP Kosovo and other parts of SR Serbia was the last major outbreak of smallpox in Europe since World War II.

In 1989, Slobodan Milošević rose to power in Serbia. Milošević promised a reduction of powers for the autonomous provinces of Kosovo and Vojvodina, where his allies subsequently took over power, during the Anti-bureaucratic revolution. This ignited tensions between the communist leadership of the other republics of Yugoslavia, and awoke ethnic nationalism across Yugoslavia that eventually resulted in its breakup, with Slovenia, Croatia, Bosnia and Herzegovina, and Macedonia declaring independence during 1991 and 1992. Serbia and Montenegro remained together as the Federal Republic of Yugoslavia (FRY). However, according to the Badinter Commission, the country was not legally considered a continuation of the former SFRY, but a new state.
Fueled by ethnic tensions, the Yugoslav Wars (1991–2001) erupted, with the most severe conflicts taking place in Croatia and Bosnia, where the large ethnic Serb communities opposed independence from Yugoslavia. The FRY remained outside the conflicts, but provided logistic, military and financial support to Serb forces in the wars. In response, the UN imposed sanctions against Serbia which led to political isolation and the collapse of the economy (GDP decreased from $24 billion in 1990 to under $10 billion in 1993). Following the rise of nationalism and political tensions after Slobodan Milošević came to power, numerous anti-war movements developed in Serbia and many anti-war protests were held in Belgrade.

Multi-party democracy was introduced in Serbia in 1990, officially dismantling the one-party system. Critics of Milošević stated that the government continued to be authoritarian despite constitutional changes, as Milošević maintained strong political influence over the state media and security apparatus. When the ruling Socialist Party of Serbia refused to accept its defeat in municipal elections in 1996, Serbians engaged in large protests against the government.
In 1998, continued clashes between the Albanian guerilla Kosovo Liberation Army and Yugoslav security forces led to the short Kosovo War (1998–99), in which NATO intervened, leading to the withdrawal of Serbian forces and the establishment of UN administration in the province. After the Yugoslav Wars, Serbia became home to highest number of refugees and internally displaced persons in Europe.

After presidential elections in September 2000, opposition parties accused Milošević of electoral fraud. A campaign of civil resistance followed, led by the Democratic Opposition of Serbia (DOS), a broad coalition of anti-Milošević parties. This culminated on 5 October when half a million people from all over the country congregated in Belgrade, compelling Milošević to concede defeat. The fall of Milošević ended Yugoslavia's international isolation. Milošević was sent to the International Criminal Tribunal for the former Yugoslavia. The DOS announced that FR Yugoslavia would seek to join the European Union. In 2003, the Federal Republic of Yugoslavia was renamed Serbia and Montenegro; the EU opened negotiations with the country for the Stabilisation and Association Agreement. Serbia's political climate remained tense and in 2003, the Prime Minister Zoran Đinđić was assassinated as result of a plot originating from circles of organised crime and former security officials.

On 21 May 2006, Montenegro held a referendum to determine whether to end its union with Serbia. The results showed 55.4% of voters in favour of independence, which was just above the 55% required by the referendum. On 5 June 2006, the National Assembly of Serbia declared Serbia to be the legal successor to the former state union. The Assembly of Kosovo unilaterally declared independence from Serbia on 17 February 2008. Serbia immediately condemned the declaration and continues to deny any statehood to Kosovo. The declaration has sparked varied responses from the international community, some welcoming it, while others condemned the unilateral move. Status-neutral talks between Serbia and Kosovo-Albanian authorities are held in Brussels, mediated by the EU.

In April 2008 Serbia was invited to join the Intensified Dialogue programme with NATO, despite the diplomatic rift with the alliance over Kosovo. Serbia officially applied for membership in the European Union on 22 December 2009, and received candidate status on 1 March 2012, following a delay in December 2011. Following a positive recommendation of the European Commission and European Council in June 2013, negotiations to join the EU commenced in January 2014.

Since Aleksandar Vučić came to power, Serbia has suffered from democratic backsliding into authoritarianism, followed by a decline in media freedom and civil liberties. Massive anti-government protests began in 2018 and continued into 2020, making them one of Europe's longest-running protests. After the COVID-19 pandemic spread to Serbia in March 2020, a state of emergency was declared and a curfew was introduced for the first time in Serbia since World War II.

Situated at the crossroads between Central and Southern Europe, Serbia is located in the Balkan peninsula and the Pannonian Plain. Serbia lies between latitudes 41° and 47° N, and longitudes 18° and 23° E. The country covers a total of 88,361 km (including Kosovo), which places it at 113th place in the world; with Kosovo excluded, the total area is 77,474 km, which would make it 117th. Its total border length amounts to 2,027 km (Albania 115 km, Bosnia and Herzegovina 302 km, Bulgaria 318 km, Croatia 241 km, Hungary 151 km, North Macedonia 221 km, Montenegro 203 km and Romania 476 km). All of Kosovo's border with Albania (115 km), North Macedonia (159 km) and Montenegro (79 km) are under control of the Kosovo border police. Serbia treats the 352 km long border between Kosovo and rest of Serbia as an "administrative line"; it is under shared control of Kosovo border police and Serbian police forces, and there are 11 crossing points.
The Pannonian Plain covers the northern third of the country (Vojvodina and Mačva) while the easternmost tip of Serbia extends into the Wallachian Plain. 
The terrain of the central part of the country, with the region of Šumadija at its heart, consists chiefly of hills traversed by rivers. Mountains dominate the southern third of Serbia. Dinaric Alps stretch in the west and the southwest, following the flow of the rivers Drina and Ibar. The Carpathian Mountains and Balkan Mountains stretch in a north–south direction in eastern Serbia.

Ancient mountains in the southeast corner of the country belong to the Rilo-Rhodope Mountain system. Elevation ranges from the Midžor peak of the Balkan Mountains at (the highest peak in Serbia, excluding Kosovo) to the lowest point of just near the Danube river at Prahovo. The largest lake is Đerdap Lake (163 square kilometres) and the longest river passing through Serbia is the Danube (587.35 kilometres).

The climate of Serbia is under the influences of the landmass of Eurasia and the Atlantic Ocean and Mediterranean Sea. With mean January temperatures around , and mean July temperatures of , it can be classified as a warm-humid continental or humid subtropical climate. In the north, the climate is more continental, with cold winters, and hot, humid summers along with well-distributed rainfall patterns. In the south, summers and autumns are drier, and winters are relatively cold, with heavy inland snowfall in the mountains.

Differences in elevation, proximity to the Adriatic Sea and large river basins, as well as exposure to the winds account for climate variations. Southern Serbia is subject to Mediterranean influences. The Dinaric Alps and other mountain ranges contribute to the cooling of most of the warm air masses. Winters are quite harsh in the Pešter plateau, because of the mountains which encircle it. One of the climatic features of Serbia is Košava, a cold and very squally southeastern wind which starts in the Carpathian Mountains and follows the Danube northwest through the Iron Gate where it gains a jet effect and continues to Belgrade and can spread as far south as Niš.
The average annual air temperature for the period 1961–1990 for the area with an altitude of up to is . The areas with an altitude of have an average annual temperature of around , and over of altitude around . The lowest recorded temperature in Serbia was on 13 January 1985, Karajukića Bunari in Pešter, and the highest was , on 24 July 2007, recorded in Smederevska Palanka.

Serbia is one of few European countries with "very high risk" exposure to natural hazards (earthquakes, storms, floods, droughts). It is estimated that potential floods, particularly in areas of Central Serbia, threaten over 500 larger settlements and an area of 16,000 square kilometres. The most disastrous were the floods in May 2014, when 57 people died and a damage of over a 1.5 billion euro was inflicted.

Almost all of Serbia's rivers drain to the Black Sea, by way of the Danube river. The Danube, the second largest European river, passes through Serbia with 588 kilometres (21% of its overall length) and represents the major source of fresh water. It is joined by its biggest tributaries, the Great Morava (longest river entirely in Serbia with 493 km of length), Sava and Tisza rivers. One notable exception is the Pčinja which flows into the Aegean. Drina river forms the natural border between Bosnia and Herzegovina and Serbia, and represents the main kayaking and rafting attraction in both countries.
Due to configuration of the terrain, natural lakes are sparse and small; most of them are located in the lowlands of Vojvodina, like the aeolian lake Palić or numerous oxbow lakes along river flows (like Zasavica and Carska Bara). However, there are numerous artificial lakes, mostly due to hydroelectric dams, the biggest being Đerdap (Iron Gates) on the Danube with 163 km on the Serbian side (a total area of 253 km is shared with Romania); Perućac on the Drina, and Vlasina. The largest waterfall, Jelovarnik, located in Kopaonik, is 71 m high. Abundance of relatively unpolluted surface waters and numerous underground natural and mineral water sources of high water quality presents a chance for export and economy improvement; however, more extensive exploitation and production of bottled water began only recently.

With 29.1% of its territory covered by forest, Serbia is considered to be a middle-forested country, compared on a global scale to world forest coverage at 30%, and European average of 35%. The total forest area in Serbia is 2,252,000 ha (1,194,000 ha or 53% are state-owned, and 1,058,387 ha or 47% are privately owned) or 0.3 ha per inhabitant. 

The most common trees are oak, beech, pines, and firs. Serbia is a country of rich ecosystem and species diversity—covering only 1.9% of the whole European territory, Serbia is home to 39% of European vascular flora, 51% of European fish fauna, 40% of European reptiles and amphibian fauna, 74% of European bird fauna, and 67% European mammal fauna. Its abundance of mountains and rivers make it an ideal environment for a variety of animals, many of which are protected including wolves, lynx, bears, foxes, and stags. There are 17 snake species living all over the country, 8 of them are venomous. 
Mountain of Tara in western Serbia is one of the last regions in Europe where bears can still live in absolute freedom. Serbia is home home to about 380 species of birds. In Carska Bara, there are over 300 bird species on just a few square kilometres. Uvac Gorge is considered one of the last habitats of the Griffon vulture in Europe. In area around the city of Kikinda, in the northernmost part of the country, some 145 endangered long-eared owls are noted, making it the world's biggest settlement of these species. The country is considerably rich with threatened species of bats and butterflies as well.

There are 380 protected areas of Serbia, encompassing 4,947 square kilometres or 6.4% of the country. The "Spatial plan of the Republic of Serbia" states that the total protected area should be increased to 12% by 2021. Those protected areas include 5 national parks (Đerdap, Tara, Kopaonik, Fruška Gora and Šar Mountain), 15 nature parks, 15 "landscapes of outstanding features", 61 nature reserves, and 281 natural monuments.

Air pollution is a significant problem in Bor area, due to work of large copper mining and smelting complex, and Pančevo where oil and petrochemical industry is based. Some cities suffer from water supply problems, due to mismanagement and low investments in the past, as well as water pollution (like the pollution of the Ibar River from the Trepča zinc-lead combinate, affecting the city of Kraljevo, or the presence of natural arsenic in underground waters in Zrenjanin).

Poor waste management has been identified as one of the most important environmental problems in Serbia and the recycling is a fledgling activity, with only 15% of its waste being turned back for reuse. The 1999 NATO bombing caused serious damage to the environment, with several thousand tonnes of toxic chemicals stored in targeted factories and refineries released into the soil and water basins.

Serbia is a parliamentary republic, with the government divided into legislative, executive, and judiciary branches.
Serbia had one of the first modern constitutions in Europe, the 1835 Constitution (known as the Sretenje Constitution), which was at the time considered among the most progressive and liberal constitutions in Europe. Since then it has adopted 10 different constitutions. The current constitution was adopted in 2006 in the aftermath of Montenegro independence referendum which by consequence renewed the independence of Serbia itself. The Constitutional Court rules on matters regarding the Constitution.

The President of the Republic ("Predsednik Republike") is the head of state, is elected by popular vote to a five-year term and is limited by the Constitution to a maximum of two terms. In addition to being the commander in chief of the armed forces, the president has the procedural duty of appointing the prime minister with the consent of the parliament, and has some influence on foreign policy. Aleksandar Vučić of the Serbian Progressive Party is the current president following the 2017 presidential election. Seat of the presidency is Novi Dvor.

The Government ("Vlada") is composed of the prime minister and cabinet ministers. The Government is responsible for proposing legislation and a budget, executing the laws, and guiding the foreign and internal policies. The current prime minister is Ana Brnabić, nominated by the Serbian Progressive Party.

The National Assembly ("Narodna skupština") is a unicameral legislative body. The National Assembly has the power to enact laws, approve the budget, schedule presidential elections, select and dismiss the Prime Minister and other ministers, declare war, and ratify international treaties and agreements. It is composed of 250 proportionally elected members who serve four-year terms.

The largest political parties in Serbia are the centre-right Serbian Progressive Party, leftist Socialist Party of Serbia and far-right Serbian Radical Party.

Serbia is the fourth modern-day European country, after France, Austria and the Netherlands, to have a codified legal system.

The country has a three-tiered judicial system, made up of the Supreme Court of Cassation as the court of the last resort, Courts of Appeal as the appellate instance, and Basic and High courts as the general jurisdictions at first instance.

Courts of special jurisdictions are the Administrative Court, commercial courts (including the Commercial Court of Appeal at second instance) and misdemeanor courts (including High Misdemeanor Court at second instance). The judiciary is overseen by the Ministry of Justice. Serbia has a typical civil law legal system.

Law enforcement is the responsibility of the Serbian Police, which is subordinate to the Ministry of the Interior. Serbian Police fields 27,363 uniformed officers.
National security and counterintelligence are the responsibility of the Security Intelligence Agency (BIA).

Serbia has established diplomatic relations with 188 UN member states, the Holy See, the Sovereign Military Order of Malta, and the European Union. Foreign relations are conducted through the Ministry of Foreign Affairs. Serbia has a network of 65 embassies and 23 consulates internationally. There are 69 foreign embassies, 5 consulates and 4 liaison offices in Serbia.

Serbian foreign policy is focused on achieving the strategic goal of becoming a member state of the European Union (EU). Serbia started the process of joining the EU by signing of the Stabilisation and Association Agreement on 29 April 2008 and officially applied for membership in the European Union on 22 December 2009. It received a full candidate status on 1 March 2012 and started accession talks on 21 January 2014. The European Commission considers accession possible by 2025.

The province of Kosovo declared independence from Serbia on 17 February 2008, which sparked varied responses from the international community, some welcoming it, while others condemn the unilateral move. In protest, Serbia initially recalled its ambassadors from countries that recognised Kosovo's independence. The resolution of 26 December 2007 by the National Assembly stated that both the Kosovo declaration of independence and recognition thereof by any state would be gross violation of international law.

Serbia began cooperation and dialogue with NATO in 2006, when the country joined the Partnership for Peace programme and the Euro-Atlantic Partnership Council. The country's military neutrality was formally proclaimed by a resolution adopted by Serbia's parliament in December 2007, which makes joining any military alliance contingent on a popular referendum, a stance acknowledged by NATO. On the other hand, Serbia's relations with Russia are habitually described by mass media as a "centuries-old religious, ethnic and political alliance" and Russia is said to have sought to solidify its relationship with Serbia since the imposition of sanctions against Russia in 2014.

The Serbian Armed Forces are subordinate to the Ministry of Defence, and are composed of the Army and the Air Force. Although a landlocked country, Serbia operates a River Flotilla which patrols on the Danube, Sava, and Tisza rivers. The Serbian Chief of the General Staff reports to the Defence Minister. The Chief of Staff is appointed by the President, who is the Commander-in-chief. , Serbian defence budget amounts to $804 million.

Traditionally having relied on a large number of conscripts, Serbian Armed Forces went through a period of downsizing, restructuring and professionalisation. Conscription was abolished in 2011. Serbian Armed Forces have 28,000 active troops, supplemented by the "active reserve" which numbers 20,000 members and "passive reserve" with about 170,000.

Serbia participates in the NATO Individual Partnership Action Plan programme, but has no intention of joining NATO, due to significant popular rejection, largely a legacy of the NATO bombing of Yugoslavia in 1999. It is an observer member of the Collective Securities Treaty Organisation (CSTO) The country also signed the Stability Pact for South Eastern Europe. The Serbian Armed Forces take part in several multinational peacekeeping missions, including deployments in Lebanon, Cyprus, Ivory Coast, and Liberia.

Serbia is a major producer and exporter of military equipment in the region. Defence exports totaled around $600 million in 2018. The defence industry has seen significant growth over the years and it continues to grow on a yearly basis.

Serbia is a unitary state composed of municipalities/cities, districts, and two autonomous provinces. In Serbia, excluding Kosovo, there are 145 municipalities ("opštine") and 29 cities ("gradovi"), which form the basic units of local self-government. Apart from municipalities/cities, there are 24 districts ("okruzi", 10 most populated listed below), with the City of Belgrade constituting an additional district. Except for Belgrade, which has an elected local government, districts are regional centres of state authority, but have no powers of their own; they present purely administrative divisions.
The Constitution of Serbia recognizes two autonomous provinces, Vojvodina in the north, and the disputed territory of Kosovo and Metohija in the south, while the remaining area of Central Serbia never had its own regional authority. Following the Kosovo War, UN peacekeepers entered Kosovo and Metohija, as per UNSC Resolution 1244. In 2008, Kosovo declared independence. The government of Serbia did not recognise the declaration, considering it illegal and illegitimate.

 census, Serbia (excluding Kosovo) has a total population of 7,186,862 and the overall population density is medium as it stands at 92.8 inhabitants per square kilometre. The census was not conducted in Kosovo which held its own census that numbered their total population at 1,739,825, excluding Serb-inhabited North Kosovo, as Serbs from that area (about 50,000) boycotted the census.

Serbia has been enduring a demographic crisis since the beginning of the 1990s, with a death rate that has continuously exceeded its birth rate. It is estimated that 300,000 people left Serbia during the 1990s, 20% of whom had a higher education. Serbia subsequently has one of the oldest populations in the world, with the average age of 42.9 years, and its population is shrinking at one of the fastest rates in the world. A fifth of all households consist of only one person, and just one-fourth of four and more persons. Average life expectancy in Serbia at birth is 76.1 years.

During the 1990s, Serbia had the largest refugee population in Europe. Refugees and internally displaced persons (IDPs) in Serbia formed between 7% and 7.5% of its population at the time – about half a million refugees sought refuge in the country following the series of Yugoslav wars, mainly from Croatia (and to a lesser extent from Bosnia and Herzegovina) and the IDPs from Kosovo.

Serbs with 5,988,150 are the largest ethnic group in Serbia, representing 83% of the total population (excluding Kosovo). Serbia is one of the European countries with high numbers of registered national minorities, while the Autonomous Province of Vojvodina is recognizable for its multi-ethnic and multi-cultural identity. With a population of 253,899, Hungarians are the largest ethnic minority in Serbia, concentrated predominantly in northern Vojvodina and representing 3.5% of the country's population (13% in Vojvodina). Romani population stands at 147,604 according to the 2011 census but unofficial estimates place their actual number between 400,000 and 500,000. Bosniaks with 145,278 are concentrated in Raška (Sandžak), in the southwest. Other minority groups include Croats, Slovaks, Albanians, Montenegrins, Vlachs, Romanians, Macedonians and Bulgarians. Chinese, estimated at about 15,000, are the only significant non-European immigrant minority.

The majority of the population, or 59.4%, reside in urban areas and some 16.1% in Belgrade alone. Belgrade is the only city with more than a million inhabitants and there are four more with over 100,000 inhabitants.
The Constitution of Serbia defines it as a secular state with guaranteed religious freedom. Orthodox Christians with 6,079,396 comprise 84.5% of country's population. The Serbian Orthodox Church is the largest and traditional church of the country, adherents of which are overwhelmingly Serbs. Other Orthodox Christian communities in Serbia include Montenegrins, Romanians, Vlachs, Macedonians and Bulgarians.

Roman Catholics number 356,957 in Serbia, or roughly 6% of the population, mostly in Vojvodina (especially its northern part) which is home to minority ethnic groups such as Hungarians, Croats, Bunjevci, as well as to some Slovaks and Czechs.

Protestantism accounts for about 1% of the country's population, chiefly Lutheranism among Slovaks in Vojvodina as well as Calvinism among Reformed Hungarians. Greek Catholic Church is adhered by around 25,000 citizens (0.37% of the population), mostly Rusyns in Vojvodina.

Muslims, with 222,282 or 3% of the population, form the third largest religious group. Islam has a strong historic following in the southern regions of Serbia, primarily in southern Raška. Bosniaks are the largest Islamic community in Serbia; estimates are that around a third of the country's Roma people are Muslim.

There are only 578 Jews in Serbia. Atheists numbered 80,053 or 1.1% of the population and an additional 4,070 declared themselves to be agnostics.

The official language is Serbian, native to 88% of the population. Serbian is the only European language with active digraphia, using both Cyrillic and Latin alphabets. Serbian Cyrillic is designated in the Constitution as the "official script" and was devised in 1814 by Serbian philologist Vuk Karadžić, who based it on phonemic principles. A survey from 2014 showed that 47% of Serbians favour the Latin alphabet, 36% favour the Cyrillic one and 17% have no preference.

Standard Serbian is based on the most widespread Shtokavian dialect (more specifically on the dialects of Šumadija-Vojvodina and Eastern Herzegovina). 

Recognised minority languages are: Hungarian, Bosnian, Slovak, Croatian, Albanian, Romanian, Bulgarian, Rusyn, and Macedonian. All these languages are in official use in municipalities or cities where the ethnic minority exceeds 15% of the total population. In Vojvodina, the provincial administration uses, besides Serbian, five other languages (Hungarian, Slovak, Croatian, Romanian and Rusyn).
Serbia has an emerging market economy in upper-middle income range. According to the International Monetary Fund, Serbian nominal GDP in 2018 is officially estimated at $50.651 billion or $7,243 per capita while purchasing power parity GDP stood at $122.759 billion or $17,555 per capita. The economy is dominated by services which accounts for 67.9% of GDP, followed by industry with 26.1% of GDP, and agriculture at 6% of GDP. The official currency of Serbia is Serbian dinar (ISO code: RSD), and the central bank is National Bank of Serbia. The Belgrade Stock Exchange is the only stock exchange in the country, with market capitalisation of $8.65 billion and BELEX15 as the main index representing the 15 most liquid stocks.

The economy has been affected by the global economic crisis. After almost a decade of strong economic growth (average of 4.45% per year), Serbia entered the recession in 2009 with negative growth of −3% and again in 2012 and 2014 with −1% and −1.8%, respectively. As the government was fighting effects of crisis the public debt has more than doubled: from pre-crisis level of just under 30% to about 70% of GDP and trending downwards recently to around 50%. Labour force stands at 3.2 million, with 56% employed in services sector, 28.1% in industry and 15.9% in the agriculture. The average monthly net salary in May 2019 stood at 47,575 dinars or $525. The unemployment remains an acute problem, with rate of 12.7% .

Since 2000, Serbia has attracted over $40 billion in foreign direct investment (FDI). Blue-chip corporations making investments include: Fiat Chrysler Automobiles, Siemens, Bosch, Philip Morris, Michelin, Coca-Cola, Carlsberg and others. In the energy sector, Russian energy giants, Gazprom and Lukoil have made large investments. In metallurgy sector, Chinese steel and copper giants, Hesteel and Zijin Mining have acquired key complexes.

Serbia has an unfavourable trade balance: imports exceed exports by 25%. Serbia's exports, however, recorded a steady growth in last couple of years reaching $19.2 billion in 2018. The country has free trade agreements with the EFTA and CEFTA, a preferential trade regime with the European Union, a Generalised System of Preferences with the United States, and individual free trade agreements with Russia, Belarus, Kazakhstan, and Turkey.

Serbia has very favourable natural conditions (land and climate) for varied agricultural production. It has 5,056,000 ha of agricultural land (0.7 ha per capita), out of which 3,294,000 ha is arable land (0.45 ha per capita). In 2016, Serbia exported agricultural and food products worth $3.2 billion, and the export-import ratio was 178%. Agricultural exports constitute more than one-fifth of all Serbia's sales on the world market. Serbia is one of the largest provider of frozen fruit to the EU (largest to the French market, and 2nd largest to the German market). 

Agricultural production is most prominent in Vojvodina on the fertile Pannonian Plain. Other agricultural regions include Mačva, Pomoravlje, Tamnava, Rasina, and Jablanica.

In the structure of the agricultural production 70% is from the crop field production, and 30% is from the livestock production. Serbia is world's second largest producer of plums (582,485 tonnes; second to China), second largest of raspberries (89,602 tonnes, second to Poland), it is also a significant producer of maize (6.48 million tonnes, ranked 32nd in the world) and wheat (2.07 million tonnes, ranked 35th in the world). Other important agricultural products are: sunflower, sugar beet, soybean, potato, apple, pork meat, beef, poultry and dairy.

There are 56,000 ha of vineyards in Serbia, producing about 230 million litres of wine annually. Most famous viticulture regions are located in Vojvodina and Šumadija.

The industry was the economic sector hardest hit by the UN sanctions and trade embargo and NATO bombing during the 1990s and transition to market economy during the 2000s. The industrial output saw dramatic downsizing: in 2013 it was expected to be only a half of that of 1989. Main industrial sectors include: automotive, mining, non-ferrous metals, food-processing, electronics, pharmaceuticals, clothes. Serbia has 14 free economic zones as of September 2017, in which many foreign direct investments are realised.

Automotive industry (with Fiat Chrysler Automobiles as a forebearer) is dominated by cluster located in Kragujevac and its vicinity, and contributes to export with about $2 billion. Country is a leading steel producer in the wider region of Southeast Europe and had production of nearly 2 million tonnes of raw steel in 2018, coming entirely from Smederevo steel mill, owned by the Chinese Hesteel. Serbia's mining industry is comparatively strong: Serbia is the 18th largest producer of coal (7th in the Europe) extracted from large deposits in Kolubara and Kostolac basins; it is also world's 23rd largest (3rd in Europe) producer of copper which is extracted by Zijin Bor Copper, a large copper mining company, acquired by Chinese Zijin Mining in 2018; significant gold extraction is developed around Majdanpek. Serbia notably manufactures intel smartphones named Tesla smartphones.

Food industry is well known both regionally and internationally and is one of the strong points of the economy. Some of the international brand-names established production in Serbia: PepsiCo and Nestlé in food-processing sector; Coca-Cola (Belgrade), Heineken (Novi Sad) and Carlsberg (Bačka Palanka) in beverage industry; Nordzucker in sugar industry. Serbia's electronics industry had its peak in the 1980s and the industry today is only a third of what it was back then, but has witnessed a something of revival in last decade with investments of companies such as Siemens (wind turbines) in Subotica, Panasonic (lighting devices) in Svilajnac, and Gorenje (electrical home appliances) in Valjevo. The pharmaceutical industry in Serbia comprises a dozen manufacturers of generic drugs, of which Hemofarm in Vršac and Galenika in Belgrade, account for 80% of production volume. Domestic production meets over 60% of the local demand.

The energy sector is one of the largest and most important sectors to the country's economy. Serbia is a net exporter of electricity and importer of key fuels (such as oil and gas).

Serbia has an abundance of coal, and significant reserves of oil and gas. Serbia's proven reserves of 5.5 billion tonnes of coal lignite are the 5th largest in the world (second in Europe, after Germany). Coal is found in two large deposits: Kolubara (4 billion tonnes of reserves) and Kostolac (1.5 billion tonnes). Despite being small on a world scale, Serbia's oil and gas resources (77.4 million tonnes of oil equivalent and 48.1 billion cubic metres, respectively) have a certain regional importance since they are largest in the region of former Yugoslavia as well as the Balkans (excluding Romania). Almost 90% of the discovered oil and gas are to be found in Banat and those oil and gas fields are by size among the largest in the Pannonian basin but are average on a European scale.

The production of electricity in 2015 in Serbia was 36.5 billion kilowatt-hours (KWh), while the final electricity consumption amounted to 35.5 billion kilowatt-hours (KWh). Most of the electricity produced comes from thermal-power plants (72.7% of all electricity) and to a lesser degree from hydroelectric-power plants (27.3%). There are 6 lignite-operated thermal-power plants with an installed power of 3,936 MW; largest of which are 1,502 MW-Nikola Tesla 1 and 1,160 MW-Nikola Tesla 2, both in Obrenovac. Total installed power of 9 hydroelectric-power plants is 2,831 MW, largest of which is Đerdap 1 with capacity of 1,026 MW. In addition to this, there are mazute and gas-operated thermal-power plants with an installed power of 353 MW. The entire production of electricity is concentrated in Elektroprivreda Srbije (EPS), public electric-utility power company.

The current oil production in Serbia amounts to over 1.1 million tonnes of oil equivalent and satisfies some 43% of country's needs while the rest is imported. National petrol company, Naftna Industrija Srbije (NIS), was acquired in 2008 by Gazprom Neft. The company's refinery in Pančevo (capacity of 4.8 million tonnes) is one of the most modern oil-refineries in Europe; it also operates network of 334 filling stations in Serbia (74% of domestic market) and additional 36 stations in Bosnia and Herzegovina, 31 in Bulgaria, and 28 in Romania. There are 155 kilometers of crude oil pipelines connecting Pančevo and Novi Sad refineries as a part of trans-national Adria oil pipeline.

Serbia is heavily dependent on foreign sources of natural gas, with only 17% coming from domestic production (totalling 491 million cubic meters in 2012) and the rest is imported, mainly from Russia (via gas pipelines that run through Ukraine and Hungary). Srbijagas, public company, operates the natural gas transportation system which comprise 3,177 kilometers of trunk and regional natural gas pipelines and a 450 million cubic meter underground gas storage facility at Banatski Dvor.

Serbia has a strategic transportation location since the country's backbone, Morava Valley, represents by far the easiest route of land travel from continental Europe to Asia Minor and the Near East.

Serbian road network carries the bulk of traffic in the country. Total length of roads is 45,419 km of which 962 km are "class-IA state roads" (i.e. motorways); 4,517 km are "class-IB state roads" (national roads); 10,941 km are "class-II state roads" (regional roads) and 23,780 km are "municipal roads". The road network, except for the most of class-IA roads, are of comparatively lower quality to the Western European standards because of lack of financial resources for their maintenance in the last 20 years.

Over 300 kilometers of new motorways has been constructed in the last decade and additional 142 kilometers are currently under construction: A5 motorway (from south of Pojate (north of Kruševac ) to Čačak) and 30 km-long segment of A2 (between Čačak and Požega). Coach transport is very extensive: almost every place in the country is connected by bus, from largest cities to the villages; in addition there are international routes (mainly to countries of Western Europe with large Serb diaspora). Routes, both domestic and international, are served by more than hundred intercity coach services, biggest of which are Lasta and Niš-Ekspres. , there were 1,999,771 registered passenger cars or 1 passenger car per 3.5 inhabitants.

Serbia has 3,819 kilometres of rail tracks, of which 1,279 are electrified and 283 kilometres are double-track railroad. The major rail hub is Belgrade (and to a lesser degree Niš), while the most important railroads include: Belgrade–Bar (Montenegro), Belgrade–Šid–Zagreb (Croatia)/Belgrade–Niš–Sofia (Bulgaria) (part of Pan-European Corridor X), Belgrade–Subotica–Budapest (Hungary) and Niš–Thessaloniki (Greece). Although still a major mode of freight transportation, railroads face increasing problems with the maintenance of the infrastructure and lowering speeds. Rail services are operated Srbija Voz (passenger transport) and Srbija Kargo (freight transport).

There are only two airports with regular passenger traffic. Belgrade Nikola Tesla Airport served 5.6 million passengers in 2018 and is a hub of flagship carrier Air Serbia which flies to 59 destinations in 32 countries and carried some 2.5 million passengers in 2018. Niš Constantine the Great Airport is mainly catering low-cost airlines. 

Serbia has a developed inland water transport since there are 1,716 kilometres of navigable inland waterways (1,043 km of navigable rivers and 673 km of navigable canals), which are almost all located in northern third of the country. The most important inland waterway is the Danube (part of Pan-European Corridor VII). Other navigable rivers include Sava, Tisza, Begej and Timiş River, all of which connect Serbia with Northern and Western Europe through the Rhine–Main–Danube Canal and North Sea route, to Eastern Europe via the Tisza, Begej and Danube Black Sea routes, and to Southern Europe via the Sava river. More than 2 million tonnes of cargo were transported on Serbian rivers and canals in 2016 while the largest river ports are: Novi Sad, Belgrade, Pančevo, Smederevo, Prahovo and Šabac.

Fixed telephone lines connect 81% of households in Serbia, and with about 9.1 million users the number of cellphones surpasses the total population of by 28%. The largest mobile operator is Telekom Srbija with 4.2 million subscribers, followed by Telenor with 2.8 million users and Vip mobile with about 2 million. Some 58% of households have fixed-line (non-mobile) broadband Internet connection while 67% are provided with pay television services (i.e. 38% cable television, 17% IPTV, and 10% satellite). Digital television transition has been completed in 2015 with DVB-T2 standard for signal transmission.

Serbia is not a mass-tourism destination but nevertheless has a diverse range of touristic products. In 2019, total of over 3.6 million tourists were recorded in accommodations, of which half were foreign. Foreign exchange earnings from tourism were estimated at $1.5 billion.

Tourism is mainly focused on the mountains and spas of the country, which are mostly visited by domestic tourists, as well as Belgrade and, to a lesser degree, Novi Sad, which are preferred choices of foreign tourists (almost two-thirds of all foreign visits are made to these two cities). The most famous mountain resorts are Kopaonik, Stara Planina and Zlatibor. There are also many spas in Serbia, the biggest of which are Vrnjačka Banja, Soko Banja, and Banja Koviljača. City-break and conference tourism is developed in Belgrade and Novi Sad. Other touristic products that Serbia offer are natural wonders like Đavolja varoš, Christian pilgrimage to the many Orthodox monasteries across the country and the river cruising along the Danube. There are several internationally popular music festivals held in Serbia, such as EXIT (with 25–30,000 foreign visitors coming from 60 different countries) and the Guča trumpet festival.

According to 2011 census, literacy in Serbia stands at 98% of population while computer literacy is at 49% (complete computer literacy is at 34.2%). Same census showed the following levels of education: 16.2% of inhabitants have higher education (10.6% have bachelors or master's degrees, 5.6% have an associate degree), 49% have a secondary education, 20.7% have an elementary education, and 13.7% have not completed elementary education.

Education in Serbia is regulated by the Ministry of Education and Science. Education starts in either preschools or elementary schools. Children enroll in elementary schools at the age of seven. Compulsory education consists of eight grades of elementary school. Students have the opportunity to attend gymnasiums and vocational schools for another four years, or to enroll in vocational training for 2 to 3 years. Following the completion of gymnasiums or vocational schools, students have the opportunity to attend university. Elementary and secondary education are also available in languages of recognised minorities in Serbia, where classes are held in Hungarian, Slovak, Albanian, Romanian, Rusyn, Bulgarian as well as Bosnian and Croatian languages. Petnica Science Center is a notable institution for extracurricular science education focusing on gifted students.

There are 19 universities in Serbia (nine public universities with a total number of 86 faculties and ten private universities with 51 faculties). In 2018/2019 academic year, 210,480 students attended 19 universities (181,310 at public universities and some 29,170 at private universities) while 47,169 attended 81 "higher schools". Public universities in Serbia are: the University of Belgrade (oldest, founded in 1808, and largest university with 97,696 undergraduates and graduates), University of Novi Sad (founded in 1960 and with student body of 42,489), University of Niš (founded in 1965; 20,559 students), University of Kragujevac (founded in 1976; 14,053 students), University of Priština (located in North Mitrovica), Public University of Novi Pazar as well as three specialist universities – University of Arts, University of Defence and University of Criminal Investigation and Police Studies. Largest private universities include Megatrend University and Singidunum University, both in Belgrade, and Educons University in Novi Sad. The University of Belgrade (placed in 301–400 bracket on 2013 Shanghai Ranking of World Universities, being best-placed university in Southeast Europe after those in Athens and Thessaloniki) and University of Novi Sad are generally considered as the best institutions of higher learning in the country.
Serbia spent 0.9% of GDP on scientific research in 2017, which is slightly below the European average. Since 2018, Serbia is a full member of CERN. Serbia has a long history of excellence in maths and computer sciences which has created a strong pool of engineering talent, although economic sanctions during the 1990s and chronic underinvestment in research forced many scientific professionals to leave the country. Nevertheless, there are several areas in which Serbia still excels such as growing information technology sector, which includes software development as well as outsourcing. It generated over $1.2 billion in exports in 2018, both from international investors and a significant number of dynamic homegrown enterprises. Serbia is one of the countries with the highest proportion of women in science.
Among the scientific institutes operating in Serbia, the largest are the Mihajlo Pupin Institute and Vinča Nuclear Institute, both in Belgrade. The Serbian Academy of Sciences and Arts is a learned society promoting science and arts from its inception in 1841. With a strong science and technological ecosystem, Serbia has produced a number of renowned scientists that have greatly contributed to the field of science and technology.

For centuries straddling the boundaries between East and West, the territory of Serbia had been divided among the Eastern and Western halves of the Roman Empire; then between Byzantium and the Kingdom of Hungary; and in the Early modern period between the Ottoman Empire and the Habsburg Empire. These overlapping influences have resulted in cultural varieties throughout Serbia; its north leans to the profile of Central Europe, while the south is characteristic of the wider Balkans and even the Mediterranean. The Byzantine influence on Serbia was profound, firstly through the introduction of Eastern Christianity in the Early Middle Ages. The Serbian Orthodox Church has had an enduring status in Serbia, with the many Serbian monasteries constituting cultural monuments left from Serbia in the Middle Ages. Serbia has seen influences of Republic of Venice as well, mainly though trade, literature and romanesque architecture.

Serbia has five cultural monuments inscribed in the list of UNESCO World Heritage: the early medieval capital Stari Ras and the 13th-century monastery Sopoćani; the 12th-century Studenica monastery; the Roman complex of Gamzigrad–Felix Romuliana; medieval tombstones Stećci; and finally the endangered Medieval Monuments in Kosovo (the monasteries of Visoki Dečani, Our Lady of Ljeviš, Gračanica and Patriarchal Monastery of Peć).

There are two literary monuments on UNESCO's Memory of the World Programme: the 12th-century "Miroslav Gospel", and scientist Nikola Tesla's archive. The "slava" (patron saint veneration), kolo (traditional folk dance) and singing to the accompaniment of the gusle are inscribed on UNESCO Intangible Cultural Heritage Lists. The Ministry of Culture and Information is tasked with preserving the nation's cultural heritage and overseeing its development. Further activities supporting development of culture are undertaken at local government level.

Traces of Roman and early Byzantine Empire architectural heritage are found in many royal cities and palaces in Serbia, like Sirmium, Felix Romuliana and Justiniana Prima, since 535 the seat of the Archbishopric of Justiniana Prima.

Serbian monasteries are the pinnacle of Serbian medieval art. At the beginning, they were under the influence of Byzantine Art which was particularly felt after the fall of Constantinople in 1204, when many Byzantine artists fled to Serbia. Noted of these monasteries is Studenica (built around 1190). It was a model for later monasteries, like the Mileševa, Sopoćani, Žiča, Gračanica and Visoki Dečani. Numerous monuments and cultural sites were destroyed at various stages of Serbian history, with destruuction in Kosovo being the recent example. In the end of 14th and the 15th centuries, autochthonous architectural style known as Morava style evolved in area around Morava Valley. A characteristic of this style was the wealthy decoration of the frontal church walls. Examples of this include Manasija, Ravanica and Kalenić monasteries. 

Icons and fresco paintings are often considered the peak of Serbian art. The most famous frescos are White Angel (Mileševa monastery), "Crucifixion" (Studenica monastery) and "Dormition of the Virgin" (Sopoćani).

Country is dotted with many well-preserved medieval fortifications and castles such as Smederevo Fortress (largest lowland fortress in Europe), Golubac, Maglič, Soko grad, Belgrade Fortress, Ostrvica and Ram.

During the time of Ottoman occupation, Serbian art was virtually non-existent, with the exception of several Serbian artists who lived in the lands ruled by the Habsburg Monarchy. 
Traditional Serbian art showed Baroque influences at the end of the 18th century as shown in the works of Nikola Nešković, Teodor Kračun, Zaharije Orfelin and Jakov Orfelin.
Serbian painting showed the influence of Biedermeier and Neoclassicism as seen in works by Konstantin Danil, Arsenije Teodorović and Pavel Đurković. Many painters followed the artistic trends set in the 19th century Romanticism, notably Đura Jakšić, Stevan Todorović, Katarina Ivanović and Novak Radonić.

Important Serbian painters of the first half of the 20th century were Paja Jovanović and Uroš Predić of Realism, Cubist Sava Šumanović, Milena Pavlović-Barili and Nadežda Petrović of Impressionism, Expressionist Milan Konjović. Noted painters of the second half of 20th century include Marko Čelebonović, Petar Lubarda, Milo Milunović, Ljubomir Popović and Vladimir Veličković.

Anastas Jovanović was one of the earliest photographes in the world, while Marina Abramović is one of the world leading performance artists. Pirot carpet is known as one of the most important traditional handicrafts in Serbia.

There are around 180 museums in Serbia, of which the most prominent is the National Museum of Serbia, founded in 1844. It houses one of the largest art collections in the Balkans, including many foreign masterpiece collections. Other art museums of note are Museum of Contemporary Art in Belgrade, Museum of Vojvodina and the Gallery of Matica Srpska in Novi Sad.

The beginning of Serbian literacy dates back to the activity of the brothers Cyril and Methodius in the Balkans. Monuments of Serbian literacy from the early 11th century can be found, written in Glagolitic. Starting in the 12th century, books were written in Cyrillic. From this epoch, the oldest Serbian Cyrillic book editorial are the Miroslav Gospels from 1186. "The Miroslav Gospels" are considered to be the oldest book of Serbian medieval history and as such has entered UNESCO's Memory of the World Register.

Notable medieval authors include Saint Sava, Jefimija, Stefan Lazarević, Constantine of Kostenets and others. Due to Ottoman occupation, when every aspect of formal literacy stopped, Serbia stayed excluded from the entire Renaissance flow in Western culture. However, the tradition of oral story-telling blossomed, shaping itself through epic poetry inspired by at the times still recent Kosovo battle and folk tales deeply rooted in Slavic mythology. Serbian epic poetry in those times has seen as the most effective way in preserving the national identity. The oldest known, entirely fictional poems, make up the "Non-historic cycle"; this one is followed by poems inspired by events before, during and after Kosovo Battle. The special cycles are dedicated to Serbian legendary hero, Marko Kraljević, then about hajduks and uskoks, and the last one dedicated to the liberation of Serbia in the 19th century. Some of the best known folk ballads are "The Death of the Mother of the Jugović Family" and The Mourning Song of the Noble Wife of the Asan Aga (1646), translated into European languages by Goethe, Walter Scott, Pushkin and Mérimée. One of the most notable tales from Serbian folklore is The Nine Peahens and the Golden Apples.
Baroque trends in Serbian literature emerged in the late 17th century. Notable Baroque-influenced authors were Gavril Stefanović Venclović, Jovan Rajić, Zaharije Orfelin, Andrija Zmajević and others. Dositej Obradović was a prominent figure of the Age of Enlightenment, while the notable Classicist writer was Jovan Sterija Popović, although his works also contained elements of Romanticism. In the era of national revival, in the first half of the 19th century, Vuk Stefanović Karadžić collected Serbian folk literature, and reformed the Serbian language and spelling, paving the way for Serbian Romanticism. The first half of the 19th century was dominated by Romanticism, with Petar II Petrović-Njegoš, Branko Radičević, Đura Jakšić, Jovan Jovanović Zmaj and Laza Kostić being the notable representatives, while the second half of the century was marked by Realist writers such as Milovan Glišić, Laza Lazarević, Simo Matavulj, Stevan Sremac, Vojislav Ilić, Branislav Nušić, Radoje Domanović and Borisav Stanković.

The 20th century was dominated by the prose writers Meša Selimović ("Death and the Dervish"), Miloš Crnjanski ("Migrations"), Isidora Sekulić ("The Cronicle of a Small Town Cemetery"), Branko Ćopić ("Eagles Fly Early"), Borislav Pekić ("The Time of Miracles"), Danilo Kiš ("The Encyclopedia of the Dead"), Dobrica Ćosić ("The Roots"), Aleksandar Tišma ("The Use of Man"), Milorad Pavić and others. Notable poets include Milan Rakić, Jovan Dučić, Vladislav Petković Dis, Rastko Petrović, Stanislav Vinaver, Dušan Matić, Branko Miljković, Vasko Popa, Oskar Davičo, Miodrag Pavlović, and Stevan Raičković.

Pavić is widely acclaimed Serbian author of the beginning of the 21st century, most notably for his "Dictionary of the Khazars", which has been translated into 38 languages. Notable contemporary authors include David Albahari, Svetislav Basara, Goran Petrović, Gordana Kuić, Vuk Drašković and Vladislav Bajac. Serbian comics emerged in the 1930s and the medium remains popular today.

Ivo Andrić ("The Bridge on the Drina") is possibly the best-known Serbian author; he was awarded the Nobel Prize in Literature in 1961. The most beloved face of Serbian literature was Desanka Maksimović, who for seven decades remained "the leading lady of Yugoslav poetry". She is honoured with statues, postage stamps, and the names of streets across Serbia.

There are 551 public libraries biggest of which are: National Library of Serbia in Belgrade with funds of about 6 million items, and Matica Srpska (the oldest matica and Serbian cultural institution, founded in 1826) in Novi Sad with nearly 3.5 million volumes. In 2010, there were 10,989 books and brochures published. The book publishing market is dominated by several major publishers such as Laguna and Vulkan (both of which operate their own bookstore chains) and the industry's centrepiece event, annual Belgrade Book Fair, is the most visited cultural event in Serbia with 158,128 visitors in 2013. The highlight of the literary scene is awarding of NIN Prize, given every January since 1954 for the best newly published novel in Serbian language.

Composer and musicologist Stevan Stojanović Mokranjac is considered the founder of modern Serbian music. The Serbian composers of the first generation Petar Konjović, Stevan Hristić, and Miloje Milojević maintained the national expression and modernised the romanticism into the direction of impressionism. Other famous classical Serbian composers include Isidor Bajić, Stanislav Binički and Josif Marinković. There are three opera houses in Serbia: Opera of the National Theatre and Madlenianum Opera, both in Belgrade, and Opera of the Serbian National Theatre in Novi Sad. Four symphonic orchestra operate in the country: Belgrade Philharmonic Orchestra, Niš Symphony Orchestra, Symphonic Orchestra of Radio Television of Serbia, and Novi Sad Philharmonic Orchestra. The Choir of Radio Television of Serbia is a leading vocal ensemble in the country. The BEMUS is one of the most prominent classical music festivals in the South East Europe.

Traditional Serbian music includes various kinds of bagpipes, flutes, horns, trumpets, lutes, psalteries, drums and cymbals. The "kolo" is the traditional collective folk dance, which has a number of varieties throughout the regions. The most popular are those from Užice and Morava region. Sung epic poetry has been an integral part of Serbian and Balkan music for centuries. In the highlands of Serbia these long poems are typically accompanied on a one-string fiddle called the "gusle", and concern themselves with themes from history and mythology. There are records of "gusle" being played at the court of the 13th-century King Stefan Nemanjić.

Pop music has mainstream popularity. Željko Joksimović won second place at the 2004 Eurovision Song Contest and Marija Šerifović managed to win the 2007 Eurovision Song Contest with the song "Molitva", and Serbia was the host of the 2008 edition of the contest. Most popular pop singers include likes of Đorđe Balašević, Goca Tržan, Zdravko Čolić, Aleksandra Radović, Vlado Georgiev, Jelena Tomašević and Nataša Bekvalac among others.
The Serbian rock which was during the 1960s, 1970s and 1980s part of former Yugoslav rock scene, used to be well developed and covered in the media. During the 1990s and 2000s popularity of rock music declined in Serbia, and although several major mainstream acts managed to sustain their popularity, an underground and independent music scene developed. The 2000s saw a revival of the mainstream scene and the appearance of a large number of notable acts. Notable Serbian rock acts include Bajaga i Instruktori, Disciplina Kičme, Ekatarina Velika, Električni Orgazam, Eva Braun, Kerber, Neverne Bebe, Partibrejkers, Ritam Nereda, Orthodox Celts, Rambo Amadeus, Riblja Čorba, S.A.R.S., Smak, Van Gogh, YU Grupa and others.

Folk music in its original form has been a prominent music style since World War One following the early success of Sofka Nikolić. The music has been further promoted by Danica Obrenić, Anđelija Milić, Nada Mamula, and even later, during 60s and 70s, with stars like Silvana Armenulić, Toma Zdravković, Lepa Lukić, Vasilija Radojčić, Vida Pavlović and Gordana Stojićević.

Turbo-folk music is subgenre that has developed in Serbia in the late 1980s and the beginning of the 1990s and has since enjoyed an immense popularity through acts of Dragana Mirković, Zorica Brunclik, Šaban Šaulić, Ana Bekuta, Sinan Sakić, Vesna Zmijanac, Mile Kitić, Snežana Đurišić, Šemsa Suljaković, and Nada Topčagić. It is a blend of folk music with pop and/or dance elements and can be seen as a result of the urbanisation of folk music. In recent period turbo-folk featured even more pop music elements, and some of the performers were labeled as pop-folk. The most famous among them are Ceca (often considered to be the biggest music star of Serbia), Jelena Karleuša, Aca Lukas, Seka Aleksić, Dara Bubamara, Indira Radić, Saša Matić, Viki Miljković, Stoja and Lepa Brena, arguably the most prominent performer of former Yugoslavia.

Balkan Brass, or "truba" ("trumpet") is a popular genre, especially in Central and Southern Serbia where Balkan Brass originated. The music has its tradition from the First Serbian Uprising. The trumpet was used as a military instrument to wake and gather soldiers and announce battles, the trumpet took on the role of entertainment during downtime, as soldiers used it to transpose popular folk songs. When the war ended and the soldiers returned to the rural life, the music entered civilian life and eventually became a music style, accompanying births, baptisms, weddings, and funerals. There are two main varieties of this genre, one from Western Serbia and the other from Southern Serbia, with brass musician Boban Marković being one of the most respected names in the world of modern brass band bandleaders.

Most popular music festival are Guča Trumpet Festival with over 300,000 annual visitors and EXIT in Novi Sad (won the Best Major Festival award at the European Festivals Awards for 2013 and 2017.) with 200,000 visitors in 2013. Other festivals include Nišville Jazz Festival in Niš and Gitarijada rock festival in Zaječar.

Serbia has a well-established theatrical tradition with Joakim Vujić considered the founder of modern Serbian theatre. Serbia has 38 professional theatres and 11 theatres for children, the most important of which are National Theatre in Belgrade, Serbian National Theatre in Novi Sad, National Theatre in Subotica, National Theatre in Niš and Knjaževsko-srpski teatar in Kragujevac (the oldest theatre in Serbia, established in 1835). The Belgrade International Theatre Festival – BITEF, founded in 1967, is one of the oldest theatre festivals in the world, and it has become one of the five biggest European festivals. Sterijino pozorje is, on the other hand, festival showcasing national drama plays. The most important Serbian playwrighters were Jovan Sterija Popović and Branislav Nušić, while recent renowned names are Dušan Kovačević and Biljana Srbljanović.
The foundation of Serbian cinema dates back to 1896 with the release of the oldest movie in the Balkans, "The Life and Deeds of the Immortal Vožd Karađorđe", a biopic about Serbian revolutionary leader, Karađorđe. 

Serbian cinema is one of the dynamic smaller European cinematographies. Serbia's film industry is heavily subsidised by the government, mainly through grants approved by the Film Centre of Serbia. As of 2011, there were 17 domestic feature films produced. There are 22 operating cinemas in the country, of which 12 are multiplexes, with total attendance exceeding 2.6 million and comparatively high percentage of 32.3% of total sold tickets for domestic films. Modern PFI Studios located in Šimanovci is nowadays Serbia's only major film studio complex; it consists of 9 sound stages and attracts mainly international productions, primarily American and West European. The Yugoslav Film Archive used to be former Yugoslavia's and now is Serbia national film archive – with over 100 thousand film prints, it is among five largest film archives in the world.

Famous Serbian filmmaker Emir Kusturica won two Golden Palms for Best Feature Film at the Cannes Film Festival, for "When Father Was Away on Business" in 1985 and then again for "Underground" in 1995. Other renowned directors include Dušan Makavejev, Želimir Žilnik (Golden Berlin Bear winner), Aleksandar Petrović, Živojin Pavlović, Goran Paskaljević, Goran Marković, Srđan Dragojević, Srdan Golubović and Mila Turajlić among others. Serbian-American screenwriter Steve Tesich won the Academy Award for Best Original Screenplay in 1979 for the movie Breaking Away.

Prominent movie stars in Serbia have left celebrated heritage in cinematography of Yugoslavia as well. Notable mentions are Zoran Radmilović, Pavle Vuisić, Ljubiša Samardžić, Olivera Marković, Mija Aleksić, Miodrag Petrović Čkalja, Ružica Sokić, Velimir Bata Živojinović, Danilo Bata Stojković, Seka Sablić, Olivera Katarina, Dragan Nikolić, Mira Stupica, Nikola Simić, Bora Todorović and others. Milena Dravić was one of the most celebrated actress in Serbian cinematography. She has won Best Actress Award on Cannes Film Festival in 1980.

The freedom of the press and the freedom of speech are guaranteed by the constitution of Serbia. Serbia is ranked 90th out of 180 countries in the 2019 Press Freedom Index report compiled by Reporters Without Borders. Report noted that media outlets and journalists continue to face partisan and government pressure over editorial policies. Also, the media are now more heavily dependent on advertising contracts and government subsidies to survive financially.

According to AGB Nielsen Research in 2009, Serbs on average watch five hours of television per day, making it the highest average in Europe. There are seven nationwide free-to-air television channels, with public broadcaster Radio Television of Serbia (RTS) operating three (RTS1, RTS2 and RTS3) and private broadcasters operating four (Pink, Happy, Prva, and O2). In 2017, preferred usage of these channels were as follows: 20.2% for RTS1, 14.1% for Pink, 9.4% for Happy, 9.0% for Prva, 4.7% for O2, and 2.5% for RTS2. There are 28 regional television channels and 74 local television channels. Besides terrestrial channels there are dozens Serbian television channels available only on cable or satellite.

There are 247 radio stations in Serbia. Out of these, six are radio stations with national coverage, including two of public broadcaster Radio Television of Serbia (Radio Belgrade 1 and Radio Belgrade 2/Radio Belgrade 3) and four private ones (Radio S1, Radio S2, Play Radio, and Radio Hit FM). Also, there are 34 regional stations and 207 local stations.

There are 305 newspapers published in Serbia of which 12 are daily newspapers. Dailies "Politika" and "Danas" are Serbia's papers of record, former being the oldest newspaper in the Balkans, founded in 1904. Highest circulation newspapers are tabloids "Večernje Novosti", "Blic", "Kurir", and "Informer", all with more than 100,000 copies sold. There are one daily newspaper devoted to sports – "Sportski žurnal", one business daily "Privredni pregled", two regional newspapers ("Dnevnik" published in Novi Sad and "Narodne novine" from Niš), and one minority-language daily ("Magyar Szo" in Hungarian, published in Subotica).

There are 1,351 magazines published in the country. Those include weekly news magazines "NIN", "Vreme" and "Nedeljnik", popular science magazine of "Politikin Zabavnik", women's "Lepota & Zdravlje", auto magazine "SAT revija", IT magazine "Svet kompjutera". In addition, there is a wide selection of Serbian editions of international magazines, such as "Cosmopolitan", "Elle", "Men's Health", "National Geographic", "Le Monde diplomatique", "Playboy", and "Hello!", among others.

The main news agencies are Tanjug, Beta and Fonet.

, out of 432 web-portals (mainly on the .rs domain) the most visited are online editions of printed dailies Blic and Kurir, news web-portal B92, and classifieds KupujemProdajem.

Serbian cuisine is largely heterogeneous in a way characteristic of the Balkans and, especially, the former Yugoslavia. It features foods characteristic of lands formerly under Turkish suzerainty as well as cuisine originating from other parts of Central Europe (especially Austria and Hungary). Food is very important in Serbian social life, particularly during religious holidays such as Christmas, Easter and feast days i.e. slava.

Staples of the Serbian diet include bread, meat, fruits, vegetables, and dairy products. Bread is the basis of all Serbian meals, and it plays an important role in Serbian cuisine and can be found in religious rituals. A traditional Serbian welcome is to offer bread and salt to guests. Meat is widely consumed, as is fish. Serbian specialties include ćevapčići (caseless sausages made of minced meat, which is always grilled and seasoned), pljeskavica, sarma, kajmak (a dairy product similar to clotted cream), gibanica (cheese and kajmak pie), ajvar (a roasted red pepper spread), proja (cornbread), and kačamak (corn-flour porridge).

Serbians claim their country as the birthplace of rakia ("rakija"), a highly alcoholic drink primarily distilled from fruit. Rakia in various forms is found throughout the Balkans, notably in Bulgaria, Croatia, Slovenia, Montenegro, Hungary and Turkey. Slivovitz ("šljivovica"), a plum brandy, is a type of rakia which is considered the national drink of Serbia.

Winemaking traditions in Serbia dates back to Roman times. Serbian wines are produced in 22 different geographical regions, with white wine dominating the total amount. Besides rakia and beer, wine is a very popular alcoholic beverage in the country.

Sports play an important role in Serbian society, and the country has a strong sporting history. The most popular sports in Serbia are football, basketball, tennis, volleyball, water polo and handball.
Professional sports in Serbia are organised by sporting federations and leagues (in case of team sports). One of particularities of Serbian professional sports is existence of many multi-sports clubs (called "sports societies"), biggest and most successful of which are Red Star, Partizan, and Beograd in Belgrade, Vojvodina in Novi Sad, Radnički in Kragujevac, Spartak in Subotica.

Football is the most popular sport in Serbia, and the Football Association of Serbia with 146,845 registered players, is the largest sporting association in the country. FK Bačka 1901 is the oldest football club in Serbia and the former Yugoslavia. Dragan Džajić was officially recognised as "the best Serbian player of all times" by the Football Association of Serbia, and more recently the likes of Nemanja Vidić, Dejan Stanković, Branislav Ivanović, Aleksandar Kolarov and Nemanja Matić play for the elite European clubs, developing the nation's reputation as one of the world's biggest exporters of footballers. The Serbia national football team lacks relative success although it qualified for three of the last four FIFA World Cups. Serbia national youth football teams have won 2013 U-19 European Championship and 2015 U-20 World Cup. The two main football clubs in Serbia are Red Star (winner of the 1991 European Cup) and Partizan (finalist of the 1966 European Cup), both from Belgrade. The rivalry between the two clubs is known as the "Eternal Derby", and is often cited as one of the most exciting sports rivalries in the world.
Serbia is one of the traditional powerhouses of world basketball, as Serbia men's national basketball team have won two World Championships (in 1998 and 2002), three European Championships (1995, 1997, and 2001) and two Olympic silver medals (in 1996 and 2016) as well. The women's national basketball team won the European Championship in 2015 and Olympic bronze medal in 2016. A total of 31 Serbian players have played in the NBA in last three decades, including Nikola Jokić (2019 All-NBA First team), Predrag "Peja" Stojaković (2011 NBA champion and three-time NBA All-Star), and Vlade Divac (2001 NBA All-Star and Basketball Hall of Famer). The renowned "Serbian coaching school" produced many of the most successful European basketball coaches of all times, such as Željko Obradović (who won a record 9 Euroleague titles as a coach), Dušan Ivković, Svetislav Pešić, and Igor Kokoškov (the first coach born and raised outside of North America to be hired as a head coach in the NBA). KK Partizan basketball club was the 1992 European champion.

The Serbia men's national water polo team is the one of the most successful national teams, having won Olympic gold medal in 2016, three World Championships (2005, 2009 and 2015), and seven European Championships in 2001, 2003, 2006, 2012, 2014, 2016 and 2018, respectively. VK Partizan has won a joint-record seven European champion titles.

Recent success of Serbian tennis players has led to an immense growth in the popularity of tennis in the country. Novak Djokovic has won seventeen Grand Slam singles title and has held the No. 1 spot in the ATP rankings for over 270 weeks. He became the eighth player in history to achieve the Career Grand Slam and the third man to hold all four major titles at once and the first ever to do so on three different surfaces. Ana Ivanovic (champion of 2008 French Open) and Jelena Janković were both ranked No. 1 in the WTA Rankings. There were two No. 1 ranked-tennis double players as well: Nenad Zimonjić (three-time men's double and four-time mixed double Grand Slam champion) and Slobodan Živojinović. The Serbia men's tennis national team won the 2010 Davis Cup and 2020 ATP Cup, while Serbia women's tennis national team reached the final at 2012 Fed Cup.
Serbia is one of the leading volleyball countries in the world. Its men's national team won the gold medal at 2000 Olympics, the European Championship three times as well as the 2016 FIVB World League. The women's national volleyball team are current world Champions, has won European Championship three times as well as Olympic silver medal in 2016. 

Jasna Šekarić, sport shooter, is one of the athletes with the most appearances at the Olympic Games. She has won a total of five Olympic medals and also three World Championship gold medals. Other noted Serbian athletes include: swimmers Milorad Čavić (2009 World championships gold and silver medalist as well as 2008 Olympic silver medalist on 100-metre butterfly in historic race with American swimmer Michael Phelps) and Nađa Higl (2009 World champion in 200-metre breaststroke); track and field athletes Vera Nikolić (former world record holder in 800 metres) and Ivana Španović (long-jumper; four-time European champion, World indoor champion and bronze medalist at the 2016 Olympics); wrestler Davor Štefanek (2016 Olympic gold medalist and 2014 World champion), and taekwondoist Milica Mandić (2012 Olympic gold medalist and 2017 world champion).

Serbia has hosted several major sport competitions, including the 2005 Men's European Basketball Championship, 2005 Men's European Volleyball Championship, 2006 and 2016 Men's European Water Polo Championships, 2009 Summer Universiade, 2012 European Men's Handball Championship, and 2013 World Women's Handball Championship. The most important annual sporting events held in the country are the Belgrade Marathon and the Tour de Serbie cycling race.


Sources:



</doc>
<doc id="29266" url="https://en.wikipedia.org/wiki?curid=29266" title="Relationship between religion and science">
Relationship between religion and science

Historians of science and of religion, philosophers, theologians, scientists, and others from various geographical regions and cultures have addressed numerous aspects of the relationship between religion and science. 

Even though the ancient and medieval worlds did not have conceptions resembling the modern understandings of "science" or of "religion", certain elements of modern ideas on the subject recur throughout history. The pair-structured phrases "religion and science" and "science and religion" first emerged in the literature in the 19th century. This coincided with the refining of "science" (from the studies of "natural philosophy") and of "religion" as distinct concepts in the preceding few centuries—partly due to professionalization of the sciences, the Protestant Reformation, colonization, and globalization. Since then the relationship between science and religion has been characterized in terms of 'conflict', 'harmony', 'complexity', and 'mutual independence', among others.

Both science and religion are complex social and cultural endeavors that vary across cultures and change over time. Most scientific (and technical) innovations prior to the scientific revolution were achieved by societies organized by religious traditions. Ancient pagan, Islamic, and Christian scholars pioneered individual elements of the scientific method. Roger Bacon, often credited with formalizing the scientific method, was a Franciscan friar. Confucian thought, whether religious or non-religious in nature, has held different views of science over time. Many 21st-century Buddhists view science as complementary to their beliefs. While the classification of the material world by the ancient Indians and Greeks into air, earth, fire and water was more metaphysical, and figures like Anaxagoras questioned certain popular views of Greek divinities, medieval Middle Eastern scholars empirically classified materials.

Events in Europe such as the Galileo affair of the early 17th century, associated with the scientific revolution and the Age of Enlightenment, led scholars such as John William Draper to postulate () a conflict thesis, suggesting that religion and science have been in conflict methodologically, factually and politically throughout history. Some contemporary scientists (such as Richard Dawkins, Lawrence Krauss, Peter Atkins, and Donald Prothero) subscribe to this thesis. However, the conflict thesis has lost favor among most contemporary historians of science.

Many scientists, philosophers, and theologians throughout history, such as Francisco Ayala, Kenneth R. Miller and Francis Collins, have seen compatibility or interdependence between religion and science. Biologist Stephen Jay Gould, other scientists, and some contemporary theologians regard religion and science as non-overlapping magisteria, addressing fundamentally separate forms of knowledge and aspects of life. Some theologians or historians of science, including John Lennox, Thomas Berry, Brian Swimme and Ken Wilber propose an interconnection between science and religion, while others such as Ian Barbour believe there are even parallels.

Public acceptance of scientific facts may sometimes be influenced by religious beliefs such as in the United States, where some reject the concept of evolution by natural selection, especially regarding human beings. Nevertheless, the American National Academy of Sciences has written that "the evidence for evolution can be fully compatible with religious faith",
a view endorsed by many religious denominations.

The concepts of "science" and "religion" are a recent invention: "religion" emerged in the 17th century in the midst of colonization and globalization and the Protestant Reformation, "science" emerged in the 19th century in the midst of attempts to narrowly define those who studied nature. Originally what is now known as "science" was pioneered as "natural philosophy". 

It was in the 19th century that the terms "Buddhism", "Hinduism", "Taoism", "Confucianism" and "World Religions" first emerged. In the ancient and medieval world, the etymological Latin roots of both science ("scientia") and religion ("religio") were understood as inner qualities of the individual or virtues, never as doctrines, practices, or actual sources of knowledge.

It was in the 19th century that the concept of "science" received its modern shape with new titles emerging such as "biology" and "biologist", "physics", and "physicist", among other technical fields and titles; institutions and communities were founded, and unprecedented applications to and interactions with other aspects of society and culture occurred. The term "scientist" was coined by the naturalist-theologian William Whewell in 1834 and it was applied to those who sought knowledge and understanding of nature. From the ancient world, starting with Aristotle, to the 19th century, the practice of studying nature was commonly referred to as "natural philosophy". Isaac Newton's book Philosophiae Naturalis Principia Mathematica (1687), whose title translates to "Mathematical Principles of Natural Philosophy", reflects the then-current use of the words "natural philosophy", akin to "systematic study of nature". Even in the 19th century, a treatise by Lord Kelvin and Peter Guthrie Tait's, which helped define much of modern physics, was titled Treatise on Natural Philosophy (1867).

It was in the 17th century that the concept of "religion" received its modern shape despite the fact that ancient texts like the Bible, the Quran, and other texts did not have a concept of religion in the original languages and neither did the people or the cultures in which these texts were written. In the 19th century, Max Müller noted that what is called ancient religion today, would have been called "law" in antiquity. For example, there is no precise equivalent of "religion" in Hebrew, and Judaism does not distinguish clearly between religious, national, racial, or ethnic identities. The Sanskrit word "dharma", sometimes translated as "religion", also means law or duty. Throughout classical South Asia, the study of law consisted of concepts such as penance through piety and ceremonial as well as practical traditions. Medieval Japan at first had a similar union between "imperial law" and universal or "Buddha law", but these later became independent sources of power. Throughout its long history, Japan had no concept of "religion" since there was no corresponding Japanese word, nor anything close to its meaning, but when American warships appeared off the coast of Japan in 1853 and forced the Japanese government to sign treaties demanding, among other things, freedom of religion, the country had to contend with this Western idea.

The development of sciences (especially natural philosophy) in Western Europe during the Middle Ages, has considerable foundation in the works of the Arabs who translated Greek and Latin compositions. The works of Aristotle played a major role in the institutionalization, systematization, and expansion of reason. Christianity accepted reason within the ambit of faith. In Christendom, reason was considered subordinate to revelation, which contained the ultimate truth and this truth could not be challenged. In medieval universities, the faculty for natural philosophy and theology were separate, and discussions pertaining to theological issues were often not allowed to be undertaken by the faculty of philosophy.

Natural philosophy, as taught in the arts faculties of the universities, was seen as an essential area of study in its own right and was considered necessary for almost every area of study. It was an independent field, separated from theology, and enjoyed a good deal of intellectual freedom as long as it was restricted to the natural world. In general, there was religious support for natural science by the late Middle Ages and a recognition that it was an important element of learning.

The extent to which medieval science led directly to the new philosophy of the scientific revolution remains a subject for debate, but it certainly had a significant influence.

The Middle Ages laid ground for the developments that took place in science, during the Renaissance which immediately succeeded it. By 1630, ancient authority from classical literature and philosophy, as well as their necessity, started eroding, although scientists were still expected to be fluent in Latin, the international language of Europe's intellectuals. With the sheer success of science and the steady advance of rationalism, the individual scientist gained prestige. Along with the inventions of this period, especially the printing press by Johannes Gutenberg, allowed for the dissemination of the Bible in languages of the common people (languages other than Latin). This allowed more people to read and learn from the scripture, leading to the Evangelical movement. The people who spread this message, concentrated more on individual agency rather than the structures of the Church.

In the 17th century, founders of the Royal Society largely held conventional and orthodox religious views, and a number of them were prominent Churchmen. While theological issues that had the potential to be divisive were typically excluded from formal discussions of the early Society, many of its fellows nonetheless believed that their scientific activities provided support for traditional religious belief. Clerical involvement in the Royal Society remained high until the mid-nineteenth century, when science became more professionalised.

Albert Einstein supported the compatibility of some interpretations of religion with science. In "Science, Philosophy and Religion, A Symposium" published by the Conference on Science, Philosophy and Religion in Their Relation to the Democratic Way of Life, Inc., New York in 1941, Einstein stated:

Einstein thus expresses views of ethical non-naturalism (contrasted to ethical naturalism).

Prominent modern scientists who are atheists include evolutionary biologist Richard Dawkins and Nobel Prize–winning physicist Steven Weinberg. Prominent scientists advocating religious belief include Nobel Prize–winning physicist and United Church of Christ member Charles Townes, evangelical Christian and past head of the Human Genome Project Francis Collins, and climatologist John T. Houghton.

The kinds of interactions that might arise between science and religion have been categorized by theologian, Anglican priest, and physicist John Polkinghorne: (1) conflict between the disciplines, (2) independence of the disciplines, (3) dialogue between the disciplines where they overlap and (4) integration of both into one field.

This typology is similar to ones used by theologians Ian Barbour and John Haught. More typologies that categorize this relationship can be found among the works of other science and religion scholars such as theologian and biochemist Arthur Peacocke.

According to Guillermo Paz-y-Miño-C and Avelina Espinosa, the historical conflict between evolution and religion is intrinsic to the incompatibility between scientific rationalism/empiricism and the belief in supernatural causation. According to evolutionary biologist Jerry Coyne, views on evolution and levels of religiosity in some countries, along with the existence of books explaining reconciliation between evolution and religion, indicate that people have trouble in believing both at the same time, thus implying incompatibility. According to physical chemist Peter Atkins, "whereas religion scorns the power of human comprehension, science respects it." Planetary scientist Carolyn Porco describes a hope that "the confrontation between science and formal religion will come to an end when the role played by science in the lives of all people is the same played by religion today."
Geologist and paleontologist Donald Prothero has stated that religion is the reason "questions about evolution, the age of the earth, cosmology, and human evolution nearly always cause Americans to flunk science literacy tests compared to other nations." However, Jon Miller, who studies science literacy across nations, states that Americans in general are slightly more scientifically literate than Europeans and the Japanese.
According to cosmologist and astrophysicist Lawrence Krauss, compatibility or incompatibility is a theological concern, not a scientific concern. In Lisa Randall's view, questions of incompatibility or otherwise are not answerable, since by accepting revelations one is abandoning rules of logic which are needed to identify if there are indeed contradictions between holding certain beliefs. Daniel Dennett holds that incompatibility exists because religion is not problematic to a certain point before it collapses into a number of excuses for keeping certain beliefs, in light of evolutionary implications.

According to theoretical physicist Steven Weinberg, teaching cosmology and evolution to students should decrease their self-importance in the universe, as well as their religiosity. Evolutionary developmental biologist PZ Myers' view is that all scientists should be atheists, and that science should never accommodate any religious beliefs. Physicist Sean M. Carroll claims that since religion makes claims that are supernatural, both science and religion are incompatible.

Evolutionary biologist Richard Dawkins is openly hostile to religion because he believes it actively debauches the scientific enterprise and education involving science. According to Dawkins, religion "subverts science and saps the intellect". He believes that when science teachers attempt to expound on evolution, there is hostility aimed towards them by parents who are skeptical because they believe it conflicts with their own religious beliefs, and that even in some textbooks have had the word 'evolution' systematically removed. He has worked to argue the negative effects that he believes religion has on education of science.

According to Renny Thomas' study on Indian scientists, atheistic scientists in India called themselves atheists even while accepting that their lifestyle is very much a part of tradition and religion. Thus, they differ from Western atheists in that for them following the lifestyle of a religion is not antithetical to atheism.

Others such as Francis Collins, George F. R. Ellis,
Kenneth R. Miller, Katharine Hayhoe, George Coyne and Simon Conway Morris argue for compatibility since they do not agree that science is incompatible with religion and vice versa. They argue that science provides many opportunities to look for and find God in nature and to reflect on their beliefs. According to Kenneth Miller, he disagrees with Jerry Coyne's assessment and argues that since significant portions of scientists are religious and the proportion of Americans believing in evolution is much higher, it implies that both are indeed compatible. Elsewhere, Miller has argued that when scientists make claims on science and theism or atheism, they are not arguing scientifically at all and are stepping beyond the scope of science into discourses of meaning and purpose. What he finds particularly odd and unjustified is in how atheists often come to invoke scientific authority on their non-scientific philosophical conclusions like there being no point or no meaning to the universe as the only viable option when the scientific method and science never have had any way of addressing questions of meaning or God in the first place. Furthermore, he notes that since evolution made the brain and since the brain can handle both religion and science, there is no natural incompatibility between the concepts at the biological level.

Karl Giberson argues that when discussing compatibility, some scientific intellectuals often ignore the viewpoints of intellectual leaders in theology and instead argue against less informed masses, thereby, defining religion by non intellectuals and slanting the debate unjustly. He argues that leaders in science sometimes trump older scientific baggage and that leaders in theology do the same, so once theological intellectuals are taken into account, people who represent extreme positions like Ken Ham and Eugenie Scott will become irrelevant. Cynthia Tolman notes that religion does not have a method per se partly because religions emerge through time from diverse cultures, but when it comes to Christian theology and ultimate truths, she notes that people often rely on scripture, tradition, reason, and experience to test and gauge what they experience and what they should believe.

The conflict thesis, which holds that religion and science have been in conflict continuously throughout history, was popularized in the 19th century by John William Draper's and Andrew Dickson White's accounts. It was in the 19th century that relationship between science and religion became an actual formal topic of discourse, while before this no one had pitted science against religion or vice versa, though occasional complex interactions had been expressed before the 19th century. Most contemporary historians of science now reject the conflict thesis in its original form and no longer support it. Instead, it has been superseded by subsequent historical research which has resulted in a more nuanced understanding: Historian of science, Gary Ferngren, has stated: "Although popular images of controversy continue to exemplify the supposed hostility of Christianity to new scientific theories, studies have shown that Christianity has often nurtured and encouraged scientific endeavour, while at other times the two have co-existed without either tension or attempts at harmonization. If Galileo and the Scopes trial come to mind as examples of conflict, they were the exceptions rather than the rule."

Most historians today have moved away from a conflict model, which is based mainly on two historical episodes (Galileo and Darwin), toward compatibility theses (either the integration thesis or non-overlapping magisteria) or toward a "complexity" model, because religious figures were on both sides of each dispute and there was no overall aim by any party involved to discredit religion.

An often cited example of conflict, that has been clarified by historical research in the 20th century, was the Galileo affair, whereby interpretations of the Bible were used to attack ideas by Copernicus on heliocentrism. By 1616 Galileo went to Rome to try to persuade Catholic Church authorities not to ban Copernicus' ideas. In the end, a decree of the Congregation of the Index was issued, declaring that the ideas that the Sun stood still and that the Earth moved were "false" and "altogether contrary to Holy Scripture", and suspending Copernicus's "De Revolutionibus" until it could be corrected. Galileo was found "vehemently suspect of heresy", namely of having held the opinions that the Sun lies motionless at the center of the universe, that the Earth is not at its centre and moves. He was required to "abjure, curse and detest" those opinions. However, before all this, Pope Urban VIII had personally asked Galileo to give arguments for and against heliocentrism in a book, and to be careful not to advocate heliocentrism as physically proven since the scientific consensus at the time was that the evidence for heliocentrism was very weak. The Church had merely sided with the scientific consensus of the time. Pope Urban VIII asked that his own views on the matter be included in Galileo's book. Only the latter was fulfilled by Galileo. Whether unknowingly or deliberately, Simplicio, the defender of the Aristotelian/Ptolemaic geocentric view in "Dialogue Concerning the Two Chief World Systems", was often portrayed as an unlearned fool who lacked mathematical training. Although the preface of his book claims that the character is named after a famous Aristotelian philosopher (Simplicius in Latin, Simplicio in Italian), the name "Simplicio" in Italian also has the connotation of "simpleton". Unfortunately for his relationship with the Pope, Galileo put the words of Urban VIII into the mouth of Simplicio. Most historians agree Galileo did not act out of malice and felt blindsided by the reaction to his book. However, the Pope did not take the suspected public ridicule lightly, nor the physical Copernican advocacy. Galileo had alienated one of his biggest and most powerful supporters, the Pope, and was called to Rome to defend his writings.

The actual evidences that finally proved heliocentrism came centuries after Galileo: the stellar aberration of light by James Bradley in the 18th century, the orbital motions of binary stars by William Herschel in the 19th century, the accurate measurement of the stellar parallax in the 19th century, and Newtonian mechanics in the 17th century. According to physicist Christopher Graney, Galileo's own observations did not actually support the Copernican view, but were more consistent with Tycho Brahe's hybrid model where that Earth did not move and everything else circled around it and the Sun.

British philosopher A. C. Grayling, still believes there is competition between science and religions and point to the origin of the universe, the nature of human beings and the possibility of miracles

A modern view, described by Stephen Jay Gould as "non-overlapping magisteria" (NOMA), is that science and religion deal with fundamentally separate aspects of human experience and so, when each stays within its own domain, they co-exist peacefully. While Gould spoke of independence from the perspective of science, W. T. Stace viewed independence from the perspective of the philosophy of religion. Stace felt that science and religion, when each is viewed in its own domain, are both consistent and complete. They originate from different perceptions of reality, as Arnold O. Benz points out, but meet each other, for example, in the feeling of amazement and in ethics.

The USA's National Academy of Science supports the view that science and religion are independent.
Science and religion are based on different aspects of human experience. In science, explanations must be based on evidence drawn from examining the natural world. Scientifically based observations or experiments that conflict with an explanation eventually must lead to modification or even abandonment of that explanation. Religious faith, in contrast, does not depend on empirical evidence, is not necessarily modified in the face of conflicting evidence, and typically involves supernatural forces or entities. Because they are not a part of nature, supernatural entities cannot be investigated by science. In this sense, science and religion are separate and address aspects of human understanding in different ways. Attempts to put science and religion against each other create controversy where none needs to exist.

According to Archbishop John Habgood, both science and religion represent distinct ways of approaching experience and these differences are sources of debate. He views science as descriptive and religion as prescriptive. He stated that if science and mathematics concentrate on what the world "ought to be", in the way that religion does, it may lead to improperly ascribing properties to the natural world as happened among the followers of Pythagoras in the sixth century B.C. In contrast, proponents of a normative moral science take issue with the idea that science has "no" way of guiding "oughts". Habgood also stated that he believed that the reverse situation, where religion attempts to be descriptive, can also lead to inappropriately assigning properties to the natural world. A notable example is the now defunct belief in the Ptolemaic (geocentric) planetary model that held sway until changes in scientific and religious thinking were brought about by Galileo and proponents of his views.

In the view of the Lubavitcher rabbi Menachem Mendel Schneerson, non-Euclidean geometry such as Lobachevsky's hyperbolic geometry and Riemann's elliptic geometry proved that Euclid's axioms, such as, "there is only one straight line between two points", are in fact arbitrary. Therefore, science, which relies on arbitrary axioms, can never refute Torah, which is absolute truth.

According to Ian Barbour, Thomas S. Kuhn asserted that science is made up of paradigms that arise from cultural traditions, which is similar to the secular perspective on religion.

Michael Polanyi asserted that it is merely a commitment to universality that protects against subjectivity and has nothing at all to do with personal detachment as found in many conceptions of the scientific method. Polanyi further asserted that all knowledge is personal and therefore the scientist must be performing a very personal if not necessarily subjective role when doing science. Polanyi added that the scientist often merely follows intuitions of "intellectual beauty, symmetry, and 'empirical agreement'". Polanyi held that science requires moral commitments similar to those found in religion.

Two physicists, Charles A. Coulson and Harold K. Schilling, both claimed that "the methods of science and religion have much in common." Schilling asserted that both fields—science and religion—have "a threefold structure—of experience, theoretical interpretation, and practical application." Coulson asserted that science, like religion, "advances by creative imagination" and not by "mere collecting of facts," while stating that religion should and does "involve critical reflection on experience not unlike that which goes on in science." Religious language and scientific language also show parallels (cf. rhetoric of science).

The "religion and science community" consists of those scholars who involve themselves with what has been called the "religion-and-science dialogue" or the "religion-and-science field." The community belongs to neither the scientific nor the religious community, but is said to be a third overlapping community of interested and involved scientists, priests, clergymen, theologians and engaged non-professionals. Institutions interested in the intersection between science and religion include the Center for Theology and the Natural Sciences, the Institute on Religion in an Age of Science, the Ian Ramsey Centre, and the Faraday Institute. Journals addressing the relationship between science and religion include "Theology and Science" and "Zygon". Eugenie Scott has written that the "science and religion" movement is, overall, composed mainly of theists who have a healthy respect for science and may be beneficial to the public understanding of science. She contends that the "Christian scholarship" movement is not a problem for science, but that the "Theistic science" movement, which proposes abandoning methodological materialism, does cause problems in understanding of the nature of science. The Gifford Lectures were established in 1885 to further the discussion between "natural theology" and the scientific community. This annual series continues and has included William James, John Dewey, Carl Sagan, and many other professors from various fields.

The modern dialogue between religion and science is rooted in Ian Barbour's 1966 book "Issues in Science and Religion". Since that time it has grown into a serious academic field, with academic chairs in the subject area, and two dedicated academic journals, "Zygon" and "Theology and Science". Articles are also sometimes found in mainstream science journals such as "American Journal of Physics"
and "Science".

Philosopher Alvin Plantinga has argued that there is superficial conflict but deep concord between science and religion, and that there is deep conflict between science and naturalism. Plantinga, in his book "Where the Conflict Really Lies: Science, Religion, and Naturalism", heavily contests the linkage of naturalism with science, as conceived by Richard Dawkins, Daniel Dennett and like-minded thinkers; while Daniel Dennett thinks that Plantinga stretches science to an unacceptable extent. Philosopher Maarten Boudry, in reviewing the book, has commented that he resorts to creationism and fails to "stave off the conflict between theism and evolution." Cognitive scientist Justin L. Barrett, by contrast, reviews the same book and writes that "those most needing to hear Plantinga's message may fail to give it a fair hearing for rhetorical rather than analytical reasons."

As a general view, this holds that while interactions are complex between influences of science, theology, politics, social, and economic concerns, the productive engagements between science and religion throughout history should be duly stressed as the norm.

Scientific and theological perspectives often coexist peacefully. Christians and some non-Christian religions have historically integrated well with scientific ideas, as in the ancient Egyptian technological mastery applied to monotheistic ends, the flourishing of logic and mathematics under Hinduism and Buddhism, and the scientific advances made by Muslim scholars during the Ottoman empire. Even many 19th-century Christian communities welcomed scientists who claimed that science was not at all concerned with discovering the ultimate nature of reality. According to Lawrence M. Principe, the Johns Hopkins University Drew Professor of the Humanities, from a historical perspective this points out that much of the current-day clashes occur between limited extremists—both religious and scientistic fundamentalists—over a very few topics, and that the movement of ideas back and forth between scientific and theological thought has been more usual. To Principe, this perspective would point to the fundamentally common respect for written learning in religious traditions of rabbinical literature, Christian theology, and the Islamic Golden Age, including a Transmission of the Classics from Greek to Islamic to Christian traditions which helped spark the Renaissance. Religions have also given key participation in development of modern universities and libraries; centers of learning & scholarship were coincident with religious institutions – whether pagan, Muslim, or Christian.

A fundamental principle of the Bahá'í Faith is the harmony of religion and science. Bahá'í scripture asserts that true science and true religion can never be in conflict. `Abdu'l-Bahá, the son of the founder of the religion, stated that religion without science is superstition and that science without religion is materialism. He also admonished that true religion must conform to the conclusions of science.

Buddhism and science have been regarded as compatible by numerous authors. Some philosophic and psychological teachings found in Buddhism share points in common with modern Western scientific and philosophic thought. For example, Buddhism encourages the impartial investigation of nature (an activity referred to as "Dhamma-Vicaya" in the Pali Canon)—the principal object of study being oneself. Buddhism and science both show a strong emphasis on causality. However, Buddhism does not focus on materialism.

Tenzin Gyatso, the 14th Dalai Lama, mentions that empirical scientific evidence supersedes the traditional teachings of Buddhism when the two are in conflict. In his book "The Universe in a Single Atom" he wrote, "My confidence in venturing into science lies in my basic belief that as in science, so in Buddhism, understanding the nature of reality is pursued by means of critical investigation." He also stated, "If scientific analysis were conclusively to demonstrate certain claims in Buddhism to be false," he says, "then we must accept the findings of science and abandon those claims."

The Jesuits from Europe taught Western math and science to the Chinese bureaucrats in hopes of religious conversion. This process saw several challenges of both European and Chinese spiritual and scientific beliefs. The keynote text of Chinese scientific philosophy, "The Book of Changes" (or Yi Jing) was initially mocked and disregarded by the Westerners. In return, Chinese scholars Dai Zhen and Ji Yun found the concept of phantoms laughable and ridiculous. "The Book of Changes" outlined orthodoxy cosmology in the Qing, including "yin" and "yang" and the five cosmic phases. Sometimes the missionary exploits proved dangerous for the Westerners. Jesuit missionaries and scholars Ferdinand Vervbiest and Adam Schall were punished after using scientific methods to determine the exact time of the 1664 eclipse. However, the European mission eastward did not only cause conflict. Joachim Bouvet, a theologian who held equal respect for both the Bible and the Book of Changes, was productive in his mission of spreading the faith. Jesuit missionaries had the lunisolar calendar model in common with the Chinese, but were tasked with enforcing the seven day week onto their Eastern colleagues. Without this model, the Sabbath could not be respected, and thus this reorientation of the Chinese calendar was necessary for their missionary purposes.     

Among early Christian teachers, Tertullian (c. 160–220) held a generally negative opinion of Greek philosophy, while Origen (c. 185–254) regarded it much more favorably and required his students to read nearly every work available to them.

Earlier attempts at reconciliation of Christianity with Newtonian mechanics appear quite different from later attempts at reconciliation with the newer scientific ideas of evolution or relativity. Many early interpretations of evolution polarized themselves around a "struggle for existence." These ideas were significantly countered by later findings of universal patterns of biological cooperation. According to John Habgood, the universe seems to be a mix of good and evil, beauty and pain, and that suffering may somehow be part of the process of creation. Habgood holds that Christians should not be surprised that suffering may be used creatively by God, given their faith in the symbol of the Cross. 
Robert John Russell has examined consonance and dissonance between modern physics, evolutionary biology, and Christian theology.

Christian philosophers Augustine of Hippo (354–430) and Thomas Aquinas (1225-1274) held that scriptures can have multiple interpretations on certain areas where the matters were far beyond their reach, therefore one should leave room for future findings to shed light on the meanings. The "Handmaiden" tradition, which saw secular studies of the universe as a very important and helpful part of arriving at a better understanding of scripture, was adopted throughout Christian history from early on. Also the sense that God created the world as a self operating system is what motivated many Christians throughout the Middle Ages to investigate nature.

Modern historians of science such as J.L. Heilbron, Alistair Cameron Crombie, David Lindberg, Edward Grant, Thomas Goldstein, and Ted Davis have reviewed the popular notion that medieval Christianity was a negative influence in the development of civilization and science. In their views, not only did the monks save and cultivate the remnants of ancient civilization during the barbarian invasions, but the medieval church promoted learning and science through its sponsorship of many universities which, under its leadership, grew rapidly in Europe in the 11th and 12th centuries. Saint Thomas Aquinas, the Church's "model theologian", not only argued that reason is in harmony with faith, he even recognized that reason can contribute to understanding revelation, and so encouraged intellectual development. He was not unlike other medieval theologians who sought out reason in the effort to defend his faith. Some of today's scholars, such as Stanley Jaki, have claimed that Christianity with its particular worldview, was a crucial factor for the emergence of modern science.

David C. Lindberg states that the widespread popular belief that the Middle Ages was a time of ignorance and superstition due to the Christian church is a "caricature". According to Lindberg, while there are some portions of the classical tradition which suggest this view, these were exceptional cases. It was common to tolerate and encourage critical thinking about the nature of the world. The relation between Christianity and science is complex and cannot be simplified to either harmony or conflict, according to Lindberg. Lindberg reports that "the late medieval scholar rarely experienced the coercive power of the church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led. There was no warfare between science and the church." Ted Peters in "Encyclopedia of Religion" writes that although there is some truth in the "Galileo's condemnation" story but through exaggerations, it has now become "a modern myth perpetuated by those wishing to see warfare between science and religion who were allegedly persecuted by an atavistic and dogma-bound ecclesiastical authority". In 1992, the Catholic Church's seeming vindication of Galileo attracted much comment in the media.

A degree of concord between science and religion can be seen in religious belief and empirical science. The belief that God created the world and therefore humans, can lead to the view that he arranged for humans to know the world. This is underwritten by the doctrine of imago dei. In the words of Thomas Aquinas, "Since human beings are said to be in the image of God in virtue of their having a nature that includes an intellect, such a nature is most in the image of God in virtue of being most able to imitate God".

During the Enlightenment, a period "characterized by dramatic revolutions in science" and the rise of Protestant challenges to the authority of the Catholic Church via individual liberty, the authority of Christian scriptures became strongly challenged. As science advanced, acceptance of a literal version of the Bible became "increasingly untenable" and some in that period presented ways of interpreting scripture according to its spirit on its authority and truth.

After the Black Death in Europe, there occurred a generalized decrease in faith in the Catholic Church. The "Natural Sciences" during the Medieval Era focused largely on scientific arguments. The Copernicans, who were generally a small group of privately-sponsored individuals, who were deemed Heretics by the Church in some instances. Copernicus and his work challenged the view held by the Catholic Church and the common scientific view at the time, yet according to scholar J. L. Heilbron, the Roman Catholic Church sometimes provided financial support to the Copernicans. In doing so, the Church did support and promote scientific research when the goals in question were in alignment with those of the faith, so long as the findings were in line with the rhetoric of the Church. A case example is the Catholic need for an accurate calendar. Calendar reform was a touchy subject: civilians doubted the accuracy of the mathematics and were upset that the process unfairly selected curators of the reform. The Roman Catholic Church needed a precise date for the Easter Sabbath, and thus the Church was highly supportive of calendar reform. The need for the correct date of Easter was also the impetus of cathedral construction. Cathedrals essentially functioned as massive scale sun dials and, in some cases, camera obscuras. They were efficient scientific devices because they rose high enough for their naves to determine the summer and winter solstices. Heilbron contends that as far back as the twelfth century, the Roman Catholic Church was funding scientific discovery and the recovery of ancient Greek scientific texts. However, the Copernican revolution challenged the view held the Catholic Church and placed the Sun at the center of the solar system.

In recent history, the theory of evolution has been at the center of some controversy between Christianity and science. Christians who accept a literal interpretation of the biblical account of creation find incompatibility between Darwinian evolution and their interpretation of the Christian faith. Creation science or scientific creationism is a branch of creationism that attempts to provide scientific support for the Genesis creation narrative in the Book of Genesis and attempts to disprove generally accepted scientific facts, theories and scientific paradigms about the geological history of the Earth, cosmology of the early universe, 
the chemical origins of life and biological evolution. It began in the 1960s as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and falsify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. In 1925, The State of Tennessee passed the Butler Act, which prohibited the teaching of the theory of evolution in all schools in the state. Later that year, a similar law was passed in Mississippi, and likewise, Arkansas in 1927. In 1968, these "anti-monkey" laws were struck down by the Supreme Court of the United States as unconstitutional, "because they established a religious doctrine violating both the First and Fourth Amendments to the Constitution.

Most scientists have rejected creation science for several reasons, including that its claims do not refer to natural causes and cannot be tested. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms. In 2018, the "Orlando Sentinel" reported that "Some private schools in Florida that rely on public funding teach students" Creationism.

Theistic evolution attempts to reconcile Christian beliefs and science by accepting the scientific understanding of the age of the Earth and the process of evolution. It includes a range of beliefs, including views described as evolutionary creationism, which accepts some findings of modern science but also upholds classical religious teachings about God and creation in Christian context.

While refined and clarified over the centuries, the Roman Catholic position on the relationship between science and religion is one of harmony, and has maintained the teaching of natural law as set forth by Thomas Aquinas. For example, regarding scientific study such as that of evolution, the church's unofficial position is an example of theistic evolution, stating that faith and scientific findings regarding human evolution are not in conflict, though humans are regarded as a special creation, and that the existence of God is required to explain both monogenism and the spiritual component of human origins. Catholic schools have included all manners of scientific study in their curriculum for many centuries.

Galileo once stated "The intention of the Holy Spirit is to teach us how to go to heaven, not how the heavens go." In 1981 John Paul II, then pope of the Roman Catholic Church, spoke of the relationship this way: "The Bible itself speaks to us of the origin of the universe and its make-up, not in order to provide us with a scientific treatise, but in order to state the correct relationships of man with God and with the universe. Sacred Scripture wishes simply to declare that the world was created by God, and in order to teach this truth it expresses itself in the terms of the cosmology in use at the time of the writer".

According to Andrew Dickson White's "A History of the Warfare of Science with Theology in Christendom" from the 19th century, a biblical world view affected negatively the progress of science through time. Dickinson also argues that immediately following the Reformation matters were even worse . The interpretations of Scripture by Luther and Calvin became as sacred to their followers as the Scripture itself. For instance, when Georg Calixtus ventured, in interpreting the Psalms, to question the accepted belief that "the waters above the heavens" were contained in a vast receptacle upheld by a solid vault, he was bitterly denounced as heretical. Today, much of the scholarship in which the conflict thesis was originally based is considered to be inaccurate. For instance, the claim that early Christians rejected scientific findings by the Greco-Romans is false, since the "handmaiden" view of secular studies was seen to shed light on theology. This view was widely adapted throughout the early medieval period and afterwards by theologians (such as Augustine) and ultimately resulted in fostering interest in knowledge about nature through time. Also, the claim that people of the Middle Ages widely believed that the Earth was flat was first propagated in the same period that originated the conflict thesis and is still very common in popular culture. Modern scholars regard this claim as mistaken, as the contemporary historians of science David C. Lindberg and Ronald L. Numbers write: "there was scarcely a Christian scholar of the Middle Ages who did not acknowledge [earth's] sphericity and even know its approximate circumference." From the fall of Rome to the time of Columbus, all major scholars and many vernacular writers interested in the physical shape of the earth held a spherical view with the exception of Lactantius and Cosmas.

H. Floris Cohen argued for a biblical Protestant, but not excluding Catholicism, influence on the early development of modern science. He presented Dutch historian R. Hooykaas' argument that a biblical world-view holds all the necessary antidotes for the hubris of Greek rationalism: a respect for manual labour, leading to more experimentation and empiricism, and a supreme God that left nature open to emulation and manipulation. It supports the idea early modern science rose due to a combination of Greek and biblical thought.

Oxford historian Peter Harrison is another who has argued that a biblical worldview was significant for the development of modern science. Harrison contends that Protestant approaches to the book of scripture had significant, if largely unintended, consequences for the interpretation of the book of nature. Harrison has also suggested that literal readings of the Genesis narratives of the Creation and Fall motivated and legitimated scientific activity in seventeenth-century England. For many of its seventeenth-century practitioners, science was imagined to be a means of restoring a human dominion over nature that had been lost as a consequence of the Fall.

Historian and professor of religion Eugene M. Klaaren holds that "a belief in divine creation" was central to an emergence of science in seventeenth-century England. The philosopher Michael Foster has published analytical philosophy connecting Christian doctrines of creation with empiricism. Historian William B. Ashworth has argued against the historical notion of distinctive mind-sets and the idea of Catholic and Protestant sciences. Historians James R. Jacob and Margaret C. Jacob have argued for a linkage between seventeenth-century Anglican intellectual transformations and influential English scientists (e.g., Robert Boyle and Isaac Newton). John Dillenberger and Christopher B. Kaiser have written theological surveys, which also cover additional interactions occurring in the 18th, 19th, and 20th centuries. Philosopher of Religion, Richard Jones, has written a philosophical critique of the "dependency thesis" which assumes that modern science emerged from Christian sources and doctrines. Though he acknowledges that modern science emerged in a religious framework, that Christianity greatly elevated the importance of science by sanctioning and religiously legitimizing it in the medieval period, and that Christianity created a favorable social context for it to grow; he argues that direct Christian beliefs or doctrines were not primary sources of scientific pursuits by natural philosophers, nor was Christianity, in and of itself, exclusively or directly necessary in developing or practicing modern science.

Oxford University historian and theologian John Hedley Brooke wrote that "when natural philosophers referred to "laws" of nature, they were not glibly choosing that metaphor. Laws were the result of legislation by an intelligent deity. Thus the philosopher René Descartes (1596–1650) insisted that he was discovering the "laws that God has put into nature." Later Newton would declare that the regulation of the solar system presupposed the "counsel and dominion of an intelligent and powerful Being." Historian Ronald L. Numbers stated that this thesis "received a boost" from mathematician and philosopher Alfred North Whitehead's "Science and the Modern World" (1925). Numbers has also argued, "Despite the manifest shortcomings of the claim that Christianity gave birth to science—most glaringly, it ignores or minimizes the contributions of ancient Greeks and medieval Muslims—it too, refuses to succumb to the death it deserves." The sociologist Rodney Stark of Baylor University, argued in contrast that "Christian theology was essential for the rise of science."

Protestantism had an important influence on science. According to the Merton Thesis there was a positive correlation between the rise of Puritanism and Protestant Pietism on the one hand and early experimental science on the other. The Merton Thesis has two separate parts: Firstly, it presents a theory that science changes due to an accumulation of observations and improvement in experimental techniques and methodology; secondly, it puts forward the argument that the popularity of science in 17th-century England and the religious demography of the Royal Society (English scientists of that time were predominantly Puritans or other Protestants) can be explained by a correlation between Protestantism and the scientific values. In his theory, Robert K. Merton focused on English Puritanism and German Pietism as having been responsible for the development of the scientific revolution of the 17th and 18th centuries. Merton explained that the connection between religious affiliation and interest in science was the result of a significant synergy between the ascetic Protestant values and those of modern science. Protestant values encouraged scientific research by allowing science to study God's influence on the world and thus providing a religious justification for scientific research.

In "Reconciling Science and Religion: The Debate in Early-twentieth-century Britain", historian of biology Peter J. Bowler argues that in contrast to the conflicts between science and religion in the U.S. in the 1920s (most famously the Scopes Trial), during this period Great Britain experienced a concerted effort at reconciliation, championed by intellectually conservative scientists, supported by liberal theologians but opposed by younger scientists and secularists and conservative Christians. These attempts at reconciliation fell apart in the 1930s due to increased social tensions, moves towards neo-orthodox theology and the acceptance of the modern evolutionary synthesis.

In the 20th century, several ecumenical organizations promoting a harmony between science and Christianity were founded, most notably the American Scientific Affiliation, The Biologos Foundation, Christians in Science, The Society of Ordained Scientists, and The Veritas Forum.

The historical process of Confucianism has largely been antipathic towards scientific discovery. However the religio-philosophical system itself is more neutral on the subject than such an analysis might suggest. In his writings On Heaven, Xunzi espoused a proto-scientific world view. However, during the Han Synthesis the more anti-empirical Mencius was favored and combined with Daoist skepticism regarding the nature of reality. Likewise, during the Medieval period, Zhu Xi argued against technical investigation and specialization proposed by Chen Liang. After contact with the West, scholars such as Wang Fuzhi would rely on Buddhist/Daoist skepticism to denounce all science as a subjective pursuit limited by humanity's fundamental ignorance of the true nature of the world. After the May Fourth Movement, attempts to modernize Confucianism and reconcile it with scientific understanding were attempted by many scholars including Feng Youlan and Xiong Shili. Given the close relationship that Confucianism shares with Buddhism, many of the same arguments used to reconcile Buddhism with science also readily translate to Confucianism. However, modern scholars have also attempted to define the relationship between science and Confucianism on Confucianism's own terms and the results have usually led to the conclusion that Confucianism and science are fundamentally compatible.

In Hinduism, the dividing line between objective sciences and spiritual knowledge ("adhyatma vidya") is a linguistic paradox. Hindu scholastic activities and ancient Indian scientific advancements were so interconnected that many Hindu scriptures are also ancient scientific manuals and vice versa. In 1835, English was made the primary language for teaching in higher education in India, exposing Hindu scholars to Western secular ideas; this started a renaissance regarding religious and philosophical thought. Hindu sages maintained that logical argument and rational proof using Nyaya is the way to obtain correct knowledge. The scientific level of understanding focuses on how things work and from where they originate, while Hinduism strives to understand the ultimate purposes for the existence of living things. To obtain and broaden the knowledge of the world for spiritual perfection, many refer to the Bhāgavata for guidance because it draws upon a scientific and theological dialogue. Hinduism offers methods to correct and transform itself in course of time. For instance, Hindu views on the development of life include a range of viewpoints in regards to evolution, creationism, and the origin of life within the traditions of Hinduism. For instance, it has been suggested that Wallace-Darwininan evolutionary thought was a part of Hindu thought centuries before modern times. The Shankara and the Sāmkhya did not have a problem with the theory of evolution, but instead, argued about the existence of God and what happened after death. These two distinct groups argued among each other's philosophies because of their texts, not the idea of evolution. With the publication of Darwin's "On the Origin of Species", many Hindus were eager to connect their scriptures to Darwinism, finding similarities between Brahma's creation, Vishnu's incarnations, and evolution theories.

Samkhya, the oldest school of Hindu philosophy prescribes a particular method to analyze knowledge. According to Samkhya, all knowledge is possible through three means of valid knowledge –

Nyaya, the Hindu school of logic, accepts all these 3 means and in addition accepts one more – "Upamāna" (comparison).

The accounts of the emergence of life within the universe vary in description, but classically the deity called Brahma, from a Trimurti of three deities also including Vishnu and Shiva, is described as performing the act of 'creation', or more specifically of 'propagating life within the universe' with the other two deities being responsible for 'preservation' and 'destruction' (of the universe) respectively. In this respect some Hindu schools do not treat the scriptural creation myth literally and often the creation stories themselves do not go into specific detail, thus leaving open the possibility of incorporating at least some theories in support of evolution. Some Hindus find support for, or foreshadowing of evolutionary ideas in scriptures, namely the Vedas.

The incarnations of Vishnu (Dashavatara) is almost identical to the scientific explanation of the sequence of biological evolution of man and animals. The sequence of avatars starts from an aquatic organism (Matsya), to an amphibian (Kurma), to a land-animal (Varaha), to a humanoid (Narasimha), to a dwarf human (Vamana), to 5 forms of well developed human beings (Parashurama, Rama, Balarama/Buddha, Krishna, Kalki) who showcase an increasing form of complexity (Axe-man, King, Plougher/Sage, wise Statesman, mighty Warrior). In fact, many Hindu gods are represented with features of animals as well as those of humans, leading many Hindus to easily accept evolutionary links between animals and humans. In India, the home country of Hindus, educated Hindus widely accept the theory of biological evolution. In a survey of 909 people, 77% of respondents in India agreed with Charles Darwin's Theory of Evolution, and 85 per cent of God-believing people said they believe in evolution as well.

As per Vedas, another explanation for the creation is based on the five elements: earth, water, fire, air and aether.
The Hindu religion traces its beginnings to the Vedas. Everything that is established in the Hindu faith such as the gods and goddesses, doctrines, chants, spiritual insights, etc. flow from the poetry of Vedic hymns. The Vedas offer an honor to the sun and moon, water and wind, and to the order in Nature that is universal. This naturalism is the beginning of what further becomes the connection between Hinduism and science.

From an Islamic standpoint, science, the study of nature, is considered to be linked to the concept of "Tawhid" (the Oneness of God), as are all other branches of knowledge. In Islam, nature is not seen as a separate entity, but rather as an integral part of Islam's holistic outlook on God, humanity, and the world. The Islamic view of science and nature is continuous with that of religion and God. This link implies a sacred aspect to the pursuit of scientific knowledge by Muslims, as nature itself is viewed in the Qur'an as a compilation of signs pointing to the Divine. It was with this understanding that science was studied and understood in Islamic civilizations, specifically during the eighth to sixteenth centuries, prior to the colonization of the Muslim world. Robert Briffault, in "The Making of Humanity", asserts that the very existence of science, as it is understood in the modern sense, is rooted in the scientific thought and knowledge that emerged in Islamic civilizations during this time. Ibn al-Haytham, an Arab Muslim, was an early proponent of the concept that a hypothesis must be proved by experiments based on confirmable procedures or mathematical evidence—hence understanding the scientific method 200 years before Renaissance scientists. Ibn al-Haytham described his theology: 
With the decline of Islamic Civilizations in the late Middle Ages and the rise of Europe, the Islamic scientific tradition shifted into a new period. Institutions that had existed for centuries in the Muslim world looked to the new scientific institutions of European powers. This changed the practice of science in the Muslim world, as Islamic scientists had to confront the western approach to scientific learning, which was based on a different philosophy of nature. From the time of this initial upheaval of the Islamic scientific tradition to the present day, Muslim scientists and scholars have developed a spectrum of viewpoints on the place of scientific learning within the context of Islam, none of which are universally accepted or practiced. However, most maintain the view that the acquisition of knowledge and scientific pursuit in general is not in disaccord with Islamic thought and religious belief.

During the thirteenth century, the Caliphate system in the Islamic Empire fell, and scientific discovery thrived. The Islamic Civilization has a long history of scientific advancement; and their theological practices catalyzed a great deal of scientific discovery. In fact, it was due to necessities of Muslim worship and their vast empire that much science and philosophy was created. People needed to know in which direction they needed to pray toward in order to face Mecca. Many historians through time have asserted that all modern science originates from ancient Greek scholarship; but scholars like Martin Bernal have claimed that most ancient Greek scholarship relied heavily on the work of scholars from ancient Egypt and the Levant. Ancient Egypt was the foundational site of the Hermetic School, which believed that the sun represented an invisible God. Amongst other things, Islamic civilization was key because it documented and recorded Greek scholarship.variant

The Ahmadiyya movement emphasize that "there is no contradiction between Islam and science". For example, Ahmadi Muslims universally accept in principle the process of evolution, albeit divinely guided, and actively promote it. Over the course of several decades the movement has issued various publications in support of the scientific concepts behind the process of evolution, and frequently engages in promoting how religious scriptures, such as the Qur'an, supports the concept. For general purposes, the second Khalifa of the community, Mirza Basheer-ud-Din Mahmood Ahmad says:
The Holy Quran directs attention towards science, time and again, rather than evoking prejudice against it. The Quran has never advised against studying science, lest the reader should become a non-believer; because it has no such fear or concern. The Holy Quran is not worried that if people will learn the laws of nature its spell will break. The Quran has not prevented people from science, rather it states, "Say, 'Reflect on what is happening in the heavens and the earth.'" (Al Younus)

Jainism does not support belief in a creator deity. According to Jain doctrine, the universe and its constituents – soul, matter, space, time, and principles of motion have always existed (a static universe similar to that of Epicureanism and steady state cosmological model). All the constituents and actions are governed by universal natural laws. It is not possible to create matter out of nothing and hence the sum total of matter in the universe remains the same (similar to law of conservation of mass). Similarly, the soul of each living being is unique and uncreated and has existed since beginningless time.

The Jain theory of causation holds that a cause and its effect are always identical in nature and hence a conscious and immaterial entity like God cannot create a material entity like the universe. Furthermore, according to the Jain concept of divinity, any soul who destroys its karmas and desires, achieves liberation. A soul who destroys all its passions and desires has no desire to interfere in the working of the universe. Moral rewards and sufferings are not the work of a divine being, but a result of an innate moral order in the cosmos; a self-regulating mechanism whereby the individual reaps the fruits of his own actions through the workings of the karmas.

Through the ages, Jain philosophers have adamantly rejected and opposed the concept of creator and omnipotent God and this has resulted in Jainism being labeled as "nastika darsana" or atheist philosophy by the rival religious philosophies. The theme of non-creationism and absence of omnipotent God and divine grace runs strongly in all the philosophical dimensions of Jainism, including its cosmology, karma, moksa and its moral code of conduct. Jainism asserts a religious and virtuous life is possible without the idea of a creator god.

Since 1901–2013, 22% of all Nobel prizes have been awarded to Jews despite them being less than 1% of the world population.

Between 1901 and 2000, 654 Laureates belonged to 28 different religions. Most (65%) have identified Christianity in its various forms as their religious preference. Specifically on the science related prizes, Christians have won a total of 73% of all the Chemistry, 65% in Physics, 62% in Medicine, and 54% in all Economics awards. Jews have won 17% of the prizes in Chemistry, 26% in Medicine, and 23% in Physics. Atheists, Agnostics, and Freethinkers have won 7% of the prizes in Chemistry, 9% in Medicine, and 5% in Physics. Muslims have won 13 prizes (three were in scientific categories).

In 1916, 1,000 leading American scientists were randomly chosen from "American Men of Science" and 42% believed God existed, 42% disbelieved, and 17% had doubts/did not know; however when the study was replicated 80 years later using "American Men and Women of Science" in 1996, results were very much the same with 39% believing God exists, 45% disbelieved, and 15% had doubts/did not know. In the same 1996 survey, for scientists in the fields of biology, mathematics, and physics/astronomy, belief in a god that is "in intellectual and affective communication with humankind" was most popular among mathematicians (about 45%) and least popular among physicists (about 22%). In total, in terms of belief toward a personal god and personal immortality, about 60% of United States scientists in these fields expressed either disbelief or agnosticism and about 40% expressed belief. This compared with 62.9% in 1914 and 33% in 1933.

A survey conducted between 2005 and 2007 by Elaine Howard Ecklund of University at Buffalo, The State University of New York of 1,646 natural and social science professors at 21 US research universities found that, in terms of belief in God or a higher power, more than 60% expressed either disbelief or agnosticism and more than 30% expressed belief. More specifically, nearly 34% answered "I do not believe in God" and about 30% answered "I do not know if there is a God and there is no way to find out." In the same study, 28% said they believed in God and 8% believed in a higher power that was not God. Ecklund stated that scientists were often able to consider themselves spiritual without religion or belief in god. Ecklund and Scheitle concluded, from their study, that the individuals from non-religious backgrounds disproportionately had self-selected into scientific professions and that the assumption that becoming a scientist necessarily leads to loss of religion is untenable since the study did not strongly support the idea that scientists had dropped religious identities due to their scientific training. Instead, factors such as upbringing, age, and family size were significant influences on religious identification since those who had religious upbringing were more likely to be religious and those who had a non-religious upbringing were more likely to not be religious. The authors also found little difference in religiosity between social and natural scientists.

Many studies have been conducted in the United States and have generally found that scientists are less likely to believe in God than are the rest of the population. Precise definitions and statistics vary, with some studies concluding that about of scientists in the U.S. are atheists, agnostic, and have some belief in God (although some might be deistic, for example). This is in contrast to the more than roughly of the general population that believe in some God in the United States. Other studies on scientific organizations like the AAAS show that 51% of their scientists believe in either God or a higher power and 48% having no religion. Belief also varies slightly by field. Two surveys on physicists, geoscientists, biologists, mathematicians, and chemists have noted that, from those specializing in these fields, physicists had lowest percentage of belief in God (29%) while chemists had highest (41%). Other studies show that among members of the National Academy of Sciences, concerning the existence of a personal god who answers prayer, 7% expressed belief, 72% expressed disbelief, and 21% were agnostic, however Eugenie Scott argued that there are methodological issues in the study, including ambiguity in the questions. A study with simplified wording to include impersonal or non-interventionist ideas of God concluded that 40% of leading scientists in the US scientists believe in a god.

In terms of perceptions, most social and natural scientists from 21 American universities did not perceive conflict between science and religion, while 37% did. However, in the study, scientists who had experienced limited exposure to religion tended to perceive conflict. In the same study they found that nearly one in five atheist scientists who are parents (17%) are part of religious congregations and have attended a religious service more than once in the past year. Some of the reasons for doing so are their scientific identity (wishing to expose their children to all sources of knowledge so they can make up their own minds), spousal influence, and desire for community.

A 2009 report by the Pew Research Center found that members of the American Association for the Advancement of Science (AAAS) were "much less religious than the general public," with 51% believing in some form of deity or higher power. Specifically, 33% of those polled believe in God, 18% believe in a universal spirit or higher power, and 41% did not believe in either God or a higher power. 48% say they have a religious affiliation, equal to the number who say they are not affiliated with any religious tradition. 17% were atheists, 11% were agnostics, 20% were nothing in particular, 8% were Jewish, 10% were Catholic, 16% were Protestant, 4% were Evangelical, 10% were other religion. The survey also found younger scientists to be "substantially more likely than their older counterparts to say they believe in God". Among the surveyed fields, chemists were the most likely to say they believe in God.

Elaine Ecklund conducted a study from 2011 to 2014 involving the general US population, including rank and file scientists, in collaboration with the American Association for the Advancement of Science (AAAS). The study noted that 76% of the scientists identified with a religious tradition. 85% of evangelical scientists had no doubts about the existence of God, compared to 35% of the whole scientific population. In terms of religion and science, 85% of evangelical scientists saw no conflict (73% collaboration, 12% independence), while 75% of the whole scientific population saw no conflict (40% collaboration, 35% independence).

Religious beliefs of US professors were examined using a nationally representative sample of more than 1,400 professors. They found that in the social sciences: 23% did not believe in God, 16% did not know if God existed, 43% believed God existed, and 16% believed in a higher power. Out of the natural sciences: 20% did not believe in God, 33% did not know if God existed, 44% believed God existed, and 4% believed in a higher power. Overall, out of the whole study: 10% were atheists, 13% were agnostic, 19% believe in a higher power, 4% believe in God some of the time, 17% had doubts but believed in God, 35% believed in God and had no doubts.

Farr Curlin, a University of Chicago Instructor in Medicine and a member of the MacLean Center for Clinical Medical Ethics, noted in a study that doctors tend to be science-minded religious people. He helped author a study that "found that 76 percent of doctors believe in God and 59 percent believe in some sort of afterlife." Furthermore, "90 percent of doctors in the United States attend religious services at least occasionally, compared to 81 percent of all adults." He reasoned, "The responsibility to care for those who are suffering and the rewards of helping those in need resonate throughout most religious traditions."

Physicians in the United States, by contrast, are much more religious than scientists, with 76% stating a belief in God.

According to the Study of Secularism in Society and Culture's report on 1,100 scientists in India: 66% are Hindu, 14% did not report a religion, 10% are atheist/no religion, 3% are Muslim, 3% are Christian, 4% are Buddhist, Sikh or other. 39% have a belief in a god, 6% have belief in a god sometimes, 30% do not believe in a god but believe in a higher power, 13% do not know if there is a god, and 12% do not believe in a god. 49% believe in the efficacy of prayer, 90% strongly agree or somewhat agree with approving degrees in Ayurvedic medicine. Furthermore, the term "secularism" is understood to have diverse and simultaneous meanings among Indian scientists: 93% believe it to be tolerance of religions and philosophies, 83% see it as involving separation of church and state, 53% see it as not identifying with religious traditions, 40% see it as absence of religious beliefs, and 20% see it as atheism. Accordingly, 75% of Indian scientists had a "secular" outlook in terms of being tolerant of other religions.

According to the Religion Among Scientists in International Context (RASIC) study on 1,581 scientists from the United Kingdom and 1,763 scientists from India, along with 200 interviews: 65% of U.K. scientists identified as nonreligious and only 6% of Indian scientists identify as nonreligious, 12% of scientists in the U.K. attend religious services on a regular basis and 32% of scientists in India do. In terms of the Indian scientists, 73% of scientists responded that there are basic truths in many religions, 27% said they believe in God and 38% expressed belief in a higher power of some kind. In terms of perceptions of conflict between science and religion, less than half of both U.K. scientists (38%) and Indian scientists (18%) perceived conflict between religion and science.

Global studies which have pooled data on religion and science from 1981–2001, have noted that countries with high religiosity also have stronger faith in science, while less religious countries have more skepticism of the impact of science and technology. The United States is noted there as distinctive because of greater faith in both God and scientific progress. Other research cites the National Science Foundation's finding that America has more favorable public attitudes towards science than Europe, Russia, and Japan despite differences in levels of religiosity in these cultures.

A study conducted on adolescents from Christian schools in Northern Ireland, noted a positive relationship between attitudes towards Christianity and science once attitudes towards scientism and creationism were accounted for.

A study on people from Sweden concludes that though the Swedes are among the most non-religious, paranormal beliefs are prevalent among both the young and adult populations. This is likely due to a loss of confidence in institutions such as the Church and Science.

Concerning specific topics like creationism, it is not an exclusively American phenomenon. A poll on adult Europeans revealed that 40% believed in naturalistic evolution, 21% in theistic evolution, 20% in special creation, and 19% are undecided; with the highest concentrations of young earth creationists in Switzerland (21%), Austria (20%), Germany (18%). Other countries such as Netherlands, Britain, and Australia have experienced growth in such views as well.

According to a 2015 Pew Research Center Study on the public perceptions on science, people's perceptions on conflict with science have more to do with their perceptions of other people's beliefs than their own personal beliefs. For instance, the majority of people with a religious affiliation (68%) saw no conflict between their own personal religious beliefs and science while the majority of those without a religious affiliation (76%) perceived science and religion to be in conflict. The study noted that people who are not affiliated with any religion, also known as "religiously unaffiliated", often have supernatural beliefs and spiritual practices despite them not being affiliated with any religion and also that "just one-in-six religiously unaffiliated adults (16%) say their own religious beliefs conflict with science." Furthermore, the study observed, "The share of all adults who perceive a conflict between science and their own religious beliefs has declined somewhat in recent years, from 36% in 2009 to 30% in 2014. Among those who are affiliated with a religion, the share of people who say there is a conflict between science and their personal religious beliefs dropped from 41% to 34% during this period."

The 2013 MIT Survey on Science, Religion and Origins examined the views of religious people in America on origins science topics like evolution, the Big Bang, and perceptions of conflicts between science and religion. It found that a large majority of religious people see no conflict between science and religion and only 11% of religious people belong to religions openly rejecting evolution. The fact that the gap between personal and official beliefs of their religions is so large suggests that part of the problem, might be defused by people learning more about their own religious doctrine and the science it endorses, thereby bridging this belief gap. The study concluded that "mainstream religion and mainstream science are neither attacking one another nor perceiving a conflict." Furthermore, they note that this conciliatory view is shared by most leading science organizations such as the American Association for the Advancement of Science (AAAS).

A study was made in collaboration with the American Association for the Advancement of Science (AAAS) collecting data on the general public from 2011 to 2014, with the focus on evangelicals and evangelical scientists. Even though evangelicals make up only 26% of the US population, the study found that nearly 70 percent of all evangelical Christians do not view science and religion as being in conflict with each other (48% saw them as complementary and 21% saw them as independent) while 73% of the general US population saw no conflict either.

Other lines of research on perceptions of science among the American public conclude that most religious groups see no general epistemological conflict with science and they have no differences with nonreligious groups in the propensity of seeking out scientific knowledge, although there may be subtle epistemic or moral conflicts when scientists make counterclaims to religious tenets. Findings from the Pew Center note similar findings and also note that the majority of Americans (80–90%) show strong support for scientific research, agree that science makes society and individual's lives better, and 8 in 10 Americans would be happy if their children were to become scientists. Even strict creationists tend to have very favorable views on science.

According to a 2007 poll by the Pew Forum, "while large majorities of Americans respect science and scientists, they are not always willing to accept scientific findings that squarely contradict their religious beliefs." The Pew Forum states that specific factual disagreements are "not common today", though 40% to 50% of Americans do not accept the evolution of humans and other living things, with the "strongest opposition" coming from evangelical Christians at 65% saying life did not evolve. 51% of the population believes humans and other living things evolved: 26% through natural selection only, 21% somehow guided, 4% don't know. In the U.S., biological evolution is the only concrete example of conflict where a significant portion of the American public denies scientific consensus for religious reasons. In terms of advanced industrialized nations, the United States is the most religious.

A 2009 study from the Pew Research Center on Americans perceptions of science, showed a broad consensus that most Americans, including most religious Americans, hold scientific research and scientists themselves in high regard. The study showed that 84% of Americans say they view science as having a mostly positive impact on society. Among those who attend religious services at least once a week, the number is roughly the same at 80%. Furthermore, 70% of U.S. adults think scientists contribute "a lot" to society.

A 2011 study on a national sample of US college students examined whether these students viewed the science / religion relationship as reflecting primarily conflict, collaboration, or independence. The study concluded that the majority of undergraduates in both the natural and social sciences do not see conflict between science and religion. Another finding in the study was that it is more likely for students to move away from a conflict perspective to an independence or collaboration perspective than towards a conflict view.

In the US, people who had no religious affiliation were no more likely than the religious population to have New Age beliefs and practices.

By tradition:

In the US:




</doc>
<doc id="29268" url="https://en.wikipedia.org/wiki?curid=29268" title="Stephen Sondheim">
Stephen Sondheim

Stephen Joshua Sondheim (; born March 22, 1930) is an American composer and lyricist known for his work in musical theatre.

One of the most important figures in 20th-century musical theatre, Sondheim has been praised for having “reinvented the American musical" with shows that tackle "unexpected themes that range far beyond the [genre's] traditional subjects" with "music and lyrics of unprecedented complexity and sophistication." His shows have been acclaimed for addressing "darker, more harrowing elements of the human experience," with songs often tinged with "ambivalence" about various aspects of life. His best-known works as composer and lyricist include "A Funny Thing Happened on the Way to the Forum" (1962), "Company" (1970), "Follies" (1971), "A Little Night Music" (1973), "" (1979), "Sunday in the Park with George" (1984), and "Into the Woods" (1987). He is also known for writing the lyrics for "West Side Story" (1957) and "Gypsy" (1959).

He has received an Academy Award, eight Tony Awards (more than any other composer, including a Special Tony Award for Lifetime Achievement in the Theatre), eight Grammy Awards, a Pulitzer Prize, a Laurence Olivier Award, and a 2015 Presidential Medal of Freedom. In 2010, the former Henry Miller's Theater on Broadway was renamed the Stephen Sondheim Theatre; in 2019, it was announced that the Queen's Theatre in the West End of London would be renamed the Sondheim Theatre at the end of the year. Sondheim has written film music, contributing "Goodbye for Now" for Warren Beatty's 1981 "Reds". He wrote five songs for 1990's "Dick Tracy", including "Sooner or Later (I Always Get My Man)", sung in the film by Madonna, which won the Academy Award for Best Original Song. Film adaptations of Sondheim's work include "West Side Story" (1961), "" (2007), and "Into the Woods" (2014).

Sondheim is Jewish, having been born into a Jewish family in New York City, the son of Etta Janet ("Foxy", née Fox; 1897–1992) and Herbert Sondheim (1895–1966). His father manufactured dresses designed by his mother. The composer grew up on the Upper West Side of Manhattan and, after his parents divorced, on a farm near Doylestown, Pennsylvania. As the only child of well-to-do parents living in the San Remo on Central Park West, he was described in Meryle Secrest's biography ("Stephen Sondheim: A Life") as an isolated, emotionally neglected child. When he lived in New York, Sondheim attended the Ethical Culture Fieldston School. He later attended the New York Military Academy and George School, a private Quaker preparatory school in Bucks County, Pennsylvania where he wrote his first musical, "By George," and from which he graduated in 1946. Sondheim spent several summers at Camp Androscoggin. He later matriculated to Williams College and graduated in 1950.

He traces his interest in theatre to "Very Warm for May", a Broadway musical he saw when he was nine. "The curtain went up and revealed a piano," Sondheim recalled. "A butler took a duster and brushed it up, tinkling the keys. I thought that was thrilling."

When Sondheim was ten years old, his father (already a distant figure) had left his mother for another woman (Alicia, with whom he had two sons). Herbert sought custody of Stephen but was unsuccessful. Sondheim explained to biographer Secrest that he was "what they call an institutionalized child, meaning one who has no contact with any kind of family. You're in, though it's luxurious, you're in an environment that supplies you with everything but human contact. No brothers and sisters, no parents, and yet plenty to eat, and friends to play with and a warm bed, you know?"

Sondheim detested his mother, who was said to be psychologically abusive and projected her anger from her failed marriage on her son: "When my father left her, she substituted me for him. And she used me the way she used him, to come on to and to berate, beat up on, you see. What she did for five years was treat me like dirt, but come on to me at the same time." She once wrote him a letter saying that the "only regret [she] ever had was giving him birth". When his mother died in the spring of 1992, Sondheim did not attend her funeral. He had already been estranged from her for nearly 20 years.

When Sondheim was about ten years old (around the time of his parents' divorce), he became friends with James Hammerstein, son of lyricist and playwright Oscar Hammerstein II. The elder Hammerstein became Sondheim's surrogate father, influencing him profoundly and developing his love of musical theatre. Sondheim met Hal Prince, who would direct many of his shows, at the opening of "South Pacific," Hammerstein's musical with Richard Rodgers. The comic musical he wrote at George School, "By George", was a success among his peers and buoyed the young songwriter's self-esteem. When Sondheim asked Hammerstein to evaluate it as though he had no knowledge of its author, he said it was the worst thing he had ever seen: "But if you want to know why it's terrible, I'll tell you." They spent the rest of the day going over the musical, and Sondheim later said, "In that afternoon I learned more about songwriting and the musical theater than most people learn in a lifetime."

Hammerstein designed a course of sorts for Sondheim on constructing a musical. He had the young composer write four musicals, each with one of the following conditions:

None of the "assignment" musicals were produced professionally. "High Tor" and "Mary Poppins" have never been produced: The rights holder for the original "High Tor" refused permission, and "Mary Poppins" was unfinished.

Sondheim began attending Williams College, a liberal arts college in Williamstown, Massachusetts whose theatre program attracted him. His first teacher there was Robert Barrow:

 ... everybody hated him because he was very dry, and I thought he was wonderful because he was very dry. And Barrow made me realize that all my romantic views of art were nonsense. I had always thought an angel came down and sat on your shoulder and whispered in your ear 'dah-dah-dah-DUM.' Never occurred to me that art was something worked out. And suddenly it was skies opening up. As soon as you find out what a leading tone is, you think, Oh my God. What a diatonic scale is – Oh my God! The logic of it. And, of course, what that meant to me was: Well, I can do that. Because you just don't know. You think it's a talent, you think you're born with this thing. What I've found out and what I believed is that everybody is talented. It's just that some people get it developed and some don't.

The composer told Meryle Secrest, "I just wanted to study composition, theory, and harmony without the attendant musicology that comes in graduate school. But I knew I wanted to write for the theatre, so I wanted someone who did not disdain theatre music." Barrow suggested that Sondheim study with Milton Babbitt, whom Sondheim described as "a frustrated show composer" with whom he formed "a perfect combination". When he met Babbitt, he was working on a musical for Mary Martin based on the myth of Helen of Troy. Sondheim and Babbitt would meet once a week in New York City for four hours (at the time, Babbitt was teaching at Princeton University). According to Sondheim, they spent the first hour dissecting Rodgers and Hart or George Gershwin or studying Babbitt's favorites (Buddy DeSylva, Lew Brown and Ray Henderson). They then proceeded to other forms of music (such as Mozart's Jupiter Symphony), critiquing them the same way. Babbitt and Sondheim, fascinated by mathematics, studied songs by a variety of composers (especially Jerome Kern). Sondheim told Secrest that Kern had the ability "to develop a single motif through tiny variations into a long and never boring line and his maximum development of the minimum of material". He said about Babbitt, "I am his maverick, his one student who went into the popular arts with all his serious artillery". At Williams, Sondheim wrote a musical adaption of "Beggar on Horseback" (a 1924 play by George S. Kaufman and Marc Connelly, with permission from Kaufman) which had three performances. A member of the Beta Theta Pi fraternity, he graduated "magna cum laude" in 1950.

"A few painful years of struggle" followed, when Sondheim auditioned songs, lived in his father's dining room to save money and spent time in Hollywood writing for the television series "Topper". He devoured 1940s and 1950s films, and has called cinema his "basic language"; his film knowledge got him through "The $64,000 Question" contestant tryouts. Sondheim dislikes movie musicals, favoring classic dramas such as "Citizen Kane", "The Grapes of Wrath" and "A Matter of Life and Death": "Studio directors like Michael Curtiz and Raoul Walsh ... were heroes of mine. They went from movie to movie to movie, and every third movie was good and every fifth movie was great. There wasn't any cultural pressure to make art".

At age 22, Sondheim had finished the four shows requested by Hammerstein. Julius and Philip Epstein's "Front Porch in Flatbush", unproduced at the time, was being shopped around by Lemuel (Lem) Ayers. Ayers approached Frank Loesser and another composer, who turned him down. Ayers and Sondheim met as ushers at a wedding, and Ayers commissioned Sondheim for three songs for the show; Julius Epstein flew in from California and hired Sondheim, who worked with him in California for four or five months. After eight auditions for backers, half the money needed was raised. The show, retitled "Saturday Night", was intended to open during the 1954–55 Broadway season; however, Ayers died of leukemia in his early forties. The rights transferred to his widow, Shirley, and due to her inexperience the show did not continue as planned; it opened off-Broadway in 2000. Sondheim later said, "I don't have any emotional reaction to "Saturday Night" at all – except fondness. It's not bad stuff for a 23-year-old. There are some things that embarrass me so much in the lyrics – the missed accents, the obvious jokes. But I decided, leave it. It's my baby pictures. You don't touch up a baby picture – you're a baby!"

Burt Shevelove invited Sondheim to a party; Sondheim arrived before him, and knew no one else well. He saw a familiar face: Arthur Laurents, who had seen one of the auditions of "Saturday Night", and they began talking. Laurents told him he was working on a musical version of "Romeo and Juliet" with Leonard Bernstein, but they needed a lyricist; Betty Comden and Adolph Green, who were supposed to write the lyrics, were under contract in Hollywood. He said that although he was not a big fan of Sondheim's music, he enjoyed the lyrics from "Saturday Night" and he could audition for Bernstein. The following day, Sondheim met and played for Bernstein, who said he would let him know. The composer wanted to write music and lyrics; after consulting with Hammerstein, Bernstein told Sondheim he could write music later. In 1957, "West Side Story" opened; directed by Jerome Robbins, it ran for 732 performances. Sondheim has expressed dissatisfaction with his lyrics, saying that they do not always fit the characters and are sometimes too consciously poetic. Initially Bernstein was also credited as a co-writer of the lyrics; later, however, Bernstein offered Sondheim solo credit, as Sondheim had essentially done all of them. "The New York Times" review of the show never even mentioned the lyrics. Sondheim described the division of the royalties, saying that Bernstein received three percent and he received one percent. Bernstein suggested evening the percentage at two percent each, but Sondheim refused because he was satisfied just getting the credit. Sondheim later said he wished "someone stuffed a handkerchief in my mouth because it would have been nice to get that extra percentage".

After "West Side Story" opened, Shevelove lamented the lack of "low-brow comedy" on Broadway and mentioned a possible musical based on Plautus' Roman comedies. When Sondheim was interested in the idea he called a friend, Larry Gelbart, to co-write the script. The show went through a number of drafts, and was interrupted briefly by Sondheim's next project.

In 1959, Sondheim was approached by Laurents and Robbins for a musical version of Gypsy Rose Lee's memoir after Irving Berlin and Cole Porter turned it down. Sondheim agreed, but Ethel Merman – cast as Mama Rose – had just finished "Happy Hunting" with an unknown composer (Harold Karr) and lyricist (Matt Dubey). Although Sondheim wanted to write the music and lyrics, Merman refused to let another first-time composer write for her and demanded that Jule Styne write the music. Sondheim, concerned that writing lyrics again would pigeonhole him as a lyricist, called his mentor for advice. Hammerstein told him he should take the job, because writing a vehicle for a star would be a good learning experience. Sondheim agreed; "Gypsy" opened on May 21, 1959, and ran for 702 performances.

In 1960, Sondheim lost his mentor and father figure, Oscar Hammerstein II. He remembered that shortly before Hammerstein's death, Hammerstein had given him a portrait of himself. Sondheim asked him to inscribe it, and said later about the request that it was "weird ... it's like asking your father to inscribe something". Reading the inscription ("For Stevie, My Friend and Teacher") choked up the composer, who said: "That describes Oscar better than anything I could say."

When he walked away from the house that evening, Sondheim remembered a sad, sinking feeling that they had said their final goodbye. He never saw his mentor again; three days later, Hammerstein died of stomach cancer and Hammerstein's protégé eulogized him at his funeral.

The first musical for which Sondheim wrote the music and lyrics was "A Funny Thing Happened on the Way to the Forum", which opened in 1962 and ran for 964 performances. The book, based on farces by Plautus, was written by Burt Shevelove and Larry Gelbart. Sondheim's score was not well received; although the show won several Tony Awards (including best musical), he did not receive a nomination.

Sondheim had participated in three straight hits, but his next show – 1964's "Anyone Can Whistle" – was a nine-performance bomb (although it introduced Angela Lansbury to musical theatre). "Do I Hear a Waltz?", based on Arthur Laurents' 1952 play "The Time of the Cuckoo", was intended as another Rodgers and Hammerstein musical with Mary Martin in the lead. A new lyricist was needed, and Laurents and Rodgers' daughter, Mary, asked Sondheim to fill in. Although Richard Rodgers and Sondheim agreed that the original play did not lend itself to musicalization, they began writing the musical version. The project had many problems, Rodgers' alcoholism among them; Sondheim, calling it the one project he regretted, then decided to work only when he could write both music and lyrics. He asked author and playwright James Goldman to join him as bookwriter for a new musical. Inspired by a "New York Times" article about a gathering of former Ziegfeld Follies showgirls, it was entitled "The Girls Upstairs" (and would later become "Follies").

In 1966, Sondheim semi-anonymously provided lyrics for "The Boy From...", a parody of "The Girl from Ipanema" in the off-Broadway revue "The Mad Show". The song was credited to "Esteban Ria Nido", Spanish for "Stephen River Nest", and in the show's playbill the lyrics were credited to "Nom De Plume". That year Goldman and Sondheim hit a creative wall on "The Girls Upstairs", and Goldman asked Sondheim about writing a TV musical. The result was "Evening Primrose", with Anthony Perkins and Charmian Carr. Written for the anthology series "ABC Stage 67" and produced by Hubbell Robinson, it was broadcast on November 16, 1966. According to Sondheim and director Paul Bogart, the musical was written only because Goldman needed money for rent. The network disliked the title and Sondheim's alternative, "A Little Night Music".

After Sondheim finished "Evening Primrose", Jerome Robbins asked him to adapt Bertolt Brecht's "The Measures Taken" despite the composer's general dislike of Brecht's work. Robbins wanted to adapt another Brecht play, "The Exception and the Rule", and asked John Guare to adapt the book. Leonard Bernstein had not written for the stage in some time, and his contract as conductor of the New York Philharmonic was ending. Sondheim was invited to Robbins' house in the hope that Guare would convince him to write the lyrics for a musical version of "The Exception and the Rule"; according to Robbins, Bernstein would not work without Sondheim. When Sondheim agreed, Guare asked: "Why haven't you all worked together since "West Side Story"?" Sondheim answered, "You'll see". Guare said that working with Sondheim was like being with an old college roommate, and he depended on him to "decode and decipher their crazy way of working"; Bernstein worked only after midnight, and Robbins only in the early morning. Bernstein's score, which was supposed to be light, was influenced by his need to make a musical statement. Stuart Ostrow, who worked with Sondheim on "The Girls Upstairs", agreed to produce the musical (now entitled "A Pray By Blecht" and, later, "The Race to Urga"). An opening night was scheduled, but during auditions Robbins asked to be excused for a moment. When he did not return, a doorman said he had gotten into a limousine to go to John F. Kennedy International Airport. Bernstein burst into tears and said, "It's over." Sondheim later said of this experience: "I was ashamed of the whole project. It was arch and didactic in the worst way." He wrote one-and-a-half songs and threw them away, the only time he has ever done that. Eighteen years later, Sondheim refused Bernstein and Robbins' request to retry the show.
He has lived in a Turtle Bay, Manhattan brownstone since writing "Gypsy" in 1959. Ten years later, while he was playing music he heard a knock on the door. His neighbor, Katharine Hepburn, was in "bare feet – this angry, red-faced lady" and told him "You have been keeping me awake all night!" (she was practicing for her musical debut in "Coco"). When Sondheim asked why she had not asked him to play for her, she said she lost his phone number. According to Sondheim, "My guess is that she wanted to stand there in her bare feet, suffering for her art".

After "Do I Hear a Waltz?", Sondheim devoted himself solely to writing both music and lyrics for the theater - and in 1970, he began a collaboration with director Harold Prince that would result in a body of work that is considered one of the high water marks of musical theater history.

Their first show with Prince as director was the 1970 concept musical "Company". A show about a single man and his married friends, "Company" (with a book by George Furth) lacked a straightforward plot, and was instead centered around themes such as marriage and the difficulty of making an emotional connection with another person. It opened on April 26, 1970 at the Alvin Theatre, where it ran for 705 performances after seven previews, and won Tony Awards for best musical, best music and best lyrics. It was revived on Broadway in 1995 and 2006, and will be revived again in 2020 (in a version where the main character is gender-swapped).

"Follies" (1971), with a book by James Goldman, opened on April 4, 1971 at the Winter Garden Theatre and ran for 522 performances after 12 previews. The plot centers on a reunion, in a crumbling Broadway theatre scheduled for demolition, of performers in "Weismann's" "Follies" (a musical revue, based on the "Ziegfeld Follies", which played in that theatre between the world wars). The production, one of the most lavish of its time, also featured choreography and co-direction by Michael Bennett"," who went on to create "A Chorus Line" (1975). The show enjoyed two revivals on Broadway in 2001 and 2011.

"A Little Night Music" (1973), with a more traditional plot based on Ingmar Bergman's "Smiles of a Summer Night" and a score primarily in waltz time, was one of the composer's greatest commercial successes. "Time" magazine called it "Sondheim's most brilliant accomplishment to date". "Send in the Clowns", a song from the musical, was a hit for Judy Collins, and became Sondheim's most well-known song. The show opened on Broadway at the Shubert Theatre on February 25, 1973, and ran for 601 performances and 12 previews. It was revived on Broadway in 2009.

"Pacific Overtures" (1976), with a book by John Weidman, was the most non-traditional of the Sondheim-Prince collaborations: the show explored the westernization of Japan, and was originally presented in Kabuki style. The show closed after a run of 193 performances, and was revived on Broadway in 2004.

"" (1979), Sondheim's most operatic score and libretto (which, with "Pacific Overtures" and "A Little Night Music", has been produced in opera houses), explores an unlikely topic: murderous revenge and cannibalism. The book, by Hugh Wheeler, is based on Christopher Bond's 1973 stage version of the Victorian original. The show has since been revived on Broadway twice (1989, 2005), and has been performed in musical theaters and opera houses alike. It ran off-Broadway at the Barrow Street Theatre until August 26, 2018.

"Merrily We Roll Along" (1981), with a book by George Furth, is one of Sondheim's more traditional scores; Frank Sinatra and Carly Simon have recorded songs from the musical. According to Sondheim's music director, Paul Gemignani, "Part of Steve's ability is this extraordinary versatility." However, the show was not the success their previous collaborations had been: after a chaotic series of preview performances, the show opened to widely negative reviews, and closed after a run of less than two weeks. Due to the high quality of Sondheim's score, however, the show has been repeatedly revised and produced in the ensuing years. Martin Gottfried wrote, "Sondheim had set out to write traditional songs ... But [despite] that there is nothing ordinary about the music." Sondheim later said: "Did I feel betrayed? I'm not sure I would put it like that. What did surprise me was the feeling around the Broadway community – if you can call it that, though I guess I will for lack of a better word – that they wanted Hal and me to fail."

"Merrily"s failure greatly affected Sondheim; he was ready to quit theatre and do movies, create video games or write mysteries: "I wanted to find something to satisfy myself that does not involve Broadway and dealing with all those people who hate me and hate Hal." Sondheim and Prince's collaboration was suspended from "Merrily" to the 2003 production of "Bounce", another failure.

However, Sondheim decided "that there are better places to start a show" and found a new collaborator in James Lapine after he saw Lapine's "Twelve Dreams" off-Broadway in 1981: "I was discouraged, and I don't know what would have happened if I hadn't discovered "Twelve Dreams" at the Public Theatre"; Lapine has a taste "for the avant-garde and for visually-oriented theatre in particular". Their first collaboration was "Sunday in the Park with George" (1984), with Sondheim's music evoking Georges Seurat's pointillism. Sondheim and Lapine won the 1985 Pulitzer Prize for Drama for the play, and it was revived on Broadway in 2008, and again in a limited run in 2017.

They collaborated on "Into the Woods" (1987), a musical based on several Brothers Grimm fairy tales. Although Sondheim has been called the first composer to bring rap music to Broadway (with the Witch in the opening number of "Into the Woods"), he attributes the first rap in theatre to Meredith Willson's "Rock Island" from "The Music Man". The show was revived on Broadway in 2002.

Sondheim and Lapine's last work together was the rhapsodic "Passion" (1994), adapted from Ettore Scola's Italian film "Passione D'Amore". With a run of 280 performances, "Passion" was the shortest-running show to win a Tony Award for Best Musical.

"Assassins" opened off-Broadway at Playwrights Horizons on December 18, 1990, with a book by John Weidman. The show explored, in revue form, a group of historical figures who tried (either with success or without) to assassinate the President of the United States. The musical closed on February 16, 1991, after 73 performances. The show eventually received a Broadway production in 2004.

"Saturday Night" was shelved until its 1997 production at London's Bridewell Theatre. The following year, its score was recorded; a revised version, with two new songs, ran off-Broadway at Second Stage Theatre in 2000 and at London's Jermyn Street Theatre in 2009.

During the late 1990s, Sondheim and Weidman reunited for "Wise Guys", a musical comedy based on the lives of Addison and Wilson Mizner. A Broadway production, starring Nathan Lane and Victor Garber, directed by Sam Mendes and planned for the spring of 2000, was delayed. Renamed "Bounce" in 2003, it was produced at the Goodman Theatre in Chicago and the Kennedy Center in Washington, D.C., in a production directed by Harold Prince, his first collaboration with Sondheim since 1981. Although after poor reviews "Bounce" never reached Broadway, a revised version opened off-Broadway as "Road Show" at the Public Theater on October 28, 2008. Directed by John Doyle, it closed on December 28, 2008.

Asked about writing new work, Sondheim replied in 2006: "No ... It's age. It's a diminution of energy and the worry that there are no new ideas. It's also an increasing lack of confidence. I'm not the only one. I've checked with other people. People expect more of you and you're aware of it and you shouldn't be." In December 2007 he said that in addition to continuing work on "Bounce", he was "nibbling at a couple of things with John Weidman and James Lapine".

Lapine created a multimedia production, originally entitled "Sondheim: a Musical Revue", which was scheduled to open in April 2009 at the Alliance Theatre in Atlanta; however, it was canceled due to "difficulties encountered by the commercial producers attached to the project ... in raising the necessary funds". A revised version, "Sondheim on Sondheim", was produced at Studio 54 by the Roundabout Theatre Company; previews began on March 19, 2010, and it ran from April 22 to June 13. The revue's cast included Barbara Cook, Vanessa L. Williams, Tom Wopat, Norm Lewis and Leslie Kritzer.

Sondheim collaborated with Wynton Marsalis on "A Bed and a Chair: A New York Love Affair", an Encores! concert on November 13–17, 2013 at New York City Center. Directed by John Doyle with choreography by Parker Esse, it consisted of "more than two dozen Sondheim compositions, each piece newly re-imagined by Marsalis". The concert featured Bernadette Peters, Jeremy Jordan, Norm Lewis, Cyrille Aimée, four dancers and the Jazz at Lincoln Center Orchestra conducted by David Loud. In "Playbill", Steven Suskin described the concert as "neither a new musical, a revival, nor a standard songbook revue; it is, rather, a staged-and-sung chamber jazz rendition of a string of songs ... Half of the songs come from "Company" and "Follies"; most of the other Sondheim musicals are represented, including the lesser-known "Passion" and "Road Show"".

For the 2014 film adaptation of "Into the Woods", Sondheim wrote a new song, "She'll Be Back", which was to be sung by The Witch, but was eventually cut.

In February 2012 it was announced that Sondheim would collaborate on a new musical with David Ives, and he had "about 20–30 minutes of the musical completed". The show, tentatively called "All Together Now", was assumed to follow the format of "Merrily We Roll Along". Sondheim described the project as "two people and what goes into their relationship ... We'll write for a couple of months, then have a workshop. It seemed experimental and fresh 20 years ago. I have a feeling it may not be experimental and fresh any more". On October 11, 2014, it was confirmed the Sondheim and Ives musical would be based on two Luis Buñuel films ("The Exterminating Angel" and "The Discreet Charm of the Bourgeoisie") and would reportedly open (in previews) at the Public Theater in 2017.

In August 2016, a reading for the musical was held at the Public Theater, and it was reported that only the first act was finished, which cast doubt on the speculated 2017 start of previews. There was a workshop in November 2016, with the participation of Matthew Morrison, Shuler Hensley, Heidi Blickenstaff, Sierra Boggess, Gabriel Ebert, Sara Stiles, Michael Cerveris and Jennifer Simard. The working title was reported to be "Buñuel" by the New York Post and other outlets, but Sondheim later clarified that this was an error and that they still had no title. As of April 2019, a date for a musical titled "Buñuel" (by Sondheim and David Ives; directed by Joe Mantello) has been announced on the New York City Theatre website, beginning August 24, 2019. However, in June 2019, the Public Theatre announced that it would not be part of its 2019–2020 season, as it was still in development, but will be produced "when it is ready".

The Kennedy Center held a Sondheim Celebration, running from May to August 2002, consisting of six of Sondheim's musicals: "Sweeney Todd", "Company", "Sunday in the Park With George", "Merrily We Roll Along", "Passion" and "A Little Night Music". On April 28, 2002, in connection with the Sondheim Celebration Sondheim and Frank Rich of "the New York Times" had a conversation. They appeared in four interviews, entitled "A Little Night Conversation with Stephen Sondheim", in California and Portland, Oregon in March 2008 and at Oberlin College in September. The "Cleveland Jewish News" reported on their Oberlin appearance: "Sondheim said: 'Movies are photographs; the stage is larger than life.' What musicals does Sondheim admire the most? "Porgy and Bess" tops a list which includes "Carousel", "She Loves Me", and "The Wiz", which he saw six times. Sondheim took a dim view of today's musicals. What works now, he said, are musicals that are easy to take; audiences don't want to be challenged". Sondheim and Rich had additional conversations on January 18, 2009 at Avery Fisher Hall, on February 2 at the Landmark Theatre in Richmond, Virginia, on February 21 at the Kimmel Center in Philadelphia and on April 20 at the University of Akron in Akron, Ohio. The conversations were reprised at Tufts and Brown University in February 2010, at the University of Tulsa in April and at Lafayette College on March 8, 2011. Sondheim had another "conversation with" Sean Patrick Flahaven (associate editor of "The Sondheim Review") at the Kravis Center in West Palm Beach on February 4, 2009, in which he discussed many of his songs and shows: "On the perennial struggles of Broadway: 'I don't see any solution for Broadway's problems except subsidized theatre, as in most civilized countries of the world.'"

On February 1, 2011, Sondheim joined former "Salt Lake Tribune" theatre critic Nancy Melich before an audience of 1,200 at Kingsbury Hall. Melich described the evening:

He was visibly taken by the university choir, who sang two songs during the evening, "Children Will Listen" and "Sunday", and then returned to reprise "Sunday". During that final moment, Sondheim and I were standing, facing the choir of students from the University of Utah's opera program, our backs to the audience, and I could see tears welling in his eyes as the voices rang out. Then, all of a sudden, he raised his arms and began conducting, urging the student singers to go full out, which they did, the crescendo building, their eyes locked with his, until the final "on an ordinary Sunday" was sung. It was thrilling, and a perfect conclusion to a remarkable evening – nothing ordinary about it.

On March 13, 2008, "A Salon With Stephen Sondheim" (which sold out in three minutes) was hosted by the Academy for New Musical Theatre in Hollywood.

An avid fan of games, in 1968 and 1969 Sondheim published a series of cryptic crossword puzzles in "New York" magazine. In 1987 "Time" called his love of puzzlemaking "legendary in theater circles", adding that the central character of Anthony Shaffer's play "Sleuth" was inspired by the composer. According to a rumor (denied by Shaffer in a March 10, 1996 "New York Times" interview), "Sleuth" had the working title "Who's Afraid of Stephen Sondheim?" His love of puzzles and mysteries is evident in "The Last of Sheila", an intricate whodunit written with longtime friend Anthony Perkins. The 1973 film, directed by Herbert Ross, featured Dyan Cannon, Raquel Welch, James Mason, James Coburn and Richard Benjamin.

Sondheim tried playwriting one more time, collaborating with "Company" librettist George Furth on "Getting Away with Murder" in 1996, but the unsuccessful Broadway production closed after 29 previews and 17 performances. His compositions have included a number of film scores, including a set of songs written for Warren Beatty's 1990 film version of "Dick Tracy". One of Sondheim's songs for the film, "Sooner or Later (I Always Get My Man)", sung in the movie by Madonna, won him an Academy Award.

According to Sondheim, he was asked to translate "Mahagonny-Songspiel": "But, I'm not a Brecht/Weill fan and that's really all there is to it. I'm an apostate: I like Weill's music when he came to America better than I do his stuff before ... I love "The Threepenny Opera" but, outside of "The Threepenny Opera", the music of his I like is the stuff he wrote in America – when he was not writing with Brecht, when he was writing for Broadway." He turned down an offer to musicalize Nathanael West's "A Cool Million" with James Lapine around 1982.

Sondheim worked with William Goldman on "Singing Out Loud", a musical film, in 1992, penning the song "Water Under the Bridge". According to the composer, Goldman wrote one or two drafts of the script and Sondheim wrote six-and-a-half songs when director Rob Reiner lost interest in the project. "Dawn" and "Sand", from the film, were recorded for the albums "Sondheim at the Movies" and "Unsung Sondheim". Sondheim and Leonard Bernstein wrote "The Race to Urga", scheduled for Lincoln Center in 1969, but when Jerome Robbins left the project it was not produced.

In 1991 Sondheim worked with Terrence McNally on a musical, "All Together Now". McNally said, "Steve was interested in telling the story of a relationship from the present back to the moment when the couple first met. We worked together a while, but we were both involved with so many other projects that this one fell through". The story follows Arden Scott, a 30-something female sculptor, and Daniel Nevin (a slightly-younger, sexually attractive restaurateur). Its script, with concept notes by McNally and Sondheim, is archived in the Harry Ransom Center at the University of Texas at Austin.

In August 2003, Sondheim expressed interest in the idea of a creating a musical adaption of the 1993 comedy film "Groundhog Day". However, in a 2008 live chat, he said that "to make a musical of "Groundhog Day" would be to gild the lily. It cannot be improved." The musical was later created and premiered in 2016 with music and lyrics by Tim Minchin and book by Danny Rubin (screenwriter of the film) with Sondheim's blessing.

Sondheim's 2010 "Finishing the Hat" annotates his lyrics "from productions dating 1954–1981. In addition to published and unpublished lyrics from "West Side Story", "Follies" and "Company", the tome finds Sondheim discussing his relationship with Oscar Hammerstein II and his collaborations with composers, actors and directors throughout his lengthy career". The book, first of a two-part series, is named after a song from "Sunday in the Park With George". Sondheim said, "It's going to be long. I'm not, by nature, a prose writer, but I'm literate, and I have a couple of people who are vetting it for me, whom I trust, who are excellent prose writers". "Finishing the Hat" was published in October 2010. According to a "New York Times" review, "The lyrics under consideration here, written during a 27-year period, aren't presented as fixed and sacred paradigms, carefully removed from tissue paper for our reverent inspection. They're living, evolving, flawed organisms, still being shaped and poked and talked to by the man who created them". The book was 11th on the "New York Times" Hardcover Nonfiction list for November 5, 2010.

Its sequel, "Look, I Made a Hat: Collected Lyrics (1981–2011) with Attendant Comments, Amplifications, Dogmas, Harangues, Digressions, Anecdotes and Miscellany", was published on November 22, 2011. The book, continuing from "Sunday in the Park With George" (where "Finishing the Hat" ended), includes sections on Sondheim's work in film and television.

After he was mentored by Oscar Hammerstein II Sondheim has returned the favor, saying that he loves "passing on what Oscar passed on to me". In an interview with Sondheim for "The Legacy Project", composer-lyricist Adam Guettel (son of Mary Rodgers and grandson of Richard Rodgers) recalls how as a 14-year-old boy he showed Sondheim his work. Guettel was "crestfallen" since he had come in "sort of all puffed up thinking [he] would be rained with compliments and things", which was not the case since Sondheim had some "very direct things to say". Later, Sondheim wrote and apologized to Guettel for being "not very encouraging" when he was actually trying to be "constructive".

Sondheim also mentored a fledgling Jonathan Larson, attending Larson's workshop for his "Superbia" (a musical version of "Nineteen Eighty-Four"). In Larson's musical "Tick, Tick... Boom!", the phone message is played in which Sondheim apologizes for leaving early, says he wants to meet him and is impressed with his work. After Larson's death, Sondheim called him one of the few composers "attempting to blend contemporary pop music with theater music, which doesn't work very well; he was on his way to finding a real synthesis. A good deal of pop music has interesting lyrics, but they are not theater lyrics". A musical-theatre composer "must have a sense of what is theatrical, of how you use music to tell a story, as opposed to writing a song. Jonathan understood that instinctively."

Around 2008, Sondheim approached Lin-Manuel Miranda to work with him translating "West Side Story" lyrics into Spanish for an upcoming Broadway revival. Miranda then approached Sondheim with his new project "Hamilton", then called "The Hamilton Mixtape", which Sondheim gave notes on. Sondheim was originally wary of the project saying he was "worried that an evening of rap might get monotonous". However, Sondheim believed Miranda's attention to, and respect for, good rhyming made it work.

A supporter for writers' rights in the theatre industry, Stephen Sondheim is an active member of the Dramatists Guild of America. In 1973, he was elected as the Guild's sixteenth president, and he continued his presidency for the non-profit organization until 1981.

Revues and anthologies

"Side by Side by Sondheim" (1976), "Marry Me a Little" (1980), "Putting It Together" (1993), and "Sondheim on Sondheim" (2010) are anthologies or revues of Sondheim's work as composer and lyricist, with songs performed in or cut from productions. "Jerome Robbins' Broadway" features "You Gotta Have a Gimmick" from "Gypsy", "Suite of Dances" from "West Side Story" and "Comedy Tonight" from "A Funny Thing Happened on the Way to the Forum". A new revue, "Secret Sondheim ... a celebration of his lesser known work", conceived and directed by Tim McArthur, was produced at the Jermyn Street Theatre in July 2010. Sondheim's "Pretty Women" and "Everybody Ought to Have a Maid" are featured in "The Madwoman of Central Park West".

Stage
Film and television
Drama Desk Awards

OBIE Awards

Laurence Olivier Award


Several benefits and concerts were performed to celebrate Sondheim's 80th birthday in 2010. Among them were the New York Philharmonic's March 15 and 16 "Sondheim: The Birthday Concert" at Lincoln Center's Avery Fisher Hall, hosted by David Hyde Pierce. The concert included Sondheim's music, performed by some of the original performers. Lonny Price directed, and Paul Gemignani conducted; performers included Laura Benanti, Matt Cavenaugh, Michael Cerveris, Victoria Clark, Jenn Colella, Jason Danieley, Alexander Gemignani, Joanna Gleason, Nathan Gunn, George Hearn, Patti LuPone, Marin Mazzie, Audra McDonald, John McMartin, Donna Murphy, Karen Olivo, Laura Osnes, Mandy Patinkin, Bernadette Peters, Bobby Steggert, Elaine Stritch, Jim Walton, Chip Zien and the 2009 Broadway revival cast of "West Side Story". A ballet was performed by Blaine Hoven and María Noel Riccetto to Sondheim's score for "Reds", and Jonathan Tunick paid tribute to his longtime collaborator. The concert was broadcast on PBS' "Great Performances" show in November, and its DVD was released on November 16.

"Sondheim 80", a Roundabout Theatre Company benefit, was held on March 22. The evening included a performance of "Sondheim on Sondheim", dinner and a show at the New York Sheraton. "A very personal star-studded musical tribute" featured new songs by contemporary musical-theatre writers. The composers (who sang their own songs) included Tom Kitt and Brian Yorkey, Michael John LaChiusa, Andrew Lippa, Robert Lopez and Kristen Anderson-Lopez, Lin-Manuel Miranda (accompanied by Rita Moreno), Duncan Sheik, and Jeanine Tesori and David Lindsay-Abaire. Bernadette Peters performed a song which had been cut from a Sondheim show.

An April 26 New York City Center birthday celebration and concert to benefit Young Playwrights, among others, featured (in order of appearance) Michael Cerveris, Alexander Gemignani, Donna Murphy, Debra Monk, Joanna Gleason, Maria Friedman, Mark Jacoby, Len Cariou, BD Wong, Claybourne Elder, Alexander Hanson, Catherine Zeta-Jones, Raúl Esparza, Sutton Foster, Nathan Lane, Michele Pawk, the original cast of "Into the Woods", Kim Crosby, Chip Zien, Danielle Ferland and Ben Wright, Angela Lansbury and Jim Walton. The concert, directed by John Doyle, was co-hosted by Mia Farrow; greetings from Sheila Hancock, Julia McKenzie, Milton Babbitt, Judi Dench and Glynis Johns were read. After Catherine Zeta-Jones performed "Send in the Clowns", Julie Andrews sang part of "Not a Day Goes By" in a recorded greeting. Although Patti LuPone, Barbara Cook, Bernadette Peters, Tom Aldredge and Victor Garber were originally scheduled to perform, they did not appear.

A July 31 BBC Proms concert celebrated Sondheim's 80th birthday at the Royal Albert Hall. The concert featured songs from many of his musicals, including "Send in the Clowns" sung by Judi Dench (reprising her role as Desirée in the 1995 production of "A Little Night Music"), and performances by Bryn Terfel and Maria Friedman.

On November 19 the New York Pops, led by Steven Reineke, performed at Carnegie Hall for the composer's 80th birthday. Kate Baldwin, Aaron Lazar, Christiane Noll, Paul Betz, Renee Rakelle, Marilyn Maye (singing "I'm Still Here"), and Alexander Gemignani appeared, and songs included "I Remember", "Another Hundred People", "Children Will Listen" and "Getting Married Today". Sondheim took the stage during an encore of his song, "Old Friends".

To honor Sondheim's 90th birthday, "The New York Times" published a special nine-page Theater supplement on March 15, 2020 featuring comments by "Critics, Performers and Fans on the Bard of Broadway." Due to theater closures during the COVID-19 pandemic, the Broadway revival of "Company" set to open March 22, 2020, Sondheim's 90th birthday, was ultimately delayed. However, a virtual concert "" was livestreamed on the Broadway.com YouTube channel on April 26. Participants in the event included Lin-Manuel Miranda, Steven Spielberg, Meryl Streep, Nathan Lane, Mandy Patinkin, Victor Garber, Bernadette Peters, Patti LuPone, Neil Patrick Harris, Jake Gyllenhaal, Christine Baranski, Sutton Foster, Josh Groban, Ben Platt, Brandon Uranowitz, Katrina Lenk, Kelli O'Hara, Jason Alexander, Brian Stokes Mitchell, Beanie Feldstein, Audra McDonald, and Raúl Esparza.

Sondheim founded Young Playwrights Inc. in 1981 to introduce young people to writing for the theatre, and is the organization's executive vice-president. The Stephen Sondheim Center for the Performing Arts, at the Fairfield Arts and Convention Center in Fairfield, Iowa, opened in December 2007 with performances by Len Cariou, Liz Callaway, and Richard Kind (all of whom had participated in Sondheim musicals).

The Stephen Sondheim Society was established in 1993 to provide information about his work, with its "Sondheim - the Magazine" provided to its membership. The society maintains a database, organizes productions, meetings, outings and other events and assists with publicity. Its annual Student Performer of the Year Competition awards a £1,000 prize to one of twelve musical-theatre students from UK drama schools and universities. At Sondheim's request, an additional prize is offered for a new song by a young composer. Judged by George Stiles and Anthony Drewe, each contestant performs a Sondheim song and a new song.

Most episode titles of the television series "Desperate Housewives" refer to Sondheim's song titles or lyrics, and the series finale is entitled "Finishing the Hat". In 1990 Sondheim, as the Cameron Mackintosh chair in musical theatre at Oxford, conducted workshops with promising musical writers including George Stiles, Anthony Drewe, Andrew Peggie, Paul James, Kit Hesketh-Harvey and Stephen Keeling. The writers founded the Mercury Workshop in 1992, which merged with the New Musicals Alliance to become MMD (a UK-based organization to develop new musical theatre, of which Sondheim is a patron).

Signature Theatre in Arlington, Virginia established its Sondheim Award, which includes a $5,000 donation to a nonprofit organization of the recipient's choice, "as a tribute to America's most influential contemporary musical theatre composer". The first award, to Sondheim, was presented at an April 27, 2009 benefit with performances by Bernadette Peters, Michael Cerveris, Will Gartshore and Eleasha Gamble. The 2010 recipient was Angela Lansbury, with Peters and Catherine Zeta-Jones hosting the April benefit. The 2011 honoree was Bernadette Peters. Other recipients were Patti LuPone in 2012, Hal Prince in 2013, Jonathan Tunick in 2014, and James Lapine in 2015. The 2016 awardee was John Weidman and the 2017 awardee was Cameron Mackintosh.

Henry Miller's Theatre, on West 43rd Street in New York City, was renamed the Stephen Sondheim Theatre on September 15, 2010 for the composer's 80th birthday. In attendance were Nathan Lane, Patti LuPone and John Weidman. Sondheim said in response to the honor, "I'm deeply embarrassed. Thrilled, but deeply embarrassed. I've always hated my last name. It just doesn't sing. I mean, it's not Belasco. And it's not Rodgers and it's not Simon. And it's not Wilson. It just doesn't sing. It sings better than Schoenfeld and Jacobs. But it just doesn't sing". Lane said, "We love our corporate sponsors and we love their money, but there's something sacred about naming a theatre, and there's something about this that is right and just".

In 2010, "The Daily Telegraph" wrote that Sondheim is "almost certainly" the only living composer with a quarterly journal published in his name; "The Sondheim Review", founded in 1994, chronicled and promoted his work. It ceased publication in 2016.

In 2019, it was observed in the media that three major films of that year prominently featured Sondheim songs: "Joker" (Wall Street businessmen sing "Send In the Clowns" on the subway), "Marriage Story" (Adam Driver sings the song "Being Alive", Scarlett Johansson, Merritt Wever, and Julie Hagerty sing "You Can Drive a Person Crazy"), and "Knives Out" (Daniel Craig sings "Losing My Mind" in the car). Sondheim's work is also referenced in television such as "The Morning Show", where Jennifer Aniston and Billy Crudup sing "Not While I'm Around".

According to Sondheim, when he asked Milton Babbitt if he could study atonality, Babbitt replied: "You haven't exhausted tonal resources for yourself yet, so I'm not going to teach you atonal". Sondheim agreed, and despite frequent dissonance and a highly-chromatic style, his music is tonal.

He is noted for complex polyphony in his vocals, such as the five minor characters who make up a Greek chorus in 1973's "A Little Night Music". Sondheim uses angular harmonies and intricate melodies. His musical influences are varied; although he has said that he "loves Bach", his favorite musical period is from Brahms to Stravinsky.

Sondheim has been described as introverted and solitary. In an interview with Frank Rich, he said, "The outsider feeling—somebody who people want to both kiss and kill—occurred quite early in my life". He did not come out as gay until he was 40. He lived with dramatist Peter Jones for eight years in the 1990s. As of 2010, the composer was in a relationship with Jeffrey Scott Romley. The couple married on December 31, 2017.




 


</doc>
<doc id="29269" url="https://en.wikipedia.org/wiki?curid=29269" title="Self-determination">
Self-determination

The right of a people to self-determination is a cardinal principle in modern international law (commonly regarded as a "jus cogens" rule), binding, as such, on the United Nations as authoritative interpretation of the Charter's norms. It states that people, based on respect for the principle of equal rights and fair equality of opportunity, have the right to freely choose their sovereignty and international political status with no interference.

The concept was first expressed in the 1860s, and spread rapidly thereafter. During and after World War I, the principle was encouraged by both Vladimir Lenin and United States President Woodrow Wilson. Having announced his Fourteen Points on 8 January 1918, on 11 February 1918 Wilson stated: "National aspirations must be respected; people may now be dominated and governed only by their own consent. 'Self determination' is not a mere phrase; it is an imperative principle of action."

During World War II, the principle was included in the Atlantic Charter, declared on 14 August 1941, by Franklin D. Roosevelt, President of the United States, and Winston Churchill, Prime Minister of the United Kingdom, who pledged The Eight Principal points of the Charter. It was recognized as an international legal right after it was explicitly listed as a right in the UN Charter.

The principle does not state how the decision is to be made, nor what the outcome should be, whether it be independence, federation, protection, some form of autonomy or full assimilation. Neither does it state what the delimitation between peoples should be—nor what constitutes a people. There are conflicting definitions and legal criteria for determining which groups may legitimately claim the right to self-determination.

By extension, the term self-determination has come to mean the free choice of one's own acts without external compulsion.

The employment of imperialism, through the expansion of empires, and the concept of political sovereignty, as developed after the Treaty of Westphalia, also explains the emergence of self-determination during the modern era. During, and after, the Industrial Revolution many groups of people recognized their shared history, geography, language, and customs. Nationalism emerged as a uniting ideology not only between competing powers, but also for groups that felt subordinated or disenfranchised inside larger states; in this situation, self-determination can be seen as a reaction to imperialism. Such groups often pursued independence and sovereignty over territory, but sometimes a different sense of autonomy has been pursued or achieved.

The world possessed several traditional, continental empires such as the Ottoman, Russian, Austrian/Habsburg, and the Qing Empire. Political scientists often define competition in Europe during the Modern Era as a balance of power struggle, which also induced various European states to pursue colonial empires, beginning with the Spanish and Portuguese, and later including the British, French, Dutch, and German.
During the early 19th century, competition in Europe produced multiple wars, most notably the Napoleonic Wars. After this conflict, the British Empire became dominant and entered its "imperial century", while nationalism became a powerful political ideology in Europe.

Later, after the Franco-Prussian War in 1870, "New Imperialism" was unleashed with France and later Germany establishing colonies in Middle East, Southeast Asia, the South Pacific, and Africa. Japan also emerged as a new power. Multiple theaters of competition developed across the world:


The Ottoman Empire, Austrian Empire, Russian Empire, Qing Empire and the new Empire of Japan maintained themselves, often expanding or contracting at the expense of another empire. All ignored notions of self-determination for those governed.

The revolt of New World British colonists in North America, during the mid-1770s, has been seen as the first assertion of the right of national and democratic self-determination, because of the explicit invocation of natural law, the natural rights of man, as well as the consent of, and sovereignty by, the people governed; these ideas were inspired particularly by John Locke's enlightened writings of the previous century. Thomas Jefferson further promoted the notion that the will of the people was supreme, especially through authorship of the United States Declaration of Independence which inspired Europeans throughout the 19th century. The French Revolution was motivated similarly and legitimatized the ideas of self-determination on that Old World continent.

Within the New World during the early 19th century, most of the nations of Spanish America achieved independence from Spain. The United States supported that status, as policy in the hemisphere relative to European colonialism, with the Monroe Doctrine. The American public, organized associated groups, and Congressional resolutions, often supported such movements, particularly the Greek War of Independence (1821–29) and the demands of Hungarian revolutionaries in 1848. Such support, however, never became official government policy, due to balancing of other national interests. After the American Civil War and with increasing capability, the United States government did not accept self-determination as a basis during its Purchase of Alaska and attempted purchase of the West Indian islands of Saint Thomas and Saint John in the 1860s, or its growing influence in the Hawaiian Islands, that led to annexation in 1898. With its victory in the Spanish–American War in 1899 and its growing stature in the world, the United States supported annexation of the former Spanish colonies of Guam, Puerto Rico and the Philippines, without the consent of their peoples, and it retained "quasi-suzerainty" over Cuba, as well.

Nationalist sentiments emerged inside the traditional empires including: Pan-Slavism in Russia; Ottomanism, Kemalist ideology and Arab nationalism in the Ottoman Empire; State Shintoism and Japanese identity in Japan; and Han identity in juxtaposition to the Manchurian ruling class in China. Meanwhile, in Europe itself there was a rise of nationalism, with nations such as Greece, Hungary, Poland and Bulgaria seeking or winning their independence.

Karl Marx supported such nationalism, believing it might be a "prior condition" to social reform and international alliances. In 1914 Vladimir Lenin wrote: "[It] would be wrong to interpret the right to self-determination as meaning anything but the right to existence as a separate state."

Woodrow Wilson revived America's commitment to self-determination, at least for European states, during World War I. When the Bolsheviks came to power in Russia in November 1917, they called for Russia's immediate withdrawal as a member of the Allies of World War I. They also supported the right of all nations, including colonies, to self-determination." The 1918 Constitution of the Soviet Union acknowledged the right of secession for its constituent republics.

This presented a challenge to Wilson's more limited demands. In January 1918 Wilson issued his Fourteen Points of January 1918 which, among other things, called for adjustment of colonial claims, insofar as the interests of colonial powers had equal weight with the claims of subject peoples. The Treaty of Brest-Litovsk in March 1918 led to Soviet Russia's exit from the war and the nominal independence of Armenia, Finland, Estonia, Latvia, Ukraine, Lithuania, Georgia and Poland, though in fact those territories were under German control. The end of the war led to the dissolution of the defeated Austro-Hungarian Empire and Czechoslovakia and the union of the State of Slovenes, Croats and Serbs and the Kingdom of Serbia as new states out of the wreckage of the Habsburg empire. However, this imposition of states where some nationalities (especially Poles, Czechs, and Serbs and Romanians) were given power over nationalities who disliked and distrusted them eventually used as a pretext for German aggression in World War II.

Wilson publicly argued that the agreements made in the aftermath of the war would be a "readjustment of those great injustices which underlie the whole structure of European and Asiatic society", which he attributed to the absence of democratic rule. The new order emerging in the postwar period would, according to Wilson, place governments "in the hands of the people and taken out of the hands of coteries and of sovereigns, who had to right to rule over the people." The League of Nations was established as the symbol of the emerging postwar order; one of its earliest tasks was to legitimize the territorial boundaries of the new nations-states created in the territories of the former Ottoman Empire, Asia, and Africa. The principle of self-determination did not extend so far as to end colonialism; under the reasoning that the local populations were not civilized enough the League of Nations was to assign each of the post-Ottoman, Asian and African states and colonies to a European power by the grant of a League of Nations mandate.

One of the German objections to the Treaty of Versailles was a somewhat selective application of the principle of self-determination as the majority of the people in Austria and in the Sudetenland region of Czechoslovakia wanted to join Germany while the majority of people in Danzig wanted to remain within the "Reich", but the Allies ignored the German objections. Wilson's 14 Points had called for Polish independence to be restored and Poland to have "secure access to the sea", which would imply that the German city of Danzig (modern Gdańsk, Poland), which occupied a strategic location where the Vistula river flowed into the Baltic sea, be ceded to Poland. At the Paris peace conference in 1919, the Polish delegation led by Roman Dmowski asked for Wilson to honor point 14 of the 14 points by transferring Danzig to Poland. arguing that Poland would not be economically viable without Danzig. However, as the 90% of the people in Danzig in this period were German, the Allied leaders at the Paris peace conference compromised by creating the Free City of Danzig, a city-state in which Poland had certain special rights. Through the city of Danzig was 90% German and 10% Polish, the surrounding countryside around Danzig was overwhelmingly Polish, and the ethnically Polish rural areas included in the Free City of Danzig objected, arguing that they wanted to be part of Poland. Neither the Poles nor the Germans were happy with this compromise and the Danzig issue became a flash-point of German-Polish tension throughout the interwar period.

During the 1920s and 1930s there were some successful movements for self-determination in the beginnings of the process of decolonization. In the Statute of Westminster the United Kingdom granted independence to Canada, New Zealand, Newfoundland, the Irish Free State, the Commonwealth of Australia, and the Union of South Africa after the British parliament declared itself as incapable of passing laws over them without their consent. Egypt, Afghanistan and Iraq also achieved independence from Britain and Lebanon from France. Other efforts were unsuccessful, like the Indian independence movement. And Italy, Japan and Germany all initiated new efforts to bring certain territories under their control, leading to World War II. In particular, the National Socialist Program invoked this right of nations in its first point (out of 25), as it was publicly proclaimed on 24 February 1920 by Adolf Hitler.

In Asia, Japan became a rising power and gained more respect from Western powers after its victory in the Russo-Japanese War. Japan joined the Allied Powers in World War I and attacked German colonial possessions in the Far East, adding former German possessions to its own empire. In the 1930s, Japan gained significant influence in Inner Mongolia and Manchuria after it invaded Manchuria. It established Manchukuo, a puppet state in Manchuria and eastern Inner Mongolia. This was essentially the model Japan followed as it invaded other areas in Asia and established the Greater East Asia Co-Prosperity Sphere. Japan went to considerable trouble to argue that Manchukuo was justified by the principle of self-determination, claiming that people of Manchuria wanted to break away from China and asked the Kwantung Army to intervene on their behalf. However, the Lytton commission which had been appointed by the League of Nations to decide if Japan had committed aggression or not, stated the majority of people in Manchuria who were Han Chinese who did not wish to leave China.

In 1912, the Republic of China officially succeeded the Qing Dynasty, while Outer Mongolia, Tibet and Tuva proclaimed their independence. Independence was not accepted by the government of China. By the Treaty of Kyakhta (1915) Outer Mongolia recognized China's sovereignty. However, the Soviet threat of seizing parts of Inner Mongolia induced China to recognize Outer Mongolia's independence, provided that a referendum was held. The referendum took place on October 20, 1945, with (according to official numbers) 100% of the electorate voting for independence.

Many of Eastern Asia's current disputes to sovereignty and self-determination stem from unresolved disputes from World War II. After its fall, the Empire of Japan renounced control over many of its former possessions including Korea, Sakhalin Island, and Taiwan. In none of these areas were the opinions of affected people consulted, or given significant priority. Korea was specifically granted independence but the receiver of various other areas was not stated in the Treaty of San Francisco, giving Taiwan "de facto" independence although its political status continues to be ambiguous.

In 1941 Allies of World War II declared the Atlantic Charter and accepted the principle of self-determination. In January 1942 twenty-six states signed the Declaration by United Nations, which accepted those principles. The ratification of the United Nations Charter in 1945 at the end of World War II placed the right of self-determination into the framework of international law and diplomacy.

On 14 December 1960, the United Nations General Assembly adopted United Nations General Assembly Resolution 1514 (XV) subtitled "Declaration on the Granting of Independence to Colonial Countries and Peoples", which supported the granting of independence to colonial countries and people by providing an inevitable legal linkage between self-determination and its goal of decolonisation. It postulated a new international law-based right of freedom to exercise economic self-determination. Article 5 states: Immediate steps shall be taken in Trust and Non-Self-Governing Territories, or all other territories which have not yet attained independence, to transfer all powers to the people of those territories, without any conditions or reservations, in accordance with their freely expressed will and desire, without any distinction as to race, creed or colour, in order to enable them to enjoy complete independence and freedom.

On 15 December 1960 the United Nations General Assembly adopted United Nations General Assembly Resolution 1541 (XV), subtitled "Principles which should guide members in determining whether or nor an obligation exists to transmit the information called for under Article 73e of the United Nations Charter in Article 3", which provided that "[t]he inadequacy of political, economic, social and educational preparedness should never serve as a pretext for delaying the right to self-determination and independence." To monitor the implementation of Resolution 1514, in 1961 the General Assembly created the Special Committee referred to popularly as the Special Committee on Decolonization to ensure decolonization complete compliance with the principles of self-determination in General Assembly Resolution 1541 (XV).

However, the charter and other resolutions did not insist on full independence as the best way of obtaining self-government, nor did they include an enforcement mechanism. Moreover, new states were recognized by the legal doctrine of uti possidetis juris, meaning that old administrative boundaries would become international boundaries upon independence if they had little relevance to linguistic, ethnic, and cultural boundaries. Nevertheless, justified by the language of self-determination, between 1946 and 1960, thirty-seven new nations in Asia, Africa, and the Middle East gained independence from colonial powers. The territoriality issue inevitably would lead to more conflicts and independence movements within many states and challenges to the assumption that territorial integrity is as important as self-determination.

Decolonization in the world was contrasted by the Soviet Union's successful post-war expansionism. Tuva and several regional states in Eastern Europe, the Baltic, and Central Asia had been fully annexed by the Soviet Union during World War II. Now, it extended its influence by establishing the satellite states of Eastern Germany and the countries of Eastern Europe, along with support for revolutionary movements in China and North Korea. Although satellite states were independent and possessed sovereignty, the Soviet Union violated principles of self-determination by suppressing the Hungarian revolution of 1956 and the Prague Spring Czechoslovak reforms of 1968. It invaded Afghanistan to support a communist government assailed by local tribal groups. However, Marxism–Leninism and its theory of imperialism were also strong influences in the national emancipation movements of Third World nations rebelling against colonial or puppet regimes. In many Third World countries, communism became an ideology that united groups to oppose imperialism or colonization.

Soviet actions were contained by the United States which saw communism as a menace to its interests. Throughout the cold war, the United States created, supported, and sponsored regimes with various success that served their economic and political interests, among them anti-communist regimes such as that of Augusto Pinochet in Chile and Suharto in Indonesia. To achieve this, a variety of means was implemented, including the orchestration of coups, sponsoring of anti-communist countries and military interventions. Consequently, many self-determination movements, which spurned some type of anti-communist government, were accused of being Soviet-inspired or controlled.

In Asia, the Soviet Union had already converted Mongolia into a satellite state but abandoned propping up the Second East Turkestan Republic and gave up its Manchurian claims to China. The new People's Republic of China had gained control of mainland China in the Chinese Civil War. The Korean War shifted the focus of the Cold War from Europe to Asia, where competing superpowers took advantage of decolonization to spread their influence.

In 1947, India gained independence from the British Empire. The empire was in decline but adapted to these circumstances by creating the British Commonwealth—since 1949 the Commonwealth of Nations—which is a free association of equal states. As India obtained its independence, multiple ethnic conflicts emerged in relation to the formation of a statehood during the Partition of India which resulted in Islamic Pakistan and Secular India. Before the advent of the British, no empire based in mainland India had controlled any part of what now makes up the country's Northeast, part of the reason for the ongoing insurgency in Northeast India. In 1971 Bangladesh obtained independence from Pakistan.

Burma also gained independence from the British Empire, but declined membership in the Commonwealth.

Indonesia gained independence from the Netherlands in 1949 after the latter failed to restore colonial control. As mentioned above, Indonesia also wanted a powerful position in the region that could be lessened by the creation of united Malaysia. The Netherlands retained Dutch New Guinea, but Indonesia threatened to invade and annex it. A vote was supposedly taken under the UN sponsored Act of Free Choice to allow West New Guineans to decide their fate, although many dispute its veracity. Later, Portugal relinquished control over East Timor in 1975, at which time Indonesia promptly invaded and annexed it.

The Cold War began to wind down after Mikhail Gorbachev assumed power in March 1985. With the cooperation of the American president Ronald Reagan, Gorbachev wound down the size of the Soviet Armed Forces and reduced nuclear arms in Europe, while liberalizing the economy.

In 1989 – 90, the communist regimes of Soviet satellite states collapsed in rapid succession in Poland, Hungary, Czechoslovakia, East Germany, Bulgaria, Romania, and Mongolia. East and West Germany united, Czechoslovakia peacefully split into Czech Republic and Slovakia, while in 1990 Yugoslavia began a violent break up into 6 states. Kosovo, which was previously an autonomous unit of Serbia declared independence in 2008, but has received less international recognition.

In December 1991, Gorbachev resigned as president and the Soviet Union dissolved relatively peacefully into fifteen sovereign republics, all of which rejected communism and most of which adopted democratic reforms and free-market economies. Inside those new republics, four major areas have claimed their own independence, but not received widespread international recognition.

After decades of civil war, Indonesia finally recognized the independence of East Timor in 2002.

In 1949, the Communists won the civil war and established the People's Republic of China in Mainland China. The Kuomintang-led Republic of China government retreated to Taipei, its jurisdiction now limited to Taiwan and several outlying islands. Since then, the People's Republic of China has been involved in disputes with the ROC over issues of sovereignty and the political status of Taiwan.

As noted, self-determination movements remain strong in some areas of the world. Some areas possess "de facto" independence, such as Taiwan, North Cyprus, Kosovo, and South Ossetia, but their independence is disputed by one or more major states. Significant movements for self-determination also persist for locations that lack "de facto" independence, such as Kurdistan, Balochistan, Chechnya, and the State of Palestine

Since the early 1990s, the legitimatization of the principle of national self-determination has led to an increase in the number of conflicts within states, as sub-groups seek greater self-determination and full secession, and as their conflicts for leadership within groups and with other groups and with the dominant state become violent. The international reaction to these new movements has been uneven and often dictated more by politics than principle. The 2000 United Nations Millennium Declaration failed to deal with these new demands, mentioning only "the right to self-determination of peoples which remain under colonial domination and foreign occupation."

In an issue of "Macquarie University Law Journal" Associate Professor Aleksandar Pavkovic and Senior Lecturer Peter Radan outlined current legal and political issues in self-determination. These include:

There is not yet a recognized legal definition of "peoples" in international law. Vita Gudeleviciute of Vytautas Magnus University Law School, reviewing international law and UN resolutions, finds in cases of non-self-governing peoples (colonized and/or indigenous) and foreign military occupation "a people" is the entire population of the occupied territorial unit, no matter their other differences. In cases where people lack representation by a state's government, the unrepresented become a separate people. Present international law does not recognize ethnic and other minorities as separate peoples, with the notable exception of cases in which such groups are systematically disenfranchised by the government of the state they live in. Other definitions offered are "peoples" being self-evident (from ethnicity, language, history, etc.), or defined by "ties of mutual affection or sentiment", i.e. "loyalty", or by mutual obligations among peoples. Or the definition may be simply that a people is a group of individuals who unanimously choose a separate state. If the "people" are unanimous in their desire for self-determination, it strengthens their claim. For example, the populations of federal units of the Yugoslav federation were considered a people in the breakup of Yugoslavia, although some of those units had very diverse populations. Although there is no fully accepted definition of peoples, references are often made to a definition proposed by UN Special Rapporteur Martínez Cobo in his study on discrimination against indigenous populations. UN Independent Expert on the Promotion of a democratic and equitable International Order, Alfred de Zayas, relied on the "Kirby definition" in his 2014 Report to the General Assembly A/69/272 as "a group of persons with a common historical tradition, racial or ethnic identity, cultural homogeneity, linguistic unity, religious or ideological affinity, territorial connection,or common economic life. To this should be added a subjective element: the will to be identified as a people and the consciousness of being a people.". 

Abulof suggests that self-determination entails the "moral double helix" of duality (personal right to align with a people, and the people's right to determine their politics) and mutuality (the right is as much the other's as the self's). Thus, self-determination grants individuals the right to form "a people," which then has the right to establish an independent state, as long as they grant the same to all other individuals and peoples.

Criteria for the definition of "people having the right of self-determination" was proposed during 2010 Kosovo case decision of the International Court of Justice: 1. traditions and culture 2. ethnicity 3. historical ties and heritage 4. language 5. religion 6. sense of identity or kinship 7. the will to constitute a people 8. common suffering.

National self-determination appears to challenge the principle of territorial integrity (or sovereignty) of states as it is the will of the people that makes a state legitimate. This implies a people should be free to choose their own state and its territorial boundaries. However, there are far more self-identified nations than there are existing states and there is no legal process to redraw state boundaries according to the will of these peoples. According to the Helsinki Final Act of 1975, the UN, ICJ and international law experts, there is no contradiction between the principles of self-determination and territorial integrity, with the latter taking precedence.
Pavkovic and Radan describe three theories of international relations relevant to self-determination.


Allen Buchanan, author of seven books on self-determination and secession, supports territorial integrity as a moral and legal aspect of constitutional democracy. However, he also advances a "Remedial Rights Only Theory" where a group has "a general right to secede if and only if it has suffered certain injustices, for which secession is the appropriate remedy of last resort. " He also would recognize secession if the state grants, or the constitution includes, a right to secede.

Vita Gudeleviciute holds that in cases of non-self-governing peoples and foreign military occupation the principle of self-determination trumps that of territorial integrity. In cases where people lack representation by a state's government, they also may be considered a separate people, but under current law cannot claim the right to self-determination. On the other hand, she finds that secession within a single state is a domestic matter not covered by international law. Thus there are no on what groups may constitute a seceding people.
A number of states have laid claim to territories, which they allege were removed from them as a result of colonialism. This is justified by reference to Paragraph 6 of UN Resolution 1514(XV), which states that any attempt "aimed at partial or total disruption of the national unity and the territorial integrity of a country is incompatible with the purposes and principles of the Charter". This, it is claimed, applies to situations where the territorial integrity of a state had been disrupted by colonisation, so that the people of a territory subject to a historic territorial claim are prevented from exercising a right to self-determination. This interpretation is rejected by many states, who argue that Paragraph 2 of UN Resolution 1514(XV) states that "all peoples have the right to self-determination" and Paragraph 6 cannot be used to justify territorial claims. The original purpose of Paragraph 6 was "to ensure that acts of self-determination occur within the established boundaries of colonies, rather than within sub-regions". Further, the use of the word "attempt" in Paragraph 6 denotes future action and cannot be construed to justify territorial redress for past action. An attempt sponsored by Spain and Argentina to qualify the right to self-determination in cases where there was a territorial dispute was rejected by the UN General Assembly, which re-iterated the right to self-determination was a universal right.

In order to accommodate demands for minority rights and avoid secession and the creation of a separate new state, many states decentralize or devolve greater decision-making power to new or existing subunits or autonomous areas. More limited measures might include restricting demands to the maintenance of national cultures or granting non-territorial autonomy in the form of national associations which would assume control over cultural matters. This would be available only to groups that abandoned secessionist demands and the territorial state would retain political and judicial control, but only if would remain with the territorially organized state.

Pavković explores how national self-determination, in the form of creation of a new state through secession, could override the principles of majority rule and of equal rights, which are primary liberal principles. This includes the question of how an unwanted state can be imposed upon a minority. He explores five contemporary theories of secession. In "anarcho-capitalist" theory only landowners have the right to secede. In communitarian theory, only those groups that desire direct or greater political participation have the right, including groups deprived of rights, per Allen Buchanan. In two nationalist theories, only national cultural groups have a right to secede. Australian professor Harry Beran's democratic theory endorses the equality of the right of secession to all types of groups. Unilateral secession against majority rule is justified if the group allows secession of any other group within its territory.

Most sovereign states do not recognize the right to self-determination through secession in their constitutions. Many expressly forbid it. However, there are several existing models of self-determination through greater autonomy and through secession.

In liberal constitutional democracies the principle of majority rule has dictated whether a minority can secede. In the United States Abraham Lincoln acknowledged that secession might be possible through amending the United States Constitution. The Supreme Court in "Texas v. White" held secession could occur "through revolution, or through consent of the States." The British Parliament in 1933 held that Western Australia only could secede from Australia upon vote of a majority of the country as a whole; the previous two-thirds majority vote for secession via referendum in Western Australia was insufficient.

The Chinese Communist Party followed the Soviet Union in including the right of secession in its 1931 constitution in order to entice ethnic nationalities and Tibet into joining. However, the Party eliminated the right to secession in later years, and had anti-secession clause written into the Constitution before and after the founding the People's Republic of China. The 1947 Constitution of the Union of Burma contained an express state right to secede from the union under a number of procedural conditions. It was eliminated in the 1974 constitution of the Socialist Republic of the Union of Burma (officially the "Union of Myanmar"). Burma still allows "local autonomy under central leadership".

As of 1996 the constitutions of Austria, Ethiopia, France, and Saint Kitts and Nevis have express or implied rights to secession. Switzerland allows for the secession from current and the creation of new cantons. In the case of proposed Quebec separation from Canada the Supreme Court of Canada in 1998 ruled that only both a clear majority of the province and a constitutional amendment confirmed by all participants in the Canadian federation could allow secession.

The 2003 draft of the European Union Constitution allowed for the voluntary withdrawal of member states from the union, although the State which wanted to leave could not be involved in the vote deciding whether or not they can leave the Union. There was much discussion about such self-determination by minorities before the final document underwent the unsuccessful ratification process in 2005.

As a result of the successful constitutional referendum held in 2003, every municipality in the Principality of Liechtenstein has the right to secede from the Principality by a vote of a majority of the citizens residing in this municipality.

In determining international borders between sovereign states, self-determination has yielded to a number of other principles. Once groups exercise self-determination through secession, the issue of the proposed borders may prove more controversial than the fact of secession. The bloody Yugoslav wars in the 1990s were related mostly to border issues because the international community applied a version of uti possidetis juris in transforming the existing internal borders of the various Yugoslav republics into international borders, despite the conflicts of ethnic groups within those boundaries. In the 1990s indigenous populations of the northern two-thirds of Quebec province opposed being incorporated into a Quebec nation and stated a determination to resist it by force.

The border between Northern Ireland and the Irish Free State was based on the borders of existing counties and did not include all of historic Ulster. A Boundary Commission was established to consider re-drawing it. Its proposals, which amounted to a small net transfer to Northern Ireland, were leaked to the press and then not acted upon. In December 1925, the governments of the Irish Free State, Northern Ireland, and the United Kingdom agreed to accept the existing border.

There have been a number of notable cases of self-determination. For more information on past movements see list of historical separatist movements and lists of decolonized nations. Also see list of autonomous areas by country and lists of active separatist movements.

The Republic of Artsakh (Republic of Nagorno-Karabakh), in the Caucasus region, declared its independence basing on self-determination rights on September 2, 1991. It successfully defended its independence in subsequent war with Azerbaijan, but remains largely unrecognized by UN states today. It is a member of the Community for Democracy and Rights of Nations along with three other Post-Soviet disputed republics.

Self-determination has become the topic of some debate in Australia in relation to Aboriginal Australians and Torres Strait Islanders. In the 1970s, Aboriginal requested the right to administer their own remote communities as part of the homelands movement, also known as the outstation movement. These grew in number through the 1980s, but funding dried up in the 2000s.
The traditional homeland of the Tuareg peoples was divided up by the modern borders of Mali, Algeria and Niger. Numerous rebellions occurred over the decades, but in 2012 the Tuaregs succeeded in occupying their land and declaring the independence of Azawad. However, their movement was hijacked by the Islamist terrorist group Ansar Dine.

The Basque Country (, , ) as a cultural region (not to be confused with the homonym Autonomous Community of the Basque country) is a European region in the western Pyrenees that spans the border between France and Spain, on the Atlantic coast. It comprises the autonomous communities of the Basque Country and Navarre in Spain and the Northern Basque Country in France.
Since the 19th century, Basque nationalism has demanded the right of some kind of self-determination. This desire for independence is particularly stressed among leftist Basque nationalists. The right of self-determination was asserted by the Basque Parliament in 1990, 2002 and 2006.
Since self-determination is not recognized in the Spanish Constitution of 1978, some Basques abstained and some voted against it in the referendum of December 6 of that year. It was approved by a clear majority at the Spanish level, and with 74.6% of the votes in the Basque Country. However, the overall turnout in the Basque Country was 45% when the Spanish overall turnover was 67.9%. The derived autonomous regime for the BAC was approved by Spanish Parliament and also by the Basque citizens in referendum. The autonomous statue of Navarre ("Amejoramiento del Fuero": "improvement of the charter") was approved by the Spanish Parliament and, like the statues of 13 out of 17 Spanish autonomous communities, it didn't need a referendum to enter into force.

"Euskadi Ta Askatasuna" or ETA (; pronounced ), was an armed Basque nationalist, separatist and terrorist organization. Founded in 1959, it evolved from a group advocating traditional cultural ways to a paramilitary group with the goal of Basque independence. Its ideology was Marxist–Leninist.

The Nigerian Civil War was fought between Biafran secessionists of the Republic of Biafra and the Nigerian central government. From 1999 to the present day, the indigenous people of Biafra have been agitating for independence to revive their country. They have registered a human rights organization known as Bilie Human Rights Initiative both in Nigeria and in the United Nations to advocate for their right to self-determination and achieve independence by the rule of law.

After the 2012 Catalan march for independence, in which between 600,000 and 1.5 million citizens marched, the President of Catalonia, Artur Mas, called for new parliamentary elections on 25 November 2012 to elect a new parliament that would exercise the right of self-determination for Catalonia, a right not recognised under the Spanish constitution. The Parliament of Catalonia voted to hold a vote in the next four-year legislature on the question of self-determination. The parliamentary decision was approved by a large majority of MPs: 84 voted for, 21 voted against, and 25 abstained. The Catalan Parliament applied to the Spanish Parliament for the power to call a referendum to be devolved, but this was turned down. In December 2013 the President of the Generalitat Artur Mas and the governing coalition agreed to set the referendum for self-determination on 9 November 2014, and legislation specifically saying that the consultation would not be a "referendum" was enacted, only to be blocked by the Spanish Constitutional Court, at the request of the Spanish government. Given the block, the Government turned it into a simple "consultation to the people" instead.

The question in the consultation was "Do you want Catalonia to be a State?" and, if the answer to this question was yes, "Do you want this State to be an independent State?". However, as the consultation was not a formal referendum, these (printed) answers were just suggestions and other answers were also accepted and catalogued as "other answers" instead as null votes. The turnout in this consultation was about 2·3m people out of 6·2m people that were called to vote (this figure does not coincide with the census figure of 5·3m for two main reasons: first, because organisers had no access to an official census due to the non-binding character of the consultation, and second, because the legal voting age was set to 16 rather than 18). Due to the lack of an official census, potential voters were assigned to electoral tables according to home address and first family name. Participants had to sign up first with their full name and national ID in a voter registry before casting their ballot, which prevented participants from potentially casting multiple ballots. The overall result was 80·76% in favor of both questions, 11% in favor of the first question but not of the second questions, 4·54% against both; the rest were classified as "other answers". The voter turnout was around 37% (most people against the consultation didn't go to vote). Four top members of Catalonia's political leadership were barred from public office for having defied the Constitutional court's last-minute ban.
Almost three years later (1 October 2017), the Catalan government called a referendum for independence under legislation adopted in September 2017 (despite being blocked by the Constitutional Court of Spain), with the question "Do you want Catalonia to become an independent state in the form of a Republic?". On polling day, the Catalan police prevented voting in over 500 polling stations, without incident, while the Spanish police confiscated ballot boxes and closed down 92, voting centres with violent truncheon charges. The opposition parties had called for non-participation. The turnout (according to the votes that were counted) was 2.3m out of 5.3m (43.03% of the census), and 90.18% of the ballots were in favour of independence. The turnout, ballot count and results were similar to those of the 2014 "consultation".

Under Dzhokhar Dudayev, Chechnya declared independence as the Chechen Republic of Ichkeria, using self-determination, Russia's history of bad treatment of Chechens, and a history of independence before invasion by Russia as main motives. Russia has restored control over Chechnya, but the separatist government functions still in exile, though it has been split into two entities: the Akhmed Zakayev-run secular Chechen Republic (based in Poland, the UK and the US), and the Islamic Caucasus Emirate.

There is an active secessionist movement based on the self-determination of the residents of the Donetsk and Luhansk regions of eastern Ukraine, allegedly against the illegitimacy and corruption of the Ukrainian government. However, many in the international community assert that referendums held there in 2014 regarding independence from Ukraine were illegitimate and undemocratic. Similarly, there are reports that presidential elections in May 2014 were prevented from taking place in the two regions after armed gunmen took control of polling stations, kidnapped election officials, and stole lists of electors, thus denying the population the chance to express their will in a free, fair, and internationally recognised election. There are also arguments that the de facto separation of Eastern Ukraine from the rest of the country is not an expression of self-determination, but rather a manipulation through pro-Soviet sentiment revival and an invasion by neighbouring Russia, with Ukrainian President Petro Poroshenko claiming in 2015 that up to 9,000 Russian soldiers were deployed in Ukraine.

Self-determination is referred to in the Falkland Islands Constitution and is a factor in the Falkland Islands sovereignty dispute. The population has existed for over nine generations, continuously for over 185 years. In the 2013 referendum organised by the Falkland Islands Government, 99.8% voted to remain British. As administering power, the British Government considers since the majority of inhabitants wish to remain British, transfer of sovereignty to Argentina would be counter to their right to self-determination.
Argentina states the principle of self-determination is not applicable since the current inhabitants are not aboriginal and were brought to replace the Argentine population, which was expelled by an 'act of force', forcing the Argentinian inhabitants to directly leave the islands. This refers to the re-establishment of British rule in the year 1833 during which Argentina claims the existing population living in the islands was expelled. Argentina thus argues that, in the case of the Falkland Islands, the principle of territorial integrity should have precedence over self-determination. Historical records dispute Argentina's claims and whilst acknowledging the garrison was expelled note the existing civilian population remained at Port Louis and there was no attempt to settle the islands until 1841.

The right to self-determination is referred to in the pre-amble of Chapter 1 of the Gibraltar constitution, and, since the United Kingdom also gave assurances that the right to self-determination of Gibraltarians would be respected in any transfer of sovereignty over the territory, is a factor in the dispute with Spain over the territory. The impact of the right to self-determination of Gibraltarians was seen in the 2002 Gibraltar sovereignty referendum, where Gibraltarian voters overwhelmingly rejected a plan to share sovereignty over Gibraltar between the UK and Spain. However, the UK government differs with the Gibraltarian government in that it considers Gibraltarian self-determination to be limited by the Treaty of Utrecht, which prevents Gibraltar achieving independence without the agreement of Spain, a position that the Gibraltarian government does not accept.

The Spanish government denies that Gibraltarians have the right to self-determination, considering them to be "an artificial population without any genuine autonomy" and not "indigenous". However, the Partido Andalucista has agreed to recognise the right to self-determination of Gibraltarians.

Before the United Nations's adoption of resolution 2908 (XXVII) on 2 November 1972, The People's Republic of China vetoed the former British colony of Hong Kong's right to self-determination on 8 March 1972. This sparked several nation's protest along with Great Britain's declaration on 14 December that the decision is invalid. 
Decades later , a nationalist independence movement, dubbed as the Hong Kong independence movement emerged in the now Communist Chinese controlled territory. It advocates the autonomous region to become a fully independent sovereign state.

The city is considered a special administrative region (SAR) which, according to the PRC, enjoys a high degree of autonomy under the People's Republic of China (PRC), guaranteed under Article 2 of Hong Kong Basic Law (which is ratified under the Sino-British Joint Declaration), since the transfer of the sovereignty of Hong Kong from the United Kingdom to the PRC in 1997. Since the handover, many Hongkongers are increasingly concerned about Beijing's growing encroachment on the territory's freedoms and the failure of the Hong Kong government to deliver 'true' democracy.

The 2014–15 Hong Kong electoral reform package deeply divided the city, as it allowed Hongkongers to have universal suffrage, but Beijing would have authority to screen the candidates to restrict the electoral method for the Chief Executive of Hong Kong (CE), the highest-ranking official of the territory. This sparked the 79-day massive peaceful protests which was dubbed as the "Umbrella Revolution" and the pro-independence movement emerged on the Hong Kong political scene. 

Since then, localism has gained momentum, particularly after the failure of the peaceful Umbrella Movement. Young localist leaders have led numerous protest actions against pro-Chinese policies to raise awareness of social problems of Hong Kong under Chinese rule. These include the sit-in protest against the Bill to Strengthen Internet Censorship, demonstrations against Chinese political interference in the University of Hong Kong, the Recover Yuen Long protests and the 2016 Mong Kok civil unrest. According to a survey conducted by the Chinese University of Hong Kong (CUHK) in July 2016, 17.4% of respondents supported the city becoming an independent entity after 2047, while 3.6% stated that it is "possible".

Indigenous peoples have claimed through the 2007 Declaration on the Rights of Indigenous Peoples the term peoples, and gaining with it the right to self-determination. Though it was also established that it is merely a right within existing sovereign states, afterall peoples also need territory and a central government to reach sovereignty in international politics.

Ever since Pakistan and India's inception in 1947 the legal state of Jammu and Kashmir, the land between India and Pakistan, has been contested as Britain was resigning from their rule over this land. Maharaja Hari Singh, the ruler of Kashmir at the time of accession, signed the Instrument of Accession Act on October 26, 1947 as his territory was being attacked by Pakistani tribesmen. The passing of this Act allowed Jammu and Kashmir to accede to India on legal terms. When this Act was taken to Lord Mountbatten, the last viceroy of British India, he agreed to it and stated that a referendum needed to be held by the citizens in India, Pakistan, and Kashmir so that they could vote as to where Kashmir should accede to. This referendum that Mountbatten called for never took place and framed one of the legal disputes for Kashmir. In 1948 the United Nations intervened and ordered a plebiscite to be taken in order to hear the voices of the Kashmiris if they would like to accede to Pakistan or India. This plebiscite left out the right for Kashmiris to have the right of self-determination and become an autonomous state. To this date the Kashmiris have been faced with numerous human rights violations committed by both India and Pakistan and have yet to gain complete autonomy which they have been seeking through self-determination. 

The insurgency in Kashmir against Indian rule has existed in various forms. A widespread armed insurgency started in Kashmir against India rule in 1989 after allegations of rigging by the Indian government in the 1987 Jammu and Kashmir state election. This led to some parties in the state assembly forming militant wings, which acted as a catalyst for the emergence of armed insurgency in the region. The conflict over Kashmir has resulted in tens of thousands of deaths.

The Inter-Services Intelligence of Pakistan has been accused by India of supporting and training both pro-Pakistan and pro-independence militants to fight Indian security forces in Jammu and Kashmir, a charge that Pakistan denies. According to official figures released in the Jammu and Kashmir assembly, there were 3,400 disappearance cases and the conflict has left more than 47,000 to 100,000 people dead as of July 2009. However, violence in the state had fallen sharply after the start of a slow-moving peace process between India and Pakistan. After the peace process failed in 2008, mass demonstrations against Indian rule, and also low-scale militancy have emerged again.

However, despite boycott calls by separatist leaders in 2014, the Jammu and Kashmir Assembly elections saw highest voters turnout in last 25 years since insurgency erupted. As per the Indian government, it recorded more than 65% of voters turnout which was more than usual voters turnout in other state assembly elections of India. It considered as increase in faith of Kashmiri people in democratic process of India. However, activists say that the voter turnout is highly exaggerated and that elections are held under duress. Votes are cast because the people want stable governance of the state and this cannot be mistaken as an endorsement of Indian rule.

Kurdistan is a historical region primarily inhabited by the Kurdish people of the Middle East. The territory is currently part of Turkey, Iraq, Syria and Iran. There are Kurdish self-determination movements in each of the 4 states. Iraqi Kurdistan has to date achieved the largest degree of self-determination through the formation of the Kurdistan Regional Government, an entity recognised by the Iraqi Federal Constitution.

Although the right of the creation of a Kurdish state was recognized following World War I in the Treaty of Sèvres, the treaty was then annulled by the Treaty of Lausanne (1923). To date two separate Kurdish republics and one Kurdish Kingdom have declared sovereignty. The Republic of Ararat (Ağrı Province, Turkey), the Republic of Mehabad (West Azerbaijan Province, Iran) and the Kingdom of Kurdistan (Sulaymaniyah Governorate, Iraqi Kurdistan, Iraq), each of these fledgling states was crushed by military intervention. The Patriotic Union of Kurdistan which currently holds the Iraqi presidency and the Kurdistan Democratic Party which governs the Kurdistan Regional Government both explicitly commit themselves to the development of Kurdish self-determination, but opinions vary as to the question of self-determination sought within the current borders and countries.

Efforts towards Kurdish self-determination are considered illegal separatism by the governments of Turkey and Iran, and the movement is politically repressed in both states. This is intertwined with Kurdish nationalist insurgencies in Iran and in Turkey, which in turn justify and are justified by the repression of peaceful advocacy. In Syria, a self-governing local Kurdish-dominated polity was established in 2012, amongst the upheaval of the Syrian Civil War, but has not been recognized by any foreign state.

Naga refers to a vaguely-defined conglomeration of distinct tribes living on the border of India and Burma. Each of these tribes lived in a sovereign village before the arrival of the British, but developed a common identity as the area was Christianized. After the British left India, a section of Nagas under the leadership of Angami Zapu Phizo sought to establish a separate country for the Nagas. Phizo's group, the Naga National Council (NNC), claimed that 99. 9% of the Nagas wanted an independent Naga country according to a referendum conducted by it. It waged a secessionist insurgency against the Government of India. The NNC collapsed after Phizo got his dissenters killed or forced them to seek refuge with the Government. Phizo escaped to London, while NNC's successor secessionist groups continued to stage violent attacks against the Indian Government. The Naga People's Convention (NPC), another major Naga organization, was opposed to the secessionists. Its efforts led to the creation of a separate Nagaland state within India in 1963. The secessionist violence declined considerably after the Shillong Accord of 1975. However, three factions of the National Socialist Council of Nagaland (NSCN) continue to seek an independent country which would include parts of India and Burma. They envisage a sovereign, predominantly Christian nation called "Nagalim".

Another controversial episode with perhaps more relevance was the British beginning their exit from British Malaya. An experience concerned the findings of a "United Nations Assessment Team" that led the British territories of North Borneo and Sarawak in 1963 to determine whether or not the populations wished to become a part of the new Malaysia Federation. The United Nation Team's mission followed on from an earlier assessment by the British-appointed Cobbold Commission which had arrived in the territories in 1962 and held hearings to determine public opinion. It also sifted through 1600 letters and memoranda submitted by individuals, organisations and political parties. Cobbold concluded that around two thirds of the population favoured to the formation of Malaysia while the remaining third wanted either independence or continuing control by the United Kingdom. The United Nations team largely confirmed these findings, which were later accepted by the General Assembly, and both territories subsequently wish to form the new Federation of Malaysia. The conclusions of both the Cobbold Commission and the United Nations team were arrived at without any referendums self-determination being held. Unlike in Singapore, however, no referendum was ever conducted in Sarawak and North Borneo. they sought to consolidate several of the previous ruled entities then there was Manila Accord, an agreement between the Philippines, Federation of Malaya and Indonesia on 31 July 1963 to abide by the wishes of the people of North Borneo and Sarawak within the context of United Nations General Assembly Resolution 1541 (XV), Principle 9 of the Annex taking into account referendums in North Borneo and Sarawak that would be free and without coercion. This also triggered the Indonesian confrontation because Indonesia opposed the violation of the agreements.

Cyprus was settled by Mycenaean Greeks in two waves in the 2nd millennium BC. As a strategic location in the Middle East, it was subsequently occupied by several major powers, including the empires of the Assyrians, Egyptians and Persians, from whom the island was seized in 333 BC by Alexander the Great. Subsequent rule by Ptolemaic Egypt, the Classical and Eastern Roman Empire, Arab caliphates for a short period and the French Lusignan dynasty. Following the death in 1473 of James II, the last Lusignan king, the Republic of Venice assumed control of the island, while the late king's Venetian widow, Queen Catherine Cornaro, reigned as figurehead. Venice formally annexed the Kingdom of Cyprus in 1489, following the abdication of Catherine. The Venetians fortified Nicosia by building the Walls of Nicosia, and used it as an important commercial hub.

Although the Lusignan French aristocracy remained the dominant social class in Cyprus throughout the medieval period, the former assumption that Greeks were treated only as serfs on the island is no longer considered by academics to be accurate. It is now accepted that the medieval period saw increasing numbers of Greek Cypriots elevated to the upper classes, a growing Greek middle ranks, and the Lusignan royal household even marrying Greeks. This included King John II of Cyprus who married Helena Palaiologina.

Throughout Venetian rule, the Ottoman Empire frequently raided Cyprus. In 1539 the Ottomans destroyed Limassol and so fearing the worst, the Venetians also fortified Famagusta and Kyrenia.

Invaded in 1570, Turks controlled and solely governed all of the Cyprus island from 1571 till its leasing to the United Kingdom in 1878. Cyprus was placed under British administration based on Cyprus Convention in 1878 and formally annexed by Britain in 1914. While Turkish Cypriots made up 18% of the population, the partition of Cyprus and creation of a Turkish state in the north became a policy of Turkish Cypriot leaders and Turkey in the 1950s. Politically, there was no majority/minority relation between Greek Cypriots and Turkish Cypriots; and hence, in 1960, Republic of Cyprus was founded by the constituent communities in Cyprus (Greek Cypriots and Turkish Cypriots) as a non-unitary state; the 1960 Constitution set both Turkish and Greek as the
official languages. During 1963–74, the island experienced ethnic clashes and turmoil, the coup to unify the island to Greece and eventual Turkish invasion in 1974. Turkish Republic of Northern Cyprus was declared in 1983 and recognized only by Turkey. Monroe Leigh, 1990, The Legal Status in International Law of the Turkish Cypriot and the Greek Cypriot Communities in Cyprus. The Greek Cypriot and Turkish Cypriot regimes participating in these negotiations, and the respective communities which they represent, are presently entitled to exercise equal rights under international law, including rights of self-determination. Before the Turkey's invasion in 1974, Turkish Cypriots were concentrated in Turkish Cypriot enclaves in the island.

Northern Cyprus fulfills all the classical criteria of statehood. United Nations Peace Force in Cyprus (UNFICYP) operates based on the laws of Northern Cyprus in north of Cyprus island. According to European Court of Human Rights (ECtHR), the laws of Northern Cyprus is valid in the north of Cyprus. ECtHR did "not" accept the claim that the Courts of Northern Cyprus lacked "independence and/or impartiality". ECtHR directed all Cypriots to exhaust "domestic remedies" applied by Northern Cyprus before taking their cases to ECtHR. In 2014, United States' Federal Court qualified Turkish Republic of Northern Cyprus as a "democratic country". In 2017, United Kingdom's High Court decided that "There was no duty in UK law upon the UK's Government to refrain from recognising Northern Cyprus. The United Nations itself works with Northern Cyprus law enforcement agencies and facilitates cooperation between the two parts of the island." UK's High Court also dismissed the claim that "cooperation between UK police and law agencies in northern Cyprus was illegal".

In Canada, many in the province of Quebec have wanted the province to separate from Confederation. The Parti Québécois has asserted Quebec's "right to self-determination. " There is debate on under which conditions would this right be realized. French-speaking Quebec nationalism and support for maintaining Québécois culture would inspire Quebec nationalists, many of whom were supporters of the Quebec sovereignty movement during the late-20th century.

Scotland has a long-standing independence movement, with polls suggesting in January 2020 that 52% of eligible voters would vote for an independent Scotland. The country's largest political party, The SNP
, campaigns for Scottish independence. A referendum on independence was held in 2014, where it was rejected by 55% of voters. The Independence debate was reignited in the wake of the UK referendum on EU membership where Scotland voted overwhelmingly to remain a member of the EU. Results in the rest of the UK, however, led to Scotland being taken out of the EU.. In late 2019 the Scottish Government announced plans to hold another referendum on Scottish Independence. This was given assent by the Scottish Parliament but, as of February 2020, the UK Prime Minister has refused to grant the powers required to hold the referendum.

Section 235 of the South African Constitution allows for the right to self-determination of a community, within the framework of "the right of the South African people as a whole to self-determination", and pursuant to national legislation. This section of the constitution was one of the negotiated settlements during the handing over of political power in 1994. Supporters of an independent Afrikaner homeland have argued that their goals are reasonable under this new legislation.

In Italy, South Tyrol/Alto Adige was annexed after the First World War. The German-speaking inhabitants of South Tyrol are protected by the Gruber-De Gasperi Agreement, but there are still supporters of the self determination of South Tyrol, e.g. the party Die Freiheitlichen and the South Tyrolean independence movement. At the end of WWII the Allies offered to separate South Tyrol from Italy, but the South Tyrolean People's Party refused, preferring to obtain huge fiscal and economic advantages from Rome.

The colonization of the North American continent and its Native American population has been the source of legal battles since the early 19th century. Many Native American tribes were resettled onto separate tracts of land (reservations), which have retained a certain degree of autonomy within the United States. The federal government recognizes Tribal Sovereignty and has established a number of laws attempting to clarify the relationship among the federal, state, and tribal governments. The Constitution and later federal laws recognize the local sovereignty of tribal nations, but do not recognize full sovereignty equivalent to that of foreign nations, hence the term "domestic dependent nations" to qualify the federally recognized tribes.

Certain Chicano nationalist groups seek to "recreate" an ethnic-based state to be called Aztlán, after the legendary homeland of the Aztecs. It would comprise the Southwestern United States, historic territory of indigenous peoples and their descendants, as well as colonists and later settlers under the Spanish colonial and Mexican governments. Black nationalists have argued that, by virtue of slaves' unpaid labor and the harsh experiences of African Americans under slavery and Jim Crow, African Americans have a moral claim to the areas where the highest percentage of the population classified as black lives. They believe this area should be the basis of forming an independent state of New Afrika, designed to have an African-American majority and political control.

There are several active Hawaiian autonomy or independence movements, each with the goal of realizing some level of political control over single or several islands. The groups range from those seeking territorial units similar to Indian reservations under the United States, with the least amount of independent control, to the Hawaiian sovereignty movement, which is projected to have the most independence. The Hawaiian Sovereignty movement seeks to revive the Hawaiian nation under the Hawaiian constitution. Supporters of this concept say that Hawaii retained its sovereignty while under control of the United States.

Since 1972, the U.N. Decolonization Committee has called for Puerto Rico's "decolonization" and for the US to recognize the island's right to self-determination and independence. In 2007 the Decolonization Subcommittee called for the United Nations General Assembly to review the political status of Puerto Rico, a power reserved by the 1953 Resolution. This followed the 1967 passage of a plebiscite act that provided for a vote on the status of Puerto Rico with three status options: continued commonwealth, statehood, and independence. In the first plebscite, the commonwealth option won with 60.4% of the votes, but US congressional committees failed to enact legislation to address the status issue. In subsequent plebiscites in 1993 and 1998, the status quo was favored.

In a referendum that took place in November 2012, a majority of Puerto Rican residents voted to change the territory's relationship with the United States, with the statehood option being the preferred option. But a large number of ballots—one-third of all votes cast—were left blank on the question of preferred alternative status. Supporters of the commonwealth status had urged voters to blank their ballots. When the blank votes are counted as anti-statehood votes, the statehood option would have received less than 50% of all ballots received. As of January 2014, Washington has not taken action to address the results of this plebiscite.

Many current US state, regional and city secession groups use the language of self-determination. A 2008 Zogby International poll revealed that 22% of Americans believe that "any state or region has the right to peaceably secede and become an independent republic."

Since the late 20th century, some states periodically discuss desires to secede from the United States. Unilateral secession was ruled unconstitutional by the US Supreme Court in "Texas v. White" (1869).

In the case of Hawaii, the struggle for self-determination does not fall under secession, as it is less a break from federal administration, than a return to the process through which cession was claimed to have occurred: namely the ongoing occupation via a US imposed military coup; and/or removal from the UN list of Non-Self-Governing Territories. to educate or properly inform the citizenry of Hawaii of its options for self-determination and sidestepped guidelines laid out in UN General Assembly resolution 742 (1953).

The self-determination of the West Papuan people has been violently suppressed by the Indonesian government since the withdrawal of Dutch colonial rule under the Netherlands New Guinea in 1962.

There is an active movement based on the self-determination of the Sahrawi people in the Western Sahara region. Morocco also claims the entire territory, and maintains control of about two-thirds of the region.



</doc>
<doc id="29271" url="https://en.wikipedia.org/wiki?curid=29271" title="Scale">
Scale

Scale or scale may refer to:










</doc>
<doc id="29275" url="https://en.wikipedia.org/wiki?curid=29275" title="Southcentral Alaska">
Southcentral Alaska

Southcentral Alaska () is the portion of the U.S. state of Alaska consisting of the shorelines and uplands of the central Gulf of Alaska. Most of the population of the state lives in this region, concentrated in and around the city of Anchorage.

The area includes Cook Inlet, the Matanuska-Susitna Valley, the Kenai Peninsula, Prince William Sound, and the Copper River Valley. Tourism, fisheries, and petroleum production are important economic activities.

The major city is Anchorage. Other towns include Palmer, Wasilla, Kenai, Soldotna, Homer, Seward, Valdez, and Cordova.

The climate of Southcentral Alaska is subarctic. Temperatures range from an average high of 65°F (18°C) in July to an average low of 10°F (-12°C) in December. The hours of daylight per day varies from 20 hours in June and July to 6 hours in December and January. The coastal areas consist of temperate rainforests and alder shrublands. The interior areas are covered by boreal forests.

The terrain of Southcentral Alaska is shaped by six mountain ranges:

Southcentral Alaska contains several dormant and active volcanoes. The Wrangell Volcanoes are older, lie in the East, and include Mount Blackburn, Mount Bona, Mount Churchill, Mount Drum, Mount Gordon, Mount Jarvis, Mount Sanford, and Mount Wrangell. The Cook Inlet volcanoes, located in the Tordrillo Mountains and in the north end of the Aleutian Range, are newer, lie in the West, and include Mount Redoubt, Mount Iliamna, Hayes Volcano, Mount Augustine, Fourpeaked Mountain and Mount Spurr. Most recently, Augustine and Fourpeaked erupted in 2006, and Mount Redoubt erupted in March 2009, resulting in airplane flight cancellations.



</doc>
<doc id="29276" url="https://en.wikipedia.org/wiki?curid=29276" title="Spinor">
Spinor

In geometry and physics, spinors are elements of a complex vector space that can be associated with Euclidean space. Like geometric vectors and more general tensors, spinors transform linearly when the Euclidean space is subjected to a slight (infinitesimal) rotation. However, when a sequence of such small rotations is composed (integrated) to form an overall final rotation, the resulting spinor transformation depends on which sequence of small rotations was used. Unlike vectors and tensors, a spinor transforms to its negative when the space is continuously rotated through a complete turn from 0° to 360° (see picture). This property characterizes spinors: spinors can be viewed as the "square roots" of vectors (although this is inaccurate and may be misleading; they are better viewed as "square roots" of sections of vector bundles – in the case of the exterior algebra bundle of the cotangent bundle, they thus become "square roots" of differential forms).

It is also possible to associate a substantially similar notion of spinor to Minkowski space, in which case the Lorentz transformations of special relativity play the role of rotations. Spinors were introduced in geometry by Élie Cartan in 1913. In the 1920s physicists discovered that spinors are essential to describe the intrinsic angular momentum, or "spin", of the electron and other subatomic particles.

Spinors are characterized by the specific way in which they behave under rotations. They change in different ways depending not just on the overall final rotation, but the details of how that rotation was achieved (by a continuous path in the rotation group). There are two topologically distinguishable classes (homotopy classes) of paths through rotations that result in the same overall rotation, as illustrated by the belt trick puzzle. These two inequivalent classes yield spinor transformations of opposite sign. The spin group is the group of all rotations keeping track of the class. It doubly covers the rotation group, since each rotation can be obtained in two inequivalent ways as the endpoint of a path. The space of spinors by definition is equipped with a (complex) linear representation of the spin group, meaning that elements of the spin group act as linear transformations on the space of spinors, in a way that genuinely depends on the homotopy class. In mathematical terms, spinors are described by a double-valued projective representation of the rotation group SO(3).

Although spinors can be defined purely as elements of a representation space of the spin group (or its Lie algebra of infinitesimal rotations), they are typically defined as elements of a vector space that carries a linear representation of the Clifford algebra. The Clifford algebra is an associative algebra that can be constructed from Euclidean space and its inner product in a basis-independent way. Both the spin group and its Lie algebra are embedded inside the Clifford algebra in a natural way, and in applications the Clifford algebra is often the easiest to work with. A Clifford space operates on a spinor space, and the elements of a spinor space are spinors. After choosing an orthonormal basis of Euclidean space, a representation of the Clifford algebra is generated by gamma matrices, matrices that satisfy a set of canonical anti-commutation relations. The spinors are the column vectors on which these matrices act. In three Euclidean dimensions, for instance, the Pauli spin matrices are a set of gamma matrices, and the two-component complex column vectors on which these matrices act are spinors. However, the particular matrix representation of the Clifford algebra, hence what precisely constitutes a "column vector" (or spinor), involves the choice of basis and gamma matrices in an essential way. As a representation of the spin group, this realization of spinors as (complex) column vectors will either be irreducible if the dimension is odd, or it will decompose into a pair of so-called "half-spin" or Weyl representations if the dimension is even.

What characterizes spinors and distinguishes them from geometric vectors and other tensors is subtle. Consider applying a rotation to the coordinates of a system. No object in the system itself has moved, only the coordinates have, so there will always be a compensating change in those coordinate values when applied to any object of the system. Geometrical vectors, for example, have components that will undergo "the same" rotation as the coordinates. More broadly, any tensor associated with the system (for instance, the stress of some medium) also has coordinate descriptions that adjust to compensate for changes to the coordinate system itself.

Spinors do not appear at this level of the description of a physical system, when one is concerned only with the properties of a single isolated rotation of the coordinates. Rather, spinors appear when we imagine that instead of a single rotation, the coordinate system is gradually (continuously) rotated between some initial and final configuration. For any of the familiar and intuitive ("tensorial") quantities associated with the system, the transformation law does not depend on the precise details of how the coordinates arrived at their final configuration. Spinors, on the other hand, are constructed in such a way that makes them "sensitive" to how the gradual rotation of the coordinates arrived there: They exhibit path-dependence. It turns out that, for any final configuration of the coordinates, there are actually two ("topologically") inequivalent "gradual" (continuous) rotations of the coordinate system that result in this same configuration. This ambiguity is called the homotopy class of the gradual rotation. The belt trick puzzle (shown) demonstrates two different rotations, one through an angle of 2 and the other through an angle of 4, having the same final configurations but different classes. Spinors actually exhibit a sign-reversal that genuinely depends on this homotopy class. This distinguishes them from vectors and other tensors, none of which can feel the class.

Spinors can be exhibited as concrete objects using a choice of Cartesian coordinates. In three Euclidean dimensions, for instance, spinors can be constructed by making a choice of Pauli spin matrices corresponding to (angular momenta about) the three coordinate axes. These are 2×2 matrices with complex entries, and the two-component complex column vectors on which these matrices act by matrix multiplication are the spinors. In this case, the spin group is isomorphic to the group of 2×2 unitary matrices with determinant one, which naturally sits inside the matrix algebra. This group acts by conjugation on the real vector space spanned by the Pauli matrices themselves, realizing it as a group of rotations among them, but it also acts on the column vectors (that is, the spinors).

More generally, a Clifford algebra can be constructed from any vector space "V" equipped with a (nondegenerate) quadratic form, such as Euclidean space with its standard dot product or Minkowski space with its standard Lorentz metric. The space of spinors is the space of column vectors with formula_1 components. The orthogonal Lie algebra (i.e., the infinitesimal "rotations") and the spin group associated to the quadratic form are both (canonically) contained in the Clifford algebra, so every Clifford algebra representation also defines a representation of the Lie algebra and the spin group. Depending on the dimension and metric signature, this realization of spinors as column vectors may be irreducible or it may decompose into a pair of so-called "half-spin" or Weyl representations. When the vector space "V" is four-dimensional, the algebra is described by the gamma matrices.

The space of spinors is formally defined as the fundamental representation of the Clifford algebra. (This may or may not decompose into irreducible representations.) The space of spinors may also be defined as a spin representation of the orthogonal Lie algebra. These spin representations are also characterized as the finite-dimensional projective representations of the special orthogonal group that do not factor through linear representations. Equivalently, a spinor is an element of a finite-dimensional group representation of the spin group on which the center acts non-trivially.

There are essentially two frameworks for viewing the notion of a spinor.

From a representation theoretic point of view, one knows beforehand that there are some representations of the Lie algebra of the orthogonal group that cannot be formed by the usual tensor constructions. These missing representations are then labeled the spin representations, and their constituents "spinors". From this view, a spinor must belong to a representation of the double cover of the rotation group , or more generally of a double cover of the generalized special orthogonal group on spaces with a metric signature of . These double covers are Lie groups, called the spin groups or . All the properties of spinors, and their applications and derived objects, are manifested first in the spin group. Representations of the double covers of these groups yield double-valued projective representations of the groups themselves. (This means that the action of a particular rotation on vectors in the quantum Hilbert space is only defined up to a sign.)

From a geometrical point of view, one can explicitly construct the spinors and then examine how they behave under the action of the relevant Lie groups. This latter approach has the advantage of providing a concrete and elementary description of what a spinor is. However, such a description becomes unwieldy when complicated properties of the spinors, such as Fierz identities, are needed.

The language of Clifford algebras (sometimes called geometric algebras) provides a complete picture of the spin representations of all the spin groups, and the various relationships between those representations, via the classification of Clifford algebras. It largely removes the need for "ad hoc" constructions.

In detail, let "V" be a finite-dimensional complex vector space with nondegenerate bilinear form "g". The Clifford algebra is the algebra generated by "V" along with the anticommutation relation It is an abstract version of the algebra generated by the gamma or Pauli matrices. If "V" = , with the standard form we denote the Clifford algebra by Cℓ(). Since by the choice of an orthonormal basis every complex vectorspace with non-degenerate form is isomorphic to this standard example, this notation is abused more generally if . If is even, Cℓ() is isomorphic as an algebra (in a non-unique way) to the algebra of complex matrices (by the Artin-Wedderburn theorem and the easy to prove fact that the Clifford algebra is central simple). If is odd, Cℓ() is isomorphic to the algebra of two copies of the complex matrices. Therefore, in either case has a unique (up to isomorphism) irreducible representation (also called simple Clifford module), commonly denoted by Δ, of dimension 2. Since the Lie algebra is embedded as a Lie subalgebra in equipped with the Clifford algebra commutator as Lie bracket, the space Δ is also a Lie algebra representation of called a spin representation. If "n" is odd, this Lie algebra representation is irreducible. If "n" is even, it splits further into two irreducible representations called the Weyl or "half-spin representations".

Irreducible representations over the reals in the case when "V" is a real vector space are much more intricate, and the reader is referred to the Clifford algebra article for more details.

Spinors form a vector space, usually over the complex numbers, equipped with a linear group representation of the spin group that does not factor through a representation of the group of rotations (see diagram). The spin group is the group of rotations keeping track of the homotopy class. Spinors are needed to encode basic information about the topology of the group of rotations because that group is not simply connected, but the simply connected spin group is its double cover. So for every rotation there are two elements of the spin group that represent it. Geometric vectors and other tensors cannot feel the difference between these two elements, but they produce "opposite" signs when they affect any spinor under the representation. Thinking of the elements of the spin group as homotopy classes of one-parameter families of rotations, each rotation is represented by two distinct homotopy classes of paths to the identity. If a one-parameter family of rotations is visualized as a ribbon in space, with the arc length parameter of that ribbon being the parameter (its tangent, normal, binormal frame actually gives the rotation), then these two distinct homotopy classes are visualized in the two states of the belt trick puzzle (above). The space of spinors is an auxiliary vector space that can be constructed explicitly in coordinates, but ultimately only exists up to isomorphism in that there is no "natural" construction of them that does not rely on arbitrary choices such as coordinate systems. A notion of spinors can be associated, as such an auxiliary mathematical object, with any vector space equipped with a quadratic form such as Euclidean space with its standard dot product, or Minkowski space with its Lorentz metric. In the latter case, the "rotations" include the Lorentz boosts, but otherwise the theory is substantially similar.

The constructions given above, in terms of Clifford algebra or representation theory, can be thought of as defining spinors as geometric objects in zero-dimensional space-time. To obtain the spinors of physics, such as the Dirac spinor, one extends the construction to obtain a spin structure on 4-dimensional space-time (Minkowski space). Effectively, one starts with the tangent manifold of space-time, each point of which is a 4-dimensional vector space with "SO"(3,1) symmetry, and then builds the spin group at each point. The neighborhoods of points are endowed with concepts of smoothness and differentiability: the standard construction is one of a fibre bundle, the fibers of which are affine spaces transforming under the spin group. After constructing the fiber bundle, one may then consider differential equations, such as the Dirac equation, or the Weyl equation on the fiber bundle. These equations (Dirac or Weyl) have solutions that are plane waves, having symmetries characteristic of the fibers, "i.e." having the symmetries of spinors, as obtained from the (zero-dimensional) Clifford algebra/spin representation theory described above. Such plane-wave solutions (or other solutions) of the differential equations can then properly be called fermions; fermions have the algebraic qualities of spinors. By general convention, the terms "fermion" and "spinor" are often used interchangeably in physics, as synonyms of one-another.

It appears that all fundamental particles in nature that are spin-1/2 are described by the Dirac equation, with the possible exception of the neutrino. There does not seem to be any "a priori" reason why this would be the case. A perfectly valid choice for spinors would be the non-complexified version of , the Majorana spinor. There also does not seem to be any particular prohibition to having Weyl spinors appear in nature as fundamental particles.

The Dirac, Weyl, and Majorana spinors are interrelated, and their relation can be elucidated on the basis of real geometric algebra. Dirac and Weyl spinors are complex representations while Majorana spinors are real representations.

Weyl spinors are insufficient to describe massive particles, such as electrons, since the Weyl plane-wave solutions necessarily travel at the speed of light; for massive particles, the Dirac equation is needed. The initial construction of the Standard Model of particle physics starts with both the electron and the neutrino as massless Weyl spinors; the Higgs mechanism gives electrons a mass; the classical neutrino remained massless, and was thus an example of a Weyl spinor. However, because of observed neutrino oscillation, it is now believed that they are not Weyl spinors, but perhaps instead Majorana spinors. It is not known whether Weyl spinor fundamental particles exist in nature.

The situation for condensed matter physics is different: one can can construct two and three-dimensional "spacetimes" in a large variety of different physical materials, ranging from semiconductors to far more exotic materials. In 2015, an international team led by Princeton University scientists announced that they had found a quasiparticle that behaves as a Weyl fermion.

One major mathematical application of the construction of spinors is to make possible the explicit construction of linear representations of the Lie algebras of the special orthogonal groups, and consequently spinor representations of the groups themselves. At a more profound level, spinors have been found to be at the heart of approaches to the Atiyah–Singer index theorem, and to provide constructions in particular for discrete series representations of semisimple groups.

The spin representations of the special orthogonal Lie algebras are distinguished from the tensor representations given by Weyl's construction by the weights. Whereas the weights of the tensor representations are integer linear combinations of the roots of the Lie algebra, those of the spin representations are half-integer linear combinations thereof. Explicit details can be found in the spin representation article.

The spinor can be described, in simple terms, as "vectors of a space the transformations of which are related in a particular way to rotations in physical space". Stated differently:
Several ways of illustrating everyday analogies have been formulated in terms of the plate trick, tangloids and other examples of orientation entanglement.

Nonetheless, the concept is generally considered notoriously difficult to understand, as illustrated by Michael Atiyah's statement that is recounted by Dirac's biographer Graham Farmelo:
The most general mathematical form of spinors was discovered by Élie Cartan in 1913. The word "spinor" was coined by Paul Ehrenfest in his work on quantum physics.

Spinors were first applied to mathematical physics by Wolfgang Pauli in 1927, when he introduced his spin matrices. The following year, Paul Dirac discovered the fully relativistic theory of electron spin by showing the connection between spinors and the Lorentz group. By the 1930s, Dirac, Piet Hein and others at the Niels Bohr Institute (then known as the Institute for Theoretical Physics of the University of Copenhagen) created toys such as Tangloids to teach and model the calculus of spinors.

Spinor spaces were represented as left ideals of a matrix algebra in 1930, by G. Juvet and by Fritz Sauter. More specifically, instead of representing spinors as complex-valued 2D column vectors as Pauli had done, they represented them as complex-valued 2 × 2 matrices in which only the elements of the left column are non-zero. In this manner the spinor space became a minimal left ideal in .

In 1947 Marcel Riesz constructed spinor spaces as elements of a minimal left ideal of Clifford algebras. In 1966/1967, David Hestenes replaced spinor spaces by the even subalgebra Cℓ() of the spacetime algebra Cℓ(). As of the 1980s, the theoretical physics group at Birkbeck College around David Bohm and Basil Hiley has been developing algebraic approaches to quantum theory that build on Sauter and Riesz' identification of spinors with minimal left ideals.

Some simple examples of spinors in low dimensions arise from considering the even-graded subalgebras of the Clifford algebra . This is an algebra built up from an orthonormal basis of mutually orthogonal vectors under addition and multiplication, "p" of which have norm +1 and "q" of which have norm −1, with the product rule for the basis vectors

The Clifford algebra Cℓ() is built up from a basis of one unit scalar, 1, two orthogonal unit vectors, "σ" and "σ", and one unit pseudoscalar . From the definitions above, it is evident that , and .

The even subalgebra Cℓ(), spanned by "even-graded" basis elements of Cℓ(), determines the space of spinors via its representations. It is made up of real linear combinations of 1 and "σ""σ". As a real algebra, Cℓ() is isomorphic to the field of complex numbers . As a result, it admits a conjugation operation (analogous to complex conjugation), sometimes called the "reverse" of a Clifford element, defined by

which, by the Clifford relations, can be written

The action of an even Clifford element on vectors, regarded as 1-graded elements of Cℓ(), is determined by mapping a general vector to the vector

where "γ" is the conjugate of "γ", and the product is Clifford multiplication. In this situation, a spinor is an ordinary complex number. The action of "γ" on a spinor "φ" is given by ordinary complex multiplication:

An important feature of this definition is the distinction between ordinary vectors and spinors, manifested in how the even-graded elements act on each of them in different ways. In general, a quick check of the Clifford relations reveals that even-graded elements conjugate-commute with ordinary vectors:

On the other hand, comparing with the action on spinors , "γ" on ordinary vectors acts as the "square" of its action on spinors.

Consider, for example, the implication this has for plane rotations. Rotating a vector through an angle of "θ" corresponds to , so that the corresponding action on spinors is via . In general, because of logarithmic branching, it is impossible to choose a sign in a consistent way. Thus the representation of plane rotations on spinors is two-valued.

In applications of spinors in two dimensions, it is common to exploit the fact that the algebra of even-graded elements (that is just the ring of complex numbers) is identical to the space of spinors. So, by abuse of language, the two are often conflated. One may then talk about "the action of a spinor on a vector." In a general setting, such statements are meaningless. But in dimensions 2 and 3 (as applied, for example, to computer graphics) they make sense.





The Clifford algebra Cℓ() is built up from a basis of one unit scalar, 1, three orthogonal unit vectors, "σ", "σ" and "σ", the three unit bivectors "σ""σ", "σ""σ", "σ""σ" and the pseudoscalar . It is straightforward to show that , and .

The sub-algebra of even-graded elements is made up of scalar dilations,

and vector rotations

where

corresponds to a vector rotation through an angle "θ" about an axis defined by a unit vector .

As a special case, it is easy to see that, if , this reproduces the "σ""σ" rotation considered in the previous section; and that such rotation leaves the coefficients of vectors in the "σ" direction invariant, since

The bivectors "σ""σ", "σ""σ" and "σ""σ" are in fact Hamilton's quaternions i, j, and k, discovered in 1843:

With the identification of the even-graded elements with the algebra of quaternions, as in the case of two dimensions the only representation of the algebra of even-graded elements is on itself. Thus the (real) spinors in three-dimensions are quaternions, and the action of an even-graded element on a spinor is given by ordinary quaternionic multiplication.

Note that the expression (1) for a vector rotation through an angle , "the angle appearing in γ was halved". Thus the spinor rotation (ordinary quaternionic multiplication) will rotate the spinor through an angle one-half the measure of the angle of the corresponding vector rotation. Once again, the problem of lifting a vector rotation to a spinor rotation is two-valued: the expression (1) with in place of "θ"/2 will produce the same vector rotation, but the negative of the spinor rotation.

The spinor/quaternion representation of rotations in 3D is becoming increasingly prevalent in computer geometry and other applications, because of the notable brevity of the corresponding spin matrix, and the simplicity with which they can be multiplied together to calculate the combined effect of successive rotations about different axes.

A space of spinors can be constructed explicitly with concrete and abstract constructions. The
equivalence of these constructions are a consequence of the uniqueness of the spinor representation of the complex Clifford algebra. For a complete example in dimension 3, see spinors in three dimensions.

Given a vector space "V" and a quadratic form "g" an explicit matrix representation of the Clifford algebra can be defined as follows. Choose an orthonormal basis for "V" i.e. where and for . Let . Fix a set of matrices such that (i.e. fix a convention for the gamma matrices). Then the assignment extends uniquely to an algebra homomorphism by sending the monomial in the Clifford algebra to the product of matrices and extending linearly. The space on which the gamma matrices act is now a space of spinors. One needs to construct such matrices explicitly, however. In dimension 3, defining the gamma matrices to be the Pauli sigma matrices gives rise to the familiar two component spinors used in non relativistic quantum mechanics. Likewise using the Dirac gamma matrices gives rise to the 4 component Dirac spinors used in 3+1 dimensional relativistic quantum field theory. In general, in order to define gamma matrices of the required kind, one can use the Weyl–Brauer matrices.

In this construction the representation of the Clifford algebra , the Lie algebra , and the Spin group , all depend on the choice of the orthonormal basis and the choice of the gamma matrices. This can cause confusion over conventions, but invariants like traces are independent of choices. In particular, all physically observable quantities must be independent of such choices. In this construction a spinor can be represented as a vector of 2 complex numbers and is denoted with spinor indices (usually "α", "β", "γ"). In the physics literature, abstract spinor indices are often used to denote spinors even when an abstract spinor construction is used.

There are at least two different, but essentially equivalent, ways to define spinors abstractly. One approach seeks to identify the minimal ideals for the left action of on itself. These are subspaces of the Clifford algebra of the form , admitting the evident action of by left-multiplication: . There are two variations on this theme: one can either find a primitive element that is a nilpotent element of the Clifford algebra, or one that is an idempotent. The construction via nilpotent elements is more fundamental in the sense that an idempotent may then be produced from it. In this way, the spinor representations are identified with certain subspaces of the Clifford algebra itself. The second approach is to construct a vector space using a distinguished subspace of , and then specify the action of the Clifford algebra "externally" to that vector space.

In either approach, the fundamental notion is that of an isotropic subspace . Each construction depends on an initial freedom in choosing this subspace. In physical terms, this corresponds to the fact that there is no measurement protocol that can specify a basis of the spin space, even if a preferred basis of is given.

As above, we let be an -dimensional complex vector space equipped with a nondegenerate bilinear form. If is a real vector space, then we replace by its complexification and let denote the induced bilinear form on . Let be a maximal isotropic subspace, i.e. a maximal subspace of such that . If is even, then let be an isotropic subspace complementary to . If is odd, let be a maximal isotropic subspace with , and let be the orthogonal complement of . In both the even- and odd-dimensional cases and have dimension . In the odd-dimensional case, is one-dimensional, spanned by a unit vector .

Since "W" is isotropic, multiplication of elements of "W" inside is skew. Hence vectors in "W" anti-commute, and is just the exterior algebra Λ"W". Consequently, the "k"-fold product of "W" with itself, "W", is one-dimensional. Let "ω" be a generator of "W". In terms of a basis of in "W", one possibility is to set

Note that (i.e., "ω" is nilpotent of order 2), and moreover, for all . The following facts can be proven easily:

In detail, suppose for instance that "n" is even. Suppose that "I" is a non-zero left ideal contained in . We shall show that "I" must be equal to by proving that it contains a nonzero scalar multiple of "ω".

Fix a basis "w" of "W" and a complementary basis "w"′ of "W" so that

Note that any element of "I" must have the form "αω", by virtue of our assumption that . Let be any such element. Using the chosen basis, we may write

where the "a" are scalars, and the "B" are auxiliary elements of the Clifford algebra. Observe now that the product
Pick any nonzero monomial "a" in the expansion of "α" with maximal homogeneous degree in the elements "w":
then
is a nonzero scalar multiple of "ω", as required.

Note that for "n" even, this computation also shows that
as a vector space. In the last equality we again used that "W" is isotropic. In physics terms, this shows that Δ is built up like a Fock space by creating spinors using anti-commuting creation operators in "W" acting on a vacuum "ω".

The computations with the minimal ideal construction suggest that a spinor representation can
also be defined directly using the exterior algebra of the isotropic subspace "W".
Let denote the exterior algebra of "W" considered as vector space only. This will be the spin representation, and its elements will be referred to as spinors.

The action of the Clifford algebra on Δ is defined first by giving the action of an element of "V" on Δ, and then showing that this action respects the Clifford relation and so extends to a homomorphism of the full Clifford algebra into the endomorphism ring End(Δ) by the universal property of Clifford algebras. The details differ slightly according to whether the dimension of "V" is even or odd.

When dim() is even, where "W′" is the chosen isotropic complement. Hence any decomposes uniquely as with and . The action of on a spinor is given by
where "i"("w′") is interior product with "w′" using the non degenerate quadratic form to identify "V" with "V", and ε(w) denotes the exterior product. It may be verified that
and so respects the Clifford relations and extends to a homomorphism from the Clifford algebra to End(Δ).

The spin representation Δ further decomposes into a pair of irreducible complex representations of the Spin group (the half-spin representations, or Weyl spinors) via

When dim("V") is odd, , where "U" is spanned by a unit vector "u" orthogonal to "W". The Clifford action "c" is defined as before on , while the Clifford action of (multiples of) "u" is defined by
As before, one verifies that "c" respects the Clifford relations, and so induces a homomorphism.

If the vector space "V" has extra structure that provides a decomposition of its complexification into two maximal isotropic subspaces, then the definition of spinors (by either method) becomes natural.

The main example is the case that the real vector space "V" is a hermitian vector space , i.e., "V" is equipped with a complex structure "J" that is an orthogonal transformation with respect to the inner product "g" on "V". Then splits in the ±"i" eigenspaces of "J". These eigenspaces are isotropic for the complexification of "g" and can be identified with the complex vector space and its complex conjugate . Therefore, for a hermitian vector space the vector space Λ (as well as its complex conjugate Λ"V") is a spinor space for the underlying real euclidean vector space.

With the Clifford action as above but with contraction using the hermitian form, this construction gives a spinor space at every point of an almost Hermitian manifold and is the reason why every almost complex manifold (in particular every symplectic manifold) has a Spin structure. Likewise, every complex vector bundle on a manifold carries a Spin structure.

A number of Clebsch–Gordan decompositions are possible on the tensor product of one spin representation with another. These decompositions express the tensor product in terms of the alternating representations of the orthogonal group.

For the real or complex case, the alternating representations are

In addition, for the real orthogonal groups, there are three characters (one-dimensional representations)

The Clebsch–Gordan decomposition allows one to define, among other things:

If is even, then the tensor product of Δ with the contragredient representation decomposes as
which can be seen explicitly by considering (in the Explicit construction) the action of the Clifford algebra on decomposable elements . The rightmost formulation follows from the transformation properties of the Hodge star operator. Note that on restriction to the even Clifford algebra, the paired summands are isomorphic, but under the full Clifford algebra they are not.

There is a natural identification of Δ with its contragredient representation via the conjugation in the Clifford algebra:
So also decomposes in the above manner. Furthermore, under the even Clifford algebra, the half-spin representations decompose

For the complex representations of the real Clifford algebras, the associated reality structure on the complex Clifford algebra descends to the space of spinors (via the explicit construction in terms of minimal ideals, for instance). In this way, we obtain the complex conjugate of the representation Δ, and the following isomorphism is seen to hold:

In particular, note that the representation Δ of the orthochronous spin group is a unitary representation. In general, there are Clebsch–Gordan decompositions

In metric signature , the following isomorphisms hold for the conjugate half-spin representations
Using these isomorphisms, one can deduce analogous decompositions for the tensor products of the half-spin representations .

If is odd, then
In the real case, once again the isomorphism holds
Hence there is a Clebsch–Gordan decomposition (again using the Hodge star to dualize) given by

There are many far-reaching consequences of the Clebsch–Gordan decompositions of the spinor spaces. The most fundamental of these pertain to Dirac's theory of the electron, among whose basic requirements are




</doc>
<doc id="29278" url="https://en.wikipedia.org/wiki?curid=29278" title="Safety engineering">
Safety engineering

Safety engineering is an engineering discipline which assures that engineered systems provide acceptable levels of safety. It is strongly related to industrial engineering/systems engineering, and the subset system safety engineering. Safety engineering assures that a life-critical system behaves as needed, even when components fail.

Analysis techniques can be split into two categories: qualitative and quantitative methods. Both approaches share the goal of finding causal dependencies between a hazard on system level and failures of individual components. Qualitative approaches focus on the question "What must go wrong, such that a system hazard may occur?", while quantitative methods aim at providing estimations about probabilities, rates and/or severity of consequences.

The complexity of the technical systems such as Improvements of Design and Materials, Planned Inspections, Fool-proof design, and Backup Redundancy decreases risk and increases the cost. The risk can be decreased to ALARA (as low as reasonably achievable) or ALAPA (as low as practically achievable) levels.

Traditionally, safety analysis techniques rely solely on skill and expertise of the safety engineer. In the last decade model-based approaches have become prominent. In contrast to traditional methods, model-based techniques try to derive relationships between causes and consequences from some sort of model of the system.

The two most common fault modeling techniques are called failure mode and effects analysis and fault tree analysis. These techniques are just ways of finding problems and of making plans to cope with failures, as in probabilistic risk assessment. One of the earliest complete studies using this technique on a commercial nuclear plant was the WASH-1400 study, also known as the Reactor Safety Study or the Rasmussen Report.

Failure Mode and Effects Analysis (FMEA) is a bottom-up, inductive analytical method which may be performed at either the functional or piece-part level. For functional FMEA, failure modes are identified for each function in a system or equipment item, usually with the help of a functional block diagram. For piece-part FMEA, failure modes are identified for each piece-part component (such as a valve, connector, resistor, or diode). The effects of the failure mode are described, and assigned a probability based on the failure rate and failure mode ratio of the function or component. This quantiazation is difficult for software ---a bug exists or not, and the failure models used for hardware components do not apply. Temperature and age and manufacturing variability affect a resistor; they do not affect software.

Failure modes with identical effects can be combined and summarized in a Failure Mode Effects Summary. When combined with criticality analysis, FMEA is known as Failure Mode, Effects, and Criticality Analysis or FMECA, pronounced "fuh-MEE-kuh".

Fault tree analysis (FTA) is a top-down, deductive analytical method. In FTA, initiating primary events such as component failures, human errors, and external events are traced through Boolean logic gates to an undesired top event such as an aircraft crash or nuclear reactor core melt. The intent is to identify ways to make top events less probable, and verify that safety goals have been achieved.

Fault trees are a logical inverse of success trees, and may be obtained by applying de Morgan's theorem to success trees (which are directly related to reliability block diagrams).

FTA may be qualitative or quantitative. When failure and event probabilities are unknown, qualitative fault trees may be analyzed for minimal cut sets. For example, if any minimal cut set contains a single base event, then the top event may be caused by a single failure. Quantitative FTA is used to compute top event probability, and usually requires computer software such as CAFTA from the Electric Power Research Institute or SAPHIRE from the Idaho National Laboratory.

Some industries use both fault trees and event trees. An event tree starts from an undesired initiator (loss of critical supply, component failure etc.) and follows possible further system events through to a series of final consequences. As each new event is considered, a new node on the tree is added with a split of probabilities of taking either branch. The probabilities of a range of "top events" arising from the initial event can then be seen.

The offshore oil and gas industry uses a qualitative safety systems analysis technique to ensure the protection of offshore production systems and platforms. The analysis is used during the design phase to identify process engineering hazards together with risk mitigation measures. The methodology is described in the American Petroleum Institute Recommended Practice 14C "Analysis, Design, Installation, and Testing of Basic Surface Safety Systems for Offshore Production Platforms."

The technique uses system analysis methods to determine the safety requirements to protect any individual process component, e.g. a vessel, pipeline, or pump. The safety requirements of individual components are integrated into a complete platform safety system, including liquid containment and emergency support systems such as fire and gas detection.

The first stage of the analysis identifies individual process components, these can include: flowlines, headers, pressure vessels, atmospheric vessels, fired heaters, exhaust heated components, pumps, compressors, pipelines and heat exchangers. Each component is subject to a safety analysis to identify undesirable events (equipment failure, process upsets, etc.) for which protection must be provided. The analysis also identifies a detectable condition (e.g. high pressure) which is used to initiate actions to prevent or minimize the effect of undesirable events. A Safety Analysis Table (SAT) for pressure vessels includes the following details.

Other undesirable events for a pressure vessel are under-pressure, gas blowby, leak, and excess temperature together with their associated causes and detectable conditions.

Once the events, causes and detectable conditions have been identified the next stage of the methodology uses a Safety Analysis Checklist (SAC) for each component. This lists the safety devices that may be required or factors that negate the need for such a device. For example, for the case of liquid overflow from a vessel (as above) the SAC identifies:


The analysis ensures that two levels of protection are provided to mitigate each undesirable event. For example, for a pressure vessel subjected to over-pressure the primary protection would be a PSH (pressure switch high) to shut off inflow to the vessel, secondary protection would be provided by a pressure safety valve (PSV) on the vessel.

The next stage of the analysis relates all the sensing devices, shutdown valves (ESVs), trip systems and emergency support systems in the form of a Safety Analysis Function Evaluation (SAFE) chart.

X denotes that the detection device on the left (e.g. PSH) initiates the shutdown or warning action on the top right (e.g. ESV closure).

The SAFE chart constitutes the basis of Cause and Effect Charts which relate the sensing devices to shutdown valves and plant trips which defines the functional architecture of the process shutdown system.

The methodology also specifies the systems testing that is necessary to ensure the functionality of the protection systems.

API RP 14C was first published in June 1974. The 8th edition was published in February 2017. API RP 14C was adapted as ISO standard ISO 10418 in 1993 entitled "Petroleum and natural gas industries — Offshore production installations — Analysis, design, installation and testing of basic surface process safety systems." The latest 2003 edition of ISO 10418 is currently (2019) undergoing revision.

Typically, safety guidelines prescribe a set of steps, deliverable documents, and exit criterion focused around planning, analysis and design, implementation, verification and validation, configuration management, and quality assurance activities for the development of a safety-critical system. In addition, they typically formulate expectations regarding the creation and use of traceability in the project. For example, depending upon the criticality level of a requirement, the US Federal Aviation Administration guideline DO-178B/C requires traceability from requirements to design, and from requirements to source code and executable object code for software components of a system. Thereby, higher quality traceability information can simplify the certification process and help to establish trust in the maturity of the applied development process.

Usually a failure in safety-certified systems is acceptable if, on average, less than one life per 10 hours of continuous operation is lost to failure.{as per FAA document AC 25.1309-1A} Most Western nuclear reactors, medical equipment, and commercial aircraft are certified to this level. The cost versus loss of lives has been considered appropriate at this level (by FAA for aircraft systems under Federal Aviation Regulations).

Once a failure mode is identified, it can usually be mitigated by adding extra or redundant equipment to the system. For example, nuclear reactors contain dangerous radiation, and nuclear reactions can cause so much heat that no substance might contain them. Therefore, reactors have emergency core cooling systems to keep the temperature down, shielding to contain the radiation, and engineered barriers (usually several, nested, surmounted by a containment building) to prevent accidental leakage. Safety-critical systems are commonly required to permit no single event or component failure to result in a catastrophic failure mode.

Most biological organisms have a certain amount of redundancy: multiple organs, multiple limbs, etc.

For any given failure, a fail-over or redundancy can almost always be designed and incorporated into a system.

There are two categories of techniques to reduce the probability of failure:
Fault avoidance techniques increase the reliability of individual items (increased design margin, de-rating, etc.).
Fault tolerance techniques increase the reliability of the system as a whole (redundancies, barriers, etc.).

Safety engineering and reliability engineering have much in common, but safety is not reliability. If a medical device fails, it should fail safely; other alternatives will be available to the surgeon. If the engine on a single-engine aircraft fails, there is no backup. Electrical power grids are designed for both safety and reliability; telephone systems are designed for reliability, which becomes a safety issue when emergency (e.g. US "911") calls are placed.

Probabilistic risk assessment has created a close relationship between safety and reliability. Component reliability, generally defined in terms of component failure rate, and external event probability are both used in quantitative safety assessment methods such as FTA. Related probabilistic methods are used to determine system Mean Time Between Failure (MTBF), system availability, or probability of mission success or failure. Reliability analysis has a broader scope than safety analysis, in that non-critical failures are considered. On the other hand, higher failure rates are considered acceptable for non-critical systems.

Safety generally cannot be achieved through component reliability alone. Catastrophic failure probabilities of 10 per hour correspond to the failure rates of very simple components such as resistors or capacitors. A complex system containing hundreds or thousands of components might be able to achieve a MTBF of 10,000 to 100,000 hours, meaning it would fail at 10 or 10 per hour. If a system failure is catastrophic, usually the only practical way to achieve 10 per hour failure rate is through redundancy.

When adding equipment is impractical (usually because of expense), then the least expensive form of design is often "inherently fail-safe". That is, change the system design so its failure modes are not catastrophic. Inherent fail-safes are common in medical equipment, traffic and railway signals, communications equipment, and safety equipment.

The typical approach is to arrange the system so that ordinary single failures cause the mechanism to shut down in a safe way (for nuclear power plants, this is termed a passively safe design, although more than ordinary failures are covered). Alternately, if the system contains a hazard source such as a battery or rotor, then it may be possible to remove the hazard from the system so that its failure modes cannot be catastrophic. The U.S. Department of Defense Standard Practice for System Safety (MIL–STD–882) places the highest priority on elimination of hazards through design selection.

One of the most common fail-safe systems is the overflow tube in baths and kitchen sinks. If the valve sticks open, rather than causing an overflow and damage, the tank spills into an overflow. Another common example is that in an elevator the cable supporting the car keeps spring-loaded brakes open. If the cable breaks, the brakes grab rails, and the elevator cabin does not fall.

Some systems can never be made fail safe, as continuous availability is needed. For example, loss of engine thrust in flight is dangerous. Redundancy, fault tolerance, or recovery procedures are used for these situations (e.g. multiple independent controlled and fuel fed engines). This also makes the system less sensitive for the reliability prediction errors or quality induced uncertainty for the separate items. On the other hand, failure detection & correction and avoidance of common cause failures becomes here increasingly important to ensure system level reliability.





</doc>
<doc id="29279" url="https://en.wikipedia.org/wiki?curid=29279" title="SIGGRAPH">
SIGGRAPH

SIGGRAPH (Special Interest Group on Computer GRAPHics and Interactive Techniques) is an annual conference on computer graphics (CG) organized by the ACM SIGGRAPH, starting in 1974. The main conference is held in North America; SIGGRAPH Asia, a second conference held annually, has been held since 2008 in countries throughout Asia.

The conference incorporates both academic presentations as well as an industry trade show. Other events at the conference include educational courses and panel discussions on recent topics in computer graphics and interactive techniques.

The SIGGRAPH conference proceedings, which are published in the ACM Transactions on Graphics, has one of the highest impact factors among academic publications in the field of computer graphics. The paper acceptance rate for SIGGRAPH has historically been between 17% and 29%, with the average accept rate between 2015 and 2019 of 27%. The submitted papers are peer-reviewed under a process that was historically single-blind, but has recently changed to double-blind. The papers accepted for presentation at SIGGRAPH are printed since 2003 in a special issue of the "ACM Transactions on Graphics" journal.
Prior to 1992, SIGGRAPH papers were printed as part of the "Computer Graphics" publication; between 1993 and 2001, there was a dedicated "SIGGRAPH Conference Proceedings" series of publications.

SIGGRAPH has several awards programs to recognize contributions to computer graphics. The most prestigious is the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics. It has been awarded every two years since 1983 to recognize an individual's lifetime achievement in computer graphics.

The SIGGRAPH conference experienced significant growth starting in the 1970s, peaking around the turn of the century. A second conference, SIGGRAPH Asia, started in 2008.




</doc>
<doc id="29285" url="https://en.wikipedia.org/wiki?curid=29285" title="Semtex">
Semtex

Semtex is a general-purpose plastic explosive containing RDX and PETN. It is used in commercial blasting, demolition, and in certain military applications. 

Semtex was developed and manufactured in Czechoslovakia, originally under the name B 1 and then under the "Semtex" designation since 1964, labeled as "SEMTEX 1A", since 1967 as "SEMTEX H", and since 1987 as "SEMTEX 10".

Originally developed for Czechoslovak military use and export, Semtex eventually became popular with paramilitary groups and rebels or terrorists because prior to 2000 it was extremely difficult to detect, as in the case of Pan Am Flight 103.

The composition of the two most common variants differ according to their use. The 1A (or 10) variant is used for mining, and is based mostly on crystalline PETN. The versions 1AP and 2P are formed as hexagonal booster charges; a special assembly of PETN and wax inside the charge assures high reliability for detonating cord or detonator. The H (or SE) variant is intended for explosion hardening.

Semtex was invented in the late 1950s by Stanislav Brebera and Radim Fukátko, chemists at VCHZ Synthesia, Czechoslovakia (now Czech Republic). The explosive is named after Semtín, a suburb of Pardubice where the mixture was first manufactured starting in 1964. The plant was later renamed to become Explosia a.s., a subsidiary of Synthesia.

Semtex was very similar to other plastic explosives, especially C-4, in being highly malleable; but it is usable over a greater temperature range than other plastic explosives, since it stays plastic between −40 and +60 °C. It is also waterproof. There are visual differences between Semtex and other plastic explosives, too: while C-4 is off-white in colour, Semtex is red or brick-orange.

The new explosive was widely exported, notably to the government of North Vietnam, which received 14 tons during the Vietnam War. However, the main consumer was Libya; about 700 tons of Semtex were exported to Libya between 1975 and 1981 by Omnipol. It has also been used by Islamic militants in the Middle East and by the Provisional Irish Republican Army (PIRA) and the Irish National Liberation Army in Northern Ireland.

Sales declined after Semtex became closely associated with terrorist attacks. Rules governing the explosive's exportation were progressively tightened over the years, and since 2002 all of Explosia's trading has been controlled by a government ministry. , only approximately 10 tons of Semtex were produced annually, almost all for domestic use. On December 21, 1988, 12 ounces (340g) of Semtex brought down a Boeing 747 over Lockerbie, Scotland killing all 259 passengers and crew aboard the aircraft and 11 bystanders on the ground.

Also in response to international agreements, Semtex has a detection taggant added to produce a distinctive vapor signature to aid detection. First, ethylene glycol dinitrate was used, but was later switched to 2,3-dimethyl-2,3-dinitrobutane (DMDNB) or "p"-mononitrotoluene (1-methyl-4-nitrobenzene), which is used currently. According to the manufacturer, the taggant agent was voluntarily being added by 1991, years before the protocol became compulsory. Batches of Semtex made before 1990, however, are untagged, though it is not known whether there are still major stocks of such old batches of Semtex. According to the manufacturer, even this untagged Semtex can now be detected. The shelf life of Semtex was reduced from ten years before the 1990s to five years now. Explosia states that there is no compulsory tagging allowing reliable post-detonation detection of a certain plastic explosive (such as incorporating a unique metallic code into the mass of the explosive), so Semtex is not tagged in this way.

On 25 May 1997, Bohumil Šole, a scientist who claimed to have been involved with inventing Semtex, committed suicide at a spa in Jeseník by blowing himself up with explosives. Šole, 63, was being treated there for psychological problems. It was unclear what explosives were used. Twenty other people were hurt in the explosion, while six were seriously injured. According to the manufacturer, Explosia, he was not a member of the team that developed the explosive in the 1960s.

According to the producer's 2017 catalog, several variants of Semtex are offered: Semtex 1A, Semtex 1H, Semtex 10, Semtex 10-SE, Semtex S 30, Semtex C-4, Semtex PW 4, and Semtex 90.



</doc>
<doc id="29286" url="https://en.wikipedia.org/wiki?curid=29286" title="Schedl">
Schedl

Schedl is a German surname. Notable people with the surname include: 



</doc>
<doc id="29287" url="https://en.wikipedia.org/wiki?curid=29287" title="Lehi (militant group)">
Lehi (militant group)

Lehi (; "Lohamei Herut Israel – Lehi", "Fighters for the Freedom of Israel – Lehi"), often known pejoratively as the Stern Gang, was a Zionist paramilitary organization founded by Avraham ("Yair") Stern in Mandatory Palestine. Its avowed aim was to evict the British authorities from Palestine by resort to force, allowing unrestricted immigration of Jews and the formation of a Jewish state, a "new totalitarian Hebrew republic". It was initially called the National Military Organization in Israel, upon being founded in August 1940, but was renamed Lehi one month later. The group referred to its members as terrorists and admitted to having carried out terrorist attacks.

Lehi split from the Irgun militant group in 1940 in order to continue fighting the British during World War II. Lehi initially sought an alliance with Fascist Italy and Nazi Germany, offering to fight alongside them against the British in return for the transfer of all Jews from Nazi-occupied Europe to Palestine. Believing that Nazi Germany was a lesser enemy of the Jews than Britain, Lehi twice attempted to form an alliance with the Nazis. During World War II, it declared that it would establish a Jewish state based upon "nationalist and totalitarian principles". After Stern's death in 1942, the new leadership of Lehi began to move it towards support for Joseph Stalin's Soviet Union. In 1944, Lehi officially declared its support for National Bolshevism. It said that its National Bolshevism involved an amalgamation of left-wing and right-wing political elements – Stern said Lehi incorporated elements of both the left and the right – however this change was unpopular and Lehi began to lose support as a result.

Lehi and the Irgun were jointly responsible for the massacre in Deir Yassin. Lehi assassinated Lord Moyne, British Minister Resident in the Middle East, and made many other attacks on the British in Palestine. On 29 May 1948, the government of Israel, having inducted its activist members into the Israel Defense Forces, formally disbanded Lehi, though some of its members carried out one more terrorist act, the assassination of Folke Bernadotte some months later, an act condemned by Bernadotte's replacement as mediator, Ralph Bunche. After the assassination, the new Israeli government declared Lehi a terrorist organization, arresting some 200 members and convicting some of the leaders. Just before the first Israeli elections in January 1949, a general amnesty to Lehi members was granted by the government. In 1980, Israel instituted a military decoration, an "award for activity in the struggle for the establishment of Israel", the Lehi ribbon. Former Lehi leader Yitzhak Shamir became Prime Minister of Israel in 1983.

Lehi was created in August 1940 by Avraham Stern. Stern had been a member of the Irgun ("Irgun Tsvai Leumi" – "National Military Organization") high command. Zeev Jabotinsky, then the Irgun's supreme commander, had decided that diplomacy and working with Britain would best serve the Zionist cause. World War II was in progress, and Britain was fighting Nazi Germany. The Irgun suspended its underground military activities against the British for the duration of the war.

Stern argued that the time for Zionist diplomacy was over and that it was time for armed struggle against the British. Like other Zionists, he objected to the White Paper of 1939, which restricted both Jewish immigration and Jewish land purchases in Palestine. For Stern, "no difference existed between Hitler and Chamberlain, between Dachau or Buchenwald and sealing the gates of Eretz Israel."

Stern wanted to open Palestine to all Jewish refugees from Europe, and considered this as by far the most important issue of the day. Britain would not allow this. Therefore, he concluded, the "Yishuv" (Jews of Palestine) should fight the British rather than support them in the war. When the Irgun made a truce with the British, Stern left the Irgun to form his own group, which he called "Irgun Tsvai Leumi B'Yisrael" ("National Military Organization in Israel"), later "Lohamei Herut Israel" ("Fighters for the Freedom of Israel"). In September 1940, the organization was officially named "Lehi", the Hebrew acronym of the latter name.

Stern and his followers believed that dying for the "foreign occupier" who was obstructing the creation of the Jewish State was useless. They differentiated between "enemies of the Jewish people" (the British) and "Jew haters" (the Nazis), believing that the former needed to be defeated and the latter manipulated.

In 1940, the idea of the Final Solution was still "unthinkable", and Stern believed that Hitler wanted to make Germany "judenrein" through emigration, as opposed to extermination. In December 1940, Lehi even contacted Germany with a proposal to aid German conquest in the Middle East in return for recognition of a Jewish state open to unlimited immigration.
Lehi had three main goals:

Lehi believed in its early years that its goals would be achieved by finding a strong international ally that would expel the British from Palestine, in return for Jewish military help; this would require the creation of a broad and organised military force "demonstrating its desire for freedom through military operations."

Lehi also referred to themselves as 'terrorists' and may have been one of the last organizations to do so.

An article titled "Terror" in the Lehi underground newspaper "He Khazit" ("The Front") argued as follows:

Neither Jewish ethics nor Jewish tradition can disqualify terrorism as a means of combat. We are very far from having any moral qualms as far as our national war goes. We have before us the command of the Torah, whose morality surpasses that of any other body of laws in the world: "Ye shall blot them out to the last man."
But first and foremost, terrorism is for us a part of the political battle being conducted under the present circumstances, and it has a great part to play: speaking in a clear voice to the whole world, as well as to our wretched brethren outside this land, it proclaims our war against the occupier.
We are particularly far from this sort of hesitation in regard to an enemy whose moral perversion is admitted by all.

The article described the goals of terror:


Yitzhak Shamir, one of the three leaders of Lehi after Avraham Stern's assassination, argued for the legitimacy of Lehi's actions:
There are those who say that to kill [T.G.] Martin [a CID sergeant who had recognised Shamir in a lineup] is terrorism, but to attack an army camp is guerrilla warfare and to bomb civilians is professional warfare. But I think it is the same from the moral point of view. Is it better to drop an atomic bomb on a city than to kill a handful of persons? I don’t think so. But nobody says that President Truman was a terrorist. All the men we went for individually – Wilkin, Martin, MacMichael and others – were personally interested in succeeding in the fight against us.

So it was more efficient and more moral to go for selected targets. In any case, it was the only way we could operate, because we were so small. For us it was not a question of the professional honor of a soldier, it was the question of an idea, an aim that had to be achieved. We were aiming at a political goal. There are many examples of what we did to be found in the Bible – Gideon and Samson, for instance. This had an influence on our thinking. And we also learned from the history of other peoples who fought for their freedom – the Russian and Irish revolutionaries, Giuseppe Garibaldi and Josip Broz Tito.

Avraham Stern laid out the ideology of Lehi in the essay "18 Principles of Rebirth":




Unlike the left-wing Haganah and right-wing Irgun, Lehi members were not a homogeneous collective with a single political, religious, or economic ideology. They were a combination of militants united by the goal of liberating the land of Israel from British rule. Most Lehi leaders defined their organization as an anti-imperialism movement and stated that their opposition to British colonial rule in Palestine was not based on a particular policy but rather on the presence of a foreign power over the homeland of the Jewish people. Avraham Stern defined the British Mandate as "foreign rule" regardless of British policies and took a radical position against such imperialism even if it were to be benevolent.

In the early years of the state of Israel Lehi veterans could be found supporting nearly all political parties and some Lehi leaders founded a left-wing political party called the Fighters' List with Natan Yellin-Mor as its head. The party took part in the elections in January 1949 and won a single parliamentary seat. A number of Lehi veterans established the Semitic Action movement in 1956 which sought the creation of a regional federation encompassing Israel and its Arab neighbors on the basis of an anti-colonialist alliance with other indigenous inhabitants of the Middle East.

Some writers have stated that Lehi's true goals were the creation of a totalitarian state. Perlinger and Weinberg write that the organisation's ideology placed "its world view in the quasi-fascist radical Right, which is characterised by xenophobia, a national egotism that completely subordinates the individual to the needs of the nation, anti-liberalism, total denial of democracy and a highly centralised government." Perliger and Weinberg state that most Lehi members were admirers of the Italian Fascist movement. According to Kaplan and Penslar, Lehi's ideology was a mix of fascist and communist thought combined with racism and universalism.

Others counter these claims. They note that when Lehi founder Avraham Stern went to study in fascist Italy, he refused to join the for foreign students, even though members got large reductions in tuition.

According to Yaacov Shavit professor at the Department of Jewish History, Tel Aviv University articles in publications by Lehi contained references to a Jewish "master race", contrasting the Jews with Arabs who were seen as a "nation of slaves" Sasha Polakow-Suransky writes about Lehi "Lehi was also unabashedly racist towards Arabs. Their publications described Jews as a master race and Arabs as a slave race." Lehi advocated mass expulsion of all Arabs from Palestine and Transjordan or even their physical annihilation.

Many Lehi combatants had received military training. Some had attended the state military academy in Civitavecchia, in Fascist Italy. Others received military training from instructors of the Polish Armed Forces in 1938–1939. This training was conducted in Trochenbrod (Zofiówka) in Wołyń Voivodeship, Podębin near Łódź, and the forests around Andrychów. They were taught how to use explosives. One of them reported later: "Poles treated terrorism as a science. We have mastered mathematical principles of demolishing constructions made of concrete, iron, wood, bricks and dirt."

The group was initially unsuccessful. Early attempts to raise funds through criminal activities, including a bank robbery in Tel Aviv in 1940 and another robbery on 9 January 1942 in which Jewish passers-by were killed, brought about the temporary collapse of the group. An attempt to assassinate the head of the British secret police in Lod in which three police personnel were killed, two Jewish and one British, elicited a severe response from the British and Jewish establishments who collaborated against Lehi.

Stern's group was seen as a terrorist organisation by the British authorities, who instructed the Defence Security Office (the colonial branch of MI5) to track down its leaders. In 1942, Stern, after he was arrested, was shot dead in disputed circumstances by Inspector Geoffrey J. Morton of the CID. The arrest of several other members led momentarily to the group's eclipse, until it was revived after the September 1942 escape of two of its leaders, Yitzhak Shamir and Eliyahu Giladi, aided by two other escapees Natan Yellin-Mor (Friedman) and Israel Eldad (Sheib). (Giladi was later killed by Lehi under circumstances that remain mysterious.) Shamir's codename was "Michael", a reference to one of Shamir's heroes, Michael Collins. Lehi was guided by spiritual and philosophical leaders such as Uri Zvi Greenberg and Israel Eldad. After the killing of Giladi, the organization was led by a triumvirate of Eldad, Shamir, and Yellin-Mor.

Lehi adopted a non-socialist platform of anti-imperialist ideology. It viewed the continued British rule of Palestine as a violation of the Mandate's provision generally, and its restrictions on Jewish immigration to be an intolerable breach of international law. However they also targeted Jews whom they regarded as traitors, and during the 1948 Arab-Israeli War they joined in operations with the Haganah and Irgun against Arab targets, for example Deir Yassin.

According to a compilation by Nachman Ben-Yehuda, Lehi was responsible for 42 assassinations, more than twice as many as the Irgun and Haganah combined during the same period. Of those Lehi assassinations that Ben-Yehuda classified as political, more than half the victims were Jews.

Lehi also rejected the authority of the Jewish Agency for Israel and related organizations, operating entirely on its own throughout nearly all of its existence.

Lehi prisoners captured by the British generally refused to employ lawyers in their defense. The defendants would conduct their own defense, and would deny the right of the military court to try them, saying that in accordance with the Hague Convention they should be accorded the status of prisoners of war. For the same reason, Lehi prisoners refused to plead for amnesty, even when it was clear that this would have spared them the death penalty. Moshe Barazani, a Lehi member, and Meir Feinstein, an Irgun member, took their own lives in prison with a grenade smuggled inside an orange so the British could not hang them.

In mid-1940, Stern became convinced that the Italians were interested in the establishment of a fascist Jewish state in Palestine. He conducted negotiations, he thought, with the Italians via an intermediary Moshe Rotstein, and drew up a document that became known as the "Jerusalem Agreement". In exchange for Italy's recognition of, and aid in obtaining, Jewish sovereignty over Palestine, Stern promised that Zionism would come under the aegis of Italian fascism, with Haifa as its base, and the Old City of Jerusalem under Vatican control, except for the Jewish quarter. In Heller's words, Stern's proposal would "turn the 'Kingdom of Israel' into a satellite of the Axis powers."

However, the "intermediary" Rotstein was in fact an agent of the Irgun, conducting a sting operation under the direction of the Irgun intelligence leader in Haifa, Israel Pritzker, in cooperation with the British. Secret British documents about the affair were uncovered by historian Eldad Harouvi (now director of the Palmach Archives) and confirmed by former Irgun intelligence officer Yitzhak Berman. When Rotstein's role later became clear, Lehi sentenced him to death and assigned Yaacov Eliav to kill him, but the assassination never took place. However, Pritzker was killed by Lehi in 1943.

Late in 1940, Lehi, having identified a common interest between the intentions of the new German order and Jewish national aspirations, proposed forming an alliance in World War II with Nazi Germany. The organization offered cooperation in the following terms: Lehi would rebel against the British, while Germany would recognize an independent Jewish state in Palestine/Eretz Israel, and all Jews leaving their homes in Europe, by their own will or because of government injunctions, could enter Palestine with no restriction of numbers. Late in 1940, Lehi representative Naftali Lubenchik went to Beirut to meet German official Werner Otto von Hentig. The Lehi documents outlined that its rule would be authoritarian and indicated similarities between the organization and Nazis. Israel Eldad, one of the leading members of Lehi, wrote about Hitler "it is not Hitler who is the hater of the kingdom of Israel and the return to Zion, it is not Hitler who subjects us to the cruel fate of falling a second and a third time into Hitler's hands, but the British."

Stern also proposed recruiting some 40,000 Jews from occupied Europe to invade Palestine with German support to oust the British. On 11 January 1941, Vice Admiral Ralf von der Marwitz, the German Naval attaché in Turkey, filed a report (the "Ankara document") conveying an offer by Lehi to "actively take part in the war on Germany's side" in return for German support for "the establishment of the historic Jewish state on a national and totalitarian basis, bound by a treaty with the German Reich."

According to Yellin-Mor:Lubenchik did not take along any written memorandum for the German representatives. Had there been a need for one, he would have formulated it on the spot, since he was familiar with the episode of the Italian "intermediary" and with the numerous drafts connected with it. Apparently one of von Hentig's secretaries noted down the essence of the proposal in his own words.According to Joseph Heller, "The memorandum arising from their conversation is an entirely authentic document, on which the stamp of the 'IZL in Israel' is clearly embossed."

Von der Marwitz delivered the offer, classified as secret, to the German Ambassador in Turkey and on 21 January 1941 it was sent to Berlin. There was never any response.

A second attempt to contact the Nazis was made at the end of 1941, but it was even less successful. The emissary Yellin-Mor was arrested in Syria before he could carry out his mission.

This proposed alliance with Nazi Germany cost Lehi and Stern much support. The Stern Gang also had links with, and support from, the Vichy France Sûreté's Lebanese offices.

Even as the full scale of Nazi atrocities became more evident in 1943, Lehi refused to accept Hitler as main foe (as opposed to Great Britain).

As a group that never had over a few hundred members, Lehi relied on audacious but small-scale operations to bring their message home. They adopted the tactics of groups such as the Socialist Revolutionaries and the Combat Organization of the Polish Socialist Party in Czarist Russia, and the Irish Republican Army. To this end, Lehi conducted small-scale operations such as individual assassinations of British officials (notable targets included Lord Moyne, CID detectives, and Jewish "collaborators"), and random shootings against soldiers and police officers. Another strategy, adopted in 1946, was to send bombs in the mail to British politicians. Other actions included sabotaging infrastructure targets: bridges, railroads, telephone and telegraph lines, and oil refineries, as well as the use of vehicle bombs against British military, police, and administrative targets. Lehi financed its operations from private donations, extortion, and bank robbery. Its campaign of violence lasted from 1944 to 1948. Initially conducted together with the Irgun, it included a six-month suspension to avoid being targeted by the Haganah during the Hunting Season, and later operated jointly with the Haganah and Irgun under the Jewish Resistance Movement. After the Jewish Resistance Movement was dissolved, it operated independently as part of the general Jewish insurgency in Palestine.

On 6 November 1944, Lehi assassinated Lord Moyne, the British Minister Resident in the Middle East, in Cairo. Moyne was the highest ranking British official in the region. Yitzhak Shamir claimed later that Moyne was assassinated because of his support for a Middle Eastern Arab Federation and anti-Semitic lectures in which Arabs were held to be racially superior to Jews. The assassination rocked the British government, and outraged Winston Churchill, the British Prime Minister. The two assassins, Eliahu Bet-Zouri and Eliahu Hakim were captured and used their trial as a platform to make public their political propaganda. They were executed. In 1975 their bodies were returned to Israel and given a state funeral. In 1982, postage stamps were issued for 20 Olei Hagardom, including Bet-Zouri and Hakim, in a souvenir sheet called "Martyrs of the struggle for Israel's independence."

On 25 April 1946, a Lehi unit attacked a car park in Tel Aviv occupied by the British 6th Airborne Division. Under a barrage of heavy covering fire, Lehi fighters broke into the car park, shot soldiers they encountered at close range, stole rifles from arms racks, laid mines to cover the retreat, and withdrew. Seven soldiers were killed in the attack, which caused widespread outrage among the British security forces in Palestine. It resulted in retaliatory anti-Jewish violence by British troops and a punitive curfew imposed on Tel Aviv's roads and a closure of places of entertainment in the city by the British Army.

On 12 January 1947, Lehi members drove a truckload of explosives into a British police station in Haifa killing four and injuring 140, in what has been called 'the world's first true truck bomb'.

Following the bombing of the British embassy in Rome, October 1946, a series of operations against targets in the United Kingdom were launched. On 7 March 1947, Lehi's only successful operation in Britain was carried out when a Lehi bomb severely damaged the British Colonial Club, a London recreational facility for soldiers and students from Britain's colonies in Africa and the West Indies. On 15 April 1947 a bomb consisting of twenty-four sticks of explosives was planted in the Colonial Office, Whitehall. It failed to explode due to a fault in the timer. Five weeks later, on 22 May, five alleged Lehi members were arrested in Paris with bomb making material including explosives of the same type as found in London. On 2 June, two Lehi members, Betty Knouth and Yaakov Levstein, were arrested crossing from Belgium to France. Envelopes addressed to British officials, with detonators, batteries and a time fuse were found in one of Knouth's suitcases. Knouth was sentenced to a year in prison, Levstein to eight months. The British Security Services identified Knouth as the person who planted the bomb in the Colonial Office. Shortly after their arrest, 21 letter bombs were intercepted addressed to senior British figures. The letters had been posted in Italy. The intended recipients included Bevin, Attlee, Churchill and Eden. Knouth aka Gilberte/Elizabeth Lazarus. Levstein was travelling as Jacob Elias; his fingerprints connected him to the deaths of several Palestine Policemen as well as an attempt on the life of the British High Commissioner. In 1973, Margaret Truman wrote that letter bombs were also posted to her father, U.S. President Harry S. Truman, in 1947. Former Lehi leader Yellin-Mor admitted that letter bombs had been sent to British targets but denied that any had been sent to Truman.

Shortly after the 1947 publication of "The Last Days of Hitler", Lehi issued a death threat against the author, Hugh Trevor-Roper, for his portrayal of Hitler, feeling that Trevor-Roper had attempted to exonerate the German populace from responsibility.

During the lead-up to the 1948 Arab–Israeli War, Lehi mined the Cairo–Haifa train several times. On 29 February 1948, Lehi mined the train north of Rehovot, killing 28 British soldiers and wounding 35. On 31 March, Lehi mined the train near Binyamina, killing 40 civilians and wounding 60.

Shlomo Sand writes that as a method of applying pressure on Arab villagers to abandon their settlements, Lehi planned a terror attack on Nablus and its Arab city headquarters; Lehi fighter Elisha Ibzov (Avraham Cohen) was captured with a truck filled with explosives on his way to the city. Lehi fighters in return abducted four adult villagers and a youth from al-Sheikh Muwannis with no connection to Ibzov's capture, and threatened to kill them. As rumours spread that they were already murdered, panic set out in the villagers and the settlement became increasingly abandoned, despite eventual release of the hostages

One of the most widely known acts of Lehi was the attack on the Palestinian-Arab village of Deir Yassin.

In the months before the British evacuation from Palestine, the Arab League-sponsored Arab Liberation Army (ALA) occupied several strategic points along the road between Jerusalem and Tel Aviv, cutting off supplies to the Jewish part of Jerusalem. One of these points was Deir Yassin. By March 1948, the road was cut off and Jewish Jerusalem was under siege. The Haganah launched Operation Nachshon to break the siege.

On 6 April, the Haganah attacked al-Qastal, a village two kilometers north of Deir Yassin, also overlooking the Jerusalem-Tel Aviv road.

Then on 9 April 1948, about 120 Lehi and Irgun fighters, acting in cooperation with the Haganah, attacked and captured Deir Yassin. The attack was at night, the fighting was confused, and many civilian inhabitants of the village were killed. This action had great consequences for the war, and remains a cause celebre for Palestinians ever since.

Exactly what happened has never been established clearly. The Arab League reported a great massacre: 254 killed, with rape and lurid mutilations. Israeli investigations claimed the actual number of dead was between 100 and 120, and there were no mass rapes, but most of the dead were civilians, and admitted some were killed deliberately. Lehi and Irgun both denied an organized massacre. Accounts by Lehi veterans such as Ezra Yakhin note that many of the attackers were killed or wounded, assert that Arabs fired from every building and that Iraqi and Syrian soldiers were among the dead, and even that some Arab fighters dressed as women.

However, Jewish authorities, including Haganah, the Chief Rabbinate, the Jewish Agency, and David Ben-Gurion, also condemned the attack, lending credence to the charge of massacre. The Jewish Agency even sent a letter of condemnation, apology, and condolence to King Abdullah I of Jordan.

Both the Arab reports and Jewish responses had hidden motives: the Arab leaders wanted to encourage Palestinian Arabs to fight rather than surrender, to discredit the Zionists with international opinion, and to increase popular support in their countries for an invasion of Palestine. The Jewish leaders wanted to discredit Irgun and Lehi.

Ironically, the Arab reports backfired in one respect: frightened Palestinian Arabs did not surrender, but did not fight either – they fled, allowing Israel to gain much territory with little fighting and also without absorbing many Arabs.

Lehi similarly interpreted events at Deir Yassin as turning the tide of war in favor of the Jews. Lehi leader Israel Eldad later wrote in his memoirs from the underground period that "without Deir Yassin the State of Israel could never have been established".

The Deir Yassin story did not much sway international opinion. It did increase, not only support, but pressure on Arab governments to intervene. Abdullah of Jordan was now compelled to join the invasion of Palestine after Israel's declaration of independence on 14 May.

Although Lehi had stopped operating nationally after May 1948, the group continued to function in Jerusalem. On 17 September 1948, Lehi assassinated UN mediator Count Folke Bernadotte. The assassination was directed by Yehoshua Zettler and carried out by a four-man team led by Meshulam Makover. The fatal shots were fired by Yehoshua Cohen. The Security Council described the assassination as a "cowardly act which appears to have been committed by a criminal group of terrorists".

Three days after the assassination, the Israeli government passed the Ordinance to Prevent Terrorism and declared Lehi to be a terrorist organization. Many Lehi members were arrested, including leaders Nathan Yellin-Mor and Matitiahu Schmulevitz who were arrested on 29 September. Eldad and Shamir managed to escape arrest. Yellin-Mor and Schmulevitz were charged with leadership of a terrorist organization and on 10 February 1949 were sentenced to 8 years and 5 years imprisonment, respectively. However the State (Temporary) Council soon announced a general amnesty for Lehi members and they were released.

Between 5 December 1948 and 25 January 1949, Yellin-Mor and Schmuelevitch were tried in a military court on terrorism charges. The prosecution accused them of the murder of Bernadotte, though they were not specifically charged with it. Senior officers of the IDF, including Yisrael Galili and David Shaltiel, told the court that Lehi had hindered, rather than assisted the fight against the British and the Arabs.

While the trial was in progress, some of the Lehi leadership founded a USSR-leaning political party called the Fighters' List with Yellin-Mor as its leader. The party took part in the elections in January 1949 with Yellin-Mor and Schmuelevitch heading the list. The trial verdict was handed down on 10 February, soon after the Fighters' List had won one seat with only 1.2% of the vote. Yellin-Mor was sentenced to 8 years and Schmuelevitch to 5 years imprisonment, but the court agreed to remit the sentences if the prisoners agreed to a list of conditions. The Provisional State Council then authorised their pardon. The party disbanded after several years and did not contest the 1951 elections.

In 1956, some Lehi veterans established the Semitic Action movement, which sought the creation of a regional federation encompassing Israel and its Arab neighbors on the basis of an anti-colonialist alliance with other indigenous inhabitants of the Middle East.

Not all Lehi alumni gave up political violence after independence: former members were involved in the activities of the Kingdom of Israel militant group, the 1957 assassination of Rudolf Kastner, and likely the 1952 attempted assassination of David-Zvi Pinkas.

In 1980, Israel instituted the Lehi ribbon, red, black, grey, pale blue and white, which is awarded to former members of the Lehi underground who wished to carry it, "for military service towards the establishment of the State of Israel".

The words and music of a song "Unknown Soldiers" (also translated "Anonymous Soldiers") were written by Avraham Stern in 1932 during the early days of the Irgun. It became the Irgun's anthem until the split with Lehi in 1940, after which it became the Lehi anthem.

A number of Lehi's members went on to play important roles in Israel's public life.





</doc>
<doc id="29288" url="https://en.wikipedia.org/wiki?curid=29288" title="Server-side scripting">
Server-side scripting

Server-side scripting is a technique used in web development which involves employing scripts on a web server which produce a response customized for each user's (client's) request to the website. The alternative is for the web server itself to deliver a static web page. Scripts can be written in any of a number of server-side scripting languages that are available (see below). Server-side scripting is distinguished from client-side scripting where embedded scripts, such as JavaScript, are run client-side in a web browser, but both techniques are often used together.

Server-side scripting is often used to provide a customized interface for the user. These scripts may assemble client characteristics for use in customizing the response based on those characteristics, the user's requirements, access rights, etc. Server-side scripting also enables the website owner to hide the source code that generates the interface, whereas with client-side scripting, the user has access to all the code received by the client. A down-side to the use of server-side scripting is that the client needs to make further requests over the network to the server in order to show new information to the user via the web browser. These requests can slow down the experience for the user, place more load on the server, and prevent use of the application when the user is disconnected from the server.

When the server serves data in a commonly used manner, for example according to the HTTP or FTP protocols, users may have their choice of a number of client programs (most modern web browsers can request and receive data using both of those protocols). In the case of more specialized applications, programmers may write their own server, client, and communications protocol, that can only be used with one another.

Programs that run on a user's local computer without ever sending or receiving data over a network are not considered clients, and so the operations of such programs would not be considered client-side operations.

Netscape introduced an implementation of JavaScript for server-side scripting with Netscape Enterprise Server, first released in December, 1994 (soon after releasing JavaScript for browsers).

Server-side scripting was later used in early 1995 by Fred DuFresne while developing the first web site for Boston, MA television station WCVB. The technology is described in US patent 5835712. The patent was issued in 1998 and is now owned by Open Invention Network (OIN). In 2010 OIN named Fred DuFresne a "Distinguished Inventor" for his work on server-side scripting.

Today, a variety of services use server-side scripting to deliver results back to a client as a paid or free service. An example would be WolframAlpha, which is a computational knowledge engine that computes results outside the clients environment and returns the computed result back. A more commonly used service is Google's proprietary search engine, which searches millions of cached results related to the user specified keyword and returns an ordered list of links back to the client. Apple's Siri application also employs server-side scripting outside of a web application. The application takes an input, computes a result, and returns the result back to the client.

In the earlier days of the web, server-side scripting was almost exclusively performed by using a combination of C programs, Perl scripts, and shell scripts using the Common Gateway Interface (CGI). Those scripts were executed by the operating system, and the results were served back by the web server. Many modern web servers can directly execute on-line scripting languages such as ASP, JSP, Perl, PHP and Ruby either by the web server itself or via extension modules (e.g. mod_perl or mod_php) to the web server. For example, WebDNA includes its own embedded database system. Either form of scripting (i.e., CGI or direct execution) can be used to build up complex multi-page sites, but direct execution usually results in less overhead because of the lower number of calls to external interpreters.

Dynamic websites sometimes use custom web application servers, such as Glassfish, Plack and Python's "Base HTTP Server" library, although some may not consider this to be server-side scripting. When using dynamic web-based scripting techniques, developers must have a keen understanding of the logical, temporal, and physical separation between the client and the server. For a user's action to trigger the execution of server-side code, for example, a developer working with classic ASP must explicitly cause the user's browser to make a request back to the web server. Creating such interactions can easily consume much development time and lead to unreadable code.

Server-side scripts are completely processed by the servers instead of clients. When clients request a page containing server-side scripts, the applicable server processes the scripts and returns an HTML page to the client.

There are a number of server-side scripting languages available, including:



</doc>
<doc id="29290" url="https://en.wikipedia.org/wiki?curid=29290" title="Samuel Huntington">
Samuel Huntington

Samuel Huntington may refer to:




</doc>
<doc id="29292" url="https://en.wikipedia.org/wiki?curid=29292" title="Script">
Script

Script may refer to:







</doc>
<doc id="29293" url="https://en.wikipedia.org/wiki?curid=29293" title="Optical spectrometer">
Optical spectrometer

An optical spectrometer (spectrophotometer, spectrograph or spectroscope) is an instrument used to measure properties of light over a specific portion of the electromagnetic spectrum, typically used in spectroscopic analysis to identify materials. The variable measured is most often the light's intensity but could also, for instance, be the polarization state. The independent variable is usually the wavelength of the light or a unit directly proportional to the photon energy, such as reciprocal centimeters or electron volts, which has a reciprocal relationship to wavelength.

A spectrometer is used in spectroscopy for producing spectral lines and measuring their wavelengths and intensities. Spectrometers may also operate over a wide range of non-optical wavelengths, from gamma rays and X-rays into the far infrared. If the instrument is designed to measure the spectrum in absolute units rather than relative units, then it is typically called a spectrophotometer. The majority of spectrophotometers are used in spectral regions near the visible spectrum.

In general, any particular instrument will operate over a small portion of this total range because of the different techniques used to measure different portions of the spectrum. Below optical frequencies (that is, at microwave and radio frequencies), the spectrum analyzer is a closely related electronic device.

Spectrometers are used in many fields. For example, they are used in astronomy to analyze the radiation from astronomical objects and deduce chemical composition. The spectrometer uses a prism or a grating to spread the light from a distant object into a spectrum. This allows astronomers to detect many of the chemical elements by their characteristic spectral fingerprints. If the object is glowing by itself, it will show spectral lines caused by the glowing gas itself. These lines are named for the elements which cause them, such as the hydrogen alpha, beta, and gamma lines. Chemical compounds may also be identified by absorption. Typically these are dark bands in specific locations in the spectrum caused by energy being absorbed as light from other objects passes through a gas cloud. Much of our knowledge of the chemical makeup of the universe comes from spectra.

Spectroscopes are often used in astronomy and some branches of chemistry. Early spectroscopes were simply prisms with graduations marking wavelengths of light. Modern spectroscopes generally use a diffraction grating, a movable slit, and some kind of photodetector, all automated and controlled by a computer.

Joseph von Fraunhofer developed the first modern spectroscope by combining a prism, diffraction slit and telescope in a manner that increased the spectral resolution and was reproducible in other laboratories. Fraunhofer also went on to invent the first diffraction spectroscope. Gustav Robert Kirchhoff and Robert Bunsen discovered the application of spectroscopes to chemical analysis and used this approach to discover caesium and rubidium. Kirchhoff and Bunsen's analysis also enabled a chemical explanation of stellar spectra, including Fraunhofer lines.

When a material is heated to incandescence it emits light that is characteristic of the atomic makeup of the material.
Particular light frequencies give rise to sharply defined bands on the scale which can be thought of as fingerprints. For example, the element sodium has a very characteristic double yellow band known as the Sodium D-lines at 588.9950 and 589.5924 nanometers, the color of which will be familiar to anyone who has seen a low pressure sodium vapor lamp.

In the original spectroscope design in the early 19th century, light entered a slit and a collimating lens transformed the light into a thin beam of parallel rays. The light then passed through a prism (in hand-held spectroscopes, usually an Amici prism) that refracted the beam into a spectrum because different wavelengths were refracted different amounts due to dispersion. This image was then viewed through a tube with a scale that was transposed upon the spectral image, enabling its direct measurement.

With the development of photographic film, the more accurate spectrograph was created. It was based on the same principle as the spectroscope, but it had a camera in place of the viewing tube. In recent years, the electronic circuits built around the photomultiplier tube have replaced the camera, allowing real-time spectrographic analysis with far greater accuracy. Arrays of photosensors are also used in place of film in spectrographic systems. Such spectral analysis, or spectroscopy, has become an important scientific tool for analyzing the composition of unknown material and for studying astronomical phenomena and testing astronomical theories.

In modern spectrographs in the UV, visible, and near-IR spectral ranges, the spectrum is generally given in the form of photon number per unit wavelength (nm or μm), wavenumber (μm, cm), frequency (THz), or energy (eV), with the units indicated by the abscissa. In the mid- to far-IR, spectra are typically expressed in units of Watts per unit wavelength (μm) or wavenumber (cm). In many cases, the spectrum is displayed with the units left implied (such as "digital counts" per spectral channel).

A spectrograph is an instrument that separates light by its wavelengths and records this data. A spectrograph typically has a multi-channel detector system or camera that detects and records the spectrum of light. 

The term was first used in 1876 by Dr. Henry Draper when he invented the earliest version of this device, and which he used to take several photographs of the spectrum of Vega. This earliest version of the spectrograph was cumbersome to use and difficult to manage.

There are several kinds of machines referred to as "spectrographs", depending on the precise nature of the waves. The first spectrographs used photographic paper as the detector. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.

A spectrograph is sometimes called polychromator, as an analogy to monochromator.

The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The forthcoming James Webb Space Telescope will contain both a near-infrared spectrograph (NIRSpec) and a mid-infrared spectrograph (MIRI).

An Echelle spectrograph uses two diffraction gratings, rotated 90 degrees with respect to each other and placed close to one another. Therefore, an entrance point and not a slit is used and a 2d CCD-chip records the spectrum. Usually one would guess to retrieve a spectrum on the diagonal, but when both gratings have a wide spacing and one is blazed so that only the first order is visible and the other is blazed that a lot of higher orders are visible, one gets a very fine spectrum nicely folded onto a small common CCD-chip. The small chip also means that the collimating optics need not to be optimized for coma or astigmatism, but the spherical aberration can be set to zero.




</doc>
<doc id="29294" url="https://en.wikipedia.org/wiki?curid=29294" title="IBM System/360">
IBM System/360

The IBM System/360 (S/360) is a family of mainframe computer systems that was announced by IBM on April 7, 1964, and delivered between 1965 and 1978. It was the first family of computers designed to cover the complete range of applications, from small to large, both commercial and scientific. The design made a clear distinction between architecture and implementation, allowing IBM to release a suite of compatible designs at different prices. All but the only partially compatible Model 44 and the most expensive systems use microcode to implement the instruction set, which features 8-bit byte addressing and binary, decimal and hexadecimal floating-point calculations.

The launch of the System/360 family introduced IBM's Solid Logic Technology (SLT), a new technology that was the start of more powerful but smaller computers.

The slowest System/360 model announced in 1964, the Model 30, could perform up to 34,500 instructions per second, with memory from 8 to 64 KB. High performance models came later. The 1967 IBM System/360 Model 91 could execute up to 16.6 million instructions per second. The larger 360 models could have up to 8 MB of main memory, though that much main memory was unusual—a large installation might have as little as 256 KB of main storage, but 512 KB, 768 KB or 1024 KB was more common. Up to 8 megabytes of slower (8 microsecond) Large Capacity Storage (LCS) was also available for some models.

The IBM 360 was extremely successful in the market, allowing customers to purchase a smaller system with the knowledge they would always be able to migrate upward if their needs grew, without reprogramming of application software or replacing peripheral devices. Many consider the design one of the most successful computers in history, influencing computer design for years to come.

The chief architect of System/360 was Gene Amdahl, and the project was managed by Fred Brooks, responsible to Chairman Thomas J. Watson Jr. The commercial release was piloted by another of Watson's lieutenants, John R. Opel, who managed the launch of IBM’s System 360 mainframe family in 1964.

Application-level compatibility (with some restrictions) for System/360 software is maintained to the present day with the System z mainframe servers.

Contrasting with at-the-time normal industry practice, IBM created an entire new series of computers, from small to large, low- to high-performance, all using the same instruction set (with two exceptions for specific markets). This feat allowed customers to use a cheaper model and then upgrade to larger systems as their needs increased without the time and expense of rewriting software. Before the introduction of System/360, business and scientific applications used different computers with different instruction sets and operating systems. Different-sized computers also had their own instruction sets. IBM was the first manufacturer to exploit microcode technology to implement a compatible range of computers of widely differing performance, although the largest, fastest, models had hard-wired logic instead.

This flexibility greatly lowered barriers to entry. With most other vendors customers had to choose between machines they could outgrow and machines that were potentially too powerful and thus too costly. This meant that many companies simply did not buy computers.

IBM initially announced a series of six computers and forty common peripherals. IBM eventually delivered fourteen models, including rare one-off models for NASA. The least expensive model was the Model 20 with as little as 4096 bytes of core memory, eight 16-bit registers instead of the sixteen 32-bit registers of other System/360 models, and an instruction set that was a subset of that used by the rest of the range.

The initial announcement in 1964 included Models 30, 40, 50, 60, 62, and 70. The first three were low- to middle-range systems aimed at the IBM 1400 series market. All three first shipped in mid-1965. The last three, intended to replace the 7000 series machines, never shipped and were replaced with the 65 and 75, which were first delivered in November 1965, and January 1966, respectively.

Later additions to the low-end included models 20 (1966, mentioned above), 22 (1971), and 25 (1968). The Model 20 had several sub-models; sub-model 5 was at the higher end of the model. The Model 22 was a recycled Model 30 with minor limitations: a smaller maximum memory configuration, and slower I/O channels, which limited it to slower and lower-capacity disk and tape devices than on the 30.

The Model 44 (1966) was a specialized model, designed for scientific computing and for real-time computing and process control, featuring some additional instructions, and with all storage-to-storage instructions and five other complex instructions eliminated.
A succession of high-end machines included the Model 67 (1966, mentioned below, briefly anticipated as the 64 and 66), 85 (1969), 91 (1967, anticipated as the 92), 95 (1968), and 195 (1971). The 85 design was intermediate between the System/360 line and the follow-on System/370 and was the basis for the 370/165. There was a System/370 version of the 195, but it did not include Dynamic Address Translation.

The implementations differed substantially, using different native data path widths, presence or absence of microcode, yet were extremely compatible. Except where specifically documented, the models were architecturally compatible. The 91, for example, was designed for scientific computing and provided out-of-order instruction execution (and could yield "imprecise interrupts" if a program trap occurred while several instructions were being read), but lacked the decimal instruction set used in commercial applications. New features could be added without violating architectural definitions: the 65 had a dual-processor version (M65MP) with extensions for inter-CPU signalling; the 85 introduced cache memory. Models 44, 75, 91, 95, and 195 were implemented with hardwired logic, rather than microcoded as all other models.

The Model 67, announced in August 1965, was the first production IBM system to offer dynamic address translation (virtual memory) hardware to support time-sharing. "DAT" is now more commonly referred to as an MMU. An experimental one-off unit was built based on a model 40. Before the 67, IBM had announced models 64 and 66, DAT versions of the 60 and 62, but they were almost immediately replaced with the 67 at the same time that the 60 and 62 were replaced with the 65. DAT hardware would reappear in the S/370 series in 1972, though it was initially absent from the series. Like its close relative, the 65, the 67 also offered dual CPUs.

IBM stopped marketing all System/360 models by the end of 1977.

IBM's existing customers had a large investment in software that executed on second-generation machines. Several models offered the option of emulation of the customer's previous computer using a combination of special hardware, special microcode and an emulation program that used the emulation instructions to simulate the target system, so that old programs could run on the new machine.

Customers initially had to halt the computer and load the emulation program.

IBM later added features and modified emulator programs to allow emulation of the 1401, 1440, 1460, 1410 and 7010 under the control of an operating system.
The Model 85 and later System/370 maintained the precedent, retaining emulation options and allowing emulator programs to execute under operating system control alongside native programs.

System/360 (excepting the Model 20) was replaced with the compatible System/370 range in 1970 and Model 20 users were targeted to move to the IBM System/3. (The idea of a major breakthrough with FS technology was dropped in the mid-1970s for cost-effectiveness and continuity reasons.) Later compatible IBM systems include the 4300 family, the 308x family, the 3090, the ES/9000 and 9672 families (System/390 family), and the IBM Z series.

Computers that were mostly identical or compatible in terms of the machine code or architecture of the System/360 included Amdahl's 470 family (and its successors), Hitachi mainframes, the UNIVAC 9000 series, Fujitsu as the Facom, the RCA Spectra 70 series, and the English Electric System 4. The System 4 machines were built under license to RCA. RCA sold the Spectra series to what was then UNIVAC, where they became the UNIVAC Series 70. UNIVAC also developed the UNIVAC Series 90 as successors to the 9000 series and Series 70. The Soviet Union produced a System/360 clone named the ES EVM.

The IBM 5100 portable computer, introduced in 1975, offered an option to execute the System/360's APL.SV programming language through a hardware emulator. IBM used this approach to avoid the costs and delay of creating a 5100-specific version of APL.

Special radiation-hardened and otherwise somewhat modified System/360s, in the form of the System/4 Pi avionics computer, are used in several fighter and bomber jet aircraft. In the complete 32-bit AP-101 version, 4 Pi machines were used as the replicated computing nodes of the fault-tolerant Space Shuttle computer system (in five nodes). The U.S. Federal Aviation Administration operated the IBM 9020, a special cluster of modified System/360s for air traffic control, from 1970 until the 1990s. (Some 9020 software is apparently still used via emulation on newer hardware.)

The System/360 introduced a number of industry standards to the marketplace, such as:

The System/360 series has a computer system architecture specification. This specification makes no assumptions on the implementation itself, but rather describes the interfaces and expected behavior of an implementation. The architecture describes mandatory interfaces that must be available on all implementations, and optional interfaces. Some aspects of this architecture are:

Some of the optional features are:

All models of System/360, except for the Model 20 and Model 44, implemented that specification.

Binary arithmetic and logical operations are performed as register-to-register and as memory-to-register/register-to-memory as a standard feature. If the Commercial Instruction Set option was installed, packed decimal arithmetic could be performed as memory-to-memory with some memory-to-register operations. The Scientific Instruction Set feature, if installed, provided access to four floating point registers that could be programmed for either 32-bit or 64-bit floating point operations. The Models 85 and 195 could also operate on 128-bit extended-precision floating point numbers stored in pairs of floating point registers, and software provided emulation in other models. The System/360 used an 8-bit byte, 32-bit word, 64-bit double-word, and 4-bit nibble. Machine instructions had operators with operands, which could contain register numbers or memory addresses. This complex combination of instruction options resulted in a variety of instruction lengths and formats.

Memory addressing was accomplished using a base-plus-displacement scheme, with registers 1 through F (15). A displacement was encoded in 12 bits, thus allowing a 4096-byte displacement (0-4095), as the offset from the address put in a base register.

Register 0 could not be used as a base register nor as an index register (nor as a branch address register), as "0" was reserved to indicate an address in the first 4 KB of memory, that is, if register 0 was specified as described, the value 0x00000000 was implicitly input to the effective address calculation in place of whatever value might be contained within register 0 (or if specified as a branch address register, then no branch was taken, and the content of register 0 was ignored, but any side effect of the instruction was performed).

This specific behavior permitted initial execution of an interrupt routines, since base registers would not necessarily be set to 0 during the first few instruction cycles of an interrupt routine. It isn't needed for IPL ("Initial Program Load" or boot), as one can always clear a register without the need to save it.

With the exception of the Model 67, all addresses were real memory addresses. Virtual memory was not available in most IBM mainframes until the System/370 series. The Model 67 introduced a virtual memory architecture, which MTS, CP-67, and TSS/360 used—but not IBM's mainline System/360 operating systems.

The System/360 machine-code instructions are 2 bytes long (no memory operands), 4 bytes long (one operand), or 6 bytes long (two operands). Instructions are always situated on 2-byte boundaries.

Operations like MVC (Move-Characters) (Hex: D2) can only move at most 256 bytes of information. Moving more than 256 bytes of data required multiple MVC operations. (The System/370 series introduced a family of more powerful instructions such as the MVCL "Move-Characters-Long" instruction, which supports moving up to 16 MB as a single block.)

An operand is two bytes long, typically representing an address as a 4-bit nibble denoting a base register and a 12-bit displacement relative to the contents of that register, in the range 000–FFF (shown here as hexadecimal numbers). The address corresponding to that operand is the contents of the specified general-purpose register plus the displacement. For example, an MVC instruction that moves 256 bytes (with length code 255 in hexadecimal as FF) from base register 7, plus displacement 000, to base register 8, plus displacement 001, would be coded as the 6-byte instruction "D2FF 8001 7000" (operator/length/address1/address2).

The System/360 was designed to separate the "system state" from the "problem state". This provided a basic level of security and recoverability from programming errors. Problem (user) programs could not modify data or program storage associated with the system state. Addressing, data, or operation exception errors made the machine enter the system state through a controlled routine so the operating system could try to correct or terminate the program in error. Similarly, it could recover certain processor hardware errors through the "machine check" routines.

Peripherals interfaced to the system via "channels". A channel is a specialized processor with the instruction set optimized for transferring data between a peripheral and main memory. In modern terms, this could be compared to direct memory access (DMA). The S/360 connects channels to control units with bus and tag cables; IBM eventually replaced these with (Enterprise Systems Connection (ESCON) and Fibre Connection (FICON) channels.

There were initially two types of channels; byte-multiplexer channels (known at the time simply as "multiplexor channels"), for connecting "slow speed" devices such as card readers and punches, line printers, and communications controllers, and selector channels for connecting high speed devices, such as disk drives, tape drives, data cells and drums. Every System/360 (except for the Model 20, which was not a standard 360) has a byte-multiplexer channel and 1 or more selector channels, though the model 25 has just one channel, which can be either a byte-multiplexor or selector channel. The smaller models (up to the model 50) have integrated channels, while for the larger models (model 65 and above) the channels are large separate units in separate cabinets: the IBM 2870 is the byte-multiplexor channel with up to four selector sub-channels, and the IBM 2860 is up to three selector channels.

The byte-multiplexer channel is able to handle I/O to/from several devices simultaneously at the device's highest rated speeds, hence the name, as it multiplexed I/O from those devices onto a single data path to main memory. Devices connected to a byte-multiplexer channel are configured to operate in 1-byte, 2-byte, 4-byte, or "burst" mode. The larger "blocks" of data are used to handle progressively faster devices. For example, a 2501 card reader operating at 600 cards per minute would be in 1-byte mode, while a 1403-N1 printer would be in burst mode. Also, the byte-multiplexer channels on larger models have an optional selector subchannel section that would accommodate tape drives. The byte-multiplexor's channel address was typically "0" and the selector subchannel addresses were from "C0" to "FF." Thus, tape drives on System/360 were commonly addressed at 0C0-0C7. Other common byte-multiplexer addresses are: 00A: 2501 Card Reader, 00C/00D: 2540 Reader/Punch, 00E/00F: 1403-N1 Printers, 010-013: 3211 Printers, 020-0BF: 2701/2703 Telecommunications Units. These addresses are still commonly used in z/VM virtual machines.

System/360 models 40 and 50 have an integrated 1052-7 console that is usually addressed as 01F, however, this was not connected to the byte-multiplexer channel, but rather, had a direct internal connection to the mainframe. The model 30 attached a different model of 1052 through a 1051 control unit. The models 60 through 75 also use the 1052-7.

Selector channels enabled I/O to high speed devices. These storage devices were attached to a control unit and then to the channel. The control unit let clusters of devices be attached to the channels. On higher speed models, multiple selector channels, which could operate simultaneously or in parallel, improved overall performance.

Control units are connected to the channels with "bus and tag" cable pairs. The bus cables carried the address and data information and the tag cables identified what data was on the bus. The general configuration of a channel is to connect the devices in a chain, like this: Mainframe—Control Unit X—Control Unit Y—Control Unit Z. Each control unit is assigned a "capture range" of addresses that it services. For example, control unit X might capture addresses 40-4F, control unit Y: C0-DF, and control unit Z: 80-9F. Capture ranges had to be a multiple of 8, 16, 32, 64, or 128 devices and be aligned on appropriate boundaries. Each control unit in turn has one or more devices attached to it. For example, you could have control unit Y with 6 disks, that would be addressed as C0-C5.

There are three general types of bus-and-tag cables produced by IBM. The first is the standard gray bus-and-tag cable, followed by the blue bus-and-tag cable, and finally the tan bus-and-tag cable. Generally, newer cable revisions are capable of higher speeds or longer distances, and some peripherals specified minimum cable revisions both upstream and downstream.

The cable ordering of the control units on the channel is also significant. Each control unit is "strapped" as High or Low priority. When a device selection was sent out on a mainframe's channel, the selection was sent from X->Y->Z->Y->X. If the control unit was "high" then the selection was checked in the outbound direction, if "low" then the inbound direction. Thus, control unit X was either 1st or 5th, Y was either 2nd or 4th, and Z was 3rd in line. It is also possible to have multiple channels attached to a control unit from the same or multiple mainframes, thus providing a rich high-performance, multiple-access, and backup capability.

Typically the total cable length of a channel is limited to 200 feet, less being preferred. Each control unit accounts for about 10 "feet" of the 200-foot limit.

IBM first introduced a new type of I/O channel on the Model 85 and Model 195, the 2880 block multiplexer channel, and then made them standard on the System/370. This channel allowed a device to suspend a channel program, pending the completion of an I/O operation and thus to free the channel for use by another device. A block multiplexer channel can support either standard 1.5 MB/second connections or, with the 2-byte interface feature, 3 MB/second; the latter use one tag cable and two bus cables. On the S/370 there is an option for a 3.0 MB/s data streaming channel with one bus cable and one tag cable.

The initial use for this was the 2305 fixed-head disk, which has 8 "exposures" (alias addresses) and rotational position sensing (RPS).

Block multiplexer channels can operate as a selector channel to allow compatible attachment of legacy subsystems.

Being uncertain of the reliability and availability of the then new monolithic integrated circuits, IBM chose instead to design and manufacture its own custom hybrid integrated circuits. These were built on 11 mm square ceramic substrates. Resistors were silk screened on and discrete glass encapsulated transistors and diodes were added. The substrate was then covered with a metal lid or encapsulated in plastic to create a "Solid Logic Technology" (SLT) module.

A number of these SLT modules were then flip chip mounted onto a small multi-layer printed circuit "SLT card". Each card had one or two sockets on one edge that plugged onto pins on one of the computer's "SLT boards". This was the reverse of how most other company's cards were mounted, where the cards had pins or printed contact areas and plugged into sockets on the computer's boards.

Up to twenty SLT boards could be assembled side-by-side (vertically and horizontally) to form a "logic gate". Several gates mounted together constituted a box-shaped "logic frame". The outer gates were generally hinged along one vertical edge so they could be swung open to provide access to the fixed inner gates. The larger machines could have more than one frame bolted together to produce the final unit, such as a multi-frame Central Processing Unit (CPU).

The smaller System/360 models used the Basic Operating System/360 (BOS/360), Tape Operating System (TOS/360), or Disk Operating System/360 (DOS/360, which evolved into DOS/VS, DOS/VSE, VSE/AF, VSE/SP, VSE/ESA, and then z/VSE).

The larger models used Operating System/360 (OS/360). IBM developed several versions of OS/360, with increasingly powerful features: Primary Control Program (PCP), Multiprogramming with a Fixed number of Tasks (MFT), and Multiprogramming with a Variable number of Tasks (MVT). MVT took a long time to develop into a usable system, and the less ambitious MFT was widely used. PCP was used on intermediate machines; the final releases of OS/360 included only MFT and MVT. For the System/370 and later machines, MFT evolved into OS/VS1, while MVT evolved into OS/VS2 (SVS) (Single Virtual Storage), then various versions of MVS (Multiple Virtual Storage) culminating in the current z/OS.

When it announced the Model 67 in August 1965, IBM also announced TSS/360 (Time-Sharing System) for delivery at the same time as the 67. TSS/360, a response to Multics, was an ambitious project that included many advanced features. It had performance problems, was delayed, canceled, reinstated, and finally canceled again in 1971. Customers migrated to CP-67, MTS (Michigan Terminal System), TSO (Time Sharing Option for OS/360), or one of several other time-sharing systems.

CP-67, the original virtual machine system, was also known as CP/CMS. CP/67 was developed outside the IBM mainstream at IBM's Cambridge Scientific Center, in cooperation with MIT researchers. CP/CMS eventually won wide acceptance, and led to the development of VM/370 (Virtual Machine) which had a primary interactive "sub" operating system known as VM/CMS (Conversational Monitoring System). This evolved into today's z/VM.

The Model 20 offered a simplified and rarely used tape-based system called TPS (Tape Processing System), and DPS (Disk Processing System) that provided support for the 2311 disk drive. TPS could run on a machine with 8 KB of memory; DPS required 12 KB, which was pretty hefty for a Model 20. Many customers ran quite happily with 4 KB and CPS (Card Processing System). With TPS and DPS, the card reader was used to read the Job Control Language cards that defined the stack of jobs to run and to read in transaction data such as customer payments. The operating system was held on tape or disk, and results could also be stored on the tapes or hard drives. Stacked job processing became an exciting possibility for the small but adventurous computer user.

A little-known and little-used suite of 80-column punched-card utility programs known as Basic Programming Support (BPS) (jocularly: Barely Programming Support), a precursor of TOS, was available for smaller systems.

IBM created a new naming system for the new components created for System/360, although well-known old names, like IBM 1403 and IBM 1052, were retained. In this new naming system, components were given four-digit numbers starting with 2. The second digit described the type of component, as follows:
IBM developed a new family of peripheral equipment for System/360, carrying over a few from its older 1400 series. Interfaces were standardized, allowing greater flexibility to mix and match processors, controllers and peripherals than in the earlier product lines.

In addition, System/360 computers could use certain peripherals that were originally developed for earlier computers. These earlier peripherals used a different numbering system, such as the IBM 1403 chain printer. The 1403, an extremely reliable device that had already earned a reputation as a workhorse, was sold as the 1403-N1 when adapted for the System/360.

Also available were optical character recognition (OCR) readers IBM 1287 and IBM 1288 which could read Alpha Numeric (A/N) and Numeric Hand Printed (NHP/NHW) Characters from Cashier's rolls of tape to full legal size pages. At the time this was done with very large optical/logic readers. Software was too slow and expensive at that time.

Models 65 and below sold with an IBM 1052-7 as the console typewriter. The 360/85 with feature 5450 uses a display console that was not compatible with anything else in the line; the later 3066 console for the 370/165 and 370/168 use the same basic display design as the 360/85.
The IBM System/360 models 91 and 195 use a graphical display similar to the IBM 2250 as their primary console.

Additional operator consoles were also available. Certain high-end machines could optionally be purchased with a 2250 graphical display, costing upwards of US $100,000; smaller machines could use the less expensive 2260 display or later the 3270.

The first disk drives for System/360 were IBM 2302s and IBM 2311s. The first drum for System/360 was the IBM 7320.
The 156 KB/second 2302 was based on the earlier 1302 and was available as a model 3 with two 112.79 MB modules or as a model 4 with four such modules.

The 2311, with a removable 1316 disk pack, was based on the IBM 1311 and had a theoretical capacity of 7.2 MB, although actual capacity varied with record design. (When used with a 360/20, the 1316 pack was formatted into fixed-length 270 byte sectors, giving a maximum capacity of 5.4MB.)
In 1966, the first 2314s shipped. This device had up to eight usable disk drives with an integral control unit; there were nine drives, but one was reserved as a spare. Each drive used a removable 2316 disk pack with a capacity of nearly 28 MB. The disk packs for the 2311 and 2314 were "physically" large by today's standards — e.g., the 1316 disk pack was about in diameter and had six platters stacked on a central spindle. The top and bottom outside platters did not store data. Data were recorded on the inner sides of the top and bottom platters and both sides of the inner platters, providing 10 recording surfaces. The 10 read/write heads moved together across the surfaces of the platters, which were formatted with 203 concentric tracks. To reduce the amount of head movement (seeking), data was written in a virtual cylinder from inside top platter down to inside bottom platter. These disks were not usually formatted with fixed-sized sectors as are today's hard drives (though this "was" done with CP/CMS). Rather, most System/360 I/O software could customize the length of the data record (variable-length records), as was the case with magnetic tapes.

Some of the most powerful early System/360s used high-speed head-per-track drum storage devices. The 3,500 RPM 2301, which replaced the 7320, was part of the original System/360 announcement, with a capacity of 4 MB. The 303.8 KB/second IBM 2303 was announced on January 31, 1966, with a capacity of 3.913 MB. These were the only drums announced for System/360 and System/370, and their niche was later filled by fixed-head disks.

The 6,000 RPM 2305 appeared in 1970, with capacities of 5 MB (2305-1) or 11 MB (2305-2) per module. Although these devices did not have large capacity, their speed and transfer rates made them attractive for high-performance needs. A typical use was overlay linkage (e.g. for OS and application subroutines) for program sections written to alternate in the same memory regions. Fixed head disks and drums were particularly effective as paging devices on the early virtual memory systems. The 2305, although often called a "drum" was actually a head-per-track disk device, with 12 recording surfaces and a data transfer rate up to 3 MB per second.

Rarely seen was the IBM 2321 Data Cell, a mechanically complex device that contained multiple magnetic strips to hold data; strips could be randomly accessed, placed upon a cylinder-shaped drum for read/write operations; then returned to an internal storage cartridge. The IBM Data Cell [noodle picker] was among several IBM trademarked "speedy" mass online direct-access storage peripherals (reincarnated in recent years as "virtual tape" and automated tape librarian peripherals). The 2321 file had a capacity of 400 MB, at the time when the 2311 disk drive only had 7.2 MB. The IBM Data Cell was proposed to fill cost/capacity/speed gap between magnetic tapes—which had high capacity with relatively low cost per stored byte—and disks, which had higher expense per byte. Some installations also found the electromechanical operation less dependable and opted for less mechanical forms of direct-access storage.

The Model 44 was unique in offering an integrated single-disk drive as a standard feature. This drive used the 2315 "ramkit" cartridge and provided 1,171,200 bytes of storage.

The 2400 tape drives consisted of a combined drive and control unit, plus individual 1/2" tape drives attached. With System/360, IBM switched from IBM 7 track to 9 track tape format. 2400 drives could be purchased that read and wrote 7-track tapes for compatibility with the older IBM 729 tape drives. In 1967, a slower and cheaper pair of tape drives with integrated control unit was introduced: the 2415. In 1968, the IBM 2420 tape system was released, offering much higher data rates, self-threading tape operation and 1600bpi packing density. It remained in the product line until 1979.


Despite having been sold or leased in very large numbers for a mainframe system of its era only a few of System/360 computers remain mainly as non-operating property of museums or collectors. Examples of existing systems include:
A running list of remaining System/360s can be found at World Inventory of remaining System/360 CPUs.

This gallery shows the operator's console, with register value lamps, toggle switches (middle of pictures), and "emergency pull" switch (upper right of pictures) of the various models.

In the US television series "Mad Men" (2007–2015), the "IBM 360" was featured as a plot device in which a company leased the system to the advertising agency and was a prominent background in the seventh season.

A crowdfunding campaign for rescuing and restoring an IBM 360 system from Nuremberg has received successful funding.






</doc>
<doc id="29298" url="https://en.wikipedia.org/wiki?curid=29298" title="Spouse">
Spouse

A spouse is a significant other in a marriage, civil union, or common-law marriage. The term is gender neutral, whereas a male spouse is a husband and a female spouse is a wife. Although a spouse is a form of significant other, the latter term also includes non-marital partners who play a social role similar to that of a spouse, but do not have rights and duties reserved by law to a spouse.

The legal status of a spouse, and the specific rights and obligations associated with that status, vary significantly among the jurisdictions of the world. These regulations are usually described in family law statutes. However, in many parts of the world, where civil marriage is not that prevalent, there is instead customary marriage, which is usually regulated informally by the community. In many parts of the world, spousal rights and obligations are related to the payment of bride price, dowry or dower. Historically, many societies have given sets of rights and obligations to male marital partners that have been very different from the sets of rights and obligations given to female marital partners. In particular, the control of marital property, inheritance rights, and the right to dictate the activities of children of the marriage, have typically been given to male marital partners. However, this practice was curtailed to a great deal in many countries in the twentieth century, and more modern statutes tend to define the rights and duties of a spouse without reference to gender. Among the last European countries to establish full gender equality in marriage were Switzerland, Greece, Spain, and France in the 1980s. In various marriage laws around the world, however, the husband continues to have authority; for instance the Civil Code of Iran states at Article 1105: ""In relations between husband and wife; the position of the head of the family is the exclusive right of the husband"".

Depending on jurisdiction, the refusal or inability of a spouse to perform the marital obligations may constitute a ground for divorce, legal separation or annulment. The latter two options are more prevalent in countries where the dominant religion is Roman Catholicism, some of which introduced divorce only recently (i.e. Italy in 1970, Portugal in 1975, Brazil in 1977, Spain in 1981, Argentina in 1987, Paraguay in 1991, Colombia in 1991, Ireland in 1996, Chile in 2004 and Malta in 2011). In recent years, many Western countries have adopted no fault divorce. In some parts of the world, the formal dissolution of a marriage is complicated by the payments and goods which have been exchanged between families (this is common where marriages are arranged). This often makes it difficult to leave a marriage, especially for the woman: in some parts of Africa, once the bride price has been paid, the wife is seen as belonging to the husband and his family; and if she wants to leave, the husband may demand back the bride price that he had paid to the girl's family. The girl's family often cannot or does not want to pay it back.

Regardless of legislation, personal relations between spouses may also be influenced by local culture and religion.

There is often a minimum legal marriageable age. The United Nations Population Fund stated the following:

Although in Western countries spouses sometimes choose not to have children, such a choice is not accepted in some parts of the world. In some cultures and religions, the quality of a spouse imposes an obligation to have children. In northern Ghana, for example, the payment of bride price signifies a woman's requirement to bear children, and women using birth control are at risks of threats and coercion.

There are many ways in which a spouse is chosen, which vary across the world, and include love marriage, arranged marriage, and forced marriage. The latter is in some jurisdictions a void marriage or a voidable marriage. Forcing someone to marry is also a criminal offense in some countries.



</doc>
<doc id="29299" url="https://en.wikipedia.org/wiki?curid=29299" title="Sexuality (disambiguation)">
Sexuality (disambiguation)

Human sexuality is the capacity to have erotic experiences and responses.

Sexuality may also refer to:



</doc>
<doc id="29301" url="https://en.wikipedia.org/wiki?curid=29301" title="Semiotics">
Semiotics

Semiotics (also called semiotic studies) is the study of sign process (semiosis), which is any form of activity, conduct, or any process that involves signs, including the production of meaning. A sign is anything that communicates a meaning, that is not the sign itself, to the interpreter of the sign. The meaning can be intentional such as a word uttered with a specific meaning, or unintentional, such as a symptom being a sign of a particular medical condition. Signs can communicate through any of the senses, visual, auditory, tactile, olfactory, or gustatory.

The semiotic tradition explores the study of signs and symbols as a significant part of communications. Unlike linguistics, semiotics also studies non-linguistic sign systems. Semiotics includes the study of signs and sign processes, indication, designation, likeness, analogy, allegory, metonymy, metaphor, symbolism, signification, and communication. 

Semiotics is frequently seen as having important anthropological and sociological dimensions; for example, the Italian semiotician and novelist Umberto Eco proposed that every cultural phenomenon may be studied as communication. Some semioticians focus on the logical dimensions of the science, however. They examine areas belonging also to the life sciences—such as how organisms make predictions about, and adapt to, their semiotic niche in the world (see semiosis). In general, semiotic theories take "signs" or sign systems as their object of study: the communication of information in living organisms is covered in biosemiotics (including zoosemiotics and phytosemiotics).

Semiotics is not to be confused with the Saussurean tradition called semiology, which is a subset of semiotics.

The importance of signs and signification has been recognized throughout much of the history of philosophy, and in psychology as well. The term derives from the , "observant of signs" (from σημεῖον "sēmeion", "a sign, a mark"). For the Greeks, "signs" occurred in the world of nature, and "symbols" in the world of culture. As such, Plato and Aristotle explored the relationship between signs and the world.

It would not be until Augustine of Hippo that the nature of the sign would be considered within a conventional system. Augustine introduced a thematic proposal for uniting the two under the notion of "sign" ("signum") as transcending the nature-culture divide and identifying symbols as no more than a species (or sub-species) of "signum" be formally proposed. A monograph study on this question would be done by Manetti (1987). These theories have had a lasting effect in Western philosophy, especially through scholastic philosophy. 

The general study of signs that began in Latin with Augustine culminated with the 1632 "Tractatus de Signis" of John Poinsot, and then began anew in late modernity with the attempt in 1867 by Charles Sanders Peirce to draw up a "new list of categories." More recently, Umberto Eco, in his "Semiotics and the Philosophy of Language", has argued that semiotic theories are implicit in the work of most, perhaps all, major thinkers.

John Locke (1690), himself a man of medicine, was familiar with this "semeiotics" as naming a specialized branch within medical science. In his personal library were two editions of Scapula's 1579 abridgement of Henricus Stephanus' "Thesaurus Graecae Linguae", which listed "σημειωτική" as the name for "diagnostics," the branch of medicine concerned with interpreting symptoms of disease ("symptomatology"). Indeed, physician and scholar Henry Stubbe (1670) had transliterated this term of specialized science into English precisely as ""semeiotics"," marking the first use of the term in English:"…nor is there any thing to be relied upon in Physick, but an exact knowledge of medicinal phisiology (founded on observation, not principles), semeiotics, method of curing, and tried (not excogitated, not commanding) medicines.…"Locke would use the term "sem(e)iotike" in "An Essay Concerning Human Understanding" (book IV, chap. 21), in which he explains how science may be divided into three parts:

Locke then elaborates on the nature of this third category, naming it "Σημειωτική" ("Semeiotike"), and explaining it as "the doctrine of signs" in the following terms:

Yuri Lotman would introduce Eastern Europe to semiotics and adopt Locke's coinage ("Σημειωτική") as the name to subtitle his founding at the University of Tartu in Estonia in 1964 of the first semiotics journal, "Sign Systems Studies".

Ferdinand de Saussure founded his semiotics, which he called semiology, in the social sciences:

Thomas Sebeok would assimilate "semiology" to "semiotics" as a part to a whole, and was involved in choosing the name "Semiotica" for the first international journal devoted to the study of signs. Saussurean semiotics have exercised a great deal of influence on the schools of Structuralism and Post-Structuralism. Jacques Derrida, for example, takes as his object the Saussurean relationship of signifier and signified, asserting that signifier and signified are not fixed, coining the expression "différance", relating to the endless deferral of meaning, and to the absence of a 'transcendent signified'. For Derrida, ""il n'y a pas de hors-texte"" (). 

In the nineteenth century, Charles Sanders Peirce defined what he termed "semiotic" (which he would sometimes spell as "semeiotic") as the "quasi-necessary, or formal doctrine of signs," which abstracts "what must be the characters of all signs used by…an intelligence capable of learning by experience," and which is philosophical logic pursued in terms of signs and sign processes. 

Peirce's perspective is considered as philosophical logic studied in terms of signs that are not always linguistic or artificial, and sign processes, modes of inference, and the inquiry process in general. The Peircean semiotic addresses not only the external communication mechanism, as per Saussure, but the internal representation machine, investigating sign processes, and modes of inference, as well as the whole inquiry process in general.

Peircean semiotic is triadic, including sign, object, interpretant, as opposed to the dyadic Saussurian tradition (signifier, signified). Peircean semiotics further subdivides each of the three triadic elements into three sub-types, positing the existence of signs that are symbols; semblances ("icons"); and "indices," i.e., signs that are such through a factual connection to their objects.

Peircean scholar and editor Max H. Fisch (1978) would claim that "semeiotic" was Peirce's own preferred rendering of Locke's σημιωτική. Charles W. Morris followed Peirce in using the term "semiotic" and in extending the discipline beyond human communication to animal learning and use of signals.

While the Saussurean semiotic is dyadic (sign/syntax, signal/semantics), the Peircean semiotic is triadic (sign, object, interpretant), being conceived as philosophical logic studied in terms of signs that are not always linguistic or artificial. 

Peirce would aim to base his new list directly upon experience precisely as constituted by action of signs, in contrast with the list of Aristotle's categories which aimed to articulate within experience the dimension of being that is independent of experience and knowable as such, through human understanding.

The estimative powers of animals interpret the environment as sensed to form a "meaningful world" of objects, but the objects of this world (or "Umwelt", in Jakob von Uexküll's term) consist exclusively of objects related to the animal as desirable (+), undesirable (–), or "safe to ignore" (0).

In contrast to this, human understanding adds to the animal "Umwelt" a relation of self-identity within objects which transforms objects experienced into "things" as well as +, –, 0 objects. Thus, the generically animal objective world as "Umwelt", becomes a species-specifically human objective world or "Lebenswelt" (life-world), wherein linguistic communication, rooted in the biologically underdetermined "Innenwelt" (inner-world) of humans, makes possible the further dimension of cultural organization within the otherwise merely social organization of non-human animals whose powers of observation may deal only with directly sensible instances of objectivity. 

This further point, that human culture depends upon language understood first of all not as communication, but as the biologically underdetermined aspect or feature of the human animal's "Innenwelt", was originally clearly identified by Thomas A. Sebeok. Sebeok also played the central role in bringing Peirce's work to the center of the semiotic stage in the twentieth century, first with his expansion of the human use of signs (""anthroposemiosis"") to include also the generically animal sign-usage (""zoösemiosis""), then with his further expansion of semiosis to include the vegetative world (""phytosemiosis""). Such would initially be based on the work of Martin Krampen, but takes advantage of Peirce's point that an interpretant, as the third item within a sign relation, "need not be mental."

Peirce's distinguished between the interpretant and the interpreter. The interpretant is the internal, mental representation that mediates between the object and its sign. The interpreter is the human who is creating the interpretant. Peirce's "interpretant" notion opened the way to understanding an action of signs beyond the realm of animal life (study of "phytosemiosis" + "zoösemiosis" + "anthroposemiosis" = "biosemiotics"), which was his first advance beyond Latin Age semiotics. Other early theorists in the field of semiotics include Charles W. Morris. Max Black argued that the work of Bertrand Russell was seminal in the field.

Semioticians classify signs or sign systems in relation to the way they are transmitted (see modality). This process of carrying meaning depends on the use of codes that may be the individual sounds or letters that humans use to form words, the body movements they make to show attitude or emotion, or even something as general as the clothes they wear. To coin a word to refer to a "thing" (see lexical words), the community must agree on a simple meaning (a denotative meaning) within their language, but that word can transmit that meaning only within the language's grammatical structures and codes (see syntax and semantics). Codes also represent the values of the culture, and are able to add new shades of connotation to every aspect of life.

To explain the relationship between semiotics and communication studies, communication is defined as the process of transferring data and-or meaning from a source to a receiver. Hence, communication theorists construct models based on codes, media, and contexts to explain the biology, psychology, and mechanics involved. Both disciplines recognize that the technical process cannot be separated from the fact that the receiver must decode the data, i.e., be able to distinguish the data as salient, and make meaning out of it. This implies that there is a necessary overlap between semiotics and communication. Indeed, many of the concepts are shared, although in each field the emphasis is different. In "Messages and Meanings: An Introduction to Semiotics", Marcel Danesi (1994) suggested that semioticians' priorities were to study signification first, and communication second. A more extreme view is offered by Jean-Jacques Nattiez who, as a musicologist, considered the theoretical study of communication irrelevant to his application of semiotics.

Semiotics differs from linguistics in that it generalizes the definition of a sign to encompass signs in any medium or sensory modality. Thus it broadens the range of sign systems and sign relations, and extends the definition of language in what amounts to its widest analogical or metaphorical sense. The branch of semiotics that deals with such formal relations between signs or expressions in abstraction from their signification and their interpreters, or—more generally—with formal properties of symbol systems (specifically, with reference to linguistic signs, syntax) is referred to as syntactics.

Peirce's definition of the term "semiotic" as the study of necessary features of signs also has the effect of distinguishing the discipline from linguistics as the study of contingent features that the world's languages happen to have acquired in the course of their evolutions. From a subjective standpoint, perhaps more difficult is the distinction between semiotics and the philosophy of language. In a sense, the difference lies between separate traditions rather than subjects. Different authors have called themselves "philosopher of language" or "semiotician". This difference does "not" match the separation between analytic and continental philosophy. On a closer look, there may be found some differences regarding subjects. Philosophy of language pays more attention to natural languages or to languages in general, while semiotics is deeply concerned with non-linguistic signification. Philosophy of language also bears connections to linguistics, while semiotics might appear closer to some of the humanities (including literary theory) and to cultural anthropology.

Semiosis or "semeiosis" is the process that forms meaning from any organism's apprehension of the world through signs. Scholars who have talked about semiosis in their subtheories of semiotics include C. S. Peirce, John Deely, and Umberto Eco. Cognitive semiotics is combining methods and theories developed in the disciplines of cognitive methods and theories developed in semiotics and the humanities, with providing new information into human signification and its manifestation in cultural practices. The research on cognitive semiotics brings together semiotics from linguistics, cognitive science, and related disciplines on a common meta-theoretical platform of concepts, methods, and shared data.

Cognitive semiotics may also be seen as the study of meaning-making by employing and integrating methods and theories developed in the cognitive sciences. This involves conceptual and textual analysis as well as experimental investigations. Cognitive semiotics initially was developed at the Center for Semiotics at Aarhus University (Denmark), with an important connection with the Center of Functionally Integrated Neuroscience (CFIN) at Aarhus Hospital. Amongst the prominent cognitive semioticians are Per Aage Brandt, Svend Østergaard, Peer Bundgård, Frederik Stjernfelt, Mikkel Wallentin, Kristian Tylén, Riccardo Fusaroli, and Jordan Zlatev. Zlatev later in co-operation with Göran Sonesson established CCS (Center for Cognitive Semiotics) at Lund University, Sweden.

Finite semiotics, developed by Cameron Shackell (2018, 2019), aims to unify existing theories of semiotics for application to the post-Baudrillardian world of ubiquitous technology. Its central move is to place the finiteness of thought at the root of semiotics and the sign as a secondary but fundamental analytical construct. The theory contends that the levels of reproduction that technology is bringing to human environments demands this reprioritisation if semiotics is to remain relevant in the face of effectively infinite signs. The shift in emphasis allows practical definitions of many core constructs in semiotics which Shackell has applied to areas such as human computer interaction, creativity theory, and a computational semiotics method for generating semiotic squares from digital texts.

Pictorial semiotics is intimately connected to art history and theory. It goes beyond them both in at least one fundamental way, however. While art history has limited its visual analysis to a small number of pictures that qualify as "works of art", pictorial semiotics focuses on the properties of pictures in a general sense, and on how the artistic conventions of images can be interpreted through pictorial codes. Pictorial codes are the way in which viewers of pictorial representations seem automatically to decipher the artistic conventions of images by being unconsciously familiar with them.

According to Göran Sonesson, a Swedish semiotician, pictures can be analyzed by three models: (a) the narrative model, which concentrates on the relationship between pictures and time in a chronological manner as in a comic strip; (b) the rhetoric model, which compares pictures with different devices as in a metaphor; and (c) the Laokoon model, which considers the limits and constraints of pictorial expressions by comparing textual mediums that utilize time with visual mediums that utilize space.

The break from traditional art history and theory—as well as from other major streams of semiotic analysis—leaves open a wide variety of possibilities for pictorial semiotics. Some influences have been drawn from phenomenological analysis, cognitive psychology, structuralist, and cognitivist linguistics, and visual anthropology and sociology.

Studies have shown that semiotics may be used to make or break a brand. Culture codes strongly influence whether a population likes or dislikes a brand's marketing, especially internationally. If the company is unaware of a culture's codes, it runs the risk of failing in its marketing. Globalization has caused the development of a global consumer culture where products have similar associations, whether positive or negative, across numerous markets.

Mistranslations may lead to instances of "Engrish" or "Chinglish", terms for unintentionally humorous cross-cultural slogans intended to be understood in English. This may be caused by a sign that, in Peirce's terms, mistakenly indexes or symbolizes something in one culture, that it does not in another. In other words, it creates a connotation that is culturally-bound, and that violates some culture code. Theorists who have studied humor (such as Schopenhauer) suggest that contradiction or incongruity creates absurdity and therefore, humor. Violating a culture code creates this construct of ridiculousness for the culture that owns the code. Intentional humor also may fail cross-culturally because jokes are not on code for the receiving culture.

A good example of branding according to cultural code is Disney's international theme park business. Disney fits well with Japan's cultural code because the Japanese value "cuteness", politeness, and gift giving as part of their culture code; Tokyo Disneyland sells the most souvenirs of any Disney theme park. In contrast, Disneyland Paris failed when it launched as Euro Disney because the company did not research the codes underlying European culture. Its storybook retelling of European folktales was taken as elitist and insulting, and the strict appearance standards that it had for employees resulted in discrimination lawsuits in France. Disney souvenirs were perceived as cheap trinkets. The park was a financial failure because its code violated the expectations of European culture in ways that were offensive.

On the other hand, some researchers have suggested that it is possible to successfully pass a sign perceived as a cultural icon, such as the Coca-Cola or McDonald's logos, from one culture to another. This may be accomplished if the sign is migrated from a more economically-developed to a less developed culture. The intentional association of a product with another culture has been called Foreign Consumer Culture Positioning (FCCP). Products also may be marketed using global trends or culture codes, for example, saving time in a busy world; but even these may be fine-tuned for specific cultures.

Research also found that, as airline industry brandings grow and become more international, their logos become more symbolic and less iconic. The iconicity and symbolism of a sign depends on the cultural convention and, are on that ground in relation with each other. If the cultural convention has greater influence on the sign, the signs get more symbolic value.

The flexibility of human semiotics is well demonstrated in dreams. Sigmund Freud spelled out how meaning in dreams rests on a blend of images, affects, sounds, words, and kinesthetic sensations. In his chapter on "The Means of Representation," he showed how the most abstract sorts of meaning and logical relations can be represented by spatial relations. Two images in sequence may indicate "if this, then that" or "despite this, that". Freud thought the dream started with "dream thoughts" which were like logical, verbal sentences. He believed that the dream thought was in the nature of a taboo wish that would awaken the dreamer. In order to safeguard sleep, the mindbrain converts and disguises the verbal dream thought into an imagistic form, through processes he called the "dream-work".

Subfields that have sprouted out of semiotics include, but are not limited to, the following:


Charles Sanders Peirce (1839–1914), a noted logician who founded philosophical pragmatism, defined "semiosis" as an irreducibly triadic process wherein something, as an object, logically determines or influences something as a sign to determine or influence something as an interpretation or "interpretant", itself a sign, thus leading to further interpretants. Semiosis is logically structured to perpetuate itself. The object may be quality, fact, rule, or even fictional (Hamlet), and may be "immediate" to the sign, the object as represented in the sign, or "dynamic", the object as it really is, on which the immediate object is founded. The interpretant may be "immediate" to the sign, all that the sign immediately expresses, such as a word's usual meaning; or "dynamic", such as a state of agitation; or "final" or "normal", the ultimate ramifications of the sign about its object, to which inquiry taken far enough would be destined and with which any interpretant, at most, may coincide. His "semiotic" covered not only artificial, linguistic, and symbolic signs, but also semblances such as kindred sensible qualities, and indices such as reactions. He came c. 1903 to classify any sign by three interdependent trichotomies, intersecting to form ten (rather than 27) classes of sign. Signs also enter into various kinds of meaningful combinations; Peirce covered both semantic and syntactical issues in his speculative grammar. He regarded formal semiotic as logic "per se" and part of philosophy; as also encompassing study of arguments (hypothetical, deductive, and inductive) and inquiry's methods including pragmatism; and as allied to, but distinct from logic's pure mathematics. In addition to pragmatism, Peirce provided a definition of "sign" as a "representamen", in order to bring out the fact that a sign is something that "represents" something else in order to suggest it (that is, "re-present" it) in some way: "A sign, or representamen, is something which stands to somebody for something in some respect or capacity. It addresses somebody, that is, creates in the mind of that person an equivalent sign. That sign which it creates I call the interpretant of the first sign. The sign stands for something, its object not in all respects, but in reference to a sort of idea." 

Ferdinand de Saussure (1857–1913), the "father" of modern linguistics, proposed a dualistic notion of signs, relating the "signifier" as the form of the word or phrase uttered, to the "signified" as the mental concept. According to Saussure, the sign is completely arbitrary—i.e., there is no necessary connection between the sign and its meaning. This sets him apart from previous philosophers, such as Plato or the scholastics, who thought that there must be some connection between a signifier and the object it signifies. In his "Course in General Linguistics", Saussure credits the American linguist William Dwight Whitney (1827–1894) with insisting on the arbitrary nature of the sign. Saussure's insistence on the arbitrariness of the sign also has influenced later philosophers and theorists such as Jacques Derrida, Roland Barthes, and Jean Baudrillard. Ferdinand de Saussure coined the term "sémiologie" while teaching his landmark "Course on General Linguistics" at the University of Geneva from 1906 to 1911. Saussure posited that no word is inherently meaningful. Rather a word is only a "signifier." i.e., the representation of something, and it must be combined in the brain with the "signified", or the thing itself, in order to form a meaning-imbued "sign." Saussure believed that dismantling signs was a real science, for in doing so we come to an empirical understanding of how humans synthesize physical stimuli into words and other abstract concepts.

Jakob von Uexküll (1864–1944) studied the sign processes in animals. He used the German word "umwelt", "environment," to describe the individual's subjective world, and he invented the concept of functional circle ("funktionskreis") as a general model of sign processes. In his "Theory of Meaning" ("Bedeutungslehre", 1940), he described the semiotic approach to biology, thus establishing the field that now is called biosemiotics.

Valentin Voloshinov (1895–1936) was a Soviet-Russian linguist, whose work has been influential in the field of literary theory and Marxist theory of ideology. Written in the late 1920s in the USSR, Voloshinov's "Marxism and the Philosophy of Language" () developed a counter-Saussurean linguistics, which situated language use in social process rather than in an entirely decontextualized Saussurean "langue".

Louis Hjelmslev (1899–1965) developed a formalist approach to Saussure's structuralist theories. His best known work is "Prolegomena to a Theory of Language", which was expanded in "Résumé of the Theory of Language", a formal development of "glossematics", his scientific calculus of language.

Charles W. Morris (1901–1979): Unlike his mentor George Herbert Mead, Morris was a behaviorist and sympathetic to the Vienna Circle positivism of his colleague, Rudolf Carnap. Morris was accused by John Dewey of misreading Peirce.

In his 1938 "Foundations of the Theory of Signs", he defined semiotics as grouped into three branches:


Thure von Uexküll (1908–2004), the "father" of modern psychosomatic medicine, developed a diagnostic method based on semiotic and biosemiotic analyses.

Roland Barthes (1915–1980) was a French literary theorist and semiotician. He often would critique pieces of cultural material to expose how bourgeois society used them to impose its values upon others. For instance, the portrayal of wine drinking in French society as a robust and healthy habit would be a bourgeois ideal perception contradicted by certain realities (i.e. that wine can be unhealthy and inebriating). He found semiotics useful in conducting these critiques. Barthes explained that these bourgeois cultural myths were second-order signs, or connotations. A picture of a full, dark bottle is a sign, a signifier relating to a signified: a fermented, alcoholic beverage—wine. However, the bourgeois take this signified and apply their own emphasis to it, making "wine" a new signifier, this time relating to a new signified: the idea of healthy, robust, relaxing wine. Motivations for such manipulations vary from a desire to sell products to a simple desire to maintain the status quo. These insights brought Barthes very much in line with similar Marxist theory.

Algirdas Julien Greimas (1917–1992) developed a structural version of semiotics named, "generative semiotics", trying to shift the focus of discipline from signs to systems of signification. His theories develop the ideas of Saussure, Hjelmslev, Claude Lévi-Strauss, and Maurice Merleau-Ponty.

Thomas A. Sebeok (1920–2001), a student of Charles W. Morris, was a prolific and wide-ranging American semiotician. Although he insisted that animals are not capable of language, he expanded the purview of semiotics to include non-human signaling and communication systems, thus raising some of the issues addressed by philosophy of mind and coining the term zoosemiotics. Sebeok insisted that all communication was made possible by the relationship between an organism and the environment in which it lives. He also posed the equation between "semiosis" (the activity of interpreting signs) and "life"—a view that the Copenhagen-Tartu biosemiotic school has further developed.

Yuri Lotman (1922–1993) was the founding member of the Tartu (or Tartu-Moscow) Semiotic School. He developed a semiotic approach to the study of culture—semiotics of culture—and established a communication model for the study of text semiotics. He also introduced the concept of the semiosphere. Among his Moscow colleagues were Vladimir Toporov, Vyacheslav Ivanov and Boris Uspensky.

Christian Metz (1931–1993) pioneered the application of Saussurean semiotics to film theory, applying syntagmatic analysis to scenes of films and grounding film semiotics in greater context.

Eliseo Verón (1935–2014) developed his "Social Discourse Theory" inspired in the Peircian conception of "Semiosis".

Groupe µ (founded 1967) developed a structural version of rhetorics, and the visual semiotics.

Umberto Eco (1932–2016) was an Italian novelist, semiotician and academic. He made a wider audience aware of semiotics by various publications, most notably "A Theory of Semiotics" and his novel, "The Name of the Rose", which includes (second to its plot) applied semiotic operations. His most important contributions to the field bear on interpretation, encyclopedia, and model reader. He also criticized in several works ("A theory of semiotics", "La struttura assente", "Le signe", "La production de signes") the "iconism" or "iconic signs" (taken from Peirce's most famous triadic relation, based on indexes, icons, and symbols), to which he proposed four modes of sign production: recognition, ostension, replica, and invention.

Paul Bouissac (born 1934) is a world renowned expert of circus studies, known for developing a range of semiotic interpretations of circus performances. This includes the multimodal dimensions of clowns and clowning, jugglers, and trapeze acts. He is the author of several books relating to the semiotics of the circus. Bouissac is the Series Editor for the Advances in Semiotics Series for Bloomsbury Academic. He runs the SemiotiX Bulletin which has a global readership, is a founding editor of the "Public Journal of Semiotics", and was a central founding figure in the Toronto Semiotic Circle. He is Professor Emeritus of Victoria College, University of Toronto. The personal, professional, and intellectual life of Bouissac is recounted in the book, "The Pleasures of Time: Two Men, A Life", by his life-long partner, the sociologist Stephen Harold Riggins.

Julia Kristeva (born 1941), a student of Lucien Goldmann and Roland Barthes, Bulgarian-French semiotician, literary critic, psychoanalyst, feminist, and novelist. She uses psychoanalytical concepts together with the semiotics, distinguishing the two components in the signification, the symbolic and the semiotic"." Kristeva also studies the representation of women and women's bodies in popular culture, such as horror films and has had a remarkable influence on feminism and feminist literary studies.

Some applications of semiotics include:

In some countries, the role of semiotics is limited to literary criticism and an appreciation of audio and visual media. This narrow focus may inhibit a more general study of the social and political forces shaping how different media are used and their dynamic status within modern culture. Issues of technological determinism in the choice of media and the design of communication strategies assume new importance in this age of mass media.

A world organisation of semioticians, the International Association for Semiotic Studies, and its journal "Semiotica", was established in 1969. The larger research centers together with teaching program include the semiotics departments at the University of Tartu, University of Limoges, Aarhus University, and Bologna University.

Publication of research is both in dedicated journals such as "Sign Systems Studies", established by Yuri Lotman and published by Tartu University Press; "Semiotica", founded by Thomas A. Sebeok and published by Mouton de Gruyter; "Zeitschrift für Semiotik"; "European Journal of Semiotics"; "Versus" (founded and directed by Umberto Eco), et al.; "The American Journal of Semiotics"; and as articles accepted in periodicals of other disciplines, especially journals oriented toward philosophy and cultural criticism.

The major semiotic book series "Semiotics, Communication, Cognition", published by De Gruyter Mouton (series editors Paul Cobley and Kalevi Kull) replaces the former "Approaches to Semiotics" (more than 120 volumes) and "Approaches to Applied Semiotics" (series editor Thomas A. Sebeok). Since 1980 the Semiotic Society of America has produced an annual conference series: "".





</doc>
<doc id="29305" url="https://en.wikipedia.org/wiki?curid=29305" title="Sojourner Truth">
Sojourner Truth

Sojourner Truth (; born Isabella "Belle" Baumfree; November 26, 1883) was an American abolitionist and women's rights activist. Truth was born into slavery in Swartekill, New York, but escaped with her infant daughter to freedom in 1826. After going to court to recover her son in 1828, she became the first black woman to win such a case against a white man.

She gave herself the name Sojourner Truth in 1843 after she became convinced that God had called her to leave the city and go into the countryside "testifying the hope that was in her". Her best-known speech was delivered extemporaneously, in 1851, at the Ohio Women's Rights Convention in Akron, Ohio. The speech became widely known during the Civil War by the title "Ain't I a Woman?", a variation of the original speech re-written by someone else using a stereotypical Southern dialect, whereas Sojourner Truth was from New York and grew up speaking Dutch as her first language. During the Civil War, Truth helped recruit black troops for the Union Army; after the war, she tried unsuccessfully to secure land grants from the federal government for formerly enslaved people (summarized as the promise of "forty acres and a mule").

A memorial bust of Truth was unveiled in 2009 in Emancipation Hall in the U.S. Capitol Visitor's Center. She is the first African American woman to have a statue in the Capitol building. In 2014, Truth was included in "Smithsonian" magazine's list of the "100 Most Significant Americans of All Time". 

Truth was one of the 10 or 12 children born to James and Elizabeth Baumfree (or Bomefree). Colonel Hardenbergh bought James and Elizabeth Baumfree from slave traders and kept their family at his estate in a big hilly area called by the Dutch name Swartekill (just north of present-day Rifton), in the town of Esopus, New York, north of New York City. Charles Hardenbergh inherited his father's estate and continued to enslave people as a part of that estate's property.

When Charles Hardenbergh died in 1806, nine-year-old Truth (known as Belle), was sold at an auction with a flock of sheep for $100 to John Neely, near Kingston, New York. Until that time, Truth spoke only Dutch. She later described Neely as cruel and harsh, relating how he beat her daily and once even with a bundle of rods. In 1808 Neely sold her for $105 to tavern keeper Martinus Schryver of Port Ewen, New York, who owned her for 18 months. Schryver then sold Truth in 1810 to John Dumont of West Park, New York. John Dumont was a rapist and considerable tension existed between Truth and Dumont's wife, Elizabeth Waring Dumont, who harassed her and made her life more difficult.

Around 1815, Truth met and fell in love with an enslaved man named Robert from a neighboring farm. Robert's owner (Charles Catton, Jr., a landscape painter) forbade their relationship; he did not want the people he enslaved to have children with people he was not enslaving, because he would not own the children. One day Robert sneaked over to see Truth. When Catton and his son found him, they savagely beat Robert until Dumont finally intervened. Truth never saw Robert again after that day and he died a few years later. The experience haunted Truth throughout her life. Truth eventually married an older enslaved man named Thomas. She bore five children: James, her firstborn, who died in childhood, Diana (1815), the result of a rape by John Dumont, and Peter (1821), Elizabeth (1825), and Sophia (ca. 1826), all born after she and Thomas united.

In 1799, the State of New York began to legislate the abolition of slavery, although the process of emancipating those people enslaved in New York was not complete until July 4, 1827. Dumont had promised to grant Truth her freedom a year before the state emancipation, "if she would do well and be faithful". However, he changed his mind, claiming a hand injury had made her less productive. She was infuriated but continued working, spinning of wool, to satisfy her sense of obligation to him.

Late in 1826, Truth escaped to freedom with her infant daughter, Sophia. She had to leave her other children behind because they were not legally freed in the emancipation order until they had served as bound servants into their twenties. She later said, "I did not run off, for I thought that wicked, but I walked off, believing that to be all right."

She found her way to the home of Isaac and Maria Van Wagenen in New Paltz, who took her and her baby in. Isaac offered to buy her services for the remainder of the year (until the state's emancipation took effect), which Dumont accepted for $20. She lived there until the New York State Emancipation Act was approved a year later.

Truth learned that her son Peter, then five years old, had been sold illegally by Dumont to an owner in Alabama. With the help of the Van Wagenens, she took the issue to court and in 1828, after months of legal proceedings, she got back her son, who had been abused by those who were enslaving him. Truth became one of the first black women to go to court against a white man and win the case.

Truth had a life-changing religious experience during her stay with the Van Wagenens and became a devout Christian. In 1829 she moved with her son Peter to New York City, where she worked as a housekeeper for Elijah Pierson, a Christian Evangelist. While in New York, she befriended Mary Simpson, a grocer on John Street who claimed she had once been enslaved by George Washington. They shared an interest in charity for the poor and became intimate friends. In 1832, she met Robert Matthews, also known as Prophet Matthias, and went to work for him as a housekeeper at the Matthias Kingdom communal colony. Elijah Pierson died, and Robert Matthews and Truth were accused of stealing from and poisoning him. Both were acquitted of the murder, though Matthews was convicted of lesser crimes, served time, and moved west.

In 1839, Truth's son Peter took a job on a whaling ship called the "Zone of Nantucket". From 1840 to 1841, she received three letters from him, though in his third letter he told her he had sent five. Peter said he also never received any of her letters. When the ship returned to port in 1842, Peter was not on board and Truth never heard from him again.

The year 1843 was a turning point for Baumfree. She became a Methodist, and on June 1, Pentecost Sunday, she changed her name to Sojourner Truth. She chose the name because she heard the Spirit of God calling on her to preach the truth. She told her friends: "The Spirit calls me, and I must go", and left to make her way traveling and preaching about the abolition of slavery. Taking along only a few possessions in a pillowcase, she traveled north, working her way up through the Connecticut River Valley, towards Massachusetts.

At that time, Truth began attending Millerite Adventist camp meetings. Millerites followed the teachings of William Miller of New York, who preached that Jesus would appear in 1843–1844, bringing about the end of the world. Many in the Millerite community greatly appreciated Truth's preaching and singing, and she drew large crowds when she spoke. Like many others disappointed, Truth distanced herself from her Millerite friends for a while after the anticipated second coming did not arrive.

In 1844, she joined the Northampton Association of Education and Industry in Florence, Massachusetts. Founded by abolitionists, the organization supported women's rights and religious tolerance as well as pacifism. There were, in its four-and-a-half year history, a total of 240 members, though no more than 120 at any one time. They lived on , raising livestock, running a sawmill, a gristmill, and a silk factory. Truth lived and worked in the community and oversaw the laundry, supervising both men and women. While there, Truth met William Lloyd Garrison, Frederick Douglass, and David Ruggles. Encouraged by the community, Truth delivered her first anti-slavery speech that year.

In 1846, the group disbanded, unable to support itself. In 1845, she joined the household of George Benson, the brother-in-law of William Lloyd Garrison. In 1849, she visited John Dumont before he moved west.

Truth started dictating her memoirs to her friend Olive Gilbert and in 1850 William Lloyd Garrison privately published her book, "The Narrative of Sojourner Truth: a Northern Slave". That same year, she purchased a home in Florence for $300 and spoke at the first National Women's Rights Convention in Worcester, Massachusetts. In 1854, with proceeds from sales of the narrative and "cartes-de-visite" captioned, "I sell the shadow to support the substance", she paid off the mortgage held by her friend from the community, Samuel L. Hill.

In 1851, Truth joined George Thompson, an abolitionist and speaker, on a lecture tour through central and western New York State. In May, she attended the Ohio Women's Rights Convention in Akron, Ohio, where she delivered her famous extemporaneous speech on women's rights, later known as "Ain't I a Woman?". Her speech demanded equal human rights for all women as well as for all blacks. Advocating for women and African Americans was dangerous and challenging enough, but being one and doing so was far more difficult. The pressures and severity of her speech did not get to Truth, however. Truth took to the stage with a demanding and composed presence. Audience members were baffled by the way she carried herself and were hesitant to believe that she was even a woman, prompting the name of her speech "Ain't I a Woman?" The convention was organized by Hannah Tracy and Frances Dana Barker Gage, who both were present when Truth spoke. Different versions of Truth's words have been recorded, with the first one published a month later in the "Anti-Slavery Bugle" by Rev. Marius Robinson, the newspaper owner and editor who was in the audience. Robinson's recounting of the speech included no instance of the question "Ain't I a Woman?" Nor did any of the other newspapers reporting of her speech at the time. Twelve years later, in May 1863, Gage published another, very different, version. In it, Truth's speech pattern had characteristics of Southern slaves, and the speech was vastly different than the one Robinson had reported. Gage's version of the speech became the historic standard version, and is known as "Ain't I a Woman?" because that question was repeated four times. It is highly unlikely that Truth's own speech pattern was Southern in nature, as she was born and raised in New York, and she spoke only upper New York State low-Dutch until she was nine years old.

In contrast to Robinson's report, Gage's 1863 version included Truth saying her 13 children were sold away from her into slavery. Truth is widely believed to have had five children, with one sold away, and was never known to boast more children. Gage's 1863 recollection of the convention conflicts with her own report directly after the convention: Gage wrote in 1851 that Akron in general and the press, in particular, were largely friendly to the woman's rights convention, but in 1863 she wrote that the convention leaders were fearful of the "mobbish" opponents. Other eyewitness reports of Truth's speech told a calm story, one where all faces were "beaming with joyous gladness" at the session where Truth spoke; that not "one discordant note" interrupted the harmony of the proceedings. In contemporary reports, Truth was warmly received by the convention-goers, the majority of whom were long-standing abolitionists, friendly to progressive ideas of race and civil rights. In Gage's 1863 version, Truth was met with hisses, with voices calling to prevent her from speaking.

According to Frances Gage's recount in 1863, Truth argued, "That man over there says that women need to be helped into carriages, and lifted over ditches, and to have the best place everywhere. Nobody helps "me" any best place. "And ain't I a woman?"" Truth's "Ain't I a Woman" showed the lack of recognition that black women received during this time and whose lack of recognition will continue to be seen long after her time. "Black women, of course, were virtually invisible within the protracted campaign for woman suffrage", wrote Angela Davis, supporting Truth's argument that nobody gives her "any best place"; and not just her, but black women in general.

Over the next 10 years, Truth spoke before dozens, perhaps hundreds, of audiences. From 1851 to 1853, Truth worked with Marius Robinson, the editor of the Ohio "Anti-Slavery Bugle", and traveled around that state speaking. In 1853, she spoke at a suffragist "mob convention" at the Broadway Tabernacle in New York City; that year she also met Harriet Beecher Stowe. In 1856, she traveled to Battle Creek, Michigan, to speak to a group called the "Friends of Human Progress". In 1858, someone interrupted a speech and accused her of being a man; Truth opened her blouse and revealed her breasts.

Northampton Camp Meeting – 1844, Northampton, Massachusetts: At a camp meeting where she was participating as an itinerant preacher, a band of "wild young men" disrupted the camp meeting, refused to leave, and threatened to burn down the tents. Truth caught the sense of fear pervading the worshipers and hid behind a trunk in her tent, thinking that since she was the only black person present, the mob would attack her first. However, she reasoned with herself and resolved to do something: as the noise of the mob increased and a female preacher was "trembling on the preachers' stand", Truth went to a small hill and began to sing "in her most fervid manner, with all the strength of her most powerful voice, the hymn on the resurrection of Christ". Her song, "It was Early in the Morning", gathered the rioters to her and quieted them. They urged her to sing, preach, and pray for their entertainment. After singing songs and preaching for about an hour, Truth bargained with them to leave after one final song. The mob agreed and left the camp meeting.

Abolitionist Convention – 1840s, Boston, Massachusetts: William Lloyd Garrison invited Sojourner Truth to give a speech at an annual antislavery convention. Wendell Phillips was supposed to speak after her, which made her nervous since he was known as such a good orator. So Truth sang a song, "I am Pleading for My people", which was her own original composition sung to the tune of Auld Lang Syne.

Mob Convention – September 7, 1853: At the convention, young men greeted her with "a perfect storm", hissing and groaning. In response, Truth said, "You may hiss as much as you please, but women will get their rights anyway. You can't stop us, neither". Sojourner, like other public speakers, often adapted her speeches to how the audience was responding to her. In her speech, Sojourner speaks out for women's rights. She incorporates religious references in her speech, particularly the story of Esther. She then goes on to say that, just as women in scripture, women today are fighting for their rights. Moreover, Sojourner scolds the crowd for all their hissing and rude behavior, reminding them that God says to "Honor thy father and thy mother".

American Equal Rights Association – May 9–10, 1867: Her speech was addressed to the American Equal Rights Association, and divided into three sessions. Sojourner was received with loud cheers instead of hisses, now that she had a better-formed reputation established. "The Call" had advertised her name as one of the main convention speakers. For the first part of her speech, she spoke mainly about the rights of black women. Sojourner argued that because the push for equal rights had led to black men winning new rights, now was the best time to give black women the rights they deserve too. Throughout her speech she kept stressing that "we should keep things going while things are stirring" and fears that once the fight for colored rights settles down, it would take a long time to warm people back up to the idea of colored women's having equal rights.

In the second sessions of Sojourner's speech, she utilized a story from the Bible to help strengthen her argument for equal rights for women. She ended her argument by accusing men of being self-centered, saying: "Man is so selfish that he has got women's rights and his own too, and yet he won't give women their rights. He keeps them all to himself." For the final session of Sojourner's speech, the center of her attention was mainly on women's right to vote. Sojourner told her audience that she owned her own house, as did other women, and must, therefore, pay taxes. Nevertheless, they were still unable to vote because they were women. Black women who were enslaved were made to do hard manual work, such as building roads. Sojourner argues that if these women were able to perform such tasks, then they should be allowed to vote because surely voting is easier than building roads.

Eighth Anniversary of Negro Freedom – New Year's Day, 1871: On this occasion the Boston papers related that "...seldom is there an occasion of more attraction or greater general interest. Every available space of sitting and standing room was crowded". She starts off her speech by giving a little background about her own life. Sojourner recounts how her mother told her to pray to God that she may have good masters and mistresses. She goes on to retell how her masters were not good to her, about how she was whipped for not understanding English, and how she would question God why he had not made her masters be good to her. Sojourner admits to the audience that she had once hated white people, but she says once she met her final master, Jesus, she was filled with love for everyone. Once enslaved folks were emancipated, she tells the crowd she knew her prayers had been answered.
That last part of Sojourner's speech brings in her main focus. Some freed enslaved people were living on government aid at that time, paid for by taxpayers. Sojourner announces that this is not any better for those colored people than it is for the members of her audience. She then proposes that black people are given their own land. Because a portion of the South's population contained rebels that were unhappy with the abolishment of slavery, that region of the United States was not well suited for colored people. She goes on to suggest that colored people be given land out west to build homes and prosper on.

Second Annual Convention of the American Woman Suffrage Association – Boston, 1871: In a brief speech, Truth argued that women's rights were essential, not only to their own well-being, but "for the benefit of the whole creation, not only the women, but all the men on the face of the earth, for they were the mother of them".

In 1856, Truth bought a neighboring lot in Northampton, but she did not keep the new property for long. On September 3, 1857, she sold all her possessions, new and old, to Daniel Ives and moved to Battle Creek, Michigan, where she rejoined former members of the Millerite movement who had formed the Seventh-day Adventist Church. Antislavery movements had begun early in Michigan and Ohio. Here, she also joined the nucleus of the Michigan abolitionists, the Progressive Friends, some who she had already met at national conventions. From 1857 to 1867 Truth lived in the village of Harmonia, Michigan, a Spiritualist utopia. She then moved into nearby Battle Creek, Michigan, living at her home on 38 College St. until her death in 1883. According to the 1860 census, her household in Harmonia included her daughter, Elizabeth Banks (age 35), and her grandsons James Caldwell (misspelled as "Colvin"; age 16) and Sammy Banks (age 8).

During the Civil War, Truth helped recruit black troops for the Union Army. Her grandson, James Caldwell, enlisted in the 54th Massachusetts Regiment. In 1864, Truth was employed by the National Freedman's Relief Association in Washington, D.C., where she worked diligently to improve conditions for African-Americans. In October of that year, she met President Abraham Lincoln. In 1865, while working at the Freedman's Hospital in Washington, Truth rode in the streetcars to help force their desegregation.

Truth is credited with writing a song, "", for the 1st Michigan Colored Regiment; it was said to be composed during the war and sung by her in Detroit and Washington, D.C. It is sung to the tune of "John Brown's Body" or "The Battle Hymn of the Republic". Although Truth claimed to have written the words, it has been disputed (see "Marching Song of the First Arkansas").

In 1867, Truth moved from Harmonia to Battle Creek. In 1868, she traveled to western New York and visited with Amy Post, and continued traveling all over the East Coast. At a speaking engagement in Florence, Massachusetts, after she had just returned from a very tiring trip, when Truth was called upon to speak she stood up and said, "Children, I have come here like the rest of you, to hear what I have to say."

In 1870, Truth tried to secure land grants from the federal government to former enslaved people, a project she pursued for seven years without success. While in Washington, D.C., she had a meeting with President Ulysses S. Grant in the White House. In 1872, she returned to Battle Creek, became active in Grant's presidential re-election campaign, and even tried to vote on Election Day, but was turned away at the polling place.

Truth spoke about abolition, women's rights, prison reform, and preached to the Michigan Legislature against capital punishment. Not everyone welcomed her preaching and lectures, but she had many friends and staunch support among many influential people at the time, including Amy Post, Parker Pillsbury, Frances Gage, Wendell Phillips, William Lloyd Garrison, Laura Smith Haviland, Lucretia Mott, Ellen G. White, and Susan B. Anthony.

Truth was cared for by two of her daughters in the last years of her life. Several days before Sojourner Truth died, a reporter came from the "Grand Rapids Eagle" to interview her. "Her face was drawn and emaciated and she was apparently suffering great pain. Her eyes were very bright and mind alert although it was difficult for her to talk." 

Truth died early in the morning on November 26, 1883, at her Battle Creek home. On November 28, 1883, her funeral was held at the Congregational-Presbyterian Church officiated by its pastor, the Reverend Reed Stuart. Some of the prominent citizens of Battle Creek acted as pall-bearers; nearly one thousand people attended the service. Truth was buried in the city's Oak Hill Cemetery.

Frederick Douglass offered a eulogy for her in Washington, D.C. "Venerable for age, distinguished for insight into human nature, remarkable for independence and courageous self-assertion, devoted to the welfare of her race, she has been for the last forty years an object of respect and admiration to social reformers everywhere."

There have been many memorials erected in honor of Sojourner Truth, commemorating her life and work. These include memorial plaques, busts, and full-sized statues. 

The first historical marker honoring Truth was established in Battle Creek, Michigan, in 1935, when a stone memorial was placed in Stone History Tower, in Monument Park. In 1976, the State of Michigan further recognized her legacy by naming Interstate 194 in Calhoun County, Michigan, the Sojourner Truth Downtown Parkway.

1999 marked the estimated bicentennial of Sojourner's birth. To honor the occasion, a larger-than-life sculpture of Sojourner Truth by Tina Allen was added to Monument Park in Battle Creek. The 12-foot tall Sojourner monument is cast in bronze. 

In 1981, an Ohio Historical Marker was unveiled on the site of the Universalist "Old Stone" Church in Akron, Ohio, where Sojourner Truth gave her famous "And aren't (ain't) I a woman?" speech on May 29, 1851.

In 1983, a plaque honoring Sojourner Truth was unveiled in front of the historic Ulster County Courthouse in Kingston, New York. The plaque was given by the Sojourner Truth Day Committee to commemorate the Centennial of her death.

In 1998, on the 150th anniversary of the Seneca Falls Women's Rights Convention, a life-sized, terracotta statue of Sojourner Truth by artists A. Lloyd Lillie, Jr. and Victoria "Vicki" Guerina was unveiled at the Women's Rights National Historical Park Visitor's Center. Although Truth did not attend the Seneca Falls Convention, the statue marked Truth's famous speech in Akron, Ohio in 1851, and recognized her important role in the fight for woman suffrage. 

In 2013, a bronze statue of Sojourner Truth as a 11-year-old girl was installed at Port Ewen, New York, where Truth lived for several years while still enslaved. The sculpture by New Paltz, New York sculptor Trina Greene is the only public work of art depicting Truth as a child.

In 2015, the Klyne Esopus Historical Society of Ulster Park, New York, installed a historical marker commemorating Sojourner Truth's walk to freedom in 1826. She walked about 14 miles from Esopus, up what is now Floyd Ackert Road, to Rifton, New York.

In 2020, a statue was unveiled at the Walkway Over the Hudson park in Highland, NY. It was created by Yonkers sculptor Vinnie Bagwell, commissioned by the New York State Women's Suffrage Commision. The statue includes text, braille and symbols. The folds of her skirt act as a canvas to depict Sojourner’s life experiences, including images of a young enslaved mother comforting her child, a slavery sale sign, images of her abolitionist peers, and a poster for a Women’s Suffrage March. 

On August 26, 2020, on the 100th anniversary of the passage of the 19th Amendment to the U.S. Constitution, a statue honoring Sojourner Truth, Elizabeth Cady Stanton and Susan B. Anthony was unveiled in Central Park in New York City. The sculpture, entitled "Women's Rights Pioneers Monument", was created by American artist Meredith Bergmann. It is the first sculpture in Central Park to depict historical women. (A statue to the fictional character Alice in Wonderland is the only other female figure depicted in the park.) Original plans for the memorial included only Stanton and Anthony, but after critics raised objections to the lack of inclusion of women of color, Truth was added to the design.

In 1999, "Sojourner", a Mexican limestone statue of Sojourner Truth by sculptor Elizabeth Catlett, was unveiled in Sacramento, California. It was vandalized in 2020. 

A bronze statue by San Diego sculptor Manuelita Brown was dedicated on January 22, 2015, on the campus of the Thurgood Marshall College of Law, of the University of California, San Diego, California. The artist donated the sculpture to the college. 

In 2002, the Sojourner Truth Memorial statue by Oregon sculptor Thomas "Jay" Warren was installed in Florence, Massachusetts, in a small park located on Pine Street and Park Street, on which she lived for ten years. 

In 2009, a bust of Sojourner Truth was installed in the U.S. Capitol. The bust was sculpted by noted artist Artis Lane. It is in Emancipation Hall of the U.S. Capitol Visitor Center. With this installation, Truth became the first black woman to be honored with a statue in the Capitol building. 

Truth was posthumously inducted into the National Women's Hall of Fame in Seneca Falls, New York, in 1981. She was also inducted to the Michigan Women's Hall of Fame, in Lansing, Michigan. She was part of the inaugural class of inductees when the museum was established in 1983.

The U.S. Postal Service issued a commemorative, 22-cent postage stamp honoring Sojourner Truth in 1986. The original artwork was created by Jerry Pinkney, and features a double portrait of Truth. The stamp was part of the Black Heritage series. The first day of issue was February 4, 1986.

Truth was included in a monument of "Michigan Legal Milestones" erected by the State Bar of Michigan in 1987, honoring her historic court case.

The calendar of saints of the Episcopal Church remembers Sojourner Truth annually, together with Elizabeth Cady Stanton, Amelia Bloomer and Harriet Ross Tubman, on July 20. She is also recognized individually on Nov 26. The calendar of saints of the Lutheran Church remembers Sojourner Truth together with Harriet Tubman on March 10.

In 1997, The NASA Mars Pathfinder mission's robotic rover was named "Sojourner". The following year, S.T. Writes Home appeared on the web offering "Letters to Mom from Sojourner Truth", in which the Mars Pathfinder Rover at times echoes its namesake.

In 2002, Temple University scholar Molefi Kete Asante published a list of 100 Greatest African Americans, which includes Sojourner Truth. 

In 2014, the asteroid 249521 Truth was named in her honor.

Truth was included in the Smithsonian Institution's list of the "100 Most Significant Americans", published 2014.

The U.S. Treasury Department announced in 2016 that an image of Sojourner Truth will appear on the back of a newly designed $10 bill along with Lucretia Mott, Susan B. Anthony, Elizabeth Cady Stanton, Alice Paul and the 1913 Woman Suffrage Procession. Designs for new $5, $10 and $20 bills were originally scheduled to be unveiled in 2020 in conjunction with the 100th anniversary of American women winning the right to vote via the Nineteenth Amendment to the United States Constitution. Treasury Secretary Steve Mnuchin announced that plans for the $20 redesign, which was to feature Harriet Tubman, have been postponed.

On September 19, 2018, the U.S. Secretary of the Navy Ray Mabus announced the name of the last ship of a six unit construction contract as USNS "Sojourner Truth" (T-AO 210). This ship with be part of the latest "John Lewis"-class of Fleet Replenishment Oilers named in honor of U.S. civil and human rights heroes currently under construction at General Dynamics NASSCO in San Diego, CA.

A Google Doodle was featured on February 1, 2019, in honor of Sojourner Truth. The doodle was showcased in Canada, United States, United Kingdom, Switzerland, Israel, Ireland and Germany.

For their first match of March 2019, the women of the United States women's national soccer team each wore a jersey with the name of a woman they were honoring on the back; Christen Press chose the name of Sojourner Truth.

In 1862, American sculptor William Wetmore Story completed a marble statue, inspired by Sojourner Truth, named "The Libyan Sibyl". The work won an award at the London World Exhibition. The original sculpture was gifted to the Metropolitan Museum of Art, in New York City, by the Erving Wolf Foundation in 1978.

In 1892, Albion artist Frank Courter was commissioned by Frances Titus to paint the meeting between Truth and President Abraham Lincoln that occurred on October 29, 1864.

In 1945, Elizabeth Catlett created a print entitled "I'm Sojourner Truth" as part of a series honoring the labor of black women. The print is in the Metropolitan Museum of Art's collection. She would later create a full-size statue of Truth, which was displayed in Sacramento, California.

In 1958, African-American artist John Biggers created a mural called The "Contribution of Negro Woman to American Life and Education" as his doctoral dissertation. It was unveiled at the Blue Triangle Community Center (former YWCA) - Houston, Texas and features Sojourner Truth, Harriet Tubman, and Phillis Wheatley.

Inspired by the work of pioneer women's historian Gerda Lerner, feminist artist Judy Chicago (Judith Sylvia Cohen) creates a collaborative masterpiece – "The Dinner Party", a mixed-media art installation, between the years 1974 and 1979. The Sojourner Truth placesetting is one of 39. "The Dinner Party" is gifted by the Elizabeth Sackler Foundation to the Elizabeth A. Sackler Center for Feminist Art, Brooklyn Museum – New York in 2000.

Feminist theorist and author bell hooks titled her first major work after Truth's "Ain't I a Woman?" speech. The book was published in 1981.

New York Governor Mario Cuomo presented a two-foot statue of Sojourner Truth, made by New York sculptor Ruth Inge Hardison, to Nelson Mandela during his visit to New York City, in 1990.

African-American Composer Gary Powell Nash composes "In Memoriam: Sojourner Truth," in 1992"."

The Broadway musical "The Civil War", which premiered in 1999, includes an abridged version of Truth's "Ain't I a Woman?" speech as a spoken-word segment. On the 1999 cast recording, the track was performed by Maya Angelou.

In 2018, a crocheted mural, "Sojourner Truth: Ain't I A Woman?", was hung on display at the Akron Civic Theatre's outer wall at Lock 3 Park in Ohio. It was one of four projects in New York and North Carolina as part of the "Love Across the U.S.A.", spearheaded by fiber artist OLEK.

Before her death, Dr. Faye Hersh Dambrot commissioned notable artist Woodrow Nash to create a prototype of a Sojourner Truth sculpture. The sculpture is expected to be completed and installed in Akron, Ohio, by 2021.








</doc>
<doc id="29306" url="https://en.wikipedia.org/wiki?curid=29306" title="STOVL">
STOVL

A short take-off and vertical landing aircraft (STOVL aircraft) is a fixed-wing aircraft that is able to take off from a short runway (or take off vertically if it does not have a heavy payload) and land vertically (i.e. with no runway). The formal NATO definition (since 1991) is:

On aircraft carriers, non-catapult-assisted, fixed-wing short takeoffs are accomplished with the use of thrust vectoring, which may also be used in conjunction with a runway "ski-jump". Use of STOVL tends to allow aircraft to carry a larger payload compared to vertical take-off and landing (VTOL), while still only requiring a short runway. The most famous examples are the Hawker Siddeley Harrier and the Sea Harrier. Although technically VTOL aircraft, they are operationally STOVL aircraft due to the extra weight carried at take-off for fuel and armaments. The same is true of the F-35B Lightning II, which demonstrated VTOL capability in test flights but is operationally a STOVL.

In 1951, the Lockheed XFV and the Convair XFY Pogo tailsitters were both designed around the Allison YT40 turboprop engine driving contra-rotating propellers.

The British Hawker P.1127 took off vertically in 1960, and demonstrated conventional take-off in 1961. It was developed into the Hawker Siddeley Harrier which flew in 1967.

In 1962, Lockheed built the XV-4 Hummingbird for the U.S. Army. It sought to "augment" available thrust by injecting the engine exhaust into an ejector pump in the fuselage. First flying vertically in 1963, it suffered a fatal crash in 1964. It was converted into the XV-4B Hummingbird for the U.S. Air Force as a testbed for separate, vertically mounted lift engines, similar to those used in the Yak-38 Forger. That plane flew and later crashed in 1969. The Ryan XV-5 Vertifan, which was also built for the U.S. Army at the same time as the Hummingbird, experimented with gas-driven lift fans. That plane used fans in the nose and each wing, covered by doors which resembled half garbage can lids when raised. However, it crashed twice, and proved to generate a disappointing amount of lift, and was difficult to transition to horizontal flight.

Of dozens of VTOL and V/STOL designs tried from the 1950s to 1980s, only the subsonic Hawker Siddeley Harrier and Yak-38 Forger reached operational status, with the Forger being withdrawn after the fall of the Soviet Union.

Rockwell International built, and then abandoned, the Rockwell XFV-12 supersonic fighter which had an unusual wing which opened up like window blinds to create an ejector pump for vertical flight. It never generated enough lift to get off the ground despite developing 20,000 lbf of thrust. The French had a nominally Mach 2 Dassault Mirage IIIV fitted with no less than 8 lift engines that flew (and crashed), but did not have enough space for fuel or payload for combat missions. The German EWR VJ 101 used swiveling engines mounted on the wingtips with fuselage mounted lift engines, and the VJ 101C X1 reached supersonic flight (Mach 1.08) on 29 July 1964. The supersonic Hawker Siddeley P.1154, which competed with the Mirage IIIV for use in NATO, was cancelled even as the aircraft were being built.

NASA uses the abbreviation SSTOVL for Supersonic Short Take-Off / Vertical Landing, and as of 2012, the X-35B/F-35B are the only aircraft to conform with this combination within one flight.

The experimental Mach 1.7 Yakovlev Yak-141 did not find an operational customer, but similar rotating rear nozzle technology is used on the F-35B. The F-35B Lightning II entered service on July 31, 2015. 

Larger STOVL designs were considered, the Armstrong Whitworth AW.681 cargo aircraft was under development when cancelled in 1965. The Dornier Do 31 got as far as three experimental aircraft before cancellation in 1970.

Although mostly a VTOL design, the V-22 Osprey has increased payload when taking off from a short runway.


</doc>
<doc id="29307" url="https://en.wikipedia.org/wiki?curid=29307" title="Russian aircraft carrier Admiral Kuznetsov">
Russian aircraft carrier Admiral Kuznetsov

Admiral Flota Sovetskogo Soyuza Kuznetsov ( "Admiral of the Fleet of the Soviet Union Kuznetsov", originally the name of the fifth ) is an aircraft carrier (heavy aircraft cruiser in Russian classification) serving as the flagship of the Russian Navy. It was built by the Black Sea Shipyard, the sole manufacturer of Soviet aircraft carriers, in Nikolayev within the Ukrainian Soviet Socialist Republic (SSR). The initial name of the ship was Riga; it was launched as Leonid Brezhnev, embarked on sea trials as Tbilisi, and finally named "Admiral Flota Sovetskogo Soyuza Kuznetsov" after Admiral of the fleet of the Soviet Union Nikolay Gerasimovich Kuznetsov.

It was originally commissioned in the Soviet Navy, and was intended to be the lead ship of the two-ship . However, its sister ship "Varyag" was still incomplete when the Soviet Union disbanded in 1991. The second hull was eventually sold by Ukraine to China, completed in Dalian and commissioned as .

The design of "Admiral Kuznetsov" class implies a mission different from that of the United States Navy's carriers. The term used by her builders to describe the Russian ships is (TAVKR) – "heavy aircraft-carrying cruiser" – intended to support and defend strategic missile-carrying submarines, surface ships, and naval missile-carrying aircraft of the Russian Navy.

"Admiral Kuznetsov"s main fixed-wing aircraft is the multi-role Sukhoi Su-33. It can perform air superiority, fleet defence, and air support missions and can also be used for direct fire support of amphibious assault, reconnaissance and placement of naval mines. The carrier also carries the Kamov Ka-27 and Kamov Ka-27S helicopters for anti-submarine warfare, search and rescue, and small transport.

For take-off of fixed wing aircraft, "Admiral Kuznetsov" uses a ski-jump at the end of her bow. On take-off aircraft accelerate toward and up the ski-jump using their afterburners. This results in the aircraft leaving the deck at a higher angle and elevation than on an aircraft carrier with a flat deck and catapults. The ski-jump take-off is less demanding on the pilot, since the acceleration is lower, but results in a clearance speed of only requiring an aircraft design which will not stall at those speeds. The "cruiser" role is facilitated by "Admiral Kuznetsov"s complement of 12 long-range surface-to-surface anti-ship P-700 Granit (NATO reporting name: Shipwreck) cruise missiles. As a result, this armament is the basis for the ship's Russian type designator of "heavy aircraft-carrying missile cruiser".

Unlike most western naval ships that use gas turbines or nuclear power, the "Admiral Kuznetsov" is a conventionally powered ship that uses mazut as a fuel, often leading to a visible trail of heavy black smoke that can be seen at a great distance. Russian naval officials have said that the failure to properly preheat the heavy mazut fuel prior to entering the combustion chamber may contribute to the heavy smoke trail associated with the ship.

"Admiral Kuznetsov"s designation as an aircraft-carrying cruiser is very important under the Montreux Convention, as it allows the ship to transit the Turkish Straits. The Convention prohibits countries from sending an aircraft carrier heavier than 15,000 tons through the Straits. Since the ship was built in the Ukrainian SSR, "Admiral Kuznetsov" would have been stuck in the Black Sea if Turkey had refused permission to pass into the Mediterranean Sea. However, the Convention does not limit the displacement of capital ships operated by Black Sea powers. Turkey allowed "Admiral Kuznetsov" to transit the Straits, and no signatory to the Montreux Convention ever issued a formal protest of her classification as an aircraft-carrying cruiser.

"Admiral Flota Sovetskogo Soyuza Kuznetsov", constructed at Chernomorskiy Shipyard, also known as Nikolayev South Shipyard, in Nikolayev, now Mykolaiv, Ukrainian SSR, was launched in 1985, and became fully operational in 1995. An official ceremony marking the start of construction took place on 1 September 1982; in fact she was laid down in 1983. The vessel was first named "Riga", then the name was changed to "Leonid Brezhnev", this was followed by "Tbilisi". Finally, on 4 October 1990, she was renamed "Admiral Flota Sovetskovo Soyuza N.G. Kuznetsov", referred to in short as "Admiral Kuznetsov". The ship was 71% complete by mid-1989. In November 1989 she undertook her first aircraft operation trials. In December 1991, she sailed from the Black Sea to join the Northern Fleet. Only from 1993 on did she receive aircraft.

From 23 December 1995 through 22 March 1996 "Admiral Kuznetsov" made her first 90-day Mediterranean deployment with 13 Su-33, 2 Su-25 UTG, and 11 helicopters aboard. The deployment of the Russian Navy's flagship was undertaken to mark the 300th anniversary of the establishment of the Russian Navy in October 1696. The deployment was to allow the carrier, which was accompanied by a frigate, destroyer and oiler, to adapt to the Mediterranean climate and to perform continuous flight operations until 21:00 each day, as the Barents Sea only receives about one hour of sunlight during this time of year. During that period the carrier lay at anchor off the port of Tartus, Syria. Her aircraft often made flights close to the Israeli shore line and were escorted by Israeli F-16s. During the deployment, a severe water shortage occurred due to evaporators breaking down.

At the end of 1997 she remained immobilized in a Northern Fleet shipyard, awaiting funding for major repairs, which were halted when they were only 20% complete. The overhaul was completed in July 1998, and the ship returned to active service in the Northern fleet on 3 November 1998.

"Admiral Kuznetsov" remained in port for two years before preparing for another Mediterranean deployment scheduled for the winter of 2000–2001. This deployment was cancelled due to the explosion and sinking of the nuclear-powered submarine . "Admiral Kuznetsov" participated in the "Kursk" rescue and salvage operations in late 2000. Plans for further operations were postponed or cancelled. In late 2003 and early 2004, "Admiral Kuznetsov" went to sea for inspection and trials. In October 2004, she participated in a fleet exercise of the Russian Navy in the Atlantic Ocean. During a September 2005 exercise, a Su-33 accidentally fell from the carrier into the Atlantic Ocean. On 27 September 2006, it was announced that "Admiral Kuznetsov" would return to service in the Northern Fleet by the year's end, following another modernization to correct some technical issues. Admiral Vladimir Masorin, Commander-in-Chief of the Russian Navy, also stated that Su-33 fighters assigned to her would return after undergoing their own maintenance and refits.

From 5 December 2007 through 3 February 2008 "Admiral Kuznetsov" made its second Mediterranean deployment. On 11 December 2007, "Admiral Kuznetsov" passed by Norwegian oil platforms in the North Sea, outside Bergen, Norway. Su-33 fighters and Kamov helicopters were launched from "Admiral Kuznetsov" while within international waters; Norwegian helicopter services to the rigs were halted due to the collision risk with the Russian aircraft. "Admiral Kuznetsov" later participated in an exercise on the Mediterranean Sea, together with 11 other Russian surface ships and 47 aircraft, performing three tactical training missions using live and simulated air and surface missile launches. "Admiral Kuznetsov" and her escorts returned to Severomorsk on 3 February 2008. Following maintenance, she returned to sea on 11 October 2008 for the Stability-2008 strategic exercises held in the Barents Sea. On 12 October 2008, Russian President Dmitry Medvedev visited the ship during the exercise.

From 5 December 2008 through 2 March 2009, "Admiral Kuznetsov" made her third Mediterranean deployment. On 5 December 2008, she and several other vessels left Severomorsk for the Atlantic for a combat training tour, including joint drills with Russia's Black Sea Fleet and visits to several Mediterranean ports. On 7 January 2009, a small fire broke out onboard "Admiral Kuznetsov" while anchored off Turkey. The fire, caused by a short-circuit, led to the death of one crew member by carbon monoxide poisoning. On 16 February 2009, she was involved in a large oil spill, along with other Russian naval vessels, while refuelling off the south coast of Ireland. On 2 March 2009, "Admiral Kuznetsov" returned to Severomorsk, and on September 2010 she left dry dock after scheduled repairs and preparations for a training mission in the Barents Sea, later that month.

The Russian Main Navy Staff announced that "Admiral Kuznetsov" would begin a deployment to the Atlantic and Mediterranean in December 2011. In November 2011, it was announced that "Admiral Kuznetsov" would lead a squadron to Russia's naval facility in Tartus.

A Russian naval spokesman announced via the "Izvestia" daily that "The call of the Russian ships in Tartus should not be seen as a gesture towards what is going on in Syria... This was planned already in 2010 when there were no such events there" noting that "Admiral Kuznetsov" would also be making port calls in Beirut, Genoa and Cyprus. On 29 November 2011, Army General Nikolay Makarov, Chief of the Russian General Staff, said that Russian ships in the Mediterranean were due to exercises rather than events in Syria, and noted that "Admiral Kuznetsov"s size does not allow her to moor in Tartus.
On 6 December 2011, "Admiral Kuznetsov" and her escort ships departed the Northern Fleet homebase for a Mediterranean deployment to exercise with ships from the Russian Baltic and Black Sea Fleets. On 12 December 2011, "Admiral Kuznetsov" and her escorts, were spotted northeast of Orkney off the coast of northern Scotland, the first such time she had deployed near the UK. shadowed the group for a week; due to severe weather, the group took shelter in international waters in the Moray Firth, some from the UK coast. "Admiral Kuznetsov" then sailed around the top of Scotland and into the Atlantic past western Ireland, where she conducted flight operations with her Sukhoi Su-33 'Flanker' jets and Kamov Ka-27 helicopters in international airspace. On 8 January 2012, "Admiral Kuznetsov" anchored near shore outside Tartus while other ships from her escort entered the port to use the leased Russian naval support facility to replenish their supplies, after which all ships continued their deployment on 9 January. On 17 February 2012, "Admiral Kuznetsov" returned to her homebase of Severomorsk.

On 1 June 2013, it was announced that the ship would return to the Mediterranean by the end of the year, and on 17 December, "Admiral Kuznetsov" departed her homebase for the Mediterranean.
On 1 January 2014, "Admiral Kuznetsov" celebrated New Year's Day while at anchor in international waters of the Moray Firth off northeast Scotland. The anchorage allowed replenishment of ship's supplies and respite for the crew from stormy weather off the southwest coast of Norway. She then proceeded to the Mediterranean Sea, docking in Cyprus on 28 February. In May 2014, the ship and her task group: the "Kirov"-class nuclear-powered cruiser "Petr Velikiy"; tankers; "Sergey Osipov", "Kama" and "Dubna"; ocean-going tug "Altay" and Ropucha-class landing ship "Minsk" (a part of the Black Sea Fleet), passed the UK while sailing for home. Despite financial and technical problems, resulting in limited operations for the ship, it is expected that "Admiral Kuznetsov" will remain in active service until at least 2030.

In April 2010, it was announced that by late 2012, the ship would enter Severodvinsk Sevmash shipyard for a major refit and modernization, including upgrades to obsolete electronics and sensor equipment, installation of a new anti-aircraft system (Pantsir-M) and an increase of the air wing with the removal of the P-700 Granit anti-ship missiles. Possible upgrades include exchanging the troublesome steam powerplant to gas-turbine, or even nuclear propulsion, and installation of catapults to the angled deck.

The Navy expected to acquire Mikoyan MiG-29K aircraft for "Admiral Kuznetsov" by 2011; this later was confirmed by a defense sub-contractor The MiG-29Ks would replace the 19 carrier-based Su-33 fighters, a resource set to expire by 2015. Producing more Su-33s is possible but not cost-effective for such small volumes; the MiG-29K is more convenient as the Indian Navy also placed an order for a total for 45, reducing development and manufacture costs. India paid $730 million for the development and delivery of 16 MiG-29Ks; 24 more for the Russian Navy would cost about $1 billion.

Following ongoing maintenance, "Admiral Kuznetsov" set sail on 15 October 2016 from the Kola Bay for the Mediterranean, accompanied by seven other Russian Navy vessels including the nuclear-powered battlecruiser "Pyotr Velikiy" and two Udaloy-class destroyers. The carrier was accompanied by an ocean-going tugboat, as a precaution due to potential propulsion failure. The airwing included 6-8 Su-33 fighters, four Mig-29KR/KUBR multi-role aircraft, Ka-52K "Katran" navalised attack helicopters, Ka-31R "Helix" AEW&C helicopters and Ka-27PS "Helix-D" search and rescue helicopters. All the Su-33 aircraft had been upgraded with the Gefest SVP-24 bombsights for free-fall bombs, giving them a limited ground-attack capability. Analysts, including Mikhail Barabanov of the Moscow Defense Brief, suggested that a lack of trained pilots restricted the number of fixed-wing aircraft that could be deployed from the carrier.

On 21 October, the "Admiral Kuznetsov" battle group sailed through the English Channel, escorted by Royal Navy ships, while UK Defence Minister Michael Fallon speculated that the taskforce was designed to "test" the British naval response. On 26 October 2016, the ship was reported to have passed through the Strait of Gibraltar and refuelled at sea off North Africa the following day. On 3 November 2016, the "Admiral Kuznetsov" battle group paused off the east coast of Crete. On 14 November 2016, a MiG-29K crashed into the sea after taking off from the carrier. The pilot ejected safely from the plane and was rescued by helicopter. According to initial reports from Russian officials, the crash was a result of technical malfunction, but it was later revealed that the plane had actually run out of fuel waiting to land while the crew was attempting to repair a broken arresting wire. The carrier commander could have diverted the aircraft to land at a nearby airbase, but hesitated in the hope that the arrestor gear would be repaired in time.

On 15 November 2016, "Admiral Kuznetsov", took part in "a large-scale operation against the positions of terrorist groups Islamic State and Al-Nusra, in the provinces of Idlib and Homs" in Syria by launching Su-33 fighter strikes. This was the first time a Russian aircraft carrier would take part in combat operations. Russian Defence Ministry later reported that at least 30 militants had been killed as a result of those strikes, including 3 field commanders, among them" "Abul Baha al-Asfari, leader of Al-Nusra reserves in the provinces of Homs and Aleppo. Al-Asfari had also planned and led several insurgent attacks on the city of Aleppo itself. The Su-33s reportedly used precision bombs. On 3 December 2016, an Su-33 crashed into the sea after attempting to land on the carrier. The plane crashed on its second attempt to land on the aircraft carrier in good weather conditions. The pilot was safely recovered by a search and rescue helicopter. Initially it was suspected that the plane missed the wires and failed to go around, falling short of the bow of the warship, but later it was revealed that the arresting cable failed to hold the aircraft, and was damaged in the attempt. Following the two incidents, the air wing was transferred to shore at Khmeimim Air Base near Latakia, Syria to continue military operations while the carrier's arresting gear issues were addressed.

In early January 2017, it was announced that "Admiral Kuznetsov" and her battlegroup would be ceasing operations in Syria and returning to Russia as part of a scaling back of Russian involvement in the conflict. During her deployment off Syria, aircraft from "Admiral Kuznetsov" carried out 420 combat missions, hitting 1,252 hostile targets. On 11 January 2017, "Admiral Kuznetsov" was conducting live-fire training exercises in the Mediterranean off the coast of Libya. The Russian defence ministry announced that on 11 January, "Admiral Kuznetsov" was visited by Libya′s military leader Khalifa Haftar, who had a video conference with Russian defence minister Sergey Shoygu while on board.

On 20 January, "Admiral Kuznetsov" was sighted passing west through the Strait of Gibraltar and six days later she was escorted back along the English Channel by three Eurofighter Typhoons of the Royal Air Force and the Type 23 frigate . She arrived back in Severomorsk on 9 February. On 23 February 2017, President Vladimir Putin said that the ship′s deployment to the Mediterranean had been his personal initiative.

The carrier started an overhaul and modernisation in the first quarter of 2017. This is expected to extend its service life by 25 years. "Admiral Kuznetsov" is expected to undergo modernization at the 35th Ship Repair Plant in Murmansk between 2020 and 2021, upgrading the ship's power plant and electronics systems.

On 30 October 2018, "Admiral Kuznetsov" was damaged when Russia's biggest floating dry dock, "PD-50", sank and one of the dock's 70-ton cranes crashed onto the ship's flight deck leaving behind a hole in the flight deck. One person was reported missing and four injured as the dry dock sank in Kola Bay. "Admiral Kuznetsov" was in the process of being removed from the dock when the incident happened, and was towed to a nearby yard after the incident. According to Alexei Rakhmanov, the president of the United Shipbuilding Corporation, the cost for repairs of the damage was estimated to be RUB70 million (about US$1 million) and should not affect the timing of the currently underway overhaul and modernization of the ship. Although it is unclear how the overhaul and repair schedule would not be affected with the dry dock sunk.

The fallen crane was removed within two to three months. In late May 2019, seven months later, information posted on Digital Forensic Research Lab's blog suggested that repair work of the aircraft carrier was underway. That same month it was also announced that two graving docks in Roslyakovo, Murmansk Oblast would be merged and enlarged to accommodate "Admiral Kuznetsov", with work taking 1.5 years.

In December 2019, a major fire broke out on board "Admiral Kuznetsov" as work continued on the ship's refit. Two people died and fourteen suffered injuries from the fire and smoke inhalation. The fire damage aboard "Admiral Kuznetsov" is estimated at 500 million rubles. The ship was not expected to return to active operations until at least 2022/2023.




</doc>
<doc id="29311" url="https://en.wikipedia.org/wiki?curid=29311" title="Subaru Forester">
Subaru Forester

The Subaru Forester (Japanese: スバル•フォレスター "Subaru Foresutā") is a compact crossover SUV (sport utility vehicle) that's been manufactured since 1997 by Subaru. Available in Japan from 1997, the Forester shares its platform with the Impreza.

The Forester was introduced at the Tokyo Motor Show in November 1995 as the Streega concept, and made available for sale in February 1997 in Japan, and to the US market in 1997 for MY1998. The Forester was one of the first emerging crossover SUVs. It was built in the style of a car, but had a taller stance, higher h-point seating, and an all-wheel drive drivetrain. Subaru advertising employed the slogan "SUV tough, Car Easy". It used the Impreza platform but with the larger 2.5-liter DOHC EJ25D four-cylinder boxer engine from the Subaru Outback, making at 5,600 rpm and of torque at 4,000 rpm.

In Japan, the Forester replaced the Subaru Impreza Gravel Express, known in the US as the Subaru Outback Sport. However, the Outback Sport remained in production for the U.S. market. The Forester appeared after the introduction of the Nissan Rasheen in Japan with a similar appearance, and the Forester's Japanese competitors include the Toyota RAV4, Mitsubishi RVR, and the Suzuki Grand Vitara. Due to the Forester's low center of gravity, it meets the United States federal safety standards for passenger vehicles, and does not require a "risk of rollover" warning label on the driver's visor. Size and price-wise, it fits between the shared Impreza platform, and the larger Legacy.

The automatic transmissions used on AWD equipped vehicles will normally send 60% of the engine's torque to the front wheels and 40% to the rear wheels, using a computer-controlled, continuously variable, multi-plate transfer clutch. When the transmission detects a speed difference between the front and rear axle sets, the transmission progressively sends power to the rear wheels. Under slip conditions it can achieve an equal split in front and rear axle speeds.

When accelerating or driving uphill, the vehicle's weight shifts rearward, reducing front wheel traction, causing the transmission to automatically send torque to the rear wheels to compensate. When braking or driving downhill, the vehicle's weight shifts towards the front, reducing rear wheel traction. The transmission again compensates by sending torque to the front wheels for better steering control and braking performance. If the automatic is placed in reverse or first gear, the transmission divides the torque 50/50 to both front and rear wheels. The manual transmission cars are set up with a near 50/50 torque split as a base setting, and it varies from there. Essentially, the manual cars are set up with more bias towards the rear than the automatic cars.

There was a change in body styling for all 2001–2002 models, and the 2001/2002 GT spec also had a change in engine management and power output was increased from 125 to .

The U.S. market was offered the car starting in 1997 with either the 2.5-liter DOHC (MY1998 only) or 2.5-liter SOHC naturally aspirated engine (no turbocharged engines). In 2000 Subaru updated the exterior with a modest facelift to the front, rear and sides, and the interior's dashboard MY2001.

The trim levels were the basic model "L" and the fully equipped "S" for the USA versions.

Forester L came with a high level of standard equipment, including ABS, air conditioning, power windows, power locks, cruise control, digital temperature gauge, multi-reflector halogen headlights, fog lights, roof rack, rear window defogger, trailer harness connector, reclining front bucket seats with adjustable lumbar support, tilt steering, tinted glass, AM/FM/cassette stereo with its antenna laminated in the left-rear quarter window. Notably new in 2001 were the three-point seatbelts for all five seating positions, including force limiters in front and height-adjustable shoulder belt anchors for front and rear outboard positions, plus rear seat headrests for all three seating positions.

Forester S adds a viscous limited-slip differential, rear disc brakes, 16 × 6.5-inch alloy wheels with 215/60R16 tires (the L uses 15 × 6-inch steel wheels), upgraded moquette upholstery, heated front seats with net storage pockets in back, dual vanity mirrors, heated sideview mirrors, heated windshield wipers, and keyless entry. New equipment for 2001 included Titanium pearl paint for the bumpers and cladding; six-disc in-dash CD sound system; leather-wrapped steering wheel, shift knob and handbrake handle; variable intermittent wipers with de-icers and driver's side fin; and the five-spoke alloy wheels. Some models were equipped with the $1,000 optional premium package on the Forester S, including monotone paint (Sedona Red Pearl), power moonroof, front side-impact airbags, and gold accent wheels. Other options were the $800 automatic transmission, $39 chrome tailpipe cover and $183 auto-dimming rear-view mirror with compass, bringing the sticker price to $25,412 including $495 delivery (U.S. dollars quoted).

The second generation was introduced as a 2003 model at the 2002 Chicago Auto Show, based on the new Impreza platform, featuring several fine-tune improvements over the past model. The 2003 Forester features weight-saving refinements such as an aluminum hood, perforated rails, and a hydro-formed front sub-frame. The most noticeable change was the offering of 2.5 L versions (normally aspirated and turbocharged) and in the U.S. the introduction of the turbo charged 2.5-liter model.

In the United States, the naturally aspirated X (previously L) and XS (previously S) were released in 2003. In 2004, the turbocharged XT trim was released. However, the same model had been available since the late 1990s elsewhere in the world. The X and XS models feature a 2.5 L SOHC engine, while the XT model features a 2.5 L turbocharged DOHC engine. Both engines have timing belt (camshaft). The XT model uses the same Mitsubishi Motors TD04 turbocharger used in the Subaru Impreza WRX. All Forester 2.5 L engines are of the interference engine type.

In 2004, Subaru launched the Forester STI for the Japanese Market sharing the same engine as the 2005 Subaru Impreza WRX STI. Starting with the 2004 XT, the turbocharged version had Active valve control system cylinder heads. The i-AVLS (active valve lift system) became standard on the naturally aspirated version of the Forester in 2006. This increased horsepower and torque figures to 173 HP and 166 ft-lbs. The 2006 XT received a higher compression ratio to 8.4:1 from 8.2:1. This increased the XT's power to 230 HP and 235 ft-lbs.

For the 2006 model year, Subaru gave the SG a facelift, using redesigned headlights, tail-lights, bonnet, grille, front bumper and side-moldings.

MY03-04 Models has a 4-Star ANCAP safety rating. MY05 Forester Model had a mid-life update, which increased its ANCAP safety rating to 5 Stars.

In 2006, the turbocharged engine (powering the Forester XT) was awarded International Engine of the Year. This engine is also used in the Subaru Impreza WRX, as well as the re-badged Saab 9-2XAero.

All of the 2.5-liter 4-cylinder engines for this generation have a timing belt made of rubber and cord. A belt must be replaced at . These engines are interference engines, meaning that if the timing belt breaks or stretches, the pistons will hit the valves, requiring an engine teardown, and a likely rebuild. Also, if this belt is replaced around 105,000 miles, it is a good idea to change the water pump, thermostat, belt tensioner and all the idler pulleys for this belt. The water pump and thermostat are behind this belt.
In Australia for the Series II (MY06) cars, Subaru changed the recommended service interval for the timing belt replacement from 100,000 kilometers to 125,000 kilometers.
The 2.5-liter 4-cylinder engine in the first-generation Foresters featured head gaskets which were prone to premature failure. For 2003 and later, this problem was addressed with a revised, higher performing design, but is still a problem.

The U.S. Market was offered the car with either the 2.5 SOHC naturally aspirated engine, or the 2.5 DOHC turbocharged version
added in 2004. In 2005, the L.L. Bean edition is added. In 2006, styling is updated, Active valve lift system is added to non-turbo engines to improve power and efficiency, XS model deleted, and Premium model added. In 2007, a bottle holder was added to front door panels, the 'Sports' trim level was added, which changed some interior and exterior features and added the VDT/VDC transmission to the XT Sports turbo Automatic model: In 2008, TPMS was added, L.L. Bean model deletes rear load-leveling suspension, but gains radio upgrade, the XT Turbo Limited models gets the VDT/VDC Auto transmission as well.

The Forester had three main models available in Australia until July 2005:

The Forester at the time had three main models available in Australia from August 2005 Series II:

The Luxury Pack edition was an option on all models - allowing for leather seats and a sunroof. These options were also included with the Columbia edition. The Weekender edition included fog lights, roof racks and alloy wheels.
Standard with the Manufacture Year 2006 (MY06) Forester came with larger side mirrors with indicator lights, curtain airbags giving a 5 star safety rating, remodelled centre console and exterior with a new look nose, lights and bumpers and the rear lost the large Subaru badge under the rear window.

The Forester was sold in India as a Chevrolet alongside other unique Chevrolet models sold there. However, since General Motors no longer holds an ownership stake in Subaru's parent company, Fuji Heavy Industries, sales in India of the Chevrolet-badged Forester have ended.

A look-alike was produced by Yema and known as the Yema F99 in China. It was a similar design to the pre-facelifted model. Production ran from 2012 to 2014. The engine was a 1.5l 4 cylinder mated to a 5 speed manual gearbox. The car was not related to the Forester even though they look very similar.

Despite the existence of counterfeiting in China, the second generation of Subaru Forester can still be purchased by Chinese consumers back from 2004 to 2007. 

2004 Version sold in China:

2006 Version (Facelift) sold in China:

2007 Version (Facelift) sold in China:

The third generation Forester began to move away from a traditional wagon design towards becoming a crossover SUV. It was larger in nearly every dimension and featured a sloping roof line with more cargo space. Subaru unveiled the model year 2008 Forester in Japan on December 25, 2007. The North American version made its debut at the 2008 North American International Auto Show in Detroit.

Styling was by Subaru Chief Designer Mamoru Ishii. The dimensions derive from engineers using the basic body structure of the Japanese-spec Impreza wagon with the rear platform of the U.S.-spec Impreza sedan. The Forester's wheelbase was increased , with overall increases of in length, in width and in height.

The independent double wishbone rear suspension was redesigned for better handling and a smoother ride. A "Sportshift" mode was added to the four-speed computer-controlled automatic transmission. The in-dash, touch-screen satellite navigation system became Bluetooth compatible, and integrated with a premium stereo. A six-speaker surround sound enhancement was optional.

The new model added to the Forester's wheelbase, improving interior space and cargo room ( expandable to ). Ground clearance was .

The Forester was available in Europe from 2008 with either the 2.0-liter EJ20 () 196 Nm petrol engine with Active Valve Control System (AVCS) matched to either five-speed manual or four-speed automatic gearbox, or an all-new diesel-powered horizontally opposed Subaru EE boxer engine, and six-speed manual gearbox. The new model was introduced at the 2008 Paris Motor Show in October. The diesel engine produces a power output of and 350 Nm.

In the UK, the petrol-powered Forester was offered in the popular X and XS models, while trim level for the diesel models were X, XC, and XS NavPlus.

In Russia, Belarus and Ukraine 2.5 and 2.5 Turbo engines were also available.

In Netherlands the Forester is offered with petrol or diesel engines. The petrol engine can also be fitted with an additional liquefied petroleum gas installation (LPG), usually an aftermarket installation provided directly through dealerships. The available equipment levels are Intro (petrol engine only), Comfort, Luxury, and Premium. Maximum towing abilities for the petrol or petrol with LPG are 2000 kg (manual) or 1500 kg (auto), while the manual-only diesel can tow 2000 kg.

There were seven specifications with various trim and performance levels:
Of note is a serious head gasket issue that remains unresolved by Subaru. Mostly oil and or coolant leaks / cross cylinder failure being the worst.. Can be resolved by aftermarket gaskets, however, this is an expensive engine out job.

Summary of standard trim and equipment over different Australian models.

The Forester trim levels were the 2.5X, the 2.5X Premium, the 2.5X Limited and the 2.5XT and 2.5XT Limited both with turbo. The interior color was either black or light gray, with three upholstery selections, including leather. Nine exterior colors were offered, with four colors in a pearlescent appearance.

Starting July 2009, Subaru no longer offered a special-edition L.L. Bean trim level on the Forester.

The USA 2.5X model was certified PZEV emissions (Rated instead ), with a badge attached to the rear of the vehicle on the bottom right-hand side of the tailgate. All other USA models were certified LEV2. The PZEV Forester was available for sale in all fifty states, unlike other manufacturers who only sold PZEV-certified vehicles in states that had adopted California emission standards. The engine without the turbo runs on unleaded gasoline rated at 87 octane, and the turbo engine (EJ255) requires premium fuel rated minimum 91 octane.

Safety equipment included front airbags with side curtain airbags and front passenger side airbags (for a total of six airbags) and brake assist that detects panic-braking situations and applies maximum braking force more quickly. The five-speed manual transmission was equipped with Incline Start Assist.

Some of the standard equipment found on the 2.5X included Subaru's VDC (Vehicle Dynamics Control), 16 inch steel wheels, and an auxiliary audio jack for MP3 players. Optional equipment included 17 inch alloy wheels, panoramic moonroof, heated front seats and heated side-view mirrors. The L.L. Bean edition added automatic climate control, leather upholstery, an upgraded stereo with six speakers and a six disc in-dash CD changer over the four-speaker stereo with single disc CD player, and an in-dash navigation system, as well as L.L. Bean signature floor mats and rear cargo tray.

The 2.5 XT came with the premium stereo standard, as well as 17-inch alloy wheels, and the panoramic moonroof. The 2.5 XT Limited added leather upholstery with heated front seats, in-dash navigation, a rear spoiler, and automatic climate control. For 2009, XT models came only with a four-speed automatic with Sport Shift.

The Forester XTI concept vehicle used the 2.5-liter intercooled turbo engine from the Subaru WRX STI, six-speed manual transmission, 18 × 8-inch S204 forged alloy wheels with Yokohama Advan Neova 255/40R18 performance tires, adjustable coil-over suspension, Brembo brakes with four-piston front calipers, 2-piston rear calipers, Super Sport ABS and Electronic Brake-force Distribution (EBD), leather and Alcantara sport seats, a special instrument cluster, front dash and center console and leather-wrapped steering wheel. Engine is rated and torque.

The vehicle was unveiled in the 2008 SEMA Show.

Subaru produced a specialized vehicle for the National Ski Patrol based on the 2.5XT turbo. It includes diamond plate floor, rear steel walls, a 9,500-pound winch and a roof-mounted toboggan. The vehicle was unveiled in the 2008 SEMA Show.

In 2010, USA model year 2011, the Subaru Forester received a minor facelift featuring a new grille insert and several small changes in various trim levels. A new 2.5X Touring trim level was also introduced above the 2.5X Limited. The 2.5X Touring trim added HID lighting, a rearview camera, dual-zone climate control, and silver roof rails. 2.5XT models got a slightly larger rear roof spoiler.

Subaru also quietly switched to the all new 2.5L DOHC FB25 third generation boxer engine in naturally aspirated Forester models. The new engine made the same 170 hp (126.8 kW) as the outgoing EJ253, but torque increased by 4 lb⋅ft to 174 lb⋅ft (235.9 N⋅m). Fuel economy improved by 1 mpg EPA city/highway to 21/27. 2.5XT models retained the 2.5L DOHC EJ255 turbo engine.

Pre-facelift styling

Post-facelift styling
The fourth-generation Forester was unveiled in the 2012 Guangzhou Motor Show, followed by the 2013 New York International Auto Show.

Changes to the line-up include:

Japan models went on sale in November 2012. Early model includes 2.0i, 2.0i-L, 2.0i-L EyeSight, 2.0i-S EyeSight, 2.0XT (280 PS), 2.0XT EyeSight (280 PS). 2.0i engine models include six-speed manual (2.0i, 2.0i-L) or Lineartronic CVT transmission; 2.0XT (280 PS) engine models include Lineartronic CVT transmission.

Asian models went on sale in March 2013 as 2014 model year. Early model includes 2.0i-L, 2.0i Premium and 2.0XT. Association of Southeast Asian Nations production of the Subaru Forester began in February 2016. Malaysia-based Tan Chong Motor Assemblies assembled approximately 10,000 Forester units annually for Malaysia, Thailand and Indonesia respectively.

US models went on sale in March 2013 as 2014 model year vehicles. Early models include 2.5i in base, Premium, Limited and top-line Touring versions, and performance-oriented turbocharged 2.0XT (253 PS) in Premium and Touring versions. Base and Premium model 2014 Foresters can be equipped with the manual six-speed transmission or the Lineartronic CVT. All other models are equipped with the Lineartronic CVT. An option on Limited/Touring 2.5i and Premium/Touring 2.0XT is new X-Mode control and Hill Descent Control (HDC) features. These are not available on other models.

It has been awarded "Motor Trend's" 2009 and 2014 SUV of the Year and The Car Connection's Best Car To Buy 2014.

According to IIHS (Insurance Institute for Highway Safety) the 2014 Forester achieved Good crash test ratings in Small Overlap Front, Moderate Overlap Front, Side, Roof Strength, and Head Restraining & Seats categories. The Forester had not been rated Good in the Small Overlap Front test until modifications were made for the 2014 model year. The small overlap test, introduced in 2012 by the IIHS, simulates a frontal collision on 25 percent of the driver's side front corner. Since its adoption, the IIHS has noticed several automakers making non-symmetrical modifications to their vehicles. Another small overlap test was conducted on a number of vehicles, including a 2014 Forester, but was conducted on the passenger side instead. The crash test showed substantially more intrusion into the passenger side than into the driver's side of the Forester, it would have been rated Marginal

The 2014 Forester has a new feature called X Mode that allows owners to go through more extreme conditions both on the road and off. The concept is that any driver, regardless of skill level, can drive safely on wet roads or muddy areas. It works by monitoring wheel-slip on all four wheels; should one or more wheels begin to slip, X Mode kicks in and applies the brakes to the affected wheel which results in a transfer of power to the opposite wheel. After it is engaged by a simple push button, X Mode stays engaged up until the vehicle's speed is about then disengages itself.

The 2014 top-of-the-line Touring model Forester offers Subaru's EyeSight driver assist technology that uses stereoscopic CCD cameras mounted on either side of the rearview mirror. Eyesight offers several driver assist technologies/features which include:

The system can be manually turned on or off. Being an optical, instead of radar, based system, it has limitations in limited visibility situations; driving into the sun, fog, or where the windshield is not cleared (snow, mud, etc.) may cause the system to disengage.

The 2014 and 2015 models had a major revamp of interior comfort. The passenger seat is higher, the sound system has been upgraded, the rear bench seats are higher and the console is re-positioned for the person riding in the center. The manual transmission models were also upgraded to a six-speed transmission instead of the previous generation's five-speed transmission. Engines during these year models do have an issue, this seems to be related predominantly to US manufactured vehicles and Subaru Australia have had no significant issues with excess oil consumption in Australia with the Japanese manufactured vehicles.(Source Subaru Australia and extensive research prior to purchase by Australian customer (KRH)) with oil consumption, along with numerous other automotive manufactures at the time. Subaru had its settlement of a lawsuit approved by a US Court in August 2016. The US complainants experienced over 1.5 quarts of oil consumed between change intervals with low-level dash lights coming on. The settlement provided for such complainants to receive an extended eight year warranty on the engine, allowing for an engine rebuild for that excessive oil consumption. Some speculation on this is the use of different style piston rings on the engine style change and synthetic oil.

Pre-facelift styling

Post-facelift styling (2017–2018)

The 2019 Subaru Forester was revealed on March 28, 2018 at the New York International Auto Show. Like contemporary Subaru models, the 2019 model year moved the Forester to the Subaru Global Platform.

In the US market, the 2019 Subaru Forester is available in the following trims:


As with all auto makers, each trim comes with a different level of standard features. The 2019 model year also comes standard with of ground clearance.

All 2019 Subaru Foresters have one of three versions of Subaru's Symmetrical All Wheel Drive (AWD) system. The trim level determines which system is installed. All provide a nominal torque split biased 60 front to 40 rear.


For the first time, all trim levels of Forester are only available with one engine: Subaru's new FB25 DI. The engine is a non-turbo, direct injection flat (boxer) 4 cylinder producing at 5800 rpm and at 4400 rpm. There is also only a single transmission option: the Lineartronic CVT.

All Foresters come standard with Subaru's Eyesight Driver Assistance Technology.

For the first time Subaru DriverFocus™ Distraction Mitigation System comes standard on the Touring trim, which provides an alert when it detects the driver is distracted or is drowsy. In addition, the DriverFocus system is able to recognize five different drivers and will set seat and mirror positions and climate control settings accordingly.

In 2019, the Start/Stop feature was added to all Forester models. This feature turns off the engine when the brake pedal is pressed. The engine restarts when the brake pedal is released. This feature comes on by default every time the engine is turned on but may be disabled after the engine is on.

Subaru introduced the e-BOXER hybrid powertrain for the European-market Forester and XV at Geneva in March 2019; the e-BOXER integrates an electric motor into the Lineartronic CVT to improve fuel economy and increase power. The e-BOXER powertrain features a modified FB20 rated at at 5,600–6,000 rpm and of torque at 4,000 rpm. Like the first-generation XV Crosstrek Hybrid, the Forester e-BOXER adds a single electric motor rated at maximum output. The battery for the traction motor is placed above the rear axle, improving the front/rear weight balance.




</doc>
<doc id="29313" url="https://en.wikipedia.org/wiki?curid=29313" title="Second-system effect">
Second-system effect

The second-system effect (also known as second-system syndrome) is the tendency of small, elegant, and successful systems to be succeeded by over-engineered, bloated systems, due to inflated expectations and overconfidence.

The phrase was first used by Fred Brooks in his book "The Mythical Man-Month", first published in 1975. It described the jump from a set of simple operating systems on the IBM 700/7000 series to OS/360 on the 360 series, which happened in 1964.




</doc>
<doc id="29316" url="https://en.wikipedia.org/wiki?curid=29316" title="Sandinista National Liberation Front">
Sandinista National Liberation Front

The Sandinista National Liberation Front (, FSLN) is a socialist political party in Nicaragua. Its members are called Sandinistas in both English and Spanish. The party is named after Augusto César Sandino, who led the Nicaraguan resistance against the United States occupation of Nicaragua in the 1930s.

The FSLN overthrew Anastasio Somoza DeBayle in 1979, ending the Somoza dynasty, and established a revolutionary government in its place. Having seized power, the Sandinistas ruled Nicaragua from 1979 to 1990, first as part of a Junta of National Reconstruction. Following the resignation of centrist members from this Junta, the FSLN took exclusive power in March 1981. They instituted a policy of mass literacy, devoted significant resources to health care, and promoted gender equality but came under international criticism for human rights abuses, mass execution and oppression of indigenous peoples. A U.S.-backed group, known as the Contras, was formed in 1981 to overthrow the Sandinista government and was funded and trained by the Central Intelligence Agency. In 1984 elections were held but were boycotted by some opposition parties. The FSLN won the majority of the votes, and those who opposed the Sandinistas won approximately a third of the seats. The civil war between the Contras and the government continued until 1989. After revising the constitution in 1987, and after years of fighting the Contras, the FSLN lost the 1990 election to Violeta Barrios de Chamorro but retained a plurality of seats in the legislature.

The FSLN is now Nicaragua's sole leading party. The FSLN often polls in opposition to the much smaller Constitutionalist Liberal Party, or PLC. In the 2006 Nicaraguan general election, former FSLN President Daniel Ortega was re-elected President of Nicaragua with 38.7% of the vote compared to 29% for his leading rival, bringing in the country's second Sandinista government after 17 years of other parties winning elections. Ortega and the FSLN were re-elected again in the presidential elections of November 2011 and of November 2016.

The Sandinistas took their name from Augusto César Sandino (1895–1934), the leader of Nicaragua's nationalist rebellion against the US occupation of the country during the early 20th century (ca. 1922–1934). The suffix "-ista" is simply the Spanish equivalent of "-ist".

Sandino was assassinated in 1934 by the Nicaraguan National Guard (), the US-equipped police force of Anastasio Somoza, whose family ruled the country from 1936 until they were overthrown by the Sandinistas in 1979.

The FSLN originated in the milieu of various oppositional organizations, youth and student groups in the late 1950s and early 1960s. The University of Léon, and the National Autonomous University of Nicaragua (UNAN) in Managua were two of the principal centers of activity. Inspired by the Revolution and the FLN in Algeria, the FSLN itself was founded in 1961 by Carlos Fonseca, , Tomás Borge, Casimiro Sotelo and others as "The National Liberation Front" (FLN). Only Tomás Borge lived long enough to see the Sandinista victory in 1979.

The term "Sandinista", was added two years later, establishing continuity with Sandino's movement, and using his legacy in order to develop the newer movement's ideology and strategy. By the early 1970s, the FSLN was launching limited military initiatives.

On December 23, 1972, a magnitude 6.2 earthquake leveled the capital city, Managua. The earthquake killed 10,000 of the city's 400,000 residents and left another 50,000 homeless. About 80% of Managua's commercial buildings were destroyed. President Anastasio Somoza Debayle's National Guard embezzled much of the international aid that flowed into the country to assist in reconstruction, and several parts of downtown Managua were never rebuilt. The president gave reconstruction contracts preferentially to family and friends, thereby profiting from the quake and increasing his control of the city's economy. By some estimates, his personal wealth rose to US$400 million in 1974.

In December 1974, a guerrilla group affiliated with FSLN directed by Eduardo Contreras and Germán Pomares seized government hostages at a party in the house of the Minister of Agriculture in the Managua suburb Los Robles, among them several leading Nicaraguan officials and Somoza relatives. The siege was carefully timed to take place after the departure of the US ambassador from the gathering. At 10:50 pm, a group of 15 young guerrillas and their commanders, Pomares and Contreras, entered the house. They killed the Minister, who tried to shoot them, during the takeover. The guerrillas received US$2 million ransom, and had their official communiqué read over the radio and printed in the newspaper "La Prensa".

Over the next year, the guerrillas also succeeded in getting 14 Sandinista prisoners released from jail, and with them, were flown to Cuba. One of the released prisoners was Daniel Ortega, who would later become the president of Nicaragua. The group also lobbied for an increase in wages for National Guard soldiers to 500 córdobas ($71 at the time). The Somoza government responded with further censorship, intimidation, torture, and murder.

In 1975, Somoza imposed a state of siege, censoring the press, and threatening all opponents with internment and torture. Somoza's National Guard also increased its violence against individuals and communities suspected of collaborating with the Sandinistas. Many of the FSLN guerrillas were killed, including its leader and founder Carlos Fonseca in 1976. Fonseca had returned to Nicaragua in 1975 from his exile in Cuba to try to reunite factions that existed in the FSLN. He and his group were betrayed by a peasant who informed the National Guard that they were in the area. The guerrilla group was ambushed, and Fonseca was wounded in the process. The next morning Fonseca was executed by the National Guard.

Following the FSLN's defeat at the battle of Pancasán in 1967, the organization adopted the "Prolonged Popular War" ("Guerra Popular Prolongada", GPP) theory as its strategic doctrine. The GPP was based on the "accumulation of forces in silence": while the urban organization recruited on the university campuses and robbed money from banks, the main cadres were to permanently settle in the north central mountain zone. There they would build a grassroots peasant support base in preparation for renewed rural guerrilla warfare.

As a consequence of the repressive campaign of the National Guard, in 1975 a group within the FSLN's urban mobilization arm began to question the viability of the GPP. In the view of the young orthodox Marxist intellectuals, such as Jaime Wheelock, economic development had turned Nicaragua into a nation of factory workers and wage-earning farm laborers. Wheelock's faction was known as the "Proletarian Tendency".

Shortly after, a third faction arose within the FSLN. The "Insurrectional Tendency", also known as the "Third Way" or "Terceristas", led by Daniel Ortega, his brother Humberto Ortega, and Mexican-born Victor Tirado Lopez, was more pragmatic and called for tactical, temporary alliances with non-communists, including the right-wing opposition, in a popular front against the Somoza regime. By attacking the Guard directly, the Terceristas would demonstrate the weakness of the regime and encourage others to take up arms.

In October 1977, a group of prominent Nicaraguan professionals, business leaders, and clergymen allied with the Terceristas to form ""El Grupo de los Doce"" (The Group of Twelve) in Costa Rica. The group's main idea was to organize a provisional government in Costa Rica. The new strategy of the Terceristas also included unarmed strikes and rioting by labor and student groups coordinated by the FSLN's "United People's Movement" (Movimiento Pueblo Unido – MPU).

On 10 January 1978, Pedro Joaquín Chamorro, the editor of the opposition newspaper "La Prensa" and leader of the "Democratic Union of Liberation" (Unión Democrática de Liberación – UDEL), was assassinated. Although his assassins were not identified at the time, evidence implicated President Somoza's son and other members of the National Guard. Spontaneous riots followed in several cities, while the business community organized a general strike demanding Somoza's resignation.

The Terceristas carried out attacks in early February in several Nicaraguan cities. The National Guard responded by further increasing repression and using force to contain and intimidate all government opposition. The nationwide strike that paralyzed the country for ten days weakened the private enterprises and most of them decided to suspend their participation in less than two weeks. Meanwhile, Somoza asserted his intention to stay in power until the end of his presidential term in 1981. The United States government showed its displeasure with Somoza by suspending all military assistance to the regime, but continued to approve economic assistance to the country for humanitarian reasons.

In August, the Terceristas staged a hostage-taking. Twenty-three Tercerista commandos led by Edén Pastora seized the entire Nicaraguan congress and took nearly 1,000 hostages, including Somoza's nephew José Somoza Abrego and cousin Luis Pallais Debayle. Somoza gave in to their demands and paid a $500,000 ransom, released 59 political prisoners (including GPP chief Tomás Borge), broadcast a communiqué with FSLN's call for general insurrection and gave the guerrillas safe passage to Panama.

A few days later six Nicaraguan cities rose in revolt. Armed youths took over the highland city of Matagalpa. Tercerista cadres attacked Guard posts in Managua, Masaya, León, Chinandega and Estelí. Large numbers of semi-armed civilians joined the revolt and put the Guard garrisons of the latter four cities under siege. The September Insurrection of 1978 was subdued at the cost of several thousand, mostly civilian, casualties. Members of all three factions fought in these uprisings, which began to blur the divisions and prepare the way for unified action.

In early 1979, President Jimmy Carter and the United States ended support for the Somoza government, but did not want a left-wing government to take power in Nicaragua. The moderate "Broad Opposition Front" ("Frente Amplio Opositor" – FAO) which opposed Somoza was made up of a conglomeration of dissidents within the government as well as the "Democratic Union of Liberation" (UDEL) and the "Twelve", representatives of the Terceristas (whose founding members included Casimiro A. Sotelo, later to become Ambassador to the U.S. AND Canada representing the FSLN). The FAO and Carter came up with a plan that would remove Somoza from office but left no part in government power for the FSLN. The FAO's efforts lost political legitimacy, as Nicaraguans protested that they did not want ""Somocismo sin Somoza"" (Somocism without Somoza).

The "Twelve" abandoned the coalition in protest and formed the "National Patriotic Front" ("Frente Patriotico Nacional" – FPN) together with the "United People's Movement" (MPU). This strengthened the revolutionary organizations as tens of thousands of youths joined the FSLN and the fight against Somoza. A direct consequence of the spread of the armed struggle in Nicaragua was the official reunification of the FSLN that took place on 7 March 1979. Nine men, three from each tendency, formed the National Directorate which would lead the reunited FSLN. They were: Daniel Ortega, Humberto Ortega and Víctor Tirado (Terceristas); Tomás Borge, , and Henry Ruiz (GPP faction); and Jaime Wheelock, Luis Carrión and Carlos Núñez.

The FSLN evolved from one of many opposition groups to a leadership role in the overthrow of the Somoza regime. By mid-April 1979, five guerrilla fronts opened under the joint command of the FSLN, including an internal front in the capital city Managua. Young guerrilla cadres and the National Guardsmen were clashing almost daily in cities throughout the country. The strategic goal of the Final Offensive was the division of the enemy's forces. Urban insurrection was the crucial element because the FSLN could never hope to achieve simple superiority in men and firepower over the National Guard.

On June 4, a general strike was called by the FSLN to last until Somoza fell and an uprising was launched in Managua. On June 16, the formation of a provisional Nicaraguan government in exile, consisting of a five-member Junta of National Reconstruction, was announced and organized in Costa Rica. The members of the new junta were Daniel Ortega (FSLN), Moisés Hassan (FPN), Sergio Ramírez (the "Twelve"), Alfonso Robelo (MDN) and Violeta Barrios de Chamorro, the widow of "La Prensa"s director Pedro Joaquín Chamorro. By the end of that month, with the exception of the capital, most of Nicaragua was under FSLN control, including León and Matagalpa, the two largest cities in Nicaragua after Managua.

On July 9, the provisional government in exile released a government program, in which it pledged to organize an effective democratic regime, promote political pluralism and universal suffrage, and ban ideological discrimination, except for those promoting the "return of Somoza's rule". On July 17, Somoza resigned, handed over power to Francisco Urcuyo, and fled to Miami. While initially seeking to remain in power to serve out Somoza's presidential term, Urcuyo ceded his position to the junta and fled to Guatemala two days later.

On July 19, the FSLN army entered Managua, culminating the first goal of the Nicaraguan revolution. The war left approximately 30,000-50,000 dead and 150,000 Nicaraguans in exile. The five-member junta entered the Nicaraguan capital the next day and assumed power, reiterating its pledge to work for political pluralism, a mixed economic system, and a nonaligned foreign policy.

The Sandinistas inherited a country with a debt of US$1.6 billion, an estimated 30,000 to 50,000 war dead, 600,000 homeless, and a devastated economic infrastructure. To begin the task of establishing a new government, they created a Council (or ) of National Reconstruction, made up of five appointed members. Three of the appointed members—Sandinista militants Daniel Ortega, Moises Hassan, and novelist Sergio Ramírez (a member of Los Doce "the Twelve")—belonged to the FSLN. Two opposition members, businessman Alfonso Robelo, and Violeta Barrios de Chamorro (the widow of Pedro Joaquín Chamorro), were also appointed. Only three votes were needed to pass law.

The FSLN also established a Council of State, subordinate to the junta, which was composed of representative bodies. However, the Council of State only gave political parties twelve of forty-seven seats; the rest of the seats were given to Sandinista mass-organizations. Of the twelve seats reserved for political parties, only three were not allied to the FSLN. Due to the rules governing the Council of State, in 1980 both non-FSLN junta members resigned. Nevertheless, as of the 1982 State of Emergency, opposition parties were no longer given representation in the council. The preponderance of power also remained with the Sandinistas through their mass organizations, including the Sandinista Workers' Federation (), the Luisa Amanda Espinoza Nicaraguan Women's Association (), the National Union of Farmers and Ranchers (), and most importantly the Sandinista Defense Committees (CDS). The Sandinista-controlled mass organizations were extremely influential over civil society and saw their power and popularity peak in the mid-1980s.

Upon assuming power, the FSLN's official political platform included the following: nationalization of property owned by the Somozas and their supporters; land reform; improved rural and urban working conditions; free unionization for all workers, both urban and rural; price fixing for commodities of basic necessity; improved public services, housing conditions, education; abolition of torture, political assassination and the death penalty; protection of democratic liberties; equality for women; non-aligned foreign policy; formation of a "popular army" under the leadership of the FSLN and Humberto Ortega.

The FSLN's literacy campaign sent teachers into the countryside and within six months, half a million people had been taught rudimentary reading, bringing the national illiteracy rate down from over 50% to just under 12%. Over 100,000 Nicaraguans participated as literacy teachers. One of the stated aims of the literacy campaign was to create a literate electorate which would be able to make informed choices at the promised elections. The successes of the literacy campaign was recognized by UNESCO with the award of a Nadezhda Krupskaya International Prize.

The FSLN also created neighborhood groups similar to the Cuban Committees for the Defense of the Revolution, called Sandinista Defense Committees ( or CDS). Especially in the early days following the overthrow of Somoza, the CDS's served as "de facto" units of local governance. Their obligations included political education, the organization of Sandinista rallies, the distribution of food rations, organization of neighborhood/regional cleanup and recreational activities, and policing to control looting, and the apprehension of counter-revolutionaries. The CDS's organized civilian defense efforts against Contra activities and a network of intelligence systems in order to apprehend their supporters. These activities led critics of the Sandinistas to argue that the CDS was a system of local spy networks for the government used to stifle political dissent, and the CDS did hold limited powers—such as the ability to suspend privileges such as driver licenses and passports—if locals refused to cooperate with the new government. After the initiation of heavier U.S. military involvement in the Nicaraguan conflict the CDS was empowered to enforce wartime bans on political assembly and association with other political parties (i.e., parties associated with the "Contras").

By 1980, conflicts began to emerge between the Sandinista and non-Sandinista members of the governing junta. Violeta Chamorro and Alfonso Robelo resigned from the governing junta in 1980, and rumours began that members of the Ortega junta would consolidate power amongst themselves. These allegations spread, and rumors intensified that it was Ortega's goal to turn Nicaragua into a state modeled after Cuban socialism. In 1979 and 1980, former Somoza supporters and ex-members of Somoza's National Guard formed irregular military forces, while the original core of the FSLN began to splinter. Armed opposition to the Sandinista Government eventually divided into two main groups: The Fuerza Democrática Nicaragüense (FDN), a U.S. supported army formed in 1981 by the CIA, U.S. State Department, and former members of the widely condemned Somoza-era Nicaraguan National Guard; and the Alianza Revolucionaria Democratica (ARDE) Democratic Revolutionary Alliance, a group that had existed since before the FSLN and was led by Sandinista founder and former FSLN supreme commander, Edén Pastora, a.k.a. "Commander Zero". and Milpistas, former anti-Somoza rural militias, which eventually formed the largest pool of recruits for the Contras. Although independent and often at conflict with each other, these guerrilla bands—along with several others—all became generally known as "Contras" (short for "", en. "counter-revolutionaries").

The opposition militias were initially organized and largely remained segregated according to regional affiliation and political backgrounds. They conducted attacks on economic, military, and civilian targets. During the Contra war, the Sandinistas arrested suspected members of the Contra militias and censored publications they accused of collaborating with the enemy (i.e. the U.S., the FDN, and ARDE, among others).

In March 1982 the Sandinistas declared an official State of Emergency. They argued that this was a response to attacks by counter-revolutionary forces. The State of Emergency lasted six years, until January 1988, when it was lifted.

Under the new "Law for the Maintenance of Order and Public Security" the "Tribunales Populares Anti-Somozistas" allowed for the indefinite holding of suspected counter-revolutionaries without trial. The State of Emergency, however, most notably affected rights and guarantees contained in the "Statute on Rights and Guarantees of Nicaraguans". Many civil liberties were curtailed or canceled such as the freedom to organize demonstrations, the inviolability of the home, freedom of the press, freedom of speech, and the freedom to strike.

All independent news program broadcasts were suspended. In total, twenty-four programs were cancelled. In addition, Sandinista censor Nelba Cecilia Blandón issued a decree ordering all radio stations to take broadcasts from government radio station La Voz de La Defensa de La Patria every six hours.

The rights affected also included certain procedural guarantees in the case of detention including habeas corpus. The State of Emergency was not lifted during the 1984 elections. There were many instances where rallies of opposition parties were physically broken up by Sandinista Youth or pro-Sandinista mobs. Opponents to the State of Emergency argued its intent was to crush resistance to the FSLN. James Wheelock justified the actions of the Directorate by saying "... We are annulling the license of the false prophets and the oligarchs to attack the revolution."

Some emergency measures were taken before 1982. In December 1979 special courts called "Tribunales Especiales" were established to speed up the processing of 7,000-8,000 National Guard prisoners. These courts operated through relaxed rules of evidence and due process and were often staffed by law students and inexperienced lawyers. However, the decisions of the "Tribunales Especiales" were subject to appeal in regular courts. Many of the National Guard prisoners were released immediately due to lack of evidence. Others were pardoned or released by decree. By 1986 only 2,157 remained in custody and only 39 were still being held in 1989 when they were released under the Esquipulas II agreement.

On October 5, 1985 the Sandinistas broadened the 1982 State of Emergency and suspended many more civil rights. A new regulation also forced any organization outside of the government to first submit any statement it wanted to make public to the censorship bureau for prior approval.

The FSLN lost power in the presidential election of 1990 when Daniel Ortega was defeated in an election for the Presidency of Nicaragua by Violeta Chamorro.

Upon assuming office in 1981, U.S. President Ronald Reagan condemned the FSLN for joining with Cuba in supporting "Marxist" revolutionary movements in other Latin American countries such as El Salvador. His administration authorized the CIA to begin financing, arming and training rebels, most of whom were the remnants of Somoza's National Guard, as anti-Sandinista guerrillas that were branded "counter-revolutionary" by leftists ( in Spanish). This was shortened to "Contras", a label the force chose to embrace. Edén Pastora and many of the indigenous guerrilla forces, who were not associated with the "Somozistas", also resisted the Sandinistas.

The Contras operated out of camps in the neighboring countries of Honduras to the north and Costa Rica (see Edén Pastora cited below) to the south. As was typical in guerrilla warfare, they were engaged in a campaign of economic sabotage in an attempt to combat the Sandinista government and disrupted shipping by planting underwater mines in Nicaragua's Corinto harbour, an action condemned by the International Court of Justice as illegal. The U.S. also sought to place economic pressure on the Sandinistas, and, as with Cuba, the Reagan administration imposed a full trade embargo.

The Contras also carried out a systematic campaign to disrupt the social reform programs of the government. This campaign included attacks on schools, health centers and the majority of the rural population that was sympathetic to the Sandinistas. Widespread murder, rape, and torture were also used as tools to destabilize the government and to "terrorize" the population into collaborating with the Contras. Throughout this campaign, the Contras received military and financial support from the CIA and the Reagan Administration. This campaign has been condemned internationally for its many human rights violations. Contra supporters have often tried to downplay these violations, or countered that the Sandinista government carried out much more. In particular, the Reagan administration engaged in a campaign to alter public opinion on the Contras that has been termed "white propaganda". In 1984, the International Court of Justice judged that the United States Government had been in violation of International law when it supported the Contras.

After the U.S. Congress prohibited federal funding of the Contras through the Boland Amendment in 1983, the Reagan administration continued to back the Contras by raising money from foreign allies and covertly selling arms to Iran (then engaged in a war with Iraq), and channelling the proceeds to the Contras (see the Iran–Contra affair). When this scheme was revealed, Reagan admitted that he knew about Iranian "arms for hostages" dealings but professed ignorance about the proceeds funding the Contras; for this, National Security Council aide Lt. Col. Oliver North took much of the blame.

Senator John Kerry's 1988 U.S. Senate Committee on Foreign Relations report on links between the Contras and drug imports to the US concluded that "senior U.S. policy makers were not immune to the idea that drug money was a perfect solution to the Contras' funding problems". According to the National Security Archive, Oliver North had been in contact with Manuel Noriega, the US-backed president of Panama. The Reagan administration's support for the Contras continued to stir controversy well into the 1990s. In August 1996, "San Jose Mercury News" reporter Gary Webb published a series titled "Dark Alliance", linking the origins of crack cocaine in California to the CIA-Contra alliance. Webb's allegations were repudiated by reports from the "Los Angeles Times", "The New York Times", and "The Washington Post", and the "San Jose Mercury News" eventually disavowed his work. An investigation by the United States Department of Justice also stated that their "review did not substantiate the main allegations stated and implied in the Mercury News articles". Regarding the specific charges towards the CIA, the DOJ wrote "the implication that the drug trafficking by the individuals discussed in the "Mercury News" articles was connected to the CIA was also not supported by the facts". The CIA also investigated and rejected the allegations.

The Contra war unfolded differently in the northern and southern zones of Nicaragua. Contras based in Costa Rica operated on Nicaragua's Caribbean coast, which is sparsely populated by indigenous groups including the Miskito, Sumo, Rama, Garifuna, and Mestizo. Unlike Spanish-speaking western Nicaragua, the Caribbean Coast is predominantly English-speaking and was largely ignored by the Somoza regime. The "costeños" did not participate in the uprising against Somoza and viewed Sandinismo with suspicion from the outset.

While the Sandinistas encouraged grassroots pluralism, they were perhaps less enthusiastic about national elections. They argued that popular support was expressed in the insurrection and that further appeals to popular support would be a waste of scarce resources. International pressure and domestic opposition eventually pressed the government toward a national election. Tomás Borge warned that the elections were a concession, an act of generosity and of political necessity. On the other hand, the Sandinistas had little to fear from the election given the advantages of incumbency and the restrictions on the opposition, and they hoped to discredit the armed efforts to overthrow them.

A broad range of political parties, ranging in political orientation from far-left to far-right, competed for power. Following promulgation of a new populist constitution, Nicaragua held national elections in 1984. Independent electoral observers from around the world—including groups from the UN as well as observers from Western Europe—found that the elections had been fair. Several groups, however, disputed this, including UNO, a broad coalition of anti-Sandinista activists, COSEP, an organization of business leaders, the Contra group "FDN", organized by former Somozan-era National Guardsmen, landowners, businessmen, peasant highlanders, and what some claimed as their patron, the U.S. government.

Although initially willing to stand in the 1984 elections, the UNO, headed by Arturo Cruz (a former Sandinista), declined participation in the elections based on their own objections to the restrictions placed on the electoral process by the State of Emergency and the official advisement of President Ronald Reagan's State Department, who wanted to de-legitimize the election process. Among other parties that abstained was COSEP, who had warned the FSLN that they would decline participation unless freedom of the press was reinstituted. Coordinadora Democrática (CD) also refused to file candidates and urged Nicaraguans not to take part in the election. The Independent Liberal Party (PLI), headed by Virgilio Godoy Reyes, announced its refusal to participate in October. Consequently, when the elections went ahead the U.S. raised objections based upon political restrictions instituted by the State of Emergency (e.g., censorship of the press, cancellation of habeas corpus, and the curtailing of free assembly).

Daniel Ortega and Sergio Ramírez were elected president and vice-president, and the FSLN won an overwhelming 61 out of 96 seats in the new National Assembly, having taken 67% of the vote on a turnout of 75%. Despite international validation of the elections by multiple political and independent observers (virtually all from among U.S. allies), the United States refused to recognize the elections, with President Ronald Reagan denouncing the elections as a sham. According to a study, since the 1984 election was for posts subordinate to the Sandinista Directorate, the elections were no more subject to approval by vote than the Central Committee of the Communist Party is in countries of the East Bloc. Daniel Ortega began his six-year presidential term on January 10, 1985. After the United States Congress turned down continued funding of the Contras in April 1985, the Reagan administration ordered a total embargo on United States trade with Nicaragua the following month, accusing the Sandinista government of threatening United States security in the region.

The elections of 1990, which had been mandated by the constitution passed in 1987, saw the Bush administration funnel $49.75 million of 'non-lethal' aid to the Contras, as well as $9 million to the opposition UNO—equivalent to $2 billion worth of intervention by a foreign power in a US election at the time, and proportionately five times the amount George Bush had spent on his own election campaign. When Violeta Chamorro visited the White House in November 1989, the US pledged to maintain the embargo against Nicaragua unless Violeta Chamorro won.

In August 1989, the month that campaigning began, the Contras redeployed 8,000 troops into Nicaragua, after a funding boost from Washington, continued their guerrilla war. 50 FSLN candidates were assassinated. The Contras also distributed thousands of UNO leaflets.

Years of conflict had left 50,000 casualties and $12 billion of damages in a society of 3.5 million people and an annual GNP of $2 billion. After the war, a survey was taken of voters: 75.6% agreed that if the Sandinistas had won, the war would never have ended. 91.8% of those who voted for the UNO agreed with this (William I Robinson, op cit). The Library of Congress Country Studies on Nicaragua states:

Despite limited resources and poor organization, the UNO coalition under Violeta Chamorro directed a campaign centered around the failing economy and promises of peace. Many Nicaraguans expected the country's economic crisis to deepen and the Contra conflict to continue if the Sandinistas remained in power. Chamorro promised to end the unpopular military draft, bring about democratic reconciliation, and promote economic growth. In the February 25, 1990, elections, Violeta Barrios de Chamorro carried 55 percent of the popular vote against Daniel Ortega's 41 percent.

In 1987, due to a stalemate with the Contras, the Esquipulas II treaty was brokered by Costa Rican President Óscar Arias Sánchez. The treaty's provisions included a call for a cease-fire, freedom of expression, and national elections. After the February 26, 1990 elections, the Sandinistas lost and peacefully passed power to the National Opposition Union (UNO), an alliance of 14 opposition parties ranging from the conservative business organization COSEP to Nicaraguan communists. UNO's candidate, Violeta Barrios de Chamorro, replaced Daniel Ortega as president of Nicaragua.

Reasons for the Sandinista loss in 1990 are disputed. Defenders of the defeated government assert that Nicaraguans voted for the opposition due to the continuing U.S. economic embargo and potential Contra threat. Others have alleged that the United States threatened to continue to support the Contras and continue the civil war if the regime was not voted out of power.

After their loss, the Sandinista leaders held most of the private property and businesses that had been confiscated and nationalized by the FSLN government. This process became known as the "piñata" and was tolerated by the new Chamorro government. Ortega also claimed to "rule from below" through groups he controls such as labor unions and student groups. Prominent Sandinistas also created nongovernmental organizations to promote their ideas and social goals.

Ortega remained the head of the FSLN, but his brother Humberto resigned from the party and remained at the head of the Sandinista Army, becoming a close confidante and supporter of Chamorro. The party also experienced internal divisions, with prominent Sandinistas such as Ernesto Cardenal and Sergio Ramírez resigning to protest what they described as heavy-handed domination of the party by Daniel Ortega. Ramírez also founded a separate political party, the Sandinista Renovation Movement (MRS); his faction came to be known as the , who favor a more social democratic approach than the "ortodoxos", or hardliners. In the 1996 Nicaraguan election, Ortega and Ramírez both campaigned unsuccessfully as presidential candidates on behalf of their respective parties, with Ortega receiving 43% of the vote while Arnoldo Alemán of the Constitutional Liberal Party received 51%. The Sandinistas won second place in the congressional elections, with 36 of 93 seats.

Ortega was re-elected as leader of the FSLN in 1998. Municipal elections in November 2000 saw a strong Sandinista vote, especially in urban areas, and former Tourism Minister Herty Lewites was elected mayor of Managua. This result led to expectations of a close race in the presidential elections scheduled for November 2001. Daniel Ortega and Enrique Bolaños of the Constitutional Liberal Party (PLC) ran neck-and-neck in the polls for much of the campaign, but in the end the PLC won a clear victory. The results of these elections were that the FSLN won 42.6% of the vote for parliament (versus 52.6% for the PLC), giving them 41 out of the 92 seats in the National Assembly (versus 48 for the PLC). In the presidential race, Ortega lost to Bolaños 46.3% to 53.6%.

Daniel Ortega was once again re-elected as leader of the FSLN in March 2002 and re-elected as president of Nicaragua in November 2006.

In 2006, Daniel Ortega was elected president with 38% of the vote (see 2006 Nicaraguan general election). This occurred despite the fact that the breakaway Sandinista Renovation Movement continued to oppose the FSLN, running former Mayor of Managua Herty Lewites as its candidate for president. However, Lewites died several months before the elections.

The FSLN also won 38 seats in the congressional elections, becoming the party with the largest representation in parliament. The split in the Constitutionalist Liberal Party helped to allow the FSLN to become the largest party in Congress. The Sandinista vote was also split between the FSLN and MRS, but the split was more uneven, with limited support for the MRS. The vote for the two liberal parties combined was larger than the vote for the two Sandinista parties. In 2010, several liberal congressmen raised accusations about the FSLN presumably attempting to buy votes in order to pass constitutional reforms that would allow Ortega to run for office for the 6th time since 1984. In 2011, Ortega was re-elected as President.

Ortega was allowed by Nicaraguan Supreme Court to run again as President, despite having already served two mandates, in a move which was strongly criticized by the opposition. The Supreme Court also banned the leader of the Independent Liberal Party Eduardo Montealegre from running in the election. Ortega was re-elected as President, amid claims of electoral fraud; data about turnout were unclear: while the Supreme Electoral Council claimed a turnout of 66% of voters, the opposition claimed only 30% of voters actually went to the polls.

The year 2018 was marked by particular unrest in Nicaragua that had not been seen in the country in three decades. It came in two different phases, with initial unrest in the context of a fire at the Indio Maíz Biological Reserve in the Río San Juan department (which came to an end when rain abruptly put the fire out), leading on to an outbreak of violence a few weeks later after social security reforms were announced by the government.

During this unrest there were many deaths linked to the violence, as well as many instances of torture, sexual assaults, death threats, intimidation and the ransacking and burning of building and violence against journalists. Opposition figures argued that the government was responsible for the violence, a view supported by some press outlets and NGOs such as Amnesty International. Many opposition figures and independent journalists have been arrested and police raids of opposition forces and independent media have occurred frequently.

On September 29, 2018, President Ortega declared that political protests were "illegal" in Nicaragua, stating that demonstrators would "respond to justice" if they attempted to publicly voice their opinions. The United Nations condemned the actions as being a violation of human rights regarding freedom of assembly.

Carlos Fernando Chamorro, son of former president Violeta Chamorro and editor of "Confidencial", left the country after his office was subject to police search in December 2018.

In December 2018, the government revoked the licenses of five human rights organizations, closed the offices of the cable news and online show "Confidencial", and beat journalists when they protested.

The Confidential newspaper and other media were seized and taken by the government of Daniel Ortega Several service stations of the Puma brand were closed on the afternoon, December 20, by representatives of the Nicaraguan Energy Institute (INE), a state entity that has the mandate to regulate, among others, the hydrocarbons sector.　Puma Energy entered the Nicaraguan oil and fuel derivatives market at the end of March 2011, when it bought the entire network of Esso stations in Nicaragua, as part of a regional operation that involved the purchase of 290 service stations and eight storage terminals. of fuel in four countries of Central America.

On December 21, 2018, the Nicaraguan police raided the offices of the 100% News Channel. They arrested Miguel Mora, owner of the Canal; Lucía Pineda, Head of Press of 100% Noticias and Verónica Chávez, wife of Miguel Mora and host of the Ellas Lo Dicen Program. Subsequently, Verónica Chávez was released. Miguel Mora and Lucia Pineda were accused of terrorist crimes and provoking hatred and discrimination between the police and Sandinistas.

On January 30, 2019, the FSLN was expelled from the Socialist International citing "gross violations of human rights and democratic values committed by the government of Nicaragua". The ruling Democratic Revolutionary Party of Panama, also a member of the Socialist International, rejected the expulsion of the FSLN and threatened to leave the International, saying that it has abandoned its principles and made a decision regarding Latin America without consulting the Latin American parties, and referred to a "history of brotherhood in the struggle for social justice in Central America" between the two parties .

Through the media and the works of FSLN leaders such as Carlos Fonseca, the life and times of Augusto César Sandino became its unique symbol in Nicaragua. The ideology of Sandinismo gained momentum in 1974, when a Sandinista-initiated hostage situation resulted in the Somoza government adhering to FSLN demands and publicly printing and airing work on Sandino in well known newspapers and media outlets.

During the struggle against Somoza, the FSLN leaders' internal disagreements over strategy and tactics were reflected in three main factions:

Nevertheless, while ideologies varied between FSLN leaders, all leaders essentially agreed that Sandino provided a path for the Nicaragua masses to take charge, and the FSLN would act as the legitimate vanguard. The extreme end of the ideology links Sandino to Roman Catholicism and portrays him as descending from the mountains in Nicaragua knowing he would be betrayed and killed. Generally however, most Sandinistas associated Sandino on a more practical level, as a heroic and honest person who tried to combat the evil forces of imperialist national and international governments that existed in Nicaragua's history.

For purposes of making sense of how to govern, the FSLN drew four fundamental principles from the work of Carlos Fonseca and his understanding of the lessons of Sandino. According to Bruce E. Wright, "the Governing Junta of National Reconstruction agreed, under Sandinista leadership, that these principles had guided it in putting into practice a form of government that was characterized by those principles." It is generally accepted that these following principles have evolved the "ideology of Sandinismo". Three of these (excluding popular participation, which was presumably contained in Article 2 of the Constitution of Nicaragua) were to ultimately be guaranteed by Article 5 of the Constitution of Nicaragua. They are as follows:

Bruce E. Wright claims that "this was a crucial contribution from Fonseca's work that set the template for FSLN governance during the revolutionary years and beyond".

Beginning in 1967, the Cuban General Intelligence Directorate, or DGI, had begun to establish ties with Nicaraguan revolutionary organizations. By 1970 the DGI had managed to train hundreds of Sandinista guerrilla leaders and had vast influence over the organization. After the successful ousting of Somoza, DGI involvement in the new Sandinista government expanded rapidly. An early indication of the central role that the DGI would play in the Cuban-Nicaraguan relationship is a meeting in Havana on July 27, 1979, at which diplomatic ties between the two countries were re-established after more than 25 years. Julián López Díaz, a prominent DGI agent, was named Ambassador to Nicaragua. Cuban military and DGI advisors, initially brought in during the Sandinista insurgency, would swell to over 2,500 and operated at all levels of the new Nicaraguan government.

The Cubans would like to have helped more in the development of Nicaragua towards socialism. Following the US invasion of Grenada, countries previously looking for support from Cuba saw that the United States was likely to take violent action to discourage this.

The early years of the Nicaraguan revolution had strong ties to Cuba. The Sandinista leaders acknowledged that the FSLN owed a great debt to the socialist island. Once the Sandinistas assumed power, Cuba gave Nicaragua military advice, as well as aid in education, health care, vocational training and industry building for the impoverished Nicaraguan economy. In return, Nicaragua provided Cuba with grains and other foodstuffs to help Cuba overcome the effects of the US embargo.

According to Cambridge University historian Christopher Andrew, who undertook the task of processing the Mitrokhin Archive, Carlos Fonseca Amador, one of the original three founding members of the FSLN had been recruited by the KGB in 1959 while on a trip to Moscow. This was one part of Aleksandr Shelepin's 'grand strategy' of using national liberation movements as a spearhead of the Soviet Union's foreign policy in the Third World, and in 1960 the KGB organized funding and training for twelve individuals that Fonseca handpicked. These individuals were to be the core of the new Sandinista organization. In the following several years, the FSLN tried with little success to organize guerrilla warfare against the government of Luis Somoza Debayle. After several failed attempts to attack government strongholds and little initial support from the local population, the National Guard nearly annihilated the Sandinistas in a series of attacks in 1963. Disappointed with the performance of Shelepin's new Latin American "revolutionary vanguard", the KGB reconstituted its core of the Sandinista leadership into the ISKRA group and used them for other activities in Latin America.

According to Andrew, Mitrokhin says during the following three years the KGB handpicked several dozen Sandinistas for intelligence and sabotage operations in the United States. Andrew and Mitrokhin say that in 1966, this KGB-controlled Sandinista sabotage and intelligence group was sent to northern Mexico near the US border to conduct surveillance for possible sabotage.

In July 1961 during the Berlin Crisis of 1961 KGB chief Alexander Shelepin sent a memorandum to Soviet premier Nikita Khrushchev containing proposals to create a situation in various areas of the world which would favor dispersion of attention and forces by the US and their satellites, and would tie them down during the settlement of the question of a German peace treaty and West Berlin. It was planned, inter alia, to organize an armed mutiny in Nicaragua in coordination with Cuba and with the "Revolutionary Front Sandino". Shelepin proposed to make appropriations from KGB funds in addition to the previous assistance $10,000 for purchase of arms.

Khrushchev sent the memo with his approval to his deputy Frol Kozlov and on August 1 it was, with minor revisions, passed as a CPSU Central Committee directive. The KGB and the Soviet Ministry of Defense were instructed to work out more specific measures and present them for consideration by the Central Committee.

Other researchers have documented the contribution made from other Warsaw Pact intelligence agencies to the fledgling Sandinista government including the East German Stasi, by using recently declassified documents from Berlin as well as from former Stasi spymaster Markus Wolf who described the Stasi's assistance in the creation of a secret police force modeled on East Germany's.

Cuba was instrumental in the Nicaraguan Literacy Campaign. Nicaragua was a country with a very high rate of illiteracy, but the campaign succeeded in lowering the rate from 50% to 12%. The revolution in Cuban education since the ousting of the US-backed Batista regime not only served as a model for Nicaragua but also provided technical assistance and advice. Cuba played an important part in the Campaign, providing teachers on a yearly basis after the revolution. Prevost states that "Teachers were not the only ones studying in Cuba, about 2,000 primary and secondary students were studying on the Isle of Youth and the cost was covered by the host country (Cuba)".

The goals of the 1980 Literacy Campaign were socio-political, strategic as well as educational. It was the most prominent campaign with regards to the new education system. Illiteracy in Nicaragua was significantly reduced from 50.3% to 12.9%. One of the government's major concerns was the previous education system under the Somoza regime which did not see education as a major factor on the development of the country. As mentioned in the Historical Program of the FSLN of 1969, education was seen as a right and the pressure to stay committed to the promises made in the program was even stronger. 1980 was declared the "Year of Literacy" and the major goals of the campaign that started only 8 months after the FSLN took over. This included the eradication of illiteracy and the integration of different classes, races, gender and age. Political awareness and the strengthening of political and economic participation of the Nicaraguan people was also a central goal of the Literacy Campaign. The campaign was a key component of the FSLN's cultural transformation agenda.

The basic reader which was disseminated and used by teacher was called "Dawn of the People" based on the themes of Sandino, Carlos Fonseca, and the Sandinista struggle against imperialism and defending the revolution. Political education was aimed at creating a new social values based on the principles of Sandinista socialism, such as social solidarity, worker's democracy, egalitarianism, and anti-imperialism.

Health care was another area where the Sandinistas made significant improvements and are widely recognized for this accomplishment, e.g. by Oxfam. In this area Cuba also played a role by again offering expertise to Nicaragua. Over 1,500 Cuban doctors worked in Nicaragua and provided more than five million consultations. Cuban personnel were essential in the elimination of polio, the decrease in whooping cough, rubella, measles and the lowering of the infant mortality rate. Gary Prevost states that Cuban personnel made it possible for Nicaragua to have a national health care system that reached the majority of its citizens.

Cuba has participated in the training of Nicaraguan workers in the use of new machinery imported to Nicaragua. The Nicaraguan revolution caused the United States to oppose the country's government; therefore the Sandinistas would not receive any aid from the United States. The United States embargo against Nicaragua, imposed by the Reagan administration in May 1985, made it impossible for Nicaragua to receive spare parts for US-made machines, so this led Nicaragua to look to other countries for help. Cuba was the best choice because of the shared language and proximity and also because it had imported similar machinery over the years. Nicaraguans went to Cuba for short periods of three to six months and this training involved close to 3,000 workers. Countries such as the UK, sent farm equipment to Nicaragua.

Cuba helped Nicaragua in large projects such as building roads, power plants and sugar mills. Cuba also attempted to help Nicaragua build the first overland route linking Nicaraguas Atlantic and Pacific coasts. The road was meant to traverse 420 kilometres (260 mi) of jungle, but completion of the road and usage was hindered by the Contra war, and it was never completed.

Another significant feat was the building of the Tipitapa-Malacatoya sugar mill. It was completed and inaugurated during a visit by Fidel Castro in January 1985. The plant used the newest technology available and was built by workers trained in Cuba. Also during this visit Castro announced that all debts incurred on this project were absolved. Cuba also provided technicians to aid in the sugar harvest and assist in the rejuvenation of several old sugar mills. Cubans also assisted in building schools and similar projects.

After the Nicaraguan revolution, the Sandinista government established a Ministry of Culture in 1980. The ministry was spearheaded by Ernesto Cardenal, a poet and priest. The ministry was established in order to socialize the modes of cultural production. This extended to art forms including dance, music, art, theatre and poetry. The project was created to democratize culture on a national level. The aim of the ministry was to "democratize art" by making it accessible to all social classes as well as protecting the right of the oppressed to produce, distribute and receive art. In particular, the ministry was devoted to the development of working class and "campesino", or peasant culture. Therefore, the ministry sponsored cultural workshops throughout the country until October 1988 when the Ministry of Culture was integrated into the Ministry of Education because of financial troubles.

The objective of the workshops was to recognize and celebrate neglected forms of artistic expression. The ministry created a program of cultural workshops known as, "Casas de Cultura and Centros Populares de Cultura". The workshops were set up in poor neighbourhoods and rural areas and advocated universal access and consumption of art in Nicaragua. The ministry assisted in the creation of theatre groups, folklore and artisanal production, song groups, new journals of creation and cultural criticism, and training programs for cultural workers. The ministry created a Sandinista daily newspaper named "Barricada" and its weekly cultural addition named "Ventana" along with the "Television Sandino, Radio Sandino" and the Nicaraguan film production unit called the INCINE. There were existing papers which splintered after the revolution and produced other independent, pro-Sandinista newspapers, such as "El Nuevo Diario" and its literary addition "Nuevo Amanecer Cultural". Editorial Nueva Nicaragua, a state publishing house for literature, was also created. The ministry collected and published political poetry of the revolutionary period, known as testimonial narrative, a form of literary genre that recorded the experiences of individuals in the course of the revolution.

The ministry developed a new anthology of Rubén Darío, a Nicaraguan poet and writer, established a Rubén Darío prize for Latin American writers, the Leonel Rugama prize for young Nicaraguan writers, as well as public poetry readings and contests, cultural festivals and concerts. The Sandinista regime tried to keep the revolutionary spirit alive by empowering its citizens artistically. At the time of its inception, the Ministry of Culture needed, according to Cardenal, "to bring a culture to the people who were marginalized from it. We want a culture that is not the culture of an elite, of a group that is considered 'cultivated', but rather of an entire people." Nevertheless, the success of the Ministry of Culture had mixed results and by 1985 criticism arose over artistic freedom in the poetry workshops. The poetry workshops became a matter for criticism and debate. Critics argued that the ministry imposed too many principles and guidelines for young writers in the workshop, such as, asking them to avoid metaphors in their poetry and advising them to write about events in their everyday life. Critical voices came from established poets and writers represented by the "Asociacion Sandinista de Trabajadores de la Cultura" (ASTC) and from the "Ventana" both of which were headed by Rosario Murillo. They argued that young writers should be exposed to different poetic styles of writing and resources developed in Nicaragua and elsewhere. Furthermore, they argued that the ministry exhibited a tendency that favored and fostered political and testimonial literature in post-revolutionary Nicaragua.

The new government, formed in 1979 and dominated by the Sandinistas, resulted in a socialist model of economic development. The new leadership was conscious of the social inequities produced during the previous thirty years of unrestricted economic growth and was determined to make the country's workers and peasants, the "economically underprivileged", the prime beneficiaries of the new society. Consequently, in 1980 and 1981, unbridled incentives to private investment gave way to institutions designed to redistribute wealth and income. Private property would continue to be allowed, but all land belonging to the Somozas was confiscated.

However, the ideology of the Sandinistas put the future of the private sector and of private ownership of the means of production in doubt. Although under the new government both public and private ownership were accepted, government spokespersons occasionally referred to a reconstruction phase in the country's development, in which property owners and the professional class would be tapped for their managerial and technical expertise. After reconstruction and recovery, the private sector would give way to expanded public ownership in most areas of the economy. Despite such ideas, which represented the point of view of a faction of the government, the Sandinista government remained officially committed to a mixed economy.

Economic growth was uneven in the 1980s. Restructuring of the economy and the rebuilding immediately following the end of the civil war caused the GDP to rise about 5 percent in 1980 and 1981. Each year from 1984 to 1990, however, showed a drop in the GDP. Reasons for the contraction included the reluctance of foreign banks to offer new loans, the diversion of funds to fight the new insurrection against the government, and, after 1985, the total embargo on trade with the United States, formerly Nicaragua's largest trading partner. After 1985 the government chose to fill the gap between decreasing revenues and mushrooming military expenditures by printing large amounts of paper money. Inflation rose rapidly, peaking in 1988 at more than 14,000 percent annually.

Measures taken by the government to lower inflation were largely defeated by natural disaster. In early 1988, the administration of Daniel José Ortega Saavedra (Sandinista junta coordinator 1979–85, president 1985–90) established an austerity program to lower inflation. Price controls were tightened, and a new currency was introduced. As a result, by August 1988, inflation had dropped to an annual rate of 240 percent. The following month, however, Hurricane Joan cut a path directly across the center of the country. Damage was extensive, and the government's program of large spending to repair the infrastructure destroyed its anti-inflation measures.

In its eleven years in power, the Sandinista government never overcame most of the economic inequalities that it inherited from the Somoza era. Years of war, policy missteps, natural disasters, and the effects of the United States trade embargo all hindered economic development.

The women of Nicaragua prior to, during and after the revolution played a prominent role within the nation's society as they have commonly been recognized, throughout history and across all Latin American states, as its backbone. Nicaraguan women were therefore directly affected by all of the positive and negative events that took place during this revolutionary period. The victory of the Sandinista National Liberation Front (FSLN) in 1979 brought about major changes and gains for women, mainly in legislation, broad educational opportunities, training programs for working women, childcare programs to help women enter the work force and greatly increased participation and leadership positions in a range of political activities. This, in turn, reduced the burdens that the women of Nicaragua were faced with prior to the revolution. During the Sandinista government, women were more active politically. The large majority of members of the neighborhood committees (Comités de Defensa Sandinista) were women. By 1987, 31% of the executive positions in the Sandinista government, 27% of the leadership positions of the FSLN, and 25% of the FSLN's active membership were women.

Supporters of the Sandinistas see their era as characterized by the creation and implementation of successful social programs which were free and made widely available to the entire nation. Some of the more successful programs for women that were implemented by the Sandinistas were in the areas of education (see; Nicaraguan Literacy Campaign), health, and housing. Providing subsidies for basic foodstuffs and the introduction of mass employment were also contributions of the FSLN. The Sandinistas were particularly advantageous for the women of Nicaraguan as they promoted progressive views on gender as early as 1969 claiming that the revolution would "abolish the detestable discrimination that women have suffered with regard to men and establish economic, political and cultural equality between men and women". This was evident as the FSLN began integrating women into their ranks by 1967, unlike other left-wing guerilla groups in the region. This goal was not fully reached because the roots of gender inequality were not explicitly challenged. Women's participation within the public sphere was also substantial, as many took part in the armed struggle as part of the FSLN or as part of counter-revolutionary forces.

Nicaraguan women organized independently in support of the revolution and their cause. Some of those organizations were the Socialist Party (1963), Federación Democrática (which support the FSLN in rural areas), and Luisa Amanda Espinoza Association of Nicaraguan Women (, AMNLAE). However, since Daniel Ortega, was defeated in the 1990 election by the United Nicaraguan Opposition (UNO) coalition headed by Violeta Chamorro, the situation for women in Nicaragua was seriously altered. In terms of women and the labor market, by the end of 1991 AMNLAE reported that almost 16,000 working women—9,000 agricultural laborers, 3,000 industrial workers, and 3,800 civil servants, including 2,000 in health, 800 in education, and 1,000 in administration—had lost their jobs. The change in government also resulted in the drastic reduction or suspension of all Nicaraguan social programs, which brought back the burdens characteristic of pre-revolutionary Nicaragua. The women were forced to maintain and supplement community social services on their own without economic aid or technical and human resource.

Between 2007 and 2018 under Sandinista administrations, Nicaragua has advanced from 62nd to 6th in the world in terms of gender equality, according to the Global Gender Gap Report from the World Economic Forum.

The Roman Catholic Church's relationship with the Sandinistas was extremely complex. Initially, the Church was committed to supporting the Somoza regime. The Somoza dynasty was willing to secure the Church a prominent place in society as long as it did not attempt to subvert the authority of the regime. Under the constitution of 1950 the Roman Catholic Church was recognized as the official religion and church-run schools flourished. It was not until the late 1970s that the Church began to speak out against the corruption and human rights abuses that characterized the Somoza regime.

The Catholic hierarchy initially disapproved of the Sandinistas' revolutionary struggle against the Somoza dynasty. The revolutionaries were perceived as proponents of "godless communism" that posed a threat to the traditionally privileged place that the Church occupied within Nicaraguan society. Nevertheless, the increasing corruption and repression characterizing the Somoza rule and the likelihood that the Sandinistas would emerge victorious ultimately influenced Archbishop Miguel Obando y Bravo to declare formal support for the Sandinistas' armed struggle. Throughout the revolutionary struggle, the Sandinistas had the grassroots support of clergy who were influenced by the reforming zeal of Vatican II and dedicated to a "preferential option for the poor" (for comparison, see liberation theology). Numerous Christian base communities (CEBs) were created in which lower level clergy and laity took part in consciousness raising initiatives to educate the peasants about the institutionalized violence they were suffering from. Some priests took a more active role in supporting the revolutionary struggle. For example, Father Gaspar García Laviana took up arms and became a member of FSLN.

Soon after the Sandinistas assumed power, the hierarchy began to oppose the Sandinistas government. The Archbishop was a vocal source of domestic opposition. The hierarchy was alleged to be motivated by fear of the emergence of the 'popular church' which challenged their centralized authority. The hierarchy also opposed social reforms implemented by the Sandinistas to aid the poor, allegedly because they saw it as a threat to their traditionally privileged position within society. In response to this perceived opposition, the Sandinistas shut down the church-run Radio Católica radio station on multiple occasions.

The Sandinistas' relationship with the Roman Catholic Church deteriorated as the Contra War continued. The hierarchy refused to speak out against the counterrevolutionary activities of the contras and failed to denounce American military aid. State media accused the Catholic Church of being reactionary and supporting the Contras. According to former President Ortega, "The conflict with the church was strong, and it costs us, but I don't think it was our fault. ... There were so many people being wounded every day, so many people dying, and it was hard for us to understand the position of the church hierarchy in refusing to condemn the contras." The hierarchy-state tensions were brought to the fore with Pope John Paul II 1983 visit to Nicaragua. Hostility to the Catholic Church became so great that at one point, FSLN militants shouted down Pope John Paul II as he tried to say Mass. Therefore, while the activities of the Catholic church contributed to the success of the Sandinista revolution, the hierarchy's opposition was a major factor in the downfall of the revolutionary government.

On August 23, 2020, Bishop Silvio Báez, who had been outside of Nicaragua for reasons of security since April 23, 2019, accused President Ortega of being a dictator. The "Centro Nicaragüense de Derechos Humanos" (Nicaraguan Human Rights Center, Cenidh) said that the Catholic Church had been the victim of 24 attacks since April 2018, including a fire that began in the Immaculate Conception Cathedral when a Molotov cocktail was thrown at a sacred image of the Blood of Christ on July 31, 2020.

"Time" magazine in 1983 published reports of human rights violations in an article which stated that "According to Nicaragua's Permanent Commission on Human Rights, the regime detains several hundred people a month; about half of them are eventually released, but the rest simply disappear." "Time" also interviewed a former deputy chief of Nicaraguan military counterintelligence, who stated that he had fled Nicaragua after being ordered to kill 800 Miskito prisoners and make it look like they had died in combat. Another article described Sandinista neighbourhood "Defense Committees", modeled on similar Cuban Committees for the Defense of the Revolution, which according to critics were used to unleash mobs on anyone who was labeled a counterrevolutionary. Nicaragua's only opposition newspaper, La Prensa, was subject to strict censorship. The newspaper's editors were forbidden to print anything negative about the Sandinistas either at home or abroad.

Nicaragua's Permanent Commission on Human Rights reported 2,000 murders in the first six months and 3,000 disappearances in the first few years. It has since documented 14,000 cases of torture, rape, kidnapping, mutilation and murder.

The Inter-American Commission on Human Rights (IACHR) in a 1981 report found evidence for mass executions in the period following the revolution. It stated: "In the Commission's view, while the government of Nicaragua clearly intended to respect the lives of all those defeated in the civil war, during the weeks immediately subsequent to the Revolutionary triumph, when the government was not in effective control, illegal executions took place which violated the right to life, and these acts have not been investigated and the persons responsible have not been punished." The IACHR also stated that: "The Commission is of the view that the new regime did not have, and does not now have, a policy of violating the right to life of political enemies, including among the latter the former guardsmen of the Government of General Somoza, whom a large sector of the population of Nicaragua held responsible for serious human rights violations during the former regime; proof of the foregoing is the abolition of the death penalty and the high number of former guardsmen who were prisoners and brought to trial for crimes that constituted violations of human rights."

A 1983 IACHR report documented allegations of human rights violations against the Miskito Indians, which were alleged to have taken place after opposition forces (the Contras) infiltrated a Miskito village in order to launch attacks against government soldiers, and as part of a subsequent forced relocation program. Allegations included arbitrary imprisonment without trial, "disappearances" of such prisoners, forced relocation, and destruction of property. In late 1981, the CIA conspiracy "Operation Red Christmas" was exposed to separate the Atlantic region from the rest of Nicaragua. Red Christmas aimed to seize territory on Nicaragua's mainland and overthrow the Nicaraguan government. The Nicaraguan government responded to the provocations by transferring 8,500 Miskitos 80 kilometres (50 mi) south to a settlement called Tasba Pri. The U.S. government accused Nicaragua of genocide. The U.S. government produced a photo alleged to show Miskito bodies being burned by Sandinista troops; however, the photo was actually of people killed by Somoza's National Guard in 1978.

The IACHR's 1991 annual report states: "In September 1990, the Commission was informed of the discovery of common graves in Nicaragua, especially in areas where fighting had occurred. The information was provided by the Nicaraguan Pro Human Rights Association, which had received its first complaint in June 1990. By December 1991, that Association had received reports of 60 common graves and had investigated 15 of them. While most of the graves seem to be the result of summary executions by members of the Sandinista People's Army or the State Security, some contain the bodies of individuals executed by the Nicaraguan Resistance."

The IACHR's 1992 annual report contains details of mass graves and investigations which suggest that mass executions had been carried out. One such grave contained 75 corpses of peasants who were believed to have been executed in 1984 by government security forces pretending to be members of the Contras. Another grave was also found in the town of Quininowas which contained six corpses, believed to be an entire family killed by government forces when the town was invaded. A further 72 graves were reported as being found, containing bodies of people, the majority of whom were believed to have been executed by agents of the state and some also by the Contras.

The issue of human rights also became highly politicized at this time as human rights is claimed to be a key component of propaganda created by the Reagan administration to help legitimize its policies in the region. The Inter-Church Committee on Human Rights in Latin America (ICCHRLA) in its "Newsletter" stated in 1985 that: "The hostility with which the Nicaraguan government is viewed by the Reagan administration is an unfortunate development. Even more unfortunate is the expression of that hostility in the destabilization campaign developed by the US administration. ... An important aspect of this campaign is misinformation and frequent allegations of serious human rights violations by the Nicaraguan authorities." Among the accusations in The Heritage Foundation report and the Demokratizatsiya article are references to alleged policies of religious persecution, particularly anti-semitism. The ICCHRLA in its newsletter stated that: "From time to time the current U.S. administration, and private organizations sympathetic to it, have made serious and extensive allegations of religious persecution in Nicaragua. Colleague churches in the United States undertook onsite investigation of these charges in 1984. In their report, the delegation organized by the Division of Overseas Ministries of the National Council of Churches of Christ in the United States concluded that there is 'no basis for the charge of systematic religious persecution'. The delegation 'considers this issue to be a device being used to justify aggressive opposition to the present Nicaraguan government.'" On the other hand, some elements of the Catholic Church in Nicaragua, among them Archbishop Miguel Obando y Bravo, strongly criticized the Sandinistas. The Archbishop stated "The government wants a church that is aligned with the Marxist–Leninist regime." The Inter-American Commission on Human Rights states that: "Although it is true that much of the friction between the Government and the churches arises from positions that are directly or indirectly linked to the political situation of the country, it is also true that statements by high government officials, official press statements, and the actions of groups under the control of the Government have gone beyond the limits within which political discussions should take place and have become obstacles to certain specifically religious activities."

Human Rights Watch also stated in its 1989 report on Nicaragua that: "Under the Reagan administration, U.S. policy toward Nicaragua's Sandinista government was marked by constant hostility. This hostility yielded, among other things, an inordinate amount of publicity about human rights issues. Almost invariably, U.S. pronouncements on human rights exaggerated and distorted the real human rights violations of the Sandinista regime, and exculpated those of the U.S.-supported insurgents, known as the "contras"."

In 1987, a report was published by the UK based NGO Catholic Institute for International Relations (CIIR, now known as "Progressio"), a human rights organization which identifies itself with Liberation theology. The report, "Right to Survive: Human Rights in Nicaragua", discussed the politicization of the human rights issue: "The Reagan administration, with scant regard for the truth, has made a concerted effort to paint as evil a picture as possible of Nicaragua, describing it as a 'totalitarian dungeon'. Supporters of the Sandinistas ... have argued that Nicaragua has a good record of human rights compared with other Central American countries and have compared Nicaragua with other countries at war." The CIIR report refers to estimates made by the NGO Americas Watch which count the number of non-battle related deaths and disappearances for which the government was responsible up to the year 1986 as "close to 300".

According to the CIIR report, Amnesty International and Americas Watch stated that there is no evidence that the use of torture was sanctioned by the Nicaraguan authorities, although prisoners reported the use of conditions of detention and interrogation techniques that could be described as psychological torture. The Red Cross made repeated requests to be given access to prisoners held in state security detention centers, but were refused. The CIIR was critical of the Permanent Commission on Human Rights (PCHR or CPDH in Spanish), claiming that the organisation had a tendency to immediately publish accusations against the government without first establishing a factual basis for the allegations. The CIIR report also questioned the independence of the Permanent Commission on Human Rights, referring to an article in "The Washington Post" which claims that the National Endowment for Democracy, an organization funded by the US government, allocated a concession of US$50,000 for assistance in the translation and distribution outside Nicaragua of its monthly report, and that these funds were administered by the Committee for Democracy in Central America (Prodemca), a US-based organization which later published full-page advertisements in "The Washington Post" and "The New York Times" supporting military aid to the Contras. The Permanent Commission denies that it received any money which it claims was instead used by others for translating and distributing their monthly reports in other nations.

The Nicaraguan-based magazine "Revista Envio", which describes its stance as one of "critical support for the Sandinistas", refers to the report: "The CPDH: Can It Be Trusted?" written by Scottish lawyer Paul Laverty. In the report, Laverty observes that: "The entire board of directors [of the Permanent Commission], are members of or closely identify with the 'Nicaraguan Democratic Coordinating Committee' (Coordinadora), an alliance of the more right wing parties and COSEP, the business organization." He goes on to express concern about CPDH's alleged tendency to provide relatively few names and other details in connection with alleged violations. "According to the 11 monthly bulletins of 1987 (July being the only month without an issue), the CPDH claims to have received information on 1,236 abuses of all types. However, of those cases, only 144 names are provided. The majority of those 144 cases give dates and places of alleged incidents, but not all. This means that only in 11.65% of its cases is there the minimal detail provided to identify the person, place, date, incident and perpetrator of the abuse."

On the other hand, the Inter-American Commission on Human Rights states: "During its on-site observation in 1978 under the Government of General Somoza, the Permanent Commission on Human Rights in Nicaragua, (CPDH) gave the Commission notable assistance, which certainly helped it to prepare its report promptly and correctly." and in 1980 "It cannot be denied that the CPDH continues to play an important role in the protection of human rights, and that a good number of people who consider that their human rights have been ignored by the Government are constantly coming to it." The IACHR continued to meet with representatives of the Permanent Commission and report their assessments in later years.

The Heritage Foundation stated that: "While elements of the Somoza National Guard tortured political opponents, they did not employ psychological torture." The International Commission of Jurists stated that under the Somoza regime cruel physical torture was regularly used in the interrogation of political prisoners.

Throughout the 1980s the Sandinista government was regarded as "Partly Free" by Freedom House.

The United States State Department accused the Sandinistas of many cases of illegal foreign intervention.

The first allegation was supporting the FMLN rebels in El Salvador with safe haven, training, command-and-control headquarters, advice, weapons, ammunition, and other vital supplies. Captured documents, testimonials of former rebels and Sandinistas, aerial photographs, the tracing of captured weapons back to Nicaragua, and captured vehicles from Nicaragua smuggling weapons were cited as evidence. El Salvador was in a civil war in the period in question and the US was heavily supporting the Salvadoran government against the FMLN guerrillas.

There were also accusations of subversive activities in Honduras, Costa Rica, and Colombia, and in the case of Honduras and Costa Rica outright military operations by Nicaraguan troops.

In 2015, Kentucky senator Mitch McConnell claimed during an interview with CNN that John Kerry, then Secretary of State, had visited Nicaragua and met Daniel Ortega and denounced the Reagan Administration's support for the Contras as supporting terrorism during Kerry's tenure as a United States Senator.

During the Nicaraguan Revolution in the 1980s, American Democratic politician and then mayor Bernie Sanders expressed support for the Sandinistas and condemned US support for the Contras, he wrote letters to the group denouncing the US media portrayal of the conflict, and also visited Nicaragua during the war where he attended a Sandinista rally where anti-American chants were reportedly being done.

The flag of the FSLN consists of an upper half in red, a lower half in black, and the letters F S L N in white. It is a modified version of the flag Sandino used in the 1930s, during the war against the U.S. occupation of Nicaragua which consisted of two vertical stripes, equally in size, one red and the other black with a skull (like the traditional Jolly Roger flag). These colors came from the Mexican anarchist movements that Sandino was involved with during his stay in Mexico in the early 1920s. (The traditional flag of anarcho-syndicalism, which joins diagonally the red color of the labour movement and the black color of anarchism, as in the flag of the CNT, is a negation of nationalism and reaffirmation of internationalism.)

In recent times, there has been a dispute between the FSLN and the dissident Sandinista Renovation Movement (MRS) about the use of the red and black flag in public activities. Although the MRS has its own flag (orange with a silhouette of Sandino's hat in black), they also use the red-and-black flag in honor of Sandino's legacy. They state that the red-and-black flag is a symbol of Sandinismo as a whole, not only of the FSLN party.

Sandinista Revolution Day is a national holiday, celebrated on July 19 each year.





I grew up in Urbana three houses down from the Sanderson family – Milton and Virginia and their boys Steve and Joe. My close friend was Joe. His bedroom was filled with aquariums, terrariums, snakes, hamsters, spiders, and butterfly and beetle collections. I envied him like crazy. After college he hit the road. He never made a break from his parents, but they rarely knew where he was. Sometimes he came home and his mother would have to sew $100 bills into the seams of his blue jeans. He disappeared in Nicaragua. His body was later identified as a dead Sandinista freedom fighter. From a nice little house surrounded by evergreens at the other end of Washington Street, he left to look for something he needed to find. I believe in Sean Penn's Christopher McCandless. I grew up with him.



The party has given the following Presidents of the Republic, namely:

Won, getting the 67.20% of the valid votes cast 735.067 votes equivalent to well above the second party of the Democratic Conservative Party (PCD ) who won 154,127 corresponding to 14.00% of the valid votes.
Lost, as 579,886 A total valid votes equivalent to 40.82%, below that obtained by the main opposition Mrs. Violeta Barrios de Chamorro candidate of the National Opposition Union (UNO) who won 777,552 to obtain valid votes equivalent to 54.74%.
Lost, as 669,443 A total valid votes equivalent to 37.75%, below that obtained by his main opponent on Arnoldo Alemán Lacayo candidate of the Liberal Alliance (AL) who won 904,908 to obtain valid votes equivalent to 51.03%.
Lost, as 915,417 A total valid votes equivalent to 42.30%, below that obtained by the main opposition Enrique Bolaños Geyer candidate Liberal Constitutionalist Party (PLC) who won by getting 1,216,863 valid votes equivalent to 56.30%.
Won, getting the 37.99% of the valid votes cast, 930,802 votes equivalent to relatively higher than the two main opposition parties. They were the party of the Second Nicaraguan Liberal Alliance (ALN) with the degree candidate Eduardo Montealegre Rivas who won 693,391 votes recorded corresponding to a 28.30% and third place went to the Constitutionalist Liberal Party with Dr. José Rizo Castellón who earned a total 664,225 of valid votes corresponding to 27.11%.
Won in National Elections held on November 6, 2011, was the amount of 1,569,287 for 62.46% of the total valid votes, at that moment Commander Daniel Ortega became the presidential candidate who won a presidential election with the most votes in the history of Nicaragua, in addition to that obtained a lead of more than 30% of valid votes doubling the number of votes obtained by radial businessman Fabio Gadea Mantilla on behalf of the Independent Liberal Party (PLI) who obtained the amount of 778,889 votes recorded for 31.00%. The big loser of these elections was the former President Arnoldo Alemán Lacayo candidate Liberal Constitutionalist Party (PLC) who was located in a distant third with a 5.91% equivalent to 148,507 votes. These results hint at a continuing and still fluid change in the correlation of political forces in the country.
Won in National Elections held on November 6, 2016, was the amount of 1,806,651 for 72.44% of the total valid votes. In the 2016 Presidential Elections, Commander Daniel Ortega accompanied by Rosario Murillo Zambrana became the Presidential Formula that obtained the most votes in a Presidential Election in the history of Nicaragua, obtaining an advantage of more than 57% on the formula of Secondly, demonstrating that the application of the Christian, Socialist and Solidarity Model of the Government of Reconciliation and National Unity implemented by the Sandinista National Liberation Front has the support of the immense majority of Nicaraguans. 





</doc>
<doc id="29318" url="https://en.wikipedia.org/wiki?curid=29318" title="Streptococcus">
Streptococcus

Streptococcus is a genus of gram-positive ' (plural ) or spherical bacteria that belongs to the family Streptococcaceae, within the order Lactobacillales (lactic acid bacteria), in the phylum Firmicutes. Cell division in streptococci occurs along a single axis, so as they grow, they tend to form pairs or chains that may appear bent or twisted. (Contrast with that of staphylococci, which divide along multiple axes, thereby generating irregular, grape-like clusters of cells.)

The term was coined in 1877 by Viennese surgeon Albert Theodor Billroth (1829–1894), by combining the prefix "strepto-" (from ), together with the suffix "-coccus" (from Modern , from .)

Most streptococci are oxidase-negative and catalase-negative, and many are facultative anaerobes (capable of growth both aerobically and anaerobically).

In 1984, many bacteria formerly grouped in the genus "Streptococcus" were separated out into the genera "Enterococcus" and "Lactococcus". Currently, over 50 species are recognised in this genus. This genus has been found to be part of the salivary microbiome.

In addition to streptococcal pharyngitis (strep throat), certain "Streptococcus" species are responsible for many cases of pink eye, meningitis, bacterial pneumonia, endocarditis, erysipelas, and necrotizing fasciitis (the 'flesh-eating' bacterial infections). However, many streptococcal species are not pathogenic, and form part of the commensal human microbiota of the mouth, skin, intestine, and upper respiratory tract. Streptococci are also a necessary ingredient in producing Emmentaler ("Swiss") cheese.

Species of "Streptococcus" are classified based on their hemolytic properties. Alpha-hemolytic species cause oxidization of iron in hemoglobin molecules within red blood cells, giving it a greenish color on blood agar. Beta-hemolytic species cause complete rupture of red blood cells. On blood agar, this appears as wide areas clear of blood cells surrounding bacterial colonies. Gamma-hemolytic species cause no hemolysis.

Beta-hemolytic streptococci are further classified by Lancefield grouping, a serotype classification (that is, describing specific carbohydrates present on the bacterial cell wall). The 21 described serotypes are named Lancefield groups A to W (excluding I and J). This system of classification was developed by Rebecca Lancefield, a scientist at Rockefeller University.

In the medical setting, the most important groups are the alpha-hemolytic streptococci "S. pneumoniae" and "Streptococcus" "viridans "group, and the beta-hemolytic streptococci of Lancefield groups A and B (also known as “group A strep” and “group B strep”).

Table: Medically relevant streptococci (not all are alpha-hemolytic)
When alpha-hemolysis (α-hemolysis) is present, the agar under the colony will appear dark and greenish due to the conversion of hemoglobin to green biliverdin. "Streptococcus pneumoniae" and a group of oral streptococci ("Streptococcus viridans" or viridans streptococci) display alpha-hemolysis. 
Alpha-hemolysis is also termed incomplete hemolysis or partial hemolysis because the cell membranes of the red blood cells are left intact. This is also sometimes called green hemolysis because of the color change in the agar.



Beta hemolysis (β-hemolysis), sometimes called complete hemolysis, is a complete lysis of red cells in the media around and under the colonies: the area appears lightened (yellow) and transparent. Streptolysin, an exotoxin, is the enzyme produced by the bacteria which causes the complete lysis of red blood cells. There are two types of streptolysin: Streptolysin O (SLO) and streptolysin S (SLS). Streptolysin O is an oxygen-sensitive cytotoxin, secreted by most group A "Streptococcus" (GAS), and interacts with cholesterol in the membrane of eukaryotic cells (mainly red and white blood cells, macrophages, and platelets), and usually results in beta-hemolysis under the surface of blood agar. Streptolysin S is an oxygen-stable cytotoxin also produced by most GAS strains which results in clearing on the surface of blood agar. SLS affects immune cells, including polymorphonuclear leukocytes and lymphocytes, and is thought to prevent the host immune system from clearing infection. "Streptococcus pyogenes", or GAS, displays beta hemolysis.

Some weakly beta-hemolytic species cause intense hemolysis when grown together with a strain of "Staphylococcus". This is called the CAMP test. "Streptococcus agalactiae" displays this property. "Clostridium perfringens" can be identified presumptively with this test. "Listeria monocytogenes" is also positive on sheep's blood agar.
Group A "S. pyogenes" is the causative agent in a wide range of group A streptococcal infections (GAS). These infections may be noninvasive or invasive. The noninvasive infections tend to be more common and less severe. The most common of these infections include streptococcal pharyngitis (strep throat) and impetigo. Scarlet fever is also a noninvasive infection, but has not been as common in recent years. 
The invasive infections caused by group A beta-hemolytic streptococci tend to be more severe and less common. This occurs when the bacterium is able to infect areas where it is not usually found, such as the blood and the organs. The diseases that may be caused include streptococcal toxic shock syndrome, necrotizing fasciitis, pneumonia, and bacteremia. Globally, GAS has been estimated to cause more than 500,000 deaths every year, making it one of the world's leading pathogens.

Additional complications may be caused by GAS, namely acute rheumatic fever and acute glomerulonephritis. Rheumatic fever, a disease that affects the joints, kidneys, and heart valves, is a consequence of untreated strep A infection caused not by the bacterium itself. Rheumatic fever is caused by the antibodies created by the immune system to fight off the infection cross-reacting with other proteins in the body. This "cross-reaction" causes the body to essentially attack itself and leads to the damage above. A similar autoimmune mechanism initiated by Group A beta-hemolytic streptococcal (GABHS) infection is hypothesized to cause pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections (PANDAS), wherein autoimmune antibodies affect the basal ganglia, causing rapid onset of psychiatric, motor, sleep, and other symptoms in pediatric patients.

GAS infection is generally diagnosed with a rapid strep test or by culture.

"S. agalactiae", or group B "streptococcus", GBS, causes pneumonia and meningitis in newborns and the elderly, with occasional systemic bacteremia. Importantly, "Streptococcus agalactiae" is the most common cause of meningitis in infants from one month to three months old. They can also colonize the intestines and the female reproductive tract, increasing the risk for premature rupture of membranes during pregnancy, and transmission of the organism to the infant. The American College of Obstetricians and Gynecologists, American Academy of Pediatrics, and the Centers for Disease Control recommend all pregnant women between 35 and 37 weeks gestation to be tested for GBS. Women who test positive should be given prophylactic antibiotics during labor, which will usually prevent transmission to the infant.

The United Kingdom has chosen to adopt a risk factor-based protocol, rather than the culture-based protocol followed in the US. Current guidelines state that if one or more of the following risk factors is present, then the woman should be treated with "intrapartum" antibiotics:
This protocol results in the administration of intrapartum antibiotics to 15–20% of pregnant women and prevention of 65–70% of cases of early onset GBS sepsis.

This group includes "S. equi", which causes strangles in horses, and "S. zooepidemicus"—"S. equi" is a clonal descendant or biovar of the ancestral "S. zooepidemicus"—which causes infections in several species of mammals, including cattle and horses. "S. dysgalactiae" is also a member of group C, beta-haemolytic streptococci that can cause pharyngitis and other pyogenic infections similar to group A streptococci.

Many former group D streptococci have been reclassified and placed in the genus "Enterococcus" (including "E. faecalis", "E. faecium", "E. durans", and "E. avium"). For example, "Streptococcus faecalis" is now "Enterococcus faecalis". "E. faecalis" is sometimes alpha-hemolytic and "E. faecium" is sometimes beta hemolytic.

The remaining nonenterococcal group D strains include "Streptococcus gallolyticus" and "Streptococcus equinus".

Nonhemolytic streptococci rarely cause illness. However, weakly hemolytic group D beta-hemolytic streptococci and "Listeria monocytogenes" (which is actually a gram-positive bacillus) should not be confused with nonhemolytic streptococci.

Group F streptococci were first described in 1934 by Long and Bliss amongst the "minute haemolytic streptococci". They are also known as "Streptococcus anginosus" (according to the Lancefield classification system) or as members of the "S. milleri" group (according to the European system).

These streptococci are usually, but not exclusively, beta-hemolytic. "Streptococcus dysgalactiae" is the predominant species encountered, particularly in human disease. "S. canis" is an example of a GGS which is typically found on animals, but can cause infection in humans. "S. phocae" is a GGS subspecies that has been found in marine mammals and marine fish species. In marine mammals it has been mainly associated with meningoencephalitis, sepsis, and endocarditis, but is also associated with many other pathologies. Its environmental reservoir and means of transmission in marine mammals is not well characterized.

Group H streptococci cause infections in medium-sized canines. Group H streptococci rarely cause illness unless a human has direct contact with the mouth of a canine. One of the most common ways this can be spread is human-to-canine, mouth-to-mouth contact. However, the canine may lick the human's hand and infection can be spread, as well.

Streptococci have been divided into six groups on the basis of their 16S rDNA sequences: "S. anginosus, S. gallolyticus, S. mitis, S. mutans, S. pyogenes" and "S. salivarius". The 16S groups have been confirmed by whole genome sequencing (see figure). The important pathogens "S. pneumoniae" and "S. pyogenes" belong to the "S. mitis" and "S. pyogenes" groups, respectively, while the causative agent of dental caries, "Streptococcus mutans", is basal to the "Streptococcus" group.

The genomes of hundreds of species have been sequenced. Most "Streptococcus" genomes are 1.8 to 2.3 Mb in size and encode 1,700 to 2,300 proteins. Some important genomes are listed in the table. The four species shown in the table ("S. pyogenes, S. agalactiae, S. pneumoniae", and "S. mutans") have an average pairwise protein sequence identity of about 70%.

Bacteriophages have been described for many species of "Streptococcus". 18 prophages have been described in "S. pneumoniae" that range in size from 38 to 41 kb in size, encoding from 42 to 66 genes each. Some of the first "Streptococcus" phages discovered were Dp-1
and ω1 (alias ω-1).
In 1981 the Cp (Complutense phage 1, officially "Streptococcus virus Cp1", "Picovirinae") family was discovered with Cp-1 as its first member. Dp-1 and Cp-1 infect both "S. pneumoniae" and "S. mitis". However, the host ranges of most "Streptococcus" phages have not been investigated systematically.

Natural genetic transformation involves the transfer of DNA from one bacterium to another through the surrounding medium. Transformation is a complex process dependent on expression of numerous genes. To be capable of transformation a bacterium must enter a special physiologic state referred to as competence. "S. pneumoniae", "S. mitis" and "S. oralis" can become competent, and as a result actively acquire homologous DNA for transformation by a predatory fratricidal mechanism This fratricidal mechanism mainly exploits non-competent siblings present in the same niche Among highly competent isolates of "S. pneumoniae", Li et al. showed that nasal colonization fitness and virulence (lung infectivity) depend on an intact competence system. Competence may allow the streptococcal pathogen to use external homologous DNA for recombinational repair of DNA damages caused by the hosts oxidative attack.




</doc>
