<doc id="11884" url="https://en.wikipedia.org/wiki?curid=11884" title="German language">
German language

German (, ) is a West Germanic language that is mainly spoken in Central Europe. It is the most widely spoken and official or co-official language in Germany, Austria, Switzerland, South Tyrol in Italy, the German-speaking Community of Belgium, and Liechtenstein. It is one of the three official languages of Luxembourg and a co-official language in the Opole Voivodeship in Poland. The German language is most similar to other languages within the West Germanic language branch, including Afrikaans, Dutch, English, the Frisian languages, Low German/Low Saxon, Luxembourgish, and Yiddish. It also contains close similarities in vocabulary to Danish, Norwegian and Swedish, although they belong to the North Germanic group. German is the second most widely spoken Germanic language, after English.

One of the major languages of the world, German is a native language to almost 100 million people worldwide and the most widely spoken native language in the European Union. German is the third most commonly spoken foreign language in the EU after English and French, making it the second biggest language in the EU in terms of overall speakers. German is also the second most widely taught foreign language in the EU after English at primary school level (but third after English and French at lower secondary level), the fourth most widely taught non-English language in the US (after Spanish, French and American Sign Language), the second most commonly used scientific language and the third most widely used language on websites after English and Russian. The German-speaking countries are ranked fifth in terms of annual publication of new books, with one tenth of all books (including e-books) in the world being published in German. In the United Kingdom, German and French are the most sought-after foreign languages for businesses (with 49% and 50% of businesses identifying these two languages as the most useful, respectively).

German is an inflected language with four cases for nouns, pronouns and adjectives (nominative, accusative, genitive, dative), three genders (masculine, feminine, neuter), two numbers (singular, plural), and strong and weak verbs. It derives the majority of its vocabulary from the ancient Germanic branch of the Indo-European language family. Some of its vocabulary is derived from Latin and Greek, and fewer are borrowed from French and Modern English. German is a pluricentric language, with its standardized variants being (German, Austrian, and Swiss Standard German). It is also notable for its broad spectrum of dialects, with many unique varieties existing in Europe and other parts of the world. Italy recognizes all the German-speaking minorities in its territory as national historic minorities and protects the varieties of German spoken in several regions of Northern Italy besides South Tyrol. Due to the limited intelligibility between certain varieties and Standard German, as well as the lack of an undisputed, scientific difference between a "dialect" and a "language", some German varieties or dialect groups (e.g. Low German or Plautdietsch) can be described as either "languages" or "dialects".

Modern Standard German is a West Germanic language in the Germanic branch of the Indo-European languages. The Germanic languages are traditionally subdivided into three branches: North Germanic, East Germanic, and West Germanic. The first of these branches survives in modern Danish, Swedish, Norwegian, Faroese, and Icelandic, all of which are descended from Old Norse. The East Germanic languages are now extinct, and Gothic is the only language in this branch which survives in written texts. The West Germanic languages, however, have undergone extensive dialectal subdivision and are now represented in modern languages such as English, German, Dutch, Yiddish, Afrikaans, and others. 

Within the West Germanic language dialect continuum, the Benrath and Uerdingen lines (running through Düsseldorf-Benrath and Krefeld-Uerdingen, respectively) serve to distinguish the Germanic dialects that were affected by the High German consonant shift (south of Benrath) from those that were not (north of Uerdingen). The various regional dialects spoken south of these lines are grouped as High German dialects "(nos. 29–34 on the map)", while those spoken to the north comprise the Low German/Low Saxon "(nos. 19–24)" and Low Franconian "(no. 25)" dialects. As members of the West Germanic language family, High German, Low German, and Low Franconian can be further distinguished historically as Irminonic, Ingvaeonic, and Istvaeonic, respectively. This classification indicates their historical descent from dialects spoken by the Irminones (also known as the Elbe group), Ingvaeones (or North Sea Germanic group), and Istvaeones (or Weser-Rhine group).

Standard German is based on a combination of Thuringian-Upper Saxon and Upper Franconian and Bavarian dialects, which are Central German and Upper German dialects, belonging to the Irminonic High German dialect group "(nos. 29–34)". German is therefore closely related to the other languages based on High German dialects, such as Luxembourgish (based on Central Franconian dialects – "no. 29"), and Yiddish. Also closely related to Standard German are the Upper German dialects spoken in the southern German-speaking countries, such as Swiss German (Alemannic dialects – "no. 34"), and the various Germanic dialects spoken in the French region of Grand Est, such as Alsatian (mainly Alemannic, but also Central- and Upper Franconian "(no. 32)" dialects) and Lorraine Franconian (Central Franconian – "no. 29").

After these High German dialects, standard German is less closely related to languages based on Low Franconian dialects (e.g. Dutch and Afrikaans) or Low German/Low Saxon dialects (spoken in northern Germany and southern Denmark), neither of which underwent the High German consonant shift. As has been noted, the former of these dialect types is Istvaeonic and the latter Ingvaeonic, whereas the High German dialects are all Irminonic: the differences between these languages and standard German are therefore considerable. Also related to German are the Frisian languages—North Frisian (spoken in Nordfriesland – "no. 28"), Saterland Frisian (spoken in Saterland – "no. 27"), and West Frisian (spoken in Friesland – "no. 26")—as well as the Anglic languages of English and Scots. These Anglo-Frisian dialects are all members of the Ingvaeonic family of West Germanic languages, which did not take part in the High German consonant shift.

The history of the German language begins with the High German consonant shift during the migration period, which separated Old High German (OHG) dialects from Old Saxon. This sound shift involved a drastic change in the pronunciation of both voiced and voiceless stop consonants ("b", "d", "g", and "p", "t", "k", respectively). The primary effects of the shift were the following:

While there is written evidence of the Old High German language in several Elder Futhark inscriptions from as early as the 6th century AD (such as the Pforzen buckle), the Old High German period is generally seen as beginning with the "Abrogans" (written c.765–775), a Latin-German glossary supplying over 3,000 OHG words with their Latin equivalents. After the "Abrogans", the first coherent works written in OHG appear in the 9th century, chief among them being the "Muspilli", the "Merseburg Charms", and the "Hildebrandslied", as well as a number of other religious texts (the "Georgslied", the "Ludwigslied", the "Evangelienbuch", and translated hymns and prayers). The "Muspilli" is a Christian poem written in a Bavarian dialect offering an account of the soul after the Last Judgment, and the "Merseburg Charms" are transcriptions of spells and charms from the pagan Germanic tradition. Of particular interest to scholars, however, has been the "Hildebrandslied", a secular epic poem telling the tale of an estranged father and son unknowingly meeting each other in battle. Linguistically this text is highly interesting due to the mixed use of Old Saxon and Old High German dialects in its composition. The written works of this period stem mainly from the Alamanni, Bavarian, and Thuringian groups, all belonging to the Elbe Germanic group (Irminones), which had settled in what is now southern-central Germany and Austria between the 2nd and 6th centuries during the great migration.

In general, the surviving texts of OHG show a wide range of dialectal diversity with very little written uniformity. The early written tradition of OHG survived mostly through monasteries and scriptoria as local translations of Latin originals; as a result, the surviving texts are written in highly disparate regional dialects and exhibit significant Latin influence, particularly in vocabulary. At this point monasteries, where most written works were produced, were dominated by Latin, and German saw only occasional use in official and ecclesiastical writing.

The German language through the OHG period was still predominantly a spoken language, with a wide range of dialects and a much more extensive oral tradition than a written one. Having just emerged from the High German consonant shift, OHG was also a relatively new and volatile language still undergoing a number of phonetic, phonological, morphological, and syntactic changes. The scarcity of written work, instability of the language, and widespread illiteracy of the time explain the lack of standardization up to the end of the OHG period in 1050.

While there is no complete agreement over the dates of the Middle High German (MHG) period, it is generally seen as lasting from 1050 to 1350. This was a period of significant expansion of the geographical territory occupied by Germanic tribes, and consequently of the number of German speakers. Whereas during the Old High German period the Germanic tribes extended only as far east as the Elbe and Saale rivers, the MHG period saw a number of these tribes expanding beyond this eastern boundary into Slavic territory (known as the "Ostsiedlung"). With the increasing wealth and geographic spread of the Germanic groups came greater use of German in the courts of nobles as the standard language of official proceedings and literature. A clear example of this is the "mittelhochdeutsche Dichtersprache" employed in the Hohenstaufen court in Swabia as a standardized supra-dialectal written language. While these efforts were still regionally bound, German began to be used in place of Latin for certain official purposes, leading to a greater need for regularity in written conventions.

While the major changes of the MHG period were socio-cultural, German was still undergoing significant linguistic changes in syntax, phonetics, and morphology as well (e.g. diphthongization of certain vowel sounds: "hus" (OHG "house")"→haus" (MHG), and weakening of unstressed short vowels to schwa [ə]: "taga" (OHG "days")→"tage" (MHG)).

A great wealth of texts survives from the MHG period. Significantly, these texts include a number of impressive secular works, such as the "Nibelungenlied", an epic poem telling the story of the dragon-slayer Siegfried ( 13th century), and the "Iwein," an Arthurian verse poem by Hartmann von Aue ( 1203), as well as several lyric poems and courtly romances such as "Parzival" and "Tristan". Also noteworthy is the "Sachsenspiegel", the first book of laws written in Middle "Low" German ( 1220). The abundance and especially the secular character of the literature of the MHG period demonstrate the beginnings of a standardized written form of German, as well as the desire of poets and authors to be understood by individuals on supra-dialectal terms.

The Middle High German period is generally seen as ending when the 1346-53 Black Death decimated Europe's population.

Modern German begins with the Early New High German (ENHG) period, which the influential German philologist Wilhelm Scherer dates 1350–1650, terminating with the end of the Thirty Years' War. This period saw the further displacement of Latin by German as the primary language of courtly proceedings and, increasingly, of literature in the German states. While these states were still under the control of the Holy Roman Empire and far from any form of unification, the desire for a cohesive written language that would be understandable across the many German-speaking principalities and kingdoms was stronger than ever. As a spoken language German remained highly fractured throughout this period, with a vast number of often mutually incomprehensible regional dialects being spoken throughout the German states; the invention of the printing press 1440 and the publication of Luther's vernacular translation of the Bible in 1534, however, had an immense effect on standardizing German as a supra-dialectal written language.

The ENHG period saw the rise of several important cross-regional forms of chancery German, one being "gemeine tiutsch," used in the court of the Holy Roman Emperor Maximilian I, and the other being "Meißner Deutsch", used in the Electorate of Saxony in the Duchy of Saxe-Wittenberg. Alongside these courtly written standards, the invention of the printing press led to the development of a number of printers' languages ("Druckersprachen") aimed at making printed material readable and understandable across as many diverse dialects of German as possible. The greater ease of production and increased availability of written texts brought about increased standardization in the written form of German.One of the central events in the development of ENHG was the publication of Luther's translation of the Bible into German (the New Testament was published in 1522; the Old Testament was published in parts and completed in 1534). Luther based his translation primarily on the "Meißner Deutsch" of Saxony, spending much time among the population of Saxony researching the dialect so as to make the work as natural and accessible to German speakers as possible. Copies of Luther's Bible featured a long list of glosses for each region, translating words which were unknown in the region into the regional dialect. Luther said the following concerning his translation method:One who would talk German does not ask the Latin how he shall do it; he must ask the mother in the home, the children on the streets, the common man in the market-place and note carefully how they talk, then translate accordingly. They will then understand what is said to them because it is German. When Christ says 'ex abundantia cordis os loquitur,' I would translate, if I followed the papists, "aus dem Überflusz des Herzens redet der Mund". But tell me is this talking German? What German understands such stuff? No, the mother in the home and the plain man would say, "Wesz das Herz voll ist, des gehet der Mund über".With Luther's rendering of the Bible in the vernacular, German asserted itself against the dominance of Latin as a legitimate language for courtly, literary, and now ecclesiastical subject-matter. Furthermore, his Bible was ubiquitous in the German states: nearly every household possessed a copy. Nevertheless, even with the influence of Luther's Bible as an unofficial written standard, a widely accepted standard for written German did not appear until the middle of the 18th century.

German was the language of commerce and government in the Habsburg Empire, which encompassed a large area of Central and Eastern Europe. Until the mid-19th century, it was essentially the language of townspeople throughout most of the Empire. Its use indicated that the speaker was a merchant or someone from an urban area, regardless of nationality.

Some cities, such as Prague () and Budapest (Buda, ), were gradually Germanized in the years after their incorporation into the Habsburg domain. Others, such as Pozsony (, now Bratislava), were originally settled during the Habsburg period and were primarily German at that time. Prague, Budapest and Bratislava, as well as cities like Zagreb () and Ljubljana (), contained significant German minorities.

In the eastern provinces of Banat, Bukovina, and Transylvania (), German was the predominant language not only in the larger towns – such as (Timișoara), (Sibiu) and (Brașov) – but also in many smaller localities in the surrounding areas.

The most comprehensive guide to the vocabulary of the German language is found within the . This dictionary was created by the Brothers Grimm and is composed of 16 parts which were issued between 1852 and 1860. In 1872, grammatical and orthographic rules first appeared in the "Duden Handbook".

In 1901, the 2nd Orthographical Conference ended with a complete standardization of the German language in its written form and the "Duden Handbook" was declared its standard definition. The (literally, German stage language) had established conventions for German pronunciation in theatres (Bühnendeutsch) three years earlier; however, this was an artificial standard that did not correspond to any traditional spoken dialect. Rather, it was based on the pronunciation of Standard German in Northern Germany, although it was subsequently regarded often as a general prescriptive norm, despite differing pronunciation traditions especially in the Upper-German-speaking regions that still characterize the dialect of the area today – especially the pronunciation of the ending as [ɪk] instead of [ɪç]. In Northern Germany, Standard German was a foreign language to most inhabitants, whose native dialects were subsets of Low German. It was usually encountered only in writing or formal speech; in fact, most of Standard German was a written language, not identical to any spoken dialect, throughout the German-speaking area until well into the 19th century.

Official revisions of some of the rules from 1901 were not issued until the controversial German orthography reform of 1996 was made the official standard by governments of all German-speaking countries. Media and written works are now almost all produced in Standard German (often called , "High German") which is understood in all areas where German is spoken.

Due to the German diaspora as well as German being the second most widely spoken language in Europe and the third most widely taught foreign language in the US and the EU (in upper secondary education) amongst others, the geographical distribution of German speakers (or "Germanophones") spans all inhabited continents. As for the number of speakers of any language worldwide, an assessment is always compromised by the lack of sufficient, reliable data. For an exact, global number of native German speakers, this is further complicated by the existence of several varieties whose status as separate "languages" or "dialects" is disputed for political and/or linguistic reasons, including quantitatively strong varieties like certain forms of Alemannic (e.g., Alsatian) and Low German/Plautdietsch. Depending on the inclusion or exclusion of certain varieties, it is estimated that approximately 90–95 million people speak German as a first language, 10–25 million as a second language, and 75–100 million as a foreign language. This would imply the existence of approximately 175–220 million German speakers worldwide. It is estimated that including every person studying German, regardless of their actual proficiency, would amount to about 280 million people worldwide with at least some knowledge of German.

In Europe, German is the second most widely spoken mother tongue (after Russian) and the second biggest language in terms of overall speakers (after English). The area in central Europe where the majority of the population speaks German as a first language and has German as a (co-)official language is called the "German "Sprachraum"". It comprises an estimated 88 million native speakers and 10 million who speak German as a second language (e.g. immigrants). Excluding regional minority languages, German is the only official language of the following countries:

German is a co-official language of the following countries:

Although expulsions and (forced) assimilation after the two World Wars greatly diminished them, minority communities of mostly bilingual German native speakers exist in areas both adjacent to and detached from the Sprachraum.

Within Europe and Asia, German is a recognized minority language in the following countries:

In France, the High German varieties of Alsatian and Moselle Franconian are identified as "regional languages", but the European Charter for Regional and Minority Languages of 1998 has not yet been ratified by the government. In the Netherlands, the Limburgish, Frisian, and Low German languages are protected regional languages according to the European Charter for Regional and Minority Languages; however, they are widely considered separate languages and neither German nor Dutch dialects.

Namibia was a colony of the German Empire from 1884 to 1919. Mostly descending from German settlers who immigrated during this time, 25–30,000 people still speak German as a native tongue today. The period of German colonialism in Namibia also led to the evolution of a Standard German-based pidgin language called "Namibian Black German", which became a second language for parts of the indigenous population. Although it is nearly extinct today, some older Namibians still have some knowledge of it.

German, along with English and Afrikaans, was a co-official language of Namibia from 1984 until its independence from South Africa in 1990. At this point, the Namibian government perceived Afrikaans and German as symbols of apartheid and colonialism, and decided English would be the sole official language, stating that it was a "neutral" language as there were virtually no English native speakers in Namibia at that time. German, Afrikaans and several indigenous languages became "national languages" by law, identifying them as elements of the cultural heritage of the nation and ensuring that the state acknowledged and supported their presence in the country. Today, German is used in a wide variety of spheres, especially business and tourism, as well as the churches (most notably the German-speaking Evangelical Lutheran Church in Namibia (GELK)), schools (e.g. the ), literature (German-Namibian authors include Giselher W. Hoffmann), radio (the Namibian Broadcasting Corporation produces radio programs in German), and music (e.g. artist EES). The is one of the three biggest newspapers in Namibia and the only German-language daily in Africa.

Mostly originating from different waves of immigration during the 19th and 20th centuries, an estimated 12,000 people speak German or a German variety as a first language in South Africa. One of the largest communities consists of the speakers of "Nataler Deutsch", a variety of Low German concentrated in and around Wartburg. The small town of Kroondal in the North-West Province also has a mostly German-speaking population. The South African constitution identifies German as a "commonly used" language and the Pan South African Language Board is obligated to promote and ensure respect for it. The community is strong enough that several German International schools are supported, such as the Deutsche Schule Pretoria.

In the United States, the states of North Dakota and South Dakota are the only states where German is the most common language spoken at home after English. German geographical names can be found throughout the Midwest region of the country, such as New Ulm and many other towns in Minnesota; Bismarck (North Dakota's state capital), Munich, Karlsruhe, and Strasburg (named after a town near Odessa in Ukraine) in North Dakota; New Braunfels, Fredericksburg, Weimar, and Muenster in Texas; Corn (formerly Korn), Kiefer and Berlin in Oklahoma; and Kiel, Berlin, and Germantown in Wisconsin.

In Brazil, the largest concentrations of German speakers are in the states of Rio Grande do Sul (where Riograndenser Hunsrückisch developed), Santa Catarina, Paraná, São Paulo and Espírito Santo.


There are important concentrations of German-speaking descendants in Argentina, Chile, Paraguay, Venezuela, Peru, and Bolivia.

The impact of nineteenth century German immigration to southern Chile was such that Valdivia was for a while a Spanish-German bilingual city with ""German signboards and placards alongside the Spanish"". The prestige the German language caused it to acquire qualities of a superstratum in southern Chile. The word for blackberry, a ubiquitous plant in southern Chile, is "murra", instead of the ordinary Spanish words "mora" and "zarzamora", from Valdivia to the Chiloé Archipelago and in some towns in the Aysén Region. The use of "rr" is an adaptation of guttural sounds found in German but difficult to pronounce in Spanish. Similarly the name for marbles, a traditional children's game, is different in Southern Chile compared to areas further north. From Valdivia to the Aysén Region this game is called "bochas", in contrast to the word "bolitas" used further north. The word "bocha" is likely a derivative of the German "Bocciaspiel".

In Australia, the state of South Australia experienced a pronounced wave of immigration in the 1840s from Prussia (particularly the Silesia region). With the prolonged isolation from other German speakers and contact with Australian English, a unique dialect known as Barossa German developed, spoken predominantly in the Barossa Valley near Adelaide. Usage of German sharply declined with the advent of World War I, due to the prevailing anti-German sentiment in the population and related government action. It continued to be used as a first language into the 20th century, but its use is now limited to a few older speakers.

German migration to New Zealand in the 19th century was less pronounced than migration from Britain, Ireland, and perhaps even Scandinavia. Despite this there were significant pockets of German-speaking communities which lasted until the first decades of the 20th century. German speakers settled principally in Puhoi, Nelson, and Gore. At the last census (2013), 36,642 people in New Zealand spoke German, making it the third most spoken European language after English and French and overall the ninth most spoken language.

There is also an important German creole being studied and recovered, named , spoken in the former German colony of German New Guinea, across Micronesia and in northern Australia (i.e. coastal parts of Queensland and Western Australia) by a few elderly people. The risk of its extinction is serious and efforts to revive interest in the language are being implemented by scholars.

Like French and Spanish, German has become a standard second foreign language in the western world. German ranks second (after English) among the best known foreign languages in the EU (on a par with French) as well as in Russia. In terms of student numbers across all levels of education, German ranks third in the EU (after English and French) as well as in the United States (after Spanish and French). In 2015, approximately 15.4 million people were in the process of learning German across all levels of education worldwide. As this number remained relatively stable since 2005 (± 1 million), roughly 75–100 million people able to communicate in German as a foreign language can be inferred, assuming an average course duration of three years and other estimated parameters. According to a 2012 survey, 47 million people within the EU (i.e., up to two-thirds of the 75–100 million worldwide) claimed to have sufficient German skills to have a conversation. Within the EU, not counting countries where it is an official language, German as a foreign language is most popular in Eastern and northern Europe, namely the Czech Republic, Croatia, Denmark, the Netherlands, Slovakia, Hungary, Slovenia, Sweden and Poland. German was once, and to some extent still is, a lingua franca in those parts of Europe.

The basis of Standard German is the Luther Bible, which was translated by Martin Luther and which had originated from the Saxon court language (it being a convenient norm). However, there are places where the traditional regional dialects have been replaced by new vernaculars based on standard German; that is the case in large stretches of Northern Germany but also in major cities in other parts of the country. It is important to note, however, that the colloquial standard German differs greatly from the formal written language, especially in grammar and syntax, in which it has been influenced by dialectal speech.

Standard German differs regionally among German-speaking countries in vocabulary and some instances of pronunciation and even grammar and orthography. This variation must not be confused with the variation of local dialects. Even though the regional varieties of standard German are only somewhat influenced by the local dialects, they are very distinct. German is thus considered a pluricentric language.

In most regions, the speakers use a continuum from more dialectal varieties to more standard varieties depending on the circumstances.

In German linguistics, German dialects are distinguished from varieties of standard German.
The "varieties of standard German" refer to the different local varieties of the pluricentric standard German. They differ only slightly in lexicon and phonology. In certain regions, they have replaced the traditional German dialects, especially in Northern Germany.

In the German-speaking parts of Switzerland, mixtures of dialect and standard are very seldom used, and the use of Standard German is largely restricted to the written language. About 11% of the Swiss residents speak "High German" (Standard German) at home, but this is mainly due to German immigrants. This situation has been called a "medial diglossia". Swiss Standard German is used in the Swiss education system, while Austrian Standard German is officially used in the Austrian education system.

A mixture of dialect and standard does not normally occur in Northern Germany either. The traditional varieties there are Low German, whereas Standard German is a High German "variety". Because their linguistic distance is greater, they do not mesh with Standard German the way that High German dialects (such as Bavarian, Swabian, and Hessian) can.

The German dialects are the traditional local varieties of the language; many of them are not mutually intelligibile with standard German, and they have great differences in lexicon, phonology, and syntax. If a narrow definition of language based on mutual intelligibility is used, many German dialects are considered to be separate languages (for instance in the "Ethnologue"). However, such a point of view is unusual in German linguistics.

The German dialect continuum is traditionally divided most broadly into High German and Low German, also called Low Saxon. However, historically, High German dialects and Low Saxon/Low German dialects do not belong to the same language. Nevertheless, in today's Germany, Low Saxon/Low German is often perceived as a dialectal variation of Standard German on a functional level even by many native speakers. The same phenomenon is found in the eastern Netherlands, as the traditional dialects are not always identified with their Low Saxon/Low German origins, but with Dutch.

The variation among the German dialects is considerable, with often only neighbouring dialects being mutually intelligible. Some dialects are not intelligible to people who know only Standard German. However, all German dialects belong to the dialect continuum of High German and Low Saxon.

Middle Low German was the lingua franca of the Hanseatic League. It was the predominant language in Northern Germany until the 16th century. In 1534, the Luther Bible was published. The translation is considered to be an important step towards the evolution of the Early New High German. It aimed to be understandable to a broad audience and was based mainly on Central and Upper German varieties. The Early New High German language gained more prestige than Low German and became the language of science and literature. Around the same time, the Hanseatic League, based around northern ports, lost its importance as new trade routes to Asia and the Americas were established, and the most powerful German states of that period were located in Middle and Southern Germany.

The 18th and 19th centuries were marked by mass education in Standard German in schools. Gradually, Low German came to be politically viewed as a mere dialect spoken by the uneducated. Today, Low Saxon can be divided in two groups: Low Saxon varieties with a reasonable level of Standard German influence and varieties of Standard German with a Low Saxon influence known as . Sometimes, Low Saxon and Low Franconian varieties are grouped together because both are unaffected by the High German consonant shift. However, the proportion of the population who can understand and speak it has decreased continuously since World War II. The largest cities in the Low German area are Hamburg and Dortmund.

The Low Franconian dialects are the dialects that are more closely related to Dutch than to Low German. Most of the Low Franconian dialects are spoken in the Netherlands and Belgium, where they are considered as dialects of Dutch, which is itself a Low Franconian language. In Germany, Low Franconian dialects are spoken in the northwest of North Rhine-Westphalia, along the Lower Rhine. The Low Franconian dialects spoken in Germany are referred to as Meuse-Rhenish or Low Rhenish. In the north of the German Low Franconian language area, North Low Franconian dialects (also referred to as Cleverlands or as dialects of South Guelderish) are spoken. These dialects are more closely related to Dutch (also North Low Franconian) than the South Low Franconian dialects (also referred to as East Limburgish and, east of the Rhine, Bergish), which are spoken in the south of the German Low Franconian language area. The South Low Franconian dialects are more closely related to Limburgish than to Dutch, and are transitional dialects between Low Franconian and Ripuarian (Central Franconian). The East Bergish dialects are the easternmost Low Franconian dialects, and are transitional dialects between North- and South Low Franconian, and Westphalian (Low German), with most of their features being North Low Franconian. The largest cities in the German Low Franconian area are Düsseldorf and Duisburg.

The High German dialects consist of the Central German, High Franconian, and Upper German dialects. The High Franconian dialects are transitional dialects between Central and Upper German. The High German varieties spoken by the Ashkenazi Jews have several unique features and are considered as a separate language, Yiddish, written with the Hebrew alphabet.

The Central German dialects are spoken in Central Germany, from Aachen in the west to Görlitz in the east. They consist of Franconian dialects in the west (West Central German) and non-Franconian dialects in the east (East Central German). Modern Standard German is mostly based on Central German dialects.

The Franconian, West Central German dialects are the Central Franconian dialects (Ripuarian and Moselle Franconian) and the Rhine Franconian dialects (Hessian and Palatine). These dialects are considered as
Luxembourgish as well as the Transylvanian Saxon dialect spoken in Transylvania are based on Moselle Franconian dialects. The largest cities in the Franconian Central German area are Cologne and Frankfurt.

Further east, the non-Franconian, East Central German dialects are spoken (Thuringian, Upper Saxon, Ore Mountainian, and Lusatian-New Markish, and earlier, in the then German-speaking parts of Silesia also Silesian, and in then German southern East Prussia also High Prussian). The largest cities in the East Central German area are Berlin and Leipzig.

The High Franconian dialects are transitional dialects between Central and Upper German. They consist of the East and South Franconian dialects.

The East Franconian dialect branch is one of the most spoken dialect branches in Germany. These dialects are spoken in the region of Franconia and in the central parts of Saxon Vogtland. Franconia consists of the Bavarian districts of Upper, Middle, and Lower Franconia, the region of South Thuringia (Thuringia), and the eastern parts of the region of Heilbronn-Franken (Tauber Franconia and Hohenlohe) in Baden-Württemberg. The largest cities in the East Franconian area are Nuremberg and Würzburg.

South Franconian is mainly spoken in northern Baden-Württemberg in Germany, but also in the northeasternmost part of the region of Alsace in France. While these dialects are considered as dialects of German in Baden-Württemberg, they are considered as dialects of Alsatian in Alsace (most Alsatian dialects are Low Alemannic, however). The largest cities in the South Franconian area are Karlsruhe and Heilbronn.

The Upper German dialects are the Alemannic dialects in the west and the Bavarian dialects in the east.

Alemannic dialects are spoken in Switzerland (High Alemannic in the densely populated Swiss Plateau, in the south also Highest Alemannic, and Low Alemannic in Basel), Baden-Württemberg (Swabian and Low Alemannic, in the southwest also High Alemannic), Bavarian Swabia (Swabian, in the southwesternmost part also Low Alemannic), Vorarlberg (Low, High, and Highest Alemannic), Alsace (Low Alemannic, in the southernmost part also High Alemannic), Liechtenstein (High and Highest Alemannic), and in the Tyrolean district of Reutte (Swabian). The Alemannic dialects are considered as Alsatian in Alsace. The largest cities in the Alemannic area are Stuttgart and Zürich.

Bavarian dialects are spoken in Austria (Vienna, Lower and Upper Austria, Styria, Carinthia, Salzburg, Burgenland, and in most parts of Tyrol), Bavaria (Upper and Lower Bavaria as well as Upper Palatinate), South Tyrol, southwesternmost Saxony (Southern Vogtlandian), and in the Swiss village of Samnaun. The largest cities in the Bavarian area are Vienna and Munich.

German is a fusional language with a moderate degree of inflection, with three grammatical genders; as such, there can be a large number of words derived from the same root.

German nouns inflect by case, gender, and number:

This degree of inflection is considerably less than in Old High German and other old Indo-European languages such as Latin, Ancient Greek, and Sanskrit, and it is also somewhat less than, for instance, Old English, modern Icelandic, or Russian. The three genders have collapsed in the plural. With four cases and three genders plus plural, there are 16 permutations of case and gender/number of the article (not the nouns), but there are only six forms of the definite article, which together cover all 16 permutations. In nouns, inflection for case is required in the singular for strong masculine and neuter nouns only in the genitive and in the dative (only in fixed or archaic expressions), and even this is losing ground to substitutes in informal speech. The singular dative noun ending is considered archaic or at least old-fashioned in almost all contexts and is almost always dropped in writing, except in poetry, songs, proverbs, and other petrified forms. Weak masculine nouns share a common case ending for genitive, dative, and accusative in the singular. Feminine nouns are not declined in the singular. The plural has an inflection for the dative. In total, seven inflectional endings (not counting plural markers) exist in German: .

In German orthography, nouns and most words with the syntactical function of nouns are capitalised to make it easier for readers to determine the function of a word within a sentence ( – "On Friday I went shopping.";  – "One day he finally showed up.") This convention is almost unique to German today (shared perhaps only by the closely related Luxembourgish language and several insular dialects of the North Frisian language), but it was historically common in other languages such as Danish (which abolished the capitalization of nouns in 1948) and English.

Like the other Germanic languages, German forms noun compounds in which the first noun modifies the category given by the second: ("dog hut"; specifically: "dog kennel"). Unlike English, whose newer compounds or combinations of longer nouns are often written "open" with separating spaces, German (like some other Germanic languages) nearly always uses the "closed" form without spaces, for example: ("tree house"). Like English, German allows arbitrarily long compounds in theory (see also English compounds). The longest German word verified to be actually in (albeit very limited) use is , which, literally translated, is "beef labelling supervision duties assignment law" [from (cattle), (meat), (labelling), (supervision), (duties), (assignment), (law)]. However, examples like this are perceived by native speakers as excessively bureaucratic, stylistically awkward, or even satirical.

The inflection of standard German verbs includes:

The meaning of basic verbs can be expanded and sometimes radically changed through the use of a number of prefixes. Some prefixes have a specific meaning; the prefix ' refers to destruction, as in (to tear apart), (to break apart), (to cut apart). Other prefixes have only the vaguest meaning in themselves; ' is found in a number of verbs with a large variety of meanings, as in (to try) from (to seek), (to interrogate) from (to take), (to distribute) from (to share), (to understand) from (to stand).

Other examples include the following:

Many German verbs have a separable prefix, often with an adverbial function. In finite verb forms, it is split off and moved to the end of the clause and is hence considered by some to be a "resultative particle". For example, , meaning "to go along", would be split, giving (Literal: "Go you with?"; Idiomatic: "Are you going along?").

Indeed, several parenthetical clauses may occur between the prefix of a finite verb and its complement (ankommen = to arrive, er kam an = he arrived, er ist angekommen = he has arrived):
A selectively literal translation of this example to illustrate the point might look like this:

German word order is generally with the V2 word order restriction and also with the SOV word order restriction for main clauses. For polar questions, exclamations, and wishes, the finite verb always has the first position. In subordinate clauses, the verb occurs at the very end.

German requires a verbal element (main verb or auxiliary verb) to appear second in the sentence. The verb is preceded by the topic of the sentence. The element in focus appears at the end of the sentence. For a sentence without an auxiliary, these are several possibilities:

The position of a noun in a German sentence has no bearing on its being a subject, an object or another argument. In a declarative sentence in English, if the subject does not occur before the predicate, the sentence could well be misunderstood.

However, German's flexible word order allows one to emphasise specific words:

Normal word order:

Object in front:

Adverb of time in front:

Both time expressions in front:

Another possibility:

Swapped adverbs:

Swapped object:

The flexible word order also allows one to use language "tools" (such as poetic meter and figures of speech) more freely.

When an auxiliary verb is present, it appears in second position, and the main verb appears at the end. This occurs notably in the creation of the perfect tense. Many word orders are still possible:

The main verb may appear in first position to put stress on the action itself. The auxiliary verb is still in second position.

Sentences using modal verbs place the infinitive at the end. For example, the English sentence "Should he go home?" would be rearranged in German to say "Should he (to) home go?" (). Thus, in sentences with several subordinate or relative clauses, the infinitives are clustered at the end. Compare the similar clustering of prepositions in the following (highly contrived) English sentence: "What did you bring that book that I do not like to be read to out of up for?"

German subordinate clauses have all verbs clustered at the end. Given that auxiliaries encode future, passive, modality, and the perfect, very long chains of verbs at the end of the sentence can occur. In these constructions, the past participle formed with is often replaced by the infinitive.

The order at the end of such strings is subject to variation, but the second one in the last example is unusual.

Most German vocabulary is derived from the Germanic branch of the Indo-European language family. However, there is a significant amount of loanwords from other languages, in particular Latin, Greek, Italian, French, and most recently English. In the early 19th century, Joachim Heinrich Campe estimated that one fifth of the total German vocabulary was of French or Latin origin.

Latin words were already imported into the predecessor of the German language during the Roman Empire and underwent all the characteristic phonetic changes in German. Their origin is thus no longer recognizable for most speakers (e.g. , , , , from Latin , , , , ). Borrowing from Latin continued after the fall of the Roman Empire during Christianization, mediated by the church and monasteries. Another important influx of Latin words can be observed during Renaissance humanism. In a scholarly context, the borrowings from Latin have continued until today, in the last few decades often indirectly through borrowings from English. During the 15th to 17th centuries, the influence of Italian was great, leading to many Italian loanwords in the fields of architecture, finance, and music. The influence of the French language in the 17th to 19th centuries resulted in an even greater import of French words. The English influence was already present in the 19th century, but it did not become dominant until the second half of the 20th century.

Thus, Notker Labeo was able to translate Aristotelian treatises into pure (Old High) German in the decades after the year 1000. The tradition of loan translation was revitalized in the 18th century with linguists like Joachim Heinrich Campe, who introduced close to 300 words that are still used in modern German. Even today, there are movements that try to promote the (substitution) of foreign words that are deemed unnecessary with German alternatives. It is claimed that this would also help in spreading modern or scientific notions among the less educated and as well democratise public life.

As in English, there are many pairs of synonyms due to the enrichment of the Germanic vocabulary with loanwords from Latin and Latinized Greek. These words often have different connotations from their Germanic counterparts and are usually perceived as more scholarly.


The size of the vocabulary of German is difficult to estimate. The ("German Dictionary") initiated by Jacob and Wilhelm Grimm already contained over 330,000 headwords in its first edition. The modern German scientific vocabulary is estimated at nine million words and word groups (based on the analysis of 35 million sentences of a corpus in Leipzig, which as of July 2003 included 500 million words in total).

The Duden is the "de facto" official dictionary of the German language, first published by Konrad Duden in 1880. The Duden is updated regularly, with new editions appearing every four or five years. , it was in its 27th edition and in 12 volumes, each covering different aspects such as loanwords, etymology, pronunciation, synonyms, and so forth.The first of these volumes, (German Orthography), has long been the prescriptive source for the spelling of German. The "Duden" has become the bible of the German language, being the definitive set of rules regarding grammar, spelling and usage of German.

The ("Austrian Dictionary"), abbreviated , is the official dictionary of the German language in the Republic of Austria. It is edited by a group of linguists under the authority of the Austrian Federal Ministry of Education, Arts and Culture (). It is the Austrian counterpart to the German "Duden" and contains a number of terms unique to Austrian German or more frequently used or differently pronounced there. A considerable amount of this "Austrian" vocabulary is also common in Southern Germany, especially Bavaria, and some of it is used in Switzerland as well. Since the 39th edition in 2001 the orthography of the has been adjusted to the German spelling reform of 1996. The dictionary is also officially used in the Italian province of South Tyrol.

This is a selection of cognates in both English and German. Instead of the usual infinitive ending "-en", German verbs are indicated by a hyphen after their stems. Words that are written with capital letters in German are nouns.

German is written in the Latin alphabet. In addition to the 26 standard letters, German has three vowels with an umlaut mark, namely "ä", "ö" and "ü", as well as the eszett or (sharp s): "ß". In Switzerland and Liechtenstein, "ss" is used instead of "ß". Since "ß" can never occur at the beginning of a word, it has no traditional uppercase form.

Written texts in German are easily recognisable as such by distinguishing features such as umlauts and certain orthographical features – German is the only major language that capitalizes all nouns, a relic of a widespread practice in Northern Europe in the early modern era (including English for a while, in the 1700s) – and the frequent occurrence of long compounds. Because legibility and convenience set certain boundaries, compounds consisting of more than three or four nouns are almost exclusively found in humorous contexts. (In contrast, although English can also string nouns together, it usually separates the nouns with spaces. For example, "toilet bowl cleaner".)

Before the German orthography reform of 1996, "ß" replaced "ss" after long vowels and diphthongs and before consonants, word-, or partial-word endings. In reformed spelling, "ß" replaces "ss" only after long vowels and diphthongs.

Since there is no traditional capital form of "ß", it was replaced by "SS" when capitalization was required. For example, (tape measure) became in capitals. An exception was the use of ß in legal documents and forms when capitalizing names. To avoid confusion with similar names, lower case "ß" was maintained (thus "" instead of ""). Capital ß (ẞ) was ultimately adopted into German orthography in 2017, ending a long orthographic debate (thus " and ").

Umlaut vowels (ä, ö, ü) are commonly transcribed with ae, oe, and ue if the umlauts are not available on the keyboard or other medium used. In the same manner ß can be transcribed as ss. Some operating systems use key sequences to extend the set of possible characters to include, amongst other things, umlauts; in Microsoft Windows this is done using Alt codes. German readers understand these transcriptions (although they appear unusual), but they are avoided if the regular umlauts are available, because they are a makeshift and not proper spelling. (In Westphalia and Schleswig-Holstein, city and family names exist where the extra e has a vowel lengthening effect, e.g. "Raesfeld" , "Coesfeld" and "Itzehoe" , but this use of the letter e after a/o/u does not occur in the present-day spelling of words other than proper nouns.)

There is no general agreement on where letters with umlauts occur in the sorting sequence. Telephone directories treat them by replacing them with the base vowel followed by an e. Some dictionaries sort each umlauted vowel as a separate letter after the base vowel, but more commonly words with umlauts are ordered immediately after the same word without umlauts. As an example in a telephone book occurs after but before (because Ä is replaced by Ae). In a dictionary comes after , but in some dictionaries and all other words starting with "Ä" may occur after all words starting with "A". In some older dictionaries or indexes, initial "Sch" and "St" are treated as separate letters and are listed as separate entries after "S", but they are usually treated as S+C+H and S+T.

Written German also typically uses an alternative opening inverted comma (quotation mark) as in .

Until the early 20th century, German was mostly printed in blackletter typefaces (mostly in Fraktur, but also in Schwabacher) and written in corresponding handwriting (for example Kurrent and Sütterlin). These variants of the Latin alphabet are very different from the serif or sans-serif Antiqua typefaces used today, and the handwritten forms in particular are difficult for the untrained to read. The printed forms, however, were claimed by some to be more readable when used for Germanic languages. (Often, foreign names in a text were printed in an Antiqua typeface even though the rest of the text was in Fraktur.) The Nazis initially promoted Fraktur and Schwabacher because they were considered Aryan, but they abolished them in 1941, claiming that these letters were Jewish. It is also believed that the Nazi régime had banned this script as they realized that Fraktur would inhibit communication in the territories occupied during World War II.

The Fraktur script however remains present in everyday life in pub signs, beer brands and other forms of advertisement, where it is used to convey a certain rusticality and antiquity.

A proper use of the long s (), ſ, is essential for writing German text in Fraktur typefaces. Many Antiqua typefaces also include the long s. A specific set of rules applies for the use of long s in German text, but nowadays it is rarely used in Antiqua typesetting. Any lower case "s" at the beginning of a syllable would be a long s, as opposed to a terminal s or short s (the more common variation of the letter s), which marks the end of a syllable; for example, in differentiating between the words (guard-house) and (tube of polish/wax). One can easily decide which "s" to use by appropriate hyphenation, ( vs. ). The long s only appears in lower case.

The orthography reform of 1996 led to public controversy and considerable dispute. The states () of North Rhine-Westphalia and Bavaria refused to accept it. At one point, the dispute reached the highest court, which quickly dismissed it, claiming that the states had to decide for themselves and that only in schools could the reform be made the official rule – everybody else could continue writing as they had learned it. After 10 years, without any intervention by the federal parliament, a major revision was installed in 2006, just in time for the coming school year. In 2007, some traditional spellings were finally invalidated; however, in 2008, many of the old comma rules were again put in force.

The most noticeable change was probably in the use of the letter "ß", called ("Sharp S") or (pronounced "ess-tsett"). Traditionally, this letter was used in three situations:


Examples are , , and . Currently, only the first rule is in effect, making the correct spellings , , and . The word 'foot' has the letter "ß" because it contains a long vowel, even though that letter occurs at the end of a syllable. The logic of this change is that an 'ß' is a single letter whereas 'ss' are two letters, so the same distinction applies as (for example) between the words and .

In German, vowels (excluding diphthongs; see below) are either "short" or "long", as follows:

Short is realized as in stressed syllables (including secondary stress), but as in unstressed syllables. Note that stressed short can be spelled either with "e" or with "ä" (for instance, 'would have' and 'chain' rhyme). In general, the short vowels are open and the long vowels are close. The one exception is the open sound of long "Ä"; in some varieties of standard German, and have merged into , removing this anomaly. In that case, pairs like 'bears/berries' or 'spike (of wheat)/honour' become homophonous (see: Captain Bluebear).

In many varieties of standard German, an unstressed is not pronounced but vocalised to .

Whether any particular vowel letter represents the long or short phoneme is not completely predictable, although the following regularities exist:
Both of these rules have exceptions (e.g. "has" is short despite the first rule; "moon" is long despite the second rule). For an "i" that is neither in the combination "ie" (making it long) nor followed by a double consonant or cluster (making it short), there is no general rule. In some cases, there are regional differences. In central Germany (Hesse), the "o" in the proper name "Hoffmann" is pronounced long, whereas most other Germans would pronounce it short. The same applies to the "e" in the geographical name "Mecklenburg" for people in that region. The word "cities" is pronounced with a short vowel by some (Jan Hofer, ARD Television) and with a long vowel by others (Marietta Slomka, ZDF Television). Finally, a vowel followed by "ch" can be short ( "compartment", "kitchen") or long ( "search", "books") almost at random. Thus, is homographous between "puddle" and "manner of laughing" (colloquial) or "laugh!" (imperative).

German vowels can form the following digraphs (in writing) and diphthongs (in pronunciation); note that the pronunciation of some of them (ei, äu, eu) is very different from what one would expect when considering the component letters:

Additionally, the digraph "ie" generally represents the phoneme , which is not a diphthong. In many varieties, an at the end of a syllable is vocalised. However, a sequence of a vowel followed by such a vocalised is not a phonemic diphthong: "bear", "he", "we", "gate", "short", "words".

In most varieties of standard German, syllables that begin with a vowel are preceded by a glottal stop .

With approximately 26 phonemes, the German consonant system exhibits an average number of consonants in comparison with other languages. One of the more noteworthy ones is the unusual affricate . The consonant inventory of the standard language is shown below.


German does not have any dental fricatives (as English th). The th sound, which the English language still has, disappeared on the continent in German with the consonant shifts between the 8th and 10th centuries. It is sometimes possible to find parallels between English and German by replacing the English th with d in German: "Thank" → in German , "this" and "that" → and , "thou" (old 2nd person singular pronoun) → , "think" → , "thirsty" → and many other examples.

Likewise, the gh in Germanic English words, pronounced in several different ways in modern English (as an f or not at all), can often be linked to German ch: "to laugh" → , "through" → , "high" → , "naught" → , "light" → or , "sight" → , "daughter" → , "neighbour" → .

The German language is used in German literature and can be traced back to the Middle Ages, with the most notable authors of the period being Walther von der Vogelweide and Wolfram von Eschenbach.
The , whose author remains unknown, is also an important work of the epoch. The fairy tales collected and published by Jacob and Wilhelm Grimm in the 19th century became famous throughout the world.

Reformer and theologian Martin Luther, who was the first to translate the Bible into German, is widely credited for having set the basis for the modern "High German" language. Among the best-known poets and authors in German are Lessing, Goethe, Schiller, Kleist, Hoffmann, Brecht, Heine, and Kafka. Fourteen German-speaking people have won the Nobel Prize in literature: Theodor Mommsen, Rudolf Christoph Eucken, Paul von Heyse, Gerhart Hauptmann, Carl Spitteler, Thomas Mann, Nelly Sachs, Hermann Hesse, Heinrich Böll, Elias Canetti, Günter Grass, Elfriede Jelinek, Herta Müller and Peter Handke, making it the second most awarded linguistic region (together with French) after English.

English has taken many loanwords from German, often without any change of spelling (aside from frequently eliminating umlauts and not capitalizing nouns):
Several organisations promote the use and learning of the German language:

The government-backed (named after Johann Wolfgang von Goethe) aims to enhance the knowledge of German culture and language within Europe and the rest of the world. This is done by holding exhibitions and conferences with German-related themes, and providing training and guidance in the learning and use of the German language. For example, the teaches the German language qualification.

The Dortmund-based , founded in 1997, supports the German language and is the largest language association of citizens in the world. The VDS has more than thirty-five thousand members in over seventy countries. Its founder, statistics professor Dr. Walter Krämer, has remained chairperson of the association from its formation.

The German state broadcaster provides radio and television broadcasts in German and 30 other languages across the globe. Its German language services are spoken slowly and thus tailored for learners. also provides an website for teaching German.





</doc>
<doc id="11887" url="https://en.wikipedia.org/wiki?curid=11887" title="Greek language">
Greek language

Greek () is an independent branch of the Indo-European family of languages, native to Greece, Cyprus, Albania, other parts of the Eastern Mediterranean and the Black Sea. It has the longest documented history of any living Indo-European language, spanning at least 3,500 years of written records. Its writing system has been the Greek alphabet for the major part of its history; other systems, such as Linear B and the Cypriot syllabary, were used previously. The alphabet arose from the Phoenician script and was in turn the basis of the Latin, Cyrillic, Armenian, Coptic, Gothic, and many other writing systems.

The Greek language holds an important place in the history of the Western world and Christianity; the canon of ancient Greek literature includes works in the Western canon such as the epic poems "Iliad" and "Odyssey". Greek is also the language in which many of the foundational texts in science, especially astronomy, mathematics and logic and Western philosophy, such as the Platonic dialogues and the works of Aristotle, are composed; the New Testament of the Christian Bible was written in Koiné Greek. Together with the Latin texts and traditions of the Roman world, the study of the Greek texts and society of antiquity constitutes the discipline of Classics.

During antiquity, Greek was a widely spoken lingua franca in the Mediterranean world, West Asia and many places beyond. It would eventually become the official parlance of the Byzantine Empire and develop into Medieval Greek. In its modern form, Greek is the official language in two countries, Greece and Cyprus, a recognized minority language in seven other countries, and is one of the 24 official languages of the European Union. The language is spoken by at least 13.4 million people today in Greece, Cyprus, Italy, Albania, and Turkey and by the Greek diaspora.

Greek roots are often used to coin new words for other languages; Greek and Latin are the predominant sources of international scientific vocabulary.

Greek has been spoken in the Balkan peninsula since around the 3rd millennium BC, or possibly earlier. The earliest written evidence is a Linear B clay tablet found in Messenia that dates to between 1450 and 1350 BC, making Greek the world's oldest recorded living language. Among the Indo-European languages, its date of earliest written attestation is matched only by the now-extinct Anatolian languages.

The Greek language is conventionally divided into the following periods:


In the modern era, the Greek language entered a state of diglossia: the coexistence of vernacular and archaizing written forms of the language. What came to be known as the Greek language question was a polarization between two competing varieties of Modern Greek: Dimotiki, the vernacular form of Modern Greek proper, and Katharevousa, meaning 'purified', a compromise between Dimotiki and Ancient Greek, which was developed in the early 19th century, and was used for literary and official purposes in the newly formed Greek state. In 1976, Dimotiki was declared the official language of Greece, having incorporated features of Katharevousa and giving birth to Standard Modern Greek, which is used today for all official purposes and in education.

The historical unity and continuing identity between the various stages of the Greek language are often emphasized. Although Greek has undergone morphological and phonological changes comparable to those seen in other languages, never since classical antiquity has its cultural, literary, and orthographic tradition been interrupted to the extent that one can speak of a new language emerging. Greek speakers today still tend to regard literary works of ancient Greek as part of their own rather than a foreign language. It is also often stated that the historical changes have been relatively slight compared with some other languages. According to one estimation, "Homeric Greek is probably closer to Demotic than 12-century Middle English is to modern spoken English".

Greek is spoken today by at least 13 million people, principally in Greece and Cyprus along with a sizable Greek-speaking minority in Albania near the Greek-Albanian border. A significant percentage of Albania's population has some basic knowledge of the Greek language due in part to the Albanian wave of immigration to Greece in the 1980s and '90s. Prior to the Greco-Turkish War and the resulting population exchange in 1923 a very large population of Greek-speakers also existed in Turkey, though very few remain today. A small Greek-speaking community is also found in Bulgaria near the Greek-Bulgarian border. Greek is also spoken worldwide by the sizable Greek diaspora which as notable communities in the United States, Australia, Canada, South Africa, Chile, Brazil, Argentina, Russia, Ukraine, the United Kingdom, and throughout the European Union, especially in Germany.

Historically, significant Greek-speaking communities and regions were found throughout the Eastern Mediterranean, in what are today Southern Italy, Turkey, Cyprus, Syria, Lebanon, Israel, Egypt, and Libya; in the area of the Black Sea, in what are today Turkey, Bulgaria, Romania, Ukraine, Russia, Georgia, Armenia, and Azerbaijan; and, to a lesser extent, in the Western Mediterranean in and around colonies such as Massalia, Monoikos, and Mainake.

Greek, in its modern form, is the official language of Greece, where it is spoken by almost the entire population. It is also the official language of Cyprus (nominally alongside Turkish). Because of the membership of Greece and Cyprus in the European Union, Greek is one of the organization's 24 official languages. Furthermore, Greek is officially recognized as official in Dropull and Himara (Albania), and as a minority language all over Albania, as well as in parts of Italy, Armenia, Romania, and Ukraine as a regional or minority language in the framework of the European Charter for Regional or Minority Languages. Greeks are also a recognized ethnic minority in Hungary.

The phonology, morphology, syntax, and vocabulary of the language show both conservative and innovative tendencies across the entire attestation of the language from the ancient to the modern period. The division into conventional periods is, as with all such periodizations, relatively arbitrary, especially because at all periods, Ancient Greek has enjoyed high prestige, and the literate borrowed heavily from it.

Across its history, the syllabic structure of Greek has varied little: Greek shows a mixed syllable structure, permitting complex syllabic onsets but very restricted codas. It has only oral vowels and a fairly stable set of consonantal contrasts. The main phonological changes occurred during the Hellenistic and Roman period (see Koine Greek phonology for details):

In all its stages, the morphology of Greek shows an extensive set of productive derivational affixes, a limited but productive system of compounding and a rich inflectional system. Although its morphological categories have been fairly stable over time, morphological changes are present throughout, particularly in the nominal and verbal systems. The major change in the nominal morphology since the classical stage was the disuse of the dative case (its functions being largely taken over by the genitive). The verbal system has lost the infinitive, the synthetically-formed future, and perfect tenses and the optative mood. Many have been replaced by periphrastic (analytical) forms.

Pronouns show distinctions in person (1st, 2nd, and 3rd), number (singular, dual, and plural in the ancient language; singular and plural alone in later stages), and gender (masculine, feminine, and neuter), and decline for case (from six cases in the earliest forms attested to four in the modern language). Nouns, articles, and adjectives show all the distinctions except for a person. Both attributive and predicative adjectives agree with the noun.

The inflectional categories of the Greek verb have likewise remained largely the same over the course of the language's history but with significant changes in the number of distinctions within each category and their morphological expression. Greek verbs have synthetic inflectional forms for:

Many aspects of the syntax of Greek have remained constant: verbs agree with their subject only, the use of the surviving cases is largely intact (nominative for subjects and predicates, accusative for objects of most verbs and many prepositions, genitive for possessors), articles precede nouns, adpositions are largely prepositional, relative clauses follow the noun they modify and relative pronouns are clause-initial. However, the morphological changes also have their counterparts in the syntax, and there are also significant differences between the syntax of the ancient and that of the modern form of the language. Ancient Greek made great use of participial constructions and of constructions involving the infinitive, and the modern variety lacks the infinitive entirely (instead of having a raft of new periphrastic constructions) and uses participles more restrictively. The loss of the dative led to a rise of prepositional indirect objects (and the use of the genitive to directly mark these as well). Ancient Greek tended to be verb-final, but neutral word order in the modern language is VSO or SVO.

Modern Greek inherits most of its vocabulary from Ancient Greek, which in turn is an Indo-European language, but also includes a number of borrowings from the languages of the populations that inhabited Greece before the arrival of Proto-Greeks, some documented in Mycenaean texts; they include a large number of Greek toponyms. The form and meaning of many words have evolved. Loanwords (words of foreign origin) have entered the language, mainly from Latin, Venetian, and Turkish. During the older periods of Greek, loanwords into Greek acquired Greek inflections, thus leaving only a foreign root word. Modern borrowings (from the 20th century on), especially from French and English, are typically not inflected; other modern borrowings are derived from South Slavic (Macedonian/Bulgarian) and Eastern Romance languages (Aromanian and Megleno-Romanian).

Greek words have been widely borrowed into other languages, including English: "mathematics", "physics", "astronomy", "democracy", "philosophy", "athletics, theatre, rhetoric", "baptism", "evangelist", etc. Moreover, Greek words and word elements continue to be productive as a basis for coinages: "anthropology", "photography", "telephony", "isomer", "biomechanics", "cinematography", etc. and form, with Latin words, the foundation of international scientific and technical vocabulary like all words ending with "–logy" ("discourse"). There are many English words of Greek origin.

Greek is an independent branch of the Indo-European language family. The ancient language most closely related to it may be ancient Macedonian, which many scholars suggest may have been a dialect of Greek itself, but it is so poorly attested that it is difficult to conclude anything about it. Independently of the Macedonian question, some scholars have grouped Greek into Graeco-Phrygian, as Greek and the extinct Phrygian share features that are not found in other Indo-European languages. Among living languages, some Indo-Europeanists suggest that Greek may be most closely related to Armenian (see Graeco-Armenian) or the Indo-Iranian languages (see Graeco-Aryan), but little definitive evidence has been found for grouping the living branches of the family. In addition, Albanian has also been considered somewhat related to Greek and Armenian by some linguists. If proven and recognized, the three languages would form a new Balkan sub-branch with other dead European languages.

Linear B, attested as early as the late 15th century BC, was the first script used to write Greek. It is basically a syllabary, which was finally deciphered by Michael Ventris and John Chadwick in the 1950s (its precursor, Linear A, has not been deciphered and most likely encodes a non-Greek language). The language of the Linear B texts, Mycenaean Greek, is the earliest known form of Greek.

Another similar system used to write the Greek language was the Cypriot syllabary (also a descendant of Linear A via the intermediate Cypro-Minoan syllabary), which is closely related to Linear B but uses somewhat different syllabic conventions to represent phoneme sequences. The Cypriot syllabary is attested in Cyprus from the 11th century BC until its gradual abandonment in the late Classical period, in favor of the standard Greek alphabet.

Greek has been written in the Greek alphabet since approximately the 9th century BC. It was created by modifying the Phoenician alphabet, with the innovation of adopting certain letters to represent the vowels. The variant of the alphabet in use today is essentially the late Ionic variant, introduced for writing classical Attic in 403 BC. In classical Greek, as in classical Latin, only upper-case letters existed. The lower-case Greek letters were developed much later by medieval scribes to permit a faster, more convenient cursive writing style with the use of ink and quill.

The Greek alphabet consists of 24 letters, each with an uppercase (majuscule) and lowercase (minuscule) form. The letter sigma has an additional lowercase form (ς) used in the final position:
In addition to the letters, the Greek alphabet features a number of diacritical signs: three different accent marks (acute, grave, and circumflex), originally denoting different shapes of pitch accent on the stressed vowel; the so-called breathing marks (rough and smooth breathing), originally used to signal presence or absence of word-initial /h/; and the diaeresis, used to mark the full syllabic value of a vowel that would otherwise be read as part of a diphthong. These marks were introduced during the course of the Hellenistic period. Actual usage of the grave in handwriting saw a rapid decline in favor of uniform usage of the acute during the late 20th century, and it has only been retained in typography.

After the writing reform of 1982, most diacritics are no longer used. Since then, Greek has been written mostly in the simplified monotonic orthography (or monotonic system), which employs only the acute accent and the diaeresis. The traditional system, now called the polytonic orthography (or polytonic system), is still used internationally for the writing of Ancient Greek.

In Greek, the question mark is written as the English semicolon, while the functions of the colon and semicolon are performed by a raised point (•), known as the "ano teleia" (). In Greek the comma also functions as a silent letter in a handful of Greek words, principally distinguishing ("ó,ti", 'whatever') from ("óti", 'that').

Ancient Greek texts often used "scriptio continua" ('continuous writing'), which means that ancient authors and scribes would write word after word with no spaces or punctuation between words to differentiate or mark boundaries. Boustrophedon, or bi-directional text, was also used in Ancient Greek.

Greek has occasionally been written in the Latin script, especially in areas under Venetian rule or by Greek Catholics. The term / applies when the Latin script is used to write Greek in the cultural ambit of Catholicism (because / is an older Greek term for West-European dating to when most of (Roman Catholic Christian) West Europe was under the control of the Frankish Empire). / (meaning 'Catholic Chiot') alludes to the significant presence of Catholic missionaries based on the island of Chios. Additionally, the term Greeklish is often used when the Greek language is written in a Latin script in online communications.

The Latin script is nowadays used by the Greek-speaking communities of Southern Italy.

The Yevanic dialect was written by Romaniote and Constantinopolitan Karaite Jews using the Hebrew Alphabet.

Some Greek Muslims from Crete wrote their Cretan Greek in the Arabic alphabet.
The same happened among Epirote Muslims in Ioannina.
This usage is sometimes called aljamiado as when Romance languages are written in the Arabic alphabet.









</doc>
<doc id="11888" url="https://en.wikipedia.org/wiki?curid=11888" title="Golem">
Golem

In Jewish folklore, a golem ( ; ) is an animated anthropomorphic being that is created entirely from inanimate matter (usually clay or mud). The word was used to mean an amorphous, unformed material in Psalms and medieval writing.

The most famous golem narrative involves Judah Loew ben Bezalel, the late-16th-century rabbi of Prague. Many tales differ on how the golem was brought to life and afterward controlled. According to "Moment Magazine", "the golem is a highly mutable metaphor with seemingly limitless symbolism. It can be a victim or villain, Jew or non-Jew, man or woman—or sometimes both. Over the centuries it has been used to connote war, community, isolation, hope, and despair."

The word "golem" occurs once in the Bible in , which uses the word ("golmi"; my golem), that means "my light form", "raw" material, connoting the unfinished human being before God's eyes. The Mishnah uses the term for an uncultivated person: "Seven characteristics are in an uncultivated person, and seven in a learned one," (שבעה דברים בגולם) (Pirkei Avot 5:10 in the Hebrew text; English translations vary). In Modern Hebrew, "golem" is used to mean "dumb" or "helpless". Similarly, it is often used today as a metaphor for a mindless lunk or entity who serves a man under controlled conditions but is hostile to him under others. "Golem" passed into Yiddish as "goylem" to mean someone who is lethargic or beneath a stupor.

The oldest stories of golems date to early Judaism. In the Talmud (Tractate Sanhedrin 38b), Adam was initially created as a golem (גולם) when his dust was "kneaded into a shapeless husk." Like Adam, all golems are created from mud by those close to divinity, but no anthropogenic golem is fully human. Early on, the main disability of the golem was its inability to speak. Sanhedrin 65b describes Rava creating a man ("gavra"). He sent the man to Rav Zeira. Rav Zeira spoke to him, but he did not answer. Rav Zeira said, "You were created by the sages; return to your dust".

During the Middle Ages, passages from the "Sefer Yetzirah" ("Book of Creation") were studied as a means to create and animate a golem, although there is little in the writings of Jewish mysticism that supports this belief. It was believed that golems could be activated by an ecstatic experience induced by the ritualistic use of various letters of the Hebrew Alphabet forming a ""shem"" (any one of the Names of God), wherein the "shem" was written on a piece of paper and inserted in the mouth or in the forehead of the golem.

A golem is inscribed with Hebrew words in some tales (for example, some versions of Chełm and Prague, as well as in Polish tales and versions of Brothers Grimm), such as the word "emet" (אמת, "truth" in Hebrew) written on its forehead. The golem could then be deactivated by removing the aleph (א) in "emet", thus changing the inscription from "truth" to "death" ("met" מת, meaning "dead"). 

Rabbi Jacob Ben Shalom arrived at Barcelona from Germany in 1325 and remarked that the law of destruction is the reversal of the law of creation.

One source credits 11th century Solomon ibn Gabirol with creating a golem, possibly female, for household chores.

Joseph Delmedigo informs us in 1625 that "many legends of this sort are current, particularly in Germany."

The earliest known written account of how to create a golem can be found in "Sodei Razayya" by Eleazar ben Judah of Worms of the late 12th and early 13th century.

The oldest description of the creation of a golem by a historical figure is included in a tradition connected to Rabbi Eliyahu of Chełm (1550–1583).

A Polish Kabbalist, writing in about 1630–1650, reported the creation of a golem by Rabbi Eliyahu thus: "And I have heard, in a certain and explicit way, from several respectable persons that one man [living] close to our time, whose name is R. Eliyahu, the master of the name, who made a creature out of matter [Heb. "Golem"] and form [Heb. "tzurah"] and it performed hard work for him, for a long period, and the name of "emet" was hanging upon his neck until he finally removed it for a certain reason, the name from his neck and it turned to dust." A similar account was reported by a Christian author, Christoph Arnold, in 1674.

Rabbi Jacob Emden (d. 1776) elaborated on the story in a book published in 1748: "As an aside, I'll mention here what I heard from my father's holy mouth regarding the Golem created by his ancestor, the Gaon R. Eliyahu Ba'al Shem of blessed memory. When the Gaon saw that the Golem was growing larger and larger, he feared that the Golem would destroy the universe. He then removed the Holy Name that was embedded on his forehead, thus causing him to disintegrate and return to dust. Nonetheless, while he was engaged in extracting the Holy Name from him, the Golem injured him, scarring him on the face."

According to the Polish Kabbalist, "the legend was known to several persons, thus allowing us to speculate that the legend had indeed circulated for some time before it was committed to writing and, consequently, we may assume that its origins are to be traced to the generation immediately following the death of R. Eliyahu, if not earlier."

The most famous golem narrative involves Judah Loew ben Bezalel, the late 16th century rabbi of Prague, also known as the Maharal, who reportedly "created a golem out of clay from the banks of the Vltava River and brought it to life through rituals and Hebrew incantations to defend the Prague ghetto from anti-Semitic attacks" and pogroms. Depending on the version of the legend, the Jews in Prague were to be either expelled or killed under the rule of Rudolf II, the Holy Roman Emperor. The Golem was called Josef and was known as Yossele. It was said that he could make himself invisible and summon spirits from the dead. Rabbi Loew deactivated the Golem on Friday evenings by removing the "shem" before the Sabbath (Saturday) began, so as to let it rest on Sabbath. One Friday evening Rabbi Loew forgot to remove the "shem", and feared that the Golem would desecrate the Sabbath. A different story tells of a golem that fell in love, and when rejected, became the violent monster seen in most accounts. Some versions have the golem eventually going on a murderous rampage.

The rabbi then managed to pull the "shem" from his mouth and immobilize him in front of the synagogue, whereupon the golem fell in pieces. The Golem's body was stored in the attic "genizah" of the Old New Synagogue, where it would be restored to life again if needed. According to legend, the body of Rabbi Loew's Golem still lies in the synagogue's attic. When the attic was renovated in 1883, no evidence of the Golem was found. Some versions of the tale state that the Golem was stolen from the "genizah" and entombed in a graveyard in Prague's Žižkov district, where the Žižkov Television Tower now stands. A recent legend tells of a Nazi agent ascending to the synagogue attic during World War II and trying to stab the Golem, but he died instead. The attic is not open to the general public.

Some Orthodox Jews believe that the Maharal did actually create a golem. The evidence for this belief has been analyzed from an Orthodox Jewish perspective by Shnayer Z. Leiman.

The general view of historians and critics is that the story of the Golem of Prague was a German literary invention of the early 19th century. According to John Neubauer, the first writers on the Prague Golem were:

However, there are in fact a couple of slightly earlier examples, in 1834 and 1836.

All of these early accounts of the Golem of Prague are in German by Jewish writers. It has been suggested that they emerged as part of a Jewish folklore movement parallel with the contemporary German folklore movement.

The origins of the story have been obscured by attempts to exaggerate its age and to pretend that it dates from the time of the Maharal. It has been said that Rabbi Yudel Rosenberg (1859–1935) of Tarłów (before moving to Canada where he became one of its most prominent rabbis) originated the idea that the narrative dates from the time of the Maharal. Rosenberg published "Nifl'os Maharal" ("Wonders of Maharal") (Piotrków, 1909) which purported to be an eyewitness account by the Maharal's son-in-law, who had helped to create the Golem. Rosenberg claimed that the book was based upon a manuscript that he found in the main library in Metz. "Wonders of Maharal" "is generally recognized in academic circles to be a literary hoax". Gershom Sholem observed that the manuscript "contains not ancient legends but modern fiction". Rosenberg's claim was further disseminated in Chayim Bloch's (1881–1973) "The Golem: Legends of the Ghetto of Prague" (English edition 1925).

The "Jewish Encyclopedia" of 1906 cites the historical work "Zemach David" by David Gans, a disciple of the Maharal, published in 1592. In it, Gans writes of an audience between the Maharal and Rudolph II: "Our lord the emperor ... Rudolph ... sent for and called upon our master Rabbi Low ben Bezalel and received him with a welcome and merry expression, and spoke to him face to face, as one would to a friend. The nature and quality of their words are mysterious, sealed and hidden." But it has been said of this passage, "Even when [the Maharal is] eulogized, whether in David Gans' "Zemach David" or on his epitaph …, not a word is said about the creation of a golem. No Hebrew work published in the 16th, 17th, and 18th centuries (even in Prague) is aware that the Maharal created a golem." Furthermore, the Maharal himself did not refer to the Golem in his writings. Rabbi Yedidiah Tiah Weil (1721–1805), a Prague resident, who described the creation of golems, including those created by Rabbis Avigdor Kara of Prague (died 1439) and Eliyahu of Chelm, did not mention the Maharal, and Rabbi Meir Perils' biography of the Maharal published in 1718 does not mention a golem.

There is a similar tradition relating to the Vilna Gaon or "the saintly genius from Vilnius" (1720–1797). Rabbi Chaim Volozhin (Lithuania 1749–1821) reported in an introduction to "Sifra de Tzeniuta" that he once presented to his teacher, the Vilna Gaon, ten different versions of a certain passage in the "Sefer Yetzira" and asked the Gaon to determine the correct text. The Gaon immediately identified one version as the accurate rendition of the passage. The amazed student then commented to his teacher that, with such clarity, he should easily be able to create a live human. The Gaon affirmed Rabbi Chaim's assertion and said that he once began to create a person when he was a child, under the age of 13, but during the process, he received a sign from Heaven ordering him to desist because of his tender age.

The existence of a golem is sometimes a mixed blessing. Golems are not intelligent, and if commanded to perform a task, they will perform the instructions literally. In many depictions, Golems are inherently perfectly obedient. In its earliest known modern form, the Golem of Chełm became enormous and uncooperative. In one version of this story, the rabbi had to resort to trickery to deactivate it, whereupon it crumbled upon its creator and crushed him. There is a similar hubris theme in "Frankenstein", "The Sorcerer's Apprentice", and some other stories in popular culture, such as "The Terminator". The theme also manifests itself in "R.U.R. (Rossum's Universal Robots)", Karel Čapek's 1921 play which coined the term robot; the play was written in Prague, and while Čapek denied that he modeled the robot after the Golem, there are many similarities in the plot.

The Golem is a popular figure in the Czech Republic. There are several restaurants and other businesses whose names make reference to the creature, a Czech strongman (René Richter) goes by the nickname "Golem", and a Czech monster truck outfit calls itself the "Golem Team."

Abraham Akkerman preceded his article on human automatism in the contemporary city with a short satirical poem on a pair of golems turning human.

A Yiddish and Slavic folktale is the Clay Boy, which combines elements of the Golem and "The Gingerbread Man", in which a lonely couple makes a child out of clay, with disastrous or comical consequences. In one common Russian version, an older couple, whose children have left home, makes a boy out of clay and dries him by their hearth. The Clay Boy comes to life; at first, the couple is delighted and treats him like a real child, but the Clay Boy does not stop growing and eats all their food, then all their livestock, and then the Clay Boy eats his parents. The Clay Boy rampages through the village until he is smashed by a quick-thinking goat.











</doc>
<doc id="11891" url="https://en.wikipedia.org/wiki?curid=11891" title="George Orwell">
George Orwell

Eric Arthur Blair (25 June 1903 – 21 January 1950), known by his pen name George Orwell, was an English novelist, essayist, journalist and critic. His work is characterised by lucid prose, biting social criticism, opposition to totalitarianism, and outspoken support of democratic socialism.

As a writer, Orwell produced literary criticism and poetry, fiction and polemical journalism; and is best known for the allegorical novella "Animal Farm" (1945) and the dystopian novel "Nineteen Eighty-Four" (1949). His non-fiction works, including "The Road to Wigan Pier" (1937), documenting his experience of working-class life in the north of England, and "Homage to Catalonia" (1938), an account of his experiences soldiering for the Republican faction of the Spanish Civil War (1936–1939), are as critically respected as his essays on politics and literature, language and culture. In 2008, "The Times" ranked George Orwell second among "The 50 greatest British writers since 1945".

Orwell's work remains influential in popular culture and in political culture, and the adjective "Orwellian"—describing totalitarian and authoritarian social practices—is part of the English language, like many of his neologisms, such as "Big Brother", "Thought Police", "Two Minutes Hate", "Room 101", "memory hole", "Newspeak", "doublethink", "proles", "unperson", and "thoughtcrime".

Eric Arthur Blair was born on 25 June 1903 in Motihari, Bihar, British India. His great-grandfather, Charles Blair, was a wealthy country gentleman in Dorset who married Lady Mary Fane, daughter of the Earl of Westmorland, and had income as an absentee landlord of plantations in Jamaica. His grandfather, Thomas Richard Arthur Blair, was a clergyman. Eric Blair described his family as "lower-upper-middle class".

His father, Richard Walmesley Blair, worked in the Opium Department of the Indian Civil Service. His mother, Ida Mabel Blair ("née" Limouzin), grew up in Moulmein, Burma, where her French father was involved in speculative ventures. Eric had two sisters: Marjorie, five years older; and Avril, five years younger. When Eric was one year old, his mother took him and Marjorie to England. His birthplace and ancestral house in Motihari has been declared a protected monument of historical importance.

In 1904 Ida Blair settled with her children at Henley-on-Thames in Oxfordshire. Eric was brought up in the company of his mother and sisters, and apart from a brief visit in mid-1907, the family did not see their husband or father, Richard Blair, until 1912. His mother's diary from 1905 describes a lively round of social activity and artistic interests.

Aged five, Eric was sent as a day-boy to a convent school in Henley-on-Thames, which Marjorie also attended. It was a Roman Catholic convent run by French Ursuline nuns, who had been exiled from France after Catholic education was banned in 1903 due to the Dreyfus Affair. His mother wanted him to have a public school education, but his family could not afford the fees, and he needed to earn a scholarship. Ida Blair's brother Charles Limouzin recommended St Cyprian's School, Eastbourne, East Sussex. Limouzin, who was a proficient golfer, knew of the school and its headmaster through the Royal Eastbourne Golf Club, where he won several competitions in 1903 and 1904. The headmaster undertook to help Blair to win a scholarship, and made a private financial arrangement that allowed Blair's parents to pay only half the normal fees. In September 1911, Eric arrived at St Cyprian's. He boarded at the school for the next five years, returning home only for school holidays. During this period, while working for the Ministry of Pensions, his mother lived at 23 Cromwell Crescent, Earls Court. He knew nothing of the reduced fees, although he "soon recognised that he was from a poorer home". Blair hated the school and many years later wrote an essay "Such, Such Were the Joys", published posthumously, based on his time there. At St Cyprian's, Blair first met Cyril Connolly, who became a writer. Many years later, as the editor of "Horizon", Connolly published several of Orwell's essays.

Before the First World War, the family moved to Shiplake, Oxfordshire where Eric became friendly with the Buddicom family, especially their daughter Jacintha. When they first met, he was standing on his head in a field. On being asked why, he said, "You are noticed more if you stand on your head than if you are right way up." Jacintha and Eric read and wrote poetry, and dreamed of becoming famous writers. He said that he might write a book in the style of H. G. Wells's "A Modern Utopia". During this period, he also enjoyed shooting, fishing and birdwatching with Jacintha's brother and sister.
While at St Cyprian's, Blair wrote two poems that were published in the "Henley and South Oxfordshire Standard". He came second to Connolly in the Harrow History Prize, had his work praised by the school's external examiner, and earned scholarships to Wellington and Eton. But inclusion on the Eton scholarship roll did not guarantee a place, and none was immediately available for Blair. He chose to stay at St Cyprian's until December 1916, in case a place at Eton became available.

In January, Blair took up the place at Wellington, where he spent the Spring term. In May 1917 a place became available as a King's Scholar at Eton. At this time the family lived at Mall Chambers, Notting Hill Gate. Blair remained at Eton until December 1921, when he left midway between his 18th and 19th birthday. Wellington was "beastly", Orwell told his childhood friend Jacintha Buddicom, but he said he was "interested and happy" at Eton. His principal tutor was A. S. F. Gow, Fellow of Trinity College, Cambridge, who also gave him advice later in his career. Blair was briefly taught French by Aldous Huxley. Steven Runciman, who was at Eton with Blair, noted that he and his contemporaries appreciated Huxley's linguistic flair. Cyril Connolly followed Blair to Eton, but because they were in separate years, they did not associate with each other.

Blair's academic performance reports suggest that he neglected his academic studies, but during his time at Eton he worked with Roger Mynors to produce a College magazine, "The Election Times", joined in the production of other publications—"College Days" and "Bubble and Squeak"—and participated in the Eton Wall Game. His parents could not afford to send him to a university without another scholarship, and they concluded from his poor results that he would not be able to win one. Runciman noted that he had a romantic idea about the East, and the family decided that Blair should join the Imperial Police, the precursor of the Indian Police Service. For this he had to pass an entrance examination. In December 1921 he left Eton and travelled to join his retired father, mother, and younger sister Avril, who that month had moved to 40 Stradbroke Road, Southwold, Suffolk, the first of their four homes in the town. Blair was enrolled at a crammer there called Craighurst, and brushed up on his Classics, English, and History. He passed the entrance exam, coming seventh out of the 26 candidates who exceeded the pass mark.

Blair's maternal grandmother lived at Moulmein, so he chose a posting in Burma, then still a province of British India. In October 1922 he sailed on board SS "Herefordshire" via the Suez Canal and Ceylon to join the Indian Imperial Police in Burma. A month later, he arrived at Rangoon and travelled to the police training school in Mandalay. He was appointed an Assistant District Superintendent (on probation) on 29 November 1922, with effect from 27 November and at a base salary of Rs. 325 per month, with an overseas supplement of Rs. 125/month and a "Burma Allowance" of Rs. 75/month (a total of Rs. 525, or approximately £52–10s per month at prevailing exchange rates, equivalent to £ in ). After a short posting at Maymyo, Burma's principal hill station, he was posted to the frontier outpost of Myaungmya in the Irrawaddy Delta at the beginning of 1924.

Working as an imperial police officer gave him considerable responsibility while most of his contemporaries were still at university in England. When he was posted farther east in the Delta to Twante as a sub-divisional officer, he was responsible for the security of some 200,000 people. At the end of 1924, he was posted to Syriam, closer to Rangoon. Syriam had the refinery of the Burmah Oil Company, "the surrounding land a barren waste, all vegetation killed off by the fumes of sulphur dioxide pouring out day and night from the stacks of the refinery." But the town was near Rangoon, a cosmopolitan seaport, and Blair went into the city as often as he could, "to browse in a bookshop; to eat well-cooked food; to get away from the boring routine of police life". In September 1925 he went to Insein, the home of Insein Prison, the second largest prison in Burma. In Insein, he had "long talks on every conceivable subject" with Elisa Maria Langford-Rae (who later married Kazi Lhendup Dorjee). She noted his "sense of utter fairness in minutest details". By this time, Blair had completed his training and was receiving a monthly salary of Rs. 740, including allowances (approximately £74 per month, equivalent to £ in ).
In Burma, Blair acquired a reputation as an outsider. He spent much of his time alone, reading or pursuing non-"pukka" activities, such as attending the churches of the Karen ethnic group. A colleague, Roger Beadon, recalled (in a 1969 recording for the BBC) that Blair was fast to learn the language and that before he left Burma, "was able to speak fluently with Burmese priests in 'very high-flown Burmese.'" Blair made changes to his appearance in Burma that remained for the rest of his life. This included adopting a pencil moustache, a thin line above the lip (he previously had a toothbrush moustache). Emma Larkin writes in the introduction to "Burmese Days", "While in Burma, he acquired a moustache similar to those worn by officers of the British regiments stationed there. [He] also acquired some tattoos; on each knuckle he had a small untidy blue circle. Many Burmese living in rural areas still sport tattoos like this—they are believed to protect against bullets and snake bites." Later, he wrote that he felt guilty about his role in the work of empire and he "began to look more closely at his own country and saw that England also had its oppressed."

In April 1926 he moved to Moulmein, where his maternal grandmother lived. At the end of that year, he was assigned to Katha in Upper Burma, where he contracted dengue fever in 1927. Entitled to a leave in England that year, he was allowed to return in July due to his illness. While on leave in England and on holiday with his family in Cornwall in September 1927, he reappraised his life. Deciding against returning to Burma, he resigned from the Indian Imperial Police to become a writer, with effect from 12 March 1928 after five-and-a-half years of service. He drew on his experiences in the Burma police for the novel "Burmese Days" (1934) and the essays "A Hanging" (1931) and "Shooting an Elephant" (1936).

In England, he settled back in the family home at Southwold, renewing acquaintance with local friends and attending an Old Etonian dinner. He visited his old tutor Gow at Cambridge for advice on becoming a writer. In 1927 he moved to London. Ruth Pitter, a family acquaintance, helped him find lodgings, and by the end of 1927 he had moved into rooms in Portobello Road; a blue plaque commemorates his residence there. Pitter's involvement in the move "would have lent it a reassuring respectability in Mrs Blair's eyes." Pitter had a sympathetic interest in Blair's writing, pointed out weaknesses in his poetry, and advised him to write about what he knew. In fact he decided to write of "certain aspects of the present that he set out to know" and ventured into the East End of London—the first of the occasional sorties he would make to discover for himself the world of poverty and the down-and-outers who inhabit it. He had found a subject. These sorties, explorations, expeditions, tours or immersions were made intermittently over a period of five years.

In imitation of Jack London, whose writing he admired (particularly "The People of the Abyss"), Blair started to explore the poorer parts of London. On his first outing he set out to Limehouse Causeway, spending his first night in a common lodging house, possibly George Levy's 'kip'. For a while he "went native" in his own country, dressing like a tramp, adopting the name P.S. Burton and making no concessions to middle-class "mores" and expectations; he recorded his experiences of the low life for use in "The Spike", his first published essay in English, and in the second half of his first book, "Down and Out in Paris and London" (1933).
In early 1928 he moved to Paris. He lived in the rue du Pot de Fer, a working class district in the 5th Arrondissement. His aunt Nellie Limouzin also lived in Paris and gave him social and, when necessary, financial support. He began to write novels, including an early version of "Burmese Days", but nothing else survives from that period. He was more successful as a journalist and published articles in "Monde", a political/literary journal edited by Henri Barbusse (his first article as a professional writer, "La Censure en Angleterre", appeared in that journal on 6 October 1928); "G. K.'s Weekly", where his first article to appear in England, "A Farthing Newspaper", was printed on 29 December 1928; and "Le Progrès Civique" (founded by the left-wing coalition Le Cartel des Gauches). Three pieces appeared in successive weeks in "Le Progrès Civique": discussing unemployment, a day in the life of a tramp, and the beggars of London, respectively. "In one or another of its destructive forms, poverty was to become his obsessive subject—at the heart of almost everything he wrote until "Homage to Catalonia"."

He fell seriously ill in February 1929 and was taken to the Hôpital Cochin in the 14th arrondissement, a free hospital where medical students were trained. His experiences there were the basis of his essay "How the Poor Die", published in 1946. He chose not to identify the hospital, and indeed was deliberately misleading about its location. Shortly afterwards, he had all his money stolen from his lodging house. Whether through necessity or to collect material, he undertook menial jobs such as dishwashing in a fashionable hotel on the rue de Rivoli, which he later described in "Down and Out in Paris and London". In August 1929, he sent a copy of "The Spike" to John Middleton Murry's "New Adelphi" magazine in London. The magazine was edited by Max Plowman and Sir Richard Rees, and Plowman accepted the work for publication.

In December 1929 after nearly two years in Paris, Blair returned to England and went directly to his parents' house in Southwold, a coastal town in Suffolk, which remained his base for the next five years. The family was well established in the town, and his sister Avril was running a tea-house there. He became acquainted with many local people, including Brenda Salkeld, the clergyman's daughter who worked as a gym-teacher at St Felix Girls' School in the town. Although Salkeld rejected his offer of marriage, she remained a friend and regular correspondent for many years. He also renewed friendships with older friends, such as Dennis Collings, whose girlfriend Eleanor Jacques was also to play a part in his life.

In early 1930 he stayed briefly in Bramley, Leeds, with his sister Marjorie and her husband Humphrey Dakin, who was as unappreciative of Blair as when they knew each other as children. Blair was writing reviews for "Adelphi" and acting as a private tutor to a disabled child at Southwold. He then became tutor to three young brothers, one of whom, Richard Peters, later became a distinguished academic. "His history in these years is marked by dualities and contrasts. There is Blair leading a respectable, outwardly eventless life at his parents' house in Southwold, writing; then in contrast, there is Blair as Burton (the name he used in his down-and-out episodes) in search of experience in the kips and spikes, in the East End, on the road, and in the hop fields of Kent." He went painting and bathing on the beach, and there he met Mabel and Francis Fierz, who later influenced his career. Over the next year he visited them in London, often meeting their friend Max Plowman. He also often stayed at the homes of Ruth Pitter and Richard Rees, where he could "change" for his sporadic tramping expeditions. One of his jobs was domestic work at a lodgings for half a crown (two shillings and sixpence, or one-eighth of a pound) a day.

Blair now contributed regularly to "Adelphi", with "A Hanging" appearing in August 1931. From August to September 1931 his explorations of poverty continued, and, like the protagonist of "A Clergyman's Daughter", he followed the East End tradition of working in the Kent hop fields. He kept a diary about his experiences there. Afterwards, he lodged in the Tooley Street kip, but could not stand it for long, and with financial help from his parents moved to Windsor Street, where he stayed until Christmas. "Hop Picking", by Eric Blair, appeared in the October 1931 issue of "New Statesman", whose editorial staff included his old friend Cyril Connolly. Mabel Fierz put him in contact with Leonard Moore, who became his literary agent.

At this time Jonathan Cape rejected "A Scullion's Diary", the first version of "Down and Out". On the advice of Richard Rees, he offered it to Faber and Faber, but their editorial director, T. S. Eliot, also rejected it. Blair ended the year by deliberately getting himself arrested, so that he could experience Christmas in prison, but the authorities did not regard his "drunk and disorderly" behaviour as imprisonable, and he returned home to Southwold after two days in a police cell.

In April 1932 Blair became a teacher at The Hawthorns High School, a school for boys, in Hayes, West London. This was a small school offering private schooling for children of local tradesmen and shopkeepers, and had only 14 or 16 boys aged between ten and sixteen, and one other master. While at the school he became friendly with the curate of the local parish church and became involved with activities there. Mabel Fierz had pursued matters with Moore, and at the end of June 1932, Moore told Blair that Victor Gollancz was prepared to publish "A Scullion's Diary" for a £40 advance, through his recently founded publishing house, Victor Gollancz Ltd, which was an outlet for radical and socialist works.

At the end of the summer term in 1932, Blair returned to Southwold, where his parents had used a legacy to buy their own home. Blair and his sister Avril spent the holidays making the house habitable while he also worked on "Burmese Days". He was also spending time with Eleanor Jacques, but her attachment to Dennis Collings remained an obstacle to his hopes of a more serious relationship.
"Clink", an essay describing his failed attempt to get sent to prison, appeared in the August 1932 number of "Adelphi". He returned to teaching at Hayes and prepared for the publication of his book, now known as "Down and Out in Paris and London". He wished to publish under a different name to avoid any embarrassment to his family over his time as a "tramp". In a letter to Moore (dated 15 November 1932), he left the choice of pseudonym to Moore and to Gollancz. Four days later, he wrote to Moore, suggesting the pseudonyms P. S. Burton (a name he used when tramping), Kenneth Miles, George Orwell, and H. Lewis Allways. He finally adopted the "nom de plume" George Orwell because "It is a good round English name." "Down and Out in Paris and London" was published on 9 January 1933 as Orwell continued to work on "Burmese Days". "Down and Out" was modestly successful and was next published by Harper & Brothers in New York.

In mid-1933 Blair left Hawthorns to become a teacher at Frays College, in Uxbridge, Middlesex. This was a much larger establishment with 200 pupils and a full complement of staff. He acquired a motorcycle and took trips through the surrounding countryside. On one of these expeditions he became soaked and caught a chill that developed into pneumonia. He was taken to Uxbridge Cottage Hospital, where for a time his life was believed to be in danger. When he was discharged in January 1934, he returned to Southwold to convalesce and, supported by his parents, never returned to teaching.

He was disappointed when Gollancz turned down "Burmese Days", mainly on the grounds of potential suits for libel, but Harper were prepared to publish it in the United States. Meanwhile, Blair started work on the novel "A Clergyman's Daughter", drawing upon his life as a teacher and on life in Southwold. Eleanor Jacques was now married and had gone to Singapore and Brenda Salkeld had left for Ireland, so Blair was relatively isolated in Southwold—working on the allotments, walking alone and spending time with his father. Eventually in October, after sending "A Clergyman's Daughter" to Moore, he left for London to take a job that had been found for him by his aunt Nellie Limouzin.

This job was as a part-time assistant in Booklovers' Corner, a second-hand bookshop in Hampstead run by Francis and Myfanwy Westrope, who were friends of Nellie Limouzin in the Esperanto movement. The Westropes were friendly and provided him with comfortable accommodation at Warwick Mansions, Pond Street. He was sharing the job with Jon Kimche, who also lived with the Westropes. Blair worked at the shop in the afternoons and had his mornings free to write and his evenings free to socialise. These experiences provided background for the novel "Keep the Aspidistra Flying" (1936). As well as the various guests of the Westropes, he was able to enjoy the company of Richard Rees and the "Adelphi" writers and Mabel Fierz. The Westropes and Kimche were members of the Independent Labour Party, although at this time Blair was not seriously politically active. He was writing for the "Adelphi" and preparing "A Clergyman's Daughter" and "Burmese Days" for publication.

At the beginning of 1935 he had to move out of Warwick Mansions, and Mabel Fierz found him a flat in Parliament Hill. "A Clergyman's Daughter" was published on 11 March 1935. In early 1935 Blair met his future wife Eileen O'Shaughnessy, when his landlady, Rosalind Obermeyer, who was studying for a master's degree in psychology at University College London, invited some of her fellow students to a party. One of these students, Elizaveta Fen, a biographer and future translator of Chekhov, recalled Blair and his friend Richard Rees "draped" at the fireplace, looking, she thought, "moth-eaten and prematurely aged." Around this time, Blair had started to write reviews for "The New English Weekly".
In June, "Burmese Days" was published and Cyril Connolly's review in the "New Statesman" prompted Blair (as he then became known) to re-establish contact with his old friend. In August, he moved into a flat in Kentish Town, which he shared with Michael Sayers and Rayner Heppenstall. The relationship was sometimes awkward and Blair and Heppenstall even came to blows, though they remained friends and later worked together on BBC broadcasts. Blair was now working on "Keep the Aspidistra Flying", and also tried unsuccessfully to write a serial for the "News Chronicle". By October 1935 his flatmates had moved out and he was struggling to pay the rent on his own. He remained until the end of January 1936, when he stopped working at Booklovers' Corner.

At this time, Victor Gollancz suggested Orwell spend a short time investigating social conditions in economically depressed northern England. Two years earlier, J. B. Priestley had written about England north of the Trent, sparking an interest in reportage. The depression had also introduced a number of working-class writers from the North of England to the reading public. It was one of these working-class authors, Jack Hilton, whom Orwell sought for advice. Orwell had written to Hilton seeking lodging and asking for recommendations on his route. Hilton was unable to provide him lodging, but suggested that he travel to Wigan rather than Rochdale, "for there are the colliers and they're good stuff."

On 31 January 1936, Orwell set out by public transport and on foot, reaching Manchester via Coventry, Stafford, the Potteries and Macclesfield. Arriving in Manchester after the banks had closed, he had to stay in a common lodging-house. The next day he picked up a list of contacts sent by Richard Rees. One of these, the trade union official Frank Meade, suggested Wigan, where Orwell spent February staying in dirty lodgings over a tripe shop. At Wigan, he visited many homes to see how people lived, took detailed notes of housing conditions and wages earned, went down Bryn Hall coal mine, and used the local public library to consult public health records and reports on working conditions in mines.

During this time, he was distracted by concerns about style and possible libel in "Keep the Aspidistra Flying". He made a quick visit to Liverpool and during March, stayed in south Yorkshire, spending time in Sheffield and Barnsley. As well as visiting mines, including Grimethorpe, and observing social conditions, he attended meetings of the Communist Party and of Oswald Mosley ("his speech the usual claptrap—The blame for everything was put upon mysterious international gangs of Jews") where he saw the tactics of the Blackshirts ("...one is liable to get both a hammering and a fine for asking a question which Mosley finds it difficult to answer."). He also made visits to his sister at Headingley, during which he visited the Brontë Parsonage at Haworth, where he was "chiefly impressed by a pair of Charlotte Brontë's cloth-topped boots, very small, with square toes and lacing up at the sides."
Orwell needed somewhere he could concentrate on writing his book, and once again help was provided by Aunt Nellie, who was living at Wallington, Hertfordshire in a very small 16th-century cottage called the "Stores". Wallington was a tiny village 35 miles north of London, and the cottage had almost no modern facilities. Orwell took over the tenancy and moved in on 2 April 1936. He started work on "The Road to Wigan Pier" by the end of April, but also spent hours working on the garden and testing the possibility of reopening the Stores as a village shop. "Keep the Aspidistra Flying" was published by Gollancz on 20 April 1936. On 4 August, Orwell gave a talk at the Adelphi Summer School held at Langham, entitled "An Outsider Sees the Distressed Areas"; others who spoke at the school included John Strachey, Max Plowman, Karl Polanyi and Reinhold Niebuhr.

The result of his journeys through the north was "The Road to Wigan Pier", published by Gollancz for the Left Book Club in 1937. The first half of the book documents his social investigations of Lancashire and Yorkshire, including an evocative description of working life in the coal mines. The second half is a long essay on his upbringing and the development of his political conscience, which includes an argument for socialism (although he goes to lengths to balance the concerns and goals of socialism with the barriers it faced from the movement's own advocates at the time, such as "priggish" and "dull" socialist intellectuals and "proletarian" socialists with little grasp of the actual ideology). Gollancz feared the second half would offend readers and added a disculpatory preface to the book while Orwell was in Spain.

Orwell's research for "The Road to Wigan Pier" led to him being placed under surveillance by the Special Branch from 1936, for 12 years, until one year before the publication of "Nineteen Eighty-Four".

Orwell married Eileen O'Shaughnessy on 9 June 1936. Shortly afterwards, the political crisis began in Spain and Orwell followed developments there closely. At the end of the year, concerned by Francisco Franco's military uprising (supported by Nazi Germany, Fascist Italy and local groups such as Falange), Orwell decided to go to Spain to take part in the Spanish Civil War on the Republican side. Under the erroneous impression that he needed papers from some left-wing organisation to cross the frontier, on John Strachey's recommendation he applied unsuccessfully to Harry Pollitt, leader of the British Communist Party. Pollitt was suspicious of Orwell's political reliability; he asked him whether he would undertake to join the International Brigade and advised him to get a safe-conduct from the Spanish Embassy in Paris. Not wishing to commit himself until he had seen the situation "in situ", Orwell instead used his Independent Labour Party contacts to get a letter of introduction to John McNair in Barcelona.

Orwell set out for Spain on about 23 December 1936, dining with Henry Miller in Paris on the way. The American writer told Orwell that going to fight in the Civil War out of some sense of obligation or guilt was "sheer stupidity" and that the Englishman's ideas "about combating Fascism, defending democracy, etc., etc., were all baloney." A few days later in Barcelona, Orwell met John McNair of the Independent Labour Party (ILP) Office who quoted him: "I've come to fight against Fascism". Orwell stepped into a complex political situation in Catalonia. The Republican government was supported by a number of factions with conflicting aims, including the Workers' Party of Marxist Unification (POUM – Partido Obrero de Unificación Marxista), the anarcho-syndicalist Confederación Nacional del Trabajo (CNT) and the Unified Socialist Party of Catalonia (a wing of the Spanish Communist Party, which was backed by Soviet arms and aid). The ILP was linked to the POUM so Orwell joined the POUM.

After a time at the Lenin Barracks in Barcelona he was sent to the relatively quiet Aragon Front under Georges Kopp. By January 1937 he was at Alcubierre above sea level, in the depth of winter. There was very little military action and Orwell was shocked by the lack of munitions, food and firewood as well as other extreme deprivations. With his Cadet Corps and police training, Orwell was quickly made a corporal. On the arrival of a British ILP Contingent about three weeks later, Orwell and the other English militiaman, Williams, were sent with them to Monte Oscuro. The newly arrived ILP contingent included Bob Smillie, Bob Edwards, Stafford Cottman and Jack Branthwaite. The unit was then sent on to Huesca.

Meanwhile, back in England, Eileen had been handling the issues relating to the publication of "The Road to Wigan Pier" before setting out for Spain herself, leaving Nellie Limouzin to look after The Stores. Eileen volunteered for a post in John McNair's office and with the help of Georges Kopp paid visits to her husband, bringing him English tea, chocolate and cigars. Orwell had to spend some days in hospital with a poisoned hand and had most of his possessions stolen by the staff. He returned to the front and saw some action in a night attack on the Nationalist trenches where he chased an enemy soldier with a bayonet and bombed an enemy rifle position.

In April, Orwell returned to Barcelona. Wanting to be sent to the Madrid front, which meant he "must join the International Column", he approached a Communist friend attached to the Spanish Medical Aid and explained his case. "Although he did not think much of the Communists, Orwell was still ready to treat them as friends and allies. That would soon change." This was the time of the Barcelona May Days and Orwell was caught up in the factional fighting. He spent much of the time on a roof, with a stack of novels, but encountered Jon Kimche from his Hampstead days during the stay. The subsequent campaign of lies and distortion carried out by the Communist press, in which the POUM was accused of collaborating with the fascists, had a dramatic effect on Orwell. Instead of joining the International Brigades as he had intended, he decided to return to the Aragon Front. Once the May fighting was over, he was approached by a Communist friend who asked if he still intended transferring to the International Brigades. Orwell expressed surprise that they should still want him, because according to the Communist press he was a fascist. "No one who was in Barcelona then, or for months later, will forget the horrible atmosphere produced by fear, suspicion, hatred, censored newspapers, crammed jails, enormous food queues and prowling gangs of armed men."

After his return to the front, he was wounded in the throat by a sniper's bullet. At 6 ft 2 in (1.88 m), Orwell was considerably taller than the Spanish fighters and had been warned against standing against the trench parapet. Unable to speak, and with blood pouring from his mouth, Orwell was carried on a stretcher to Siétamo, loaded on an ambulance and after a bumpy journey via Barbastro arrived at the hospital at Lérida. He recovered sufficiently to get up and on 27 May 1937 was sent on to Tarragona and two days later to a POUM sanatorium in the suburbs of Barcelona. The bullet had missed his main artery by the barest margin and his voice was barely audible. It had been such a clean shot that the wound immediately went through the process of cauterisation. He received electrotherapy treatment and was declared medically unfit for service.

By the middle of June the political situation in Barcelona had deteriorated and the POUM—painted by the pro-Soviet Communists as a Trotskyist organisation—was outlawed and under attack. The Communist line was that the POUM were "objectively" Fascist, hindering the Republican cause. "A particularly nasty poster appeared, showing a head with a POUM mask being ripped off to reveal a Swastika-covered face beneath." Members, including Kopp, were arrested and others were in hiding. Orwell and his wife were under threat and had to lie low, although they broke cover to try to help Kopp.

Finally with their passports in order, they escaped from Spain by train, diverting to Banyuls-sur-Mer for a short stay before returning to England. In the first week of July 1937 Orwell arrived back at Wallington; on 13 July 1937 a deposition was presented to the Tribunal for Espionage & High Treason in Valencia, charging the Orwells with "rabid Trotskyism", and being agents of the POUM. The trial of the leaders of the POUM and of Orwell (in his absence) took place in Barcelona in October and November 1938. Observing events from French Morocco, Orwell wrote that they were "only a by-product of the Russian Trotskyist trials and from the start every kind of lie, including flagrant absurdities, has been circulated in the Communist press." Orwell's experiences in the Spanish Civil War gave rise to "Homage to Catalonia" (1938).

Orwell returned to England in June 1937, and stayed at the O'Shaughnessy home at Greenwich. He found his views on the Spanish Civil War out of favour. Kingsley Martin rejected two of his works and Gollancz was equally cautious. At the same time, the communist "Daily Worker" was running an attack on "The Road to Wigan Pier", taking out of context Orwell writing that "the working classes smell"; a letter to Gollancz from Orwell threatening libel action brought a stop to this. Orwell was also able to find a more sympathetic publisher for his views in Fredric Warburg of Secker & Warburg. Orwell returned to Wallington, which he found in disarray after his absence. He acquired goats, a cockerel (rooster) he called Henry Ford and a poodle puppy he called Marx; and settled down to animal husbandry and writing "Homage to Catalonia".

There were thoughts of going to India to work on the Pioneer, a newspaper in Lucknow, but by March 1938 Orwell's health had deteriorated. He was admitted to Preston Hall Sanatorium at Aylesford, Kent, a British Legion hospital for ex-servicemen to which his brother-in-law Laurence O'Shaughnessy was attached. He was thought initially to be suffering from tuberculosis and stayed in the sanatorium until September. A stream of visitors came to see him, including Common, Heppenstall, Plowman and Cyril Connolly. Connolly brought with him Stephen Spender, a cause of some embarrassment as Orwell had referred to Spender as a "pansy friend" some time earlier. "Homage to Catalonia" was published by Secker & Warburg and was a commercial flop. In the latter part of his stay at the clinic, Orwell was able to go for walks in the countryside and study nature.

The novelist L. H. Myers secretly funded a trip to French Morocco for half a year for Orwell to avoid the English winter and recover his health. The Orwells set out in September 1938 via Gibraltar and Tangier to avoid Spanish Morocco and arrived at Marrakech. They rented a villa on the road to Casablanca and during that time Orwell wrote "Coming Up for Air". They arrived back in England on 30 March 1939 and "Coming Up for Air" was published in June. Orwell spent time in Wallington and Southwold working on a Dickens essay and it was in June 1939 that Orwell's father, Richard Blair, died.

At the outbreak of the Second World War, Orwell's wife Eileen started working in the Censorship Department of the Ministry of Information in central London, staying during the week with her family in Greenwich. Orwell also submitted his name to the Central Register for war work, but nothing transpired. "They won't have me in the army, at any rate at present, because of my lungs", Orwell told Geoffrey Gorer. He returned to Wallington, and in late 1939 he wrote material for his first collection of essays, "Inside the Whale". For the next year he was occupied writing reviews for plays, films and books for "The Listener", "Time and Tide" and "New Adelphi". On 29 March 1940 his long association with "Tribune" began with a review of a sergeant's account of Napoleon's retreat from Moscow. At the beginning of 1940, the first edition of Connolly's "Horizon" appeared, and this provided a new outlet for Orwell's work as well as new literary contacts. In May the Orwells took lease of a flat in London at Dorset Chambers, Chagford Street, Marylebone. It was the time of the Dunkirk evacuation and the death in France of Eileen's brother Lawrence caused her considerable grief and long-term depression. Throughout this period Orwell kept a wartime diary.

Orwell was declared "unfit for any kind of military service" by the Medical Board in June, but soon afterwards found an opportunity to become involved in war activities by joining the Home Guard. He shared Tom Wintringham's socialist vision for the Home Guard as a revolutionary People's Militia. His lecture notes for instructing platoon members include advice on street fighting, field fortifications, and the use of mortars of various kinds. Sergeant Orwell managed to recruit Fredric Warburg to his unit. During the Battle of Britain he used to spend weekends with Warburg and his new Zionist friend, Tosco Fyvel, at Warburg's house at Twyford, Berkshire. At Wallington he worked on "England Your England" and in London wrote reviews for various periodicals. Visiting Eileen's family in Greenwich brought him face-to-face with the effects of the Blitz on East London. In mid-1940, Warburg, Fyvel and Orwell planned Searchlight Books. Eleven volumes eventually appeared, of which Orwell's "", published on 19 February 1941, was the first.

Early in 1941 he began to write for the American "Partisan Review" which linked Orwell with The New York Intellectuals who were also anti-Stalinist, and contributed to the Gollancz anthology "The Betrayal of the Left", written in the light of the Molotov–Ribbentrop Pact (although Orwell referred to it as the Russo-German Pact and the Hitler-Stalin Pact). He also applied unsuccessfully for a job at the Air Ministry. Meanwhile, he was still writing reviews of books and plays and at this time met the novelist Anthony Powell. He also took part in a few radio broadcasts for the Eastern Service of the BBC. In March the Orwells moved to a seventh-floor flat at Langford Court, St John's Wood, while at Wallington Orwell was "digging for victory" by planting potatoes.

In August 1941, Orwell finally obtained "war work" when he was taken on full-time by the BBC's Eastern Service. When interviewed for the job he indicated that he "accept[ed] absolutely the need for propaganda to be directed by the government" and stressed his view that, in wartime, discipline in the execution of government policy was essential. He supervised cultural broadcasts to India to counter propaganda from Nazi Germany designed to undermine Imperial links. This was Orwell's first experience of the rigid conformity of life in an office, and it gave him an opportunity to create cultural programmes with contributions from T. S. Eliot, Dylan Thomas, E. M. Forster, Ahmed Ali, Mulk Raj Anand, and William Empson among others.

At the end of August he had a dinner with H. G. Wells which degenerated into a row because Wells had taken offence at observations Orwell made about him in a "Horizon" article. In October Orwell had a bout of bronchitis and the illness recurred frequently. David Astor was looking for a provocative contributor for "The Observer" and invited Orwell to write for him—the first article appearing in March 1942. In early 1942 Eileen changed jobs to work at the Ministry of Food and in mid-1942 the Orwells moved to a larger flat, a ground floor and basement, 10a Mortimer Crescent in Maida Vale/Kilburn—"the kind of lower-middle-class ambience that Orwell thought was London at its best." Around the same time Orwell's mother and sister Avril, who had found work in a sheet-metal factory behind King's Cross Station, moved into a flat close to George and Eileen.

At the BBC, Orwell introduced "Voice", a literary programme for his Indian broadcasts, and by now was leading an active social life with literary friends, particularly on the political left. Late in 1942, he started writing regularly for the left-wing weekly "Tribune" directed by Labour MPs Aneurin Bevan and George Strauss. In March 1943 Orwell's mother died and around the same time he told Moore he was starting work on a new book, which turned out to be "Animal Farm".

In September 1943, Orwell resigned from the BBC post that he had occupied for two years. His resignation followed a report confirming his fears that few Indians listened to the broadcasts, but he was also keen to concentrate on writing "Animal Farm". Just six days before his last day of service, on 24 November 1943, his adaptation of the fairy tale, Hans Christian Andersen's "The Emperor's New Clothes" was broadcast. It was a genre in which he was greatly interested and which appeared on "Animal Farm"s title-page. At this time he also resigned from the Home Guard on medical grounds.

In November 1943, Orwell was appointed literary editor at "Tribune", where his assistant was his old friend Jon Kimche. Orwell was on staff until early 1945, writing over 80 book reviews and on 3 December 1943 started his regular personal column, "As I Please", usually addressing three or four subjects in each. He was still writing reviews for other magazines, including "Partisan Review", "Horizon", and the New York "Nation" and becoming a respected pundit among left-wing circles but also a close friend of people on the right such as Powell, Astor and Malcolm Muggeridge. By April 1944 "Animal Farm" was ready for publication. Gollancz refused to publish it, considering it an attack on the Soviet regime which was a crucial ally in the war. A similar fate was met from other publishers (including T. S. Eliot at Faber and Faber) until Jonathan Cape agreed to take it.

In May the Orwells had the opportunity to adopt a child, thanks to the contacts of Eileen's sister Gwen O'Shaughnessy, then a doctor in Newcastle upon Tyne. In June a V-1 flying bomb struck Mortimer Crescent and the Orwells had to find somewhere else to live. Orwell had to scrabble around in the rubble for his collection of books, which he had finally managed to transfer from Wallington, carting them away in a wheelbarrow.

Another blow was Cape's reversal of his plan to publish "Animal Farm". The decision followed his personal visit to Peter Smollett, an official at the Ministry of Information. Smollett was later identified as a Soviet agent.

The Orwells spent some time in the North East, near Carlton, County Durham, dealing with matters in the adoption of a boy whom they named Richard Horatio Blair. By September 1944 they had set up home in Islington, at 27b Canonbury Square. Baby Richard joined them there, and Eileen gave up her work at the Ministry of Food to look after her family. Secker & Warburg had agreed to publish "Animal Farm", planned for the following March, although it did not appear in print until August 1945. By February 1945 David Astor had invited Orwell to become a war correspondent for the "Observer". Orwell had been looking for the opportunity throughout the war, but his failed medical reports prevented him from being allowed anywhere near action. He went to Paris after the liberation of France and to Cologne once it had been occupied by the Allies.

It was while he was there that Eileen went into hospital for a hysterectomy and died under anaesthetic on 29 March 1945. She had not given Orwell much notice about this operation because of worries about the cost and because she expected to make a speedy recovery. Orwell returned home for a while and then went back to Europe. He returned finally to London to cover the 1945 general election at the beginning of July. "Animal Farm: A Fairy Story" was published in Britain on 17 August 1945, and a year later in the US, on 26 August 1946.

"Animal Farm" had particular resonance in the post-war climate and its worldwide success made Orwell a sought-after figure. For the next four years, Orwell mixed journalistic work—mainly for "Tribune", "The Observer" and the "Manchester Evening News", though he also contributed to many small-circulation political and literary magazines—with writing his best-known work, "Nineteen Eighty-Four", which was published in 1949.
In the year following Eileen's death he published around 130 articles and a selection of his "Critical Essays", while remaining active in various political lobbying campaigns. He employed a housekeeper, Susan Watson, to look after his adopted son at the Islington flat, which visitors now described as "bleak". In September he spent a fortnight on the island of Jura in the Inner Hebrides and saw it as a place to escape from the hassle of London literary life. David Astor was instrumental in arranging a place for Orwell on Jura. Astor's family owned Scottish estates in the area and a fellow Old Etonian, Robin Fletcher, had a property on the island. In late 1945 and early 1946 Orwell made several hopeless and unwelcome marriage proposals to younger women, including Celia Kirwan (who later became Arthur Koestler's sister-in-law), Ann Popham who happened to live in the same block of flats and Sonia Brownell, one of Connolly's coterie at the "Horizon" office. Orwell suffered a tubercular haemorrhage in February 1946 but disguised his illness. In 1945 or early 1946, while still living at Canonbury Square, Orwell wrote an article on "British Cookery", complete with recipes, commissioned by the British Council. Given the post-war shortages, both parties agreed not to publish it. His sister Marjorie died of kidney disease in May and shortly after, on 22 May 1946, Orwell set off to live on the Isle of Jura at a house known as Barnhill.

This was an abandoned farmhouse with outbuildings near the northern end of the island, situated at the end of a five-mile (8 km), heavily rutted track from Ardlussa, where the owners lived. Conditions at the farmhouse were primitive but the natural history and the challenge of improving the place appealed to Orwell. His sister Avril accompanied him there and young novelist Paul Potts made up the party. In July Susan Watson arrived with Orwell's son Richard. Tensions developed and Potts departed after one of his manuscripts was used to light the fire. Orwell meanwhile set to work on "Nineteen Eighty-Four". Later Susan Watson's boyfriend David Holbrook arrived. A fan of Orwell since school days, he found the reality very different, with Orwell hostile and disagreeable probably because of Holbrook's membership of the Communist Party. Susan Watson could no longer stand being with Avril and she and her boyfriend left.

Orwell returned to London in late 1946 and picked up his literary journalism again. Now a well-known writer, he was swamped with work. Apart from a visit to Jura in the new year he stayed in London for one of the coldest British winters on record and with such a national shortage of fuel that he burnt his furniture and his child's toys. The heavy smog in the days before the Clean Air Act 1956 did little to help his health about which he was reticent, keeping clear of medical attention. Meanwhile, he had to cope with rival claims of publishers Gollancz and Warburg for publishing rights. About this time he co-edited a collection titled "British Pamphleteers" with Reginald Reynolds. As a result of the success of "Animal Farm", Orwell was expecting a large bill from the Inland Revenue and he contacted a firm of accountants of which the senior partner was Jack Harrison. The firm advised Orwell to establish a company to own his copyright and to receive his royalties and set up a "service agreement" so that he could draw a salary. Such a company "George Orwell Productions Ltd" (GOP Ltd) was set up on 12 September 1947 although the service agreement was not then put into effect. Jack Harrison left the details at this stage to junior colleagues.

Orwell left London for Jura on 10 April 1947. In July he ended the lease on the Wallington cottage. Back on Jura he worked on "Nineteen Eighty-Four" and made good progress. During that time his sister's family visited, and Orwell led a disastrous boating expedition, on 19 August, which nearly led to loss of life whilst trying to cross the notorious Gulf of Corryvreckan and gave him a soaking which was not good for his health. In December a chest specialist was summoned from Glasgow who pronounced Orwell seriously ill and a week before Christmas 1947 he was in Hairmyres Hospital in East Kilbride, then a small village in the countryside, on the outskirts of Glasgow. Tuberculosis was diagnosed and the request for permission to import streptomycin to treat Orwell went as far as Aneurin Bevan, then Minister of Health. David Astor helped with supply and payment and Orwell began his course of streptomycin on 19 or 20 February 1948. By the end of July 1948 Orwell was able to return to Jura and by December he had finished the manuscript of "Nineteen Eighty-Four". In January 1949, in a very weak condition, he set off for a sanatorium at Cranham, Gloucestershire, escorted by Richard Rees.

The sanatorium at Cranham consisted of a series of small wooden chalets or huts in a remote part of the Cotswolds near Stroud. Visitors were shocked by Orwell's appearance and concerned by the shortcomings and ineffectiveness of the treatment. Friends were worried about his finances, but by now he was comparatively well-off. He was writing to many of his friends, including Jacintha Buddicom, who had "rediscovered" him, and in March 1949, was visited by Celia Kirwan. Kirwan had just started working for a Foreign Office unit, the Information Research Department, set up by the Labour government to publish anti-communist propaganda, and Orwell gave her a list of people he considered to be unsuitable as IRD authors because of their pro-communist leanings. Orwell's list, not published until 2003, consisted mainly of writers but also included actors and Labour MPs. Orwell received more streptomycin treatment and improved slightly. In June 1949 "Nineteen Eighty-Four" was published to immediate critical and popular acclaim.

Orwell's health had continued to decline since the diagnosis of tuberculosis in December 1947. In mid-1949, he courted Sonia Brownell, and they announced their engagement in September, shortly before he was removed to University College Hospital in London. Sonia took charge of Orwell's affairs and attended him diligently in the hospital. In September 1949, Orwell invited his accountant Harrison to visit him in hospital, and Harrison claimed that Orwell then asked him to become director of GOP Ltd and to manage the company, but there was no independent witness. Orwell's wedding took place in the hospital room on 13 October 1949, with David Astor as best man. Orwell was in decline and visited by an assortment of visitors including Muggeridge, Connolly, Lucian Freud, Stephen Spender, Evelyn Waugh, Paul Potts, Anthony Powell, and his Eton tutor Anthony Gow. Plans to go to the Swiss Alps were mooted. Further meetings were held with his accountant, at which Harrison and Mr and Mrs Blair were confirmed as directors of the company, and at which Harrison claimed that the "service agreement" was executed, giving copyright to the company. Orwell's health was in decline again by Christmas. On the evening of 20 January 1950, Potts visited Orwell and slipped away on finding him asleep. Jack Harrison visited later and claimed that Orwell gave him 25% of the company. Early on the morning of 21 January, an artery burst in Orwell's lungs, killing him at age 46.

Orwell had requested to be buried in accordance with the Anglican rite in the graveyard of the closest church to wherever he happened to die. The graveyards in central London had no space, and so in an effort to ensure his last wishes could be fulfilled, his widow appealed to his friends to see whether any of them knew of a church with space in its graveyard.
David Astor lived in Sutton Courtenay, Oxfordshire, and arranged for Orwell to be interred in the churchyard of All Saints' there. Orwell's gravestone bears the epitaph: "Here lies Eric Arthur Blair, born June 25th 1903, died January 21st 1950"; no mention is made on the gravestone of his more famous pen name.

Orwell's son, Richard Horatio Blair, was brought up by Orwell's sister Avril. He is patron of The Orwell Society.

In 1979, Sonia Brownell brought a High Court action against Harrison when he declared an intention to subdivide his 25 percent share of the company between his three children. For Sonia, the consequence of this manoeuvre would be to have made getting overall control of the company three times more difficult. She was considered to have a strong case, but was becoming increasingly ill and eventually was persuaded to settle out of court on 2 November 1980. She died on 11 December 1980, aged 62.

During most of his career, Orwell was best known for his journalism, in essays, reviews, columns in newspapers and magazines and in his books of reportage: "Down and Out in Paris and London" (describing a period of poverty in these cities), "The Road to Wigan Pier" (describing the living conditions of the poor in northern England, and class division generally) and "Homage to Catalonia". According to Irving Howe, Orwell was "the best English essayist since Hazlitt, perhaps since Dr Johnson."

Modern readers are more often introduced to Orwell as a novelist, particularly through his enormously successful titles "Animal Farm" and "Nineteen Eighty-Four". The former is often thought to reflect degeneration in the Soviet Union after the Russian Revolution and the rise of Stalinism; the latter, life under totalitarian rule. "Nineteen Eighty-Four" is often compared to "Brave New World" by Aldous Huxley; both are powerful dystopian novels warning of a future world where the state machine exerts complete control over social life. In 1984, "Nineteen Eighty-Four" and Ray Bradbury's "Fahrenheit 451" were honoured with the Prometheus Award for their contributions to dystopian literature. In 2011 he received it again for "Animal Farm".

"Coming Up for Air", his last novel before World War II, is the most "English" of his novels; alarms of war mingle with images of idyllic Thames-side Edwardian childhood of protagonist George Bowling. The novel is pessimistic; industrialism and capitalism have killed the best of Old England, and there were great, new external threats. In homely terms, its protagonist George Bowling posits the totalitarian hypotheses of Franz Borkenau, Orwell, Ignazio Silone and Koestler: "Old Hitler's something different. So's Joe Stalin. They aren't like these chaps in the old days who crucified people and chopped their heads off and so forth, just for the fun of it ... They're something quite new—something that's never been heard of before".

In an autobiographical piece that Orwell sent to the editors of "Twentieth Century Authors" in 1940, he wrote: "The writers I care about most and never grow tired of are: Shakespeare, Swift, Fielding, Dickens, Charles Reade, Flaubert and, among modern writers, James Joyce, T. S. Eliot and D. H. Lawrence. But I believe the modern writer who has influenced me most is W. Somerset Maugham, whom I admire immensely for his power of telling a story straightforwardly and without frills." Elsewhere, Orwell strongly praised the works of Jack London, especially his book "The Road". Orwell's investigation of poverty in "The Road to Wigan Pier" strongly resembles that of Jack London's "The People of the Abyss", in which the American journalist disguises himself as an out-of-work sailor to investigate the lives of the poor in London. In his essay "Politics vs. Literature: An Examination of Gulliver's Travels" (1946) Orwell wrote: "If I had to make a list of six books which were to be preserved when all others were destroyed, I would certainly put "Gulliver's Travels" among them."

Orwell was an admirer of Arthur Koestler and became a close friend during the three years that Koestler and his wife Mamain spent at the cottage of Bwlch Ocyn, a secluded farmhouse that belonged to Clough Williams-Ellis, in the Vale of Ffestiniog. Orwell reviewed Koestler's. "Darkness at Noon" for the "New Statesman" in 1941, saying:

Brilliant as this book is as a novel, and a piece of brilliant literature, it is probably most valuable as an interpretation of the Moscow "confessions" by someone with an inner knowledge of totalitarian methods. What was frightening about these trials was not the fact that they happened—for obviously such things are necessary in a totalitarian society—but the eagerness of Western intellectuals to justify them.
Other writers admired by Orwell included: Ralph Waldo Emerson, George Gissing, Graham Greene, Herman Melville, Henry Miller, Tobias Smollett, Mark Twain, Joseph Conrad, and Yevgeny Zamyatin. He was both an admirer and a critic of Rudyard Kipling, praising Kipling as a gifted writer and a "good bad poet" whose work is "spurious" and "morally insensitive and aesthetically disgusting," but undeniably seductive and able to speak to certain aspects of reality more effectively than more enlightened authors. He had a similarly ambivalent attitude to G. K. Chesterton, whom he regarded as a writer of considerable talent who had chosen to devote himself to "Roman Catholic propaganda", and to Evelyn Waugh, who was, he wrote, "ab[ou]t as good a novelist as one can be (i.e. as novelists go today) while holding untenable opinions".

In 1980, the English Heritage honoured Orwell with a blue plaque at 50 Lawford Road, Kentish Town, London, where he lived from August 1935 until January 1936.

Throughout his life Orwell continually supported himself as a book reviewer. His reviews are well known and have had an influence on literary criticism. He wrote in the conclusion to his 1940 essay on Charles Dickens,
George Woodcock suggested that the last two sentences also describe Orwell.

Orwell wrote a critique of George Bernard Shaw's play "Arms and the Man". He considered this Shaw's best play and the most likely to remain socially relevant, because of its theme that war is not, generally speaking, a glorious romantic adventure. His 1945 essay "In Defence of P.G. Wodehouse" contains an amusing assessment of Wodehouse's writing and also argues that his broadcasts from Germany (during the war) did not really make him a traitor. He accused The Ministry of Information of exaggerating Wodehouse's actions for propaganda purposes.

In 1946, the British Council commissioned Orwell to write an essay on British food as part of a drive to promote British relations abroad. In the essay titled "British Cookery," Orwell described the British diet as "a simple, rather heavy, perhaps slightly barbarous diet" and where "hot drinks are acceptable at most hours of the day". He discusses the ritual of breakfast in the UK, "this is not a snack but a serious meal. The hour at which people have their breakfast is of course governed by the time at which they go to work." He wrote that high tea in the United Kingdom consisted of a variety of savoury and sweet dishes, but "no tea would be considered a good one if it did not include at least one kind of cake.” Orwell also added a recipe for marmalade, a popular British spread on bread. However, the British Council declined to publish the essay on the grounds that it was too problematic to write about food at the time of strict rationing in the UK. In 2019, the essay was discovered in the British Council's archives along with the rejection letter. The British Council issued an official apology to Orwell over the rejection of the commissioned essay.

Arthur Koestler said that Orwell's "uncompromising intellectual honesty made him appear almost inhuman at times." Ben Wattenberg stated: "Orwell's writing pierced intellectual hypocrisy wherever he found it." According to historian Piers Brendon, "Orwell was the saint of common decency who would in earlier days, said his BBC boss Rushbrook Williams, 'have been either canonised—or burnt at the stake'". Raymond Williams in describes Orwell as a "successful impersonation of a plain man who bumps into experience in an unmediated way and tells the truth about it." Christopher Norris declared that Orwell's "homespun empiricist outlook—his assumption that the truth was just there to be told in a straightforward common-sense way—now seems not merely naïve but culpably self-deluding". The American scholar Scott Lucas has described Orwell as an enemy of the Left. John Newsinger has argued that Lucas could only do this by portraying "all of Orwell's attacks on Stalinism [–] as if they were attacks on socialism, despite Orwell's continued insistence that they were not."

Orwell's work has taken a prominent place in the school literature curriculum in England, with "Animal Farm" a regular examination topic at the end of secondary education (GCSE), and "Nineteen Eighty-Four" a topic for subsequent examinations below university level (A Levels). A 2016 UK poll saw "Animal Farm" ranked the nation's favourite book from school.

Historian John Rodden stated: "John Podhoretz did claim that if Orwell were alive today, he'd be standing with the neo-conservatives and against the Left. And the question arises, to what extent can you even begin to predict the political positions of somebody who's been dead three decades and more by that time?"

In "Orwell's Victory", Christopher Hitchens argues: "In answer to the accusation of inconsistency Orwell as a writer was forever taking his own temperature. In other words, here was someone who never stopped testing and adjusting his intelligence".

John Rodden points out the "undeniable conservative features in the Orwell physiognomy" and remarks on how "to some extent Orwell facilitated the kinds of uses and abuses by the Right that his name has been put to. In other ways there has been the politics of selective quotation." Rodden refers to the essay "Why I Write", in which Orwell refers to the Spanish Civil War as being his "watershed political experience", saying: "The Spanish War and other events in 1936–37, turned the scale. Thereafter I knew where I stood. Every line of serious work that I have written since 1936 has been written directly or indirectly against totalitarianism and "for" Democratic Socialism as I understand it." (emphasis in original) Rodden goes on to explain how, during the McCarthy era, the introduction to the Signet edition of "Animal Farm", which sold more than 20 million copies, makes use of "the politics of ellipsis":

If the book itself, "Animal Farm", had left any doubt of the matter, Orwell dispelled it in his essay "Why I Write": 'Every line of serious work that I've written since 1936 has been written directly or indirectly against Totalitarianism ... dot, dot, dot, dot.' "For Democratic Socialism" is vaporised, just like Winston Smith did it at the Ministry of Truth, and that's very much what happened at the beginning of the McCarthy era and just continued, Orwell being selectively quoted.

Fyvel wrote about Orwell: "His crucial experience [...] was his struggle to turn himself into a writer, one which led through long periods of poverty, failure and humiliation, and about which he has written almost nothing directly. The sweat and agony was less in the slum-life than in the effort to turn the experience into literature."

In October 2015 Finlay Publisher, for the Orwell Society, published "George Orwell 'The Complete Poetry"', compiled and presented by Dione Venables.

In his essay "Politics and the English Language" (1946), Orwell wrote about the importance of precise and clear language, arguing that vague writing can be used as a powerful tool of political manipulation because it shapes the way we think. In that essay, Orwell provides six rules for writers:

Andrew N. Rubin argues that "Orwell claimed that we should be attentive to how the use of language has limited our capacity for critical thought just as we should be equally concerned with the ways in which dominant modes of thinking have reshaped the very language that we use."

The adjective "Orwellian" connotes an attitude and a policy of control by propaganda, surveillance, misinformation, denial of truth and manipulation of the past. In "Nineteen Eighty-Four", Orwell described a totalitarian government that controlled thought by controlling language, making certain ideas literally unthinkable. Several words and phrases from "Nineteen Eighty-Four" have entered popular language. "Newspeak" is a simplified and obfuscatory language designed to make independent thought impossible. "Doublethink" means holding two contradictory beliefs simultaneously. The "Thought Police" are those who suppress all dissenting opinion. "Prolefeed" is homogenised, manufactured superficial literature, film and music used to control and indoctrinate the populace through docility. "Big Brother" is a supreme dictator who watches everyone.

Orwell may have been the first to use the term "cold war" to refer to the state of tension between powers in the Western Bloc and the Eastern Bloc that followed World War II in his essay, "You and the Atom Bomb", published in "Tribune" on 19 October 1945. He wrote:
We may be heading not for general breakdown but for an epoch as horribly stable as the slave empires of antiquity. James Burnham's theory has been much discussed, but few people have yet considered its ideological implications—this is, the kind of world-view, the kind of beliefs, and the social structure that would probably prevail in a State which was at once unconquerable and in a permanent state of 'cold war' with its neighbours.

In 2014, a play written by playwright Joe Sutton titled "Orwell in America" was first performed by the Northern Stage theatre company in White River Junction, Vermont. It is a fictitious account of Orwell doing a book tour in the United States (something he never did in his lifetime). It moved to Off-Broadway in 2016.

Orwell's birthplace, a bungalow in Motihari, Bihar, India, was opened as a museum in May 2015.

A statue of George Orwell, sculpted by the British sculptor Martin Jennings, was unveiled on 7 November 2017 outside Broadcasting House, the headquarters of the BBC. The wall behind the statue is inscribed with the following phrase: "If liberty means anything at all, it means the right to tell people what they do not want to hear". These are words from his proposed preface to "Animal Farm" and a rallying cry for the idea of free speech in an open society.

Jacintha Buddicom's account, "Eric & Us", provides an insight into Blair's childhood. She quoted his sister Avril that "he was essentially an aloof, undemonstrative person" and said herself of his friendship with the Buddicoms: "I do not think he needed any other friends beyond the schoolfriend he occasionally and appreciatively referred to as 'CC'". She could not recall his having schoolfriends to stay and exchange visits as her brother Prosper often did in holidays. Cyril Connolly provides an account of Blair as a child in "Enemies of Promise". Years later, Blair mordantly recalled his prep school in the essay "Such, Such Were the Joys", claiming among other things that he "was made to study like a dog" to earn a scholarship, which he alleged was solely to enhance the school's prestige with parents. Jacintha Buddicom repudiated Orwell's schoolboy misery described in the essay, stating that "he was a specially happy child". She noted that he did not like his name because it reminded him of a book he greatly disliked—"Eric, or, Little by Little", a Victorian boys' school story.

Connolly remarked of him as a schoolboy, "The remarkable thing about Orwell was that alone among the boys he was an intellectual and not a parrot for he thought for himself". At Eton, John Vaughan Wilkes, his former headmaster's son recalled that "he was extremely argumentative—about anything—and criticising the masters and criticising the other boys [...] We enjoyed arguing with him. He would generally win the arguments—or think he had anyhow." Roger Mynors concurs: "Endless arguments about all sorts of things, in which he was one of the great leaders. He was one of those boys who thought for himself."

Blair liked to carry out practical jokes. Buddicom recalls him swinging from the luggage rack in a railway carriage like an orangutan to frighten a woman passenger out of the compartment. At Eton, he played tricks on John Crace, his Master in College, among which was to enter a spoof advertisement in a College magazine implying pederasty. Gow, his tutor, said he "made himself as big a nuisance as he could" and "was a very unattractive boy". Later Blair was expelled from the crammer at Southwold for sending a dead rat as a birthday present to the town surveyor. In one of his "As I Please" essays he refers to a protracted joke when he answered an advertisement for a woman who claimed a cure for obesity.

Blair had an interest in natural history which stemmed from his childhood. In letters from school he wrote about caterpillars and butterflies, and Buddicom recalls his keen interest in ornithology. He also enjoyed fishing and shooting rabbits, and conducting experiments as in cooking a hedgehog or shooting down a jackdaw from the Eton roof to dissect it. His zeal for scientific experiments extended to explosives—again Buddicom recalls a cook giving notice because of the noise. Later in Southwold, his sister Avril recalled him blowing up the garden. When teaching he enthused his students with his nature-rambles both at Southwold and Hayes. His adult diaries are permeated with his observations on nature.

Buddicom and Blair lost touch shortly after he went to Burma and she became unsympathetic towards him. She wrote that it was because of the letters he wrote complaining about his life, but an addendum to "Eric & Us" by Venables reveals that he may have lost her sympathy through an incident which was, at best, a clumsy attempt at seduction.

Mabel Fierz, who later became Blair's confidante, said: "He used to say the one thing he wished in this world was that he'd been attractive to women. He liked women and had many girlfriends I think in Burma. He had a girl in Southwold and another girl in London. He was rather a womaniser, yet he was afraid he wasn't attractive."

Brenda Salkield (Southwold) preferred friendship to any deeper relationship and maintained a correspondence with Blair for many years, particularly as a sounding board for his ideas. She wrote: "He was a great letter writer. Endless letters, and I mean when he wrote you a letter he wrote pages." His correspondence with Eleanor Jacques (London) was more prosaic, dwelling on a closer relationship and referring to past rendezvous or planning future ones in London and Burnham Beeches.

When Orwell was in the sanatorium in Kent, his wife's friend Lydia Jackson visited. He invited her for a walk and out of sight "an awkward situation arose." Jackson was to be the most critical of Orwell's marriage to Eileen O'Shaughnessy, but their later correspondence hints at a complicity. Eileen at the time was more concerned about Orwell's closeness to Brenda Salkield. Orwell had an affair with his secretary at "Tribune" which caused Eileen much distress, and others have been mooted. In a letter to Ann Popham he wrote: "I was sometimes unfaithful to Eileen, and I also treated her badly, and I think she treated me badly, too, at times, but it was a real marriage, in the sense that we had been through awful struggles together and she understood all about my work, etc." Similarly he suggested to Celia Kirwan that they had both been unfaithful. There are several testaments that it was a well-matched and happy marriage.

In June 1944, Orwell and Eileen adopted a three-week-old boy they named Richard Horatio. According to Richard, Orwell was a wonderful father who gave him devoted, if rather rugged, attention and a great degree of freedom. After Orwell's death Richard went to live with Orwell's sister and her husband.

Blair was very lonely after Eileen's death in 1945, and desperate for a wife, both as companion for himself and as mother for Richard. He proposed marriage to four women, including Celia Kirwan, and eventually Sonia Brownell accepted. Orwell had met her when she was assistant to Cyril Connolly, at "Horizon" literary magazine. They were married on 13 October 1949, only three months before Orwell's death. Some maintain that Sonia was the model for Julia in "Nineteen Eighty-Four".

Orwell was noted for very close and enduring friendships with a few friends, but these were generally people with a similar background or with a similar level of literary ability. Ungregarious, he was out of place in a crowd and his discomfort was exacerbated when he was outside his own class. Though representing himself as a spokesman for the common man, he often appeared out of place with real working people. His brother-in-law Humphrey Dakin, a "Hail fellow, well met" type, who took him to a local pub in Leeds, said that he was told by the landlord: "Don't bring that bugger in here again." Adrian Fierz commented "He wasn't interested in racing or greyhounds or pub crawling or shove ha'penny. He just did not have much in common with people who did not share his intellectual interests." Awkwardness attended many of his encounters with working-class representatives, as with Pollitt and McNair, but his courtesy and good manners were often commented on. Jack Common observed on meeting him for the first time, "Right away manners, and more than manners—breeding—showed through."

In his tramping days, he did domestic work for a time. His extreme politeness was recalled by a member of the family he worked for; she declared that the family referred to him as "Laurel" after the film comedian. With his gangling figure and awkwardness, Orwell's friends often saw him as a figure of fun. Geoffrey Gorer commented "He was awfully likely to knock things off tables, trip over things. I mean, he was a gangling, physically badly co-ordinated young man. I think his feeling [was] that even the inanimate world was against him." When he shared a flat with Heppenstall and Sayer, he was treated in a patronising manner by the younger men. At the BBC in the 1940s, "everybody would pull his leg" and Spender described him as having real entertainment value "like, as I say, watching a Charlie Chaplin movie." A friend of Eileen's reminisced about her tolerance and humour, often at Orwell's expense.

One biography of Orwell accused him of having had an authoritarian streak. In Burma, he struck out at a Burmese boy who, while "fooling around" with his friends, had "accidentally bumped into him" at a station, resulting in Orwell falling "heavily" down some stairs. One of his former pupils recalled being beaten so hard he could not sit down for a week. When sharing a flat with Orwell, Heppenstall came home late one night in an advanced stage of loud inebriation. The upshot was that Heppenstall ended up with a bloody nose and was locked in a room. When he complained, Orwell hit him across the legs with a shooting stick and Heppenstall then had to defend himself with a chair. Years later, after Orwell's death, Heppenstall wrote a dramatic account of the incident called "The Shooting Stick" and Mabel Fierz confirmed that Heppenstall came to her in a sorry state the following day.

Orwell got on well with young people. The pupil he beat considered him the best of teachers and the young recruits in Barcelona tried to drink him under the table without success. His nephew recalled Uncle Eric laughing louder than anyone in the cinema at a Charlie Chaplin film.

In the wake of his most famous works, he attracted many uncritical hangers-on, but many others who sought him found him aloof and even dull. With his soft voice, he was sometimes shouted down or excluded from discussions. At this time, he was severely ill; it was wartime or the austerity period after it; during the war his wife suffered from depression; and after her death he was lonely and unhappy. In addition to that, he always lived frugally and seemed unable to care for himself properly. As a result of all this, people found his circumstances bleak. Some, like Michael Ayrton, called him "Gloomy George," but others developed the idea that he was an "English secular saint."

Although Orwell was frequently heard on the BBC for panel discussion and one-man broadcasts, no recorded copy of his voice is known to exist.

Orwell was a heavy smoker, who rolled his own cigarettes from strong shag tobacco, despite his bronchial condition. His penchant for the rugged life often took him to cold and damp situations, both in the long term, as in Catalonia and Jura, and short term, for example, motorcycling in the rain and suffering a shipwreck. Described by "The Economist" as "perhaps the 20th century's best chronicler of English culture", Orwell considered fish and chips, association football, the pub, strong tea, cut price chocolate, the movies, and radio among the chief comforts for the working class.

Orwell enjoyed strong tea—he had Fortnum & Mason's tea brought to him in Catalonia. His 1946 essay, "A Nice Cup of Tea", appeared in the "Evening Standard" article on how to make tea, with Orwell writing, "tea is one of the mainstays of civilisation in this country and causes violent disputes over how it should be made", with the main issue being whether to put tea in the cup first and add the milk afterward, or the other way round, on which he states, "in every family in Britain there are probably two schools of thought on the subject". He appreciated English beer, taken regularly and moderately, despised drinkers of lager and wrote about an imagined, ideal British pub in his 1946 "Evening Standard" article, "The Moon Under Water". Not as particular about food, he enjoyed the wartime "Victory Pie" and extolled canteen food at the BBC. He preferred traditional English dishes, such as roast beef, and kippers. His 1945 essay, "In Defence of English Cooking", included Yorkshire pudding, crumpets, muffins, innumerable biscuits, Christmas pudding, shortbread, various British cheeses and Oxford marmalade. Reports of his Islington days refer to the cosy afternoon tea table.

His dress sense was unpredictable and usually casual. In Southwold, he had the best cloth from the local tailor but was equally happy in his tramping outfit. His attire in the Spanish Civil War, along with his size-12 boots, was a source of amusement. David Astor described him as looking like a prep school master, while according to the Special Branch dossier, Orwell's tendency to dress "in Bohemian fashion" revealed that the author was "a Communist".

Orwell's confusing approach to matters of social decorum—on the one hand expecting a working-class guest to dress for dinner, and on the other, slurping tea out of a saucer at the BBC canteen—helped stoke his reputation as an English eccentric.

Orwell was an atheist who identified himself with the humanist outlook on life. Despite this, and despite his criticisms of both religious doctrine and of religious organisations, he nevertheless regularly participated in the social and civic life of the church, including by attending Church of England Holy Communion. Acknowledging this contradiction, he once said: "It seems rather mean to go to HC [Holy Communion] when one doesn't believe, but I have passed myself off for pious & there is nothing for it but to keep up with the deception." He had two Anglican marriages and left instructions for an Anglican funeral. Orwell was also extremely well-read in Biblical literature and could quote lengthy passages from the Book of Common Prayer from memory. His extensive knowledge of the Bible came coupled with unsparing criticism of its philosophy, and as an adult he could not bring himself to believe in its tenets. He said in part V of his essay, "Such, Such Were the Joys", that "Till about the age of fourteen I believed in God, and believed that the accounts given of him were true. But I was well aware that I did not love him." Orwell directly contrasted Christianity with secular humanism in his essay "Lear, Tolstoy and the Fool", finding the latter philosophy more palatable and less "self-interested." Literary critic James Wood wrote that in the struggle, as he saw it, between Christianity and humanism, "Orwell was on the humanist side, of course—basically an unmetaphysical, English version of Camus's philosophy of perpetual godless struggle."

Orwell's writing was often explicitly critical of religion, and Christianity in particular. He found the church to be a "selfish [...] church of the landed gentry" with its establishment "out of touch" with the majority of its communicants and altogether a pernicious influence on public life. In their 1972 study, "The Unknown Orwell", the writers Peter Stansky and William Abrahams noted that at Eton Blair displayed a "sceptical attitude" to Christian belief. Crick observed that Orwell displayed "a pronounced anti-Catholicism". Evelyn Waugh, writing in 1946, acknowledged Orwell's high moral sense and respect for justice but believed "he seems never to have been touched at any point by a conception of religious thought and life." His contradictory and sometimes ambiguous views about the social benefits of religious affiliation mirrored the dichotomies between his public and private lives: Stephen Ingle wrote that it was as if the writer George Orwell "vaunted" his unbelief while Eric Blair the individual retained "a deeply ingrained religiosity".

Orwell liked to provoke arguments by challenging the status quo, but he was also a traditionalist with a love of old English values. He criticised and satirised, from the inside, the various social milieux in which he found himself—provincial town life in "A Clergyman's Daughter"; middle-class pretension in "Keep the Aspidistra Flying"; preparatory schools in "Such, Such Were the Joys"; colonialism in "Burmese Days", and some socialist groups in "The Road to Wigan Pier". In his "Adelphi" days, he described himself as a "Tory-anarchist".

In 1928, Orwell began his career as a professional writer in Paris at a journal owned by the French Communist Henri Barbusse. His first article, "La Censure en Angleterre" ("Censorship in England"), was an attempt to account for the "extraordinary and illogical" moral censorship of plays and novels then practised in Britain. His own explanation was that the rise of the "puritan middle class", who had stricter morals than the aristocracy, tightened the rules of censorship in the 19th century. Orwell's first published article in his home country, "A Farthing Newspaper", was a critique of the new French daily the "Ami de Peuple". This paper was sold much more cheaply than most others, and was intended for ordinary people to read. Orwell pointed out that its proprietor François Coty also owned the right-wing dailies "Le Figaro" and "Le Gaulois", which the "Ami de Peuple" was supposedly competing against. Orwell suggested that cheap newspapers were no more than a vehicle for advertising and anti-leftist propaganda, and predicted the world might soon see free newspapers which would drive legitimate dailies out of business.

Writing for "Le Progrès Civique", Orwell described the British colonial government in Burma and India:

The Spanish Civil War played the most important part in defining Orwell's socialism. He wrote to Cyril Connolly from Barcelona on 8 June 1937: "I have seen wonderful things and at last really believe in Socialism, which I never did before." Having witnessed the success of the anarcho-syndicalist communities, for example in Anarchist Catalonia, and the subsequent brutal suppression of the anarcho-syndicalists, anti-Stalin communist parties and revolutionaries by the Soviet Union-backed Communists, Orwell returned from Catalonia a staunch anti-Stalinist and joined the British Independent Labour Party, his card being issued on 13 June 1938. Although he was never a Trotskyist, he was strongly influenced by the Trotskyist and anarchist critiques of the Soviet regime, and by the anarchists' emphasis on individual freedom. In Part 2 of "The Road to Wigan Pier", published by the Left Book Club, Orwell stated that "a real Socialist is one who wishes—not merely conceives it as desirable, but actively wishes—to see tyranny overthrown". Orwell stated in "Why I Write" (1946): "Every line of serious work that I have written since 1936 has been written, directly or indirectly, against totalitarianism and for democratic socialism, as I understand it." Orwell was a proponent of a federal socialist Europe, a position outlined in his 1947 essay "Toward European Unity," which first appeared in "Partisan Review". According to biographer John Newsinger:
In his 1938 essay "Why I joined the Independent Labour Party," published in the ILP-affiliated "New Leader", Orwell wrote:
Towards the end of the essay, he wrote: "I do not mean I have lost all faith in the Labour Party. My most earnest hope is that the Labour Party will win a clear majority in the next General Election."

Orwell was opposed to rearmament against Nazi Germany and at the time of the Munich Agreement he signed a manifesto entitled "If War Comes We Shall Resist"—but he changed his view after the Molotov–Ribbentrop Pact and the outbreak of the war. He left the ILP because of its opposition to the war and adopted a political position of "revolutionary patriotism". In December 1940 he wrote in "Tribune" (the Labour left's weekly): "We are in a strange period of history in which a revolutionary has to be a patriot and a patriot has to be a revolutionary." During the war, Orwell was highly critical of the popular idea that an Anglo-Soviet alliance would be the basis of a post-war world of peace and prosperity. In 1942, commenting on London "Times" editor E. H. Carr's pro-Soviet views, Orwell stated that "all the appeasers, e.g. Professor E.H. Carr, have switched their allegiance from Hitler to Stalin".

On anarchism, Orwell wrote in "The Road to Wigan Pier": "I worked out an anarchistic theory that all government is evil, that the punishment always does more harm than the crime and the people can be trusted to behave decently if you will only let them alone." He continued and argued that "it is always necessary to protect peaceful people from violence. In any state of society where crime can be profitable you have got to have a harsh criminal law and administer it ruthlessly."

In his reply (dated 15 November 1943) to an invitation from the Duchess of Atholl to speak for the British League for European Freedom, he stated that he did not agree with their objectives. He admitted that what they said was "more truthful than the lying propaganda found in most of the press", but added that he could not "associate himself with an essentially Conservative body" that claimed to "defend democracy in Europe" but had "nothing to say about British imperialism". His closing paragraph stated: "I belong to the Left and must work inside it, much as I hate Russian totalitarianism and its poisonous influence in this country."

Orwell joined the staff of "Tribune" as literary editor, and from then until his death, was a left-wing (though hardly orthodox) Labour-supporting democratic socialist. 

On 1 September 1944, about the Warsaw uprising, Orwell expressed in "Tribune" his hostility against the influence of the alliance with the USSR over the allies: "Do remember that dishonesty and cowardice always have to be paid for. Do not imagine that for years on end you can make yourself the boot-licking propagandist of the sovietic regime, or any other regime, and then suddenly return to honesty and reason. Once a whore, always a whore." According to Newsinger, although Orwell "was always critical of the 1945–51 Labour government's moderation, his support for it began to pull him to the right politically. This did not lead him to embrace conservatism, imperialism or reaction, but to defend, albeit critically, Labour reformism." Between 1945 and 1947, with A. J. Ayer and Bertrand Russell, he contributed a series of articles and essays to "Polemic", a short-lived British "Magazine of Philosophy, Psychology, and Aesthetics" edited by the ex-Communist Humphrey Slater.

Writing in early 1945 a long essay titled "Antisemitism in Britain," for the "Contemporary Jewish Record", Orwell stated that antisemitism was on the increase in Britain and that it was "irrational and will not yield to arguments". He argued that it would be useful to discover why anti-Semites could "swallow such absurdities on one particular subject while remaining sane on others". He wrote: "For quite six years the English admirers of Hitler contrived not to learn of the existence of Dachau and Buchenwald. ... Many English people have heard almost nothing about the extermination of German and Polish Jews during the present war. Their own anti-Semitism has caused this vast crime to bounce off their consciousness." In "Nineteen Eighty-Four", written shortly after the war, Orwell portrayed the Party as enlisting anti-Semitic passions against their enemy, Goldstein.

Orwell publicly defended P. G. Wodehouse against charges of being a Nazi sympathiser—occasioned by his agreement to do some broadcasts over the German radio in 1941—a defence based on Wodehouse's lack of interest in and ignorance of politics.

Special Branch, the intelligence division of the Metropolitan Police, maintained a file on Orwell for more than 20 years of his life. The dossier, published by The National Archives, states that, according to one investigator, Orwell had "advanced Communist views and several of his Indian friends say that they have often seen him at Communist meetings". MI5, the intelligence department of the Home Office, noted: "It is evident from his recent writings—'The Lion and the Unicorn'—and his contribution to Gollancz's symposium "The Betrayal of the Left" that he does not hold with the Communist Party nor they with him."

Sexual politics plays an important role in "Nineteen Eighty-Four". In the novel, people's intimate relationships are strictly governed by the party's Junior Anti-Sex League, by opposing sexual relations and instead encouraging artificial insemination. Personally, Orwell disliked what he thought as misguided middle-class revolutionary emancipatory views, expressing disdain for "every fruit-juice drinker, nudist, sandal-wearer, sex-maniacs."

Orwell was also openly against homosexuality, at a time when such prejudice was common. Speaking at the 2003 George Orwell Centenary Conference, Daphne Patai said: "Of course he was homophobic. That has nothing to do with his relations with his homosexual friends. Certainly, he had a negative attitude and a certain kind of anxiety, a denigrating attitude towards homosexuality. That is definitely the case. I think his writing reflects that quite fully."

Orwell used the homophobic epithets "nancy" and "pansy", such in his expressions of contempt for what he called the "pansy Left", and "nancy poets", i.e. left-wing homosexual or bisexual writers and intellectuals such as Stephen Spender and W. H. Auden. The protagonist of "Keep the Aspidistra Flying", Gordon Comstock, conducts an internal critique of his customers when working in a bookshop, and there is an extended passage of several pages in which he concentrates on a homosexual male customer, and sneers at him for his "nancy" characteristics, including a lisp, which he identifies in detail, with some disgust. Stephen Spender "thought Orwell's occasional homophobic outbursts were part of his rebellion against the public school".

Orwell's will requested that no biography of him be written, and his widow, Sonia Brownell, repelled every attempt by those who tried to persuade her to let them write about him. Various recollections and interpretations were published in the 1950s and '60s, but Sonia saw the 1968 "Collected Works" as the record of his life. She did appoint Malcolm Muggeridge as official biographer, but later biographers have seen this as deliberate spoiling as Muggeridge eventually gave up the work. In 1972, two American authors, Peter Stansky and William Abrahams, produced "The Unknown Orwell", an unauthorised account of his early years that lacked any support or contribution from Sonia Brownell.

Sonia Brownell then commissioned Bernard Crick, a professor of politics at the University of London, to complete a biography and asked Orwell's friends to co-operate. Crick collated a considerable amount of material in his work, which was published in 1980, but his questioning of the factual accuracy of Orwell's first-person writings led to conflict with Brownell, and she tried to suppress the book. Crick concentrated on the facts of Orwell's life rather than his character, and presented primarily a political perspective on Orwell's life and work.

After Sonia Brownell's death, other works on Orwell were published in the 1980s, particularly in 1984. These included collections of reminiscences by Coppard and Crick and Stephen Wadhams.

In 1991, Michael Shelden, an American professor of literature, published a biography. More concerned with the literary nature of Orwell's work, he sought explanations for Orwell's character and treated his first-person writings as autobiographical. Shelden introduced new information that sought to build on Crick's work. Shelden speculated that Orwell possessed an obsessive belief in his failure and inadequacy.

Peter Davison's publication of the "Complete Works of George Orwell", completed in 2000, made most of the Orwell Archive accessible to the public. Jeffrey Meyers, a prolific American biographer, was first to take advantage of this and published a book in 2001 that investigated the darker side of Orwell and questioned his saintly image. "Why Orwell Matters" (released in the United Kingdom as "Orwell's Victory") was published by Christopher Hitchens in 2002.

In 2003, the centenary of Orwell's birth resulted in biographies by Gordon Bowker and D. J. Taylor, both academics and writers in the United Kingdom. Taylor notes the stage management which surrounds much of Orwell's behaviour and Bowker highlights the essential sense of decency which he considers to have been Orwell's main motivation.






</doc>
<doc id="11894" url="https://en.wikipedia.org/wiki?curid=11894" title="Goeldi's marmoset">
Goeldi's marmoset

The Goeldi's marmoset or Goeldi's monkey ("Callimico goeldii") is a small, South American New World monkey that lives in the upper Amazon basin region of Bolivia, Brazil, Colombia, Ecuador, and Peru. It is the only species classified in the genus Callimico, and the monkeys are sometimes referred to as "callimicos".

Goeldi's marmosets are blackish or blackish-brown in color and the hair on their head and tail sometimes has red, white, or silverly brown highlights. Their bodies are about long, and their tails are about long.

Goeldi's marmoset was first described in 1904, making "Callimico" one of the more recent monkey genera to be described. In older classification schemes it was sometimes placed in its own family Callimiconidae and sometimes, along with the marmosets and tamarins, in the subfamily Callitrichinae in the family Cebidae. More recently, Callitrichinae has been (re-)elevated to family status as Callitrichidae.

Females reach sexual maturity at 8.5 months, males at 16.5 months. The gestation period lasts from 140 to 180 days. Unlike other New World monkeys, they have the capacity to give birth twice a year. The mother carries a single baby monkey per pregnancy, whereas most other species in the family Callitrichidae usually give birth to twins. For the first 2–3 weeks the mother acts as the primary caregiver until the father takes over most of the responsibilities except for nursing. The infant is weaned after about 65 days. Females outnumber males by 2 to 1. The life expectancy in captivity is about 10 years. The monkeys are able to jump as far as one end of a tennis court to another.

Goeldi's marmosets prefer to forage in dense scrubby undergrowth; perhaps because of this, they are rare, with groups living in separate patches of suitable habitat, separated by miles of unsuitable flora. In the wet season, their diet includes fruit, insects, spiders, lizards, frogs, and snakes. In the dry season, they feed on fungi, the only tropical primates known to depend on this source of food. They live in small social groups (approximately six individuals) that stay within a few feet of one another most of the time, staying in contact via high-pitched calls. They are also known to form polyspecific groups with tamarins, perhaps because Goeldi's marmosets are not known to have the X-linked polymorphism which enables some individuals of other New World monkey species to see in full tri-chromatic vision.

The species takes its name from its discoverer, the Swiss naturalist Emil August Goeldi.



</doc>
<doc id="11921" url="https://en.wikipedia.org/wiki?curid=11921" title="Gambling">
Gambling

Gambling (also known as betting) is the wagering of money or something of value (referred to as "the stakes") on an event with an uncertain outcome, with the primary intent of winning money or material goods. Gambling thus requires three elements to be present: consideration (an amount wagered), risk (chance), and a prize. The outcome of the wager is often immediate, such as a single roll of dice, a spin of a roulette wheel, or a horse crossing the finish line, but longer time frames are also common, allowing wagers on the outcome of a future sports contest or even an entire sports season.

The term "gaming" in this context typically refers to instances in which the activity has been specifically permitted by law. The two words are not mutually exclusive; "i.e.", a "gaming" company offers (legal) "gambling" activities to the public and may be regulated by one of many gaming control boards, for example, the Nevada Gaming Control Board. However, this distinction is not universally observed in the English-speaking world. For instance, in the United Kingdom, the regulator of gambling activities is called the Gambling Commission (not the Gaming Commission). The word "gaming" is used more frequently since the rise of computer and video games to describe activities that do not necessarily involve wagering, especially online gaming, with the new usage still not having displaced the old usage as the primary definition in common dictionaries. "Gaming" has also been used to circumvent laws against "gambling". The media and others have used one term or the other to frame conversations around the subjects, resulting in a shift of perceptions among their audiences.

Gambling is also a major international commercial activity, with the legal gambling market totaling an estimated $335 billion in 2009. In other forms, gambling can be conducted with materials which have a value, but are not real money. For example, players of marbles games might wager marbles, and likewise games of "Pogs" or "" can be played with the collectible game pieces (respectively, small discs and trading cards) as stakes, resulting in a meta-game regarding the value of a player's collection of pieces.

Gambling dates back to the Paleolithic period, before written history. In Mesopotamia the earliest six-sided dice date to about 3000 BC. However, they were based on astragali dating back thousands of years earlier. In China, gambling houses were widespread in the first millennium BC, and betting on fighting animals was common. Lotto games and dominoes (precursors of Pai Gow) appeared in China as early as the 10th century.

Playing cards appeared in the 9th century AD in China. Records trace gambling in Japan back at least as far as the 14th century.

Poker, the most popular U.S. card game associated with gambling, derives from the Persian game As-Nas, dating back to the 17th century.

The first known casino, the Ridotto, started operating in 1638 in Venice, Italy.

Gambling has been a main recreational activity in Great Britain for centuries. Horseracing has been a favorite theme for over three centuries. It has been heavily regulated. Historically much of the opposition comes from evangelical Protestants, and from social reformers.

Gambling has been a popular activity in the United States for centuries. It has also been suppressed by law in many areas for almost as long. By the early 20th century, gambling was almost uniformly outlawed throughout the U.S. and thus became a largely illegal activity, helping to spur the growth of the mafia and other criminal organizations. The late 20th century saw a softening in attitudes towards gambling and a relaxation of laws against it.

Many jurisdictions, local as well as national, either ban gambling or heavily control it by licensing the vendors. Such regulation generally leads to gambling tourism and illegal gambling in the areas where it is not allowed. The involvement of governments, through regulation and taxation, has led to a close connection between many governments and gaming organizations, where legal gambling provides significant government revenue, such as in Monaco and Macau, China.

There is generally legislation requiring that gaming devices be statistically random, to prevent manufacturers from making some high-payoff results impossible. Since these high payoffs have very low probability, a house bias can quite easily be missed unless the devices are checked carefully.

Most jurisdictions that allow gambling require participants to be above a certain age. In some jurisdictions, the gambling age differs depending on the type of gambling. For example, in many American states one must be over 21 to enter a casino, but may buy a lottery ticket after turning 18.

Because contracts of insurance have many features in common with wagers, insurance contracts are often distinguished in law as agreements in which either party has an interest in the "bet-upon" outcome "beyond" the specific financial terms. e.g.: a "bet" with an insurer on whether one's house will burn down is not gambling, but rather "insurance" – as the homeowner has an obvious interest in the continued existence of his/her home "independent of" the purely financial aspects of the "bet" (i.e. the insurance policy). Nonetheless, both insurance and gambling contracts are typically considered aleatory contracts under most legal systems, though they are subject to different types of regulation.

Under common law, particularly English Law (English unjust enrichment), a gambling contract may not give a casino "bona fide" purchaser status, permitting the recovery of stolen funds in some situations. In "Lipkin Gorman v Karpnale Ltd", where a solicitor used stolen funds to gamble at a casino, the House of Lords overruled the High Court's previous verdict, adjudicating that the casino return the stolen funds less those subject to any change of position defence. U.S. Law precedents are somewhat similar. For case law on recovery of gambling losses where the loser had stolen the funds see "Rights of owner of stolen money as against one who won it in gambling transaction from thief".

An interesting question is what happens when the person trying to make recovery is the gambler's spouse, and the money or property lost was either the spouse's, or was community property. This was a minor plot point in a Perry Mason novel, "The Case of the Singing Skirt", and it cites an actual case "Novo v. Hotel Del Rio".

Ancient Hindu poems like the Gambler's Lament and the "Mahabharata" testify to the popularity of gambling among ancient Indians. However, the text "Arthashastra" (c. 4th century BC) recommends taxation and control of gambling.

Ancient Jewish authorities frowned on gambling, even disqualifying professional gamblers from testifying in court.

The Catholic Church holds the position that there is no moral impediment to gambling, so long as it is fair, all bettors have a reasonable chance of winning, there is no fraud involved, and the parties involved do not have actual knowledge of the outcome of the bet (unless they have disclosed this knowledge), and as long as the following conditions are met: the gambler can afford to lose the bet, and stops when the limit is reached, and the motivation is entertainment and not personal gain leading to the "love of money" or making a living. In general, Catholic bishops have opposed casino gambling on the grounds that it too often tempts people into problem gambling or addiction, and has particularly negative effects on poor people; they sometimes also cite secondary effects such as increases in loan sharking, prostitution, corruption, and general public immorality. Some parish pastors have also opposed casinos for the additional reason that they would take customers away from church bingo and annual festivals where games such as blackjack, roulette, craps, and poker are used for fundraising. St. Thomas Aquinas wrote that gambling should be especially forbidden where the losing bettor is underage or otherwise not able to consent to the transaction. Gambling has often been seen as having social consequences, as satirized by Balzac. For these social and religious reasons, most legal jurisdictions limit gambling, as advocated by Pascal.

Gambling views among Protestants vary, with some either discouraging or forbidding their members from participation in gambling. Methodists, in accordance with the doctrine of outward holiness, oppose gambling which they believe is a sin that feeds on greed; examples are the United Methodist Church, the Free Methodist Church, the Evangelical Wesleyan Church, the Salvation Army, and the Church of the Nazarene.

Other Protestants that oppose gambling include many Mennonites, Quakers, the Christian Reformed Church in North America, the Church of the Lutheran Confession, the Southern Baptist Convention, the Assemblies of God, and the Seventh-day Adventist Church.

Other churches that oppose gambling include the Jehovah's Witnesses, The Church of Jesus Christ of Latter-day Saints, the Iglesia Ni Cristo, and the Members Church of God International.

Although different interpretations of Shari‘ah (Islamic Law) exist in the Muslim world, there is a consensus among the "‘Ulema’" (, Scholars (of Islam)) that gambling is "haraam" (, sinful or forbidden). In assertions made during its prohibition, Muslim jurists describe gambling as being both un-Qur’anic, and as being generally harmful to the Muslim Ummah (, Community). The Arabic terminology for gambling is "Maisir". 

In parts of the world that implement full Shari‘ah, such as Aceh, punishments for Muslim gamblers can range up to 12 lashes or a one-year prison term and a fine for those who provide a venue for such practises. Some Islamic nations prohibit gambling; most other countries regulate it.

While almost any game can be played for money, and any game typically played for money can also be played just for fun, some games are generally offered in a casino setting.



Gambling games that take place outside of casinos include Bingo (as played in the US and UK), dead pool, lotteries, pull-tab games and scratchcards, and Mahjong.

Other non-casino gambling games include:

<nowiki>*Although coin tossing is not usually played in a casino, it has been known to be an official gambling game in some Australian casinos</nowiki>

Fixed-odds betting and Parimutuel betting frequently occur at many types of sporting events, and political elections. In addition many bookmakers offer fixed odds on a number of non-sports related outcomes, for example the direction and extent of movement of various financial indices, the winner of television competitions such as "Big Brother", and election results. Interactive prediction markets also offer trading on these outcomes, with "shares" of results trading on an open market.

One of the most widespread forms of gambling involves betting on horse or greyhound racing. Wagering may take place through parimutuel pools, or bookmakers may take bets personally. Parimutuel wagers pay off at prices determined by support in the wagering pools, while bookmakers pay off either at the odds offered at the time of accepting the bet; or at the median odds offered by track bookmakers at the time the race started.

Betting on team sports has become an important service industry in many countries. For example, millions of people play the football pools every week in the United Kingdom. In addition to organized sports betting, both legal and illegal, there are many side-betting games played by casual groups of spectators, such as NCAA Basketball Tournament Bracket Pools, Super Bowl Squares, Fantasy Sports Leagues with monetary entry fees and winnings, and in-person spectator games like Moundball.

Based on Sports Betting, Virtual Sports are fantasy and never played sports events made by software that can be played every time without wondering about external things like weather conditions.

Arbitrage betting is a theoretically risk-free betting system in which every outcome of an event is bet upon so that a known profit will be made by the bettor upon completion of the event, regardless of the outcome. Arbitrage betting is a combination of the ancient art of arbitrage trading and gambling, which has been made possible by the large numbers of bookmakers in the marketplace, creating occasional opportunities for arbitrage.

One can also bet with another person that a statement is true or false, or that a specified event will happen (a "back bet") or will not happen (a "lay bet") within a specified time. This occurs in particular when two people have opposing but strongly held views on truth or events. Not only do the parties hope to gain from the bet, they place the bet also to demonstrate their certainty about the issue. Some means of determining the issue at stake must exist. Sometimes the amount bet remains nominal, demonstrating the outcome as one of principle rather than of financial importance.

Betting exchanges allow consumers to both back and lay at odds of their choice. Similar in some ways to a stock exchange, a bettor may want to back a horse (hoping it will win) or lay a horse (hoping it will lose, effectively acting as bookmaker).

Spread betting allows gamblers to wagering on the outcome of an event where the pay-off is based on the accuracy of the wager, rather than a simple "win or lose" outcome. For example, a wager can be based on the when a point is scored in the game in minutes and each minute away from the prediction increases or reduces the payout.

Many betting systems have been created in an attempt to "beat the house" but no system can make a mathematically unprofitable bet in terms of expected value profitable over time. Widely used systems include:

Many risk-return choices are sometimes referred to colloquially as "gambling." Whether this terminology is acceptable is a matter of debate:

Investments are also usually not considered gambling, although some investments can involve significant risk. Examples of investments include stocks, bonds and real estate. Starting a business can also be considered a form of investment. Investments are generally not considered gambling when they meet the following criteria:

Some speculative investment activities are particularly risky, but are sometimes perceived to be different from gambling:

Studies show that though many people participate in gambling as a form of recreation or even as a means to gain an income, gambling, like any behavior that involves variation in brain chemistry, can become a harmful, behavioral addiction. Behavioral addiction can occur with all the negative consequences in a person's life minus the physical issues faced by people who compulsively engage in drug and alcohol abuse. Reinforcement schedules may also make gamblers persist in gambling even after repeated losses. This is where the organized crime often ends up making large profits, allowing gamblers lines of credit and charge high percentage rates known as vigs to be paid weekly, with crime family enforcement.

The Russian writer and problem gambler Fyodor Dostoevsky portrays in his novella "The Gambler" the psychological implications of gambling and how gambling can affect gamblers. He also associates gambling and the idea of "getting rich quick", suggesting that Russians may have a particular affinity for gambling. Dostoevsky shows the effect of betting money for the chance of gaining more in 19th-century Europe. The association between Russians and gambling has fed legends of the origins of Russian roulette.
There are many symptoms and reasons for gambling. Gamblers gamble more money to try to win back money that they have lost and some gamble to relieve feelings of helplessness and anxiety.

In the United Kingdom, the Advertising Standards Authority has censured several betting firms for advertisements disguised as news articles suggesting falsely a person had cleared debts and paid for medical expenses by online gambling. The firms face possible fines.

Gamblers exhibit a number of cognitive and motivational biases that distort the perceived odds of events and that influence their preferences for gambles.





</doc>
<doc id="11924" url="https://en.wikipedia.org/wiki?curid=11924" title="Game theory">
Game theory

Game theory is the study of mathematical models of strategic interaction among rational decision-makers. It has applications in all fields of social science, as well as in logic, systems science and computer science. Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.

Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book "Theory of Games and Economic Behavior", co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.

Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. , with the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole, eleven game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.

Discussions of two-person games began long before the rise of modern, mathematical game theory. In 1713, a letter attributed to Charles Waldegraveto analyzed a game called "le her". He was an active Jacobite and uncle to James Waldegrave, a British diplomat. The true identity of the original correspondent is somewhat elusive given the limited details and evidence available and the subjective nature of its interpretation. One theory postulates Francis Waldegrave as the true correspondent, but this has yet to be proven. In this letter, Waldegrave provides a minimax mixed strategy solution to a two-person version of the card game le Her, and the problem is now known as Waldegrave problem. In his 1838 "Recherches sur les principes mathématiques de la théorie des richesses" ("Researches into the Mathematical Principles of the Theory of Wealth"), Antoine Augustin Cournot considered a duopoly and presents a solution that is the Nash equilibrium of the game.

In 1913, Ernst Zermelo published "Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels" ("On an Application of Set Theory to the Theory of the Game of Chess"), which proved that the optimal chess strategy is strictly determined. This paved the way for more general theorems.

In 1938, the Danish mathematical economist Frederik Zeuthen proved that the mathematical model had a winning strategy by using Brouwer's fixed point theorem. In his 1938 book "Applications aux Jeux de Hasard" and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix was symmetric and provides a solution to a non-trivial infinite game (known in English as Blotto game). Borel conjectured the non-existence of mixed-strategy equilibria in finite two-person zero-sum games, a conjecture that was proved false by von Neumann.

Game theory did not really exist as a unique field until John von Neumann published the paper "On the Theory of Games of Strategy" in 1928. Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by his 1944 book "Theory of Games and Economic Behavior" co-authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of money) as an independent discipline. Von Neumann's work in game theory culminated in this 1944 book. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. Subsequent work focused primarily on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.

In 1950, the first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by notable mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy. Around this same time, John Nash developed a criterion for mutual consistency of players' strategies known as the Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n-player, non-zero-sum (not just two-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium in mixed strategies.

Game theory experienced a flurry of activity in the 1950s, during which the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. The 1950s also saw the first applications of game theory to philosophy and political science.

In 1979 Robert Axelrod tried setting up computer programs as players and found that in tournaments between them the winner was often a simple "tit-for-tat" program--submitted by Anatol Rapoport--that cooperates on the first step, then, on subsequent steps, does whatever its opponent did on the previous step. The same winner was also often obtained by natural selection; a fact that is widely taken to explain cooperation phenomena in evolutionary biology and the social sciences.

In 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium. Later he would introduce trembling hand perfection as well. In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.

In the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection, and common knowledge were introduced and analyzed.

In 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.

In 2007, Leonid Hurwicz, Eric Maskin, and Roger Myerson were awarded the Nobel Prize in Economics "for having laid the foundations of mechanism design theory". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: "Game Theory, Analysis of Conflict". Hurwicz introduced and formalized the concept of incentive compatibility.

In 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics "for the theory of stable allocations and the practice of market design". In 2014, the Nobel went to game theorist Jean Tirole.

A game is "cooperative" if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is "non-cooperative" if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).

Cooperative games are often analyzed through the framework of "cooperative game theory", which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is opposed to the traditional "non-cooperative game theory" which focuses on predicting individual players' actions and payoffs and analyzing Nash equilibria.

Cooperative game theory provides a high-level approach as it describes only the structure, strategies, and payoffs of coalitions, whereas non-cooperative game theory also looks at how bargaining procedures will affect the distribution of payoffs within each coalition. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation. While it would thus be optimal to have all games expressed under a non-cooperative framework, in many instances insufficient information is available to accurately model the formal procedures available during the strategic bargaining process, or the resulting model would be too complex to offer a practical tool in the real world. In such cases, cooperative game theory provides a simplified approach that allows analysis of the game at large without having to make any assumption about bargaining powers.

A symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. That is, if the identities of the players can be changed without changing the payoff to the strategies, then a game is symmetric. Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games. Some scholars would consider certain asymmetric games as examples of these games as well. However, the most common payoffs for each of these games are symmetric.

Most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured to the right is asymmetric despite having identical strategy sets for both players.

Zero-sum games are a special case of constant-sum games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit to all players in the game, for every combination of strategies, always adds to zero (more informally, a player benefits only at the equal expense of others). Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.

Many games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.

Constant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called "the board") whose losses compensate the players' net winnings.

Simultaneous games are games where both players move simultaneously, or if they do not move simultaneously, the later players are unaware of the earlier players' actions (making them "effectively" simultaneous). Sequential games (or dynamic games) are games where later players have some knowledge about earlier actions. This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed.

The difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.

In short, the differences between sequential and simultaneous games are as follows:

An important subset of sequential games consists of games of perfect information. A game is one of perfect information if all players know the moves previously made by all other players. Most games studied in game theory are imperfect-information games. Examples of perfect-information games include tic-tac-toe, checkers, infinite chess, and Go.

Many card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken. Games of incomplete information can be reduced, however, to games of imperfect information by introducing "moves by nature".

Games in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve particular problems and answer general questions.

Games of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including "loopy" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or "economic") game theory. A typical game that has been solved this way is Hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.

Research in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha–beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.

Games, as studied by economists and real-world game players, are generally finished in finitely many moves. Pure mathematicians are not so constrained, and set theorists in particular study games that last for infinitely many moves, with the winner (or other payoff) not known until "after" all those moves are completed.

The focus of attention is usually not so much on the best way to play such a game, but whether one player has a winning strategy. (It can be proven, using the axiom of choice, that there are gameseven with perfect information and where the only outcomes are "win" or "lose"for which "neither" player has a winning strategy.) The existence of such strategies, for cleverly designed games, has important consequences in descriptive set theory.

Much of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.

Differential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.

A particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.

Evolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted. In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest.

In biology, such models can represent (biological) evolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.

Individual decision problems with stochastic outcomes are sometimes considered "one-player games". These situations are not considered game theoretical by some authors. They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).

Stochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes "chance moves" ("moves by nature"). This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.

For some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen. (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)

General models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The "gold standard" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.

These are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.

The term metagame analysis is also used to refer to a practical approach developed by Nigel Howard. whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.

These are games prevailing over all forms of society. Pooling games are repeated plays with changing payoff table in general over an experienced path, and their equilibrium strategies usually take a form of evolutionary social convention and economic convention. Pooling game theory emerges to formally recognize the interaction between optimal choice in one play and the emergence of forthcoming payoff table update path, identify the invariance existence and robustness, and predict variance over time. The theory is based upon topological transformation classification of payoff table update over time to predict variance and invariance, and is also within the jurisdiction of the computational law of reachable optimality for ordered system.

Mean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines, and by mathematician Pierre-Louis Lions and Jean-Michel Lasry.

The games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the "players" of the game, the "information" and "actions" available to each player at each decision point, and the "payoffs" for each outcome. (Eric Rasmusen refers to these four "essential elements" by the acronym "PAPI".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.

Most cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.

The extensive form can be used to formalize games with a time sequencing of moves. Games here are played on trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.

The game pictured consists of two players. The way this particular game is structured (i.e., with sequential decision making and perfect information), "Player 1" "moves" first by choosing either or (fair or unfair). Next in the sequence, "Player 2", who has now seen "Player 1"s move, chooses to play either or . Once "Player 2" has made their choice, the game is considered finished and each player gets their respective payoff. Suppose that "Player 1" chooses and then "Player 2" chooses : "Player 1" then gets a payoff of "eight" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and "Player 2" gets a payoff of "two".

The extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)

The normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays "Up" and that Player 2 plays "Left". Then Player 1 gets a payoff of 4, and Player 2 gets 3.

When a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.

Every extensive-form game has an equivalent normal-form game, however the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.

In games that possess removable utility, separate rewards are not given; rather, the characteristic function decides the payoff of each unity. The idea is that the unity that is 'empty', so to speak, does not receive a reward at all.

The origin of this form is to be found in John von Neumann and Oskar Morgenstern's book; when looking at these instances, they guessed that when a union formula_1 appears, it works against the fraction
formula_2
as if two individuals were playing a normal game. The balanced payoff of C is a basic function. Although there are differing examples that help determine coalitional amounts from normal games, not all appear that in their function form can be derived from such.

Formally, a characteristic function is seen as: (N,v), where N represents the group of people and formula_3 is a normal utility.

Such characteristic functions have expanded to describe games where there is no removable utility.

Alternative game representation forms exist and are used for some subclasses of games or adjusted to the needs of interdisciplinary research. In addition to classical game representions, some of the alternative representations also encode time related aspects.

As a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.

Although pre-twentieth century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name "game theory", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 book "Evolution and the Theory of Games".

In addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic arguments of this type can be found as far back as Plato. An alternative version of game theory, called chemical game theory, represents the player's choices as metaphorical chemical reactant molecules called “knowlecules”.  Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.

The primary use of game theory is to describe and model how human populations behave. Some scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice human behavior often deviates from this model. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.

Some game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).

Some scholars see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.

Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers and acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.

This research usually focuses on particular sets of strategies known as "solution concepts" or "equilibria". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.

The payoffs of the game are generally taken to represent the utility of individual players.

A prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Naturally one might wonder to what use this information should be put. Economists and business professors suggest two primary uses (noted above): "descriptive" and "prescriptive".

Sensible decision-making is critical for the success of projects. In project management, game theory is used to model the decision-making process of players, such as investors, project managers, contractors, sub-contractors, governments and customers. Quite often, these players have competing interests, and sometimes their interests are directly detrimental to other players, making project management scenarios well-suited to be modeled by game theory.

Piraveenan (2019) in his review provides several examples where game theory is used to model project management scenarios. For instance, an investor typically has several investment options, and each option will likely result in a different project, and thus one of the investment options has to be chosen before the project charter can be produced. Similarly, any large project involving subcontractors, for instance, a construction project, has a complex interplay between the main contractor (the project manager) and subcontractors, or among the subcontractors themselves, which typically has several decision points. For example, if there is an ambiguity in the contract between the contractor and subcontractor, each must decide how hard to push their case without jeopardizing the whole project, and thus their own stake in it. Similarly, when projects from competing organizations are launched, the marketing personnel have to decide what is the best timing and strategy to market the project, or its resultant product or service, so that it can gain maximum traction in the face of competition. In each of these scenarios, the required decisions depend on the decisions of other players who, in some way, have competing interests to the interests of the decision-maker, and thus can ideally be modeled using game theory.

Piraveenan summarises that two-player games are predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.


In terms of types of games, both cooperative as well as non-cooperative games, normal-form as well as extensive-form games, and zero-sum as well as non-zero-sum games are used to model various project management scenarios.

The application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.

Early examples of game theory applied to political science are provided by Anthony Downs. In his 1957 book "An Economic Theory of Democracy", he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game Theory was applied in 1962 to the Cuban Missile Crisis during the presidency of John F. Kennedy.

It has also been proposed that game theory explains the stability of any form of political government. Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects. Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed. Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime. Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.

A game-theoretic explanation for democratic peace is that public and open debate in democracies sends clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.

On the other hand, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.

Game theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example would be Peter John Wood's (2013) research when he looked into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce greenhouse gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma to the nations.

Unlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best-known equilibrium in biology is known as the "evolutionarily stable strategy" (ESS), first introduced in . Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.

In biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.

Additionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication. The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's "Butterfly Economics").

Biologists have used the game of chicken to analyze fighting behavior and territoriality.

According to Maynard Smith, in the preface to "Evolution and the Theory of Games", "paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.

One such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival. All of these actions increase the overall fitness of a group, but occur at a cost to the individual.

Evolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation , where the cost to the altruist must be less than the benefit to the recipient multiplied by the coefficient of relatedness . The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of , because (on average) an individual shares half of the alleles in its sibling's offspring. Ensuring that enough of a sibling's offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring. The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a coefficient that was in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.

Game theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.

Separately, game theory has played a role in online algorithms; in particular, the -server problem, which has in the past been referred to as "games with moving costs" and "request-answer games". Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.

The emergence of the Internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory and within it algorithmic mechanism design combine computational algorithm design and analysis of complex systems with economic theory.

Game theory has been put to several uses in philosophy. Responding to two papers by , used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis. Following game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.

Game theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993), Skyrms (1990), and Stalnaker (1999).

In ethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton) authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see and ).

Other authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., and ).

Game theory applications are used heavily in the pricing strategies of retail and consumer markets, particularly for the sale of inelastic goods. With retailers constantly competing against one another for consumer market share, it has become a fairly common practice for retailers to discount certain goods, intermittently, in the hopes of increasing foot-traffic in brick and mortar locations (websites visits for e-commerce retailers) or increasing sales of ancillary or complimentary products.

Black Friday, a popular shopping holiday in the US, is when many retailers focus on optimal pricing strategies to capture the holiday shopping market. In the Black Friday scenario, retailers using game theory applications typically ask “what is the dominant competitor’s reaction to me?" In such a scenario, the game has two players: the retailer, and the consumer. The retailer is focused on an optimal pricing strategy, while the consumer is focused on the best deal. In this closed system, there often is no dominant strategy as both players have alternative options. That is, retailers can find a different customer, and consumers can shop at a different retailer. Given the market competition that day, however, the dominant strategy for retailers lies in outperforming competitors. The open system assumes multiple retailers selling similar goods, and a finite number of consumers demanding the goods at an optimal price. A blog by a Cornell University professor provided an example of such a strategy, when Amazon priced a Samsung TV $100 below retail value, effectively undercutting competitors. Amazon made up part of the difference by increasing the price of HDMI cables, as it has been found that consumers are less price discriminatory when it comes to the sale of secondary items.

Retail markets continue to evolve strategies and applications of game theory when it comes to pricing consumer goods. The key insights found between simulations in a controlled environment and real-world retail experiences show that the applications of such strategies are more complex, as each retailer has to find an optimal balance between pricing, supplier relations, brand image, and the potential to cannibalize the sale of more profitable items.


Lists






</doc>
<doc id="11929" url="https://en.wikipedia.org/wiki?curid=11929" title="Demographics of Germany">
Demographics of Germany

The demography of Germany is monitored by the "Statistisches Bundesamt" (Federal Statistical Office of Germany). According to the first census since reunification, Germany's population was 80,219,695 (9 May 2011), making it the sixteenth-most populous country in the world and the most populous in the European Union. The total fertility rate was rated at 1.57 in 2018. In 2008, fertility was related to educational achievement (women with lower levels of education were having more children than women who had completed higher education). In 2011, this was no longer true for Eastern Germany, where more highly educated women now had a somewhat higher fertility rate than the rest of the population. Persons who said they had no religion tend to have fewer children than those who identify as Christians, and studies also found that conservative-leaning Christians had more children compared to liberal-leaning Christians. 

The United Nations Population Fund lists Germany as host to the third-highest number of international migrants worldwide, behind the United States and Saudi Arabia. More than 16 million people are descended from immigrants (first and second generation, including mixed heritage and ethnic German repatriates and their descendants). 96.1% of those reside in western Germany and Berlin. About 7,000,000 of these 16,000,000 are foreign residents, defined as those without German citizenship. The largest ethnic group of non-German origin are the Turkish. Since the 1960s, West and later reunified Germany has attracted immigrants primarily from Southern and Eastern Europe as well as Turkey, many of whom (or their children) have acquired German citizenship over time. While most of these immigrants initially arrived as guest workers, Germany has also been a prime destination for refugees who have applied for asylum in Germany, in part because the German constitution has long had a clause guaranteeing political asylum as a human right; but restrictions over the years have since limited the scope of this guarantee.

Germany has one of the world's highest levels of education, technological development, and economic productivity. Since the end of World War II, the number of students entering university has more than tripled, and the trade and technical schools are among the world's best. With a per capita income of about €40,883 in 2018, Germany is a broadly middle-class society. However, there has been a strong increase in the number of children living in poverty. In 1965, one in 75 children was on the welfare rolls; but by 2007 this had increased to one child in six. These children live in relative poverty, but not necessarily in absolute poverty. Germans are typically well-travelled, with millions travelling overseas each year. The social welfare system provides for universal health care, unemployment compensation, child benefits and other social programmes. Germany's ageing population and struggling economy strained the welfare system in the 1990s, so the government adopted a wide-ranging programme of belt-tightening reforms, Agenda 2010, including the labour-market reforms known as Hartz concept.

The contemporary demographics of Germany are also measured by a series of full censuses, with the most recent held in 1987. Since reunification, German authorities rely on a "micro census".

The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire period. Sources: Our World In Data and Gapminder Foundation.

Sources: Our World In Data and the United Nations.

1875-1950

1950-2015

Source: "UN World Population Prospects"

Population statistics since 1900. Territorial changes of Germany occurred in 1918/1919, 1921/1922, 1945/1946 and in 1990.

In 2019, 588,401 (75.6%) children were born to mothers with German citizenship, while 189,689 (24.4%) children were born to mothers with foreign citizenship.


After the World War II border shifts and expulsions, the Germans from Central and Eastern Europe and the former eastern territories moved westward to post-war Germany. During the partition of Germany, many Germans from East Germany fled to West Germany for political and economic reasons. Since Germany's reunification, there are ongoing migrations from the eastern "New Länder" to the western "Old Länder" for economic reasons.

The Federal Republic of Germany and the German Democratic Republic followed different paths when it came to demographics. The politics of the German Democratic Republic was pronatalistic while that of the Federal Republic was compensatory.

Fertility in the GDR was higher than that in the FRG. Demographic politics was only one of the reasons. Women in the GDR had fewer "biographic options", young motherhood was expected of them. State funded costfree childcare was available to all mothers.

Note: Berlin is included into East Germany for the year 2002 and 2008. Source: Kreyenfeld (2002); Kreyenfeld et al. (2010); HFD Germany (2010)

About 1.7 million people have left the new federal states (the East) since the fall of the Berlin Wall, or 12% of the population; a disproportionately high number of them were women under 35.

After 1990, the total fertility rate (TFR) in the East dropped to 0.772 in 1994. This has been attributed to a "demographic shock": people not only had fewer children, they were also less likely to marry or divorce after the end of the GDR; the biographic options of the citizens of the former GDR had increased. Young motherhood seemed to be less attractive and the age of the first birth rose sharply.

In the following years, the TFR in the East started to rise again, surpassing 1.0 in 1997 and 1.3 in 2004, and reaching the West's TFR (1.37) in 2007. In 2010, the East's fertility rate (1.459) clearly exceeded that of the West (1.385), while Germany's overall TFR had risen to 1.393, the highest value since 1990, which was still far below the natural replacement rate of 2.1 and the birth rates seen under communism. In 2016, the TFR was 1.64 in the East and 1.60 in the West.

Between 1989 and 2009, about 2,000 schools closed because there were fewer children.

In some regions the number of women between the ages of 20 and 30 has dropped by more than 30%. In 2004, in the age group 18-29 (statistically important for starting families) there were only 90 women for every 100 men in the new federal states (the East, including Berlin).

Until 2007 family politics in the federal republic was compensatory, which means that poor families received more family benefits (such as the "Erziehungsgeld") than rich ones. In 2007 the so-called "Elterngeld" was introduced. According to Christoph Butterwegge the Elterngeld was meant to "motivate highly educated women to have more children"; the poor on the other hand were disadvantaged by the "Elterngeld", and now received lower child benefits than the middle classes. The very well-off (who earn more than 250.000 Euro per annum) and those on welfare receive no Elterngeld payments.

In 2013 the following most recent developments were noticed:


In the new federal states the fertility rate of college-educated women is now higher than that of those without college degrees. Differences in value priorities and the better availability of childcare in the eastern states are discussed as possible reasons.

Muslims are younger and have more children than non-Muslims in Germany, although their fertility rate is still below replacement level.

In 2019, the non-profit Austrian Institute of Economic Research and the Bertelsmann Stiftung published a study about the economic impact of demographics. The researchers assume a reduction in the per capita income of €3,700 until 2040.

Demographic statistics according to the World Population Review.


Demographic statistics according to the CIA World Factbook, unless otherwise indicated.

















While most childbirths in Germany happen within marriage, a growing number of children are born out-of-wedlock. In 2010 the out-of-wedlock-rate was 33%, more than twice of what it was in 1990.

The Mikrozensus done in 2008 revealed that the number of children a German woman aged 40 to 75 had, was closely linked to her educational achievement.
In Western Germany the most educated women were the most likely to be childless. 26% of those groups stated they were childless, while 16% of those having an intermediate education, and 11% of those having compulsory education, stated the same.
In Eastern Germany however, 9% of the most educated women of that age group and 7% of those who had an intermediary education were childless, while 12% of those having only compulsory education were childless.

The reason for that east-western difference is that the GDR had an "educated mother scheme" and actively tried to encourage first births among the more educated. It did so by propagandizing the opinion that every educated woman should "present at least one child to socialism" and also by financially rewarding its more educated citizen to become parents. The government especially tried to persuade students to become parents while still in college and it was quite successful in doing so. In 1986 38% of all women, who were about to graduate from college, were mothers of at least one child and additional 14% were pregnant and 43% of all men, who were about to graduate from college, were fathers of at least one child. There was a sharp decline in the birth rate and especially in the birth rate of the educated after the fall of the Berlin wall. Nowadays, 5% of those about to graduate from college are parents.

The more educated a Western German mother aged 40 to 75 was in 2008, the less likely she was to have a big family.

The same was true for a mother living in Eastern Germany in 2008.

A study done in 2005 in the western German state of Nordrhein-Westfalen by the HDZ revealed that childlessness was especially widespread among scientists. It showed that 78% of the women scientists and 71% of the male scientists working in that state were childless.

The Federal Statistical Office defines persons with a migrant background as all persons who migrated to the present area of the Federal Republic of Germany after 1949, plus all foreign nationals born in Germany and all persons born in Germany as German nationals with at least one parent who migrated to Germany or was born in Germany as a foreign national. The figures presented here are based on this definition only.

In 2010, 2.3 million families with children under 18 years were living in Germany, in which at least one parent had foreign roots. They represented 29% of the total of 8.1 million families with minor children. Compared with 2005 – the year when the microcensus started to collect detailed information on the population with a migrant background – the proportion of migrant families has risen by 2 percentage points. In 2015, 36% children under 5 years old had migrant background.(number includes ethnic German repatriates)

Most of the families with a migrant background live in the western part of Germany. In 2010, the proportion of migrant families in all families was 32% in the former territory of the Federal Republic. This figure was more than double that in the new Länder (incl. Berlin) where it stood at 15%.

Families with a migrant background more often have three or more minor children in the household than families without a migrant background. In 2010, about 15% of the families with a migrant background contained three or more minor children, as compared with just 9% of the families without a migrant background.

In 2009, 3.0 million of the persons of immigrant background had Turkish roots, 2.9 million had their roots in the successor states of the Soviet Union (including a large number of Russian-speaking ethnic Germans), 1.5 million had their roots in the successor states of Yugoslavia including 200.000 Albanians and 1.5 million had Polish roots.

In 2008, 18.4% of Germans of any age group and 30% of German children had at least one parent born abroad. Median age for Germans with at least one parent born abroad was 33.8 years, while that for Germans, who had two parents born in Germany was 44.6 years.

Germany is home to the third-highest number of international migrants worldwide after the United States and Saudi Arabia.

, the population by background was as follows:

Four other sizable groups of people are referred to as "national minorities" ("nationale Minderheiten") because they have lived in their respective regions for centuries: Danes, Frisians, Roma and Sinti, and Sorbs. There is a Danish minority (about 50,000, according to government sources) in the northern-most state of Schleswig-Holstein. Eastern and Northern Frisians live at Schleswig-Holstein's western coast, and in the north-western part of Lower Saxony. They are part of a wider community (Frisia) stretching from Germany to the northern Netherlands. The Sorbs, a Slavic people with about 60,000 members (according to government sources), are in the Lusatia region of Saxony and Brandenburg. They are the last remnants of the Slavs that lived in central and eastern Germany since the 7th century to have kept their traditions and not been completely integrated into the wider German nation.

Until World War II the Poles were recognized as one of the national minorities. In 1924 the Union of Poles in Germany had initiated cooperation between all national minorities in Germany under the umbrella organization Association of National Minorities in Germany. Some of the union members wanted the Polish communities in easternmost Germany (now Poland) to join the newly established Polish nation after World War I. Even before the German invasion of Poland, leading anti-Nazi members of the Polish minority were deported to concentration camps; some were executed at the Piaśnica murder site. Minority rights for Poles in Germany were revoked by Hermann Göring's World War II decree of 27 February 1940, and their property was confiscated.

After the war ended, the German government did not re-implement national minority rights for ethnic Poles. The reason for this is that the areas of Germany which formerly had a native Polish minority were annexed to Poland and the Soviet Union, while almost all of the native German populations (formerly the ethnic majority) in these areas subsequently fled or were expelled by force. With the mixed German-Polish territories now lost, the German government subsequently regarded ethnic Poles residing in what remained of Germany as immigrants, just like any other ethnic population with a recent history of arrival. In contrast, Germans living in Poland are recognized as national minority and have granted seats in Polish Parliament. It must be said, however, that an overwhelming number of Germans in Poland have centuries-old historical ties to the lands they now inhabit, whether from living in territory that once belonged to the German state, or from centuries-old communities. In contrast, most Poles in present-day Germany are recent immigrants, though there are some communities which have been present since the 19th and perhaps even the 18th centuries. Despite protests by some in the older Polish-German communities, and despite Germany being now a signatory to the Framework Convention for the Protection of National Minorities, Germany has so far refused to re-implement minority rights for ethnic Poles, based on the fact that almost all areas of historically mixed German-Polish heritage (where the minority rights formerly existed) are no longer part of Germany and because the vast majority of ethnic Poles now residing in Germany are recent immigrants.

Roma people have been in Germany since the Middle Ages. They were persecuted by the Nazis, and thousands of Roma living in Germany were killed by the Nazi regime. Nowadays, they are spread all over Germany, mostly living in major cities. It is difficult to estimate their exact number, as the German government counts them as "persons without migrant background" in their statistics. There are also many assimilated Sinti and Roma. A vague figure given by the German Department of the Interior is about 70,000. In contrast to the old-established Roma population, the majority of them do not have German citizenship, they are classified as immigrants or refugees.

After World War II, 14 million ethnic Germans were expelled from the eastern territories of Germany and homelands outside the former German Empire. The accommodation and integration of these "Heimatvertriebene" in the remaining part of Germany, in which many cities and millions of apartments had been destroyed, was a major effort in the post-war occupation zones and later states of Germany.

Since the 1960s, ethnic Germans from the People's Republic of Poland and Soviet Union (especially from Kazakhstan, Russia, and Ukraine), have come to Germany. During the time of Perestroika, and after the dissolution of the Soviet Union, the number of immigrants increased heavily. Some of these immigrants are of mixed ancestry. During the 10-year period between 1987 and 2001, a total of 1,981,732 ethnic Germans from the FSU immigrated to Germany, along with more than a million of their non-German relatives. After 1997, however ethnic Slavs or those belonging to Slavic-Germanic mixed origins outnumbered these with only Germanic descent amongst the immigrants. The total number of people currently living in Germany having FSU connection is around 4 to 4.5 million (Including Germans, Slavs, Jews and those of mixed origins), out of that more than 50% is of German descent.

Germany now has Europe's third-largest Jewish population. In 2004, twice as many Jews from former Soviet republics settled in Germany as in Israel, bringing the total inflow to more than 100,000 since 1991. Jews have a voice in German public life through the Central Council of Jews in Germany (Zentralrat der Juden in Deutschland). Some Jews from the former Soviet Union are of mixed heritage.

In 2000 there were also around 300,000–500,000 Afro-Germans (those who have German citizenship) and 150,000+ African nationals. Most of them live in Berlin and Hamburg. Numerous persons from Tunisia and Morocco live in Germany. While they are considered members of a minority group, for the most part, they do not considers themselves "Afro-Germans," nor are most of them perceived as such by the German people. However, Germany does not keep any statistics regarding ethnicity or race. Hence, the exact number of Germans of African descent is unknown.

Germany's biggest East Asian minority are the Vietnamese people in Germany. About 40,000 Vietnamese live in Berlin and surroundings. Also there are about 20,000 to 25,000 Japanese people residing in Germany. Some South Asian and Southeast Asian immigration has taken place. Nearly 50,000 Indians live in Germany. As of 2008, there were 68,000 Filipino residents and an unknown number of Indonesians residing in Germany.

Numerous descendants of the so-called "Gastarbeiter" live in Germany. The "Gastarbeiter" mostly came from Chile, Greece, Italy, Morocco, Portugal, Spain, Tunisia, Turkey and the former Yugoslavia.
Also included were Vietnam, Mongolia, North Korea, Angola, Mozambique and Cuba when the former East Germany existed until reunification in 1990. The (socialist) German Democratic Republic (East Germany) however had their guest-workers stay in single-sex dormitories. Female guest workers had to sign contracts saying that they were not allowed to fall pregnant during their stay. If they fell pregnant nevertheless they faced forced abortion or deportation. This is one of the reasons why the vast majority of ethnic minorities today lives in western Germany and also one of the reasons why minorities such as the Vietnamese have the most unusual population pyramid, with nearly all second-generation Vietnamese Germans born after 1989.

, the numbers of selected groups of resident foreign nationals (non-naturalized residents) in Germany were as follows:

This list does not include foreigners with German nationality and foreign nationals without resident status.
The most common Y chromosome haplogroups among German males are Haplogroup R1b, followed by Haplogroup I1, and Haplogroup R1a.

With an estimated more than 81.8 million inhabitants in late 2011, Germany is the most populous country in the European Union and ranks as the 16th largest country in the world in terms of population. Its population density stands at 229.4 inhabitants per square kilometer.

Germany comprises sixteen states that are collectively referred to as "Länder". Due to differences in size and population the subdivision of these states varies, especially between city-states ("Stadtstaaten") and states with larger territories ("Flächenländer"). For regional administrative purposes five states, namely Baden-Württemberg, Bavaria, Hesse, North Rhine-Westphalia and Saxony, consist of a total of 22 Government Districts ("Regierungsbezirke"). As of 2009 Germany is divided into 403 districts ("Kreise") on municipal level, these consist of 301 rural districts and 102 urban districts.

Germany officially has eleven metropolitan regions. In 2005, Germany had 82 cities with more than 100,000 inhabitants.

Germany had signed special visa agreements with several countries in times of severe labour shortages or when particular skills were deficient within the country. During the 1960s and 1970s, agreements were signed with the governments of Turkey, Yugoslavia, Italy and Spain to help Germany overcome its severe labour shortage.

As of 2012, the largest sources of net immigration to Germany are other European countries, most importantly Poland, Romania, Bulgaria, Hungary, Italy, Spain, and Greece; notably, in the case of Turkey, German Turks moving to Turkey slightly outnumber new immigrants.

In 2015, there were 476,649 asylum applications.

Responsibility for educational oversight in Germany lies primarily with the individual federated states. Since the 1960s, a reform movement has attempted to unify secondary education into a "Gesamtschule" (comprehensive school); several West German states later simplified their school systems to two or three tiers. A system of apprenticeship called "Duale Ausbildung" ("dual education") allows pupils in vocational training to learn in a company as well as in a state-run vocational school.

Optional kindergarten education is provided for all children between three and six years old, after which school attendance is compulsory for at least nine years. Primary education usually lasts for four years and public schools are not stratified at this stage. In contrast, secondary education includes three traditional types of schools focused on different levels of academic ability: the "Gymnasium" enrols the most academically promising children and prepares students for university studies; the "Realschule" for intermediate students lasts six years; the "Hauptschule" prepares pupils for vocational education.

In addition Germany has a comprehensive school known as the "Gesamtschule". While some German schools such as the Gymnasium and the Realschule have rather strict entrance requirements, the Gesamtschule does not have such requirements. They offer college preparatory classes for the students who are doing well, general education classes for average students, and remedial courses for those who aren't doing that well. In most cases students attending a Gesamtschule may graduate with the Hauptschulabschluss, the Realschulabschluss or the Abitur depending on how well they did in school.
The percentage of students attending a Gesamtschule varies by Bundesland. In 2007 the State of Brandenburg more than 50% of all students attended a Gesamtschule, while in the State of Bavaria less than 1% did.

The general entrance requirement for university is Abitur, a qualification normally based on continuous assessment during the last few years at school and final examinations; however there are a number of exceptions, and precise requirements vary, depending on the state, the university and the subject. Germany's universities are recognised internationally; in the Academic Ranking of World Universities (ARWU) for 2008, six of the top 100 universities in the world are in Germany, and 18 of the top 200. Nearly all German universities are public institutions, tuition fees in the range of €500 were introduced in some states after 2006, but quickly abolished again until 2014.

"Percentage of jobholders holding Hauptschulabschluss, Realschulabschluss or Abitur in Germany"

Over 99% of those of age 15 and above are estimated to be able to read and write. However, a growing number of inhabitants are functionally illiterate. The young are much more likely to be functionally illiterate than the old. According to a study done by the University of Bremen in cooperation with the "Bundesverband Alphabetisierung e.V.", 10% of youngsters living in Germany are functionally illiterate and one quarter are able to understand only basic level texts. Illiteracy rates of youngsters vary by ethnic group and parents' socioeconomic class.

The life expectancy in Germany is 81.1 years (78.7 years males, 83.6 years females, 2020 est.). 
, the principal cause of death was cardiovascular disease, at 42%, followed by malignant tumours, at 25%.
, about 82,000 Germans had been infected with HIV/AIDS and 26,000 had died from the disease (cumulatively, since 1982).
According to a 2005 survey, 27% of German adults are smokers.
A 2009 study shows Germany is near the median in terms of overweight and obese people in Europe.

The national constitutions of 1919 and 1949 guarantee freedom of faith and religion; earlier, these freedoms were mentioned only in state constitutions. The modern constitution of 1949 also states that no one may be discriminated against due to their faith or religious opinions. A state church does not exist in Germany (see Freedom of religion in Germany).
According to a 1990s poll by "Der Spiegel", 45% of Germans believe in God, and a quarter in Jesus Christ. According to the Eurobarometer Poll 2010, 44% of German citizens responded that "they believe there is a God", 25% responded that "they believe there is some sort of spirit or life force" and 27% responded that "they don't believe there is any sort of spirit, God or life force". 4% gave no response.

Christianity is the largest religion in Germany, comprising an estimated 55.1% of the country's population.

Smaller religious groups (less than 1%) include Judaism, Buddhism and Hinduism.

The two largest churches, the Roman Catholic Church and the Protestant Evangelical Church in Germany (EKD), have lost significant number of adherents. In 2019 the Catholic Church accounted for 27.2% and the Evangelical Church for 24.9% of the population. Orthodox Church has 1.9% and other Christian churches and groups summed up to 1.1% of the population. Since the reunification of Germany, the number of non-religious people has grown and an estimated 38.8% of the country's population are not affiliated with any church or religion.

The other religions make up to less than 1% of the population. Buddhism has around 200,000 adherents (0.2%), Judaism has around 200.000 adherents (0.2%), Hinduism 90,000 (0.1%), Sikhism 75,000 (0.1%) and Yazidis religion (45,000-60,000). All other religious communities in Germany have fewer than 50,000 (<0.1%) adherents.

Protestantism is concentrated in the north and east and Roman Catholicism is concentrated in the south and west. According to the last nationwide census, Protestantism is more widespread among the population with German citizenship; there are slightly more Catholics total because of the Catholic immigrant population (including such groups as Poles and Italians). The former Pope, Benedict XVI, was born in Bavaria. Non-religious people, including atheists and agnostics, might make up as many as 55% of the total population, and are especially numerous in the former East Germany and major metropolitan areas.

Of the roughly 4 million Muslims, most are Sunnis and Alevites from Turkey, but there are a small number of Shi'ites and other denominations. 1.9% of the country's overall population declare themselves Orthodox Christians, with Serbs, Greeks, Montenegrins, Ukrainians and Russians being the most numerous. Germany has Europe's third-largest Jewish population (after France and the United Kingdom). In 2004, twice as many Jews from former Soviet republics settled in Germany as in Israel, bringing the total Jewish population to more than 200,000, compared to 30,000 prior to German reunification. Large cities with significant Jewish populations include Berlin, Frankfurt and Munich. Around 250,000 active Buddhists live in Germany; 50% of them are Asian immigrants.

Census results were as follows:


German is the only official and most widely spoken language. Standard German is understood throughout the country.

Danish, Low German, Low Rhenish, the Sorbian languages (Lower Sorbian and Upper Sorbian), and the two Frisian languages, Saterfrisian and North Frisian, are officially recognized and protected as minority languages by the European Charter for Regional or Minority Languages in their respective regions. With speakers of Romany living in all parts of Germany, the federal government has promised to take action to protect the language. Until now, only Hesse has followed Berlin's announcement, and agreed on implementing concrete measures to support Romany speakers.

Implementation of the Charter is poor. The monitoring reports on charter implementation in Germany show many provisions unfulfilled. 

German dialects – some quite distinct from the standard language – are used in everyday speech, especially in rural regions. Many dialects, for example the Upper German varieties, are to some degree cultivated as symbols of regional identity and have their own literature, theaters and some TV programming. While speaking a dialect outside its native region might be frowned upon, in their native regions some dialects can be spoken by all social classes. . Nevertheless, partly due to the prevalence of Standard German in media, the use of dialects has declined over the past century, especially in the younger population.

The social status of different German dialects can vary greatly. The Alemannic and Bavarian dialects of the south are positively valued by their speakers and can be used in almost all social circumstances. The Saxonian and Thuringian dialects have less prestige and are subject to derision. While Bavarian and Alemannic have kept much of their distinctiveness, the Middle German dialects, which are closer to Standard German, have lost some of their distinctive lexical and grammatical features and tend to be only pronunciation variants of Standard German.

Low Saxon is officially recognized as a language on its own, but despite this fact, there's little official action taken on fostering the language. Historically one third of Germany's territory and population was Low Saxon speaking. No data was ever collected on the actual number of speakers, but today the number of speakers ranges around 5 million persons. Despite this relatively high number of speakers there is very little coverage in the media (mostly on NDR TV, no regular programming) and very little education in or on the language. The language is not fixed as part of the school curriculum and Low Saxon is used as a medium of instruction in one school only in the whole Germany (as a "model project" in primary school sided by education in Standard German). As a consequence the younger generation refused to adopt the native language of their parents. Language prevalence dropped from more than 90% (depending on the exact region) in the 1930s to less than 5% today. This accounts for a massive intergenerational gap in language use. Older people regularly use the language and take private initiative to maintain the language, but the lack of innovative potential of the younger generation hinders language maintenance. The language too has an own literature (around 150 published books every year) and there are many theatres (mostly lay stages, but some professional ones, like for example Ohnsorg-Theater).

Use of Low Saxon is mainly restricted to use among acquaintances, like family members, neighbours and friends. A meeting of a village council can be held almost completely in Low Saxon if all participants know each other (as long as written protocols are written in Standard German), but a single foreigner can make the whole switching to Standard German.

The Low Saxon dialects are different in their status too. There's a north-south gradient in language maintenance. The Southern dialects of Westfalian, Eastfalian and Brandenburgish have had much stronger speaker losses, than the northern coastal dialects of Northern Low Saxon. While Eastfalian has lost speakers to Standard German, Westfalian has lost speakers to Standard German and Standard German based regiolect of the Rhine-Ruhr area. Brandenburgish speakers mostly switched to the Standard German-based regiolect of Berlin. Brandenburgish is almost completely replaced by the Berlin regiolect. Northern Low Saxon speakers switched mostly to pure Standard German.

English is the most common foreign language and almost universally taught by the secondary level; it is also taught at elementary level in some states. Other commonly-taught languages are French, Italian, Spanish, Portuguese, and Russian. Dutch is taught in states bordering the Netherlands, and Polish in the eastern states bordering Poland. Latin and Ancient Greek are part of the classical education syllabus offered in many secondary schools.

According to a 2004 survey, two-thirds of Germany's citizens have at least basic knowledge of English. About 20% consider themselves to be competent speakers of French, followed by speakers of Russian (7%), Italian (6.1%), and Spanish (5.6%). The relatively high number of Russian speakers is a result of immigration from the former Soviet Union to Germany for almost 10 consecutive years, plus its having been learned in school by many older former East Germans.




</doc>
<doc id="11930" url="https://en.wikipedia.org/wiki?curid=11930" title="Economy of Germany">
Economy of Germany

The economy of Germany is a highly developed social market economy. It has the largest national economy in Europe, the fourth-largest by nominal GDP in the world, and fifth by GDP (PPP). In 2017, the country accounted for 28% of the euro area economy according to the IMF. Germany is a founding member of the European Union and the Eurozone. 

In 2016, Germany recorded the highest trade surplus in the world worth $310 billion, making it the biggest capital exporter globally. Germany is one of the largest exporters globally with $1448.17 billion worth of goods and services exported in 2017. The service sector contributes around 70% of the total GDP, industry 29.1%, and agriculture 0.9%. Exports account for 41% of national output. The top 10 exports of Germany are vehicles, machinery, chemical goods, electronic products, electrical equipment, pharmaceuticals, transport equipment, basic metals, food products, and rubber and plastics. The economy of Germany is the largest manufacturing economy in Europe and it is less likely to be affected by the financial downturn and conduct applied research with practical industrial value and sees itself as a bridge between the latest university insights and industry-specific product and process improvements, and by generating a great deal of knowledge in its own laboratories as well. In July 2017, the International Monetary Fund gave the country's economy "yet another bill of good health" and some advice on steps it might take to maintain this level in the long run.

Germany is rich in timber, lignite, potash and salt. Some minor sources of natural gas are being exploited in the state of Lower Saxony. Until reunification, the German Democratic Republic mined for uranium in the Ore Mountains (see also: SAG/SDAG Wismut). Energy in Germany is sourced predominantly by fossil fuels (30%), followed by wind second, then nuclear power, gas, solar, biomass (wood and biofuels) and hydro. Germany is the first major industrialized nation to commit to the renewable energy transition called Energiewende. Germany is the leading producer of wind turbines in the world. Renewables produced 46% of electricity consumed in Germany (as of 2019).
99 percent of all German companies belong to the German "Mittelstand," small and medium-sized enterprises, which are mostly family-owned. Of the world's 2000 largest publicly listed companies measured by revenue, the Fortune Global 2000, 53 are headquartered in Germany, with the Top 10 being Allianz, Daimler, Volkswagen, Siemens, BMW, Deutsche Telekom, Bayer, BASF, Munich Re and SAP.

Germany is the world's top location for trade fairs. Around two thirds of the world's leading trade fairs take place in Germany. The largest annual international trade fairs and congresses are held in several German cities such as Hanover, Frankfurt, Cologne, Leipzig and Düsseldorf.

The Industrial Revolution in Germany got underway approximately a century later than in the United Kingdom, France, and Belgium, partly because Germany only became a unified country in 1871.The establishment of the Deutscher Zollverein (German Customs Union) in 1834 and the expansion of railway systems were the main drivers of Germany's industrial development and political union. From 1834, tariff barriers between increasing numbers of the Kleindeutschland German states were eliminated. In 1835 the first German railway linked the Franconian cities of Nuremberg and Fürth – it proved so successful that the decade of the 1840s saw "railway mania" in all the German states. Between 1845 and 1870, of rail had been built and in 1850 Germany was building its own locomotives. Over time, other German states joined the customs union and started linking their railroads, which began to connect the corners of Germany together. The growth of free trade and of a rail system across Germany intensified economic development which opened up new markets for local products, created a pool of middle managers, increased the demand for engineers, architects and skilled machinists, and stimulated investments in coal and iron.

Another factor which propelled German industry forward was the unification of the monetary system, made possible in part by political unification. The Deutsche Mark, a new monetary coinage system backed by gold, was introduced in 1871. However, this system did not fully come into use as silver coins retained their value until 1907.

The victory of Prussia and her allies over Napoleon III of France in the Franco-Prussian War of 1870-1871 marked the end of French hegemony in Europe and resulted in the proclamation of the German Empire in 1871. The establishment of the empire inherently presented Europe with the reality of a new populous and industrializing polity possessing a considerable, and undeniably increasing, economic and diplomatic presence. The influence of French economic principles produced important institutional reforms in Germany, including the abolition of feudal restrictions on the sale of large landed estates, the reduction of the power of the guilds in the cities, and the introduction of a new, more efficient commercial law. Nonetheless, political decisions about the economy of the empire were still largely controlled by a coalition of "rye and iron", that is the Prussian Junker landowners of the east and the Ruhr heavy industry of the west.

Regarding politics and society, between 1881 and 1889 Chancellor Otto von Bismarck promoted laws that provided social insurance and improved working conditions. He instituted the world's first welfare state. Germany was the first to introduce social insurance programs including universal healthcare, compulsory education, sickness insurance, accident insurance, disability insurance, and a retirement pension. Moreover, the government's universal education policy bore fruit with Germany achieving the highest literacy rate in the world – 99% – education levels that provided the nation with more people good at handling numbers, more engineers, chemists, opticians, skilled workers for its factories, skilled managers, knowledgeable farmers and skilled military personnel.

By 1900 Germany surpassed Britain and the United States in steel production. The German economic miracle was also intensified by an unprecedented population growth from 35 million in 1850 to 67 million in 1913. From 1895 to 1907, the number of workers engaged in machine building doubled from half a million to well over a million. Only 40 percent of Germans lived in rural areas by 1910, a drop from 67% at the birth of the Empire. Industry accounted for 60 percent of the gross national product in 1913. The German chemical industry became the most advanced in the world, and by 1914 the country was producing half the world's electrical equipment.

The rapid advance to industrial maturity led to a drastic shift in Germany's economic situation – from a rural economy into a major exporter of finished goods. The ratio of finished product to total exports jumped from 38% in 1872 to 63% in 1912. By 1913 Germany had come to dominate all the European markets. By 1914 Germany had become one of the biggest exporters in the world.

The Nazis rose to power while unemployment was very high, but achieved full employment later thanks to massive public works programs such as the Reichsbahn, Reichspost and the Reichsautobahn projects. In 1935 rearmament in contravention of the Treaty of Versailles added to the economy.

Weimar and Nazi Germany By Stephen J. Lee

The post-1931 financial crisis economic policies of expansionary fiscal policies (as Germany was off the gold standard) was advised by their non-Nazi Minister of Economics, Hjalmar Schacht, who in 1933 became the president of the central bank. Hjalmar Schacht later abdicated from the post in 1938 and was replaced by Hermann Göring.

The trading policies of the Third Reich aimed at self sufficiency but with a lack of raw materials Germany would have to maintain trade links but on bilateral preferences, foreign exchange controls, import quotas and export subsidies under what was called the "New Plan"(Neuer Plan) of 19 September 1934. The "New Plan" was based on trade with less developed countries who would trade raw materials for German industrial goods saving currency. Southern Europe was preferable to Western Europe and North America as there could be no trade blockades. This policy became known as the Grosswirtschaftsraum ("greater economic area") policy.

Eventually, the Nazi party developed strong relationships with big business and abolished trade unions in 1933 in order to form the National Labor Service (RAD), German Labor Front (DAF) to set working hours, Beauty of Labour (SDA) which set working conditions and Strength through Joy (KDF) to ensure sports clubs for workers.

Beginning with the replacement of the Reichsmark with the Deutsche Mark as legal tender, a lasting period of low inflation and rapid industrial growth was overseen by the government led by German Chancellor Konrad Adenauer and his minister of economics, Ludwig Erhard, raising West Germany from total wartime devastation to one of the most developed nations in modern Europe.

In 1953 it was decided that Germany was to repay $1.1 billion of the aid it had received. The last repayment was made in June 1971.

Apart from these factors, hard work and long hours at full capacity among the population in the 1950s, 1960s and early 1970s and extra labor supplied by thousands of Gastarbeiter ("guest workers") provided a vital base for the economic upturn.

By the early 1950s the Soviet Union had seized reparations in the form of agricultural and industrial products and demanded further heavy reparation payments. Silesia with the Upper Silesian Coal Basin, and Stettin, a prominent natural port, were lost to Poland.

Exports from West Germany exceeded $323 billion in 1988. In the same year, East Germany exported $30.7 billion worth of goods; 65% to other communist states. East Germany had zero unemployment.

In 1976 the average annual GDP growth was roughly 5.9%.

The German economy practically stagnated in the beginning of the 2000s. The worst growth figures were achieved in 2002 (+1.4%), in 2003 (+1.0%) and in 2005 (+1.4%). Unemployment was also chronically high. Due to these problems, together with Germany's aging population, the welfare system came under considerable strain. This led the government to push through a wide-ranging program of belt-tightening reforms, Agenda 2010, including the labor market reforms known as Hartz I - IV.

In the later part of the first decade of 2000 the world economy experienced high growth, from which Germany as a leading exporter also profited. Some credit the Hartz reforms with achieving high growth and declining unemployment but others contend that they resulted in a massive decrease in standards of living, and that its effects are limited and temporary.

The nominal GDP of Germany contracted in the second and third quarters of 2008, putting the country in a technical recession following a global and European recession cycle. German industrial output dropped to 3.6% in September vis-à-vis August. In January 2009 the German government under Angela Merkel approved a €50 billion ($70 billion) economic stimulus plan to protect several sectors from a downturn and a subsequent rise in unemployment rates. Germany exited the recession in the second and third quarters of 2009, mostly due to rebounding manufacturing orders and exports - primarily from outside the Euro Zone - and relatively steady consumer demand.

Germany is a founding member of the EU, the G8 and the G20, and was the world's largest exporter from 2003 to 2008. In 2011 it remained the third largest exporter and third largest importer. Most of the country's exports are in engineering, especially machinery, automobiles, chemical goods and metals. Germany is a leading producer of wind turbines and solar-power technology. Annual trade fairs and congresses are held in cities throughout Germany.
2011 was a record-breaking year for the German economy. German companies exported goods worth over €1 trillion ($1.3 trillion), the highest figure in history. The number of people in work has risen to 41.6 million, the highest recorded figure.

Through 2012, Germany's economy continued to be stronger relative to local neighboring nations.

As of December 2017, the unemployment rate was at 5.5 percent.

, the unemployment rate was 4.8 percent.

, the CPI rate was 0.6 percent.

The following table shows the main economic indicators in 1980–2018. Inflation below 2% is in green.

1980 to 1995

1996 to 2010

2011 to 2018
Of the world's 500 largest stock-market-listed companies measured by revenue in 2010, the Fortune Global 500, 37 are headquartered in Germany. 30 Germany-based companies are included in the DAX, the German stock market index. Well-known global brands are Mercedes-Benz, BMW, SAP, Siemens, Volkswagen, Adidas, Audi, Allianz, Porsche, Bayer, BASF, Bosch, and Nivea.

Germany is recognised for its specialised small and medium enterprises, known as the Mittelstand model. Around 1,000 of these companies are global market leaders in their segment and are labelled hidden champions.

From 1991 to 2010, 40,301 mergers and acquisitions with an involvement of German firms with a total known value of 2,422 bil. EUR have been announced. The largest transactions since 1991 are: the acquisition of Mannesmann by Vodafone for 204.8 bil. EUR in 1999, the merger of Daimler-Benz with Chrysler to form DaimlerChrysler in 1998 valued at 36.3 bil. EUR.

Berlin (Economy of Berlin) developed an international Startup ecosystem and became a leading location for venture capital funded firms in the European Union.
The list includes the largest German companies by revenue in 2011:

Since the German reunification there have been 52,258 mergers or acquisitions deals inbound or outbound Germany. The most active year in terms of value was 1999 with a cumulated value of 48. bil. EUR, twice as much as the runner up which was 2006 with 24. bil. EUR (see graphic "M&A in Germany").

Here is a list of the top 10 deals (ranked by value) that include a German company. The Vodafone - Mannesmann deal is still the biggest deal in global history.

Germany as a federation is a polycentric country and does not have a single economic center. The stock exchange is located in Frankfurt am Main, the largest Media company (Bertelsmann SE & Co. KGaA) is headquartered in Gütersloh; the largest car manufacturers are in Wolfsburg (Volkswagen), Stuttgart (Mercedes-Benz and Porsche), and Munich (Audi and BMW).

Germany is an advocate of closer European economic and political integration. Its commercial policies are increasingly determined by agreements among European Union (EU) members and EU single market legislation. Germany introduced the common European currency, the euro on 1 January 1999. Its monetary policy is set by the European Central Bank in Frankfurt.

The southern states ("Bundesländer"), especially Bayern, Baden-Württemberg and Hessen, are economically stronger than the northern states. One of Germany's traditionally strongest (and at the same time oldest) economic regions is the Ruhr area in the west, between Duisburg and Dortmund. 27 of the country's 100 largest companies are located there. In recent years, however, the area, whose economy is based on natural resources and heavy industry, has seen a substantial rise in unemployment (2010: 8.7%).

The economy of Bayern and Baden-Württemberg, the states with the lowest number of unemployed people (2018: 2.7%, 3.1%), on the other hand, is based on high-value products. Important sectors are automobiles, electronics, aerospace and biomedicine, among others. Baden-Württemberg is an industrial center especially for automobile and machine building industry and the home of brands like Mercedes-Benz (Daimler), Porsche and Bosch.

With the reunification on 3 October 1990, Germany began the major task of reconciling the economic systems of the two former republics. Interventionist economic planning ensured gradual development in eastern Germany up to the level of former West Germany, but the standard of living and annual income remains significantly higher in western German states. The modernisation and integration of the eastern German economy continues to be a long-term process scheduled to last until the year 2019, with annual transfers from west to east amounting to roughly $80 billion. The overall unemployment rate has consistently fallen since 2005 and reached a 20-year low in 2012. The country in July 2014 began legislating to introduce a federally mandated minimum wage which would come into effect on 1 January 2015.

The following top 10 list of German billionaires is based on an annual assessment of wealth and assets compiled and published by "Forbes" magazine on 1 March 2016.


Wolfsburg is the city in Germany with the country's highest per capita GDP, at $128,000. The following top 10 list of German cities with the highest per capita GDP is based on a study by the Cologne Institute for Economic Research on 31 July 2013.


Germany has a social market economy characterised by a highly qualified labor force, a developed infrastructure, a large capital stock, a low level of corruption, and a high level of innovation. It has the largest national economy in Europe, the fourth largest by nominal GDP in the world, and ranked fifth by GDP (PPP) in 2015.

The service sector contributes around 70% of the total GDP, industry 29.1%, and agriculture 0.9%.

In 2010 agriculture, forestry, and mining accounted for only 0.9% of Germany's gross domestic product (GDP) and employed only 2.4% of the population, down from 4% in 1991. Agriculture is extremely productive, and Germany is able to cover 90% of its nutritional needs with domestic production. Germany is the third largest agricultural producer in the European Union after France and Italy. Germany's principal agricultural products are potatoes, wheat, barley, sugar beets, fruit, and cabbages.

Despite the country's high level of industrialization, almost one-third of its territory is covered by forest. The forestry industry provides for about two-thirds of domestic consumption of wood and wood products, so Germany is a net importer of these items.

The German soil is relatively poor in raw materials. Only lignite (brown coal) and potash salt (Kalisalz) are available in significant quantities. However, the former GDR's Wismut mining company produced a total of 230,400 tonnes of uranium between 1947 and 1990 and made East Germany the fourth largest producer of uranium ore worldwide (largest in USSR's sphere of control) at the time. Oil, natural gas and other resources are, for the most part, imported from other countries.

Potash salt is mined in the center of the country (Niedersachsen, Sachsen-Anhalt and Thüringen). The most important producer is K+S AG (formerly Kali und Salz AG).

Germany's bituminous coal deposits were created more than 300 million years ago from swamps which extended from the present-day South England, over the Ruhr area to Poland. Lignite deposits developed in a similar way, but during a later period, about 66 million years ago. Because the wood is not yet completely transformed into coal, brown coal contains less energy than bituminous coal.

Lignite is extracted in the extreme western and eastern parts of the country, mainly in Nordrhein-Westfalen, Sachsen and Brandenburg. Considerable amounts are burned in coal plants near to the mining areas, to produce electricity. Transporting lignite over far distances is not economically feasible, therefore the plants are located practically next to the extraction sites. Bituminous coal is mined in Nordrhein-Westfalen and Saarland. Most power plants burning bituminous coal operate on imported material, therefore the plants are located not only near to the mining sites, but throughout the country.

Industry and construction accounted for 30.7% of gross domestic product in 2017, and employed 24.2% of the workforce. Germany excels in the production of automobiles, machinery, electrical equipment and chemicals. With the manufacture of 5.2 million vehicles in 2009, Germany was the world's fourth largest producer and largest exporter of automobiles. German automotive companies enjoy an extremely strong position in the so-called premium segment, with a combined world market share of about 90%.

Small- to medium-sized manufacturing firms (Mittelstand companies) which specialize in technologically advanced niche products and are often family-owned form a major part of the German economy. It is estimated that about 1500 German companies occupy a top three position in their respective market segment worldwide. In about two thirds of all industry sectors German companies belong to the top three competitors.

Germany is the only country among the top five arms exporters that is not a permanent member of the United Nations Security Council.

In 2017 services constituted 68.6% of gross domestic product (GDP), and the sector employed 74.3% of the workforce. The subcomponents of services are financial, renting, and business activities (30.5%); trade, hotels and restaurants, and transport (18%); and other service activities (21.7%).

Germany is the seventh most visited country in the world, with a total of 407 million overnights during 2012. This number includes 68.83 million nights by foreign visitors. In 2012, over 30.4 million international tourists arrived in Germany. Berlin has become the third most visited city destination in Europe. Additionally, more than 30% of Germans spend their holiday in their own country, with the biggest share going to Mecklenburg-Vorpommern. Domestic and international travel and tourism combined directly contribute over EUR43.2 billion to German GDP. Including indirect and induced impacts, the industry contributes 4.5% of German GDP and supports 2 million jobs (4.8% of total employment). The largest annual international trade fairs and congresses are held in several German cities such as Hannover, Frankfurt, and Berlin.

The debt-to-GDP ratio of Germany had its peak in 2010 when it stood at 80.3% and decreased since then. According to Eurostat, the government gross debt of Germany amounts to €2,152.0 billion or 71.9% of its GDP in 2015. The federal government achieved a budget surplus of €12.1 billion ($13.1 billion) in 2015. Germany's credit rating by credit rating agencies Standard & Poor's, Moody's and Fitch Ratings stands at the highest possible rating "AAA" with a stable outlook in 2016.

Germany's "debt clock" ("Schuldenuhr") reversed for the first time in 20 years in January 2018. It is now running backwards at €78 per second.

Economists generally see Germany's current account surplus as undesirable.

Germany is the world's fifth largest consumer of energy, and two-thirds of its primary energy was imported in 2002. In the same year, Germany was Europe's largest consumer of electricity, totaling 512.9 terawatt-hours. Government policy promotes energy conservation and the development of renewable energy sources, such as solar, wind, biomass, hydroelectric, and geothermal energy. As a result of energy-saving measures, energy efficiency has been improving since the beginning of the 1970s. The government has set the goal of meeting half the country's energy demands from renewable sources by 2050.

In 2000, the red-green coalition under Chancellor Schröder and the German nuclear power industry agreed to phase out all nuclear power plants by 2021. The conservative coalition under Chancellor Merkel reversed this decision in January 2010, electing to keep plants open. The nuclear disaster of the Japanese nuclear plant Fukushima in March 2011 however, changed the political climate fundamentally: Older nuclear plants have been shut down and a general phase-out until 2020 or 2022 is now probable. Renewable energy yet still plays a more modest role in energy consumption, though German solar and windpower industries play a leading role worldwide.

In 2009, Germany's total energy consumption (not just electricity) came from the following sources:
Oil 34.6%, Natural gas 21.7%, Lignite 11.4%, Bituminous coal 11.1%, Nuclear power 11.0%, Hydro and wind power 1.5%, Others 9.0%.

There are 3 major entry points for oil pipelines: in the northeast (the Druzhba pipeline, coming from Gdańsk), west (coming from Rotterdam) and southeast (coming from Nelahozeves). The oil pipelines of Germany do not constitute a proper network, and sometimes only connect two different locations. Major oil refineries are located in or near the following cities: Schwedt, Spergau, Vohburg, Burghausen, Karlsruhe, Cologne, Gelsenkirchen, Lingen, Wilhelmshaven, Hamburg and Heide.

Germany's network of natural gas pipelines, on the other hand, is dense and well-connected. Imported pipeline gas comes mostly from Russia, the Netherlands and the United Kingdom. Although gas imports from Russia have been historically reliable, even during the cold war, recent price disputes between Gazprom and the former Soviet states, such as Ukraine, have also affected Germany. As a result, high political importance is placed on the construction of the Nord Stream pipeline, running from Vyborg in Russia along the Baltic sea to Greifswald in Germany. This direct connection avoids third-party transit countries. Germany imports 50% to 75% of its natural gas from Russia.

With its central position in Europe, Germany is an important transportation hub. This is reflected in its dense and modern transportation networks. The extensive motorway (Autobahn) network that ranks worldwide third largest in its total length and features a lack of blanket speed limits on the majority of routes.

Germany has established a polycentric network of high-speed trains. The InterCityExpress or "ICE" is the most advanced service category of the Deutsche Bahn and serves major German cities as well as destinations in neighbouring countries. The train maximum speed varies between 200 km/h and 320 km/h (125-200 mph). Connections are offered at either 30-minute, hourly, or two-hourly intervals. German railways are heavily subsidised, receiving €17.0 billion in 2014.

The largest German airports are the Frankfurt International Airport and the Munich International Airport, both are global hubs of Lufthansa. Other major airports are Berlin Tegel, Berlin Schönefeld, Düsseldorf, Hamburg, Hanover, Cologne-Bonn, Leipzig/Halle and in the future Berlin Brandenburg International Airport.

Germany's achievements in sciences have been significant, and research and development efforts form an integral part of the economy.

Germany is also one of the leading countries in developing and using green technologies. Companies specializing in green technology have an estimated turnover of €200 billion. German expertise in engineering, science and research is eminently respectable.

The lead markets of Germany's green technology industry are power generation, sustainable mobility, material efficiency, energy efficiency, waste management and recycling, sustainable water management.

With regard to triadic patents Germany is in third place after the US and Japan. With more than 26,500 registrations for patents submitted to the European Patent Office, Germany is the leading European nation. Siemens, Bosch and BASF, with almost 5,000 registrations for patents between them in 2008, are among the Top 5 of more than 35,000 companies registering patents. Together with the US and Japan, with regard to patents for nano, bio and new technologies Germany is one of the world's most active nations. With around one third of triadic patents Germany leads the way worldwide in the field of vehicle emission reduction.

According to Winfried Kretschmann, who is premier of the region where Daimler is based, "China dominates the production of solar cells. Tesla is ahead in electric cars and Germany has lost the first round of digitalization to Google, Apple and the like. Whether Germany has a future as an industrial economy will depend on whether we can manage the ecological and digital transformation of our economy".

Despite economic prosperity, Germany's biggest threat for future economic development is the nation's declining birthrate which is among the lowest in the world. This is particularly prevalent in parts of society with higher education. As a result, the numbers of workers is expected to decrease and the government spending needed to support pensioners and healthcare will increase if the trend is not reversed.

Less than a quarter of German people expect living conditions to improve in the coming decades.

On August 25, 2020, Federal Statistical Office of Germany revealed that German economy plunged by 9.7% in the second quarter which is the worst on record. The latest figures show how hard the German economy was hit by the coronavirus pandemic.




</doc>
<doc id="11932" url="https://en.wikipedia.org/wiki?curid=11932" title="Transport in Germany">
Transport in Germany

As a densely populated country in a central location in Europe and with a developed economy, Germany has a dense and modern transport infrastructure.

The first highway system to have been built, the extensive German Autobahn network famously has no general speed limit for light vehicles (although advisory speed limits are given in most sections today, and there is a blanket 80 km/h limit for trucks). The country's most important waterway is the river Rhine. The largest port is that of Hamburg. Frankfurt Airport is a major international airport and European transport hub. Air travel is used for greater distances within Germany but faces competition from the state-owned Deutsche Bahn's rail network. High-speed trains called ICE connect cities for passenger travel with speeds up to 300 km/h. Many German cities have rapid transit systems and public transport is available in most areas. Buses have historically only played a marginal role in long distance passenger service, as all routes directly competing with rail services were technically outlawed by a law dating to 1935 (during the Nazi era). Only in 2012 was this law officially amended and thus a long distance bus market has also emerged in Germany since then.

Since German reunification substantial effort has been made to improve and expand transport infrastructure in what was formerly East Germany.

Verkehrsmittel and Verkehrszeichen - Transportation signs in Germany are available here in German and English.

The volume of traffic in Germany, especially goods transportation, is at a very high level due to its central location in Europe.
In the past few decades, much of the freight traffic shifted from rail to road, which led the Federal Government to introduce a motor toll for trucks in 2005. Individual road usage increased resulting in a relatively high traffic density to other nations. A further increase of traffic is expected in the future.

High-speed vehicular traffic has a long tradition in Germany given that the first freeway (Autobahn) in the world, the AVUS, and the world's first automobile were developed and built in Germany. Germany possesses one of the most dense road systems of the world. German motorways have no blanket speed limit for light vehicles. However, posted limits are in place on many dangerous or congested stretches as well as where traffic noise or pollution poses a problem (20.8% under static or temporary limits and an average 2.6% under variable traffic control limit applications as of 2015).

The German government has had issues with upkeep of the country's autobahn network, having had to revamp the Eastern portion's transport system since the unification of Germany between the German Democratic Republic (East Germany) and the Federal Republic of Germany (West Germany). With that, numerous construction projects have been put on hold in the west, and a vigorous reconstruction has been going on for almost 20 years. However, ever since the European Union formed, an overall streamlining and change of route plans have occurred as faster and more direct links to former Soviet bloc countries now exist and are in the works, with intense co-operation among European countries.

Intercity bus service within Germany fell out of favour as post-war prosperity increased, and became almost extinct when legislation was introduced in the 1980s to protect the national railway. After that market was deregulated in 2012, some 150 new intercity bus lines have been established, leading to a significant shift from rail to bus for long journeys. The market has since consolidated with Flixbus controlling over 90% of it and also expanding into neighboring countries.

Germany has approximately 650,000 km of roads, of which 231,000 km are non-local roads. The road network is extensively used with nearly 2 trillion km travelled by car in 2005, in comparison to just 70 billion km travelled by rail and 35 billion km travelled by plane.

The Autobahn is the German federal highway system. The official German term is (plural "", abbreviated 'BAB'), which translates as 'federal motorway'. Where no local speed limit is posted, the advisory limit "(Richtgeschwindigkeit)" is 130 km/h. The "Autobahn" network had a total length of about in 2016, which ranks it among the most dense and longest systems in the world. Only federally built controlled-access highways meeting certain construction standards including at least two lanes per direction are called ""Bundesautobahn"". They have their own, blue-coloured signs and their own numbering system. All "Autobahnen" are named by using the capital letter A, followed by a blank and a number (for example A 8).

The main "Autobahnen" going all across Germany have single digit numbers. Shorter highways of regional importance have double digit numbers (like A 24, connecting Berlin and Hamburg). Very short stretches built for heavy local traffic (for example ring roads or the A 555 from Cologne to Bonn) usually have three digits, where the first digit depends on the region.

East-west routes are usually even-numbered, north-south routes are usually odd-numbered. The numbers of the north-south "Autobahnen" increase from west to east; that is to say, the more easterly roads are given higher numbers. Similarly, the east-west routes use increasing numbers from north to south.

The autobahns are considered the safest category of German roads: for example, in 2012, while carrying 31% of all motorized road traffic, they only accounted for 11% of Germany's traffic fatalities.<ref name="http://www.bast.de 2012"></ref>

German autobahns are still toll-free for light vehicles, but on 1 January 2005, a blanket mandatory toll on heavy trucks was introduced.

The national roads in Germany are called "Bundesstraßen" (federal roads). Their numbers are usually well known to local road users, as they appear (written in black digits on a yellow rectangle with black border) on direction traffic signs and on street maps. A Bundesstraße is often referred to as "B" followed by its number, for example "B1", one of the main east-west routes. More important routes have lower numbers. Odd numbers are usually applied to north-south oriented roads, and even numbers for east-west routes. Bypass routes are referred to with an appended "a" (alternative) or "n" (new alignment), as in "B 56n".

Other main public roads are maintained by the "Bundesländer" (states), called "Landesstraße" (country road) or "Staatsstraße" (state road). The numbers of these roads are prefixed with "L", "S" or "St", but are usually not seen on direction signs or written on maps. They appear on the kilometre posts on the roadside. Numbers are unique only within one state.

The "Landkreise" (districts) and municipalities are in charge of the minor roads and streets within villages, towns and cities. These roads have the number prefix "K" indicating a "Kreisstraße".

Germany features a total of 43,468 km railways, of which at least 19,973 km are electrified (2014).

Deutsche Bahn (German Rail) is the major German railway infrastructure and service operator. Though Deutsche Bahn is a private company, the government still holds all shares and therefore Deutsche Bahn can still be called a state-owned company. Since its reformation under private law in 1994, Deutsche Bahn AG (DB AG) no longer publishes details of the tracks it owns; in addition to the DBAG system there are about 280 privately or locally owned railway companies which own an approximate 3,000 km to 4,000 km of the total tracks and use DB tracks in "open access".

Railway subsidies amounted to €17.0 billion in 2014 and there are significant differences between the financing of long-distance and short-distance (or local) trains in Germany. While long-distance trains can be run by any railway company, the companies also receive no subsidies from the government. Local trains however are subsidised by the German states, which pay the operating companies to run these trains and indeed in 2013, 59% of the cost of short-distance passenger rail transport was covered by subsidies. This resulted in many private companies offering to run local train services as they can provide cheaper service than the state-owned Deutsche Bahn. Track construction is entirely and track maintenance partly government financed both for long and short range trains. On the other hand, all rail vehicles are charged track access charges by DB Netz which in turn delivers (part of) its profits to the federal budget.

High speed rail started in the early 1990s with the introduction of the Inter City Express (ICE) into revenue service after first plans to modernize the rail system had been drawn up under the government of Willy Brandt. While the high speed network is not as dense as those of France or Spain, ICE or slightly slower (max. speed 200 km/h) Intercity (IC) serve most major cities. Several extensions or upgrades to high speed lines are under construction or planned for the near future, some of them after decades of planning.

The fastest high-speed train operated by Deutsche Bahn, the InterCityExpress or ICE connects major German and neighbouring international centres such as Zurich, Vienna, Copenhagen, Paris, Amsterdam and Brussels. The rail network throughout Germany is extensive and provides excellent service in most areas. On regular lines, at least one train every two hours will call even in the smallest of villages during the day. Nearly all larger metropolitan areas are served by S-Bahn, U-Bahn, Straßenbahn and/or bus networks.

The German government on 13 February 2018 announced plans to make public transportation free as a means to reduce road traffic and decrease air pollution to EU-mandated levels. The new policy will be put to the test by the end of the year in the cities of Bonn, Essen, Herrenberg, Reutlingen and Mannheim. Issues remain concerning the costs of such a move as ticket sales for public transportation constitute a major source of income for cities.

While Germany and most of contiguous Europe use , differences in signalling, rules and regulations, electrification voltages, etc. create obstacles for freight operations across borders. These obstacles are slowly being overcome, with international (in- and outgoing) and transit (through) traffic being responsible for a large part of the recent uptake in rail freight volume. EU regulations have done much to harmonize standards, making cross border operations easier. Maschen Marshalling Yard near Hamburg is the second biggest in the world and the biggest in Europe. It serves as a freight hub distributing goods from Scandinavia to southern Europe and from Central Europe to the port of Hamburg and overseas. Being a densely populated prosperous country in the center of Europe, there are many important transit routes through Germany. The Mannheim–Karlsruhe–Basel railway has undergone upgrades and refurbishments since the 1980s and will likely undergo further upgrades for decades to come as it is the main route from the North Sea Ports to northern Italy via the Gotthard Base Tunnel.

Almost all major metro areas of Germany have suburban rail systems called S-Bahnen ("Schnellbahnen"). These usually connect larger agglomerations to their suburbs and often other regional towns, although the Rhein-Ruhr S-Bahn connects several large cities. A S-Bahn doesn't skip stations and runs more frequently than other trains. In Berlin and Hamburg the S-Bahn has a U-Bahn-like service and uses a third rail whereas all other S-Bahn services rely on regular catenary power supply.

Relatively few cities have a full-fledged underground U-Bahn system; S-Bahn (suburban commuter railway) systems are far more common. In some cities the distinction between U-Bahn and S-Bahn systems is blurred, for instance some S-Bahn systems run underground, have frequencies similar to U-Bahn, and form part of the same integrated transport network. A larger number of cities has upgraded their tramways to light rail standards. These systems are called Stadtbahn (not to be confused with S-Bahn), on main line rails.

Cities with U-Bahn systems are:

With the exception of Hamburg, all of those aforementioned cities also have a tram system, often with new lines built to light rail standards.

Cities with "Stadtbahn" systems can be found in the article Trams in Germany.

Germany was among the first countries to have electric street - running railways and Berlin has one of the longest tram networks in the world. Many West German cities abandoned their previous tram systems in the 1960s and 1970s while others upgraded them to "Stadtbahn" (~light rail) standard, often including underground sections. In the East, most cities retained or even expanded their tram systems and since reunification a trend towards new tram construction can be observed in most of the country. Today the only major German city without a tram or light rail system is Hamburg. Tram-train systems like the Karlsruhe model first came to prominence in Germany in the early 1990s and are implemented or discussed in several cities, providing coverage far into the rural areas surrounding cities.

Short distances and the extensive network of motorways and railways make airplanes uncompetitive for travel within Germany. Only about 1% of all distance travelled was by plane in 2002. But due to a decline in prices with the introduction of low-fares airlines, domestic air travel is becoming more attractive. In 2013 Germany had the fifth largest passenger air market in the world with 105,016,346 passengers. However, the advent of new faster rail lines often leads to cuts in service by the airlines or even total abandonment of routes like Frankfurt-Cologne, Berlin-Hannover or Berlin-Hamburg.

Germany's largest airline is Lufthansa, which was privatised in the 1990s. Lufthansa also operates two regional subsidiaries under the Lufthansa Regional brand and a low-cost subsidiary, Eurowings, which operates independently. Lufthansa flies a dense network of domestic, European and intercontinental routes. Germany's second-largest airline was Air Berlin, which also operated a network of domestic and European destinations with a focus on leisure routes as well as some long-haul services. Air Berlin declared bankruptcy in 2017 with the last flight under its own name in October of that year.

Charter and leisure carriers include Condor, TUIfly, MHS Aviation and Sundair. Major German cargo operators are Lufthansa Cargo, European Air Transport Leipzig (which is a subsidiary of DHL) and AeroLogic (which is jointly owned by DHL and Lufthansa Cargo).

Frankfurt Airport is Germany's largest airport, a major transportation hub in Europe and the world's twelfth busiest airport. It is one of the airports with the largest number of international destinations served worldwide. Depending on whether total passengers, flights or cargo traffic are used as a measure, it ranks first, second or third in Europe alongside London Heathrow Airport and Paris-Charles de Gaulle Airport. Germany's second biggest international airport is Munich Airport followed by Düsseldorf Airport.

There are several more scheduled passenger airports throughout Germany, mainly serving European metropolitan and leisure destinations. Intercontinental long-haul routes are operated to and from the airports in Frankfurt, Munich, Düsseldorf, Berlin-Tegel, Cologne/Bonn, Hamburg and Stuttgart.

Berlin Brandenburg Airport is expected to become the third largest German airport by annual passengers once it opens, serving as single airport for Berlin. Originally planned to be completed in 2011, the new airport has been delayed several times due to poor construction management and technical difficulties. As of September 2014, it is not yet known when the new airport will become operational. In 2017 it was announced that the airport wouldn't open before 2019. In the same year a non-binding referendum to keep Tegel Airport open even after the new airport opens was passed by Berlin voters.

Airports — with paved runways:

Airports — with unpaved runways:

Heliports: 23 (2013 est.)

Waterways: 7,467 km (2013); major rivers include the Rhine and Elbe; Kiel Canal is an important connection between the Baltic Sea and North Sea and one of the busiest waterways in the world, the Rhine-Main-Danube Canal links Rotterdam on the North Sea with the Black Sea. It passes through the highest point reachable by ocean-going vessels from the sea. The Canal has gained importance for leisure cruises in addition to cargo traffic.

Pipelines: oil 2,400 km (2013)

Ports and harbours: Berlin, Bonn, Brake, Bremen, Bremerhaven, Cologne, Dortmund, Dresden, Duisburg, Emden, Fürth, Hamburg, Karlsruhe, Kiel, Lübeck, Magdeburg, Mannheim, Nuremberg, Oldenburg, Rostock, Stuttgart, Wilhelmshaven

The port of Hamburg is the largest sea-harbour in Germany and ranks #3 in Europe (after Rotterdam and Antwerpen), #17 worldwide (2016), in total container traffic.

Merchant marine:
<br>total: 427 ships 
<br>Ships by type: barge carrier 2, bulk carrier 6, cargo ship 51, chemical tanker 15, container ship 298, Liquified Gas Carrier 6, passenger ship 4, petroleum tanker 10, refrigerated cargo 3, roll-on/roll-off ship 6 (2010 est.)

Ferries operate mostly between mainland Germany and its islands, serving both tourism and freight transport. Car ferries also operate across the Baltic Sea to the Nordic countries, Russia and the Baltic countries. Rail ferries operate across the Fehmahrnbelt, from Rostock to Sweden (both carrying passenger trains) and from the Mukran port in Sassnitz on the island of Rügen to numerous Baltic Sea destinations (freight only).

<br>


</doc>
<doc id="11933" url="https://en.wikipedia.org/wiki?curid=11933" title="Military of Germany (disambiguation)">
Military of Germany (disambiguation)

Military of Germany may refer to:



</doc>
<doc id="11934" url="https://en.wikipedia.org/wiki?curid=11934" title="Foreign relations of Germany">
Foreign relations of Germany

The Federal Republic of Germany (FRG) is a Central European country and member of the European Union, G4, G7, the G20, the Organisation for Economic Co-operation and Development and the North Atlantic Treaty Organization (NATO). It maintains a network of 229 diplomatic missions abroad and holds relations with more than 190 countries. As one of the world's leading industrialized countries it is recognized as a major power in European and global affairs.

The three cabinet-level ministries responsible for guiding Germany's foreign policy are the Ministry of Defense, the Ministry of Economic Cooperation and Development and the Federal Foreign Office. In practice, most German federal departments play some role in shaping foreign policy in the sense that there are few policy areas left that remain outside of international jurisdiction. The bylaws of the Federal Cabinet (as delineated in Germany's Basic Law), however, assign the Federal Foreign Office a coordinating function. Accordingly, other ministries may only invite foreign guests or participate in treaty negotiations with the approval of the Federal Foreign Office.

With respect to foreign policy, the Bundestag acts in a supervisory capacity. Each of its committees – most notably the foreign relations committee – oversees the country's foreign policy. The consent of the Bundestag (and insofar as Länder are impacted, the Bundesrat) is required to ratify foreign treaties. If a treaty legislation passes first reading, it is referred to the Committee on Foreign Affairs, which is capable of delaying ratification and prejudice decision through its report to the Bundestag.

In 1994, a full EU Committee was also created for the purpose of addressing the large flow of EU-related topics and legislation. Also, the committee has the mandate to speak on behalf of the Bundestag and represent it when deciding an EU policy position. A case in point was the committee's involvement regarding the European Union's eastern enlargement wherein the Committee on Foreign Affairs is responsible for relations with ECE states while the EU Committee is tasked with the negotiations.

There is a raft of NGOs in Germany that engage foreign policy issues. These NGOs include think-tanks (German Council on Foreign Relations), single-issue lobbying organizations (Amnesty International), as well as other organizations that promote stronger bilateral ties between Germany and other countries (Atlantic Bridge). While the budgets and methods of NGOs are distinct, the overarching goal to persuade decision-makers to the wisdom of their own views is a shared one. In 2004, a new German governance framework, particularly on foreign and security policy areas, emerged where NGOs are integrated into actual policymaking. The idea is that the cooperation between state and civil society groups increases the quality of conflict resolution, development cooperation and humanitarian aid for fragile states. The framework seeks to benefit from the expertise of the NGOs in exchange for these groups to have a chance for influencing foreign policy.

In 2001, the discovery that the terrorist cell which carried out the attacks against the United States on 11 September 2001, was based in Hamburg, sent shock waves through the country.

The government of Chancellor Gerhard Schröder backed the following U.S. military actions, sending Bundeswehr troops to Afghanistan to lead a joint NATO program to provide security in the country after the ousting of the Taliban.

Nearly all of the public was strongly against America's 2003 invasion of Iraq, and any deployment of troops. This position was shared by the SPD/Green government, which led to some friction with the United States.

In August 2006, the German government disclosed a botched plot to bomb two German trains. The attack was to occur in July 2006 and involved a 21-year-old Lebanese man, identified only as Youssef Mohammed E. H. Prosecutors said Youssef and another man left suitcases stuffed with crude propane-gas bombs on the trains.

As of February 2007, Germany had about 3,000 NATO-led International Security Assistance Force force in Afghanistan as part of the War on Terrorism, the third largest contingent after the United States (14,000) and the United Kingdom (5,200). German forces are mostly in the more secure north of the country.

However, Germany, along with some other larger European countries (with the exception of the UK and the Netherlands), have been criticised by the UK and Canada for not sharing the burden of the more intensive combat operations in southern Afghanistan.

Germany is the largest net contributor to the United Nations and has several development agencies working in Africa and the Middle East. The development policy of the Federal Republic of Germany is an independent area of German foreign policy. It is formulated by the Federal Ministry for Economic Cooperation and Development (BMZ) and carried out by the implementing organisations. The German government sees development policy as a joint responsibility of the international community. It is the world's third biggest aid donor after the United States and France. Germany spent 0.37 per cent of its gross domestic product (GDP) on development, which is below the government's target of increasing aid to 0.51 per cent of GDP by 2010. The international target of 0.7% of GNP would have not been reached either.

Germany is a member of the Council of Europe, European Union, European Space Agency, G4, G8, International Monetary Fund, NATO, OECD, Organization for Security and Co-operation in Europe, UN, World Bank Group and the World Trade Organization.

European integration has gone a long way since the European Coal and Steel Community (ECSC) and the Elysée Treaty. Peaceful collaborations with its neighbors remain one of Germany's biggest political objectives, and Germany has been on the forefront of most achievements made in European integration:

Most of the social issues facing European countries in general: immigration, aging populations, straining social-welfare and pension systems – are all important in Germany.
Germany seeks to maintain peace through the "deepening" of integration among current members of the European Union member states

Germany has been the largest net contributor to EU budgets for decades (in absolute terms – given Germany's comparatively large population – not per capita) and seeks to limit the growth of these net payments in the enlarged union.

Under the doctrine introduced by the 2003 Defense Policy Guidelines, Germany continues to give priority to the transatlantic partnership with the United States through the North Atlantic Treaty Organization. However, Germany is giving increasing attention to coordinating its policies with the European Union through the Common Foreign and Security Policy.

The German Federal Government began an initiative to obtain a permanent seat in the United Nations Security Council, as part of the Reform of the United Nations. This would require approval of a two-thirds majority of the member states and approval of all five Security Council veto powers.

This aspiration could be successful due to Germany's good relations with the People's Republic of China and the Russian Federation. Germany is a stable and democratic republic and a G7 country which are also favourable attributes. The United Kingdom and France support German ascension to the supreme body. The U.S. is sending mixed signals.

NATO member states, including Germany, decided not to sign the UN treaty on the Prohibition of Nuclear Weapons, a binding agreement for negotiations for the total elimination of nuclear weapons, supported by more than 120 nations.

The German government was a strong supporter of the enlargement of NATO.

Germany was one of the first nations to recognize Croatia and Slovenia as independent nations, rejecting the concept of Yugoslavia as the only legitimate political order in the Balkans (unlike other European powers, who first proposed a pro-Belgrade policy). This is why Serb authorities sometimes referred to "new German imperialism" as one of the main reasons for Yugoslavia's collapse. German troops participate in the multinational efforts to bring "peace and stability" to the Balkans.

Weimar triangle (France, Germany and Poland); Germany continues to be active economically in the states of Central Europe, and to actively support the development of democratic institutions. In the 2000s, Germany has been arguably the centerpiece of the European Union (though the importance of France cannot be overlooked in this connection).






</doc>
<doc id="11935" url="https://en.wikipedia.org/wiki?curid=11935" title="Politics of Germany">
Politics of Germany

Germany is a democratic, federal parliamentary republic, where federal legislative power is vested in the Bundestag (the parliament of Germany) and the Bundesrat (the representative body of the Länder, Germany's regional states).

The multilateral system has, since 1949, been dominated by the Christian Democratic Union (CDU) and the Social Democratic Party of Germany (SPD). The judiciary of Germany is independent of the executive and the legislature, while it is common for leading members of the executive to be members of the legislature as well. The political system is laid out in the 1949 constitution, the "Grundgesetz" (Basic Law), which remained in effect with minor amendments after German reunification in 1990.

The constitution emphasizes the protection of individual liberty in an extensive catalogue of human and civil rights and divides powers both between the federal and state levels and between the legislative, executive and judicial branches.

West Germany was a founding member of the European Community in 1958, which became the EU in 1993. Germany is part of the Schengen Area, and has been a member of the eurozone since 1999. It is a member of the United Nations, NATO, the G7, the G20 and the OECD.

After 1949, the Federal Republic of Germany had Christian Democratic chancellors for 20 years until a coalition between the Social Democrats and the Liberals took over. From 1982, Christian Democratic leader Helmut Kohl was chancellor in a coalition with the Liberals for 16 years. In this period fell the reunification of Germany, in 1990: the German Democratic Republic joined the Federal Republic. In the former GDR's territory, five "Länder" (states) were established or reestablished. The two parts of Berlin united as one "Land" (state).

The political system of the Federal Republic remained more or less unchanged. Specific provisions for the former GDR territory were enabled via the "unification treaty" between the Federal Republic and the GDR prior to the unification day of 3 October 1990. However, Germany saw in the following two distinct party systems: the Green party and the Liberals remained mostly West German parties, while in the East the former socialist state party, now called PDS, flourished along with the Christian Democrats and Social Democrats.

After 16 years of the Christian–Liberal coalition, led by Helmut Kohl, the Social Democratic Party of Germany (SPD) together with the Greens won the Bundestag elections of 1998. SPD vice chairman Gerhard Schröder positioned himself as a centrist candidate, in contradiction to the leftist SPD chairman Oskar Lafontaine. The Kohl government was hurt at the polls by slower economic growth in the East in the previous two years, and constantly high unemployment. The final margin of victory was sufficiently high to permit a "red-green" coalition of the SPD with Alliance 90/The Greens ("Bündnis '90/Die Grünen"), bringing the Greens into a national government for the first time.

Initial problems of the new government, marked by policy disputes between the moderate and traditional left wings of the SPD, resulted in some voter disaffection. Lafontaine left the government (and later his party) in early 1999. The CDU won in some important state elections but was hit in 2000 by a party donation scandal from the Kohl years. As a result of this Christian Democratic Union (CDU) crisis, Angela Merkel became chair.

The next election for the "Bundestag" was on 22 September 2002. Gerhard Schröder led the coalition of SPD and Greens to an eleven-seat victory over the Christian Democrat challengers headed by Edmund Stoiber (CSU). Three factors are generally cited that enabled Schröder to win the elections despite poor approval ratings a few months before and a weaker economy: good handling of the 100-year flood, firm opposition to the US 2003 invasion of Iraq, and Stoiber's unpopularity in the east, which cost the CDU crucial seats there.

In its second term, the red–green coalition lost several very important state elections, for example in Lower Saxony where Schröder was the prime minister from 1990 to 1998. On 20 April 2003, chancellor Schröder announced massive labor market reforms, called Agenda 2010, that cut unemployment benefits. Although these reforms sparked massive protests, they are now credited with being in part responsible for the relatively strong economic performance of Germany during the euro-crisis and the decrease in unemployment in Germany in the years 2006-2007.

On 22 May 2005 the SPD received a devastating defeat in its former heartland, North Rhine-Westphalia. Half an hour after the election results, the SPD chairman Franz Müntefering announced that the chancellor would clear the way for new federal elections.

This took the republic by surprise, especially because the SPD was below 20% in polls at the time. The CDU quickly announced Angela Merkel as Christian Democrat candidate for chancellor, aspiring to be the first female chancellor in German history.

New for the 2005 election was the alliance between the newly formed Electoral Alternative for Labor and Social Justice (WASG) and the PDS, planning to fuse into a common party (see Left Party.PDS). With the former SPD chairman, Oskar Lafontaine for the WASG and Gregor Gysi for the PDS as prominent figures, this alliance soon found interest in the media and in the population. Polls in July saw them as high as 12%.

Whereas in May and June 2005 victory of the Christian Democrats seemed highly likely, with some polls giving them an absolute majority, this picture changed shortly before the election on 18 September 2005.

The election results of 18 September were surprising because they differed widely from the polls of the previous weeks. The Christian Democrats even lost votes compared to 2002, narrowly reaching the first place with only 35.2%, and failed to get a majority for a "black–yellow" government of CDU/CSU and liberal FDP. But the red–green coalition also failed to get a majority, with the SPD losing votes, but polling 34.2% and the greens staying at 8.1%. The Left reached 8.7% and entered the "Bundestag", whereas the far-right NPD only got 1.6%.

The most likely outcome of coalition talks was a so-called grand coalition between the Christian Democrats (CDU/CSU) and the Social Democrats (SPD). Three party coalitions and coalitions involving The Left had been ruled out by all interested parties (including The Left itself). On 22 November 2005, Angela Merkel was sworn in by president Horst Köhler for the office of Bundeskanzlerin.

The existence of the grand coalition on federal level helped smaller parties' electoral prospects in state elections. Since in 2008, the CSU lost its absolute majority in Bavaria and formed a coalition with the FDP, the grand coalition had no majority in the "Bundesrat" and depended on FDP votes on important issues. In November 2008, the SPD re-elected its already retired chair Franz Müntefering and made Frank-Walter Steinmeier its leading candidate for the federal election in September 2009.

As a result of that federal election, the grand coalition brought losses for both parties and came to an end. The SPD suffered the heaviest losses in its history and was unable to form a coalition government. The CDU/CSU had only little losses but also reached a new historic low with its worst result since 1949. The three smaller parties thus had more seats in the German "Bundestag" than ever before, with the liberal party FDP winning 14.6% of votes.

The CDU/CSU and FDP together held 332 seats (of 622 total seats) and had been in coalition since 27 October 2009. Angela Merkel was re-elected as chancellor, and Guido Westerwelle served as the foreign minister and vice chancellor of Germany. After being elected into the federal government, the FDP suffered heavy losses in the following state elections. The FDP had promised to lower taxes in the electoral campaign, but after being part of the coalition they had to concede that this was not possible due to the economic crisis of 2008. Because of the losses, Guido Westerwelle had to resign as chair of the FDP in favor of Philipp Rösler, Federal minister of health, who was consequently appointed as vice chancellor. Shortly after, Philipp Rösler changed office and became federal minister of economics and technology.

After their electoral fall, the Social Democrats were led by Sigmar Gabriel, a former federal minister and prime minister of Lower Saxony, and by Frank-Walter Steinmeier as the head of the parliamentary group. He resigned on 16 January 2017 and proposed his longtime friend and president of European Parliament Martin Schulz as his successor and chancellor candidate.
Germany has seen increased political activity by citizens outside the established political parties with respect to local and environmental issues such as the location of Stuttgart 21, a railway hub, and construction of Berlin Brandenburg Airport.

The 18th federal elections in Germany resulted in the re-election of Angela Merkel and her Christian democratic parliamentary group of the parties CDU and CSU, receiving 41.5% of all votes. Following Merkel's first two historically low results, her third campaign marked the CDU/CSU's best result since 1994 and only for the second time in German history the possibility of gaining an absolute majority. Their former coalition partner, the FDP, narrowly failed to reach the 5% threshold and did not gain seats in the Bundestag.

Not having reached an absolute majority, the CDU/CSU formed a grand coalition with the social-democratic SPD after the longest coalition talks in history, making the head of the party Sigmar Gabriel vice-chancellor and federal Minister for Economic Affairs and Energy. Together they held 504 of a total 631 seats (CDU/CSU 311 and SPD 193). The only two opposition parties were The Left (64 seats) and Alliance '90/The Greens (63 seats), which was acknowledged as creating a critical situation in which the opposition parties did not even have enough seats to use the special controlling powers of the opposition.

The 19th federal elections in Germany took place on 24 September 2017. The two big parties, the conservative parliamentary group CDU/CSU and the social democrat SPD were in a similar situation as in 2009, after the last grand coalition had ended, and both had suffered severe losses; reaching their second worst and worst result respectively in 2017. 

Many votes in the 2017 elections went to smaller parties, leading the right-wing populist party AfD (Alternative for Germany) into the Bundestag which marked a big shift in German politics since it was the first far-right party to win seats in parliament since the 1950s. 

With Merkel's candidacy for a fourth term, the CDU/CSU only reached 33.0% of the votes, but won the highest number of seats, leaving no realistic coalition option without the CDU/CSU. As all parties in the Bundestag strictly ruled out a coalition with the AfD, the only options for a majority coalition were a so-called "Jamaican" coalition (CDU/CSU, FDP, Greens; named after the party colors resembling those of the Jamaican flag) and a grand coalition with the SPD, which was at first opposed by the Social Democrats and their leader Martin Schulz. 

Coalition talks between the three parties of the "Jamaican" coalition were held but the final proposal was rejected by the liberals of the FDP, leaving the government in limbo. Following the unprecedented situation, for the first time in German history different minority coalitions or even direct snap coalitions were also heavily discussed. At this point, Federal President Steinmeier invited leaders of all parties for talks about a government, being the first President in the history of the Federal Republic to do so. 

Official coalition talks between CDU/CSU and SPD started in January 2018 and led to a renewal of the grand coalition on 12 March 2018 as well as the subsequent re-election of Angela Merkel as chancellor.

The "Basic Law for the Federal Republic of Germany" (Grundgesetz der Bundesrepublik Deutschland) is the Constitution of Germany. It was formally approved on 8 May 1949, and, with the signature of the Allies of World War II on 12 May, came into effect on 23 May, as the constitution of those states of West Germany that were initially included within the Federal Republic. The 1949 Basic Law is a response to the perceived flaws of the 1919 Weimar Constitution, which failed to prevent the rise of the Nazi party in 1933. Since 1990, in the course of the reunification process after the fall of the Berlin Wall, the Basic Law also applies to the eastern states of the former German Democratic Republic.

The German head of state is the Federal President. As in Germany's parliamentary system of government, the Federal Chancellor runs the government and day-to-day politics, the role of the Federal President is mostly ceremonial. The Federal President, by their actions and public appearances, represents the state itself, its existence, its legitimacy, and unity. Their office involves an integrative role. Nearly all actions of the Federal President become valid only after a countersignature of a government member.

The President is not obliged by Constitution to refrain from political views. He or she is expected to give direction to general political and societal debates, but not in a way that links him to party politics. Most German Presidents were active politicians and party members prior to the office, which means that they have to change their political style when becoming President. The function comprises the official residence of Bellevue Palace.

Under Article 59 (1) of the Basic Law, the Federal President represents the Federal Republic of Germany in matters of international law, concludes treaties with foreign states on its behalf and accredits diplomats.

All federal laws must be signed by the President before they can come into effect; he or she does not have a veto, but the conditions for refusing to sign a law on the basis of unconstitutionality are the subject of debate. The office is currently held by Frank-Walter Steinmeier (since 2017). 

The Federal President does have a role in the political system, especially at the establishment of a new government and the dissolution of the Bundestag (parliament). This role is usually nominal but can become significant in case of political instability. Additionally, a Federal President together with the Federal Council can support the government in a "legislatory emergency state" to enable laws against the will of the Bundestag (Article 81 of the Basic Law). However, until now the Federal President has never had to use these "reserve powers".

The "Bundeskanzler" (federal chancellor) heads the "Bundesregierung" (federal government) and thus the executive branch of the federal government. They are elected by and responsible to the "Bundestag", Germany's parliament. The other members of the government are the Federal Ministers; they are chosen by the Chancellor. Germany, like the United Kingdom, can thus be classified as a parliamentary system. The office is currently held by Angela Merkel (since 2005). 

The Chancellor cannot be removed from office during a four-year term unless the "Bundestag" has agreed on a successor. This constructive vote of no confidence is intended to avoid a similar situation to that of the Weimar Republic in which the executive did not have enough support in the legislature to govern effectively, but the legislature was too divided to name a successor. The current system also prevents the Chancellor from calling a snap election.

Except in the periods 1969–1972 and 1976–1982, when the Social Democratic party of Chancellor Brandt and Schmidt came in second in the elections, the chancellor has always been the candidate of the largest party, usually supported by a coalition of two parties with a majority in the parliament. The chancellor appoints one of the federal ministers as their deputy, who has the unofficial title Vice Chancellor (). The office is currently held by Olaf Scholz (since March 2018). 

The German Cabinet (Bundeskabinett or Bundesregierung) is the chief executive body of the Federal Republic of Germany. It consists of the chancellor and the cabinet ministers. The fundamentals of the cabinet's organization are set down in articles 62–69 of the Basic Law. The current cabinet is Merkel IV (since 2018). 
Agencies of the German government include:

Federal legislative power is divided between the "Bundestag" and the "Bundesrat". The "Bundestag" is directly elected by the German people, while the "Bundesrat" represents the governments of the regional states ("Länder"). The federal legislature has powers of exclusive jurisdiction and concurrent jurisdiction with the states in areas specified in the constitution.

The "Bundestag" is more powerful than the "Bundesrat" and only needs the latter's consent for proposed legislation related to revenue shared by the federal and state governments, and the imposition of responsibilities on the states. In practice, however, the agreement of the "Bundesrat" in the legislative process is often required, since federal legislation frequently has to be executed by state or local agencies. In the event of disagreement between the "Bundestag" and the "Bundesrat", either side can appeal to the to find a compromise.

The "Bundestag" (Federal Diet) is elected for a four-year term and consists of 598 or more members elected by a means of mixed-member proportional representation, which Germans call "personalised proportional representation". 299 members represent single-seat constituencies and are elected by a first past the post electoral system. Parties that obtain fewer constituency seats than their national share of the vote are allotted seats from party lists to make up the difference. In contrast, parties that obtain more constituency seats than their national share of the vote are allowed to keep these so-called overhang seats. In the parliament that was elected in 2009, there were 24 overhang seats, giving the "Bundestag" a total of 622 members. After Bundestag elections since 2013, other parties obtain extra seats ("balance seats") that offset advantages from their rival's overhang seats. The current "Bundestag" is the largest in German history with 709 members.

A party must receive either five percent of the national vote or win at least three directly elected seats to be eligible for non-constituency seats in the "Bundestag". This rule, often called the "five percent hurdle", was incorporated into Germany's election law to prevent political fragmentation and disproportionately influential minority parties. The first "Bundestag" elections were held in the Federal Republic of Germany ("West Germany") on 14 August 1949. Following reunification, elections for the first all-German "Bundestag" were held on 2 December 1990. The last federal election was held on 24 September 2017.

Germany follows the civil law tradition. The judicial system comprises three types of courts.


The main difference between the Federal Constitutional Court and the Federal Court of Justice is that the Federal Constitutional Court may only be called if a constitutional matter within a case is in question (e.g. a possible violation of human rights in a criminal trial), while the Federal Court of Justice may be called in any case.

Germany maintains a network of 229 diplomatic missions abroad and holds relations with more than 190 countries. It is the largest contributor to the budget of the European Union (providing 27%) and third largest contributor to the United Nations (providing 8%). Germany is a member of the NATO defence alliance, the Organisation of Economic Co-operation and Development (OECD), the G8, the G20, the World Bank and the International Monetary Fund (IMF).

Germany has played a leading role in the European Union since its inception and has maintained a strong alliance with France since the end of World War II. The alliance was especially close in the late 1980s and early 1990s under the leadership of Christian Democrat Helmut Kohl and Socialist François Mitterrand. Germany is at the forefront of European states seeking to advance the creation of a more unified European political, defence, and security apparatus. For a number of decades after WWII, the Federal Republic of Germany kept a notably low profile in international relations, because of both its recent history and its occupation by foreign powers.

During the Cold War, Germany's partition by the Iron Curtain made it a symbol of East–West tensions and a political battleground in Europe. However, Willy Brandt's "Ostpolitik" was a key factor in the "détente" of the 1970s. In 1999, Chancellor Gerhard Schröder's government defined a new basis for German foreign policy by taking a full part in the decisions surrounding the NATO war against Yugoslavia and by sending German troops into combat for the first time since World War II.

The governments of Germany and the United States are close political allies. The 1948 Marshall Plan and strong cultural ties have crafted a strong bond between the two countries, although Schröder's very vocal opposition to the Iraq War had suggested the end of Atlanticism and a relative cooling of German–American relations. The two countries are also economically interdependent: 5.0% of German exports in goods are US-bound and 3.5% of German imported goods originate from the US with a trade deficit of -63,678.5 million dollars for the United States (2017). Other signs of the close ties include the continuing position of German–Americans as the largest reported ethnic group in the US, and the status of Ramstein Air Base (near Kaiserslautern) as the largest US military community outside the US.

The policy on foreign aid is an important area of German foreign policy. It is formulated by the Federal Ministry for Economic Cooperation and Development (BMZ) and carried out by the implementing organisations. The German government sees development policy as a joint responsibility of the international community. It is the world's fourth biggest aid donor after the United States, the United Kingdom and France. Germany spent 0.37 per cent of its gross domestic product (GDP) on development, which is below the government's target of increasing aid to 0.51 per cent of GDP by 2010.

Germany comprises sixteen states that are collectively referred to as "Länder". Due to differences in size and population, the subdivision of these states varies especially between city-states ("Stadtstaaten") and states with larger territories ("Flächenländer"). For regional administrative purposes five states, namely Baden-Württemberg, Bavaria, Hesse, North Rhine-Westphalia and Saxony, consist of a total of 22 Government Districts ("Regierungsbezirke"). As of 2009 Germany is divided into 403 districts ("Kreise") on municipal level, these consist of 301 rural districts and 102 urban districts.




</doc>
<doc id="11953" url="https://en.wikipedia.org/wiki?curid=11953" title="History of geometry">
History of geometry

Geometry (from the ; "geo-" "earth", "-metron" "measurement") arose as the field of knowledge dealing with spatial relationships. Geometry was one of the two fields of pre-modern mathematics, the other being the study of numbers (arithmetic).

Classic geometry was focused in compass and straightedge constructions. Geometry was revolutionized by Euclid, who introduced mathematical rigor and the axiomatic method still in use today. His book, "The Elements" is widely considered the most influential textbook of all time, and was known to all educated people in the West until the middle of the 20th century.

In modern times, geometric concepts have been generalized to a high level of abstraction and complexity, and have been subjected to the methods of calculus and abstract algebra, so that many modern branches of the field are barely recognizable as the descendants of early geometry. (See Areas of mathematics and Algebraic geometry.)

The earliest recorded beginnings of geometry can be traced to early peoples, who discovered obtuse triangles in the ancient Indus Valley (see Harappan mathematics), and ancient Babylonia (see Babylonian mathematics) from around 3000 BC. Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. Among these were some surprisingly sophisticated principles, and a modern mathematician might be hard put to derive some of them without the use of calculus and algebra . For example, both the Egyptians and the Babylonians were aware of versions of the Pythagorean theorem about 1500 years before Pythagoras and the Indian Sulba Sutras around 800 BC contained the first statements of the theorem; the Egyptians had a correct formula for the volume of a frustum of a square pyramid;

The ancient Egyptians knew that they could approximate the area of a circle as follows:

Problem 30 of the Ahmes papyrus uses these methods to calculate the area of a circle, according to a rule that the area is equal to the square of 8/9 of the circle's diameter. This assumes that is 4×(8/9) (or 3.160493...), with an error of slightly over 0.63 percent. This value was slightly less accurate than the calculations of the Babylonians (25/8 = 3.125, within 0.53 percent), but was not otherwise surpassed until Archimedes' approximation of 211875/67441 = 3.14163, which had an error of just over 1 in 10,000.

Ahmes knew of the modern 22/7 as an approximation for , and used it to split a hekat, hekat x 22/x x 7/22 = hekat; however, Ahmes continued to use the traditional 256/81 value for for computing his hekat volume found in a cylinder.

Problem 48 involved using a square with side 9 units. This square was cut into a 3x3 grid. The diagonal of the corner squares were used to make an irregular octagon with an area of 63 units. This gave a second value for of 3.111...

The two problems together indicate a range of values for between 3.11 and 3.16.

Problem 14 in the Moscow Mathematical Papyrus gives the only ancient example finding the volume of a frustum of a pyramid, describing the correct formula:
where "a" and "b" are the base and top side lengths of the truncated pyramid and "h" is the height.

The Babylonians may have known the general rules for measuring areas and volumes. They measured the circumference of a circle as three times the diameter and the area as one-twelfth the square of the circumference, which would be correct if "π" is estimated as 3. The volume of a cylinder was taken as the product of the base and the height, however, the volume of the frustum of a cone or a square pyramid was incorrectly taken as the product of the height and half the sum of the bases. The Pythagorean theorem was also known to the Babylonians. Also, there was a recent discovery in which a tablet used "π" as 3 and 1/8. The Babylonians are also known for the Babylonian mile, which was a measure of distance equal to about seven miles today. This measurement for distances eventually was converted to a time-mile used for measuring the travel of the Sun, therefore, representing time. There have been recent discoveries showing that ancient Babylonians may have discovered astronomical geometry nearly 1400 years before Europeans did.

The Indian Vedic period had a tradition of geometry, mostly expressed in the construction of elaborate altars.
Early Indian texts (1st millennium BC) on this topic include the "Satapatha Brahmana" and the "Śulba Sūtras".
According to , the "Śulba Sūtras" contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians." The diagonal rope (') of an oblong (rectangle) produces both which the flank ("pārśvamāni") and the horizontal (') <ropes> produce separately." 
They contain lists of Pythagorean triples, which are particular cases of Diophantine equations.
They also contain statements (that with hindsight we know to be approximate) about squaring the circle and "circling the square."

The "Baudhayana Sulba Sutra", the best-known and oldest of the "Sulba Sutras" (dated to the 8th or 7th century BC) contains examples of simple Pythagorean triples, such as: formula_2, formula_3, formula_4, formula_5, and formula_6 as well as a statement of the Pythagorean theorem for the sides of a square: "The rope which is stretched across the diagonal of a square produces an area double the size of the original square." It also contains the general statement of the Pythagorean theorem (for the sides of a rectangle): "The rope stretched along the length of the diagonal of a rectangle makes an area which the vertical and horizontal sides make together."

According to mathematician S. G. Dani, the Babylonian cuneiform tablet Plimpton 322 written c. 1850 BC "contains fifteen Pythagorean triples with quite large entries, including (13500, 12709, 18541) which is a primitive triple, indicating, in particular, that there was sophisticated understanding on the topic" in Mesopotamia in 1850 BC. "Since these tablets predate the Sulbasutras period by several centuries, taking into account the contextual appearance of some of the triples, it is reasonable to expect that similar understanding would have been there in India." Dani goes on to say:

would not correspond directly to the overall knowledge on the topic at that time. Since, unfortunately, no other contemporaneous sources have been found it may never be possible to settle this issue satisfactorily."

In all, three "Sulba Sutras" were composed. The remaining two, the "Manava Sulba Sutra" composed by Manava (fl. 750-650 BC) and the "Apastamba Sulba Sutra", composed by Apastamba (c. 600 BC), contained results similar to the "Baudhayana Sulba Sutra".

For the ancient Greek mathematicians, geometry was the crown jewel of their sciences, reaching a completeness and perfection of methodology that no other branch of their knowledge had attained. They expanded the range of geometry to many new kinds of figures, curves, surfaces, and solids; they changed its methodology from trial-and-error to logical deduction; they recognized that geometry studies "eternal forms", or abstractions, of which physical objects are only approximations; and they developed the idea of the "axiomatic method", still in use today.

Thales (635-543 BC) of Miletus (now in southwestern Turkey), was the first to whom deduction in mathematics is attributed. There are five geometric propositions for which he wrote deductive proofs, though his proofs have not survived. Pythagoras (582-496 BC) of Ionia, and later, Italy, then colonized by Greeks, may have been a student of Thales, and traveled to Babylon and Egypt. The theorem that bears his name may not have been his discovery, but he was probably one of the first to give a deductive proof of it. He gathered a group of students around him to study mathematics, music, and philosophy, and together they discovered most of what high school students learn today in their geometry courses. In addition, they made the profound discovery of incommensurable lengths and irrational numbers.

Plato (427-347 BC) was a philosopher, highly esteemed by the Greeks. There is a story that he had inscribed above the entrance to his famous school, "Let none ignorant of geometry enter here." However, the story is considered to be untrue. Though he was not a mathematician himself, his views on mathematics had great influence. Mathematicians thus accepted his belief that geometry should use no tools but compass and straightedge – never measuring instruments such as a marked ruler or a protractor, because these were a workman's tools, not worthy of a scholar. This dictum led to a deep study of possible compass and straightedge constructions, and three classic construction problems: how to use these tools to trisect an angle, to construct a cube twice the volume of a given cube, and to construct a square equal in area to a given circle. The proofs of the impossibility of these constructions, finally achieved in the 19th century, led to important principles regarding the deep structure of the real number system. Aristotle (384-322 BC), Plato's greatest pupil, wrote a treatise on methods of reasoning used in deductive proofs (see Logic) which was not substantially improved upon until the 19th century.

Euclid (c. 325-265 BC), of Alexandria, probably a student at the Academy founded by Plato, wrote a treatise in 13 books (chapters), titled "The Elements of Geometry", in which he presented geometry in an ideal axiomatic form, which came to be known as Euclidean geometry. The treatise is not a compendium of all that the Hellenistic mathematicians knew at the time about geometry; Euclid himself wrote eight more advanced books on geometry. We know from other references that Euclid's was not the first elementary geometry textbook, but it was so much superior that the others fell into disuse and were lost. He was brought to the university at Alexandria by Ptolemy I, King of Egypt.

"The Elements" began with definitions of terms, fundamental geometric principles (called "axioms" or "postulates"), and general quantitative principles (called "common notions") from which all the rest of geometry could be logically deduced. Following are his five axioms, somewhat paraphrased to make the English easier to read.


Concepts, that are now understood as algebra, were expressed geometrically by Euclid, a method referred to as Greek geometric algebra.

Archimedes (287-212 BC), of Syracuse, Sicily, when it was a Greek city-state, is often considered to be the greatest of the Greek mathematicians, and occasionally even named as one of the three greatest of all time (along with Isaac Newton and Carl Friedrich Gauss). Had he not been a mathematician, he would still be remembered as a great physicist, engineer, and inventor. In his mathematics, he developed methods very similar to the coordinate systems of analytic geometry, and the limiting process of integral calculus. The only element lacking for the creation of these fields was an efficient algebraic notation in which to express his concepts.

After Archimedes, Hellenistic mathematics began to decline. There were a few minor stars yet to come, but the golden age of geometry was over. Proclus (410-485), author of "Commentary on the First Book of Euclid", was one of the last important players in Hellenistic geometry. He was a competent geometer, but more importantly, he was a superb commentator on the works that preceded him. Much of that work did not survive to modern times, and is known to us only through his commentary. The Roman Republic and Empire that succeeded and absorbed the Greek city-states produced excellent engineers, but no mathematicians of note.

The great Library of Alexandria was later burned. There is a growing consensus among historians that the Library of Alexandria likely suffered from several destructive events, but that the destruction of Alexandria's pagan temples in the late 4th century was probably the most severe and final one. The evidence for that destruction is the most definitive and secure. Caesar's invasion may well have led to the loss of some 40,000-70,000 scrolls in a warehouse adjacent to the port (as Luciano Canfora argues, they were likely copies produced by the Library intended for export), but it is unlikely to have affected the Library or Museum, given that there is ample evidence that both existed later.

Civil wars, decreasing investments in maintenance and acquisition of new scrolls and generally declining interest in non-religious pursuits likely contributed to a reduction in the body of material available in the Library, especially in the 4th century. The Serapeum was certainly destroyed by Theophilus in 391, and the Museum and Library may have fallen victim to the same campaign.

In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero." Aryabhata's "Aryabhatiya" (499) includes the computation of areas and volumes.

Brahmagupta wrote his astronomical work "" in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral:

Brahmagupta's theorem: If a cyclic quadrilateral has diagonals that are perpendicular to each other, then the perpendicular line drawn from the point of intersection of the diagonals to any side of the quadrilateral always bisects the opposite side.

Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles ("i.e." triangles with rational sides and rational areas).

Brahmagupta's formula: The area, "A", of a cyclic quadrilateral with sides of lengths "a", "b", "c", "d", respectively, is given by

where "s", the semiperimeter, given by: formula_8

Brahmagupta's Theorem on rational triangles: A triangle with rational sides formula_9 and rational area is of the form:

for some rational numbers formula_11 and formula_12.

The first definitive work (or at least oldest existent) on geometry in China was the "Mo Jing", the Mohist canon of the early philosopher Mozi (470-390 BC). It was compiled years after his death by his followers around the year 330 BC. Although the "Mo Jing" is the oldest existent book on geometry in China, there is the possibility that even older written material existed. However, due to the infamous Burning of the Books in a political maneuver by the Qin Dynasty ruler Qin Shihuang (r. 221-210 BC), multitudes of written literature created before his time were purged. In addition, the "Mo Jing" presents geometrical concepts in mathematics that are perhaps too advanced not to have had a previous geometrical base or mathematic background to work upon.

The "Mo Jing" described various aspects of many fields associated with physical science, and provided a small wealth of information on mathematics as well. It provided an 'atomic' definition of the geometric point, stating that a line is separated into parts, and the part which has no remaining parts (i.e. cannot be divided into smaller parts) and thus forms the extreme end of a line is a point. Much like Euclid's first and third definitions and Plato's 'beginning of a line', the "Mo Jing" stated that "a point may stand at the end (of a line) or at its beginning like a head-presentation in childbirth. (As to its invisibility) there is nothing similar to it." Similar to the atomists of Democritus, the "Mo Jing" stated that a point is the smallest unit, and cannot be cut in half, since 'nothing' cannot be halved. It stated that two lines of equal length will always finish at the same place, while providing definitions for the "comparison of lengths" and for "parallels", along with principles of space and bounded space. It also described the fact that planes without the quality of thickness cannot be piled up since they cannot mutually touch. The book provided definitions for circumference, diameter, and radius, along with the definition of volume.

The Han Dynasty (202 BC-220 AD) period of China witnessed a new flourishing of mathematics. One of the oldest Chinese mathematical texts to present geometric progressions was the "Suàn shù shū" of 186 BC, during the Western Han era. The mathematician, inventor, and astronomer Zhang Heng (78-139 AD) used geometrical formulas to solve mathematical problems. Although rough estimates for pi (π) were given in the "Zhou Li" (compiled in the 2nd century BC), it was Zhang Heng who was the first to make a concerted effort at creating a more accurate formula for pi. Zhang Heng approximated pi as 730/232 (or approx 3.1466), although he used another formula of pi in finding a spherical volume, using the square root of 10 (or approx 3.162) instead. Zu Chongzhi (429-500 AD) improved the accuracy of the approximation of pi to between 3.1415926 and 3.1415927, with ⁄ (密率, Milü, detailed approximation) and ⁄ (约率, Yuelü, rough approximation) being the other notable approximation. In comparison to later works, the formula for pi given by the French mathematician Franciscus Vieta (1540-1603) fell halfway between Zu's approximations.

"The Nine Chapters on the Mathematical Art", the title of which first appeared by 179 AD on a bronze inscription, was edited and commented on by the 3rd century mathematician Liu Hui from the Kingdom of Cao Wei. This book included many problems where geometry was applied, such as finding surface areas for squares and circles, the volumes of solids in various three-dimensional shapes, and included the use of the Pythagorean theorem. The book provided illustrated proof for the Pythagorean theorem, contained a written dialogue between of the earlier Duke of Zhou and Shang Gao on the properties of the right angle triangle and the Pythagorean theorem, while also referring to the astronomical gnomon, the circle and square, as well as measurements of heights and distances. The editor Liu Hui listed pi as 3.141014 by using a 192 sided polygon, and then calculated pi as 3.14159 using a 3072 sided polygon. This was more accurate than Liu Hui's contemporary Wang Fan, a mathematician and astronomer from Eastern Wu, would render pi as 3.1555 by using ⁄. Liu Hui also wrote of mathematical surveying to calculate distance measurements of depth, height, width, and surface area. In terms of solid geometry, he figured out that a wedge with rectangular base and both sides sloping could be broken down into a pyramid and a tetrahedral wedge. He also figured out that a wedge with trapezoid base and both sides sloping could be made to give two tetrahedral wedges separated by a pyramid. Furthermore, Liu Hui described Cavalieri's principle on volume, as well as Gaussian elimination. From the "Nine Chapters", it listed the following geometrical formulas that were known by the time of the Former Han Dynasty (202 BCE–9 CE).

Areas for the


Volumes for the


Continuing the geometrical legacy of ancient China, there were many later figures to come, including the famed astronomer and mathematician Shen Kuo (1031-1095 CE), Yang Hui (1238-1298) who discovered Pascal's Triangle, Xu Guangqi (1562-1633), and many others.

By the beginning of the 9th century, the "Islamic Golden Age" flourished, the establishment of the House of Wisdom in Baghdad marking a separate tradition of science in the medieval Islamic world, building not only Hellenistic but also on Indian sources.

Although the Islamic mathematicians are most famed for their work on algebra, number theory and number systems, they also made considerable contributions to geometry, trigonometry and mathematical astronomy, and were responsible for the development of algebraic geometry.

Al-Mahani (born 820) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra. Al-Karaji (born 953) completely freed algebra from geometrical operations and replaced them with the arithmetical type of operations which are at the core of algebra today.

Thābit ibn Qurra (known as Thebit in Latin) (born 836) contributed to a number of areas in mathematics, where he played an important role in preparing the way for such important mathematical discoveries as the extension of the concept of number to (positive) real numbers, integral calculus, theorems in spherical trigonometry, analytic geometry, and non-Euclidean geometry. In astronomy Thabit was one of the first reformers of the Ptolemaic system, and in mechanics he was a founder of statics. An important geometrical aspect of Thabit's work was his book on the composition of ratios. In this book, Thabit deals with arithmetical operations applied to ratios of geometrical quantities. The Greeks had dealt with geometric quantities but had not thought of them in the same way as numbers to which the usual rules of arithmetic could be applied. By introducing arithmetical operations on quantities previously regarded as geometric and non-numerical, Thabit started a trend which led eventually to the generalisation of the number concept.

In some respects, Thabit is critical of the ideas of Plato and Aristotle, particularly regarding motion. It would seem that here his ideas are based on an acceptance of using arguments concerning motion in his geometrical arguments. Another important contribution Thabit made to geometry was his generalization of the Pythagorean theorem, which he extended from special right triangles to all triangles in general, along with a general proof.

Ibrahim ibn Sinan ibn Thabit (born 908), who introduced a method of integration more general than that of Archimedes, and al-Quhi (born 940) were leading figures in a revival and continuation of Greek higher geometry in the Islamic world. These mathematicians, and in particular Ibn al-Haytham, studied optics and investigated the optical properties of mirrors made from conic sections.

Astronomy, time-keeping and geography provided other motivations for geometrical and trigonometrical research. For example, Ibrahim ibn Sinan and his grandfather Thabit ibn Qurra both studied curves required in the construction of sundials. Abu'l-Wafa and Abu Nasr Mansur both applied spherical geometry to astronomy.

A 2007 paper in the journal "Science" suggested that girih tiles possessed properties consistent with self-similar fractal quasicrystalline tilings such as the Penrose tilings.

The transmission of the Greek Classics to medieval Europe via the Arabic literature of the 9th to 10th century "Islamic Golden Age" began in the 10th century and culminated in the Latin translations of the 12th century.
A copy of Ptolemy's "Almagest" was brought back to Sicily by Henry Aristippus (d. 1162), as a gift from the Emperor to King William I (r. 1154–1166). An anonymous student at Salerno travelled to Sicily and translated the "Almagest" as well as several works by Euclid from Greek to Latin. Although the Sicilians generally translated directly from the Greek, when Greek texts were not available, they would translate from Arabic. Eugenius of Palermo (d. 1202) translated Ptolemy's "Optics" into Latin, drawing on his knowledge of all three languages in the task.
The rigorous deductive methods of geometry found in Euclid's "Elements of Geometry" were relearned, and further development of geometry in the styles of both Euclid (Euclidean geometry) and Khayyam (algebraic geometry) continued, resulting in an abundance of new theorems and concepts, many of them very profound and elegant.

Advances in the treatment of perspective were made in Renaissance art of the 14th to 15th century which went beyond what had been achieved in antiquity. 
In Renaissance architecture of the "Quattrocento", concepts of architectural order were explored and rules were formulated. A prime example of is the Basilica di San Lorenzo in Florence by Filippo Brunelleschi (1377–1446).

In c. 1413 Filippo Brunelleschi demonstrated the geometrical method of perspective, used today by artists, by painting the outlines of various Florentine buildings onto a mirror. 
Soon after, nearly every artist in Florence and in Italy used geometrical perspective in their paintings, notably Masolino da Panicale and Donatello. Melozzo da Forlì first used the technique of upward foreshortening (in Rome, Loreto, Forlì and others), and was celebrated for that. Not only was perspective a way of showing depth, it was also a new method of composing a painting. Paintings began to show a single, unified scene, rather than a combination of several.

As shown by the quick proliferation of accurate perspective paintings in Florence, Brunelleschi likely understood (with help from his friend the mathematician Toscanelli), but did not publish, the mathematics behind perspective. Decades later, his friend Leon Battista Alberti wrote "De pictura" (1435/1436), a treatise on proper methods of showing distance in painting based on Euclidean geometry. Alberti was also trained in the science of optics through the school of Padua and under the influence of Biagio Pelacani da Parma who studied Alhazen's "Optics'.

Piero della Francesca elaborated on Della Pittura in his "De Prospectiva Pingendi" in the 1470s. Alberti had limited himself to figures on the ground plane and giving an overall basis for perspective. Della Francesca fleshed it out, explicitly covering solids in any area of the picture plane. Della Francesca also started the now common practice of using illustrated figures to explain the mathematical concepts, making his treatise easier to understand than Alberti's. Della Francesca was also the first to accurately draw the Platonic solids as they would appear in perspective.

Perspective remained, for a while, the domain of Florence. Jan van Eyck, among others, was unable to create a consistent structure for the converging lines in paintings, as in London's The Arnolfini Portrait, because he was unaware of the theoretical breakthrough just then occurring in Italy. However he achieved very subtle effects by manipulations of scale in his interiors. Gradually, and partly through the movement of academies of the arts, the Italian techniques became part of the training of artists across Europe, and later other parts of the world.
The culmination of these Renaissance traditions finds its ultimate synthesis in the research of the architect, geometer, and optician Girard Desargues on perspective, optics and projective geometry.

The "Vitruvian Man" by Leonardo da Vinci(c. 1490) depicts a man in two superimposed positions with his arms and legs apart and inscribed in a circle and square. The drawing is based on the correlations of ideal human proportions with geometry described by the ancient Roman architect Vitruvius in Book III of his treatise "De Architectura".

In the early 17th century, there were two important developments in geometry. The first and most important was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry is the study of geometry without measurement, just the study of how points align with each other. There had been some early work in this area by Hellenistic geometers, notably Pappus (c. 340). The greatest flowering of the field occurred with Jean-Victor Poncelet (1788–1867).

In the late 17th century, calculus was developed independently and almost simultaneously by Isaac Newton (1642–1727) and Gottfried Wilhelm Leibniz (1646–1716). This was the beginning of a new field of mathematics now called analysis. Though not itself a branch of geometry, it is applicable to geometry, and it solved two families of problems that had long been almost intractable: finding tangent lines to odd curves, and finding areas enclosed by those curves. The methods of calculus reduced these problems mostly to straightforward matters of computation.

The very old problem of proving Euclid's Fifth Postulate, the "Parallel Postulate", from his first four postulates had never been forgotten. Beginning not long after Euclid, many attempted demonstrations were given, but all were later found to be faulty, through allowing into the reasoning some principle which itself had not been proved from the first four postulates. Though Omar Khayyám was also unsuccessful in proving the parallel postulate, his criticisms of Euclid's theories of parallels and his proof of properties of figures in non-Euclidean geometries contributed to the eventual development of non-Euclidean geometry. By 1700 a great deal had been discovered about what can be proved from the first four, and what the pitfalls were in attempting to prove the fifth. Saccheri, Lambert, and Legendre each did excellent work on the problem in the 18th century, but still fell short of success. In the early 19th century, Gauss, Johann Bolyai, and Lobatchewsky, each independently, took a different approach. Beginning to suspect that it was impossible to prove the Parallel Postulate, they set out to develop a self-consistent geometry in which that postulate was false. In this they were successful, thus creating the first non-Euclidean geometry. By 1854, Bernhard Riemann, a student of Gauss, had applied methods of calculus in a ground-breaking study of the intrinsic (self-contained) geometry of all smooth surfaces, and thereby found a different non-Euclidean geometry. This work of Riemann later became fundamental for Einstein's theory of relativity.

It remained to be proved mathematically that the non-Euclidean geometry was just as self-consistent as Euclidean geometry, and this was first accomplished by Beltrami in 1868. With this, non-Euclidean geometry was established on an equal mathematical footing with Euclidean geometry.

While it was now known that different geometric theories were mathematically possible, the question remained, "Which one of these theories is correct for our physical space?" The mathematical work revealed that this question must be answered by physical experimentation, not mathematical reasoning, and uncovered the reason why the experimentation must involve immense (interstellar, not earth-bound) distances. With the development of relativity theory in physics, this question became vastly more complicated.

All the work related to the Parallel Postulate revealed that it was quite difficult for a geometer to separate his logical reasoning from his intuitive understanding of physical space, and, moreover, revealed the critical importance of doing so. Careful examination had uncovered some logical inadequacies in Euclid's reasoning, and some unstated geometric principles to which Euclid sometimes appealed. This critique paralleled the crisis occurring in calculus and analysis regarding the meaning of infinite processes such as convergence and continuity. In geometry, there was a clear need for a new set of axioms, which would be complete, and which in no way relied on pictures we draw or on our intuition of space. Such axioms, now known as Hilbert's axioms, were given by David Hilbert in 1894 in his dissertation "Grundlagen der Geometrie" ("Foundations of Geometry"). Some other complete sets of axioms had been given a few years earlier, but did not match Hilbert's in economy, elegance, and similarity to Euclid's axioms.

In the mid-18th century, it became apparent that certain progressions of mathematical reasoning recurred when similar ideas were studied on the number line, in two dimensions, and in three dimensions. Thus the general concept of a metric space was created so that the reasoning could be done in more generality, and then applied to special cases. This method of studying calculus- and analysis-related concepts came to be known as analysis situs, and later as topology. The important topics in this field were properties of more general figures, such as connectedness and boundaries, rather than properties like straightness, and precise equality of length and angle measurements, which had been the focus of Euclidean and non-Euclidean geometry. Topology soon became a separate field of major importance, rather than a sub-field of geometry or analysis.

Developments in algebraic geometry included the study of curves and surfaces over finite fields as demonstrated by the works of among others André Weil, Alexander Grothendieck, and Jean-Pierre Serre as well as over the real or complex numbers. Finite geometry itself, the study of spaces with only finitely many points, found applications in coding theory and cryptography. With the advent of the computer, new disciplines such as computational geometry or digital geometry deal with geometric algorithms, discrete representations of geometric data, and so forth.






</doc>
<doc id="11955" url="https://en.wikipedia.org/wiki?curid=11955" title="George H. W. Bush">
George H. W. Bush

George Herbert Walker Bush (June 12, 1924November 30, 2018) was an American politician, diplomat, and businessman who served as the 41st president of the United States from 1989 to 1993. A leader of the Republican Party, Bush also served in the U.S. House of Representatives, as U.S. Ambassador to the United Nations, as Director of Central Intelligence, and as the 43rd vice president under Ronald Reagan.

Bush was raised in Greenwich, Connecticut and attended Phillips Academy before serving in the navy during World War II. After the war, he graduated from Yale and moved to West Texas, where he established a successful oil company. After an unsuccessful run for the United States Senate, he won election to the 7th congressional district of Texas in 1966. President Richard Nixon appointed Bush to the position of Ambassador to the United Nations in 1971 and to the position of chairman of the Republican National Committee in 1973. In 1974, President Gerald Ford appointed him as the Chief of the Liaison Office to the People's Republic of China, and in 1976 Bush became the Director of Central Intelligence. Bush ran for president in 1980, but was defeated in the Republican presidential primaries by Ronald Reagan. He was then elected vice president in 1980 and 1984 as Reagan's running mate.

In the 1988 presidential election, Bush defeated Democrat Michael Dukakis, becoming the first incumbent vice president to be elected president since Martin Van Buren in 1836. Foreign policy drove the Bush presidency, as he navigated the final years of the Cold War and played a key role in the reunification of Germany. Bush presided over the invasion of Panama and the Gulf War, ending the Iraqi occupation of Kuwait in the latter conflict. Though the agreement was not ratified until after he left office, Bush negotiated and signed the North American Free Trade Agreement (NAFTA), which created a trade bloc consisting of the United States, Canada, and Mexico. Domestically, Bush reneged on by signing a bill that increased taxes and helped reduce the federal budget deficit. He also signed the Americans with Disabilities Act of 1990 and appointed David Souter and Clarence Thomas to the Supreme Court. Bush lost the 1992 presidential election to Democrat Bill Clinton following an economic recession and the decreased emphasis of foreign policy in a post–Cold War political climate.

After leaving office in 1993, Bush was active in humanitarian activities, often working alongside Clinton, his former opponent. With the victory of his son, George W. Bush, in the 2000 presidential election, the two became the second father–son pair to serve as the nation's president, following John Adams and John Quincy Adams. Another son, Jeb Bush, unsuccessfully sought the Republican presidential nomination in the 2016 Republican primaries. After a long battle with vascular Parkinson's disease, Bush died at his home on November 30, 2018. Historians generally rank Bush as an above average president.

George Herbert Walker Bush was born in Milton, Massachusetts on June 12, 1924. He was the second son of Prescott Bush and Dorothy (Walker) Bush. His paternal grandfather, Samuel P. Bush, worked as an executive for a railroad parts company in Columbus, Ohio, and his maternal grandfather, George Herbert Walker, led Wall Street investment bank W. A. Harriman & Co. Bush was named after his maternal grandfather, who was known as "Pop", and young Bush was called "Poppy" as a tribute to his namesake. The Bush family moved to Greenwich, Connecticut in 1925, and Prescott took a position with W. A. Harriman & Co. (which later merged into Brown Brothers Harriman & Co.) the following year.

Bush spent most of his childhood in Greenwich, at the family vacation home in Kennebunkport, Maine, or at his maternal grandparents' plantation in South Carolina. Because of the family's wealth, Bush was largely unaffected by the Great Depression. He attended Greenwich Country Day School from 1929 to 1937 and Phillips Academy, an elite private academy in Massachusetts, from 1937 to 1942. While at Phillips Academy, he served as president of the senior class, secretary of the student council, president of the community fund-raising group, a member of the editorial board of the school newspaper, and captain of the varsity baseball and soccer teams.

On his 18th birthday, immediately after graduating from Phillips Academy, he enlisted in the United States Navy as a naval aviator. After a period of training, he was commissioned as an ensign in the Naval Reserve at Naval Air Station Corpus Christi on June 9, 1943, becoming one of the youngest aviators in the Navy. Beginning in 1944, Bush served in the Pacific theater, where he flew a Grumman TBF Avenger, a torpedo bomber capable of taking off from aircraft carriers. His squadron was assigned to the as a member of Air Group 51, where his lanky physique earned him the nickname "Skin".

Bush flew his first combat mission in May 1944, bombing Japanese-held Wake Island, and was promoted to lieutenant (junior grade) on August 1, 1944. During an attack on a Japanese installation in Chichijima, Bush's aircraft successfully attacked several targets, but was downed by enemy fire. Though both of Bush's fellow crew members died, Bush successfully bailed out from the aircraft and was rescued by the . Several of the aviators shot down during the attack were captured and executed, and their livers were eaten by their captors. Bush's near-death experience shaped him profoundly, leading him to ask, "Why had I been spared and what did God have for me?" He was later awarded the Distinguished Flying Cross for his role in the mission.

Bush returned to "San Jacinto" in November 1944, participating in operations in the Philippines. In early 1945, he was assigned to a new combat squadron, VT-153, where he trained to take part in an invasion of mainland Japan. On September 2, 1945, before any invasion took place, Japan formally surrendered following the atomic bombings of Hiroshima and Nagasaki. Bush was released from active duty that same month, but was not formally discharged from the Navy until October 1955, at which point he had reached the rank of lieutenant. By the end of his period of active service, Bush had flown 58 missions, completed 128 carrier landings, and recorded 1228 hours of flight time.

Bush met Barbara Pierce at a Christmas dance in Greenwich in December 1941, and, after a period of courtship, they became engaged in December 1943. While Bush was on leave from the navy, they married in Rye, New York, on January 6, 1945. The Bushes enjoyed a strong marriage, and Barbara would later be a popular First Lady, seen by many as "a kind of national grandmother". The marriage produced six children: George W. (b. 1946), Robin (b. 1949), Jeb (b. 1953), Neil (b. 1955), Marvin (b. 1956), and Doro (b. 1959). Their oldest daughter, Robin, died of leukemia in 1953.

Bush enrolled at Yale College, where he took part in an accelerated program that enabled him to graduate in two and a half years rather than the usual four. He was a member of the Delta Kappa Epsilon fraternity and was elected its president. He also captained the Yale baseball team and played in the first two College World Series as a left-handed first baseman. Like his father, he was a member of the Yale cheerleading squad and was initiated into the Skull and Bones secret society. He graduated Phi Beta Kappa in 1948 with a Bachelor of Arts degree, majoring in economics and minoring in sociology.

After graduating from Yale, Bush moved his young family to West Texas. Biographer Jon Meacham writes that Bush's relocation to Texas allowed him to move out of the "daily shadow of his Wall Street father and Grandfather Walker, two dominant figures in the financial world", but would still allow Bush to "call on their connections if he needed to raise capital." His first position in Texas was an oil field equipment salesman for Dresser Industries, which was led by family friend Neil Mallon. While working for Dresser, Bush lived in various places with his family: Odessa, Texas; Ventura, Bakersfield and Compton, California; and Midland, Texas. In 1952, he volunteered for the successful presidential campaign of Republican candidate Dwight D. Eisenhower. That same year, his father won election to represent Connecticut in the United States Senate as a member of the Republican Party.

With support from Mallon and Bush's uncle, George Herbert Walker Jr., Bush and John Overbey launched the Bush-Overbey Oil Development Company in 1951. In 1953 he co-founded the Zapata Petroleum Corporation, an oil company that drilled in the Permian Basin in Texas. In 1954, he was named president of the Zapata Offshore Company, a subsidiary which specialized in offshore drilling. Shortly after the subsidiary became independent in 1959, Bush moved the company and his family from Midland to Houston. In Houston, he befriended James Baker, a prominent attorney who later became an important political ally. Bush remained involved with Zapata until the mid-1960s, when he sold his stock in the company for approximately $1 million. In 1988, "The Nation" published an article alleging that Bush worked as an operative of the Central Intelligence Agency (CIA) during the 1960s; Bush denied this allegation.

By the early 1960s, Bush was widely regarded as an appealing political candidate, and some leading Democrats attempted to convince Bush to become a Democrat. He declined to leave the Republican Party, later citing his belief that the national Democratic Party favored "big, centralized government". The Democratic Party had historically dominated Texas, but Republicans scored their first major victory in the state with John G. Tower's victory in a 1961 special election to the United States Senate. Motivated by Tower's victory, and hoping to prevent the far-right John Birch Society from coming to power, Bush ran for the chairmanship of the Harris County, Texas Republican Party, winning election in February 1963. Like most other Texas Republicans, Bush supported conservative Senator Barry Goldwater over the more centrist Nelson Rockefeller in the 1964 Republican Party presidential primaries.

In 1964, Bush sought to unseat liberal Democrat Ralph W. Yarborough in Texas's U.S. Senate election. Bolstered by superior fundraising, Bush won the Republican primary by defeating former gubernatorial nominee Jack Cox in a run-off election. In the general election, Bush attacked Yarborough's vote for the Civil Rights Act of 1964, which banned racial and gender discrimination in public institutions and in many privately owned businesses. Bush argued that the act unconstitutionally expanded the powers of the federal government, but he was privately uncomfortable with the racial politics of opposing the act. He lost the election 56 percent to 44 percent, though he did run well ahead of Goldwater, the Republican presidential nominee. Despite the loss, the "New York Times" reported that Bush was "rated by political friend and foe alike as the Republicans' best prospect in Texas because of his attractive personal qualities and the strong campaign he put up for the Senate".

In 1966, Bush ran for the United States House of Representatives in Texas's 7th congressional district, a newly redistricted seat in the Greater Houston area. Initial polling showed him trailing his Democratic opponent, Harris County District Attorney Frank Briscoe, but he ultimately won the race with 57 percent of the vote. In an effort to woo potential candidates in the South and Southwest, House Republicans secured Bush an appointment to the powerful United States House Committee on Ways and Means, making Bush the first freshman to serve on the committee since 1904. His voting record in the House was generally conservative. He supported the Nixon administration's Vietnam policies, but broke with Republicans on the issue of birth control, which he supported. He also voted for the Civil Rights Act of 1968, although it was generally unpopular in his district. In 1968, Bush joined several other Republicans in issuing the party's Response to the State of the Union address; Bush's part of the address focused on a call for fiscal responsibility.

Though most other Texas Republicans supported Ronald Reagan in the 1968 Republican Party presidential primaries, Bush endorsed Richard Nixon, who went on to win the party's nomination. Nixon considered selecting Bush as his running mate in the 1968 presidential election, but he ultimately chose Spiro Agnew instead. Bush won re-election to the House unopposed, while Nixon defeated Hubert Humphrey in the presidential election. In 1970, with President Nixon's support, Bush gave up his seat in the House to run for the Senate against Yarborough. Bush easily won the Republican primary, but Yarborough was defeated by the more centrist Lloyd Bentsen in the Democratic primary. Ultimately, Bentsen defeated Bush, taking 53.5 percent of the vote.

After the 1970 Senate election, Bush accepted a position as a senior adviser to the president, but he convinced Nixon to instead appoint him as the U.S. Ambassador to the United Nations. The position represented Bush's first foray into foreign policy, as well as his first major experiences with the Soviet Union and the People's Republic of China, the two major U.S. rivals in the Cold War. During Bush's tenure, the Nixon administration pursued a policy of détente, seeking to ease tensions with both the Soviet Union and China. Bush's ambassadorship was marked by a defeat on the China question, as the United Nations General Assembly voted to expel the Republic of China and replace it with the People's Republic of China in October 1971. In the 1971 crisis in Pakistan, Bush supported an Indian motion at the UN General Assembly to condemn the Pakistani government of Yahya Khan for waging genocide in East Pakistan (modern Bangladesh), referring to the "tradition which we have supported that the human rights question transcended domestic jurisdiction and should be freely debated". Bush's support for India at the UN put him into conflict with Nixon who was supporting Pakistan, partly because Yahya Khan was a useful intermediary in his attempts to reach out to China and partly because the president was fond of Yahya Khan.

After Nixon won a landslide victory in the 1972 presidential election, he appointed Bush as chair of the Republican National Committee (RNC). In that position, he was charged with fundraising, candidate recruitment, and making appearances on behalf of the party in the media.

When Agnew was being investigated for corruption, Bush assisted, at the request of Nixon and Agnew, in pressuring John Glenn Beall Jr., the U.S. Senator from Maryland to force his brother, George Beall the U.S. Attorney in Maryland, who was supervising the investigation into Agnew. Attorney Beall ignored the pressure.

During Bush's tenure at the RNC, the Watergate scandal emerged into public view; the scandal originated from the June 1972 break-in of the Democratic National Committee, but also involved later efforts to cover up the break-in by Nixon and other members of the White House. Bush initially defended Nixon steadfastly, but as Nixon's complicity became clear he focused more on defending the Republican Party.

Following the resignation of Vice President Agnew in 1973 for a scandal unrelated to Watergate, Bush was considered for the position of vice president, but the appointment instead went to Gerald Ford. After the public release of an audio recording that confirmed that Nixon had plotted to use the CIA to cover up the Watergate break-in, Bush joined other party leaders in urging Nixon to resign. When Nixon resigned on August 9, 1974, Bush noted in his diary that "There was an aura of sadness, like somebody died... The [resignation] speech was vintage Nixon—a kick or two at the press—enormous strains. One couldn't help but look at the family and the whole thing and think of his accomplishments and then think of the shame... [President Gerald Ford's swearing-in offered] indeed a new spirit, a new lift."

Upon his ascension to the presidency, Ford strongly considered Bush, Donald Rumsfeld, and Nelson Rockefeller for the vacant position of vice president. Ford ultimately chose Nelson Rockefeller, partly because of the publication of a news report claiming that Bush's 1970 campaign had benefited from a secret fund set up by Nixon; Bush was later cleared of any suspicion by a special prosecutor. Bush accepted appointment as Chief of the U.S. Liaison Office in the People's Republic of China, making the him the de facto ambassador to China. According to biographer Jon Meacham, Bush's time in China convinced him that American engagement abroad was needed to ensure global stability, and that the United States "needed to be visible but not pushy, muscular but not domineering."

In January 1976, Ford brought Bush back to Washington to become the Director of Central Intelligence (DCI), placing him in charge of the CIA. In the aftermath of the Watergate scandal and the Vietnam War, the CIA's reputation had been damaged for its role in various covert operations, and Bush was tasked with restoring the agency's morale and public reputation. During Bush's year in charge of the CIA, the U.S. national security apparatus actively supported Operation Condor operations and right-wing military dictatorships in Latin America. Meanwhile, Ford decided to drop Rockefeller from the ticket for the 1976 presidential election; he considered Bush as his running mate, but ultimately chose Bob Dole. In his capacity as DCI, Bush gave national security briefings to Jimmy Carter both as a presidential candidate and as president-elect.

Bush's tenure at the CIA ended after Carter narrowly defeated Ford in the 1976 presidential election. Out of public office for the first time since the 1960s, Bush became chairman on the Executive Committee of the First International Bank in Houston. He also spent a year as a part-time professor of Administrative Science at Rice University's Jones School of Business, continued his membership in the Council on Foreign Relations, and joined the Trilateral Commission. Meanwhile, he began to lay the groundwork for his candidacy in the 1980 Republican Party presidential primaries. In the 1980 Republican primary campaign, Bush faced Ronald Reagan, who was widely regarded as the front-runner, as well as other contenders like Senator Bob Dole, Senator Howard Baker, Texas Governor John Connally, Congressman Phil Crane, and Congressman John B. Anderson.
Bush's campaign cast him as a youthful, "thinking man's candidate" who would emulate the pragmatic conservatism of President Eisenhower. In the midst of the Soviet–Afghan War, which brought an end to a period of détente, and the Iran hostage crisis, in which 52 Americans were taken hostage, the campaign highlighted Bush's foreign policy experience. At the outset of the race, Bush focused heavily on winning the January 21 Iowa caucuses, making 31 visits to the state. He won a close victory Iowa with 31.5% to Reagan's 29.4%. After the win, Bush stated that his campaign was full of momentum, or "the Big Mo", and Reagan reorganized his campaign. Partly in response to the Bush campaign's frequent questioning of Reagan's age (Reagan turned 69 in 1980), the Reagan campaign stepped up attacks on Bush, painting him as an elitist who was not truly committed to conservatism. Prior to the New Hampshire primary, Bush and Reagan agreed to a two-person debate, organized by "The Nashua Telegraph" but paid for by the Reagan campaign.

Days before the debate, Reagan announced that he would invite four other candidates to the debate; Bush, who had hoped that the one-on-one debate would allow him to emerge as the main alternative to Reagan in the primaries, refused to debate the other candidate. All six candidates took the stage, but Bush refused to speak in the presence of the other candidates. Ultimately, the other four candidates left the stage and the debate continued, but Bush's refusal to debate anyone other than Reagan badly damaged his campaign in New Hampshire. He ended up decisively losing New Hampshire's primary to Reagan, winning just 23 percent of the vote. Bush revitalized his campaign with a victory in Massachusetts, but lost the next several primaries. As Reagan built up a commanding delegate lead, Bush refused to end his campaign, but the other candidates dropped out of the race. Criticizing his more conservative rival's policy proposals, Bush famously labeled Reagan's supply side-influenced plans for massive tax cuts as "voodoo economics". Though he favored lower taxes, Bush feared that dramatic reductions in taxation would lead to deficits and, in turn, cause inflation.

After Reagan clinched a majority of delegates in late May, Bush reluctantly dropped out of the race. At the 1980 Republican National Convention, Reagan made the last-minute decision to select Bush as his vice presidential nominee after negotiations with Ford regarding a Reagan-Ford ticket collapsed. Though Reagan had resented many of the Bush campaign's attacks during the primary campaign, and several conservative leaders had actively opposed Bush's nomination, Reagan ultimately decided that Bush's popularity with moderate Republicans made him the best and safest pick. Bush, who had believed his political career might be over following the primaries, eagerly accepted the position and threw himself into campaigning for the Reagan-Bush ticket. The 1980 general election campaign between Reagan and Carter was conducted amid a multitude of domestic concerns and the ongoing Iran hostage crisis, and Reagan sought to focus the race on Carter's handling of the economy. Though the race was widely regarded as a close contest for most of the campaign, Reagan ultimately won over the large majority of undecided voters. Reagan took 50.7 percent of the popular vote and 489 of the 538 electoral votes, while Carter won 41% of the popular vote and John Anderson, running as an independent candidate, won 6.6% of the popular vote.

As vice president, Bush generally maintained a low profile, recognizing the constitutional limits of the office; he avoided decision-making or criticizing Reagan in any way. This approach helped him earn Reagan's trust, easing tensions left over from their earlier rivalry. Bush also generally enjoyed a good relationship with Reagan staffers, including his close friend Jim Baker, who served as Reagan's initial chief of staff. His understanding of the vice presidency was heavily influenced by Vice President Walter Mondale, who enjoyed a strong relationship with President Carter in part because of his ability to avoid confrontations with senior staff and Cabinet members, and by Vice President Nelson Rockefeller's difficult relationship with some members of the White House staff during the Ford administration. The Bushes attended a large number of public and ceremonial events in their positions, including many state funerals, which became a common joke for comedians. As the President of the Senate, Bush also stayed in contact with members of Congress and kept the president informed on occurrences on Capitol Hill.

On March 30, 1981, while Bush was in Texas, Reagan was shot and seriously wounded by John Hinckley Jr. Bush immediately flew back from Washington D.C.; when his plane landed, his aides advised him to proceed directly to the White House by helicopter in order to show that the government was still functioning. Bush rejected the idea, as he feared that such a dramatic scene risked giving the impression that he sought to usurp Reagan's powers and prerogatives. During Reagan's short period of incapacity, Bush presided over Cabinet meetings, met with congressional leaders and foreign leaders, and briefed reporters, but he consistently rejected the possibility of invoking the Twenty-fifth Amendment. Bush's handling of the attempted assassination and its aftermath made a positive impression on Reagan, who recovered and returned to work within two weeks of the shooting. From then on, the two men would have regular Thursday lunches in the Oval Office.

Bush was assigned by Reagan to chair two special task forces, one on deregulation and one on international drug smuggling. Both were popular issues with conservatives, and Bush, largely a moderate, began courting them through his work. The deregulation task force reviewed hundreds of rules, making specific recommendations on which ones to amend or revise, in order to curb the size of the federal government. The Reagan administration's deregulation push had a strong impact on broadcasting, finance, resource extraction, and other economic activities, and the administration eliminated numerous government positions. Bush also oversaw the administration's national security crisis management organization, which had traditionally been the responsibility of the National Security Advisor. In 1983, Bush toured Western Europe as part of the Reagan administration's ultimately successful efforts to convince skeptical NATO allies to support the deployment of Pershing II missiles.

Reagan's approval ratings fell after his first year in office, but they bounced back when the United States began to emerge from recession in 1983. Former Vice President Walter Mondale was nominated by the Democratic Party in the 1984 presidential election. Down in the polls, Mondale selected Congresswoman Geraldine Ferraro as his running mate in hopes of galvanizing support for his campaign, thus making Ferraro the first female major party vice presidential nominee in U.S. history. She and Bush squared off in a single televised vice presidential debate. Public opinion polling consistently showed a Reagan lead in the 1984 campaign, and Mondale was unable to shake up the race. In the end, Reagan won re-election, winning 49 of 50 states and receiving 59% of the popular vote to Mondale's 41%.

Mikhail Gorbachev came to power in the Soviet Union in 1985; less ideologically rigid than his predecessors, Gorbachev believed that the Soviet Union urgently needed economic and political reforms. At the 1987 Washington Summit, Gorbachev and Reagan signed the Intermediate-Range Nuclear Forces Treaty, which committed both signatories to the total abolition of their respective short-range and medium-range missile stockpiles. The treaty marked the beginning of a new era of trade, openness, and cooperation between the two powers. Though President Reagan and Secretary of State George Shultz took the lead in these negotiations, Bush sat in on many meetings and promised Gorbachev that he would seek to continue improving Soviet-U.S. relations if he succeeded Reagan. On July 13, 1985, Bush became the first vice president to serve as acting president when Reagan underwent surgery to remove polyps from his colon; Bush served as the acting president for approximately eight hours.

In 1986, the Reagan administration was shaken by a scandal when it was revealed that administration officials had secretly arranged weapon sales to Iran during the Iran–Iraq War. The officials had used the proceeds to fund the anti-communist Contras in Nicaragua, which was a direct violation of law. When news of affair broke to the media, Bush, like Reagan, stated that he had been "out of the loop" and unaware of the diversion of funds, although this has assertion has since been challenged. Biographer Jon Meacham writes that "no evidence was ever produced proving Bush was aware of the diversion to the contras," but he criticizes Bush's "out of the loop" characterization, writing that the "record is clear that Bush was aware that the United States, in contravention of its own stated policy, was trading arms for hostages". The Iran–Contra scandal, as it became known, did serious damage to the Reagan presidency, raising questions about Reagan's competency. Congress established the Tower Commission to investigate the scandal, and, at Reagan's request, a panel of federal judges appointed Lawrence Walsh as a special prosecutor charged with investigating the Iran–Contra scandal. The investigations continued after Reagan left office and, though Bush was never charged with a crime, the Iran–Contra scandal would remain a political liability for him.

Bush began planning for a presidential run after the 1984 election, and he officially entered the 1988 Republican Party presidential primaries in October 1987. He put together a campaign led by Reagan staffer Lee Atwater, and which also included his son, George W. Bush, and media consultant Roger Ailes. Though he had moved to the right during his time as vice president, endorsing a Human Life Amendment and repudiating his earlier comments on "voodoo economics," Bush still faced opposition from many conservatives in the Republican Party. His major rivals for the Republican nomination were Senate Minority Leader Bob Dole of Kansas, Congressman Jack Kemp of New York, and Christian televangelist Pat Robertson. Reagan did not publicly endorse any candidate, but he privately expressed support for Bush.

Though considered the early front-runner for the nomination, Bush came in third in the Iowa caucus, behind Dole and Robertson. Much as Reagan had done in 1980, Bush reorganized his staff and concentrated on the New Hampshire primary. With help from Governor John H. Sununu and an effective campaign attacking Dole for raising taxes, Bush overcame an initial polling deficit and won New Hampshire with 39 percent of the vote. After Bush won South Carolina and 16 of the 17 states holding a primary on Super Tuesday, his competitors dropped out of the race.

Bush, occasionally criticized for his lack of eloquence when compared to Reagan, delivered a well-received speech at the Republican convention. Known as the "thousand points of light" speech, it described Bush's vision of America: he endorsed the Pledge of Allegiance, prayer in schools, capital punishment, and gun rights. Bush also , stating: "Congress will push me to raise taxes, and I'll say no, and they'll push, and I'll say no, and they'll push again. And all I can say to them is: read my lips. No new taxes." Bush selected little-known Senator Dan Quayle of Indiana as his running mate. Though Quayle had compiled an unremarkable record in Congress, he was popular among many conservatives, and the campaign hoped that Quayle's youth would appeal to younger voters.

Meanwhile, the Democratic Party nominated Governor Michael Dukakis, who was known for presiding over an economic turnaround in Massachusetts. Leading in the general election polls against Bush, Dukakis ran an ineffective, low-risk campaign. The Bush campaign attacked Dukakis as an unpatriotic liberal extremist and seized on Dukakis's pardon of Willie Horton, a convicted felon from Massachusetts who had raped a woman while on a prison furlough. The Bush campaign charged that Dukakis presided over a "revolving door" that allowed dangerous convicted felons to leave prison. Dukakis damaged his own campaign with a widely mocked ride in an M1 Abrams tank and a poor performance at the second presidential debate. Bush also attacked Dukakis for opposing a law that would require all students to recite the Pledge of Allegiance. The election is widely considered to have had a high level of negative campaigning, though political scientist John Geer has argued that the share of negative ads was in line with previous presidential elections.

Bush defeated Dukakis by a margin of 426 to 111 in the Electoral College, and he took 53.4 percent of the national popular vote. Bush ran well in all the major regions of the country, but especially in the South. He became the first sitting vice president to be elected president since Martin Van Buren in 1836, the first person to succeed a president from his own party via election since Herbert Hoover in 1929. In the concurrent congressional elections, Democrats retained control of both houses of Congress.

Bush was inaugurated on January 20, 1989, succeeding Ronald Reagan. In his inaugural address, Bush said:

Bush's first major appointment was that of James Baker as Secretary of State. Leadership of the Department of Defense went to Dick Cheney, who had had previously served as Gerald Ford's chief of staff and would later serve as vice president under George W. Bush. Jack Kemp joined the administration as Secretary of Housing and Urban Development, while Elizabeth Dole, the wife of Bob Dole and a former Secretary of Transportation, became the Secretary of Labor under Bush. Bush retained several Reagan officials, including Secretary of the Treasury Nicholas F. Brady, Attorney General Dick Thornburgh, and Secretary of Education Lauro Cavazos. New Hampshire Governor John Sununu, a strong supporter of Bush during the 1988 campaign, became chief of staff. Brent Scowcroft was appointed as the National Security Advisor, a role he had also held under Ford.

During the first year of his tenure, Bush put a pause on Reagan's détente policy toward the USSR. Bush and his advisers were initially divided on Gorbachev; some administration officials saw him as a democratic reformer, but others suspected him of trying to make the minimum changes necessary to restore the Soviet Union to a competitive position with the United States. In 1989,all the Communist governments collapsed in Eastern Europe. Gorbachev declined to send in the Soviet military, effectively abandoning the Brezhnev Doctrine. The U.S. was not directly involved in these upheavals, but the Bush administration avoided gloating over the demise of the Eastern Bloc to avoid undermining further democratic reforms. 

Bush and Gorbachev met at the Malta Summit in December 1989. Though many on the right remained wary of Gorbachev, Bush came away with the belief that Gorbachev would negotiate in good faith. For the remainder of his term, Bush sought cooperative relations with Gorbachev, believing that he was the key to peace. The primary issue at the Malta Summit was the potential reunification of Germany. While Britain and France were wary of a re-unified Germany, Bush joined West German Chancellor Helmut Kohl in pushing for German reunification. Bush believed that a reunified Germany would serve U.S. interests, but he also saw reunification as providing a final symbolic end to World War II. After extensive negotiations, Gorbachev agreed to allow a reunified Germany to be a part of NATO, and Germany officially reunified in October 1990.

Though Gorbachev acquiesced to the democratization of Soviet satellite states, he suppressed nationalist movements within the Soviet Union itself. A crisis in Lithuania left Bush in a difficult position, as he needed Gorbachev's cooperation in the reunification of Germany and feared that the collapse of the Soviet Union could leave nuclear arms in dangerous hands. The Bush administration mildly protested Gorbachev's suppression of Lithuania's independence movement, but took no action to directly intervene. Bush warned independence movements of the disorder that could come with secession from the Soviet Union; in a 1991 address that critics labeled the "Chicken Kiev speech", he cautioned against "suicidal nationalism". In July 1991, Bush and Gorbachev signed the Strategic Arms Reduction Treaty (START I) treaty, in which both countries agreed to cut their strategic nuclear weapons by 30 percent.

In August 1991, hard-line Communists launched a coup against Gorbachev; while the coup quickly fell apart, it broke the remaining power of Gorbachev and the central Soviet government. Later that month, Gorbachev resigned as general secretary of the Communist party, and Russian president Boris Yeltsin ordered the seizure of Soviet property. Gorbachev clung to power as the President of the Soviet Union until December 1991, when the Soviet Union dissolved. Fifteen states emerged from the Soviet Union, and of those states, Russia was the largest and most populous. Bush and Yeltsin met in February 1992, declaring a new era of "friendship and partnership". In January 1993, Bush and Yeltsin agreed to START II, which provided for further nuclear arms reductions on top of the original START treaty. The collapse of the Soviet Union prompted reflections on the future of the world following the end of the Cold War; one political scientist, Francis Fukuyama, speculated that humanity had reached the "end of history" in that liberal, capitalist democracy had permanently triumphed over Communism and fascism. Meanwhile, the collapse of the Soviet Union and other Communist governments led to post-Soviet conflicts in Central Europe, Eastern Europe, Central Asia, and Africa that would continue long after Bush left office.

During the 1980s, the U.S. had provided aid to Panamanian leader Manuel Noriega, an anti-Communist dictator who engaged in drug trafficking. In May 1989, Noriega annulled the results of a democratic presidential election in which Guillermo Endara had been elected. Bush objected to the annulment of the election and worried about the status of the Panama Canal with Noriega still in office. Bush dispatched 2,000 soldiers to the country, where they began conducting regular military exercises in violation of prior treaties. After a U.S. serviceman was shot by Panamanian forces in December 1989, Bush ordered the United States invasion of Panama, known as "Operation Just Cause". The invasion was the first large-scale American military operation in more than 40 years that was not related to the Cold War. American forces quickly took control of the Panama Canal Zone and Panama City. Noriega surrendered on January 3, 1990, and was quickly transported to a prison in the United States. Twenty-three Americans died in the operation, while another 394 were wounded. Noriega was convicted and imprisoned on racketeering and drug trafficking charges in April 1992. Historian Stewart Brewer argues that the invasion "represented a new era in American foreign policy" because Bush did not justify the invasion under the Monroe Doctrine or the threat of Communism, but rather on the grounds that it was in the best interests of the United States.

Faced with massive debts and low oil prices in the aftermath of the Iran–Iraq War, Iraqi leader Saddam Hussein decided to conquer the country of Kuwait, a small, oil-rich country situated on Iraq's southern border. After Iraq invaded Kuwait in August 1990, Bush imposed economic sanctions on Iraq and assembled a multi-national coalition opposed to the invasion. The administration feared that a failure to respond to the invasion would embolden Hussein to attack Saudi Arabia or Israel, and wanted to discourage other countries from similar aggression. Bush also wanted to ensure continued access to oil, as Iraq and Kuwait collectively accounted for 20 percent of the world's oil production, and Saudi Arabia produced another 26 percent of the world's oil supply.

At Bush's insistence, in November 1990, the United Nations Security Council approved a resolution authorizing the use of force if Iraq did not withdrawal from Kuwait by January 15, 1991. Gorbachev's support, as well as China's abstention, helped ensure passage of the UN resolution. Bush convinced Britain, France, and other nations to commit soldiers to an operation against Iraq, and he won important financial backing from Germany, Japan, South Korea, Saudi Arabia, and the United Arab Emirates. In January 1991, Bush asked Congress to approve a joint resolution authorizing a war against Iraq. Bush believed that the UN resolution had already provided him with the necessary authorization to launch a military operation against Iraq, but he wanted to show that the nation was united behind a military action. Despite the opposition of a majority of Democrats in both the House and the Senate, Congress approved the Authorization for Use of Military Force Against Iraq Resolution of 1991.

After the January 15 deadline passed without an Iraqi withdrawal from Kuwait, U.S. and coalition forces began a conducted a bombing campaign that devastated Iraq's power grid and communications network, and resulted in the desertion of about 100,000 Iraqi soldiers. In retaliation, Iraq launched Scud missiles at Israel and Saudi Arabia, but most of the missiles did little damage. On February 23, coalition forces began a ground invasion into Kuwait, evicting Iraqi forces by the end of February 27. About 300 Americans, as well as approximately 65 soldiers from other coalition nations, died during the military action. A cease fire was arranged on March 3, and the UN passed a resolution establishing a peacekeeping force in a demilitarized zone between Kuwait and Iraq. A March 1991 Gallup poll showed that Bush had an approval rating of 89 percent, the highest presidential approval rating in the history of Gallup polling. After 1991, the UN maintained economic sanctions against Iraq, and the United Nations Special Commission was assigned to ensure that Iraq did not revive its weapons of mass destruction program.

In 1987, the U.S. and Canada had reached a free trade agreement that eliminated many tariffs between the two countries. President Reagan had intended it as the first step towards a larger trade agreement to eliminate most tariffs among the United States, Canada, and Mexico. The Bush administration, along with the Progressive Conservative Canadian Prime Minister Brian Mulroney, spearheaded the negotiations of the North American Free Trade Agreement (NAFTA) with Mexico. In addition to lowering tariffs, the proposed treaty would affected patents, copyrights, and trademarks. In 1991, Bush sought fast track authority, which grants the president the power to submit an international trade agreement to Congress without the possibility of amendment. Despite congressional opposition led by House Majority Leader Dick Gephardt, both houses of Congress voted to grant Bush fast track authority. NAFTA was signed in December 1992, after Bush lost re-election, but President Clinton won ratification of NAFTA in 1993. NAFTA remains controversial for its impact on wages, jobs, and overall economic growth.

The U.S. economy had generally performed well since emerging from recession in late 1982, but it slipped into a mild recession in 1990. The unemployment rate rose from 5.9 percent in 1989 to a high of 7.8 percent in mid-1991. Large federal deficits, spawned during the Reagan years, rose from $152.1 billion in 1989 to $220 billion for 1990; the $220 billion deficit represented a threefold increase since 1980. As the public became increasingly concerned about the economy and other domestic affairs, Bush's well-received handling of foreign affairs became less of an issue for most voters. Bush's top domestic priority was to bring an end to federal budget deficits, which he saw as a liability for the country's long-term economic health and standing in the world. As he was opposed to major defense spending cuts and had pledged to not raise taxes, the president had major difficulties in balancing the budget.

Bush and congressional leaders agreed to avoid major changes to the budget for fiscal year 1990, which began in October 1989. However, both sides knew that spending cuts or new taxes would be necessary in the following year's budget in order to avoid the draconian automatic domestic spending cuts required by the Gramm–Rudman–Hollings Balanced Budget Act of 1987. Bush and other leaders also wanted to cut deficits because Federal Reserve Chair Alan Greenspan refused to lower interest rates, and thus stimulate economic growth, unless the federal budget deficit was reduced. In a statement released in late June 1990, Bush said that he would be open to a deficit reduction program which included spending cuts, incentives for economic growth, budget process reform, as well as tax increases. To fiscal conservatives in the Republican Party, Bush's statement represented a betrayal, and they heavily criticized him for compromising so early in the negotiations.

In September 1990, Bush and Congressional Democrats announced a compromise to cut funding for mandatory and discretionary programs while also raising revenue, partly through a higher gas tax. The compromise additionally included a "pay as you go" provision that required that new programs be paid for at the time of implementation. House Minority Whip Newt Gingrich led the conservative opposition to the bill, strongly opposing any form of tax increase. Some liberals also criticized the budget cuts in the compromise, and in October, the House rejected the deal, resulting in a brief government shutdown. Without the strong backing of the Republican Party, Bush agreed to another compromise bill, this one more favorable to Democrats. The Omnibus Budget Reconciliation Act of 1990 (OBRA-90), enacted on October 27, 1990, dropped much of the gasoline tax increase in favor of higher income taxes on top earners. It included cuts to domestic spending, but the cuts were not as deep as those that had been proposed in the original compromise. Bush's decision to sign the bill damaged his standing with conservatives and the general public, but it also laid the groundwork for the budget surpluses of the late 1990s.

The disabled had not received legal protections under the landmark Civil Rights Act of 1964, and many faced discrimination and segregation by the time Bush took office. In 1988, Lowell P. Weicker Jr. and Tony Coelho had introduced the Americans with Disabilities Act, which barred employment discrimination against qualified individuals with disabilities. The bill had passed the Senate but not the House, and it was reintroduced in 1989. Though some conservatives opposed the bill due to its costs and potential burdens on businesses, Bush strongly supported it, partly because his son, Neil, had struggled with dyslexia. After the bill passed both houses of Congress, Bush signed the Americans with Disabilities Act of 1990 into law in July 1990. The act required employers and public accommodations to make "reasonable accommodations" for the disabled, while providing an exception when such accommodations imposed an "undue hardship".

Senator Ted Kennedy later led the congressional passage of a separate civil rights bill designed to facilitate launching employment discrimination lawsuits. In vetoing the bill, Bush argued that it would lead to racial quotas in hiring. In November 1991, Bush signed the Civil Rights Act of 1991, which was largely similar to the bill he had vetoed in the previous year.

In August 1990, Bush signed the Ryan White CARE Act, the largest federally funded program dedicated to assisting persons living with HIV/AIDS. Throughout his presidency, the AIDS epidemic grew dramatically in the U.S. and around the world, and Bush often found himself at odds with AIDS activist groups who criticized him for not placing a high priority on HIV/AIDS research and funding. Frustrated by the administration's lack of urgency on the issue, ACT UP, dumped the ashes of HIV/AIDS victims on the White House lawn during a viewing of the AIDS Quilt in 1992. By that time, HIV had become the leading cause of death in the U.S. for men aged 25–44.

In June 1989, the Bush administration proposed a bill to amend the Clean Air Act. Working with Senate Majority Leader George J. Mitchell, the administration won passage of the amendments over the opposition of business-aligned members of Congress who feared the impact of tougher regulations. The legislation sought to curb acid rain and smog by requiring decreased emissions of chemicals such as sulfur dioxide, and was the first major update to the Clean Air Act since 1977. Bush also signed the Oil Pollution Act of 1990 in response to the Exxon Valdez oil spill. However, the League of Conservation Voters criticized some of Bush's other environmental actions, including his opposition to stricter auto-mileage standards.

President Bush devoted attention to voluntary service as a means of solving some of America's most serious social problems. He often used the "thousand points of light" theme to describe the power of citizens to solve community problems. In his 1989 inaugural address, President Bush said, "I have spoken of a thousand points of light, of all the community organizations that are spread like stars throughout the Nation, doing good." During his presidency, Bush honored numerous volunteers with the Daily Point of Light Award, a tradition that was continued by his presidential successors. In 1990, the Points of Light Foundation was created as a nonprofit organization in Washington to promote this spirit of volunteerism. In 2007, the Points of Light Foundation merged with the Hands On Network to create a new organization, Points of Light.

Bush appointed two justices to the Supreme Court of the United States. In 1990, Bush appointed a largely unknown state appellate judge, David Souter, to replace liberal icon William Brennan. Souter was easily confirmed and served until 2009, but joined the liberal bloc of the court, disappointing Bush. In 1991, Bush nominated conservative federal judge Clarence Thomas to succeed Thurgood Marshall, a long-time liberal stalwart. Thomas, the former head of the Equal Employment Opportunity Commission (EEOC), faced heavy opposition in the Senate, as well as from pro-choice groups and the NAACP. His nomination faced another difficulty when Anita Hill accused Thomas of having sexually harassed her during his time as the chair of EEOC. Thomas won confirmation in a narrow 52–48 vote; 43 Republicans and 9 Democrats voted to confirm Thomas's nomination, while 46 Democrats and 2 Republicans voted against confirmation. Thomas became one of the most conservative justices of his era. In addition to his two Supreme Court appointments, Bush appointed 42 judges to the United States courts of appeals, and 148 judges to the United States district courts. Among these appointments were future Supreme Court Justice Samuel Alito, as well as Vaughn R. Walker, who was later revealed to be the earliest known gay federal judge.

Bush's education platform consisted mainly of offering federal support for a variety of innovations, such as open enrollment, incentive pay for outstanding teachers, and rewards for schools that improve performance with underprivileged children. Though Bush did not pass a major educational reform package during his presidency, his ideas influenced later reform efforts, including Goals 2000 and the No Child Left Behind Act. Bush signed the Immigration Act of 1990, which led to a 40 percent increase in legal immigration to the United States. The act more than doubled the number of visas given to immigrants on the basis of job skills. In the wake of the savings and loan crisis, Bush proposed a $50 billion package to rescue the savings and loans industry, and also proposed the creation of the Office of Thrift Supervision to regulate the industry. Congress passed the Financial Institutions Reform, Recovery, and Enforcement Act of 1989, which incorporated most of Bush's proposals.

Bush was widely seen as a "pragmatic caretaker" president who lacked a unified and compelling long-term theme in his efforts. Indeed, Bush's sound bite where he refers to the issue of overarching purpose as "the vision thing" has become a metonym applied to other political figures accused of similar difficulties. His ability to gain broad international support for the Gulf War and the war's result were seen as both a diplomatic and military triumph, rousing bipartisan approval, though his decision to withdraw without removing Saddam Hussein left mixed feelings, and attention returned to the domestic front and a souring economy. A "New York Times" article mistakenly depicted Bush as being surprised to see a supermarket barcode reader; the report of his reaction exacerbated the notion that he was "out of touch". Amid the early 1990s recession, his image shifted from "conquering hero" to "politician befuddled by economic matters".

Bush announced his reelection bid in early 1992; with a coalition victory in the Persian Gulf War and high approval ratings, Bush's reelection initially looked likely. As a result, many leading Democrats, including Mario Cuomo, Dick Gephardt, and Al Gore, declined to seek their party's presidential nomination. However, Bush's tax increase had angered many conservatives, who believed that Bush had strayed from the conservative principles of Ronald Reagan. He faced a challenge from conservative political columnist Pat Buchanan in the 1992 Republican primaries. Bush fended off Buchanan's challenge and won his party's nomination at the 1992 Republican National Convention, but the convention adopted a socially conservative platform strongly influenced by the Christian right.

Meanwhile, the Democrats nominated Governor Bill Clinton of Arkansas. A moderate who was affiliated with the Democratic Leadership Council (DLC), Clinton favored welfare reform, deficit reduction, and a tax cut for the middle class. In early 1992, the race took an unexpected twist when Texas billionaire H. Ross Perot launched a third party bid, claiming that neither Republicans nor Democrats could eliminate the deficit and make government more efficient. His message appealed to voters across the political spectrum disappointed with both parties' perceived fiscal irresponsibility. Perot also attacked NAFTA, which he claimed would lead to major job losses. National polling taken in mid-1992 showed Perot in the lead, but Clinton experienced a surge through effective campaigning and the selection of Senator Al Gore, a popular and relatively young Southerner, as his running mate.

Clinton won the election, taking 43 percent of the popular vote and 370 electoral votes, while Bush won 37.5 percent of the popular vote and 168 electoral votes. Perot won 19% of the popular vote, one of the highest totals for a third party candidate in U.S. history, drawing equally from both major candidates, according to exit polls. Clinton performed well in the Northeast, the Midwest, and the West Coast, while also waging the strongest Democratic campaign in the South since the 1976 election. Several factors were important in Bush's defeat. The ailing economy which arose from recession may have been the main factor in Bush's loss, as 7 in 10 voters said on election day that the economy was either "not so good" or "poor". On the eve of the 1992 election, the unemployment rate stood at 7.8%, which was the highest it had been since 1984. The president was also damaged by his alienation of many conservatives in his party. Bush blamed Perot in part for his defeat, though exit polls showed that Perot drew his voters about equally from Clinton and Bush.

Despite his defeat, Bush left office with a 56 percent job approval rating in January 1993. Like many of his predecessors, Bush issued a series of pardons during his last days in office. In December 1992, he granted executive clemency to six former senior government officials implicated in the Iran-Contra scandal, most prominently former Secretary of Defense Caspar Weinberger. The pardons effectively brought an end to special prosecutor Lawrence Walsh's investigation of the Iran-Contra scandal.

After leaving office, Bush and his wife built a retirement house in the community of West Oaks, Houston. He established a presidential office within the Park Laureate Building on Memorial Drive in Houston. He also frequently spent time at his vacation home in Kennebunkport, took annual cruises in Greece, went on fishing trips in Florida, and visited the Bohemian Club in Northern California. He declined to serve on corporate boards, but delivered numerous paid speeches and served as an adviser to The Carlyle Group, a private equity firm. He never published his memoirs, but he and Brent Scowcroft co-wrote "A World Transformed", a 1999 work on foreign policy. Portions of his letters and his diary were later published as "The China Diary of George H. W. Bush" and "All The Best, George Bush".

During a 1993 visit to Kuwait, Bush was targeted in an assassination plot directed by the Iraqi Intelligence Service. President Clinton retaliated when he ordered the firing of 23 cruise missiles at Iraqi Intelligence Service headquarters in Baghdad. Bush did not publicly comment on the assassination attempt or the missile strike, but privately spoke with Clinton shortly before the strike took place. In the 1994 gubernatorial elections, his sons George W. and Jeb concurrently ran for Governor of Texas and Governor of Florida. Concerning their political careers, he advised them both that "[a]t some point both of you may want to say 'Well, I don't agree with my Dad on that point' or 'Frankly I think Dad was wrong on that.' Do it. Chart your own course, not just on the issues but on defining yourselves". George W. won his race against Ann Richards while Jeb lost to Lawton Chiles. After the results came in, the elder Bush told ABC, "I have very mixed emotions. Proud father, is the way I would sum it all up." Jeb would again run for governor of Florida in 1998 and win at the same time that his brother George W. won re-election in Texas. It marked the second time in United States history that a pair of brothers served simultaneously as governors.

Bush supported his son's candidacy in the 2000 presidential election, but did not actively campaign in the election and did not deliver a speech at the 2000 Republican National Convention. George W. Bush defeated Al Gore in the 2000 election and was re-elected in 2004. Bush and his son thus became the second father–son pair to each serve as President of the United States, following John Adams and John Quincy Adams. Through previous administrations, the elder Bush had ubiquitously been known as "George Bush" or "President Bush", but following his son's election the need to distinguish between them has made retronymic forms such as "George H. W. Bush" and "George Bush Sr." and colloquialisms such as "Bush 41" and "Bush the Elder" more common. Bush advised his son on some personnel choices, approving of the selection of Dick Cheney as running mate and the retention of George Tenet as CIA Director. However, he was not consulted on all appointments, including that of his old rival, Donald Rumsfeld, as Secretary of Defense. Though he avoided giving unsolicited advice to his son, Bush and his son also discussed some matters of policy, especially regarding national security issues.

In his retirement, Bush generally avoided publicly expressing his opinion on political issues, instead using the public spotlight to support various charities. Despite earlier political differences with Bill Clinton, the two former presidents eventually became friends. They appeared together in television ads, encouraging aid for victims of Hurricane Katrina and the 2004 Indian Ocean earthquake and tsunami.

Bush supported Republican John McCain in the 2008 presidential election, and Republican Mitt Romney in the 2012 presidential election, but both were defeated by Democrat Barack Obama. In 2011, Obama awarded Bush with the Presidential Medal of Freedom, the highest civilian honor in the United States.

Bush supported his son Jeb's bid in the 2016 presidential election. Jeb Bush's campaign struggled however, and he withdrew from the race during the primaries. Neither George H.W. nor George W. Bush endorsed the eventual Republican nominee, Donald Trump; all three Bushes emerged as frequent critics of Trump's policies and speaking style, while Trump frequently criticized George W. Bush's presidency. George H. W. Bush later said that he voted for the Democratic nominee, Hillary Clinton, in the general election. After the election, Bush wrote a letter to president-elect Donald Trump in January 2017 to inform him that because of his poor health, he would not be able to attend Trump's inauguration on January 20; he gave him his best wishes.

In August 2017, after the violence at Unite the Right rally in Charlottesville, both Presidents Bush released a joint statement saying, "America must always reject racial bigotry, anti-Semitism, and hatred in all forms," the statement from presidents George H. W. and George W. Bush reads. "As we pray for Charlottesville, we are all reminded of the fundamental truths recorded by that city's most prominent citizen in the Declaration of Independence: we are all created equal and endowed by our Creator with unalienable rights."

On April 17, 2018, Bush's wife Former First Lady Barbara Bush died at the age of 92, at her home in Houston, Texas. Her funeral was held at St. Martin's Episcopal Church in Houston four days later. Bush along with former Presidents Barack Obama, George W. Bush (son), Bill Clinton and fellow First Ladies Melania Trump, Michelle Obama, Laura Bush (daughter-in-law) and Hillary Clinton were representatives who attended the funeral and who also took a photo together after the service as a sign of unity which went viral online.

On November 1, Bush went to the polls to vote early in the midterm elections. This would be his final public appearance.

George H. W. Bush died on November 30, 2018, aged 94 years, 171 days, at his home in Houston. At the time of his death he was the longest-lived U.S. president, a distinction now held by Jimmy Carter. He was also the third-oldest vice president. Bush lay in state in the Rotunda of the U.S. Capitol from December 3 through December 5; he was the 12th U.S. president to be accorded this honor. Then, on December 5, Bush's casket was transferred from the Capitol rotunda to Washington National Cathedral where a state funeral was held. After the funeral, Bush's body was transported to George H.W. Bush Presidential Library in College Station, Texas, where he was buried next to his wife Barbara and daughter Robin. At the funeral, former president George W. Bush eulogized his father saying,
In 1991, "The New York Times" revealed that Bush was suffering from Graves' disease, a non-contagious thyroid condition that his wife Barbara also suffered from. Later in life, Bush suffered from vascular parkinsonism, a form of Parkinson's disease which forced him to use a motorized scooter or wheelchair.

Bush was raised in the Episcopal Church, though by the end of his life his apparent religious beliefs are considered to have more in line with Evangelical Christian doctrine and practices. He cited various moments in his life deepening of his faith, including his escape from Japanese forces in 1944, and the death of his three-year-old daughter Robin in 1953. His faith was reflected in his Thousand Points of Light speech, his support for prayer in schools, and his support for the pro-life movement (following his election as vice president).

Polls of historians and political scientists have ranked Bush in the top half of presidents. A 2018 poll of the American Political Science Association's Presidents and Executive Politics section ranked Bush as the 17th best president out of 44. A 2017 C-Span poll of historians also ranked Bush as the 20th best president out of 43. Richard Rose described Bush as a "guardian" president, and many other historians and political scientists have similarly described Bush as a passive, hands-off president who was "largely content with things as they were". Professor Steven Knott writes that "[g]enerally the Bush presidency is viewed as successful in foreign affairs but a disappointment in domestic affairs."

Biographer Jon Meacham writes that, after he left office, many Americans viewed Bush as "a gracious and underappreciated man who had many virtues but who had failed to project enough of a distinctive identity and vision to overcome the economic challenges of 1991–92 and to win a second term." Bush himself noted that his legacy was "lost between the glory of Reagan ... and the trials and tribulations of my sons." In the 2010s, Bush was fondly remembered for his willingness to compromise, which contrasted with the intensely partisan era that followed his presidency.

According to "USA Today", the legacy of Bush's presidency was defined by his victory over Iraq after the invasion of Kuwait, and for his presiding over the Dissolution of the Soviet Union and the German reunification. Michael Beschloss and Strobe Talbott praise Bush's handling of the USSR, especially how he prodded Gorbachev in terms of releasing control over the satellite states and permitting German unification—and especially a united Germany in NATO. Andrew Bacevich judges the Bush administration as “morally obtuse” in the light of its “business-as-usual” attitude towards China after the massacre in Tiananmen Square and its uncritical support of Gorbachev as the Soviet Union disintegrated. David Rothkopf argues:

In 1990, "Time "magazine named him the Man of the Year. In 1997, the Houston Intercontinental Airport was renamed as the George Bush Intercontinental Airport. In 1999, the CIA headquarters in Langley, Virginia, was named the "George Bush Center for Intelligence" in his honor. In 2011, Bush, an avid golfer, was inducted in the World Golf Hall of Fame. The (CVN-77), the tenth and last supercarrier of the United States Navy, was named for Bush. Bush is commemorated on a postage stamp that was issued by the United States Postal Service in 2019.

The George H.W. Bush Presidential Library and Museum, the tenth U.S. presidential library, was completed in 1997. It contains the presidential and vice presidential papers of Bush and the vice presidential papers of Dan Quayle. The library is located on a site on the west campus of Texas A&M University in College Station, Texas. Texas A&M University also hosts the Bush School of Government and Public Service, a graduate public policy school.






</doc>
<doc id="11956" url="https://en.wikipedia.org/wiki?curid=11956" title="GPS (disambiguation)">
GPS (disambiguation)

GPS is the Global Positioning System, a satellite-based navigation system.

GPS may also refer to:







</doc>
<doc id="11958" url="https://en.wikipedia.org/wiki?curid=11958" title="George Berkeley">
George Berkeley

George Berkeley (; 12 March 168514 January 1753) – known as Bishop Berkeley (Bishop of Cloyne) – was an Irish philosopher whose primary achievement was the advancement of a theory he called "immaterialism" (later referred to as "subjective idealism" by others). This theory denies the existence of material substance and instead contends that familiar objects like tables and chairs are only ideas in the minds of perceivers and, as a result, cannot exist without being perceived. Berkeley is also known for his critique of abstraction, an important premise in his argument for immaterialism.

In 1709, Berkeley published his first major work, "", in which he discussed the limitations of human vision and advanced the theory that the proper objects of sight are not material objects, but light and colour. This foreshadowed his chief philosophical work, "A Treatise Concerning the Principles of Human Knowledge", in 1710, which, after its poor reception, he rewrote in dialogue form and published under the title "Three Dialogues between Hylas and Philonous" in 1713.

In this book, Berkeley's views were represented by Philonous (Greek: "lover of mind"), while Hylas ("hyle", Greek: "matter") embodies the Irish thinker's opponents, in particular John Locke.
Berkeley argued against Isaac Newton's doctrine of absolute space, time and motion in "De Motu" (On Motion), published 1721. His arguments were a precursor to the views of Mach and Einstein. In 1732, he published "Alciphron", a Christian apologetic against the free-thinkers, and in 1734, he published "The Analyst", a critique of the foundations of calculus, which was influential in the development of mathematics.

Interest in Berkeley's work increased after World War II because he tackled many of the issues of paramount interest to philosophy in the 20th century, such as the problems of perception, the difference between primary and secondary qualities, and the importance of language.

Berkeley was born at his family home, Dysart Castle, near Thomastown, County Kilkenny, Ireland, the eldest son of William Berkeley, a cadet of the noble family of Berkeley. Little is known of his mother. He was educated at Kilkenny College and attended Trinity College Dublin, where he was elected a Scholar in 1702, earning a bachelor's degree in 1704 and completing a master's degree in 1707. He remained at Trinity College after completion of his degree as a tutor and Greek lecturer.

His earliest publication was on mathematics, but the first that brought him notice was his "", first published in 1709. In the essay, Berkeley examines visual distance, magnitude, position and problems of sight and touch. While this work raised much controversy at the time, its conclusions are now accepted as an established part of the theory of optics.

The next publication to appear was the "Treatise Concerning the Principles of Human Knowledge" in 1710, which had great success and gave him a lasting reputation, though few accepted his theory that nothing exists outside the mind. This was followed in 1713 by "Three Dialogues between Hylas and Philonous", in which he propounded his system of philosophy, the leading principle of which is that the world, as represented by our senses, depends for its existence on being perceived.

For this theory, the "Principles" gives the exposition and the "Dialogues" the defence. One of his main objectives was to combat the prevailing materialism of his time. The theory was largely received with ridicule, while even those such as Samuel Clarke and William Whiston, who did acknowledge his "extraordinary genius," were nevertheless convinced that his first principles were false.

Shortly afterwards, Berkeley visited England and was received into the circle of Addison, Pope and Steele. In the period between 1714 and 1720, he interspersed his academic endeavours with periods of extensive travel in Europe, including one of the most extensive Grand Tours of the length and breadth of Italy ever undertaken. In 1721, he took Holy Orders in the Church of Ireland, earning his doctorate in divinity, and once again chose to remain at Trinity College Dublin, lecturing this time in Divinity and in Hebrew. In 1721/2 he was made Dean of Dromore and, in 1724, Dean of Derry.

In 1723, following her violent quarrel with Jonathan Swift, who had been her intimate friend for many years, Esther Vanhomrigh (for whom Swift had created the nickname "Vanessa") named Berkeley her co-heir along with the barrister Robert Marshall; her choice of legatees caused a good deal of surprise since she did not know either of them well, although Berkeley as a very young man had known her father. Swift said generously that he did not grudge Berkeley his inheritance, much of which vanished in a lawsuit in any event. A story that Berkeley and Marshall disregarded a condition of the inheritance that they must publish the correspondence between Swift and Vanessa is probably untrue.

In 1725, he began the project of founding a college in Bermuda for training ministers and missionaries in the colony, in pursuit of which he gave up his deanery with its income of £1100.

In 1728, he married Anne Forster, daughter of John Forster, Chief Justice of the Irish Common Pleas, and his first wife Rebecca Monck. He then went to America on a salary of £100 per annum. He landed near Newport, Rhode Island, where he bought a plantation at Middletownthe famous "Whitehall". He purchased several slaves to work the plantation. It has been claimed that "he introduced Palladianism into America by borrowing a design from [William] Kent's "Designs of Inigo Jones" for the door-case of his house in Rhode Island, Whitehall." He also brought to New England John Smibert, the British artist he "discovered" in Italy, who is generally regarded as the founding father of American portrait painting. Meanwhile, he drew up plans for the ideal city he planned to build on Bermuda. He lived at the plantation while he waited for funds for his college to arrive. The funds, however, were not forthcoming. "With the withdrawal from London of his own persuasive energies, opposition gathered force; and the Prime Minister, Walpole grew steadily more sceptical and lukewarm. At last it became clear that the essential Parliamentary grant would be not forthcoming" and in 1732 he left America and returned to London. He and Anne had four children who survived infancy: Henry, George, William and Julia, and at least two other children who died in infancy. William's death in 1751 was a great cause of grief to his father.

Berkeley was nominated to be the bishop of Cloyne in the Church of Ireland on 18 January 1734. He was consecrated as such on 19 May 1734. He was the bishop of Cloyne until his death on 14 January 1753, although he died at Oxford (see below).

While living in London's Saville Street, he took part in efforts to create a home for the city's abandoned children. The Foundling Hospital was founded by Royal Charter in 1739, and Berkeley is listed as one of its original governors.

His last two publications were "Siris: A Chain of Philosophical Reflexions and Inquiries Concerning the Virtues of Tarwater, And divers other Subjects connected together and arising one from another" (1744) and "Further Thoughts on Tar-water" (1752). Pine tar is an effective antiseptic and disinfectant when applied to cuts on the skin, but Berkeley argued for the use of pine tar as a broad panacea for diseases. His 1744 work on tar-water sold more copies than any of his other books during Berkeley's lifetime.

He remained at Cloyne until 1752, when he retired. With his wife and daughter Julia he went to Oxford to live with his son George and supervise his education. He died soon afterward and was buried in Christ Church Cathedral, Oxford. His affectionate disposition and genial manners made him much loved and held in warm regard by many of his contemporaries. Anne outlived her husband by many years, and died in 1786.

The use of the concepts of "spirit" and "idea" is central in Berkeley's philosophy. As used by him, these concepts are difficult to translate into modern terminology. His concept of "spirit" is close to the concept of "conscious subject" or of "mind", and the concept of "idea" is close to the concept of "sensation" or "state of mind" or "conscious experience".

Thus Berkeley denied the existence of matter as a metaphysical substance, but did not deny the existence of physical objects such as apples or mountains. (""I do not argue against the existence of any one thing that we can apprehend, either by sense or reflection. That the things I see with mine eyes and touch with my hands do exist, really exist, I make not the least question. The only thing whose existence we deny, is that which philosophers call matter or corporeal substance. And in doing of this, there is no damage done to the rest of mankind, who, I dare say, will never miss it."", "Principles "#35) This basic claim of Berkeley's thought, his "idealism", is sometimes and somewhat derisively called "immaterialism" or, occasionally, subjective idealism. In "Principles #3," he wrote, using a combination of Latin and English, "esse is percipi" (to be is to be perceived), most often if slightly inaccurately attributed to Berkeley as the pure Latin phrase "esse est percipi." The phrase appears associated with him in authoritative philosophical sources, e.g., "Berkeley holds that there are no such mind-independent things, that, in the famous phrase, esse est percipi (aut percipere) – to be is to be perceived (or to perceive)."

Hence, human knowledge is reduced to two elements: that of spirits and of ideas ("Principles" #86). In contrast to ideas, a spirit cannot be perceived. A person's spirit, which perceives ideas, is to be comprehended intuitively by inward feeling or reflection ("Principles" #89). For Berkeley, we have no direct 'idea' of spirits, albeit we have good reason to believe in the existence of other spirits, for their existence explains the purposeful regularities we find in experience. (""It is plain that we cannot know the existence of other spirits otherwise than by their operations, or the ideas by them excited in us"", Dialogues #145). This is the solution that Berkeley offers to the problem of other minds. Finally, the order and purposefulness of the whole of our experience of the world and especially of nature overwhelms us into believing in the existence of an extremely powerful and intelligent spirit that causes that order. According to Berkeley, reflection on the attributes of that external spirit leads us to identify it with God. Thus a material thing such as an apple consists of a collection of ideas (shape, color, taste, physical properties, etc.) which are caused in the spirits of humans by the spirit of God.

A convinced adherent of Christianity, Berkeley believed God to be present as an immediate cause of all our experiences.

Here is Berkeley's proof of the existence of God:

As T. I. Oizerman explained:

Berkeley believed that God is not the distant engineer of Newtonian machinery that in the fullness of time led to the growth of a tree in the university quadrangle. Rather, the perception of the tree is an idea that God's mind has produced in the mind, and the tree continues to exist in the quadrangle when "nobody" is there, simply because God is an infinite mind that perceives all.

The philosophy of David Hume concerning causality and objectivity is an elaboration of another aspect of Berkeley's philosophy. A.A. Luce, the most eminent Berkeley scholar of the 20th century, constantly stressed the continuity of Berkeley's philosophy. The fact that Berkeley returned to his major works throughout his life, issuing revised editions with only minor changes, also counts against any theory that attributes to him a significant volte-face.

John Locke (Berkeley's predecessor) states that we define an object by its primary and secondary qualities. He takes heat as an example of a secondary quality. If you put one hand in a bucket of cold water, and the other hand in a bucket of warm water, then put both hands in a bucket of lukewarm water, one of your hands is going to tell you that the water is cold and the other that the water is hot. Locke says that since two different objects (both your hands) perceive the water to be hot "and" cold, then the heat is not a quality of the water.

While Locke used this argument to distinguish primary from secondary qualities, Berkeley extends it to cover primary qualities in the same way. For example, he says that size is not a quality of an object because the size of the object depends on the distance between the observer and the object, or the size of the observer. Since an object is a different size to different observers, then size is not a quality of the object. Berkeley rejects shape with a similar argument and then asks: if neither primary qualities nor secondary qualities are of the object, then how can we say that there is anything more than the qualities we observe?

Relativity is the idea that there is no objective, universal truth; it is a state of dependence in which the existence of one independent object is solely dependent on that of another. According to Locke, characteristics of primary qualities are mind-independent, such as shape, size, etc., whereas secondary qualities are mind-dependent, for example, taste and color. George Berkeley refuted John Locke’s belief on primary and secondary qualities because Berkeley believed that "we cannot abstract the primary qualities (e.g shape) from secondary ones (e.g color)". Berkeley argued that perception is dependent on the distance between the observer and the object, and "thus, we cannot conceive of mechanist material bodies which are extended but not (in themselves) colored". What perceived can be the same type of quality, but completely opposite form each other because of different positions and perceptions, what we perceive can be different even when the same types of things consist of contrary qualities. Secondary qualities aid in people’s conception of primary qualities in an object, like how the color of an object leads people to recognize the object itself. More specifically, the color red can be perceived in apples, strawberries, and tomatoes, yet we would not know what these might look like without its color. We would also be unaware of what the color red looked like if red paint, or any object that has a perceived red color, failed to exist. From this, we can see that colors cannot exist on their own and can solely represent a group of perceived objects. Therefore, both primary and secondary qualities are mind-dependent: they cannot exist without our minds.
George Berkeley was a philosopher who was against rationalism and "classical" empiricism. He was a "subjective idealist" or "empirical idealist", who believed that reality is constructed entirely of immaterial, conscious minds and their ideas; everything that exists is somehow dependent on the subject perceiving it, except the subject themselves. He refuted the existence of abstract objects that many other philosophers believed to exist, notably Plato. According to Berkeley, "an abstract object does not exist in space or time and which is therefore entirely non-physical and non-metal"; however, this argument contradicts with his relativity argument. If "esse est percipi", (Latin meaning that to exist is to be perceived) is true, then the objects in the relativity argument made by Berkeley can either exist or not. Berkeley believed that only the minds' perceptions and the Spirit that perceives are what exists in reality; what people perceive every day is only the idea of an object’s existence, but the objects themselves are not perceived. Berkeley also discussed how, at times, materials cannot be perceived by oneself, and the mind of oneself cannot understand the objects. However, there also exists an "omnipresent, eternal mind" that Berkeley believed to consist of God and the Spirit, both omniscient and all-perceiving. According to Berkeley, God is the entity who controls everything, yet Berkeley also argued that "abstract object[s] do not exist in space or time". In other words, as Warnock argues, Berkeley "had recognized that he could not square with his own talk of "spirits", of our minds and of God; for these are perceivers and not among objects of perception. Thus he says, rather weakly and without elucidation, that in addition to our ideas we also have "notions"—we know what it means to speak of "spirits" and their operations."

However, the relativity argument violates the idea of immaterialism. Berkeley’s immaterialism argues that "esse est percipi (aut percipere)", which in English is to be is to be perceived (or to perceive). That is saying only what perceived or perceives is real, and without our perception or God's nothing can be real. Yet, if the relativity argument, also by Berkeley, argues that the perception of an object depends on the different positions, then this means that what perceived can either be real or not because the perception does not show that whole picture and the whole picture cannot be perceived. Berkeley also believes that "when one perceives mediately, one perceives one idea by means of perceiving another". By this, it can be elaborated that if the standards of what perceived at first are different, what perceived after that can be different, as well. In the heat perception described above, one hand perceived the water to be hot and the other hand perceived the water to be cold due to relativity. If applying the idea "to be is to be perceived", the water should be both cold and hot because both perceptions are perceived by different hands. However, the water cannot be cold and hot at the same time for it self-contradicts, so this shows that what perceived is not always true because it sometimes can break the law of noncontradiction. In this case, "it would be arbitrary anthropocentrism to claim that humans have special access to the true qualities of objects". The truth for different people can be different, and humans are limited to accessing the absolute truth due to relativity. Summing up, nothing can be absolutely true due to relativity or the two arguments, to be is to be perceived and the relativity argument, do not always work together.

In his "Essay Towards a New Theory of Vision", Berkeley frequently criticised the views of the Optic Writers, a title that seems to include Molyneux, Wallis, Malebranche and Descartes. In sections 1–51, Berkeley argued against the classical scholars of optics by holding that: "spatial depth, as the distance that separates the perceiver from the perceived object is itself invisible". That is, we do not see space directly or deduce its form logically using the laws of optics. Space for Berkeley is no more than a contingent expectation that visual and tactile sensations will follow one another in regular sequences that we come to expect through habit.

Berkeley goes on to argue that visual cues, such as the perceived extension or 'confusion' of an object, can only be used to indirectly judge distance, because the viewer learns to associate visual cues with tactile sensations. Berkeley gives the following analogy regarding indirect distance perception: one perceives distance indirectly just as one perceives a person's embarrassment indirectly. When looking at an embarrassed person, we infer indirectly that the person is embarrassed by observing the red color on the person's face. We know through experience that a red face tends to signal embarrassment, as we've learned to associate the two.

The question concerning the visibility of space was central to the Renaissance perspective tradition and its reliance on classical optics in the development of pictorial representations of spatial depth. This matter was debated by scholars since the 11th-century Arab polymath and mathematician Alhazen (al-Hasan Ibn al-Haytham) affirmed in experimental contexts the visibility of space. This issue, which was raised in Berkeley's theory of vision, was treated at length in the "Phenomenology of Perception" of Maurice Merleau-Ponty, in the context of confirming the visual perception of spatial depth ("la profondeur"), and by way of refuting Berkeley's thesis.

Berkeley wrote about the perception of size in addition to that of distance. He is frequently misquoted as believing in size–distance invariance – a view held by the Optic Writers. This idea is that we scale the image size according to distance in a geometrical manner. The error may have become commonplace because the eminent historian and psychologist E. G. Boring perpetuated it. In fact, Berkeley argued that the same cues that evoke distance also evoke size, and that we do not first see size and then calculate distance. It is worth quoting Berkeley's words on this issue (Section 53):
What inclines men to this mistake (beside the humour of making one see by geometry) is, that the same perceptions or ideas which suggest distance, do also suggest magnitude ... I say they do not first suggest distance, and then leave it to the judgement to use that as a medium, whereby to collect the magnitude; but they have as close and immediate a connexion with the magnitude as with the distance; and suggest magnitude as independently of distance, as they do distance independently of magnitude.

"Berkeley's works display his keen interest in natural philosophy [...] from his earliest writings ("Arithmetica", 1707) to his latest ("Siris", 1744). Moreover, much of his philosophy is shaped fundamentally by his engagement with the science of his time." The profundity of this interest can be judged from numerous entries in Berkeley's "Philosophical Commentaries" (1707–1708), e.g. "Mem. to Examine & accurately discuss the scholium of the 8th Definition of Mr Newton's Principia." (#316)

Berkeley argued that forces and gravity, as defined by Newton, constituted "occult qualities" that "expressed nothing distinctly". He held that those who posited "something unknown in a body of which they have no idea and which they call the principle of motion, are in fact simply stating that the principle of motion is unknown." Therefore, those who "affirm that active force, action, and the principle of motion are really in bodies are adopting an opinion not based on experience." Forces and gravity existed nowhere in the phenomenal world. On the other hand, if they resided in the category of "soul" or "incorporeal thing", they "do not properly belong to physics" as a matter. Berkeley thus concluded that forces lay beyond any kind of empirical observation and could not be a part of proper science. He proposed his theory of signs as a means to explain motion and matter without reference to the "occult qualities" of force and gravity.

Berkeley's razor is a rule of reasoning proposed by the philosopher Karl Popper in his study of Berkeley's key scientific work "De Motu". Berkeley's razor is considered by Popper to be similar to Ockham's razor but "more powerful". It represents an extreme, empiricist view of scientific observation that states that the scientific method provides us with no true insight into the nature of the world. Rather, the scientific method gives us a variety of partial explanations about regularities that hold in the world and that are gained through experiment. The nature of the world, according to Berkeley, is only approached through proper metaphysical speculation and reasoning. Popper summarises Berkeley's razor as such:
A general practical result – which I propose to call "Berkeley's razor" – of [Berkeley's] analysis of physics allows us "a priori" to eliminate from physical science all essentialist explanations. If they have a mathematical and predictive content they may be admitted "qua" mathematical hypotheses (while their essentialist interpretation is eliminated). If not they may be ruled out altogether. This razor is sharper than Ockham's: "all" entities are ruled out except those which are perceived.

In another essay of the same book titled "Three Views Concerning Human Knowledge", Popper argues that Berkeley is to be considered as an instrumentalist philosopher, along with Robert Bellarmine, Pierre Duhem and Ernst Mach. According to this approach, scientific theories have the status of serviceable fictions, useful inventions aimed at explaining facts, and without any pretension to be true. Popper contrasts instrumentalism with the above mentioned essentialism and his own "critical rationalism".

In addition to his contributions to philosophy, Berkeley was also very influential in the development of mathematics, although in a rather indirect sense. "Berkeley was concerned with mathematics and its philosophical interpretation from the earliest stages of his intellectual life."
Berkeley's "Philosophical Commentaries" (1707–1708) witness to his interest in mathematics:

Axiom. No reasoning about things whereof we have no idea. Therefore no reasoning about Infinitesimals. (#354)

Take away the signs from Arithmetic & Algebra, & pray what remains? (#767)
These are sciences purely Verbal, & entirely useless but for Practise in Societys of Men. No speculative knowledge, no comparison of Ideas in them. (#768)
In 1707, Berkeley published two treatises on mathematics. In 1734, he published "The Analyst", subtitled "A DISCOURSE Addressed to an Infidel Mathematician", a critique of calculus. Florian Cajori called this treatise "the most spectacular event of the century in the history of British mathematics." However, a recent study suggests that Berkeley misunderstood Leibnizian calculus. The mathematician in question is believed to have been either Edmond Halley, or Isaac Newton himself—though if to the latter, then the discourse was posthumously addressed, as Newton died in 1727. "The Analyst" represented a direct attack on the foundations and principles of calculus and, in particular, the notion of fluxion or infinitesimal change, which Newton and Leibniz used to develop the calculus. In his critique, Berkeley coined the phrase "ghosts of departed quantities", familiar to students of calculus. Ian Stewart's book "From Here to Infinity" captures the gist of his criticism.

Berkeley regarded his criticism of calculus as part of his broader campaign against the religious implications of Newtonian mechanicsas a defence of traditional Christianity against deism, which tends to distance God from His worshipers. Specifically, he observed that both Newtonian and Leibnizian calculus employed infinitesimals sometimes as positive, nonzero quantities and other times as a number explicitly equal to zero. Berkeley's key point in "The Analyst" was that Newton's calculus (and the laws of motion based in calculus) lacked rigorous theoretical foundations. He claimed that

In every other Science Men prove their Conclusions by their Principles, and not their Principles by the Conclusions. But if in yours you should allow your selves this unnatural way of proceeding, the Consequence would be that you must take up with Induction, and bid adieu to Demonstration. And if you submit to this, your Authority will no longer lead the way in Points of Reason and Science.
Berkeley did not doubt that calculus produced real world truth; simple physics experiments could verify that Newton's method did what it claimed to do. "The cause of Fluxions cannot be defended by reason", but the results could be defended by empirical observation, Berkeley's preferred method of acquiring knowledge at any rate. Berkeley, however, found it paradoxical that "Mathematicians should deduce true Propositions from false Principles, be right in Conclusion, and yet err in the Premises." In "The Analyst" he endeavoured to show "how Error may bring forth Truth, though it cannot bring forth Science". Newton's science, therefore, could not on purely scientific grounds justify its conclusions, and the mechanical, deistic model of the universe could not be rationally justified.

The difficulties raised by Berkeley were still present in the work of Cauchy whose approach to calculus was a combination of infinitesimals and a notion of limit, and were eventually sidestepped by Weierstrass by means of his (ε, δ) approach, which eliminated infinitesimals altogether. More recently, Abraham Robinson restored infinitesimal methods in his 1966 book "Non-standard analysis" by showing that they can be used rigorously.

The tract "A Discourse on Passive Obedience" (1712) is considered Berkeley's major contribution to moral and political philosophy.

In "A Discourse on Passive Obedience", Berkeley defends the thesis that people have "a moral duty to observe the negative precepts (prohibitions) of the law, including the duty not to resist the execution of punishment." However, Berkeley does make exceptions to this sweeping moral statement, stating that we need not observe precepts of "usurpers or even madmen" and that people can obey different supreme authorities if there are more than one claims to the highest authority.

Berkeley defends this thesis with a deductive proof stemming from the laws of nature. First, he establishes that because God is perfectly good, the end to which he commands humans must also be good, and that end must not benefit just one person, but the entire human race. Because these commands—or laws—if practiced, would lead to the general fitness of humankind, it follows that they can be discovered by the right reason—for example, the law to never resist supreme power can be derived from reason because this law is "the only thing that stands between us and total disorder". Thus, these laws can be called the laws of nature, because they are derived from God—the creator of nature himself. "These laws of nature include duties never to resist the supreme power, lie under oath ... or do evil so that good may come of it."

One may view Berkeley’s doctrine on Passive Obedience as a kind of 'Theological Utilitarianism', insofar as it states that we have a duty to uphold a moral code which presumably is working towards the ends of promoting the good of humankind. However, the concept of 'ordinary' Utilitarianism is fundamentally different in that it "makes utility the one and only "ground" of obligation"—that is, Utilitarianism is concerned with whether particular actions are morally permissible in specific situations, while Berkeley’s doctrine is concerned with whether or not we should follow moral rules in any and all circumstances. Whereas Act Utilitarianism might, for example, justify a morally impermissible act in light of the specific situation, Berkeley’s doctrine of Passive Obedience holds that it is never morally permissible to not follow a moral rule, even when it seems like breaking that moral rule might achieve the happiest ends. Berkeley holds that even though sometimes, the consequences of an action in a specific situation might be bad, the general tendencies of that action benefits humanity.

Other important sources for Berkeley's views on morality are "Alciphron" (1732), especially dialogues I–III, and the "Discourse to Magistrates" (1738)." "Passive Obedience" is notable partly for containing one of the earliest statements of rule utilitarianism.

George Berkeley’s theory that matter does not exist comes from the belief that "sensible things are those only which are immediately perceived by sense." Berkeley says in his book called The Principles of Human Knowledge that "the ideas of sense are stronger, livelier, and clearer than those of the imagination; and they are also steady, orderly and coherent." From this we can tell that the things that we are perceiving are truly real rather than it just being a dream.

All knowledge comes from perception; what we perceive are ideas, not things in themselves; a thing in itself must be outside experience; so the world only consists of ideas and minds that perceive those ideas; a thing only exists so far as it perceives or is perceived. Through this we can see that consciousness is considered something that exists to Berkeley due to its ability to perceive. "'To be,' said of the object, means to be perceived, 'esse est percipi'; 'to be', said of the subject, means to perceive or 'percipere'." Having established this, Berkeley then attacks the "opinion strangely prevailing amongst men, that houses, mountains, rivers, and in a word all sensible objects have an existence natural or real, distinct from being perceived". He believes this idea to be inconsistent because such an object with an existence independent of perception must have both sensible qualities, and thus be known (making it an idea), and also an insensible reality, which Berkeley believes is inconsistent. Berkeley believes that the error arises because people think that perceptions can imply or infer something about the material object. Berkeley calls this concept "abstract ideas". He rebuts this concept by arguing that people cannot conceive of an object without also imagining the sensual input of the object. He argues in "Principles of Human Knowledge" that, similar to how people can only sense matter with their senses through the actual sensation, they can only conceive of matter (or, rather, ideas of matter) through the idea of sensation of matter. This implies that everything that people can conceive in regards to matter is only ideas about matter. Thus, matter, should it exist, must exist as collections of ideas, which can be perceived by the senses and interpreted by the mind. But if matter is just a collection of ideas, then Berkeley concludes that matter, functionally, does not exist (or at least, not in the way that most philosophers of Berkeley’s time believed). Indeed, if a person visualizes something, then it must have some color, however dark or light; it cannot just be a shape of no color at all if a person is to visualize it. But matter ought not to have inherent color, as well as smell or sound or taste; these properties are only the senses' interpretation of matter. Therefore, matter, should it actually exist, is, at the very least, not knowable. Berkeley’s ideas raised controversy because his argument refuted Descartes' worldview, which was expanded upon by Locke, and resulted in the rejection of Berkeley’s form of empiricism by several philosophers of the seventeenth and eighteenth centuries. In Locke’s worldview, "the world causes the perceptual ideas we have of it by the way it interacts with our senses." This contradicts with Berkeley's worldview because not only does it suggest the existence of physical causes in the world, but in fact there is no physical world beyond our ideas. The only causes that exist in Berkeley’s worldview are those that are a result of the use of the will.

Berkeley’s theory relies heavily on his form of empiricism, which in turn relies heavily on the senses. His empiricism can be defined by five propositions: all significant words stand for ideas; all knowledge is about our ideas; all ideas come from without or from within; if from without it must be by the senses, and they are called sensations, if from within they are the operations of the mind, and are called thoughts. Berkeley clarifies his distinction between ideas by saying they "are imprinted on the senses," "perceived by attending to the passions and operations of the mind," or "are formed by help of memory and imagination." One refutation of his idea was: if someone leaves a room and stops perceiving that room does that room no longer exist? Berkeley answers this by claiming that it is still being perceived and the consciousness that is doing the perceiving is God. (This makes Berkeley’s argument hinge upon an omniscient, omnipresent deity.) This claim is the only thing holding up his argument which is "depending for our knowledge of the world, and of the existence of other minds, upon a God that would never deceive us." Berkeley anticipates a second objection, which he refutes in "Principles of Human Knowledge". He anticipates that the materialist may take a representational materialist standpoint: although the senses can only perceive ideas, these ideas resemble (and thus can be compared to) the actual, existing object. Thus, through the sensing of these ideas, the mind can make inferences as to matter itself, even though pure matter is non-perceivable. Berkeley’s objection to that notion is that "an idea can be like nothing but an idea; a color or figure can be like nothing but another color or figure". Berkeley distinguishes between an idea, which is mind-dependent, and a material object, which is not an idea and is mind-independent. As they are not alike, they cannot be compared, just as one cannot compare the color red to something that is invisible, or the sound of music to silence, other than that one exists and the other does not. This is called the likeness principle: the notion that an idea can only be like (and thus compared to) another idea. If this principle is granted, then material objects lose any kind of association to the attributes that the senses give them. Since matter, should it exist, is not related to the ideas perceived by the senses according to the likeness principle, Berkeley argues that the existence of the idea is all that a person can know. Therefore, it is illogical to conclude that, because the idea of matter exists, matter itself exists. This justifies Berkeley’s skepticism regarding the existence of matter.

George Berkeley’s main argument was structured around the fact that we have no proof that anything that we are seeing isn’t just our brains playing a trick on us. Just because one can see something in front of them it does not mean it exists or it is real. This idea is known as perceptual illusion. Perceptual illusion says that just because we have the sensation of feeling something does not have to do with what they really are. Philosophers like Berkeley like to use perceptual illusion to raise more questions and skepticism about our senses and how real they are. Berkeley firmly believed that what we see or believe is solely based on sensory objects and a collection of all our senses which are nothing but the ideas and imaginations of whoever is perceiving them. A well-known quote George Berkeley said about immaterialism was, "The only things we perceive are our perceptions". This quote summarizes his whole argument and shows his advocation for common sense. Perception does not make anything anymore real; it is just our perceptions that we are seeing, not necessarily real objects. 
"It is evident to anyone who takes a survey of the objects of human knowledge, that they are either "ideas" actually imprinted on the senses; or else such as are perceived by attending to the passions and operations of the mind; or lastly "ideas" formed by help of memory and imagination—either compounding, dividing, or barely representing those originally perceived in the aforesaid ways". (Berkeley's emphasis.) In this quote from Berkeley’s works, it shows how ideas manifest themselves into different objects that one can see, and think is real. Berkeley has been trying to prove the existence of God throughout his beliefs in immaterialism. In the same work, Berkeley says that all ideas are just mental images (Pitcher, p. 70; cf. Winkler, p. 13 and Muehlmann, p. 49). The mental images that we conjure up in our minds is what manifests itself into our own personal reality that we see as real, even if it isn’t. The collection of these mental images makes up our entire world as we know it.

Berkeley's "Treatise Concerning the Principles of Human Knowledge" was published three years before the publication of Arthur Collier's "Clavis Universalis", which made assertions similar to those of Berkeley's. However, there seemed to have been no influence or communication between the two writers.

German philosopher Arthur Schopenhauer once wrote of him: "Berkeley was, therefore, the first to treat the subjective starting-point really seriously and to demonstrate irrefutably its absolute necessity. He is the father of idealism ...".

George Berkeley is considered one of the originators of British empiricism. A linear development is often traced from three great "British Empiricists", leading from Locke through Berkeley to Hume.

Berkeley influenced many modern philosophers, especially David Hume. Thomas Reid admitted that he put forward a drastic criticism of Berkeleianism after he had been an admirer of Berkeley's philosophical system for a long time. Berkeley's "thought made possible the work of Hume and thus Kant, notes Alfred North Whitehead." Some authors draw a parallel between Berkeley and Edmund Husserl.

When Berkeley visited America, the American educator Samuel Johnson visited him, and the two later corresponded. Johnson convinced Berkeley to establish a scholarship program at Yale, and to donate a large number of books as well as his plantation to the college when the philosopher returned to England. It was one of Yale's largest and most important donations; it doubled its library holdings, improved the college's financial position and brought Anglican religious ideas and English culture into New England. Johnson also took Berkeley's philosophy and used parts of it as a framework for his own American Practical Idealism school of philosophy. As Johnson's philosophy was taught to about half the graduates of American colleges between 1743 and 1776, and over half of the contributors to the "Declaration of Independence "were connected to it, Berkeley's ideas were indirectly a foundation of the American Mind.

Outside of America, during Berkeley's lifetime his philosophical ideas were comparatively uninfluential. But interest in his doctrine grew from the 1870s when Alexander Campbell Fraser, "the leading Berkeley scholar of the nineteenth century", published "The Works of George Berkeley." A powerful impulse to serious studies in Berkeley's philosophy was given by A. A. Luce and Thomas Edmund Jessop, "two of the twentieth century's foremost Berkeley scholars," thanks to whom Berkeley scholarship was raised to the rank of a special area of historico-philosophical science. In addition, the philosopher Colin Murray Turbayne wrote extensively on Berkeley's use of language as model for visual, physiological, natural and metaphysical relationships.

The proportion of Berkeley scholarship, in literature on the history of philosophy, is increasing. This can be judged from the most comprehensive bibliographies on George Berkeley. During the period of 1709–1932, about 300 writings on Berkeley were published. That amounted to 1.5 publication per annum. During the course of 1932–79, over one thousand works were brought out, i.e., 20 works per annum. Since then, the number of publications has reached 30 per annum. In 1977 publication began in Ireland of a special journal on Berkeley's life and thought ("Berkeley Studies"). Similarly, in 1988, the Australian philosopher Colin Murray Turbayne established the "International Berkeley Essay Prize Competition" at the University of Rochester in an effort to advance scholarship and research on the works of George Berkeley in the future.

Other than philosophy, Berkeley also influenced modern psychology with his work on John Locke's theory of association and how it could be used to explain how humans gain knowledge in the physical world. He also used the theory to explain perception, stating that all qualities were, as Locke would call them, "secondary qualities", therefore perception laid entirely in the perceiver and not in the object. These are both topics today studied in modern psychology.

Both the University of California, Berkeley and City of Berkeley, were named after him, although the pronunciation has evolved to suit American English: ( ). The naming was suggested in 1866 by Frederick Billings, a trustee of the then College of California. Billings was inspired by Berkeley's "Verses on the Prospect of Planting Arts and Learning in America", particularly the final stanza: "Westward the course of empire takes its way; The first four Acts already past, A fifth shall close the Drama with the day; Time's noblest offspring is the last."

The Town of Berkley, currently the least populated town in Bristol County, Massachusetts, was founded on 18 April 1735 and named after the renowned philosopher. It is located 40 miles south of Boston and 25 miles north of Middletown, Rhode Island.

A residential college and an Episcopal seminary at Yale University also bear Berkeley's name, as does the Berkeley Library at Trinity College, Dublin.

Berkeley Preparatory School in Tampa, Florida, a private school affiliated with the Episcopal Church, is also named for him.

"Bishop Berkeley's Gold Medals" are two awards given annually at Trinity College Dublin, "provided outstanding merit is shown", to candidates answering a special examination in Greek. The awards were founded in 1752 by Berkeley.

An Ulster History Circle blue plaque commemorating him is located in Bishop Street Within, city of Derry.

Berkeley's farmhouse in Rhode Island is preserved as Whitehall Museum House, also known as Berkeley House, and was listed on the National Register of Historic Places in 1970.

Berkeley is honoured together with Joseph Butler with a feast day on the liturgical calendar of the Episcopal Church (USA) on 16 June.






"The Works of George Berkeley". Ed. by Alexander Campbell Fraser. In 4 Volumes. Oxford: Clarendon Press, 1901.
Ewald, William B., ed., 1996. "From Kant to Hilbert: A Source Book in the Foundations of Mathematics", 2 vols. Oxford Uni. Press.





</doc>
<doc id="11959" url="https://en.wikipedia.org/wiki?curid=11959" title="G. E. Moore">
G. E. Moore

George Edward Moore (4 November 1873 – 24 October 1958), usually cited as G. E. Moore, was an English philosopher. He was, with Bertrand Russell, Ludwig Wittgenstein, and (before them) Gottlob Frege, one of the founders of analytic philosophy. Along with Russell, he led the turn away from idealism in British philosophy, and became well known for his advocacy of common sense concepts, his contributions to ethics, epistemology, and metaphysics, and "his exceptional personality and moral character".

Moore was Professor of Philosophy at the University of Cambridge, highly influential among (though not a member of) the Bloomsbury Group, and the editor of the influential journal "Mind". He was elected a fellow of the British Academy in 1918. He was a member of the Cambridge Apostles, the intellectual secret society, from 1894 to 1901, and chairman of the Cambridge University Moral Sciences Club from 1912 to 1944. A humanist, he served as President of the British Ethical Union (now known as Humanists UK) from 1935 to 1936.

Moore was born in Upper Norwood, in south-east London, on 4 November 1873, the middle child of seven of Dr Daniel Moore and Henrietta Sturge. His grandfather was the author Dr George Moore. His eldest brother was Thomas Sturge Moore, a poet, writer and engraver.

He was educated at Dulwich College and in 1892 went up to Trinity College, Cambridge to study classics and moral sciences. He became a Fellow of Trinity in 1898, and went on to hold the University of Cambridge chair of Mental Philosophy and Logic, from 1925 to 1939.

Moore is best known today for his defence of ethical non-naturalism, his emphasis on common sense in philosophical method, and the paradox that bears his name. He was admired by and influential among other philosophers, and also by the Bloomsbury Group, but is (unlike his colleague and admirer Russell, who, for some years thought he fulfilled his "ideal of genius") mostly unknown today outside of academic philosophy. Moore's essays are known for their clear, circumspect writing style, and for his methodical and patient approach to philosophical problems. He was critical of modern philosophy for its lack of progress, which he believed was in stark contrast to the dramatic advances in the natural sciences since the Renaissance. Among Moore's most famous works are his book "Principia Ethica", and his essays, "The Refutation of Idealism", "A Defence of Common Sense", and "A Proof of the External World".

Moore was an important and much admired member of the secretive Cambridge Apostles, a discussion group with members drawn from the British intellectual elite. At the time another member, a 22-year-old Bertrand Russell, wrote “I almost worship him as if he were a god. I have never felt such an extravagant admiration for anybody.” 

From 1918–19 he was president of the Aristotelian Society, a group committed to the systematic study of philosophy, its historical development and its methods and problems.

G. E. Moore died at the Evelyn Nursing Home on 24 October 1958; he was cremated at Cambridge Crematorium on 28 October 1958 and his ashes interred at the Parish of the Ascension Burial Ground in Cambridge; his wife, Dorothy Ely (1892-1977) was buried there. Together they had two sons, the poet Nicholas Moore and the composer Timothy Moore.

His influential work "Principia Ethica" is one of the main inspirations of the movement against ethical naturalism (see ethical non-naturalism) and is partly responsible for the twentieth-century concern with meta-ethics.

Moore asserted that philosophical arguments can suffer from a confusion between the use of a term in a particular argument and the definition of that term (in all arguments). He named this confusion the naturalistic fallacy. For example, an ethical argument may claim that if a thing has certain properties, then that thing is 'good.' A hedonist may argue that 'pleasant' things are 'good' things. Other theorists may argue that 'complex' things are 'good' things. Moore contends that, even if such arguments are correct, they do not provide definitions for the term 'good'. The property of 'goodness' cannot be defined. It can only be shown and grasped. Any attempt to define it (X is good if it has property Y) will simply shift the problem (Why is Y-ness good in the first place?).

Moore's argument for the indefinability of 'good' (and thus for the fallaciousness in the "naturalistic fallacy") is often called the open-question argument; it is presented in §13 of "Principia Ethica". The argument hinges on the nature of statements such as "Anything that is pleasant is also good" and the possibility of asking questions such as "Is it "good" that x is pleasant?". According to Moore, these questions are "open" and these statements are "significant"; and they will remain so no matter what is substituted for "pleasure". Moore concludes from this that any analysis of value is bound to fail. In other words, if value could be analysed, then such questions and statements would be trivial and obvious. Since they are anything but trivial and obvious, value must be indefinable.

Critics of Moore's arguments sometimes claim that he is appealing to general puzzles concerning analysis (cf. the paradox of analysis), rather than revealing anything special about value. The argument clearly depends on the assumption that if 'good' were definable, it would be an analytic truth about 'good', an assumption that many contemporary moral realists like Richard Boyd and Peter Railton reject. Other responses appeal to the Fregean distinction between sense and reference, allowing that value concepts are special and "sui generis", but insisting that value properties are nothing but natural properties (this strategy is similar to that taken by non-reductive materialists in philosophy of mind).

Moore contended that goodness cannot be analysed in terms of any other property. In "Principia Ethica", he writes:

Therefore, we cannot define 'good' by explaining it in other words. We can only point to a "thing" or an "action" and say "That is good." Similarly, we cannot describe to a person born totally blind exactly what yellow is. We can only show a sighted person a piece of yellow paper or a yellow scrap of cloth and say "That is yellow."

In addition to categorising 'good' as indefinable, Moore also emphasized that it is a non-natural property. This means that it cannot be empirically or scientifically tested or verifiedit is not within the bounds of "natural science".

Moore argued that, once arguments based on the naturalistic fallacy had been discarded, questions of intrinsic goodness could be settled only by appeal to what he (following Sidgwick) called "moral intuitions": self-evident propositions which recommend themselves to moral reflection, but which are not susceptible to either direct proof or disproof ("Principia", § 45). As a result of his view, he has often been described by later writers as an advocate of ethical intuitionism. Moore, however, wished to distinguish his view from the views usually described as "Intuitionist" when "Principia Ethica" was written:

Moore distinguished his view from the view of deontological intuitionists, who held that "intuitions" could determine questions about what "actions" are right or required by duty. Moore, as a consequentialist, argued that "duties" and moral rules could be determined by investigating the "effects" of particular actions or kinds of actions ("Principia", § 89), and so were matters for empirical investigation rather than direct objects of intuition (Prncipia, § 90). On Moore's view, "intuitions" revealed not the rightness or wrongness of specific actions, but only what things were good in themselves, as "ends to be pursued".

One of the most important parts of Moore's philosophical development was his break from the idealism that dominated British philosophy (as represented in the works of his former teachers F. H. Bradley and John McTaggart), and his defence of what he regarded as a "common sense" form of realism. In his 1925 essay "A Defence of Common Sense", he argued against idealism and scepticism toward the external world, on the grounds that they could not give reasons to accept that their metaphysical premises were more plausible than the reasons we have for accepting the common sense claims about our knowledge of the world, which sceptics and idealists must deny. He famously put the point into dramatic relief with his 1939 essay "Proof of an External World", in which he gave a common sense argument against scepticism by raising his right hand and saying "Here is one hand" and then raising his left and saying "And here is another", then concluding that there are at least two external objects in the world, and therefore that he knows (by this argument) that an external world exists. Not surprisingly, not everyone inclined to sceptical doubts found Moore's method of argument entirely convincing; Moore, however, defends his argument on the grounds that sceptical arguments seem invariably to require an appeal to "philosophical intuitions" that we have considerably less reason to accept than we have for the common sense claims that they supposedly refute. (In addition to fueling Moore's own work, the "Here is one hand" argument also deeply influenced Wittgenstein, who spent his last years working out a new approach to Moore's argument in the remarks that were published posthumously as "On Certainty".)

Moore is also remembered for drawing attention to the peculiar inconsistency involved in uttering a sentence such as "It is raining, but I do not believe it is raining", a puzzle now commonly called "Moore's paradox". The puzzle arises because it seems impossible for anyone to consistently "assert" such a sentence; but there doesn't seem to be any "logical contradiction" between "It is raining" and "I don't believe that it is raining", because the former is a statement about the weather and the latter a statement about a person's belief about the weather, and it is perfectly logically possible that it may rain whilst a person does not believe that it is raining.

In addition to Moore's own work on the paradox, the puzzle also inspired a great deal of work by Ludwig Wittgenstein, who described the paradox as the most impressive philosophical insight that Moore had ever introduced. It is said that when Wittgenstein first heard this paradox one evening (which Moore had earlier stated in a lecture), he rushed round to Moore's lodgings, got him out of bed and insisted that Moore repeat the entire lecture to him.

Moore's description of the principle of organic unity is extremely straightforward, nonetheless, and a variant on a pattern that began with Aristotle:

According to Moore, a moral actor cannot survey the 'goodness' inherent in the various parts of a situation, assign a value to each of them, and then generate a sum in order to get an idea of its total value. A moral scenario is a complex assembly of parts, and its total value is often created by the relations between those parts, and not by their individual value. The organic metaphor is thus very appropriate: biological organisms seem to have emergent properties which cannot be found anywhere in their individual parts. For example, a human brain seems to exhibit a capacity for thought when none of its neurons exhibit any such capacity. In the same way, a moral scenario can have a value far greater than the sum of its component parts.

To understand the application of the organic principle to questions of value, it is perhaps best to consider Moore's primary example, that of a consciousness experiencing a beautiful object. To see how the principle works, a thinker engages in "reflective isolation", the act of isolating a given concept in a kind of null-context and determining its intrinsic value. In our example, we can easily see that, of themselves, beautiful objects and consciousnesses are not particularly valuable things. They might have some value, but when we consider the total value of a consciousness experiencing a beautiful object, it seems to exceed the simple sum of these values. Hence the value of a whole must not be assumed to be the same as the sum of the values of its parts.





</doc>
<doc id="11964" url="https://en.wikipedia.org/wiki?curid=11964" title="Genus–differentia definition">
Genus–differentia definition

A genus–differentia definition is a type of intensional definition, and it is composed of two parts:

For example, consider these two definitions:
Those definitions can be expressed as one genus and two "differentiae":

The use of genus and differentia in constructing definitions goes back at least as far as Aristotle (384–322 BCE).

The process of producing new definitions by "extending" existing definitions is commonly known as differentiation (and also as derivation). The reverse process, by which just part of an existing definition is used itself as a new definition, is called abstraction; the new definition is called "an abstraction" and it is said to have been "abstracted away from" the existing definition.

For instance, consider the following:
A part of that definition may be singled out (using parentheses here):
and with that part, an abstraction may be formed:
Then, the definition of "a square" may be recast with that abstraction as its genus:

Similarly, the definition of "a square" may be rearranged and another portion singled out:
leading to the following abstraction:
Then, the definition of "a square" may be recast with that abstraction as its genus:

In fact, the definition of "a square" may be recast in terms of both of the abstractions, where one acts as the genus and the other acts as the differentia:
Hence, abstraction is crucial in simplifying definitions.

When multiple definitions could serve equally well, then all such definitions apply simultaneously. Thus, "a square" is a member of both the genus "[a] rectangle" and the genus "[a] rhombus". In such a case, it is notationally convenient to consolidate the definitions into one definition that is expressed with multiple genera (and possibly no differentia, as in the following):
or completely equivalently:

More generally, a collection of formula_1 equivalent definitions (each of which is expressed with one unique genus) can be recast as one definition that is expressed with formula_2 genera. Thus, the following:
could be recast as:

A genus of a definition provides a means by which to specify an "is-a relationship":
The non-genus portion of the differentia of a definition provides a means by which to specify a "has-a relationship":

When a system of definitions is constructed with genera and differentiae, the definitions can be thought of as nodes forming a hierarchy or—more generally—a directed acyclic graph; a node that has no predecessor is "a most general definition"; each node along a directed path is "more differentiated (or "more derived) than any one of its predecessors, and a node with no successor is "a most differentiated" (or "a most derived") definition.

When a definition, "S", is the tail of each of its successors (that is, "S" has at least one successor and each direct successor of "S" is a most differentiated definition), then "S" is often called "the species of each of its successors, and each direct successor of "S" is often called "an individual (or "an entity") of the species "S"; that is, the genus of an individual is synonymously called "the species" of that individual. Furthermore, the differentia of an individual is synonymously called "the identity" of that individual. For instance, consider the following definition:
In this case:

As in that example, the identity itself (or some part of it) is often used to refer to the entire individual, a phenomenon that is known in linguistics as a "pars pro toto synecdoche".


</doc>
<doc id="11966" url="https://en.wikipedia.org/wiki?curid=11966" title="Firearm">
Firearm

A firearm is a type of gun designed to be readily carried and used by a single individual. The term is legally defined further in different countries (see Legal definitions). 

The first primitive firearms originated in 10th-century China, when bamboo tubes containing gunpowder and pellet projectiles were mounted on spears to make the portable fire lance, operable by a single person, which was later used to good effect in the Siege of De'an in 1132. In the 13th century, the Chinese invented the metal-barreled hand cannon, widely considered the true ancestor of all firearms. The technology gradually spread throughout the rest of East Asia, South Asia, the Middle East, and Europe. Older firearms typically used black powder as a propellant, but modern firearms use smokeless powder or other propellants. Most modern firearms (with the notable exception of smoothbore shotguns) have rifled barrels to impart spin to the projectile for improved flight stability.

Modern firearms can be described by their caliber (i.e. bore diameter). For pistols and rifles this is given in millimeters or inches (e.g. 7.62mm or .308 in.), or in the case of shotguns by their gauge (e.g. 12 ga. and 20 ga.). They are also described by the type of action employed (e.g. muzzleloader, breechloader, lever, bolt, pump, revolver, semi-automatic, fully automatic, etc.), together with the usual means of deportment (i.e. hand-held or mechanical mounting). Further classification may make reference to the type of barrel used (i.e. rifled) and to the barrel length (e.g. 24 inches), to the firing mechanism (e.g. matchlock, wheellock, flintlock, or percussion lock), to the design's primary intended use (e.g. hunting rifle), or to the commonly accepted name for a particular variation (e.g. Gatling gun).

Shooters aim firearms at their targets with hand-eye coordination, using either iron sights or optical sights. The accurate range of pistols generally does not exceed , while most rifles are accurate to using iron sights, or to longer ranges whilst using optical sights. (Firearm rounds may be dangerous or lethal well beyond their accurate range; the minimum distance for safety is much greater than the specified range for accuracy). Purpose-built sniper rifles and anti-materiel rifles are accurate to ranges of more than .

A firearm is a barreled ranged weapon that inflicts damage on targets by launching one or more projectiles driven by rapidly expanding high-pressure gas produced by exothermic combustion (deflagration) of a chemical propellant, historically "gunpowder", now smokeless powder. 

In the military, firearms (also referred to as "small arms") are categorized into "heavy" and "light" weapons regarding their portability by foot soldiers. These include any kinetic-projectile firearm small and light enough to be carried and operated to full combat efficiency by a single infantryman. Such firearms include handguns such as revolvers, pistols and derringers, and long guns such as rifles (of which there are many subtypes such as anti-material rifles, sniper rifles, designated marksman rifles, battle rifles, assault rifles and carbines), shotguns, submachine guns, personal defense weapons, squad automatic weapons and light machine guns.

Among the world's arms manufacturers, the top firearms manufacturers are Browning, Remington, Colt, Ruger, Smith & Wesson, Savage, Mossberg (USA), Heckler & Koch, SIG Sauer, Walther (Germany), ČZUB (Czech Republic), Glock, Steyr-Mannlicher (Austria), FN Herstal (Belgium), Beretta (Italy), Norinco (China), Tula Arms and Kalashnikov (Russia), while former top producers included Mauser, Springfield Armory, and Rock Island Armory under Armscor (Philippines).

The smallest of all firearms is the handgun.

In South African law, a 'handgun' means a pistol or revolver which can be held in and discharged with one hand. In Australia, law considers as a handgun a firearm carry-able or concealable about the person; or
capable of being raised and fired by one hand; or not exceeding 65 centimeters

There are two common types of handguns: revolvers and semi-automatic pistols. Revolvers have a number of firing chambers or "charge holes" in a revolving cylinder; each chamber in the cylinder is loaded with a single cartridge or charge. Semi-automatic pistols have a single fixed firing-chamber machined into the rear of the barrel, and a magazine so they can be used to fire more than one round. Each press of the trigger fires a cartridge, using the energy of the cartridge to activate a mechanism so that the next cartridge may be fired immediately. This is opposed to "double-action" revolvers, which accomplish the same end using a mechanical action linked to the trigger pull.

With the invention of the revolver in 1818, handguns capable of holding multiple rounds became popular. Certain designs of auto-loading pistol appeared beginning in the 1870s and had largely supplanted revolvers in military applications by the end of World War I. By the end of the 20th century, most handguns carried regularly by military, police and civilians were semi-automatic, although revolvers were still widely used. Generally speaking, military and police forces use semi-automatic pistols due to their high magazine capacities and ability to rapidly reload by simply removing the empty magazine and inserting a loaded one. Revolvers are very common among handgun hunters because revolver cartridges are usually more powerful than similar caliber semi-automatic pistol cartridges (which are designed for self-defense) and the strength, simplicity and durability of the revolver design is well-suited to outdoor use. Revolvers, especially in .22 LR and 38 Special/357 Magnum, are also common concealed weapons in jurisdictions allowing this practice because their simple mechanics make them smaller than many autoloaders while remaining reliable. Both designs are common among civilian gun owners, depending on the owner's intention (self-defense, hunting, target shooting, competitions, collecting, etc.).

A long gun is generally any firearm that is larger than a handgun and is designed to be held and fired with both hands, while braced against either the hip or the shoulder for better stability. Long guns typically have a barrel length from (there are restrictions on minimum barrel length in many jurisdictions; maximum barrel length is usually a matter of practicality), that along with the receiver and trigger group is mounted into a wood, plastic, metal or composite "stock", composed of one or more pieces that form a foregrip, rear grip, and optionally (but typically) a shoulder mount called the "butt". Early long arms, from the Renaissance up to the mid-19th century, were generally smoothbore firearms that fired one or more ball shot, called muskets or arquebus depending on caliber and firing mechanism.

Most modern long guns are either rifles or shotguns. Both are the successors of the musket, diverging from their parent weapon in distinct ways. A rifle is so named for the spiral fluting (rifling) machined into the inner surface of its barrel, which imparts a self-stabilizing spin to the single bullets it fires. Shotguns are predominantly smoothbore firearms designed to fire a number of "shot"; pellet sizes commonly ranging between 2 mm #9 birdshot and 8.4 mm #00 (double-aught) buckshot. Shotguns are also capable of firing single slugs, or specialty (often "less lethal") rounds such as bean bags, tear gas or breaching rounds. Rifles have a very small impact area but a long range and high accuracy. Shotguns have a large impact area with considerably less range and accuracy. However, the larger impact area can compensate for reduced accuracy, since shot spreads during flight; consequently, in hunting, shotguns are generally used for flying game.

Rifles and shotguns are commonly used for hunting and often to defend a home or place of business. Usually, large game are hunted with rifles (although shotguns can be used, particularly with slugs), while birds are hunted with shotguns. Shotguns are sometimes preferred for defending a home or business due to their wide impact area, multiple wound tracks (when using buckshot), shorter range, and reduced penetration of walls (when using lighter shot), which significantly reduces the likelihood of unintended harm, although the handgun is also common.
There are a variety of types of rifles and shotguns based on the method they are reloaded. Bolt-action and lever-action rifles are manually operated. Manipulation of the bolt or the lever causes the spent cartridge to be removed, the firing mechanism recocked, and a fresh cartridge inserted. These two types of action are almost exclusively used by rifles. Slide-action (commonly called 'pump-action') rifles and shotguns are manually cycled by shuttling the foregrip of the firearm back and forth. This type of action is typically used by shotguns, but several major manufacturers make rifles that use this action.

Both rifles and shotguns also come in break-action varieties that do not have any kind of reloading mechanism at all but must be hand-loaded after each shot. Both rifles and shotguns come in single- and double-barreled varieties; however due to the expense and difficulty of manufacturing, double-barreled rifles are rare. Double-barreled rifles are typically intended for African big-game hunts where the animals are dangerous, ranges are short, and speed is of the essence. Very large and powerful calibers are normal for these firearms.

Rifles have been in nationally featured marksmanship events in Europe and the United States since at least the 18th century, when rifles were first becoming widely available. One of the earliest purely "American" rifle-shooting competitions took place in 1775, when Daniel Morgan was recruiting sharpshooters in Virginia for the impending American Revolutionary War. In some countries, rifle marksmanship is still a matter of national pride. Some specialized rifles in the larger calibers are claimed to have an accurate range of up to about , although most have considerably less. In the second half of the 20th century, competitive shotgun sports became perhaps even more popular than riflery, largely due to the motion and immediate feedback in activities such as skeet, trap and sporting clays.

In military use, bolt-action rifles with high-power scopes are common as sniper rifles, however by the Korean War the traditional bolt-action and semi-automatic rifles used by infantrymen had been supplemented by select-fire designs known as automatic rifles.

A carbine is a firearm similar to a rifle in form and intended usage, but generally shorter or smaller than the typical "full-size" hunting or battle rifle of a similar time period, and sometimes using a smaller or less-powerful cartridge. Carbines were and are typically used by members of the military in roles that are expected to engage in combat, but where a full-size rifle would be an impediment to the primary duties of that soldier (vehicle drivers, field commanders and support staff, airborne troops, engineers, etc.). Carbines are also common in law enforcement and among civilian owners where similar size, space and/or power concerns may exist. Carbines, like rifles, can be single-shot, repeating-action, semi-automatic or select-fire/fully automatic, generally depending on the time period and intended market. Common historical examples include the Winchester Model 1892, Lee–Enfield "Jungle Carbine", SKS, M1 carbine (no relation to the larger M1 Garand) and M4 carbine (a more compact variant of the current M16 rifle). Modern U.S. civilian carbines include compact customizations of the AR-15, Ruger Mini-14, Beretta Cx4 Storm, Kel-Tec SUB-2000, bolt-action rifles generally falling under the specifications of a scout rifle, and aftermarket conversion kits for popular pistols including the M1911 and Glock models.

A machine gun is a fully automatic firearm, most often separated from other classes of automatic weapons by the use of belt-fed ammunition (though some designs employ drum, pan or hopper magazines), generally in a rifle-inspired caliber ranging between 5.56×45mm NATO (.223 Remington) for a light machine gun to as large as .50 BMG or even larger for crewed or aircraft weapons. Although not widely fielded until World War I, early machine guns were being used by militaries in the second half of the 19th century. Notables in the U.S. arsenal during the 20th century included the M2 Browning .50 caliber heavy machine gun, M1919 Browning .30 caliber medium machine gun, and the M60 7.62×51mm NATO general-purpose machine gun which came into use around the Vietnam War. Machine guns of this type were originally defensive firearms crewed by at least two men, mainly because of the difficulties involved in moving and placing them, their ammunition, and their tripod. In contrast, modern light machine guns such as the FN Minimi are often wielded by a single infantryman. They provide a large ammunition capacity and a high rate of fire, and are typically used to give suppressing fire during infantry movement. Accuracy on machine guns varies based on a wide number of factors from design to manufacturing tolerances, most of which have been improved over time. Machine guns are often mounted on vehicles or helicopters, and have been used since World War I as offensive firearms in fighter aircraft and tanks (e.g. for air combat or suppressing fire for ground troop support).

The definition of machine gun is different in U.S. law. The National Firearms Act and Firearm Owners Protection Act define a "machine gun" in the United States code "Title 26, Subtitle E, Chapter 53, Subchapter B, Part 1, § 5845" as:
"... any firearm which shoots ... automatically more than one shot, without manual reloading, by a single function of the trigger". "Machine gun" is therefore largely synonymous with "automatic weapon" in the U.S. civilian parlance, covering all automatic firearms.

The definition of a sniper rifle is disputed among military, police and civilian observers alike, however most generally define a “sniper rifle” as a high powered, semi-automatic/bolt action, precision rifle with an accurate range further than that of a standard rifle. These are often purpose-built for their applications. For example, a police sniper rifle may differ in specs from a military rifle. Police snipers generally do not engage targets at extreme range, but rather, a target at medium range. They may also have multiple targets within the shorter range, and thus a semi-automatic model is preferred to a bolt action. They also may be more compact than milspec rifles as police marksmen may need more portability. On the other hand, a military rifle is more likely to use a higher powered cartridge to defeat body armor or medium-light cover. They are more commonly (but not a lot more) bolt-action, as they are simpler to build and maintain. Also, due to fewer moving and overall parts, they are much more reliable under adverse conditions. They may also have a more powerful scope to acquire targets further away. Overall, sniper units never became prominent until World War 1, when the Germans displayed their usefulness on the battlefield. Since then, they have become irrevocably embedded in warfare. Examples of sniper rifles include the Accuracy International AWM, Sako TRG-42 and the CheyTac M200. Examples of specialized sniper cartridges include the .338 Lapua Magnum, .300 Winchester Magnum, and .408 CheyTac rounds.

A submachine gun is a magazine-fed firearm, usually smaller than other automatic firearms, that fires pistol-caliber ammunition; for this reason certain submachine guns can also be referred to as "machine pistols", especially when referring to handgun-sized designs such as the Škorpion vz. 61 and Glock 18. Well-known examples are the Israeli Uzi and Heckler & Koch MP5 which use the 9×19mm Parabellum cartridge, and the American Thompson submachine gun which fires .45 ACP. Because of their small size and limited projectile penetration compared to high-power rifle rounds, submachine guns are commonly favored by military, paramilitary and police forces for close-quarters engagements such as inside buildings, in urban areas or in trench complexes.

Submachine guns were originally about the size of carbines. Because they fire pistol ammunition, they have limited long-range use, but in close combat can be used in fully automatic in a controllable manner due to the lighter recoil of the pistol ammunition. They are also extremely inexpensive and simple to build in time of war, enabling a nation to quickly arm its military. In the latter half of the 20th century, submachine guns were being miniaturized to the point of being only slightly larger than some large handguns. The most widely used submachine gun at the end of the 20th century was the Heckler & Koch MP5. The MP5 is actually designated as a "machine pistol" by Heckler & Koch (MP5 stands for "Maschinenpistole 5", or Machine Pistol 5), although some reserve this designation for even smaller submachine guns such as the MAC-10 and Glock 18, which are about the size and shape of pistols.

An automatic rifle is a magazine-fed firearm, wielded by a single infantryman, that is chambered for rifle cartridges and capable of automatic fire. The M1918 Browning Automatic Rifle was the first U.S. infantry weapon of this type, and was generally used for suppressive or support fire in the role now usually filled by the light machine gun. Other early automatic rifles include the Fedorov Avtomat and the Huot Automatic Rifle. Later, German forces fielded the Sturmgewehr 44 during World War II, a light automatic rifle firing a reduced power "intermediate cartridge". This design was to become the basis for the "assault rifle" subclass of automatic weapons, as contrasted with "battle rifles", which generally fire a traditional "full-power" rifle cartridge.

In World War II, Germany introduced the StG 44, and brought to the forefront of firearm technology what eventually became the class of firearm most widely adopted by the military, the assault rifle. An assault rifle is usually slightly smaller than a battle rifle such as the American M14, but the chief differences defining an assault rifle are select-fire capability and the use of a rifle round of lesser power, known as an intermediate cartridge.

Soviet engineer Mikhail Kalashnikov quickly adapted the German concept, using a less-powerful 7.62×39mm cartridge derived from the standard 7.62×54mmR Russian battle rifle round, to produce the AK-47, which has become the world's most widely used assault rifle. Soon after World War II, the Automatic Kalashnikov AK-47 assault rifle began to be fielded by the Soviet Union and its allies in the Eastern Bloc, as well as by nations such as China, North Korea, and North Vietnam.

In the United States, the assault rifle design was later in coming; the replacement for the M1 Garand of WWII was another John Garand design chambered for the new 7.62×51mm NATO cartridge; the select-fire M14, which was used by the U.S. military until the 1960s. The significant recoil of the M14 when fired in full-automatic mode was seen as a problem as it reduced accuracy, and in the 1960s it was replaced by Eugene Stoner's AR-15, which also marked a switch from the powerful .30 caliber cartridges used by the U.S. military up until early in the Vietnam War to the much less powerful but far lighter and light recoiling .223 caliber (5.56mm) intermediate cartridge. The military later designated the AR-15 as the "M16". The civilian version of the M16 continues to be known as the AR-15 and looks exactly like the military version, although to conform to B.A.T.F.E. regulations in the U.S., it lacks the mechanism that permits fully automatic fire.

Variants of both of the M16 and AK-47 are still in wide international use today, though other automatic rifle designs have since been introduced. A smaller version of the M16A2, the M4 carbine, is widely used by U.S. and NATO tank and vehicle crews, airbornes, support staff, and in other scenarios where space is limited. The IMI Galil, an Israeli-designed weapon based on the action of the AK-47, is in use by Israel, Italy, Burma, the Philippines, Peru, and Colombia. Swiss Arms of Switzerland produces the SIG SG 550 assault rifle used by France, Chile, and Spain among others, and Steyr Mannlicher produces the AUG, a bullpup rifle in use in Austria, Australia, New Zealand, Ireland, and Saudi Arabia among other nations.

Modern designs call for compact weapons retaining firepower. The bullpup design, by mounting the magazine behind the trigger, unifies the accuracy and firepower of the traditional assault rifle with the compact size of the submachine gun (though submachine guns are still used); examples are the French FAMAS and the British SA80.

A recently developed class of firearm is the personal defense weapon or PDW, which is in simplest terms a submachine gun designed to fire ammunitions with ballistic performance similar to rifle cartridges. While a submachine gun is desirable for its compact size and ammunition capacity, its pistol cartridges lack the penetrating capability of a rifle round. Conversely, rifle bullets can pierce light armor and are easier to shoot accurately, but even a carbine such as the Colt M4 is larger and/or longer than a submachine gun, making it harder to maneuver in close quarters. The solution many firearms manufacturers have presented is a weapon resembling a submachine gun in size and general configuration, but which fires a higher-powered armor-penetrating round (often specially designed for the weapon), thus combining the advantages of a carbine and submachine gun. This also earned the PDWs an infrequently used nickname — the submachine carbines. The FN P90 and Heckler & Koch MP7 are most famous examples of PDWs.

Battle rifles are another subtype of rifle, usually defined as selective fire rifles that use full power rifle cartridges, examples of which include the 7.62x51mm NATO, 7.92x57mm Mauser, and 7.62x54mmR. These serve similar purposes to assault rifles, as they both are usually employed by ground infantry. However, some prefer battle rifles due to their more powerful cartridge, despite added recoil. Some semi-automatic sniper rifles are configured from battle rifles.

Firearms are also categorized by their functioning cycle or "action" which describes its loading, firing, and unloading cycle.

The earliest evolution of the firearm, there are many types of manual action firearms. These can be divided into two basic categories: single shot and repeating.

A single shot firearm can only be fired once per equipped barrel before it must be reloaded or charged via an external mechanism or series of steps. A repeating firearm can be fired multiple times, but can only be fired once with each subsequent pull of the trigger. Between trigger pulls, the firearm's action must be reloaded or charged via an internal mechanism.

A gun which has a lever that is pulled down then back up to load.

Pump action weapons are primarily shotguns. A pump action is created when the user slides a lever (usually a grip) and it brings a new round in the chamber while expelling the old one.

A semi-automatic, self-loading, or "auto loader" firearm is one that performs all steps necessary to prepare it for firing again after a single discharge, until cartridges are no longer available in the weapon's feed device or magazine. Auto loaders fire one round with each pull of the trigger. Some people confuse the term with "fully automatic" firearms. (See next.) While some semi-automatic rifles may resemble military-style firearms, they are not properly classified "Assault Weapons" which refers to those that continue to fire until the trigger is no longer depressed.

An "automatic" firearm, or "fully automatic", "fully auto", or "full auto", is generally defined as one that continues to load and fire cartridges from its magazine as long as the trigger is depressed (and until the magazine is depleted of available ammunition.) The first weapon generally considered in this category is the Gatling gun, originally a carriage-mounted, crank-operated firearm with multiple rotating barrels that was fielded in the American Civil War. The modern trigger-actuated machine gun began with various designs developed in the late 19th century and fielded in World War I, such as the Maxim gun, Lewis Gun, and MG 08 "Spandau". Most automatic weapons are classed as long guns (as the ammunition used is of similar type as for rifles, and the recoil of the weapon's rapid fire is better controlled with two hands), but handgun-sized automatic weapons also exist, generally in the "submachine gun" or "machine pistol" class.

Selective fire, or "select fire", means the capability of a weapon's fire control to be adjusted in either semi-automatic, fully automatic firing modes, or 3 round burst. The modes are chosen by means of a selector, which varies depending on the weapon's design. Some selective-fire weapons have burst fire mechanisms built in to limit the maximum number of shots fired in fully automatic mode, with most common limits being two or three rounds per trigger pull. The presence of selective-fire modes on firearms allows more efficient use of ammunition for specific tactical needs, either precision-aimed or suppressive fire. This capability is most commonly found on military weapons of the 20th and 21st centuries, most notably the assault rifles.

The first primitive firearms were invented about 1250 AD in China when the man-portable fire lance (a bamboo or metal tube that could shoot ignited gunpowder) was combined with projectiles such as scrap metal, broken porcelain, or darts/arrows.

The earliest depiction of a firearm is a sculpture from a cave in Sichuan, China. The sculpture dates to the 12th century and is of a figure carrying a vase-shaped bombard, with flames and a cannonball coming out of it. The oldest surviving gun, a hand cannon made of bronze, has been dated to 1288 because it was discovered at a site in modern-day Acheng District, Heilongjiang, China, where the "Yuan Shi" records that battles were fought at that time. The firearm had a 6.9 inch barrel of a 1-inch diameter, a 2.6 inch chamber for the gunpowder and a socket for the firearm's handle. It is 13.4 inches long and 7.8 pounds without the handle, which would have been made of wood.

The Arabs and Mamluks had firearms in the late 13th century. In the 14th century, firearms were obtained by the Europeans. The Koreans adopted firearms from the Chinese in the 14th century. The Iranians (first Aq Qoyunlu and Safavids) and Indians (first Mughals) all got them no later than the 15th century, from the Ottoman Turks. The people of Nusantara archipelago of Southeast Asia used long arquebus at least by the last quarter of 15th century.

Eventhough the knowledge of making gunpowder-based weapon in Nusantara archipelago has been known after the failed Mongol invasion of Java (1293), and the predecessor of firearms, the pole gun (bedil tombak), was recorded as being used by Java in 1413, the knowledge of making "true" firearms came much later, after the middle of 15th century. It was brought by the Islamic nations of West Asia, most probably the Arabs. The precise year of introduction is unknown, but it may be safely concluded to be no earlier than 1460. Before the arrival of the Portuguese in Southeast Asia, the natives already possessed primitive firearms, the Java arquebus.

The technology of firearm in Southeast Asia further improved after the Portuguese capture of Malacca (1511). Starting in the 1513, the tradition of German-Bohemian gun making were merged with Turkish gun making traditions. This resulted in Indo-Portuguese tradition of matchlocks. Indian craftsmen modified the design by introducing a very short, almost pistol-like buttstock held against the cheek, not the shoulder, when aiming. They also reduced the caliber and made the gun lighter and more balanced. This was a hit with the Portuguese who did a lot of fighting aboard ship and on river craft, and valued a more compact gun. The Malay gunfounders, compared as being in the same level with those of Germany, quickly adapted these new firearms, and thus a new type of arquebus, the istinggar, appeared. The Japanese did not acquire firearms until the 16th century, and then from the Portuguese rather than the Chinese.
The development behind firearms accelerated during the 19th and 20th centuries. Breech-loading became more or less a universal standard for the reloading of most hand-held firearms and continues to be so with some notable exceptions (such as mortars). Instead of loading individual rounds into weapons, magazines holding multiple munitions were adopted—these aided rapid reloading. Automatic and semi-automatic firing mechanisms meant that a single soldier could fire many more rounds in a minute than a vintage weapon could fire over the course of a battle. Polymers and alloys in firearm construction made weaponry progressively lighter and thus easier to deploy. Ammunition changed over the centuries from simple metallic ball-shaped projectiles that rattled down the barrel to bullets and cartridges manufactured to high precision. Especially in the past century has particular attention been devoted to accuracy and sighting to make firearms altogether far more accurate than ever before. More than any single factor though, firearms have proliferated due to the advent of mass production—enabling arms manufacturers to produce large quantities of weaponry to a consistent standard.

Velocities of bullets increased with the use of a "jacket" of a metal such as copper or copper alloys that covered a lead core and allowed the bullet to glide down the barrel more easily than exposed lead. Such bullets are designated as "full metal jacket" (FMJ). Such FMJ bullets are less likely to fragment on impact and are more likely to traverse through a target while imparting less energy. Hence, FMJ bullets impart less tissue damage than non-jacketed bullets that expand. (Dougherty and Eidt, 2009) This led to their adoption for military use by countries adhering to the Hague Convention in 1899.

That said, the basic principle behind firearm operation remains unchanged to this day. A musket of several centuries ago is still similar in principle to a modern-day assault rifle—using the expansion of gases to propel projectiles over long distances—albeit less accurately and rapidly.

The Chinese fire lance from the 10th century was the direct predecessor to the modern concept of the firearm. It was not a gun itself, but an addition to the soldiers' spears. Originally it consisted of paper or bamboo barrels that would have incendiary gunpowder within it, that could be lit one time and would project flames at the enemy. Sometimes the Chinese troops would place small projectiles within the barrel that would also be projected when the gunpowder was lit, but most of the explosive force would create flames. Later, the barrel was changed to be made of metal, so that a more explosive gunpowder could be used and put more force into the propulsion of the projectile.

The original predecessor of all firearms, the Chinese fire lance and hand cannon were loaded with gunpowder and the shot (initially lead shot, later replaced by cast iron) through the muzzle, while a fuse was placed at the rear. This fuse was lit, causing the gunpowder to ignite and propel the cannonball. In military use, the standard hand cannon was tremendously powerful, while also being somewhat useless due to relative inability of the gunner to aim the weapon, or control the ballistic properties of the projectile. Recoil could be absorbed by bracing the barrel against the ground using a wooden support, the forerunner of the stock. Neither the quality or amount of gunpowder, nor the consistency in projectile dimensions were controlled, with resulting inaccuracy in firing due to windage, variance in gunpowder composition, and the difference in diameter between the bore and the shot. The hand cannons were replaced by lighter carriage-mounted artillery pieces, and ultimately the arquebus.

Muzzle-loading muskets (smooth-bored long guns) were among the first firearms developed. The firearm was loaded through the muzzle with gunpowder, optionally some wadding and then a bullet (usually a solid lead ball, but musketeers could shoot stones when they ran out of bullets). Greatly improved muzzleloaders (usually rifled instead of smooth-bored) are manufactured today and have many enthusiasts, many of whom hunt large and small game with their guns. Muzzleloaders have to be manually reloaded after each shot; a skilled archer could fire multiple arrows faster than most early muskets could be reloaded and fired, although by the mid-18th century, when muzzleloaders became the standard small armament of the military, a well-drilled soldier could fire six rounds in a minute using prepared cartridges in his musket. Before then, effectiveness of muzzleloaders was hindered by both the low reloading speed and, before the firing mechanism was perfected, the very high risk posed by the firearm to the person attempting to fire it.

One interesting solution to the reloading problem was the "Roman Candle Gun" with superposed loads. This was a muzzleloader in which multiple charges and balls were loaded one on top of the other, with a small hole in each ball to allow the subsequent charge to be ignited after the one ahead of it was ignited. It was neither a very reliable nor popular firearm, but it enabled a form of "automatic" fire long before the advent of the machine gun.

Most early firearms were muzzle-loading. This form of loading has several disadvantages, such as a slow rate of fire and having to expose oneself to enemy fire to reload as the weapon had to be pointed upright so the powder could be poured through the muzzle into the breech followed by the ramming the projectile into the breech. As effective methods of sealing the breech were developed through the development of sturdy, weatherproof, self-contained metallic cartridges, muzzle-loaders were replaced by single-shot breech loaders. Eventually single-shot weapons were replaced by the following repeater type weapons.

Many firearms made in the late 19th century through the 1950s used internal magazines to load the cartridge into the chamber of the weapon. The most notable and revolutionary weapons of this period appeared during the U.S. Civil War and they were the Spencer and Henry repeating rifles. Both used fixed tubular magazines, the former having the magazine in the buttstock and the latter under the barrel which allowed a larger capacity. Later weapons used fixed box magazines that could not be removed from the weapon without disassembling the weapon itself. Fixed magazines permitted the use of larger cartridges and eliminated the hazard of having the bullet of one cartridge butting next to the primer or rim of another cartridge. These magazines are loaded while they are in the weapon, often using a stripper clip. A clip is used to transfer cartridges into the magazine. Some notable weapons that use internal magazines include the Mosin–Nagant, the Mauser Kar 98k, the Springfield M1903, the M1 Garand, and the SKS. Firearms that have internal magazines are usually, but not always, rifles. Some exceptions to this include the Mauser C96 pistol, which uses an internal magazine, and the Breda 30, an Italian light machine gun.

Many modern firearms use what are called detachable or box magazines as their method of chambering a cartridge. Detachable magazines can be removed from the weapon without disassembling the firearms, usually by pushing the magazine release.

A belt or ammunition belt is a device used to retain and feed cartridges into a firearm commonly used on machine guns. Belts were originally composed of canvas or cloth with pockets spaced evenly to allow the belt to be mechanically fed into the gun. These designs were prone to malfunctions due to the effects of oil and other contaminants altering the belt. Later belt designs used permanently connected metal links to retain the cartridges during feeding. These belts were more tolerant to exposure to solvents and oil. Some notable weapons that use belts are the M240, the M249, the M134 Minigun, and the PK Machine Gun.

Matchlocks were the first and simplest firearms firing mechanisms developed. Using the matchlock mechanism, the powder in the gun barrel was ignited by a piece of burning cord called a "match". The match was wedged into one end of an S-shaped piece of steel. As the trigger (often actually a lever) was pulled, the match was brought into the open end of a "touch hole" at the base of the gun barrel, which contained a very small quantity of gunpowder, igniting the main charge of gunpowder in the gun barrel. The match usually had to be relit after each firing. The main parts to the matchlock firing mechanism are the pan, match, arm and trigger. A benefit of the pan and arm swivel being moved to the side of the gun was it gave a clear line of fire. An advantage to the matchlock firing mechanism is that it did not misfire. However, it also came with some disadvantages. One disadvantage was if it was raining the match could not be kept lit to fire the weapon. Another issue with the match was it could give away the position of soldiers because of the glow, sound, and smell. While European pistols were equipped with wheellock and flintlock mechanism, Asian pistols were equipped with matchlock mechanism.

The wheellock action, a successor to the matchlock, predated the flintlock. Despite its many faults, the wheellock was a significant improvement over the matchlock in terms of both convenience and safety, since it eliminated the need to keep a smoldering match in proximity to loose gunpowder. It operated using a small wheel much like that on cigarette lighters which was wound up with a key before use and which, when the trigger was pulled, spun against a flint, creating the shower of sparks that ignited the powder in the touch hole. Supposedly invented by Leonardo da Vinci, the Italian Renaissance man, the wheellock action was an innovation that was not widely adopted due to the high cost of the clockwork mechanism.

The flintlock action was a major innovation in firearm design. The spark used to ignite the gunpowder in the touch hole was supplied by a sharpened piece of flint clamped in the jaws of a "cock" which, when released by the trigger, struck a piece of steel called the "frizzen" to create the necessary sparks. (The spring-loaded arm that holds a piece of flint or pyrite is referred to as a cock because of its resemblance to a rooster.) The cock had to be manually reset after each firing, and the flint had to be replaced periodically due to wear from striking the frizzen. (See also flintlock mechanism, snaphance, Miquelet lock) The flintlock was widely used during the 17th, 18th, and 19th centuries in both muskets and rifles.

Percussion caps (caplock mechanisms), coming into wide service in the early 19th century, were a dramatic improvement over flintlocks. With the percussion cap mechanism, the small primer charge of gunpowder used in all preceding firearms was replaced by a completely self-contained explosive charge contained in a small brass "cap". The cap was fastened to the touch hole of the gun (extended to form a "nipple") and ignited by the impact of the gun's "hammer". (The hammer is roughly the same as the cock found on flintlocks except that it does not clamp onto anything.) In the case of percussion caps the hammer was hollow on the end to fit around the cap in order to keep the cap from fragmenting and injuring the shooter.

Once struck, the flame from the cap in turn ignited the main charge of gunpowder, as with the flintlock, but there was no longer any need to charge the touch hole with gunpowder, and even better, the touch hole was no longer exposed to the elements. As a result, the percussion cap mechanism was considerably safer, far more weatherproof, and vastly more reliable (cloth-bound cartridges containing a premeasured charge of gunpowder and a ball had been in regular military service for many years, but the exposed gunpowder in the entry to the touch hole had long been a source of misfires). All muzzleloaders manufactured since the second half of the 19th century use percussion caps except those built as replicas of the flintlock or earlier firearms.

Frenchman Louis-Nicolas Flobert invented the first rimfire metallic cartridge in 1845. His cartridge consisted of a percussion cap with a bullet attached to the top. Flobert then made what he called "parlor guns" for this cartridge, as these rifles and pistols were designed to be shot in indoor shooting parlors in large homes. These 6mm Flobert cartridges, do not contain any powder, the only propellant substance contained in the cartridge is the percussion cap. In English-speaking countries, the 6mm Flobert cartridge corresponds to .22 BB Cap and .22 CB Cap ammunition. These cartridges have a relatively low muzzle velocity of around 700 ft/s (210 m/s).

This was major innovation in firearms ammunition, previously delivered as separate bullets and powder, was combined in a single metallic (usually brass) cartridge containing a percussion cap, powder, and a bullet in one weatherproof package. The main technical advantage of the brass cartridge case was the effective and reliable sealing of high pressure gasses at the breech, as the gas pressure forces the cartridge case to expand outward, pressing it firmly against the inside of the gun barrel chamber. This prevents the leakage of hot gas which could injure the shooter. The brass cartridge also opened the way for modern repeating arms, by uniting the bullet, gunpowder and primer into one assembly that could be fed reliably into the breech by a mechanical action in the firearm.

Before this, a "cartridge" was simply a premeasured quantity of gunpowder together with a ball in a small cloth bag (or rolled paper cylinder), which also acted as wadding for the charge and ball. This early form of cartridge had to be rammed into the muzzleloader's barrel, and either a small charge of gunpowder in the touch hole or an external percussion cap mounted on the touch hole ignited the gunpowder in the cartridge. Cartridges with built-in percussion caps (called "primers") continue to this day to be the standard in firearms. In cartridge-firing firearms, a hammer (or a firing pin struck by the hammer) strikes the cartridge primer, which then ignites the gunpowder within. The primer charge is at the base of the cartridge, either within the rim (a "rimfire" cartridge) or in a small percussion cap embedded in the center of the base (a "centerfire" cartridge). As a rule, centerfire cartridges are more powerful than rimfire cartridges, operating at considerably higher pressures than rimfire cartridges. Centerfire cartridges are also safer, as a dropped rimfire cartridge has the potential to discharge if its rim strikes the ground with sufficient force to ignite the primer. This is practically impossible with most centerfire cartridges.

Nearly all contemporary firearms load cartridges directly into their breech. Some additionally or exclusively load from a magazine that holds multiple cartridges. A magazine is defined as a part of the firearm which exists to store ammunition and assist in its feeding by the action into the breech (such as through the rotation of a revolver's cylinder or by spring-loaded platforms in most pistol and rifle designs). Some magazines, such as that of most centerfire hunting rifles and all revolvers, are internal to and inseparable from the firearm, and are loaded by using a "clip". A clip, often mistakingly used to refer to a detachable "magazine", is a device that holds the ammunition by the rim of the case and is designed to assist the shooter in reloading the firearm's magazine. Examples include revolver speedloaders, the stripper clip used to aid loading rifles such as the Lee–Enfield or Mauser 98, and the en-bloc clip used in loading the M1 Garand. In this sense, "magazines" and "clips", though often used synonymously, refer to different types of devices.

Many firearms are "single shot": i.e., each time a cartridge is fired, the operator must manually re-cock the firearm and load another cartridge. The classic single-barreled shotgun is a good example. A firearm that can load multiple cartridges as the firearm is re-cocked is considered a "repeating firearm" or simply a "repeater". A lever-action rifle, a pump-action shotgun, and most bolt-action rifles are good examples of repeating firearms. A firearm that automatically re-cocks and reloads the next round with each trigger pull is considered a semi-automatic or autoloading firearm.

The first "rapid firing" firearms were usually similar to the 19th century Gatling gun, which would fire cartridges from a magazine as fast as and as long as the operator turned a crank. Eventually, the "rapid" firing mechanism was perfected and miniaturized to the extent that either the recoil of the firearm or the gas pressure from firing could be used to operate it, thus the operator needed only to pull a trigger (which made the firing mechanisms truly "automatic"). An automatic (or "fully automatic") firearm is one that automatically re-cocks, reloads, and fires as long as the trigger is depressed. An automatic firearm is capable of firing multiple rounds with one pull of the trigger. The Gatling gun may have been the first automatic weapon, though the modern trigger-actuated machine gun was not widely introduced until the First World War with the German "Spandau" and British Lewis Gun. Automatic rifles such as the Browning Automatic Rifle were in common use by the military during the early part of the 20th century, and automatic rifles that fired handgun rounds, known as submachine guns, also appeared in this time. Many modern military firearms have a selective fire option, which is a mechanical switch that allows the firearm be fired either in the semi-automatic or fully automatic mode. In the current M16A2 and M16A4 variants of the U.S.-made M16, continuous fully automatic fire is not possible, having been replaced by an automatic burst of three cartridges (this conserves ammunition and increases controllability). Automatic weapons are largely restricted to military and paramilitary organizations, though many automatic designs are infamous for their use by civilians.

Firearm hazard is quite notable, with a significant impact on the health system. In 2001, for quantification purpose, it was estimated that the cost of fatalities and injuries was US$4700 million per year in Canada (US$170 per Canadian) and US$100,000 million per year in the USA (US$300 per American).

In the 52 high- and middle-income countries counting 1400 million population and not engaged in civil conflict, fatalities due to firearm injuries are estimated at 115 000 people a year, in the 1990s

Assault by firearm resulted in 173,000 deaths, globally, in 2015, up from 128,000 deaths in 1990, however this represents a drop in rate from 2.41/100,000 in 1990 to 2.35/100,000 in 2015, as world population has increased by more than two billion. Additionally, there were 32,000 unintentional firearm global deaths in 2015.

In 2017, there were 39,773 gun-related deaths in the United States; over 60% were from firearms. Firearms are the second leading mechanism of injury deaths after motor vehicle accidents.

In those 52 countries, firearm is the first method used for homicide (two thirds) but only the second method for suicide (20%)

To prevent unintentional injury, gun safety training includes education on proper firearm storage and firearm-handling etiquette.

Based on US data, it is estimated that three people are injured for one killed.

A common hazard of repeated firearm use is noise-induced hearing loss (NIHL). NIHL can result from long-term exposure to noise or from high intensity impact noises such as gunshots. Individuals who shoot guns often have a characteristic pattern of hearing loss referred to as "shooters ear". They often have a high frequency loss with better hearing in the low frequencies and one ear is typically worse than the other. The ear on the side the shooter is holding the gun will receive protection from the sound wave from the shoulder while the other ear remains unprotected and more susceptible to the full impact of the sound wave. 

The intensity of a gunshot does vary; lower caliber guns are typically on the softer side while higher caliber guns are often louder. The intensity of a gunshot though typically ranges from 140dB to 175dB. Indoor shooting also causes loud reverberations which can also be as damaging as the actual gunshot itself. According to the National Institute on Deafness and Other Communication Disorders noise above 85dB can begin to cause hearing loss. While many sounds cause damage over time, at the intensity level of a gunshot (140dB or louder), damage to the ear can occur instantly.

Hearing protection is the only way to protect the ears against damage from gunfire as there is no option for the shooter to be further from the sound source or to reduce the intensity to a safe level. If possible, observers should attempt to move away, but hearing protection is often still necessary. Different types of shooters may benefit from different types of hearing protection. When target practicing it is recommended to wear an insert plug as well as an over the ear muff. Hunters are recommended to wear electronic type hearing protection which can amplify soft sounds like leaves crunching while reducing the intensity of the gunshot. Custom hearing protection can also be effective and is typically recommended for individuals who are skeet shooting. Hearing protection does have limitations though, and due to the high intensity of guns it is certainly possible for shooters to still develop hearing loss. However, hearing protection typically reduces the amount of damage the ear sustains even if it cannot completely protect the ear.

Firearms include a variety of ranged weapons and there is no agreed-upon definition.
For instance English language laws of big legal entities such as the United States, India the European Union and Canada use different definitions.
Other English language definitions are provided by international treaties.

In the United States, under 26 USCA § 861 (a), the term ‘‘firearm’’ means 

According to the US Bureau of Alcohol, Tobacco, Firearms and Explosives, if gas pressurization is achieved through "mechanical" gas compression rather than through chemical propellant combustion, then the device is technically an air gun, not a firearm.

In India, the arms act, 1959, provides a definition of firearms where "firearms" means arms of any description designed or adapted to discharge a projectile or projectiles of any kind by the action of any explosive or other forms of energy, and includes:

In the European Union, a European Directive amended by EU directive 2017/853 set minimum standards regarding civilian firearms acquisition and possession that EU Member States must implement into their national legal systems. In this context, since 2017, firearms are considered as "any portable barrelled weapon that expels, is designed to expel or may be converted to expel a shot, bullet or projectile by the action of a combustible propellant".
For legal reasons, objects can be considered as a firearm if they have the appearance of a firearm or are made in a way which make possible to convert them to a firearm. Member states may be allowed to exclude from their gun control law items such as antique weapons, or specific purposes items which can only be used for that sole purpose.

In Canada, firearms are defined by the Criminal Code:

Australia has a definition of firearms in its 1996 legal act:

In South Africa, Firearms Control Act [No. 60 of 2000] defines firearm since June 2001, with a 2006 amendment of the definition: 

An inter-American convention defines firearms as:
An international UN protocol on firearms considers that 


</doc>
<doc id="11968" url="https://en.wikipedia.org/wiki?curid=11968" title="George Washington">
George Washington

George Washington (February 22, 1732 (O.S. February 11, 1732), 1799) was an American political leader, military general, statesman, and founding father who served as the first president of the United States from 1789 to 1797. Previously, he led Patriot forces to victory in the nation's War for Independence. He presided at the Constitutional Convention of 1787, which established the U.S. Constitution
and a federal government. Washington has been called the "Father of His Country" for his manifold leadership in the formative days of the new nation.

Washington received his initial military training and command with the Virginia Regiment during the French and Indian War. He was later elected to the Virginia House of Burgesses and was named a delegate to the Continental Congress, where he was appointed Commanding General of the Continental Army. He commanded American forces, allied with France, in the defeat and surrender of the British during the Siege of Yorktown. He resigned his commission after the Treaty of Paris in 1783.

Washington played a key role in adopting and ratifying the Constitution and was then elected president (twice) by the Electoral College. He implemented a strong, well-financed national government while remaining impartial in a fierce rivalry between cabinet members Thomas Jefferson and Alexander Hamilton. During the French Revolution, he proclaimed a policy of neutrality while sanctioning the Jay Treaty. He set enduring precedents for the office of president, including the title "Mr. President", and his Farewell Address is widely regarded as a pre-eminent statement on republicanism.

Washington owned slaves, and, in order to preserve national unity, he supported measures passed by Congress to protect slavery. He later became troubled with the institution of slavery and freed his slaves in a 1799 will. He endeavored to assimilate Native Americans into Anglo-American culture but combated indigenous resistance during occasions of violent conflict. He was a member of the Anglican Church and the Freemasons, and he urged broad religious freedom in his roles as general and president. Upon his death, he was eulogized as "first in war, first in peace, and first in the hearts of his countrymen". He has been memorialized by monuments, art, geographical locations, stamps, and currency, and many scholars and polls rank him among the greatest U.S. presidents.

The Washington family was a wealthy Virginia family which had made its fortune in land speculation. Washington's great-grandfather John Washington immigrated in 1656 from Sulgrave, England, to the British Colony of Virginia where he accumulated of land, including Little Hunting Creek on the Potomac River. George Washington was born February 22, 1732, at Popes Creek in Westmoreland County, Virginia, and was the first of six children of Augustine and Mary Ball Washington. By English common law Washington was a naturalized subject of the King, as were all others born in the British colonies. His father was a justice of the peace and a prominent public figure who had three additional children from his first marriage to Jane Butler. The family moved to Little Hunting Creek in 1735, then to Ferry Farm near Fredericksburg, Virginia, in 1738. When Augustine died in 1743, Washington inherited Ferry Farm and ten slaves; his older half-brother Lawrence inherited Little Hunting Creek and renamed it Mount Vernon.

Washington did not have the formal education his elder brothers received at Appleby Grammar School in England, but he did learn mathematics, trigonometry, and land surveying. He was a talented draftsman and map-maker. By early adulthood he was writing with "considerable force" and "precision", however his writing displayed little wit or humor. In pursuit of admiration, status, and power, he tended to attribute his shortcomings and failures to someone else's ineffectuality.

Washington often visited Mount Vernon and Belvoir, the plantation that belonged to Lawrence's father-in-law William Fairfax. Fairfax became Washington's patron and surrogate father, and Washington spent a month in 1748 with a team surveying Fairfax's Shenandoah Valley property. He received a surveyor's license the following year from the College of William & Mary; Fairfax appointed him surveyor of Culpeper County, Virginia, and he thus familiarized himself with the frontier region, resigning from the job in 1750. By 1752 he had bought almost in the Valley and owned .

In 1751 Washington made his only trip abroad when he accompanied Lawrence to Barbados, hoping the climate would cure his brother's tuberculosis. Washington contracted smallpox during that trip, which immunized him but left his face slightly scarred. Lawrence died in 1752, and Washington leased Mount Vernon from his widow; he inherited it outright after her death in 1761.

Lawrence Washington's service as adjutant general of the Virginia militia inspired George to seek a commission. Virginia's Lieutenant Governor Robert Dinwiddie appointed him as a major and as commander of one of the four militia districts. The British and French were competing for control of the Ohio Valley at the time, the British building forts along the Ohio River and the French doing likewise between the river and Lake Erie.

In October 1753, Dinwiddie appointed Washington as a special envoy to demand that the French vacate territory which the British had claimed. Dinwiddie also appointed him to make peace with the Iroquois Confederacy and to gather intelligence about the French forces. Washington met with Half-King Tanacharison and other Iroquois chiefs at Logstown to secure their promise of support against the French, and his party reached the Ohio River in November. They were intercepted by a French patrol and escorted to Fort Le Boeuf where Washington was received in a friendly manner. He delivered the British demand to vacate to French commander Saint-Pierre, but the French refused to leave. Saint-Pierre gave Washington his official answer in a sealed envelope after a few days' delay, and he gave Washington's party food and extra winter clothing for the trip back to Virginia. Washington completed the precarious mission in 77 days in difficult winter conditions, achieving a measure of distinction when his report was published in Virginia and in London.

In February 1754, Dinwiddie promoted Washington to lieutenant colonel and second-in-command of the 300-strong Virginia Regiment, with orders to confront French forces at the Forks of the Ohio. Washington set out for the Forks with half the regiment in April but soon learned a French force of 1,000 had begun construction of Fort Duquesne there. In May, having set up a defensive position at Great Meadows, he learned that the French had made camp seven miles (11 km) away; he decided to take the offensive.
The French detachment proved to be only about fifty men, so Washington advanced on May 28 with a small force of Virginians and Indian allies to ambush them. What took place, known as the Battle of Jumonville Glen or the Jumonville affair, was disputed, but French forces were killed outright with muskets and hatchets. French commander Joseph Coulon de Jumonville, who carried a diplomatic message for the British to evacuate, was killed. French forces found Jumonville and some of his men dead and scalped and assumed Washington was responsible. Washington blamed his translator for not communicating the French intentions. Dinwiddie congratulated Washington for his victory over the French. This incident ignited the French and Indian War, which later became part of the larger Seven Years' War.

The full Virginia Regiment joined Washington at Fort Necessity the following month with news that he had been promoted to command of the regiment and to colonel upon the death of the regimental commander. The regiment was reinforced by an independent company of 100 South Carolinians led by Captain James Mackay, whose royal commission outranked that of Washington, and a conflict of command ensued. On July 3, a French force attacked with 900 men, and the ensuing battle ended in Washington's surrender. In the aftermath, Colonel James Innes took command of intercolonial forces, the Virginia Regiment was divided, and Washington was offered a captaincy which he refused, with resignation of his commission.
In 1755, Washington served voluntarily as an aide to General Edward Braddock, who led a British expedition to expel the French from Fort Duquesne and the Ohio Country. On Washington's recommendation, Braddock split the army into one main column and a lightly equipped "flying column". Suffering from a severe case of dysentery, Washington was left behind, and when he rejoined Braddock at Monongahela the French and their Indian allies ambushed the divided army. The British suffered two-thirds casualties, including the mortally wounded Braddock. Under the command of Lieutenant Colonel Thomas Gage, Washington, still very ill, rallied the survivors and formed a rear guard, allowing the remnants of the force to disengage and retreat. During the engagement he had two horses shot from under him, and his hat and coat were bullet-pierced. His conduct under fire redeemed his reputation among critics of his command in the Battle of Fort Necessity, but he was not included by the succeeding commander (Colonel Thomas Dunbar) in planning subsequent operations.

The Virginia Regiment was reconstituted in August 1755, and Dinwiddie appointed Washington its commander, again with the colonial rank of colonel. Washington clashed over seniority almost immediately, this time with John Dagworthy, another captain of superior royal rank, who commanded a detachment of Marylanders at the regiment's headquarters in Fort Cumberland. Washington, impatient for an offensive against Fort Duquesne, was convinced Braddock would have granted him a royal commission and pressed his case in February 1756 with Braddock's successor, William Shirley, and again in January 1757 with Shirley's successor, Lord Loudoun. Shirley ruled in Washington's favor only in the matter of Dagworthy; Loudoun humiliated Washington, refused him a royal commission and agreed only to relieve him of the responsibility of manning Fort Cumberland.

In 1758, the Virginia Regiment was assigned to Britain's Forbes Expedition to take Fort Duquesne. Washington disagreed with General John Forbes' tactics and chosen route. Forbes nevertheless made Washington a brevet brigadier general and gave him command of one of the three brigades that would assault the fort. The French abandoned the fort and the valley before the assault was launched; Washington saw only a friendly-fire incident which left 14 dead and 26 injured. The war lasted another four years, but Washington resigned his commission and returned to Mount Vernon.

Under Washington, the Virginia Regiment had defended of frontier against twenty Indian attacks in ten months. He increased the professionalism of the regiment as it increased from 300 to 1,000 men, and Virginia's frontier population suffered less than other colonies. Some historians have said this was Washington's "only unqualified success" during the war. Though he failed to realize a royal commission, he did gain self-confidence, leadership skills, and invaluable knowledge of British tactics. The destructive competition Washington witnessed among colonial politicians fostered his later support of strong central government.

On January 6, 1759, Washington, at age 26, married Martha Dandridge Custis, the 27-year-old widow of wealthy plantation owner Daniel Parke Custis. The marriage took place at Martha's estate; she was intelligent, gracious, and experienced in managing a planter's estate, and the couple created a happy marriage. They raised John Parke Custis (Jacky) and Martha Parke (Patsy) Custis, children from her previous marriage, and later their grandchildren Eleanor Parke Custis (Nelly) and George Washington Parke Custis (Washy). Washington's 1751 bout with smallpox is thought to have rendered him sterile, though it is equally likely "Martha may have sustained injury during the birth of Patsy, her final child, making additional births impossible." They lamented the fact that they had no children together. They moved to Mount Vernon, near Alexandria, where he took up life as a planter of tobacco and wheat and emerged as a political figure.

The marriage gave Washington control over Martha's one-third dower interest in the Custis estate, and he managed the remaining two-thirds for Martha's children; the estate also included 84 slaves. He became one of Virginia's wealthiest men, which increased his social standing.

At Washington's urging, Governor Lord Botetourt fulfilled Dinwiddie's 1754 promise of land bounties to all volunteer militia during the French and Indian War. In late 1770, Washington inspected the lands in the Ohio and Great Kanawha regions, and he engaged surveyor William Crawford to subdivide it. Crawford allotted to Washington; Washington told the veterans that their land was hilly and unsuitable for farming, and he agreed to purchase , leaving some feeling they had been duped. He also doubled the size of Mount Vernon to and increased its slave population to more than a hundred by 1775.

As a respected military hero and large landowner, Washington held local offices and was elected to the Virginia provincial legislature, representing Frederick County in the House of Burgesses for seven years beginning in 1758. He plied the voters with beer, brandy, and other beverages, although he was absent while serving on the Forbes Expedition. He won election with roughly 40 percent of the vote, defeating three other candidates with the help of several local supporters. He rarely spoke in his early legislative career, but he became a prominent critic of Britain's taxation and mercantilist policies in the 1760s.
By occupation, Washington was a planter, and he imported luxuries and other goods from England, paying for them by exporting tobacco. His profligate spending combined with low tobacco prices left him £1,800 in debt by 1764, prompting him to diversify. In 1765, because of erosion and other soil problems, he changed Mount Vernon's primary cash crop from tobacco to wheat and expanded operations to include corn flour milling and fishing. Washington also took time for leisure with fox hunting, fishing, dances, theater, cards, backgammon, and billiards.

Washington soon was counted among the political and social elite in Virginia. From 1768 to 1775, he invited some 2,000 guests to his Mount Vernon estate, mostly those whom he considered "people of rank". He became more politically active in 1769, presenting legislation in the Virginia Assembly to establish an embargo on goods from Great Britain.

Washington's step-daughter Patsy Custis suffered from epileptic attacks from age 12, and she died in his arms in 1773. The following day, he wrote to Burwell Bassett: "It is easier to conceive, than to describe, the distress of this Family". He canceled all business activity and remained with Martha every night for three months.

Washington played a central role before and during the American Revolution. His disdain for the British military had begun when he was abashedly passed over for promotion into the Regular Army. Opposed to taxes imposed by the British Parliament on the Colonies without proper representation, he and other colonists were also angered by the Royal Proclamation of 1763 which banned American settlement west of the Allegheny Mountains and protected the British fur trade.

Washington believed the Stamp Act of 1765 was an "Act of Oppression", and he celebrated its repeal the following year. In March 1766, Parliament passed the Declaratory Act asserting that Parliamentary law superseded colonial law. Washington helped lead widespread protests against the Townshend Acts passed by Parliament in 1767, and he introduced a proposal in May 1769 drafted by George Mason which called Virginians to boycott English goods; the Acts were mostly repealed in 1770.

Parliament sought to punish Massachusetts colonists for their role in the Boston Tea Party in 1774 by passing the Coercive Acts, which Washington referred to as "an invasion of our rights and privileges". He said Americans must not submit to acts of tyranny since "custom and use shall make us as tame and abject slaves, as the blacks we rule over with such arbitrary sway". That July, he and George Mason drafted a list of resolutions for the Fairfax County committee which Washington chaired, and the committee adopted the Fairfax Resolves calling for a Continental Congress. On August 1, Washington attended the First Virginia Convention, where he was selected as a delegate to the First Continental Congress. As tensions rose in 1774, he helped train county militias in Virginia and organized enforcement of the Continental Association boycott of British goods instituted by the Congress.

The American Revolutionary War began on April 19, 1775, with the Battles of Lexington and Concord and the Siege of Boston. The colonists were divided over breaking away from British rule and split into two factions: Patriots who rejected British rule, and Loyalists who desired to remain subject to the King. General Thomas Gage was commander of British forces in America at the beginning of the war. Upon hearing the shocking news of the onset of war, Washington was "sobered and dismayed", and he hastily departed Mount Vernon on May 4, 1775, to join the Continental Congress in Philadelphia.

Congress created the Continental Army on June 14, 1775, and Samuel and John Adams nominated Washington to become its commander in chief. Washington was chosen over John Hancock because of his military experience and the belief that a Virginian would better unite the colonies. He was considered an incisive leader who kept his "ambition in check". He was unanimously elected commander in chief by Congress the next day.

Washington appeared before Congress in uniform and gave an acceptance speech on June 16, declining a salary—though he was later reimbursed expenses. He was commissioned on June 19 and was roundly praised by Congressional delegates, including John Adams, who proclaimed that he was the man best suited to lead and unite the colonies. Congress appointed Washington "General & Commander in chief of the army of the United Colonies and of all the forces raised or to be raised by them", and instructed him to take charge of the siege of Boston on June 22, 1775.

Congress chose his primary staff officers, including Major General Artemas Ward, Adjutant General Horatio Gates, Major General Charles Lee, Major General Philip Schuyler, Major General Nathanael Greene, Colonel Henry Knox, and Colonel Alexander Hamilton. Washington was impressed by Colonel Benedict Arnold and gave him responsibility for invading Canada. He also engaged French and Indian War compatriot Brigadier General Daniel Morgan. Henry Knox impressed Adams with ordnance knowledge, and Washington promoted him to colonel and chief of artillery.

Washington initially protested enlistment of slaves in the Continental Army, but later he relented when the British emancipated and used theirs. On January 16, 1776, Congress allowed free blacks to serve in the militia. By the end of the war one-tenth of Washington's army were blacks.

Early in 1775, in response to the growing rebellious movement, London sent British troops, commanded by General Thomas Gage, to occupy Boston. They set up fortifications about the city, making it impervious to attack. Various local militias surrounded the city and effectively trapped the British, resulting in a standoff.

As Washington headed for Boston, word of his march preceded him, and he was greeted everywhere; gradually he became a symbol of the Patriot cause. Upon arrival on July 2, 1775, two weeks after the Patriot defeat at nearby Bunker Hill, he set up his Cambridge, Massachusetts headquarters and inspected the new army there, only to find an undisciplined and badly outfitted militia. After consultation, he initiated Benjamin Franklin's suggested reforms—drilling the soldiers and imposing strict discipline, floggings, and incarceration. Washington ordered his officers to identify the skills of recruits to ensure military effectiveness, while removing incompetent officers. He petitioned Gage, his former superior, to release captured Patriot officers from prison and treat them humanely. In October 1775, King George III declared that the colonies were in open rebellion and relieved General Gage of command for incompetence, replacing him with General William Howe.

In June 1775, Congress ordered an invasion of Canada. It was led by Benedict Arnold, who, despite Washington's strong objection, drew volunteers from the latter's force during the Siege of Boston. The move on Quebec failed, with the American forces being reduced to less than half and forced to retreat.

The Continental Army, further diminished by expiring short-term enlistments, and by January 1776 reduced by half to 9,600 men, had to be supplemented with militia, and was joined by Knox with heavy artillery captured from Fort Ticonderoga. When the Charles River froze over Washington was eager to cross and storm Boston, but General Gates and others were opposed to untrained militia striking well-garrisoned fortifications. Washington reluctantly agreed to secure Dorchester Heights, 100 feet above Boston, in an attempt to force the British out of the city. On March 9, under cover of darkness, Washington's troops brought up Knox's big guns and bombarded British ships in Boston harbor. On March 17 9,000 British troops and Loyalists began a chaotic ten-day evacuation of Boston aboard 120 ships. Soon after, Washington entered the city with 500 men, with explicit orders not to plunder the city. He ordered vaccinations against smallpox to great effect, as he did later in Morristown, New Jersey. He refrained from exerting military authority in Boston, leaving civilian matters in the hands of local authorities.

Washington then proceeded to New York City, arriving on April 13, 1776, and began constructing fortifications there to thwart the expected British attack. He ordered his occupying forces to treat civilians and their property with respect, to avoid the abuse suffered by civilians in Boston at the hands of British troops. A plot to assassinate or capture him was discovered but it failed, though his bodyguard Thomas Hickey was hanged for mutiny and sedition. General Howe transported his resupplied army, with the British fleet, from Halifax to New York, knowing the city was key to securing the continent. George Germain, who ran the British war effort in England, believed it could be won with one "decisive blow". The British forces, including more than a hundred ships and thousands of troops, began arriving on Staten Island on July2 to lay siege to the city. After the Declaration of Independence was adopted on July 4, Washington informed his troops in his general orders of July9 that Congress had declared the united colonies to be "free and independent states".

Howe's troop strength totaled 32,000 regulars and Hessians, and Washington's consisted of 23,000, mostly raw recruits and militia. In August, Howe landed 20,000 troops at Gravesend, Brooklyn, and approached Washington's fortifications, as King George III proclaimed the rebellious American colonists to be traitors. Washington, opposing his generals, chose to fight, based upon inaccurate information that Howe's army had only 8,000-plus troops. In the Battle of Long Island, Howe assaulted Washington's flank and inflicted 1,500 Patriot casualties, the British suffering 400. Washington retreated, instructing General William Heath to acquisition river craft in the area. On August 30, General William Alexander held off the British and gave cover while the army crossed the East River under darkness to Manhattan Island without loss of life or materiel, although Alexander was captured.

Howe, emboldened by his Long Island victory, dispatched Washington as "George Washington, Esq.", in futility to negotiate peace. Washington declined, demanding to be addressed with diplomatic protocol, as general and fellow belligerent, not as a "rebel", lest his men be hanged as such if captured. The British navy bombarded the unstable earthworks on lower Manhattan Island. Washington, with misgivings, heeded the advice of Generals Greene and Putnam to defend Fort Washington. They were unable to hold it, and Washington abandoned it despite General Lee's objections, as his army retired north to White Plains. Howe's pursuit forced Washington to retreat across the Hudson River to Fort Lee to avoid encirclement. Howe landed his troops on Manhattan in November and captured Fort Washington, inflicting high casualties on the Americans. Washington was responsible for delaying the retreat, though he blamed Congress and General Greene. Loyalists in New York considered Howe a liberator and spread a rumor that Washington had set fire to the city. Patriot morale reached its lowest when Lee was captured.
Now reduced to 5,400 troops, Washington's army retreated through New Jersey, and Howe broke off pursuit, delaying his advance on Philadelphia, and set up winter quarters in New York.

Washington crossed the Delaware River into Pennsylvania, where Lee's replacement John Sullivan joined him with 2,000 more troops. The future of the Continental Army was in doubt for lack of supplies, a harsh winter, expiring enlistments, and desertions. Washington was disappointed that many New Jersey residents were Loyalists or skeptical about the prospect of independence.

Howe split up his British Army and posted a Hessian garrison at Trenton to hold western New Jersey and the east shore of the Delaware, but the army appeared complacent, and Washington and his generals devised a surprise attack on the Hessians at Trenton, which he codenamed "Victory or Death". The army was to cross the Delaware River to Trenton in three divisions: one led by Washington (2,400 troops), another by General James Ewing (700), and the third by Colonel John Cadwalader (1,500). The force was to then split, with Washington taking the Pennington Road and General Sullivan traveling south on the river's edge.

Washington first ordered a 60-mile search for Durham boats, to transport his army, and he ordered the destruction of vessels that could be used by the British. He crossed the Delaware River on the night of December 25–26, 1776, and risked capture staking out the Jersey shoreline. His men followed across the ice-obstructed river in sleet and snow from McConkey's Ferry, with 40 men per vessel. Wind churned up the waters, and they were pelted with hail, but by 3:00a.m. on December 26, they made it across with no losses. Henry Knox was delayed, managing frightened horses and about 18 field guns on flat-bottomed ferries. Cadwalader and Ewing failed to cross due to the ice and heavy currents, and a waiting Washington doubted his planned attack on Trenton. Once Knox arrived, Washington proceeded to Trenton, to take only his troops against the Hessians, rather than risk being spotted returning his army to Pennsylvania.

The troops spotted Hessian positions a mile from Trenton, so Washington split his force into two columns, rallying his men: "Soldiers keep by your officers. For God's sake, keep by your officers." The two columns were separated at the Birmingham crossroads, with General Nathanael Greene's column taking the upper Ferry Road, led by Washington, and General John Sullivan's advancing on River Road. (.) The Americans marched in sleet and snowfall. Many were shoeless with bloodied feet, and two died of exposure. At sunrise, Washington led them in a surprise attack on the Hessians, aided by Major General Knox and artillery. The Hessians had 22 killed (including Colonel Johann Rall), 83 wounded, and 850 captured with supplies.
Washington retreated across the Delaware to Pennsylvania but returned to New Jersey on January 3, launching an attack on British regulars at Princeton, with 40 Americans killed or wounded and 273 British killed or captured. American Generals Hugh Mercer and John Cadwalader were being driven back by the British when Mercer was mortally wounded, then Washington arrived and led the men in a counterattack which advanced to within of the British line.

Some British troops retreated after a brief stand, while others took refuge in Nassau Hall, which became the target of Colonel Alexander Hamilton's cannons. Washington's troops charged, the British surrendered in less than an hour, and 194 soldiers laid down their arms. Howe retreated to New York City where his army remained inactive until early the next year. Washington's depleted Continental Army took up winter headquarters in Morristown, New Jersey while disrupting British supply lines and expelling them from parts of New Jersey. Washington later said the British could have successfully counterattacked his encampment before his troops were dug in.

The British still controlled New York, and many Patriot soldiers did not re-enlist or deserted after the harsh winter campaign. Congress instituted greater rewards for re-enlisting and punishments for desertion in an effort to effect greater troop numbers. Strategically, Washington's victories were pivotal for the Revolution and quashed the British strategy of showing overwhelming force followed by offering generous terms. In February 1777, word reached London of the American victories at Trenton and Princeton, and the British realized the Patriots were in a position to demand unconditional independence.

In July 1777, British General John Burgoyne led the Saratoga campaign south from Quebec through Lake Champlain and recaptured Fort Ticonderoga with the objective of dividing New England, including control of the Hudson River. But General Howe in British-occupied New York blundered, taking his army south to Philadelphia rather than up the Hudson River to join Burgoyne near Albany. Meanwhile, Washington and Gilbert du Motier, Marquis de Lafayette rushed to Philadelphia to engage Howe and were shocked to learn of Burgoyne's progress in upstate New York, where the Patriots were led by General Philip Schuyler and successor Horatio Gates. Washington's army of less experienced men were defeated in the pitched battles at Philadelphia.

Howe outmaneuvered Washington at the Battle of Brandywine on September 11, 1777, and marched unopposed into the nation's capital at Philadelphia. A Patriot attack failed against the British at Germantown in October. Major General Thomas Conway prompted some members of Congress (referred to as the Conway Cabal) to consider removing Washington from command because of the losses incurred at Philadelphia. Washington's supporters resisted and the matter was finally dropped after much deliberation. Once exposed, Conway wrote an apology to Washington, resigned, and returned to France.

Washington was concerned with Howe's movements during the Saratoga campaign to the north, and he was also aware that Burgoyne was moving south toward Saratoga from Quebec. Washington took some risks to support Gates' army, sending reinforcements north with Generals Benedict Arnold, his most aggressive field commander, and Benjamin Lincoln. On October 7, 1777, Burgoyne tried to take Bemis Heights but was isolated from support by Howe. He was forced to retreat to Saratoga and ultimately surrendered after the Battles of Saratoga. As Washington suspected, Gates' victory emboldened his critics. Biographer John Alden maintains, "It was inevitable that the defeats of Washington's forces and the concurrent victory of the forces in upper New York should be compared." The admiration for Washington was waning, including little credit from John Adams. British commander Howe resigned in May 1778, left America forever, and was replaced by Sir Henry Clinton.

Washington's army of 11,000 went into winter quarters at Valley Forge north of Philadelphia in December 1777. They suffered between 2,000 and 3,000 deaths in the extreme cold over six months, mostly from disease and lack of food, clothing, and shelter. Meanwhile, the British were comfortably quartered in Philadelphia, paying for supplies in pounds sterling, while Washington struggled with a devalued American paper currency. The woodlands were soon exhausted of game, and by February morale and increased desertions ensued.

Washington made repeated petitions to the Continental Congress for provisions. He received a congressional delegation to check the Army's conditions, and expressed the urgency of the situation, proclaiming: "Something must be done. Important alterations must be made." He recommended that Congress expedite supplies, and Congress agreed to strengthen and fund the army's supply lines by reorganizing the commissary department. By late February, supplies began arriving.
Baron Friedrich Wilhelm von Steuben's incessant drilling soon transformed Washington's recruits into a disciplined fighting force, and the revitalized army emerged from Valley Forge early the following year. Washington promoted Von Steuben to Major General and made him chief of staff.

In early 1778, the French responded to Burgoyne's defeat and entered into a Treaty of Alliance with the Americans. The Continental Congress ratified the treaty in May, which amounted to a French declaration of war against Britain. The British evacuated Philadelphia for New York that June and Washington summoned a war council of American and French Generals. He chose a partial attack on the retreating British at the Battle of Monmouth; the British were commanded by Howe's successor General Henry Clinton. Generals Charles Lee and Lafayette moved with 4,000 men, without Washington's knowledge, and bungled their first attack on June 28. Washington relieved Lee and achieved a draw after an expansive battle. At nightfall, the British continued their retreat to New York, and Washington moved his army outside the city. Monmouth was Washington's last battle in the North; he valued the safety of his army more than towns with little value to the British.

Washington became "America's first spymaster" by designing an espionage system against the British. In 1778, Major Benjamin Tallmadge formed the Culper Ring at Washington's direction to covertly collect information about the British in New York. Washington had disregarded incidents of disloyalty by Benedict Arnold, who had distinguished himself in many battles.

During mid-1780, Arnold began supplying British spymaster John André with sensitive information intended to compromise Washington and capture West Point, a key American defensive position on the Hudson River. Historians have noted several possible reasons for Arnold's treachery: his anger at losing promotions to junior officers, the repeated slights from Congress. He was also deeply in debt, had been profiteering from the war and was disappointed by Washington's lack of support during his resultant court-martial.

Arnold repeatedly asked for command of West Point, and Washington finally agreed in August. Arnold met André on September 21, giving him plans to take over the garrison. Militia forces captured André and discovered the plans, but Arnold escaped to New York. Washington recalled the commanders positioned under Arnold at key points around the fort to prevent any complicity, but he did not suspect Arnold's wife Peggy. Washington assumed personal command at West Point and reorganized its defenses. André's trial for espionage ended in a death sentence, and Washington offered to return him to the British in exchange for Arnold, but Clinton refused. André was hanged on October 2, 1780, despite his request to face a firing squad, in order to deter other spies.

In late 1778, General Clinton shipped 3,000 troops from New York to Georgia and launched a Southern invasion against Savannah, reinforced by 2,000 British and Loyalist troops. They repelled an attack by Patriots and French naval forces, which bolstered the British war effort.

In mid-1779, Washington attacked Iroquois warriors of the Six Nations in order to force Britain's Indian allies out of New York, from which they had assaulted New England towns. The Indian warriors joined with Tory rangers led by Walter Butler and viciously slew more than 200 frontiersmen in June, laying waste to the Wyoming Valley in Pennsylvania. In response, Washington ordered General John Sullivan to lead an expedition to effect "the total destruction and devastation" of Iroquois villages and take their women and children hostage. Those who managed to escape fled to Canada.

Washington's troops went into quarters at Morristown, New Jersey during the winter of 1779–1780 and suffered their worst winter of the war, with temperatures well below freezing. New York Harbor was frozen over, snow and ice covered the ground for weeks, and the troops again lacked provisions.

Clinton assembled 12,500 troops and attacked Charlestown, South Carolina in January 1780, defeating General Benjamin Lincoln who had only 5,100 Continental troops. The British went on to occupy the South Carolina Piedmont in June, with no Patriot resistance. Clinton returned to New York and left 8,000 troops commanded by General Charles Cornwallis. Congress replaced Lincoln with Horatio Gates; he failed in South Carolina and was replaced by Washington's choice of Nathaniel Greene, but the British already had the South in their grasp. Washington was reinvigorated, however, when Lafayette returned from France with more ships, men, and supplies, and 5,000 veteran French troops led by Marshal Rochambeau arrived at Newport, Rhode Island in July 1780. French naval forces then landed, led by Admiral Grasse, and Washington encouraged Rochambeau to move his fleet south to launch a joint land and naval attack on Arnold's troops.

Washington's army went into winter quarters at New Windsor, New York in December 1780, and Washington urged Congress and state officials to expedite provisions in hopes that the army would not "continue to struggle under the same difficulties they have hitherto endured". On March 1, 1781, Congress ratified the Articles of Confederation, but the government that took effect on March2 did not have the power to levy taxes, and it loosely held the states together.

General Clinton sent Benedict Arnold, now a British Brigadier General with 1,700 troops, to Virginia to capture Portsmouth and to spread terror from there; Washington responded by sending Lafayette south to counter Arnold's efforts. Washington initially hoped to bring the fight to New York, drawing off British forces from Virginia and ending the war there, but Rochambeau advised Grasse that Cornwallis in Virginia was the better target. Grasse's fleet arrived off the Virginia coast and Washington saw the advantage. He made a feint towards Clinton in New York, then headed south to Virginia.
The Siege of Yorktown, Virginia was a decisive allied victory by the combined forces of the Continental Army commanded by General Washington, the French Army commanded by the General Comte de Rochambeau, and the French Navy commanded by Admiral de Grasse, in the defeat of Cornwallis' British forces. On August 19, the march to Yorktown led by Washington and Rochambeau began, which is known now as the "celebrated march". Washington was in command of an army of 7,800 Frenchmen, 3,100 militia, and 8,000 Continentals. Lacking in experience in siege warfare, Washington often deferred judgment to Rochambeau, effectively putting him in command; however, Rochambeau never challenged Washington's authority.

By late September, Patriot-French forces completely surrounded Yorktown, trapped the British army, and prevented British reinforcements from Clinton in the North, while the French Navy was victorious at the Battle of the Chesapeake. The final American offensive was begun with a shot fired by Washington. The siege ended with a British surrender on October 19, 1781; over 7,000 British soldiers were captured, in the last major land battle of the American Revolutionary War. Washington negotiated the terms of surrender for two days, and the official signing ceremony took place on October 19; Cornwallis, in fact, claimed illness and was absent, sending General Charles O'Hara as his proxy. As a gesture of goodwill, Washington held a dinner for the American, French, and British generals, all of whom fraternized on friendly terms and identified with one another as members of the same professional military caste.

After the surrender at Yorktown, a situation developed that threatened relations between the new American nation and Britain. Following a series of retributive executions between Patriots and Loyalists, Washington, on May 18, 1782, wrote in a letter to General Moses Hazen that a British Captain would be executed for the execution of Joshua Huddy a popular patriot leader among volunteers, who was hanged at the direction of Loyalist Captain Lippincott. Washington wanted Lippincott himself to be executed but was declined. Subsequently, Charles Asgill was chosen instead, by a drawing of lots from a hat. This was a violation of the 14th article of the Yorktown Articles of Capitulation, which protected prisoners of war from acts of retaliation. Later, Washington's feelings on matters changed and in a letter of November 13, 1782, to Asgill, he acknowledged Asgill's letter and situation, expressing his desire not to see any harm come to him. After much consideration between the Continental Congress, Alexander Hamilton, Washington, and appeals from the French Crown, Asgill was finally released, where Washington issued Asgill a pass that allowed his passage to New York.

As peace negotiations started, the British gradually evacuated troops from Savannah, Charlestown, and New York by 1783, and the French army and navy likewise departed. The American treasury was empty, unpaid and mutinous soldiers forced the adjournment of Congress, and Washington dispelled unrest by suppressing the Newburgh Conspiracy in March 1783; Congress promised officers a five-year bonus. Washington submitted an account of $450,000 in expenses which he had advanced to the army. The account was settled, though it was allegedly vague about large sums and included expenses his wife had incurred through visits to his headquarters.

Washington resigned as commander-in-chief once the Treaty of Paris was signed, and he planned to retire to Mount Vernon. The treaty was ratified in April 1783, and Hamilton's Congressional committee adapted the army for peacetime. Washington gave the Army's perspective to the committee in his "Sentiments on a Peace Establishment". The Treaty was signed on September 3, 1783, and Great Britain officially recognized the independence of the United States. Washington then disbanded his army, giving an eloquent farewell address to his soldiers on November 2. On November 25, the British evacuated New York City, and Washington and Governor George Clinton took possession.

Washington advised Congress in August 1783 to keep a standing army, create a "national militia" of separate state units, and establish a navy and a national military academy. He circulated his "Farewell" orders that discharged his troops, whom he called "one patriotic band of brothers". Before his return to Mount Vernon, he oversaw the evacuation of British forces in New York and was greeted by parades and celebrations, where he announced that Knox had been promoted commander-in-chief.

After leading the Continental Army for 8½ years, Washington bade farewell to his officers at Fraunces Tavern in December 1783, and resigned his commission days later, refuting Loyalist predictions that he would not relinquish his military command. In a final appearance in uniform, he gave a statement to the Congress: "I consider it an indispensable duty to close this last solemn act of my official life, by commending the interests of our dearest country to the protection of Almighty God, and those who have the superintendence of them, to his holy keeping." Washington's resignation was acclaimed at home and abroad and showed a skeptical world that the new republic would not degenerate into chaos.
The same month, Washington was appointed president-general of the Society of the Cincinnati, a hereditary fraternity, and he served for the remainder of his life.

Washington was longing to return home after spending just 10 days at Mount Vernon out of years of war. He arrived on Christmas Eve, delighted to be "free of the bustle of a camp and the busy scenes of public life". He was a celebrity and was fêted during a visit to his mother at Fredericksburg in February 1784, and he received a constant stream of visitors wishing to pay their respects to him at Mount Vernon.

Washington reactivated his interests in the Great Dismal Swamp and Potomac canal projects begun before the war, though neither paid him any dividends, and he undertook a 34-day, trip to check on his land holdings in the Ohio Country. He oversaw the completion of the remodeling work at Mount Vernon which transformed his residence into the mansion that survives to this day—although his financial situation was not strong. Creditors paid him in depreciated wartime currency, and he owed significant amounts in taxes and wages. Mount Vernon had made no profit during his absence, and he saw persistently poor crop yields due to pestilence and poor weather. His estate recorded its eleventh year running at a deficit in 1787, and there was little prospect of improvement. Washington undertook a new landscaping plan and succeeded in cultivating a range of fast-growing trees and shrubs that were native to North America.

Before returning to private life in June 1783, Washington called for a strong union. Though he was concerned that he might be criticized for meddling in civil matters, he sent a circular letter to all the states maintaining that the Articles of Confederation was no more than "a rope of sand" linking the states. He believed the nation was on the verge of "anarchy and confusion", was vulnerable to foreign intervention and that a national constitution would unify the states under a strong central government. When Shays' Rebellion erupted in Massachusetts on August 29, 1786, over taxation, Washington was further convinced that a national constitution was needed. Some nationalists feared that the new republic had descended into lawlessness, and they met together on September 11, 1786, at Annapolis to ask Congress to revise the Articles of Confederation. One of their biggest efforts, however, was getting Washington to attend. Congress agreed to a Constitutional Convention to be held in Philadelphia in Spring 1787, and each state was to send delegates.

On December 4, 1786, Washington was chosen to lead the Virginia delegation, but he declined on December 21. He had concerns about the legality of the convention and consulted James Madison, Henry Knox, and others. They persuaded him to attend it, however, as his presence might induce reluctant states to send delegates and smooth the way for the ratification process. On March 28, Washington told Governor Edmund Randolph that he would attend the convention, but made it clear that he was urged to attend.
Washington arrived in Philadelphia on May 9, 1787, though a quorum was not attained until Friday, May 25. Benjamin Franklin nominated Washington to preside over the convention, and he was unanimously elected to serve as president general. The convention's state-mandated purpose was to revise the Articles of Confederation with "all such alterations and further provisions" required to improve them, and the new government would be established when the resulting document was "duly confirmed by the several states". Governor Edmund Randolph of Virginia introduced Madison's Virginia Plan on May 27, the third day of the convention. It called for an entirely new constitution and a sovereign national government, which Washington highly recommended.

Washington wrote Alexander Hamilton on July 10: "I almost despair of seeing a favorable issue to the proceedings of our convention and do therefore repent having had any agency in the business." Nevertheless, he lent his prestige to the goodwill and work of the other delegates. He unsuccessfully lobbied many to support ratification of the Constitution, such as anti-federalist Patrick Henry; Washington told him "the adoption of it under the present circumstances of the Union is in my opinion desirable" and declared the alternative would be anarchy. Washington and Madison then spent four days at Mount Vernon evaluating the transition of the new government.

The delegates to the Convention anticipated a Washington presidency and left it to him to define the office once elected. The state electors under the Constitution voted for the president on February 4, 1789, and Washington suspected that most republicans had not voted for him. The mandated March4 date passed without a Congressional quorum to count the votes, but a quorum was reached on April 5. The votes were tallied the next day, and Congressional Secretary Charles Thomson was sent to Mount Vernon to tell Washington he had been elected president. Washington won the majority of every state's electoral votes; John Adams received the next highest number of votes and therefore became vice president. Washington had "anxious and painful sensations" about leaving the "domestic felicity" of Mount Vernon, but departed for New York City on April 16 to be inaugurated.

Washington was inaugurated on April 30, 1789, taking the oath of office at Federal Hall in New York City. His coach was led by militia and a marching band and followed by statesmen and foreign dignitaries in an inaugural parade, with a crowd of 10,000. Chancellor Robert R. Livingston administered the oath, using a Bible provided by the Masons, after which the militia fired a 13-gun salute. Washington read a speech in the Senate Chamber, asking "that Almighty Being who rules over the universe, who presides in the councils of nations—and whose providential aids can supply every human defect, consecrate the liberties and happiness of the people of the United States". Though he wished to serve without a salary, Congress insisted adamantly that he accept it, later providing Washington $25,000 per year to defray costs of the presidency.

Washington wrote to James Madison: "As the first of everything in our situation will serve to establish a precedent, it is devoutly wished on my part that these precedents be fixed on true principles." To that end, he preferred the title "Mr. President" over more majestic names proposed by the Senate, including "His Excellency" and "His Highness the President". His executive precedents included the inaugural address, messages to Congress, and the cabinet form of the executive branch.

Washington had planned to resign after his first term, but the political strife in the nation convinced him he should remain in office. He was an able administrator and a judge of talent and character, and he talked regularly with department heads to get their advice. He tolerated opposing views, despite fears that a democratic system would lead to political violence, and he conducted a smooth transition of power to his successor. He remained non-partisan throughout his presidency and opposed the divisiveness of political parties, but he favored a strong central government, was sympathetic to a Federalist form of government, and leery of the Republican opposition.

Washington dealt with major problems. The old Confederation lacked the powers to handle its workload and had weak leadership, no executive, a small bureaucracy of clerks, a large debt, worthless paper money, and no power to establish taxes. He had the task of assembling an executive department, and relied on Tobias Lear for advice selecting its officers. Great Britain refused to relinquish its forts in the American West, and Barbary pirates preyed on American merchant ships in the Mediterranean at a time when the United States did not even have a navy.

Congress created executive departments in 1789, including the State Department in July, the Department of War in August, and the Treasury Department in September. Washington appointed fellow Virginian Edmund Randolph as Attorney General, Samuel Osgood as Postmaster General, Thomas Jefferson as Secretary of State, and Henry Knox as Secretary of War. Finally, he appointed Alexander Hamilton as Secretary of the Treasury. Washington's cabinet became a consulting and advisory body, not mandated by the Constitution.

Washington's cabinet members formed rival parties with sharply opposing views, most fiercely illustrated between Hamilton and Jefferson. Washington restricted cabinet discussions to topics of his choosing, without participating in the debate. He occasionally requested cabinet opinions in writing and expected department heads to agreeably carry out his decisions.

Washington was apolitical and opposed the formation of parties, suspecting that conflict would undermine republicanism. His closest advisors formed two factions, portending the First Party System. Secretary of the Treasury Alexander Hamilton formed the Federalist Party to promote the national credit and a financially powerful nation. Secretary of State Thomas Jefferson opposed Hamilton's agenda and founded the Jeffersonian Republicans. Washington favored Hamilton's agenda, however, and it ultimately went into effect—resulting in bitter controversy.

Washington proclaimed November 26 as a day of Thanksgiving in order to encourage national unity. "It is the duty of all nations to acknowledge the providence of Almighty God, to obey His will, to be grateful for His benefits, and humbly to implore His protection and favor." He spent that day fasting and visiting debtors in prison to provide them with food and beer.

In response to two antislavery petitions, Georgia and South Carolina objected and were threatening to "blow the trumpet of civil war". Washington and Congress responded with a series of pro-slavery measures: citizenship was denied to black immigrants; slaves were barred from serving in state militias; two more slave states (Kentucky in 1792, Tennessee in 1796) were admitted; and the continuation of slavery in federal territories south of the Ohio River was guaranteed. On February 12, 1793, Washington signed into law the Fugitive Slave Act, which overrode state laws and courts, allowing agents to cross state lines to capture and return escaped slaves. Many in the north decried the law believing the act allowed bounty hunting and the kidnappings of blacks. The Slave Trade Act of 1794, limiting American involvement in the Atlantic slave trade, was also enacted.

Washington's first term was largely devoted to economic concerns, in which Hamilton had devised various plans to address matters. The establishment of public credit became a primary challenge for the federal government. Hamilton submitted a report to a deadlocked Congress, and he, Madison, and Jefferson reached the Compromise of 1790 in which Jefferson agreed to Hamilton's debt proposals in exchange for moving the nation's capital temporarily to Philadelphia and then south near Georgetown on the Potomac River. The terms were legislated in the Funding Act of 1790 and the Residence Act, both of which Washington signed into law. Congress authorized the assumption and payment of the nation's debts, with funding provided by customs duties and excise taxes.

Hamilton created controversy among Cabinet members by advocating the establishment of the First Bank of the United States. Madison and Jefferson objected, but the bank easily passed Congress. Jefferson and Randolph insisted that the new bank was beyond the authority granted by the constitution, as Hamilton believed. Washington sided with Hamilton and signed the legislation on February 25, and the rift became openly hostile between Hamilton and Jefferson.

The nation's first financial crisis occurred in March 1792. Hamilton's Federalists exploited large loans to gain control of U.S. debt securities, causing a run on the national bank; the markets returned to normal by mid-April. Jefferson believed Hamilton was part of the scheme, in spite of Hamilton's efforts to ameliorate, and Washington again found himself in the middle of a feud.

Jefferson and Hamilton adopted diametrically opposed political principles. Hamilton believed in a strong national government requiring a national bank and foreign loans to function, while Jefferson believed the government should be primarily directed by the states and the farm element; he also resented the idea of banks and foreign loans. To Washington's dismay, the two men persistently entered into disputes and infighting. Hamilton demanded that Jefferson resign if he could not support Washington, and Jefferson told Washington that Hamilton's fiscal system would lead to the overthrow of the Republic. Washington urged them to call a truce for the nation's sake, but they ignored him.

Washington reversed his decision to retire after his first term in order to minimize party strife, but the feud continued after his re-election. Jefferson's political actions, his support of Freneau's "National Gazette", and his attempt to undermine Hamilton nearly led Washington to dismiss him from the cabinet; Jefferson ultimately resigned his position in December 1793, and Washington forsook him from that time on.

The feud led to the well-defined Federalist and Republican parties, and party affiliation became necessary for election to Congress by 1794. Washington remained aloof from congressional attacks on Hamilton, but he did not publicly protect him, either. The Hamilton–Reynolds sex scandal opened Hamilton to disgrace, but Washington continued to hold him in "very high esteem" as the dominant force in establishing federal law and government.

In March 1791, at Hamilton's urging, with support from Madison, Congress imposed an excise tax on distilled spirits to help curtail the national debt, which took effect in July. Grain farmers strongly protested in Pennsylvania's frontier districts; they argued that they were unrepresented and were shouldering too much of the debt, comparing their situation to excessive British taxation prior to the Revolutionary War. On August 2, Washington assembled his cabinet to discuss how to deal with the situation. Unlike Washington who had reservations about using force, Hamilton had long waited for such a situation and was eager to suppress the rebellion by use of Federal authority and force. Not wanting to involve the federal government if possible, Washington called on Pennsylvania state officials to take the initiative, but they declined to take military action. On August 7, Washington issued his first proclamation for calling up state militias. After appealing for peace, he reminded the protestors that, unlike the rule of the British crown, the Federal law was issued by state-elected representatives.

Threats and violence against tax collectors, however, escalated into defiance against federal authority in 1794 and gave rise to the Whiskey Rebellion. Washington issued a final proclamation on September 25, threatening the use of military force to no avail. The federal army was not up to the task, so Washington invoked the Militia Act of 1792 to summon state militias. Governors sent troops, initially commanded by Washington, who gave the command to Light-Horse Harry Lee to lead them into the rebellious districts. They took 150 prisoners, and the remaining rebels dispersed without further fighting. Two of the prisoners were condemned to death, but Washington exercised his Constitutional authority for the first time and pardoned them.

Washington's forceful action demonstrated that the new government could protect itself and its tax collectors. This represented the first use of federal military force against the states and citizens, and remains the only time an incumbent president has commanded troops in the field. Washington justified his action against "certain self-created societies" which he regarded as "subversive organizations" which threatened the national union. He did not dispute their right to protest, but he insisted that their dissent must not violate federal law. Congress agreed and extended their congratulations to him; only Madison and Jefferson expressed indifference.

In April 1792, the French Revolutionary Wars began between Great Britain and France, and Washington declared America's neutrality. The revolutionary government of France sent diplomat Citizen Genêt to America, and he was welcomed with great enthusiasm. He created a network of new Democratic-Republican Societies promoting France's interests, but Washington denounced them and demanded that the French recall Genêt. The National Assembly of France granted Washington honorary French citizenship on August 26, 1792, during the early stages of the French Revolution. Hamilton formulated the Jay Treaty to normalize trade relations with Great Britain while removing them from western forts, and also to resolve financial debts remaining from the Revolution. Chief Justice John Jay acted as Washington's negotiator and signed the treaty on November 19, 1794; critical Jeffersonians, however, supported France. Washington deliberated, then supported the treaty because it avoided war with Britain, but was disappointed that its provisions favored Britain. He mobilized public opinion and secured ratification in the Senate but faced frequent public criticism.

The British agreed to abandon their forts around the Great Lakes, and the United States modified the boundary with Canada. The government liquidated numerous pre-Revolutionary debts, and the British opened the British West Indies to American trade. The treaty secured peace with Britain and a decade of prosperous trade. Jefferson claimed that it angered France and "invited rather than avoided" war. Relations with France deteriorated afterwards, leaving succeeding president John Adams with prospective war. James Monroe was the American Minister to France, but Washington recalled him for his opposition to the Treaty. The French refused to accept his replacement Charles Cotesworth Pinckney, and the French Directory declared the authority to seize American ships two days before Washington's term ended. 

Ron Chernow describes Washington as always trying to be even-handed in dealing with the Natives. He states that Washington hoped they would abandon their itinerant hunting life and adapt to fixed agricultural communities in the manner of Anglo-Saxon settlers. He also maintains that Washington never advocated outright confiscation of tribal land or the forcible removal of tribes, and that he berated American settlers who abused natives, admitting that he held out no hope for pacific relations with the natives as long as "frontier settlers entertain the opinion that there is not the same crime (or indeed no crime at all) in killing an native as in killing a white man."

By contrast, Colin G. Calloway writes that "Washington had a lifelong obsession with getting Indian land, either for himself or for his nation, and initiated policies and campaigns that had devastating effects in Indian country." "The growth of the nation," Galloway has stated, "demanded the dispossession of Indian people. Washington hoped the process could be bloodless and that Indian people would give up their lands for a "fair" price and move away. But if Indians refused and resisted, as they often did, he felt he had no choice but to "extirpate" them and that the expeditions he sent to destroy Indian towns were therefore entirely justified."

During the Fall of 1789, Washington had to contend with the British military occupation in the Northwest frontier and their concerted efforts to incite hostile Indian tribes to attack American settlers. The Northwest tribes under Miami chief Little Turtle allied with the British Army to resist American expansion, and killed 1,500 settlers between 1783 and 1790.

Washington decided that "The Government of the United States are determined that their Administration of Indian Affairs shall be directed entirely by the great principles of Justice and humanity", and provided that their land interests should be negotiated by treaties. The administration regarded powerful tribes as foreign nations, and Washington even smoked a peace pipe and drank wine with them at the Philadelphia presidential house. He made numerous attempts to conciliate them; he equated killing indigenous peoples with killing Whites and sought to integrate them into European American culture. Secretary of War Henry Knox also attempted to encourage agriculture among the tribes.

In the Southwest, negotiations failed between federal commissioners and raiding Indian tribes seeking retribution. Washington invited Creek Chief Alexander McGillivray and 24 leading chiefs to New York to negotiate a treaty and treated them like foreign dignitaries. Knox and McGillivray concluded the "Treaty of New York" on August 7, 1790 in Federal Hall, which provided the tribes with agricultural supplies and McGillivray with a rank of Brigadier General Army and a salary of $1,500.
In 1790, Washington sent Brigadier General Josiah Harmar to pacify the Northwest tribes, but Little Turtle routed him twice and forced him to withdraw. The Western Confederacy of tribes used guerrilla tactics and were an effective force against the sparsely manned American Army. Washington sent Major General Arthur St. Clair from Fort Washington on an expedition to restore peace in the territory in 1791. On November 4, St. Clair's forces were ambushed and soundly defeated by tribal forces with few survivors, despite Washington's warning of surprise attacks. Washington was outraged over what he viewed to be excessive Native American brutality and execution of captives, including women and children.

St. Clair resigned his commission, and Washington replaced him with the Revolutionary War hero General Anthony Wayne. From 1792 to 1793, Wayne instructed his troops on Native American warfare tactics and instilled discipline which was lacking under St. Clair. In August 1794, Washington sent Wayne into tribal territory with authority to drive them out by burning their villages and crops in the Maumee Valley. On August 24, the American army under Wayne's leadership defeated the western confederacy at the Battle of Fallen Timbers, and the Treaty of Greenville in August 1795 opened up two-thirds of the Ohio Country for American settlement.

Originally Washington had planned to retire after his first term, while many Americans could not imagine anyone else taking his place. After nearly four years as president, and dealing with the infighting in his own cabinet and with partisan critics, Washington showed little enthusiasm in running for a second term, while Martha also wanted him not to run. James Madison urged him not to retire, that his absence would only allow the dangerous political rift in his cabinet, and in the House, to worsen. Jefferson also pleaded with him not to retire and agreed to drop his attacks on Hamilton, or he would also retire if Washington did. Hamilton maintained that Washington's absence would be "deplored as the greatest evil" to the country at this time. Washington's close nephew George Augustine Washington, his manager at Mount Vernon, was critically ill and had to be replaced, further increasing Washington's desire to retire and return to Mount Vernon.

When the election of 1792 neared, Washington did not publicly announce his presidential candidacy but silently consented to run, to prevent a further political-personal rift in his cabinet. The Electoral College unanimously elected him president on February 13, 1793, and John Adams as vice president by a vote of 77 to 50. Washington, with nominal fanfare, arrived alone at his inauguration in his carriage. Sworn into office by Associate Justice William Cushing on March 4, 1793 in the Senate Chamber of Congress Hall in Philadelphia, Washington gave a brief address and then immediately retired to his Philadelphia presidential house, weary of office and in poor health.
On April 22, 1793, during the French Revolution, Washington issued his famous Neutrality Proclamation and was resolved to pursue, "a conduct friendly and impartial toward the belligerent Powers" while he warned Americans not to intervene in the international conflict. Although Washington recognized France's revolutionary government, he would eventually ask French minister to America Citizen Genet be recalled over the Citizen Genet Affair. Genet was a diplomatic troublemaker who was openly hostile toward Washington's neutrality policy. He procured four American ships as privateers to strike at Spanish forces (British allies) in Florida while organizing militias to strike at other British possessions. But his efforts failed to draw America into the foreign campaigns during Washington's presidency. On July 31, 1793 Jefferson submitted his resignation from Washington's cabinet. Washington signed the Naval Act of 1794 and commissioned the first six federal frigates to combat Barbary pirates.

In January 1795, Hamilton, who desired more income for his family, resigned office and was replaced by Washington appointment Oliver Wolcott, Jr.. Washington and Hamilton remained friends. However, Washington's relationship with his Secretary of War Henry Knox deteriorated. Knox resigned office on the rumor he profited from construction contracts on U.S. Frigates.

In the final months of his presidency, Washington was assailed by his political foes and a partisan press who accused him of being ambitious and greedy, while he argued that he had taken no salary during the war and had risked his life in battle. He regarded the press as a disuniting, "diabolical" force of falsehoods, sentiments that he expressed in his Farewell Address. At the end of his second term, Washington retired for personal and political reasons, dismayed with personal attacks, and to ensure that a truly contested presidential election could be held. He did not feel bound to a two-term limit, but his retirement set a significant precedent. Washington is often credited with setting the principle of a two-term presidency, but it was Thomas Jefferson who first refused to run for a third term on political grounds.

In 1796, Washington declined to run for a third term of office, believing his death in office would create an image of a lifetime appointment. The precedent of a two-term limit was created by his retirement from office. In May 1792, in anticipation of his retirement, Washington instructed James Madison to prepare a "valedictory address", an initial draft of which was entitled the "Farewell Address". In May 1796, Washington sent the manuscript to his Secretary of Treasury Alexander Hamilton who did an extensive rewrite, while Washington provided final edits. On September 19, 1796, David Claypoole's "American Daily Advertiser" published the final version of the address.

Washington stressed that national identity was paramount, while a united America would safeguard freedom and prosperity. He warned the nation of three eminent dangers: regionalism, partisanship, and foreign entanglements, and said the "name of AMERICAN, which belongs to you, in your national capacity, must always exalt the just pride of patriotism, more than any appellation derived from local discriminations." Washington called for men to move beyond partisanship for the common good, stressing that the United States must concentrate on its own interests. He warned against foreign alliances and their influence in domestic affairs and against bitter partisanship and the dangers of political parties. He counseled friendship and commerce with all nations, but advised against involvement in European wars. He stressed the importance of religion, asserting that "religion and morality are indispensable supports" in a republic. Washington's address favored Hamilton's Federalist ideology and economic policies.

Washington closed the address by reflecting on his legacy:

After initial publication, many Republicans, including Madison, criticized the Address and believed it was an anti-French campaign document. Madison believed Washington was strongly pro-British. Madison also was suspicious of who authored the Address.

In 1839, Washington biographer Jared Sparks maintained that Washington's "...Farewell Address was printed and published with the laws, by order of the legislatures, as an evidence of the value they attached to its political precepts, and of their affection for its author." In 1972, Washington scholar James Flexner referred to the Farewell Address as receiving as much acclaim as Thomas Jefferson's Declaration of Independence and Abraham Lincoln's Gettysburg Address. In 2010, historian Ron Chernow reported the "Farewell Address" proved to be one of the most influential statements on Republicanism.

Washington retired to Mount Vernon in March 1797 and devoted time to his plantations and other business interests, including his . His plantation operations were only minimally profitable, and his lands in the west (Piedmont) were under Indian attacks and yielded little income, with the squatters there refusing to pay rent. He attempted to sell these but without success. He became an even more committed Federalist. He vocally supported the Alien and Sedition Acts and convinced Federalist John Marshall to run for Congress to weaken the Jeffersonian hold on Virginia.

Washington grew restless in retirement, prompted by tensions with France, and he wrote to Secretary of War James McHenry offering to organize President Adams' army. In a continuation of the French Revolutionary Wars, French privateers began seizing American ships in 1798, and relations deteriorated with France and led to the "Quasi-War". Without consulting Washington, Adams nominated him for a lieutenant general commission on July 4, 1798 and the position of commander-in-chief of the armies. Washington chose to accept, replacing James Wilkinson, and he served as the commanding general from July 13, 1798 until his death 17 months later. He participated in planning for a provisional army, but he avoided involvement in details. In advising McHenry of potential officers for the army, he appeared to make a complete break with Jefferson's Democratic-Republicans: "you could as soon scrub the blackamoor white, as to change the principles of a profest Democrat; and that he will leave nothing unattempted to overturn the government of this country." Washington delegated the active leadership of the army to Hamilton, a major general. No army invaded the United States during this period, and Washington did not assume a field command.

Washington was thought to be rich because of the well-known "glorified façade of wealth and grandeur" at Mount Vernon, but nearly all his wealth was in the form of land and slaves rather than ready cash. To supplement his income he erected a for substantial whiskey production. Historians estimate that the estate was worth about $1million in 1799 dollars, . He bought land parcels to spur development around the new Federal City that was named in his honor, and he sold individual lots to middle-income investors rather than multiple lots to large investors, believing they would more likely commit to making improvements.

On December 12, 1799, Washington inspected his farms on horseback in snow and sleet. He returned home late for dinner but refused to change out of his wet clothes, not wanting to keep his guests waiting. He had a sore throat the following day but again went out in freezing, snowy weather to mark trees for cutting. That evening, he complained of chest congestion, but was still cheerful. On Saturday, he awoke to an inflamed throat and difficulty breathing, so he ordered estate overseer George Rawlins to remove nearly a pint of his blood, bloodletting being a common practice of the time. His family summoned Doctors James Craik, Gustavus Richard Brown, and Elisha C. Dick. (Dr. William Thornton arrived some hours after Washington died.)

Dr. Brown thought Washington had quinsy; Dr. Dick thought the condition was a more serious "violent inflammation of the throat". They continued the process of bloodletting to approximately five pints, and Washington's condition deteriorated further. Dr. Dick proposed a tracheotomy, but the others were not familiar with that procedure and therefore disapproved. Washington instructed Brown and Dick to leave the room, while he assured Craik, "Doctor, I die hard, but I am not afraid to go."

Washington's death came more swiftly than expected. On his deathbed, he instructed his private secretary Tobias Lear to wait three days before his burial, out of fear of being entombed alive. According to Lear, he died peacefully between 10 and 11 p.m. on December 14, 1799, with Martha seated at the foot of his bed. His last words were "'Tis well", from his conversation with Lear about his burial. He was 67.
Congress immediately adjourned for the day upon news of Washington's death, and the Speaker's chair was shrouded in black the next morning. The funeral was held four days after his death on December 18, 1799, at Mount Vernon, where his body was interred. Cavalry and foot soldiers led the procession, and six colonels served as the pallbearers. The Mount Vernon funeral service was restricted mostly to family and friends. Reverend Thomas Davis read the funeral service by the vault with a brief address, followed by a ceremony performed by various members of Washington's Masonic lodge in Alexandria, Virginia. Congress chose Light-Horse Harry Lee to deliver the eulogy. Word of his death traveled slowly; church bells rang in the cities, and many places of business closed. People worldwide admired Washington and were saddened by his death, and memorial processions were held in major cities of the United States. Martha wore a black mourning cape for one year, and she burned their correspondence to protect their privacy. Only five letters between the couple are known to have survived: two from Martha to George and three from him to her.

The diagnosis of Washington's illness and the immediate cause of his death have been subjects of debate since the day he died. The published account of Drs. Craik and Brown stated that his symptoms had been consistent with "cynanche trachealis" (tracheal inflammation), a term of that period used to describe severe inflammation of the upper windpipe, including quinsy. Accusations have persisted since Washington's death concerning medical malpractice, with some believing he had been bled to death. Various modern medical authors have speculated that he died from a severe case of epiglottitis complicated by the given treatments, most notably the massive blood loss which almost certainly caused hypovolemic shock.

Washington was buried in the old Washington family vault at Mount Vernon, situated on a grassy slope overspread with willow, juniper, cypress, and chestnut trees. It contained the remains of his brother Lawrence and other family members, but the decrepit brick vault was in need of repair, prompting Washington to leave instructions in his will for the construction of a new vault. Washington's estate at the time of his death was worth an estimated $780,000 in 1799, approximately equivalent to $14.3million in 2010. Washington's peak net worth was $587.0 million, including his 300 slaves.

In 1830, a disgruntled ex-employee of the estate attempted to steal what he thought was Washington's skull, prompting the construction of a more secure vault. The next year, the new vault was constructed at Mount Vernon to receive the remains of George and Martha and other relatives. In 1832, a joint Congressional committee debated moving his body from Mount Vernon to a crypt in the Capitol. The crypt had been built by architect Charles Bulfinch in the 1820s during the reconstruction of the burned-out capital, after the Burning of Washington by the British during the War of 1812. Southern opposition was intense, antagonized by an ever-growing rift between North and South; many were concerned that Washington's remains could end up on "a shore foreign to his native soil" if the country became divided, and Washington's remains stayed in Mount Vernon.

On October 7, 1837, Washington's remains were placed, still in the original lead coffin, within a marble sarcophagus designed by William Strickland and constructed by John Struthers earlier that year. The sarcophagus was sealed and encased with planks, and an outer vault was constructed around it. The outer vault has the sarcophagi of both George and Martha Washington; the inner vault has the remains of other Washington family members and relatives.

Washington was somewhat reserved in personality, but he generally had a strong presence among others. He made speeches and announcements when required, but he was not a noted orator or debater. He was taller than most of his contemporaries; accounts of his height vary from to tall, he weighed between as an adult, and he was known for his great strength. He had grey-blue eyes and reddish-brown hair which he wore powdered in the fashion of the day. He had a rugged and dominating presence, which garnered respect from his male peers.

Washington suffered frequently from severe tooth decay and ultimately lost all his teeth but one. He had several sets of false teeth made which he wore during his presidency—none of which were made of wood, contrary to common lore. These dental problems left him in constant pain, for which he took laudanum. As a public figure, he relied upon the strict confidence of his dentist.

Washington was a talented equestrian early in life. He collected thoroughbreds at Mount Vernon, and his two favorite horses were Blueskin and Nelson. Fellow Virginian Thomas Jefferson said Washington was "the best horseman of his age and the most graceful figure that could be seen on horseback"; he also hunted foxes, deer, ducks, and other game. He was an excellent dancer and attended the theater frequently. He drank in moderation but was morally opposed to excessive drinking, smoking tobacco, gambling, and profanity.

Washington was descended from Anglican minister Lawrence Washington (his great-great-grandfather), whose troubles with the Church of England may have prompted his heirs to emigrate to America. Washington was baptized as an infant in April 1732 and became a devoted member of the Church of England (the Anglican Church). He served more than 20 years as a vestryman and churchwarden for Fairfax Parish and Truro Parish, Virginia. He privately prayed and read the Bible daily, and he publicly encouraged people and the nation to pray. He may have taken communion on a regular basis prior to the Revolutionary War, but he did not do so following the war, for which he was admonished by Pastor James Abercrombie.
Washington believed in a "wise, inscrutable, and irresistible" Creator God who was active in the Universe, contrary to deistic thought. He referred to God by the Enlightenment terms "Providence", the "Creator", or the "Almighty", and also as the "Divine Author" or the "Supreme Being". He believed in a divine power who watched over battlefields, was involved in the outcome of war, was protecting his life, and was involved in American politics—and specifically in the creation of the United States. Modern historian Ron Chernow has posited that Washington avoided evangelistic Christianity or hellfire-and-brimstone speech along with communion and anything inclined to "flaunt his religiosity". Chernow has also said Washington "never used his religion as a device for partisan purposes or in official undertakings". No mention of Jesus Christ appears in his private correspondence, and such references are rare in his public writings. He frequently quoted from the Bible or paraphrased it, and often referred to the Anglican "Book of Common Prayer". There is debate on whether he is best classed as a Christian or a theistic rationalist—or both.

Washington emphasized religious toleration in a nation with numerous denominations and religions. He publicly attended services of different Christian denominations and prohibited anti-Catholic celebrations in the Army. He engaged workers at Mount Vernon without regard for religious belief or affiliation. While president, he acknowledged major religious sects and gave speeches on religious toleration. He was distinctly rooted in the ideas, values, and modes of thinking of the Enlightenment, but he harbored no contempt of organized Christianity and its clergy, "being no bigot myself to any mode of worship". In 1793, speaking to members of the New Church in Baltimore, Washington proclaimed, "We have abundant reason to rejoice that in this Land the light of truth and reason has triumphed over the power of bigotry and superstition."

Freemasonry was a widely accepted institution in the late 18th century, known for advocating moral teachings. Washington was attracted to the Masons' dedication to the Enlightenment principles of rationality, reason, and brotherhood. The American Masonic lodges did not share the anti-clerical perspective of the controversial European lodges. A Masonic lodge was established in Fredericksburg in September 1752, and Washington was initiated two months later at the age of 20 as one of its first Entered Apprentices. Within a year, he progressed through its ranks to become a Master Mason. Washington had a high regard for the Masonic Order, but his personal lodge attendance was sporadic. In 1777, a convention of Virginia lodges asked him to be the Grand Master of the newly established Grand Lodge of Virginia, but he declined due to his commitments leading the Continental Army. After 1782, he corresponded frequently with Masonic lodges and members, and he was listed as Master in the Virginia charter of Alexandria Lodge No. 22 in 1788.

In Washington's lifetime, slavery was deeply ingrained in the economic and social fabric of Virginia. Washington owned and worked African slaves his entire adult life. He acquired them through inheritance, gained control of eighty-four dower slaves on his marriage to Martha and purchased at least seventy-one slaves between 1752 and 1773. His early views on slavery were no different from any Virginia planter of the time. He demonstrated no moral qualms about the institution and referred to his slaves as "a Species of Property". From the 1760s his attitudes underwent a slow evolution. The first doubts were prompted by his transition from tobacco to grain crops which left him with a costly surplus of slaves, causing him to question the economic efficiency of the system. His growing disillusionment with the institution was spurred by the principles of the American Revolution and revolutionary friends such as Lafayette and Hamilton. Most historians agree the Revolution was central to the evolution of Washington's attitudes on slavery; "After 1783", Kenneth Morgan writes, "...[Washington] began to express inner tensions about the problem of slavery more frequently, though always in private..."

The many contemporary reports of slave treatment at Mount Vernon are varied and conflicting. Historian Kenneth Morgan ("2000") maintains that Washington was frugal on spending for clothes and bedding for his slaves, and only provided them with just enough food, and that he maintained strict control over his slaves, instructing his overseers to keep them working hard from dawn to dusk year round. However, historian Dorothy Twohig (2001) said: "Food, clothing, and housing seem to have been at least adequate". Washington faced growing debts involved with the costs of supporting slaves. He held an "ingrained sense of racial superiority" over African Americans, but harbored no ill feelings toward them.

Some slave families worked at different locations on the plantation but were allowed to visit one another on their days off. Washington's slaves received two hours off for meals during the workday, and given time off on Sundays and religious holidays. Washington frequently cared for ill or injured slaves personally, and he provided physicians and midwives and had his slaves inoculated for smallpox. In May 1796, Martha's personal and favorite slave Ona Judge escaped to Portsmouth. At Martha's behest Washington attempted to capture Ona, using a Treasury agent, but this effort failed. In February 1797, Washington's personal slave Hercules escaped to Philadelphia and was never found.

Some accounts report that Washington opposed flogging, but at times sanctioned its use, generally as a last resort, on both male and female slaves. Washington used both reward and punishment to encourage discipline and productivity in his slaves. He tried appealing to an individual's sense of pride, gave better blankets and clothing to the "most deserving", and motivated his slaves with cash rewards. He believed "watchfulness and admonition" to be often better deterrents against transgressions, but would punish those who "will not do their duty by fair means". Punishment ranged in severity from demotion back to fieldwork, through whipping and beatings, to permanent separation from friends and family by sale. Historian Ron Chernow maintains that overseers were required to warn slaves before resorting to the lash and required Washington's written permission before whipping, though his extended absences did not always permit this. Washington remained dependent on slave labor to work his farms and negotiated the purchase of more slaves in 1786 and 1787.

In February 1786, Washington took a census of Mount Vernon and recorded 224 slaves.
By 1799, slaves at Mount Vernon totaled 317, including 143 children. Washington owned 124 slaves, leased 40, and held 153 for his wife's dower interest. Washington supported many slaves who were too young or too old to work, greatly increasing Mount Vernon's slave population and causing the plantation to operate at a loss.

Based on his letters, diary, documents, accounts from colleagues, employees, friends and visitors, Washington slowly developed a cautious sympathy toward abolitionism that eventually ended with the emancipation of his own slaves. As president, he kept publicly silent on slavery, believing it was a nationally divisive issue that could destroy the union.

In a 1778 letter to Lund Washington, he made clear his desire "to get quit of Negroes" when discussing the exchange of slaves for land he wanted to buy. The next year, he stated his intention not to separate families as a result of "a change of masters". During the 1780s Washington privately expressed his support for gradual emancipation of slaves. Between 1783 and 1786 he gave moral support to a plan proposed by Lafayette to purchase land and free slaves to work on it, but declined to participate in the experiment. Washington privately expressed support for emancipation to prominent Methodists Thomas Coke and Francis Asbury in 1785, but declined to sign their petition. In personal correspondence the next year, he made clear his desire to see the institution of slavery ended by a gradual legislative process, a view that correlated with the mainstream antislavery literature published in the 1780s that Washington possessed. He significantly reduced his purchases of slaves after the war, but continued to acquire them in small numbers.
In 1788, Washington declined a suggestion from a leading French abolitionist, Jacques Brissot, to establish an abolitionist society in Virginia, stating that although he supported the idea, the time was not yet right to confront the issue. The historian Henry Wiencek (2003) believes, based on a remark that appears in the notebook of his biographer David Humphreys, that Washington considered making a public statement by freeing his slaves on the eve of his presidency in 1789. The historian Philip D. Morgan (2005) disagrees, believing the remark was a "private expression of remorse" at his inability to free his slaves. Other historians agree with Morgan that Washington was determined not to risk national unity over an issue as divisive as slavery. Washington never responded to any of the antislavery petitions he received, and the subject was not mentioned in either his last address to Congress or his Farewell Address.

The first clear indication that Washington was seriously intending to free his own slaves appears in a letter written to his secretary, Tobias Lear, in 1794. Washington instructed Lear to find buyers for his land in western Virginia, explaining in a private coda that he was doing so "to liberate a certain species of property which I possess, very repugnantly to my own feelings". The plan, along with others Washington considered in 1795 and 1796, could not be realized because of his failure to find buyers for his land, his reluctance to break up slave families and the refusal of the Custis heirs to help prevent such separations by freeing their dower slaves at the same time.

On July 9, 1799, Washington finished making his last will; the longest provision concerned slavery. All his slaves were to be freed after the death of his wife Martha. Washington said he did not free them immediately because his slaves intermarried with his wife's dower slaves. He forbade their sale or transportation out of Virginia. His will provided that old and young freed people be taken care of indefinitely; younger ones were to be taught to read and write and placed in suitable occupations. Washington freed more than 160 slaves, including 25 he had acquired from his wife's brother in payment of a debt freed by graduation. He was among the few large slave-holding Virginians during the Revolutionary Era who emancipated their slaves.

On January 1, 1801, one year after George Washington's death, Martha Washington signed an order freeing his slaves. Many of them, having never strayed far from Mount Vernon, were naturally reluctant to try their luck elsewhere; others refused to abandon spouses or children still held as dower slaves (the Custis estate) and also stayed with or near Martha. Following George Washington's instructions in his will, funds were used to feed and clothe the young, aged, and sickly slaves until the early 1830s.

Washington's legacy endures as one of the most influential in American history, since he served as commander-in-chief of the Continental Army, a hero of the Revolution, and the first president of the United States. Various historians maintain that he also was a dominant factor in America's founding, the Revolutionary War, and the Constitutional Convention. Revolutionary War comrade Light-Horse Harry Lee as "First in war—first in peace—and first in the hearts of his countrymen". Lee's words became the hallmark by which Washington's reputation was impressed upon the American memory, with some biographers regarding him as the great exemplar of republicanism. He set many precedents for the national government and the presidency in particular, and he was called the "Father of His Country" as early as 1778.

In 1885, Congress proclaimed Washington's birthday to be a federal holiday. Twentieth-century biographer Douglas Southall Freeman concluded, "The great big thing stamped across that man is character." Modern historian David Hackett Fischer has expanded upon Freeman's assessment, defining Washington's character as "integrity, self-discipline, courage, absolute honesty, resolve, and decision, but also forbearance, decency, and respect for others".

Washington became an international symbol for liberation and nationalism, as the leader of the first successful revolution against a colonial empire. The Federalists made him the symbol of their party, but the Jeffersonians continued to distrust his influence for many years and delayed building the Washington Monument. Washington was elected a member of the American Academy of Arts and Sciences on January 31, 1781, before he had even begun his presidency. He was posthumously appointed to the grade of General of the Armies of the United States during the United States Bicentennial to ensure he would never be outranked; this was accomplished by the congressional joint resolution passed on January 19, 1976, with an effective appointment date of July 4, 1976.

Parson Weems wrote a hagiographic biography in 1809 to honor Washington. Historian Ron Chernow maintains that Weems attempted to humanize Washington, making him look less stern, and to inspire "patriotism and morality" and to foster "enduring myths", such as Washington's refusal to lie about damaging his father's cherry tree. Weems' accounts have never been proven or disproven. Historian John Ferling, however, maintains that Washington remains the only founder and president ever to be referred to as "godlike", and points out that his character has been the most scrutinized by historians, past and present. Historian Gordon S. Wood concludes that "the greatest act of his life, the one that gave him his greatest fame, was his resignation as commander-in-chief of the American forces." Chernow suggests that Washington was "burdened by public life" and divided by "unacknowledged ambition mingled with self-doubt". A 1993 review of presidential polls and surveys consistently ranked Washington number 4, 3, or2 among presidents. A 2018 Siena College Research Institute survey ranked him number1 among presidents.

Jared Sparks began collecting and publishing Washington's documentary record in the 1830s in "Life and Writings of George Washington" (12 vols., 1834–1837). "The Writings of George Washington from the Original Manuscript Sources, 1745–1799" (1931–1944) is a 39-volume set edited by John Clement Fitzpatrick, who was commissioned by the George Washington Bicentennial Commission. It contains more than 17,000 letters and documents and is available online from the University of Virginia.

Numerous universities, including George Washington University and Washington University in St. Louis, were named in honor of Washington.

Many places and monuments have been named in honor of Washington, most notably the nation's capital Washington, D.C. The state of Washington is the only state to be named after a president.

George Washington appears on contemporary U.S. currency, including the one-dollar bill and the quarter-dollar coin (the Washington quarter). Washington and Benjamin Franklin appeared on the in 1847. Washington has since appeared on many postage issues, more than any other person.


















</doc>
<doc id="11969" url="https://en.wikipedia.org/wiki?curid=11969" title="Gulf Coast of the United States">
Gulf Coast of the United States

The Gulf Coast of the United States is the coastline along the Southern United States where they meet the Gulf of Mexico. The coastal states that have a shoreline on the Gulf of Mexico are Texas, Louisiana, Mississippi, Alabama, and Florida, and these are known as the "Gulf States".

The economy of the Gulf Coast area is dominated by industries related to energy, petrochemicals, fishing, aerospace, agriculture, and tourism. The large cities of the region are (from west to east) McAllen, Brownsville, Corpus Christi, Houston, Galveston, Beaumont, Lake Charles, Lafayette, Baton Rouge, New Orleans, Gulfport, Biloxi, Mobile, Pensacola, Tallahassee, St. Petersburg, Tampa, and increasingly, Sarasota. All are the centers of their respective metropolitan areas and most contain large ports. (Baton Rouge is relatively far from the Gulf of Mexico; its port is on the Mississippi River, as is the port of New Orleans.)

The Gulf Coast is made of many inlets, bays, and lagoons. The coast is also intersected by numerous rivers, the largest of which is the Mississippi River. Much of the land along the Gulf Coast is, or was, marshland. Ringing the Gulf Coast is the Gulf Coastal Plain, which reaches from Southern Texas to the western Florida Panhandle, while the western portions of the Gulf Coast are made up of many barrier islands and peninsulas, including the Padre Island along the Texas coast. These landforms protect numerous bays and inlets providing as a barrier to oncoming waves. The central part of the Gulf Coast, from eastern Texas through Louisiana, consists primarily of marshland. The eastern part of the Gulf Coast, predominantly Florida, is dotted with many bays and inlets.

The Gulf Coast climate is humid subtropical, although the southwestern tip of Florida, such as Everglades City, features a tropical climate. Much of the year is warm to hot along the Gulf Coast, while the 3 winter months bring periods of cool (or rarely, cold) weather mixed with mild temperatures. The area is vulnerable to hurricanes as well as floods and severe thunderstorms. Much of the Gulf Coast has a summer precipitation maximum, with July or August commonly the wettest month due to the combination of frequent summer thunderstorms produced by relentless heat and humidity, and tropical weather systems (tropical depressions, tropical storms and hurricanes), while winter and early spring rainfall also can be heavy. This pattern is evident at Houston, Texas, New Orleans, Louisiana, Mobile, Alabama and Pensacola, Florida. However, the central and southern Florida peninsula and South Texas has a pronounced winter dry season, as at Tampa and Fort Myers, Florida. On the central and southern Texas coast, winter, early spring and mid-summer are markedly drier, and September is the wettest month on average (as at Corpus Christi and Brownsville, Texas). Tornadoes are infrequent at the coast but do occur; however, they occur more frequently in inland portions of Gulf Coast states. Over most of the Gulf Coast from Houston, Texas eastward, extreme rainfall events are a significant threat, commonly from tropical weather systems, which can bring 4 to 10 or more inches of rain in a single day. In August 2017, Hurricane Harvey made landfall along the central Texas coast, then migrated to and stalled over the greater Houston area for several days, producing extreme, unprecedented rainfall totals of over 40 inches (1,000 mm) in many areas, unleashing widespread flooding. Earthquakes are extremely rare to the area, but a surprising 6.0 earthquake in the Gulf of Mexico on September 10, 2006, could be felt from the cities of New Orleans to Tampa.

The Gulf Coast is a major center of economic activity. The marshlands along the Louisiana and Texas coasts provide breeding grounds and nurseries for ocean life that drive the fishing and shrimping industries. The Port of South Louisiana (Metropolitan New Orleans in Laplace) and the Port of Houston are two of the ten busiest ports in the world by cargo volume. As of 2004, seven of the top ten busiest ports in the U.S. are on the Gulf Coast.

The discovery of oil and gas deposits along the coast and offshore, combined with easy access to shipping, have made the Gulf Coast the heart of the U.S. petrochemical industry. The coast contains nearly 4,000 oil platforms.

Besides the above, the region features other important industries including aerospace and biomedical research, as well as older industries such as agriculture and — especially since the development of the Gulf Coast beginning in the 1920s and the increase in wealth throughout the United States — tourism.

Before Europeans arrived in the region, the region was home to several pre-Columbian kingdoms that had extensive trade networks with empires such as the Aztecs and the Mississippi Mound Builders. Shark and alligator teeth and shells from the Gulf have been found as far north as Ohio, in the mounds of the Hopewell culture.

The first Europeans to settle the Gulf Coast were primarily the French and the Spanish. The Louisiana Purchase, Adams–Onís Treaty and the Texas Revolution made the Gulf Coast a part of the United States during the first half of the 19th century. As the U.S. population continued to expand its frontiers westward, the Gulf Coast was a natural magnet in the South providing access to shipping lanes and both national and international commerce. The development of sugar and cotton production (enabled by slavery) allowed the South to prosper. By the mid 19th century the city of New Orleans, being situated as a key to commerce on the Mississippi River and in the Gulf, had become the largest U.S. city not on the Atlantic seaboard and the fourth largest in the U.S. overall.

Two major events were turning points in the earlier history of the Gulf Coast region. The first was the American Civil War, which caused severe damage to some economic sectors in the South, including the Gulf Coast. The second event was the Galveston Hurricane of 1900. At the end of the 19th century Galveston was, with New Orleans, one of the most developed cities in the region. The city had the third busiest port in the U.S. and its financial district was known as the "Wall Street of the South". The storm mostly destroyed the city, which has never regained its former glory, and set back development in the region.
Since then the Gulf Coast has been hit with numerous other hurricanes. On August 29, 2005, Hurricane Katrina struck the Gulf Coast as a Category 4 hurricane. It was the most damaging storm in the history of the United States, causing upwards of $80 billion in damages, and leaving over 1,800 dead. Again in 2008 the Gulf Coast was struck by a catastrophic hurricane. Due to its immense size, Hurricane Ike caused devastation from the Louisiana coastline all the way to the Kenedy County, Texas region near Corpus Christi. In addition, Ike caused flooding and significant damage along the Mississippi coastline and the Florida Panhandle Ike killed 112 people and left upwards of 300 people missing, never to be found. Hurricane Ike was the third most damaging storm in the history of the United States, causing more than $25 billion in damage along the coast, leaving hundreds of thousands of people homeless, and sparking the largest search-and-rescue operation in U.S. history.

Other than the hurricanes, the Gulf Coast has redeveloped dramatically over the course of the 20th century. The gulf coast is highly populated. The petrochemical industry, launched with the major discoveries of oil in Texas and spurred on by further discoveries in the Gulf waters, has been a vehicle for development in the central and western Gulf which has spawned development on a variety of fronts in these regions. Texas in particular has benefited tremendously from this industry over the course of the 20th century and economic diversification has made the state a magnet for population and home to more Fortune 500 companies than any other U.S. state. Florida has grown as well, driven to a great extent by its long established tourism industry but also by its position as a gateway to the Caribbean and Latin America. As of 2006, these two states are the second and fourth most populous states in the nation, respectively (see this article). Other areas of the Gulf Coast have benefited less, though economic development fueled by tourism has greatly increased property values along the coast, and is now a severe danger to the valuable but fragile ecosystems of the Gulf Coast.

The following table lists the 15 largest MSAs along the Gulf Coast.




</doc>
<doc id="11971" url="https://en.wikipedia.org/wiki?curid=11971" title="Galaxy formation and evolution">
Galaxy formation and evolution

The study of galaxy formation and evolution is concerned with the processes that formed a heterogeneous universe from a homogeneous beginning, the formation of the first galaxies, the way galaxies change over time, and the processes that have generated the variety of structures observed in nearby galaxies. Galaxy formation is hypothesized to occur from structure formation theories, as a result of tiny quantum fluctuations in the aftermath of the Big Bang. The simplest model in general agreement with observed phenomena is the Lambda-CDM model—that is, that clustering and merging allows galaxies to accumulate mass, determining both their shape and structure.

Because of the inability to conduct experiments in outer space, the only way to “test” theories and models of galaxy evolution is to compare them with observations. Explanations for how galaxies formed and evolved must be able to predict the observed properties and types of galaxies.

Edwin Hubble created the first galaxy classification scheme known as the Hubble tuning-fork diagram. It partitioned galaxies into ellipticals, normal spirals, barred spirals (such as the Milky Way), and irregulars. These galaxy types exhibit the following properties which can be explained by current galaxy evolution theories:


There is a common misconception that Hubble believed incorrectly that the tuning fork diagram described an evolutionary sequence for galaxies, from elliptical galaxies through lenticulars to spiral galaxies. This is not the case; instead, the tuning fork diagram shows an evolution from simple to complex with no temporal connotations intended. Astronomers now believe that disk galaxies likely formed first, then evolved into elliptical galaxies through galaxy mergers.

Current models also predict that the majority of mass in galaxies is made up of dark matter, a substance which is not directly observable, and might not interact through any means except gravity. This observation arises because galaxies could not have formed as they have, or rotate as they are seen to, unless they contain far more mass than can be directly observed.

The earliest stage in the evolution of galaxies is the formation. When a galaxy forms, it has a disk shape and is called a spiral galaxy due to spiral-like "arm" structures located on the disk. There are different theories on how these disk-like distributions of stars develop from a cloud of matter: however, at present, none of them exactly predicts the results of observation.

Olin Eggen, Donald Lynden-Bell, and Allan Sandage in 1962, proposed a theory that disk galaxies form through a monolithic collapse of a large gas cloud. The distribution of matter in the early universe was in clumps that consisted mostly of dark matter. These clumps interacted gravitationally, putting tidal torques on each other that acted to give them some angular momentum. As the baryonic matter cooled, it dissipated some energy and contracted toward the center. With angular momentum conserved, the matter near the center speeds up its rotation. Then, like a spinning ball of pizza dough, the matter forms into a tight disk. Once the disk cools, the gas is not gravitationally stable, so it cannot remain a singular homogeneous cloud. It breaks, and these smaller clouds of gas form stars. Since the dark matter does not dissipate as it only interacts gravitationally, it remains distributed outside the disk in what is known as the dark halo. Observations show that there are stars located outside the disk, which does not quite fit the "pizza dough" model. It was first proposed by Leonard Searle and Robert Zinn that galaxies form by the coalescence of smaller progenitors. Known as a top-down formation scenario, this theory is quite simple yet no longer widely accepted.

More recent theories include the clustering of dark matter halos in the bottom-up process. Instead of large gas clouds collapsing to form a galaxy in which the gas breaks up into smaller clouds, it is proposed that matter started out in these “smaller” clumps (mass on the order of globular clusters), and then many of these clumps merged to form galaxies, which then were drawn by gravitation to form galaxy clusters. This still results in disk-like distributions of baryonic matter with dark matter forming the halo for all the same reasons as in the top-down theory. Models using this sort of process predict more small galaxies than large ones, which matches observations.

Astronomers do not currently know what process stops the contraction. In fact, theories of disk galaxy formation are not successful at producing the rotation speed and size of disk galaxies. It has been suggested that the radiation from bright newly formed stars, or from an active galactic nucleus can slow the contraction of a forming disk. It has also been suggested that the dark matter halo can pull the galaxy, thus stopping disk contraction.

The Lambda-CDM model is a cosmological model that explains the formation of the universe after the Big Bang. It is a relatively simple model that predicts many properties observed in the universe, including the relative frequency of different galaxy types; however, it underestimates the number of thin disk galaxies in the universe. The reason is that these galaxy formation models predict a large number of mergers. If disk galaxies merge with another galaxy of comparable mass (at least 15 percent of its mass) the merger will likely destroy, or at a minimum greatly disrupt the disk, and the resulting galaxy is not expected to be a disk galaxy (see next section). While this remains an unsolved problem for astronomers, it does not necessarily mean that the Lambda-CDM model is completely wrong, but rather that it requires further refinement to accurately reproduce the population of galaxies in the universe.

Elliptical galaxies (such as IC 1101) are among some of the largest known thus far. Their stars are on orbits that are randomly oriented within the galaxy (i.e. they are not rotating like disk galaxies). A distinguishing feature of elliptical galaxies is that the velocity of the stars does not necessarily contribute to flattening of the galaxy, such as in spiral galaxies. Elliptical galaxies have central supermassive black holes, and the masses of these black holes correlate with the galaxy's mass.

Elliptical galaxies have two main stages of evolution. The first is due to the supermassive black hole growing by accreting cooling gas. The second stage is marked by the black hole stabilizing by suppressing gas cooling, thus leaving the elliptical galaxy in a stable state. The mass of the black hole is also correlated to a property called sigma which is the dispersion of the velocities of stars in their orbits. This relationship, known as the M-sigma relation, was discovered in 2000. Elliptical galaxies mostly lack disks, although some bulges of disk galaxies resemble elliptical galaxies. Elliptical galaxies are more likely found in crowded regions of the universe (such as galaxy clusters).

Astronomers now see elliptical galaxies as some of the most evolved systems in the universe. It is widely accepted that the main driving force for the evolution of elliptical galaxies is mergers of smaller galaxies. Many galaxies in the universe are gravitationally bound to other galaxies, which means that they will never escape their mutual pull. If the galaxies are of similar size, the resultant galaxy will appear similar to neither of the progenitors, but will instead be elliptical. There are many types of galaxy mergers, which do not necessarily result in elliptical galaxies, but result in a structural change. For example, a minor merger event is thought to be occurring between the Milky Way and the Magellanic Clouds.

Mergers between such large galaxies are regarded as violent, and the frictional interaction of the gas between the two galaxies can cause gravitational shock waves, which are capable of forming new stars in the new elliptical galaxy. By sequencing several images of different galactic collisions, one can observe the timeline of two spiral galaxies merging into a single elliptical galaxy.

In the Local Group, the Milky Way and the Andromeda Galaxy are gravitationally bound, and currently approaching each other at high speed. Simulations show that the Milky Way and Andromeda are on a collision course, and are expected to collide in less than five billion years. During this collision, it is expected that the Sun and the rest of the Solar System will be ejected from its current path around the Milky Way. The remnant could be a giant elliptical galaxy.

One observation (see above) that must be explained by a successful theory of galaxy evolution is the existence of two different populations of galaxies on the galaxy color-magnitude diagram. Most galaxies tend to fall into two separate locations on this diagram: a "red sequence" and a "blue cloud". Red sequence galaxies are generally non-star-forming elliptical galaxies with little gas and dust, while blue cloud galaxies tend to be dusty star-forming spiral galaxies.

As described in previous sections, galaxies tend to evolve from spiral to elliptical structure via mergers. However, the current rate of galaxy mergers does not explain how all galaxies move from the "blue cloud" to the "red sequence". It also does not explain how star formation ceases in galaxies. Theories of galaxy evolution must therefore be able to explain how star formation turns off in galaxies. This phenomenon is called galaxy "quenching".

Stars form out of cold gas (see also the Kennicutt-Schmidt law), so a galaxy is quenched when it has no more cold gas. However, it is thought that quenching occurs relatively quickly (within 1 billion years), which is much shorter than the time it would take for a galaxy to simply use up its reservoir of cold gas. Galaxy evolution models explain this by hypothesizing other physical mechanisms that remove or shut off the supply of cold gas in a galaxy. These mechanisms can be broadly classified into two categories: (1) preventive feedback mechanisms that stop cold gas from entering a galaxy or stop it from producing stars, and (2) ejective feedback mechanisms that remove gas so that it cannot form stars.

One theorized preventive mechanism called “strangulation” keeps cold gas from entering the galaxy. Strangulation is likely the main mechanism for quenching star formation in nearby low-mass galaxies. The exact physical explanation for strangulation is still unknown, but it may have to do with a galaxy's interactions with other galaxies. As a galaxy falls into a galaxy cluster, gravitational interactions with other galaxies can strangle it by preventing it from accreting more gas. For galaxies with massive dark matter halos, another preventive mechanism called “virial shock heating” may also prevent gas from becoming cool enough to form stars.

Ejective processes, which expel cold gas from galaxies, may explain how more massive galaxies are quenched. One ejective mechanism is caused by supermassive black holes found in the centers of galaxies. Simulations have shown that gas accreting onto supermassive black holes in galactic centers produces high-energy jets; the released energy can expel enough cold gas to quench star formation.

Our own Milky Way and the nearby Andromeda Galaxy currently appear to be undergoing the quenching transition from star-forming blue galaxies to passive red galaxies.




</doc>
<doc id="11973" url="https://en.wikipedia.org/wiki?curid=11973" title="Generation X">
Generation X

Generation X (or Gen X for short) is the demographic cohort following the baby boomers and preceding the millennials. Researchers and popular media typically use birth years around 1965 to 1980 to define Generation Xers, although some sources use birth years beginning as early as 1960 and ending somewhere from 1977 to 1985. By this definition and U.S. Census data, there are 65.2 million Gen Xers in the United States as of 2019. Most members of Generation X are the children of the Silent Generation and early boomers; Xers are also often the parents of millennials and Generation Z.

As children in the 1970s and 1980s, a time of shifting societal values, Gen Xers were sometimes called the "latchkey generation", due to reduced adult supervision compared to previous generations. This was a result of increasing divorce rates and increased maternal participation in the workforce, prior to widespread availability of childcare options outside the home. As adolescents and young adults in the 1980s and 1990s, Xers were dubbed the "MTV Generation" (a reference to the music video channel), sometimes being characterized as slackers, cynical, and disaffected. Some of the cultural influences on Gen X youth were the musical genres of grunge and hip hop music, and independent films. In midlife, research describes them as active, happy, and achieving a work–life balance. The cohort has been credited with entrepreneurial tendencies, and was the last generation in the United States for whom post-secondary education was broadly financially remunerative.

The term "Generation X" has been used at various times to describe alienated youth. In the early 1950s, Hungarian photographer Robert Capa first used "Generation X" as the title for a photo-essay about young men and women growing up immediately following World War II. The term first appeared in print in a December 1952 issue of "Holiday" magazine announcing their upcoming publication of Capa's photo-essay. From 1976 to 1981, English musician Billy Idol used the moniker as the name for his punk rock band. Idol had attributed the name of his band to the book "Generation X", a 1965 book on British popular youth culture written by journalists Jane Deverson and Charles Hamblett — a copy of which had been owned by Idol's mother. These uses of the term appear to have no connection to Robert Capa's photo-essay.

The term acquired its contemporary application after the release of "", a 1991 novel written by Canadian author Douglas Coupland. In 1987, Coupland had written a piece in "Vancouver Magazine" titled "Generation X" which was "the seed of what went on to become the book". Coupland referenced Billy Idol's band Generation X in the 1987 article and again in 1989 in "Vista" magazine. In the book proposal for his novel, Coupland writes that "Generation X" is "taken from the name of Billy Idol’s long-defunct punk band of the late 1970s". However, in 1995 Coupland denied the term's connection to the band, stating that:
"The book's title came not from Billy Idol's band, as many supposed, but from the final chapter of a funny sociological book on American class structure titled "", by Paul Fussell. In his final chapter, Fussell named an 'X' category of people who wanted to hop off the merry-go-round of status, money, and social climbing that so often frames modern existence."

Author William Strauss noted that around the time Coupland's 1991 novel was published the symbol "X" was prominent in popular culture, as the film "Malcolm X" was released in 1992, and that the name "Generation X" ended up sticking. The "X" refers to an unknown variable or to a desire not to be defined. Strauss's coauthor Neil Howe noted the delay in naming this demographic cohort saying, "Over 30 years after their birthday, they didn't have a name. I think that's germane." Previously, the cohort had been referred to as Post-Boomers, Baby Busters (referencing the drop in the birth rates following the baby boom), New Lost Generation, latchkey kids, MTV Generation, and the 13th Generation (the 13th generation since American independence).

Generation X is the demographic cohort following the post–World War II baby-boom, representing a generational change from the baby boomers. Many researchers and demographers use dates which correspond to the fertility-patterns in the population. For Generation X, in the U.S. (and broadly, in the Western world), the period begins at a time when fertility rates started to significantly decrease, following the baby boom peak of the late 1950s, until an upswing in the late 1970s and eventual recovery at the start of the 1980s.

In the U.S., the Pew Research Center, a non-partisan think-tank, delineates a Generation X period of 1965–1980 which has, albeit gradually, come to gain acceptance in academic circles. Moreover, although fertility rates are preponderant in the definition of start and end dates, the center remarks: "Generations are analytical constructs, it takes time for popular and expert consensus to develop as to the precise boundaries that demarcate one generation from another." Pew takes into account other factors, notably the labor market as well as attitudinal and behavioral trends of a group. Writing for Pew's "Trend" magazine in 2018, psychologist Jean Twenge observed that the "birth year boundaries of Gen X are debated but settle somewhere around 1965–1980". According to this definition, the oldest Gen Xer is 55 years old and the youngest is, or is turning, 40 years old in 2020.
The Brookings Institution, another U.S. think-tank, sets the Gen X period as between 1965 and 1981. With regard to federal agencies, the U.S. Federal Reserve Board use 1965–1980 to define Gen X. The U.S. Social Security Administration (SSA) defines the years for Gen X as between 1964 and 1979. The US Department of Defense (DoD), conversely, use dates 1965 to 1977. In their 2002 book "When Generations Collide", Lynne Lancaster and David Stillman use 1965 to 1980, while in 2012 authors Jain and Pant also used parameters of 1965 to 1980. U.S. news outlets such as "The New York Times" and "The Washington Post" describe Generation X as people born between 1965 and 1980. Bloomberg, "Business Insider", and "Forbes" use 1965–1980. "Time" magazine states that Generation X is "roughly defined as anyone born between 1965 and 1980".

U.S. polling firm Gallup uses 1965–1979 to define Generation X. In Australia, the McCrindle Research Center uses parameters 1965–1979. In France, Dejoux, a researcher from the "Conservatoire National des Arts et Métiers (CNAM)", delimits dates of 1965 to 1980. In the UK, the Resolution Foundation think-tank defines Gen X as those born between 1966 and 1980. PricewaterhouseCoopers, a multinational professional services network headquartered in London, describes Generation X employees as those born from 1965 to 1980.

On the basis of the time it takes for a generation to mature, U.S. authors William Strauss and Neil Howe define Generation X as those born between 1961 and 1981 in their 1991 book titled "Generations". Jeff Gordinier, in his 2008 book "X Saves the World", also has a wider definition to include those born between 1961 and 1977 but possibly as late as 1980. George Masnick of the Harvard Joint Center for Housing Studies puts this generation in the time-frame of 1965 to 1984, in order to satisfy the premise that boomers, Xers, and millennials "cover equal 20-year age spans". In 2004, journalist J. Markert also acknowledged the 20-year increments but goes one step further and subdivides the generation into two 10-year cohorts with early and later members of the generation. The first begins in 1966 and ends in 1975 and the second begins in 1976 and ends in 1985; this thinking is applied to each generation (Silent, boomers, Gen X, millennials, etc.).

Based on external events of historical importance, Schewe and Noble in 2002 argue that a cohort is formed against significant milestones and can be any length of time. Against this logic, Generation X begins in 1966 and ends in 1976, with those born between 1955 and 1965 being labelled as "trailing-edge boomers".

In Canada, professor David Foot describes Generation X as late boomers and includes those born between 1960 and 1966, whilst the "Bust Generation", those born between 1967 and 1979, is considered altogether a separate generation, in his 1996 book "Boom Bust & Echo: How to Profit from the Coming Demographic Shift".

Individuals born in the Generation X and millennial cusp years of the late 1970s and early to mid-1980s have been identified by the media as a "microgeneration" with characteristics of both generations. Names given to these "cuspers" include Xennials, Generation Catalano, and the Oregon Trail Generation.

There are differences in Gen X population numbers depending on the date-range selected. In the U.S., using Census population projections, the Pew Research Center found that the Gen X population born from 1965 to 1980 numbered 65.2 million in 2019. The cohort is likely to overtake boomers in 2028. A 2010 Census report counted approximately 84 million people living in the US who are defined by birth years ranging from the early 1960s to the early 1980s. In a 2012 article for the Joint Center for Housing Studies of Harvard University, George Masnick wrote that the "Census counted 82.1 million" Gen Xers in the U.S. Masnick concluded that immigration filled in any birth year deficits during low fertility years of the late 1960s and early 1970s. Jon Miller at the Longitudinal Study of American Youth at the University of Michigan wrote that "Generation X refers to adults born between 1961 and 1981" and it "includes 84 million people". In their 1991 book "Generations", authors Howe and Strauss indicated that the total number of Gen X individuals in the U.S. was 88.5 million.

The birth control pill, introduced in 1960 by the U.S. Food and Drug Administration, was one contributing factor of declining birth rates. Initially, the pill spread rapidly amongst married women as an approved treatment for menstrual disturbance. However, it was also found to prevent pregnancy and was prescribed as a contraceptive in 1964. The pill, as it became commonly known, reached younger, unmarried college women in the late 1960s when state laws were amended and reduced the age of majority from 21 to ages 18–20. These policies are commonly referred to as the Early Legal Access (ELA) laws.

Another major factor was abortion, only available in a few states until its legalization in a 1973 US Supreme Court decision in "Roe v. Wade." This was replicated elsewhere, with reproductive rights legislation passed, notably in the UK (1967), France (1975), West Germany (1976), New Zealand (1977), Italy (1978), and the Netherlands (1980). From 1973 to 1980, the abortion rate per 1,000 US women aged 15–44 increased exponentially from 16% to 29% with more than 9.6 million terminations of pregnancy practiced. Between 1970 and 1980, on average, for every 10 American citizens born, 3 were aborted. However, increased immigration during the same period of time helped to partially offset declining birth-rates and contributed to making Generation X an ethnically and culturally diverse demographic cohort.

Generally, Gen Xers are the children of the Silent Generation and older baby boomers.

Strauss and Howe, who wrote several books on generations, including one specifically on Generation X titled "13th Gen: Abort, Retry, Ignore, Fail?" (1993), reported that Gen Xers were children at a time when society was less focused on children and more focused on adults. Xers were children during a time of increasing divorce rates, with divorce rates doubling in the mid-1960s, before peaking in 1980. Strauss and Howe described a cultural shift where the long-held societal value of staying together for the sake of the children was replaced with a societal value of parental and individual self-actualization. Strauss wrote that society "moved from what Leslie Fiedler called a 1950s-era 'cult of the child' to what Landon Jones called a 1970s-era 'cult of the adult'". "The Generation Map", a report from Australia's McCrindle Research Center writes of Gen X children: "their Boomer parents were the most divorced generation in Australian history". According to Christine Henseler in the 2012 book "Generation X Goes Global: Mapping a Youth Culture in Motion", "We watched the decay and demise (of the family), and grew callous to the loss."
The Gen X childhood coincided with the sexual revolution of the 1960s to 1980s, which Susan Gregory Thomas described in her book "In Spite of Everything" as confusing and frightening for children in cases where a parent would bring new sexual partners into their home. Thomas also discussed how divorce was different during the Gen X childhood, with the child having a limited or severed relationship with one parent following divorce, often the father, due to differing societal and legal expectations. In the 1970s, only nine U.S. states allowed for joint custody of children, which has since been adopted by all 50 states following a push for joint custody during the mid-1980s. "Kramer vs. Kramer", a 1979 American legal drama based on Avery Corman's best-selling novel, came to epitomize the struggle for child custody and the demise of the traditional nuclear family.
The rapid influx of boomer women into the labor force that began in the 1970s was marked by the confidence of many in their ability to successfully pursue a career while meeting the needs of their children. This resulted in an increase in latchkey children, leading to the terminology of the "latchkey generation" for Generation X. These children lacked adult supervision in the hours between the end of the school day and when a parent returned home from work in the evening, and for longer periods of time during the summer. Latchkey children became common among all socioeconomic demographics, but this was particularly so among middle- and upper-class children. The higher the educational attainment of the parents, the higher the odds the children of this time would be latchkey children, due to increased maternal participation in the workforce at a time before childcare options outside the home were widely available. McCrindle Research Centre described the cohort as "the first to grow up without a large adult presence, with both parents working", stating this led to Gen Xers being more peer-oriented than previous generations.

Some older Gen Xers started high school in the waning years of the Carter presidency, but much of the cohort became socially and politically conscious during the Reagan Era. President Ronald Reagan, voted in office principally by the boomer generation, embraced "laissez-faire" economics with vigor with cuts in the growth of government spending, reduction in taxes for the higher echelon of society, legalization of stock buybacks, and deregulation of key industries. Measures had drastic consequences on the social fabric of the country even if, gradually, reforms gained acceptability and exported overseas to willing participants. The early 1980s recession saw unemployment rise to 10.8% in 1982 requiring, more often than not, dual parental incomes. One-in-five American children grew up in poverty during this time. The federal debt almost tripled during Reagan's time in office, from $998 billion in 1981 to $2.857 trillion in 1989 placing greater burden on repayment on the incoming generation.
Government expenditure shifted from domestic programs to defense. Remaining funding initiatives, moreover, tended to be diverted away from programs for children and often directed toward the elderly population, with cuts to Medicaid and programs for children and young families, and protection and expansion of Medicare and Social Security for the elderly population. These programs for the elderly were not tied to economic need. Congressman David Durenberger criticized this political situation, stating that while programs for poor children and for young families were cut, the government provided "free health care to elderly millionaires".

Gen Xers came of age or were children during the 1980s crack epidemic, which disproportionately impacted urban areas as well as the African-American community in the U.S. Drug turf battles increased violent crime, and crack addiction impacted communities and families. Between 1984 and 1989, the homicide rate for black males aged 14 to 17 doubled in the U.S., and the homicide rate for black males aged 18 to 24 increased almost as much. The crack epidemic had a destabilizing impact on families with an increase in the number of children in foster care. In 1986, President Reagan signed the Anti-Drug Abuse Act to enforce strict mandatory minimum sentencing for drug users and increased the federal budget for supply-reduction efforts.

Fear of the impending AIDS epidemic of the 1980s and 1990s loomed over the formative years of Generation X. The emergence of AIDS coincided with Gen X's adolescence, with the disease first clinically observed in the U.S. in 1981. By 1985, an estimated one-to-two million Americans were HIV-positive. This particularly hit the LGBT community. As the virus spread, at a time before effective treatments were available, a public panic ensued. Sex education programs in schools were adapted to address the AIDS epidemic, which taught Gen X students that sex could kill you.

Gen Xers were the first children to have access to personal computers in their homes and at schools. Those born in the early cohort will have experienced the first analog machines whilst, at the late-end, will have been pioneers, at the forefront of the Internet revolution. In the early 1980s, the growth in the use of personal computers exploded with manufacturers such as Commodore, Atari, and Apple responding to the demand via 8- and 16-bit machines. This in turn stimulated the software industries with corresponding developments for backup storage, use of the floppy disk, zip drive, and CD-ROM. At school, several computer projects were supported by the Department of Education under U.S. Secretary Bell's "Technology Initiative". This was later mirrored in the UK's 1982 Computers for Schools programme and, in France, under the 1985 scheme "plan informatique pour tous (IPT)." During the mid-1990s, AOL dial-up modems in the U.S. enabled millions of late Xer teens to log online, paving the way for "digital native" millennials.

In the U.S., Generation X was the first cohort to grow up post-integration after the racist Jim Crow laws. They were described in a marketing report by "Specialty Retail" as the kids who "lived the civil rights movement". They were among the first children to be bused to attain integration in the public school system. In the 1990s, Strauss reported Gen Xers were "by any measure the least racist of today's generations". In the U.S., Title IX, which passed in 1972, provided increased athletic opportunities to Gen X girls in the public school setting. "Roots", based on the novel by Alex Haley and broadcast as a 12-hour series, was viewed as a turning point in the country's ability to relate to the afro-American history.

Although, globally, children and adolescents of Generation X will have been heavily influenced by U.S. cultural industries with shared global currents (e.g. rising divorce rates, the AIDS epidemic, advancements in ICT), there is not one U.S.-born raised concept but multiple perspectives and geographical outgrowths. Even within the period of analysis, inside national communities, commonalities will have differed on the basis of one's birth date. The generation, Christine Henseler also remarks, was shaped as much by real-world events, within national borders, determined by specific political, cultural, and historical incidents. She adds "In other words, it is in between both real, clearly bordered spaces and more fluid global currents that we can spot the spirit of Generation X."
In Russia, for example, Generation Xers are referred to as "the last Soviet children", as the last children to come of age prior to the downfall of communism in their nation and prior to the fall of the Soviet Union. Those that reached adulthood in the 1980s and grew up educated in the doctrines of Marxism and Leninism found themselves against a background of economic and social change with the advent of Mikhail Gorbatchev to power and Perestroika. However, even before the collapse of the Soviet Union and the disbanding of the Communist Party, surveys demonstrated that Russian young people repudiated the key features of the Communist worldview that their party leaders, schoolteachers, and even parents had tried to instill in them. This generation, caught in the transition between Marxism–Leninism and an unknown future, and wooed by the new domestic political classes, remained largely apathetic.

In France, "Generation X" is not as widely known or used to define its members. Demographically, those born early during that period were sometimes referred to as 'Génération Bof' because of their tendency to use the word 'bof', which, translated into English, means 'whatever". More closely associated is "Génération Mitterrand", pertaining to socialist François Mitterrand who served as President of France during two consecutive terms between 1981 and 1995. There is general agreement that, domestically, the event that is accepted in France as the separating point between baby boomer generation and Generation X are the French strikes and violent riots of May 1968. For those at the tail-end of the generation, educational and defense reforms, a new style "baccalauréat général" with three distinct streams in 1995 (the preceding programme, introduced in 1968) and the cessation of military conscription in 1997 (for those born after December 1978) are considered as new transition points to the next.

The United Kingdom's Economic and Social Research Council described Generation X as "Thatcher's children" because the cohort grew up while Margaret Thatcher was Prime Minister from 1979 to 1990, "a time of social flux and transformation". Those born in the late 1960s and early 1970s grew up in a period of social unrest. While unemployment was low in the early 1970s, industrial and social unrest escalated. Strike action culminated in the 'winter of discontent' in 1978–79, and the Troubles began to unfold in Northern Ireland. The turn to neoliberal policies introduced and maintained by consecutive conservative governments from 1979 to 1997 marked the end of the post-war consensus. Educationally, the vast majority of the cohort attended secondary modern schools, relabelled comprehensive schools with compulsory education ending at the age of 16. The Further and Higher Education Act 1992 and the liberalisation of higher education in the UK saw greater numbers gaining places, especially those born at the tail-end of the generation.

In Germany, "Generation X" is not widely used or applied. Instead, reference is made to "Generation Golf" in the previous West German republic, based on a novel by Florian Illies whilst, in the east, children of the "Mauerfall" or coming down of the wall. For former east Germans, there was adaptation but also a sense of loss of accustomed values and structures, sometimes turning into romantic narratives of their childhood. For those in the West, a period of discovery and exploration of what had been a forbidden land.

In South Africa, Gen Xers spent their formative years of the 1980s during the "hyper-politicized environment of the final years of apartheid".

In the U.S., compared to the boomer generation, Generation X was more educated than their parents with the share of young adults enrolling in college steadily increasing from 1983 before peaking in 1998. In 1965, as early boomers entered college, total enrollment of new undergraduates was just over 5.7 million individuals across the public and private sectors. By 1983, the first year of Gen X college enrollments (as per Pew Research's definition), this figure had reached 12.2 million, an increase of 53%, effectively a doubling in student intake. As the 1990s progressed, Gen X college enrollments continued to climb with increased loan borrowing as the cost of an education became substantially more expensive compared to their peers in the mid-1980s. By 1998, the generation's last year of college enrollment, those entering the higher education sector totaled 14.3 million. In addition, unlike Boomers and previous generations, women outpaced men in college completion rates.

For early Gen Xer graduates entering the job market at the end of the 1980s, economic conditions were challenging and did not show signs of major improvements until the mid-1990s. In the U.S., restrictive monetary policy to curb rising inflation and the collapse of a large number of savings and loan associations (private banks that specialized in home mortgages) impacted the welfare of many American households and precipitated a large government bailout that placed further strain on the budget. Furthermore, three decades of growth came to an end and the unwritten social contract between employers and employees, which had endured during the 1960s and 1970s and scheduled to last until retirement was no longer applicable with, by the late 1980s, large-scale layoffs of boomers, corporate downsizing, and accelerated offshoring of production.

On the political front, in the U.S. the generation became ambivalent if not outright disaffected with politics; they had been reared in the shadow of the Vietnam War and Watergate scandal, and came to maturity under the Reagan and Bush presidencies, with first-hand experience of the impact of neoliberal policies. Few had experienced a Democratic administration and even then, only, at an atmospheric level. For those on the left of the political spectrum, the disappointments with the previous boomer student mobilizations of the 1960s and the collapse of those movements towards a consumerist "greed is good" and "yuppie" culture during the 1980s felt, to a greater extent, hypocrisy if not outright betrayal. The end of communism and the socialist utopia with the fall of the Berlin Wall, moreover, added to the disillusionment that any alternative to the capitalist model was possible.

In 1990, "Time" magazine published an article titled "Living: Proceeding with Caution", which described those then in their 20s as aimless and unfocused. Media pundits and advertisers further struggled to define the cohort, typically portraying them as "unfocused twentysomethings". A MetLife report noted: "media would portray them as the "Friends" generation: rather self-involved and perhaps aimless...but fun". Gen Xers were often portrayed as apathetic or as "slackers", lacking bearings, a stereotype which was initially tied to Richard Linklater's comedic and essentially plotless 1991 film "Slacker". After the film was released, "journalists and critics thought they put a finger on what was different about these young adults in that 'they were reluctant to grow up' and 'disdainful of earnest action'". Ben Stiller's 1994 film "Reality Bites" also sought to capture the zeitgeist of the generation with a portrayal of the attitudes and lifestyle choices of the time.

Negative stereotypes of Gen X young adults continued, including that they were "bleak, cynical, and disaffected". In 1998, such stereotypes prompted sociological research at Stanford University to study the accuracy of the characterization of Gen X young adults as cynical and disaffected. Using the national General Social Survey, the researchers compared answers to identical survey questions asked of 18–29-year-olds in three different time periods. Additionally, they compared how older adults answered the same survey questions over time. The surveys showed 18–29-year-old Gen Xers did exhibit higher levels of cynicism and disaffection than previous cohorts of 18–29-year-olds surveyed. However, they also found that cynicism and disaffection had increased among all age groups surveyed over time, not just young adults, making this a period effect, not a cohort effect. In other words, adults of all ages were more cynical and disaffected in the 1990s, not just Generation X.

By the mid-late 1990s, under Bill Clinton's presidency, economic optimism had returned to the U.S., with unemployment reduced from 7.5% in 1992 to 4% in 2000. Younger members of Gen X, straddling across administrations, politically experienced a "liberal renewal". In 1997, "Time" magazine published an article titled "Generation X Reconsidered", which retracted the previously-reported negative stereotypes and reported positive accomplishments, citing Gen Xers' tendency to found technology start-ups and small businesses as well as their ambition, which research showed was higher among Gen X young adults than older generations. Yet, the slacker moniker stuck. As the decade progressed, Gen X gained a reputation for entrepreneurship. In 1999, "The New York Times" dubbed them "Generation 1099", describing them as the "once pitied but now envied group of self-employed workers whose income is reported to the Internal Revenue Service not on a W-2 form, but on Form 1099".
The development of the internet witnessed a frenzy of IT initiatives. Newly-created companies, launched on stock exchanges globally, were formed with dubitable revenue generation or cash flow. When the dot-com bubble eventually burst in 2000, early Gen Xers who had embarked as entrepreneurs in the IT industry riding the Internet wave, as well as newly-qualified programmers at the tail-end of the generation (who had grown up with AOL and the first Web browsers), were both caught in the crash. This had major repercussions, with cross-generational consequences; five years after the bubble burst, new matriculation of IT millennial undergraduates fell by 40% and by as much as 70% in some information systems programs.

However, following the crisis, sociologist Mike Males reported continued confidence and optimism among the cohort saying "surveys consistently find 80% to 90% of Gen Xers self-confident and optimistic". Males wrote "these young Americans should finally get the recognition they deserve", praising the cohort and stating that "the permissively raised, universally deplored Generation X is the true 'great generation', for it has braved a hostile social climate to reverse abysmal trends", describing them as the hardest-working group since the World War II generation. He reported Gen Xers' entrepreneurial tendencies helped create the high-tech industry that fueled the 1990s economic recovery. In 2002, "Time" magazine published an article titled "Gen Xers Aren't Slackers After All", reporting four out of five new businesses were the work of Gen Xers.

In the U.S., Gen Xers were described as the major heroes of the September 11 terrorist attacks by author William Strauss. The firefighters and police responding to the attacks were predominantly from Generation X. Additionally, the leaders of the passenger revolt on United Airlines Flight 93 were also, by majority, Gen Xers. Author Neil Howe reported survey data showing Gen Xers were cohabiting and getting married in increasing numbers following the terrorist attacks, with Gen X survey respondents reporting they no longer wanted to live alone. In October 2001, the "Seattle Post-Intelligencer" wrote of Gen Xers: "Now they could be facing the most formative events of their lives and their generation." The "Greensboro News & Record" reported members of the cohort "felt a surge of patriotism since terrorists struck" by giving blood, working for charities, donating to charities, and by joining the military to fight the War on Terror. "The Jury Expert", a publication of The American Society of Trial Consultants, reported: "Gen X members responded to the terrorist attacks with bursts of patriotism and national fervor that surprised even themselves."

In 2011, survey analysis from the "Longitudinal Study of American Youth" found Gen Xers (defined as those who were then between the ages of 30 and 50) to be "balanced, active, and happy" in midlife and as achieving a work-life balance. The Longitudinal Study of Youth is an NIH-NIA funded study by the University of Michigan which has been studying Generation X since 1987. The study asked questions such as "Thinking about all aspects of your life, how happy are you? If zero means that you are very unhappy and 10 means that you are very happy, please rate your happiness." LSA reported that "mean level of happiness was 7.5 and the median (middle score) was 8. Only four percent of Generation X adults indicated a great deal of unhappiness (a score of three or lower). Twenty-nine percent of Generation X adults were very happy with a score of 9 or 10 on the scale."

In 2016, a global consumer insights project from Viacom International Media Networks and Viacom, based on over 12,000 respondents across 21 countries, reported on Gen X's unconventional approach to sex, friendship, and family, their desire for flexibility and fulfillment at work and the absence of midlife crisis for Gen Xers. The project also included a 20 min documentary titled "Gen X Today". In 2014, Pew Research provided further insight, adding that the cohort was "savvy, skeptical and self-reliant; they're not into preening or pampering, and they just might not give much of a hoot what others think of them. Or whether others think of them at all." Furthermore, guides regarding managing multiple generations in the workforce describes Gen Xers as: independent, resilient, resourceful, self-managing, adaptable, cynical, pragmatic, skeptical of authority, and as seeking a work-life balance.

Individualism is one of the defining traits of Generation X, and reflected in their entrepreneurial spirit. In the 2008 book "X Saves the World: How Generation X Got the Shaft but Can Still Keep Everything from Sucking", author Jeff Gordinier describes Generation X as a "dark horse demographic" which "doesn't seek the limelight". Gordiner cites examples of Gen Xers' contributions to society such as: Google, Wikipedia, Amazon.com, and YouTube, arguing that if boomers had created them, "we'd never hear the end of it". In the book, Gordinier contrasts Gen Xers to baby boomers, saying boomers tend to trumpet their accomplishments more than Gen Xers do, creating what he describes as "elaborate mythologies" around their achievements. Gordiner cites Steve Jobs as an example, while Gen Xers, he argues, are more likely to "just quietly do their thing".

In a 2007 article published in the Harvard Business Review, authors Strauss and Howe wrote of Generation X: "They are already the greatest entrepreneurial generation in U.S. history; their high-tech savvy and marketplace resilience have helped America prosper in the era of globalization." According to authors Michael Hais and Morley Winograd:
Small businesses and the entrepreneurial spirit that Gen Xers embody have become one of the most popular institutions in America. There's been a recent shift in consumer behavior and Gen Xers will join the "idealist generation" in encouraging the celebration of individual effort and business risk-taking. As a result, Xers will spark a renaissance of entrepreneurship in economic life, even as overall confidence in economic institutions declines. Customers, and their needs and wants (including Millennials) will become the North Star for an entire new generation of entrepreneurs.

A 2015 study by Sage Group reports Gen Xers "dominate the playing field" with respect to founding startups in the United States and Canada, with Xers launching the majority (55%) of all new businesses in 2015. In addition, in the UK, a 2016 study of over 2,500 office workers conducted by Workfront found that survey respondents of all ages selected those from Generation X as the hardest-working employees in today's workforce (chosen by 60%). Gen X was also ranked highest among fellow workers for having the strongest work ethic (chosen by 59.5%), being the most helpful (55.4%), the most skilled (54.5%), and the best troubleshooters/problem-solvers (41.6%).

Unlike millennials, Generation X was the last generation in the U.S. for whom higher education was broadly financially remunerative. In 2019, the Federal Reserve Bank of St. Louis published research (using data from the 2016 "Survey of Consumer Finances") demonstrating that after controlling for race and age, cohort families with heads of household with post-secondary education and born before 1980 have seen wealth and income premiums, while, for those after 1980, the wealth premium has weakened to a point of statistical insignificance (in part because of the rising cost of college). The income premium, while remaining positive, has declined to historic lows, with more pronounced downward trajectories among heads of household with postgraduate degrees.

In terms of advocating for their children in the educational setting, author Neil Howe describes Gen X parents as distinct from baby boomer parents. Howe argues that Gen Xers are not helicopter parents, which Howe describes as a parenting style of boomer parents of millennials. Howe described Gen Xers instead as "stealth fighter parents", due to the tendency of Gen X parents to let minor issues go and to not hover over their children in the educational setting, but to intervene forcefully and swiftly in the event of more serious issues. In 2012, the Corporation for National and Community Service ranked Gen X volunteer rates in the U.S. at "29.4% per year", the highest compared with other generations. The rankings were based on a three-year moving average between 2009 and 2011.

A report titled "Economic Mobility: Is the American Dream Alive and Well?" focused on the income of males 30–39 in 2004 (those born April 1964March 1974). The study was released on 25 May 2007 and emphasized that this generation's men made less (by 12%) than their fathers had at the same age in 1974, thus reversing a historical trend. It concluded that, per year increases in household income generated by fathers/sons slowed from an average of 0.9% to 0.3%, barely keeping pace with inflation. "Family incomes have risen though (over the period 1947 to 2005) because more women have gone to work", "supporting the incomes of men, by adding a second earner to the family. And as with male income, the trend is downward."

Gen Xers were the first cohort to come of age with MTV. They were the first generation to experience the emergence of music videos as teenagers and are sometimes called the MTV Generation. Gen Xers were responsible for the alternative rock movement of the 1990s and 2000s, including the grunge subgenre. Hip hop has also been described as defining music of the generation, particularly artists such as Tupac Shakur, N.W.A., and The Notorious B.I.G..

From 1974 to 1976, a new generation of rock bands arose such as the Ramones, Johnny Thunders and the Heartbreakers, The Dictators in New York City, the Sex Pistols, the Clash, the Damned, and Buzzcocks in the UK, and the Saints in Brisbane. By late 1976, these acts were generally recognized as forming the vanguard of "punk rock", and as 1977 approached, punk rock became a major and highly controversial cultural phenomenon in the UK. It spawned a punk subculture expressing youthful rebellion characterized by distinctive styles of clothing and adornment (ranging from deliberately offensive T-shirts, leather jackets, studded or spiked bands and jewelry, as well as bondage and S&M clothes) and a variety of anti-authoritarian ideologies that have since been associated with the form. By 1977 the influence of punk rock music and subculture became more pervasive, spreading throughout various countries worldwide. It generally took root in local scenes that tended to reject affiliation with the mainstream. In the late 1970s punk experienced its second wave in which acts that were not active during its formative years adopted the style.

While at first punk musicians were not Gen Xers themselves (many of them were late boomers, or Generation Jones), the fanbase for punk became increasingly Gen X-oriented as the earliest Xers entered their adolescence, and it therefore made a significant imprint on the cohort. By the 1980s, faster and more aggressive subgenres such as hardcore punk (e.g. Minor Threat), street punk (e.g. the Exploited, NOFX) and anarcho-punk (e.g. Subhumans) became the predominant modes of punk rock. Musicians identifying with or inspired by punk often later pursued other musical directions, resulting in a broad range of spinoffs, giving rise to genres such as post-punk, new wave and later indie pop, alternative rock, and noise rock. Gen Xers were no longer simply the consumers of punk but becoming the creators as well. By the 1990s punk rock re-emerged into the mainstream, as punk rock and pop punk bands with Gen X members such as Green Day, Rancid, The Offspring, and Blink-182 brought the genre widespread popularity.

A notable example of alternative rock is grunge music and the associated subculture that developed in the Pacific Northwest of the U.S. Grunge song lyrics have been called the "...product of Generation X malaise". Vulture commented: "the best bands arose from the boredom of latchkey kids". "People made records entirely to please themselves because there was nobody else to please" commented producer Jack Endino. Grunge lyrics are typically dark, nihilistic, angst-filled, anguished, and often addressing themes such as social alienation, despair and apathy. "The Guardian" wrote that grunge "didn't recycle banal cliches but tackled weighty subjects". Topics of grunge lyrics included homelessness, suicide, rape, broken homes, drug addiction, self-loathing, misogyny, domestic abuse and finding "meaning in an indifferent universe". Grunge lyrics tended to be introspective and aimed to enable the listener to see into hidden personal issues and examine depravity in the world. Notable grunge bands include: Nirvana, Pearl Jam, Alice in Chains, Stone Temple Pilots and Soundgarden.

The golden age of hip hop refers to hip hop music made from the mid-1980s to mid-1990s, typically by artists originating from the New York metropolitan area, that was characterized by its diversity, quality, innovation and influence after the genre's emergence and establishment in the previous decade. There were various types of subject matter, while the music was experimental and the sampling eclectic. The artists most often associated with the period are LL Cool J, Run–D.M.C., Public Enemy, the Beastie Boys, KRS-One, Eric B. & Rakim, De La Soul, Big Daddy Kane, EPMD, A Tribe Called Quest, Slick Rick, Ultramagnetic MC's, and the Jungle Brothers. Releases by these acts co-existed in this period with, and were as commercially viable as, those of early gangsta rap artists such as Ice-T, Geto Boys and N.W.A, the sex raps of 2 Live Crew and Too Short, and party-oriented music by acts such as Kid 'n Play, The Fat Boys, DJ Jazzy Jeff & The Fresh Prince and MC Hammer.

In addition to lyrical self-glorification, hip hop was also used as a form of social protest. Lyrical content from the era often drew attention to a variety of social issues including afrocentric living, drug use, crime and violence, religion, culture, the state of the American economy, and the modern man's struggle. Conscious and political hip hop tracks of the time were a response to the effects of American capitalism and former President Reagan's conservative political economy. According to Rose Tricia, "In rap, relationships between black cultural practice, social and economic conditions, technology, sexual and racial politics, and the institution policing of the popular terrain are complex and in constant motion". Even though hip hop was used as a mechanism for different social issues it was still very complex with issues within the movement itself. There was also often an emphasis on black nationalism. Hip hop artists often talked about urban poverty and the problems of alcohol, drugs, and gangs in their communities. Public Enemy's most influential song, "Fight the Power", came out at this time; the song speaks up to the government, proclaiming that people in the ghetto have freedom of speech and rights like every other American.

Gen Xers were largely responsible for the "indie film" movement of the 1990s, both as young directors and in large part as the movie audiences fueling demand for such films. In cinema, directors Kevin Smith, Quentin Tarantino, Sofia Coppola, John Singleton, Spike Jonze, David Fincher, Steven Soderbergh, and Richard Linklater have been called Generation X filmmakers. Smith is most known for his View Askewniverse films, the flagship film being "Clerks", which is set in New Jersey circa 1994, and focuses on two convenience-store clerks in their twenties. Linklater's "Slacker" similarly explores young adult characters who were interested in philosophizing. While not a member of Gen X himself, director John Hughes has been recognized as having created classic 1980s teen films with early Gen X characters which "an entire generation took ownership of", including "The Breakfast Club", "Sixteen Candles", "Weird Science", and "Ferris Bueller's Day Off". In France, a new movement emerged, the "Cinéma du look", spearheaded by filmmakers Luc Besson, Jean-Jacques Beineix and Leos Carax. Although not Gen Xers themselves, "Subway" (1985), "37°2 le matin" (English: "Betty Blue"; 1986), and "Mauvais Sang" (1986) sought to capture on screen the generation's malaise, sense of entrapment, and desire to escape.

The literature of early Gen Xers is often dark and introspective. In the U.S., authors such as Elizabeth Wurtzel, David Foster Wallace, Bret Easton Ellis, and Douglas Coupland captured the zeitgeist of this generation. In France, Michel Houellebecq and Frédéric Beigbeder rank among major novelists whose work also reflect the dissatisfaction and melancholies of the cohort. In the UK, Alex Garland, author of "The Beach" (1996), further added to the genre.

While previous research has indicated that the likelihood of heart attacks was declining among Americans aged 35 to 74, a 2018 study published in the American Heart Association's journal "Circulation" revealed that this was not the case among younger people. By analyzing data from 28,000 patients from across the United States who were hospitalized for heart attacks between 1995 and 2014, they found that a growing number of such patients were between the ages of 35 to 54. In particular, the number of heart-attack patients in this age group at the end of the study was 32%, up from 27% at the start of the study. This increase is most pronounced among women, for whom the number jumped from 21% to 31%. A common theme among those who suffered from heart attacks is that they also had high-blood pressure, diabetes, and chronic kidney disease. As before, such trends were found to be more common among women than among men. Experts suggest a number of reasons for this. Conditions such as coronary artery disease are traditionally viewed as a man's problem, and as such female patients are not considered high-risk individuals. Many women are not only the primary caretakers of their families but also full-time employees, meaning they do not take care of themselves as much as they should.

Generation X are usually the parents of Generation Z, and sometimes millennials. Jason Dorsey, who works for the Center of Generational Kinetics, observed that like their parents from Generation X, members of Generation Z tend to be autonomous and pessimistic. They need validation less than the millennials and typically become financially literate at an earlier age as many of their parents bore the full brunt of the Great Recession.




</doc>
<doc id="11974" url="https://en.wikipedia.org/wiki?curid=11974" title="Guam">
Guam

Guam (; ) is an organized, unincorporated territory of the United States in Micronesia in the western Pacific Ocean. It is the westernmost point and territory of the United States, along with the Northern Mariana Islands. The capital city of Guam is Hagåtña, and the most populous city is Dededo. Guam has been a member of the Pacific Community since 1983. The inhabitants of Guam are American citizens by birth. The indigenous Guamanians are the Chamorros, who are related to other Austronesian peoples of Indonesia, the Philippines, and Taiwan.

In 2016, 162,742 people resided on Guam. The territory has an area of and a population density of . In Oceania, it is the largest and southernmost of the Mariana Islands and the largest island in Micronesia. Among its municipalities, Mongmong-Toto-Maite has the highest population density at , whereas Inarajan and Umatac have the lowest density at . The highest point is Mount Lamlam at above sea level. Since the 1960s, the economy has been supported by two industries: tourism and the United States Armed Forces.

The indigenous Chamorros settled the island approximately 4,000 years ago. Portuguese explorer Ferdinand Magellan, while in the service of Spain, was the first European to visit the island, on March 6, 1521. Guam was colonized by Spain in 1668 with settlers, including Diego Luis de San Vitores, a Catholic Jesuit missionary. Between the 16th century and the 18th century, Guam was an important stopover for the Spanish Manila Galleons. During the Spanish–American War, the United States captured Guam on June 21, 1898. Under the Treaty of Paris, signed on December 10, 1898, Spain ceded Guam to the United States effective April 11, 1899. Guam is among the 17 non-self-governing territories listed by the United Nations.

Before World War II, there were five American jurisdictions in the Pacific Ocean: Guam and Wake Island in Micronesia, American Samoa and Hawaii in Polynesia, and the Philippines. On December 8, 1941, hours after the attack on Pearl Harbor, Guam was captured by the Japanese, who occupied the island for two and a half years. During the occupation, Guamanians were subjected to forced labor, incarceration, torture and execution. American forces recaptured the island on July 21, 1944; Liberation Day commemorates the victory.

An unofficial but frequently used territorial motto is "Where America's Day Begins", which refers to the island's proximity to the International Date Line.

The original inhabitants of Guam and the Northern Mariana Islands were the Chamorro people, who are believed to be descendants of Austronesian people originating from Southeast Asia as early as 2000 BC.

The ancient Chamorro society had four classes: "chamorri" (chiefs), "matua" (upper class), "achaot" (middle class), and "mana'chang" (lower class). The "matua" were located in the coastal villages, which meant they had the best access to fishing grounds, whereas the "mana'chang" were located in the interior of the island. "Matua" and "mana'chang" rarely communicated with each other, and "matua" often used "achaot" as intermediaries. There were also ""makåhna"" or ""kakahna"", shamans with magical powers and ""Suruhånu"" or ""Suruhåna"", healers who use different kinds of plants and natural materials to make medicine. Belief in spirits of ancient Chamorros called ""Taotao mo'na"" still persists as a remnant of pre-European culture. It is believed that ""Suruhånu"" or ""Suruhåna"" are the only ones who can safely harvest plants and other natural materials from their homes or ""hålomtåno"" without incurring the wrath of the ""Taotao mo'na"". Their society was organized along matrilineal clans.

"Latte" stones are stone pillars that are found only in the Mariana Islands; they are a recent development in Pre-Contact Chamorro society. The latte-stone was used as a foundation on which thatched huts were built. Latte stones consist of a base shaped from limestone called the "haligi" and with a capstone, or "tåsa", made either from a large brain coral or limestone, placed on top. A possible source for these stones, the Rota Latte Stone Quarry, was discovered in 1925 on Rota.

The first European to travel to Guam was Portuguese navigator Ferdinand Magellan, sailing for the King of Spain, when he sighted the island on March 6, 1521, during his fleet's circumnavigation of the globe. When Magellan arrived on Guam, he was greeted by hundreds of small outrigger canoes that appeared to be flying over the water, due to their considerable speed. These outrigger canoes were called Proas, and resulted in Magellan naming Guam "Islas de las Velas Latinas" ("Islands of the Lateen sails"). Antonio Pigafetta (one of Magellan's original 18) said that the name was "Island of Sails", but he also writes that the inhabitants "entered the ships and stole whatever they could lay their hands on", including "the small boat that was fastened to the poop of the flagship." "Those people are poor, but ingenious and very thievish, on account of which we called those three islands "Islas de los Ladrones" ("Islands of thieves")."

Despite Magellan's visit, Guam was not officially claimed by Spain until January 26, 1565, by General Miguel López de Legazpi. From 1565 to 1815, Guam and the Northern Mariana Islands, the only Spanish outposts in the Pacific Ocean east of the Philippines, were an important resting stop for the Manila galleons, a fleet that covered the Pacific trade route between Acapulco and Manila. To protect these Pacific fleets, Spain built several defensive structures that still stand today, such as Fort Nuestra Señora de la Soledad in Umatac. Guam is the biggest single segment of Micronesia, the largest islands between the island of Kyushu (Japan), New Guinea, the Philippines, and the Hawaiian Islands.

Spanish colonization commenced on June 15, 1668, with the arrival of Diego Luis de San Vitores and Pedro Calungsod, who established the first Catholic church. The islands were part of the Spanish East Indies, and in turn part of the Viceroyalty of New Spain, based in Mexico City. Other reminders of colonial times include the old Governor's Palace in Plaza de España and the Spanish Bridge, both in Hagatña. Guam's Cathedral Dulce Nombre de Maria was formally opened on February 2, 1669, as was the Royal College of San Juan de Letran. Guam, along with the rest of the Mariana and Caroline Islands, were treated as part of Spain's colony in the Philippines. While the island's Chamorro culture has indigenous roots, the cultures of both Guam and the Northern Marianas have many similarities with Spanish culture due to three centuries of Spanish rule.

Intermittent warfare lasting from July 23, 1670, until July 1695, plus the typhoons of 1671 and 1693, and in particular the smallpox epidemic of 1688, reduced the Chamorro population from 50,000 to 10,000, finally to less than 5,000. Precipitated by the death of Quipuha, and the murder of Father San Vitores and Pedro Calungsod by local rebel chief Matapang, tensions led to a number of conflicts. Captain Juan de Santiago started a campaign to conquer the island, which was continued by the successive commanders of the Spanish forces.

After his arrival in 1674, Captain Damian de Esplana ordered the arrest of rebels who attacked the population of certain towns. Hostilities eventually led to the destruction of villages such as Chochogo, Pepura, Tumon, Sidia-Aty, Sagua, Nagan and Ninca. Starting in June 1676, the first Spanish Governor of Guam, Capt. Francisco de Irrisarri y Vinar, controlled internal affairs more strictly than his predecessors in order to curb tensions. He also ordered the construction of schools, roads and other infrastructure.

In 1680, Captain Jose de Quiroga arrived and continued some of the development projects started by his predecessors. He also continued the search for the rebels who had assassinated Father San Vitores, resulting in campaigns against the rebels which were hiding out in some islands, eventually leading to the death of Matapang, Hurao and Aguarin. Quiroga brought some natives from the northern islands to Guam, ordering the population to live in a few large villages. These included Jinapsan, Umatac, Pago, Agat and Inarajan, where he built a number of churches. By July 1695, Quiroga had completed the conquest of Guam, Rota, Tinian and Aguigan.

On February 26, 1767, Charles III of Spain issued a decree confiscating the property of the Jesuits and banishing them from Spain and her possessions. As a consequence, the Jesuit fathers on Guam departed on November 2, 1769, on the schooner "Nuestra Señora de Guadalupe", abandoning their churches, rectories and ranches.

The arrival of Governor Don Mariano Tobias, on September 15, 1771, brought agricultural reforms, including making land available to the islanders for cultivation, encouraged the development of cattle raising, imported deer and water buffalo from Manila, donkeys and mules from Acapulco, established cotton mills and salt pans, free public schools, and the first Guam militia. Later, he was transferred to Manila in June 1774.

Following the Napoleonic Wars, many Spanish colonies in the Western Hemisphere had become independent, shifting the economic dependence of Guam from Mexico to the Philippines. Don Francisco Ramon de Villalobos, who became governor in 1831, improved economic conditions including the promotion of rice cultivation and the establishment of a leper hospital.

Otto von Kotzebue visited the island in November 1817, and Louis de Freycinet in March 1819. Jules Dumont d'Urville made two visits, the first in May 1828. The island became a rest stop for whalers starting in 1823.

A devastating typhoon struck the island on August 10, 1848, followed by a severe earthquake on January 25, 1849, which resulted in many refugees from the Caroline Islands, victims of the resultant tsunami. After a smallpox epidemic killed 3,644 Guamanians in 1856, Carolinians and Japanese were permitted to settle in the Marianas. Guam received nineteen Filipino prisoners after their failed 1872 Cavite mutiny.

After almost four centuries as part of the Kingdom of Spain, the United States occupied the island following Spain's defeat in the 1898 Spanish–American War, as part of the Treaty of Paris of 1898. Guam was transferred to the United States Navy control on December 23, 1898, by from 25th President William McKinley.

Guam came to serve as a station for American merchant and warships traveling to and from the Philippines (another American acquisition from Spain) while the Northern Mariana Islands were sold by Spain to Germany for part of its rapidly expanding German Empire. A U.S. Navy yard was established at Piti in 1899, and a United States Marine Corps barracks at Sumay in 1901.

Following the Philippine–American War of 1899–1902, rebel nationalist leader Apolinario Mabini was exiled to Guam in 1901 after his capture. Following the German defeat in World War I, the Northern Mariana Islands became part of the South Seas Mandate, a League of Nations Mandate in 1919 with the nearby Empire of Japan as the mandatory ("trustee") as a member nation of the victorious Allies in the "Great War".

A marine seaplane unit was stationed in Guam from 1921 to 1930, the first in the Pacific. Pan American World Airways established a seaplane base on the island for its trans-Pacific San Francisco-Manila-Hong Kong route, and the Commercial Pacific Cable Company had earlier built a telegraph/telephone station in 1903 for its trans-oceanic communication line.

During World War II, Guam was attacked and invaded by Japan on Monday, December 8, 1941, shortly after the attack on Pearl Harbor. In addition, Japan made major military moves into Southeast Asia and the East Indies islands of the South Pacific Ocean against the British and Dutch colonies, opening a new wider Pacific phase in the Second World War. The Japanese renamed Guam (Great Shrine Island).

The Northern Mariana Islands had become a League of Nations mandate assigned to Japan in 1919, pursuant to the Treaty of Versailles of 1919. Chamorros indigenous island people from the Northern Marianas were brought to Guam to serve as interpreters and in other capacities for the occupying Japanese force. The Guamanian Chamorros were treated as an occupied enemy by the Japanese military. After the war, this would cause resentment between the Guamanian Chamorros and the Chamorros of the Northern Marianas. Guam's Chamorros believed their northern brethren should have been compassionate towards them, whereas having been administered by Japan for over 30 years, the Northern Mariana Chamorros were loyal to the Japanese government.

The Japanese occupation of Guam lasted for approximately 31 months. During this period, the indigenous people of Guam were subjected to forced labor, family separation, incarceration, execution, concentration camps and forced prostitution. Approximately 1,000 people died during the occupation, according to later Congressional committee testimony in 2004. Some historians estimate that war violence killed 10% of Guam's then 20,000 population.

The United States returned and fought the Battle of Guam from July 21 to August 10, 1944, to recapture the island from Japanese military occupation. More than 18,000 Japanese were killed as only 485 surrendered. Sergeant Shoichi Yokoi, who surrendered in January 1972, appears to have been the last confirmed Japanese holdout, having held out for 28 years in the forested back country on Guam. The United States also captured and occupied the nearby Northern Marianas Islands.

North Field was established in 1944, and was renamed for Brigadier General James Roy Andersen of the old U.S. Army Air Forces as Andersen Air Force Base.

After World War II, the Guam Organic Act of 1950 established Guam as an unincorporated organized territory of the United States, provided for the structure of the island's civilian government, and granted the people U.S. citizenship. The Governor of Guam was federally appointed until 1968, when the Guam Elective Governor Act provided for the office's popular election. Since Guam is not a U.S. state, U.S. citizens residing on Guam are not allowed to vote for president and their congressional representative is a non-voting member. They do, however, get to vote for party delegates in presidential primaries.

Andersen Air Force Base played a major role in the Vietnam War. The host unit was later designated the 36th Wing (36 WG), assigned to the Pacific Air Forces (PACAF) Thirteenth Air Force (13AF). In September 2012, 13 AF was deactivated and its functions merged into PACAF. The multinational Cope North military exercise is an annual event.

Since 1974, about 124 historic sites in Guam have been recognized under the U.S. National Register of Historic Places. Guam temporarily hosted 100,000 Vietnamese refugees in 1975, and 6,600 Kurdish refugees in 1996.

On August 6, 1997, Guam was the site of the Korean Air Flight 801 aircraft accident. The Boeing 747–300 jetliner was preparing to land when it crashed into a hill, killing 228 of the 254 people on board.

In August 2017, North Korea warned that it might launch mid-range ballistic missiles into waters within of Guam, following an exchange of threats between the governments of North Korea and the United States.

In 2018, a Government Accountability Office report stated that Agent Orange was used as a commercial herbicide in Guam during the Vietnam and Korean Wars. An analysis of chemicals present in the island's soil, together with resolutions passed by Guam's legislature, suggest that Agent Orange was among the herbicides routinely used on and around military bases Anderson Air Force Base, Naval Air Station Agana, Guam. Despite the evidence, the Department of Defense continues to deny that Agent Orange was ever stored or used on Guam. Several Guam veterans have collected an enormous amount of evidence to assist in their disability claims for direct exposure to dioxin containing herbicides such as 2,4,5-T which are similar to the illness associations and disability coverage that has become standard for those who were harmed by the same chemical contaminant of Agent Orange used in Vietnam.

Guam lies between 13.2° and 13.7°N and 144.6° and 145.0°E. It is long and wide, giving it an area of (three-fourths the size of Singapore) and making it the 32nd largest island of the United States. It is the southernmost and largest island in the Marianas as well as the largest in Micronesia. Guam's highest point is Mount Lamlam at . Challenger Deep, at the deepest surveyed point in the Oceans, lies southwest of Guam.

The Mariana chain of which Guam is a part was created by collision of the Pacific and Philippine Sea tectonic plates. Guam is the closest land mass to the Mariana Trench, the deep subduction zone that runs east of the Marianas. Due to its location on the Mariana Plate just westward of where the Pacific Plate subducts the Mariana and the Philippine Sea Plates, Guam occasionally experiences earthquakes. In recent years, most with epicenters near Guam have had magnitudes ranging from 5.0 to 8.7. Unlike Anatahan in the Northern Mariana Islands, Guam is not volcanically active, though vog (volcanic smog) from Anatahan affects it due to proximity.

A coral table reef surrounds most of Guam, and the limestone plateau provides the source for most of the island's fresh water. Steep coastal cliffs dominate the north, while mountains inform the topography of the island's southern end; lower hills typify the area in between.

Guam has a tropical rainforest climate (Köppen "Af"), though its driest month of March almost averages dry enough to qualify as a tropical monsoon climate (Köppen "Am"), moderated by seasonal trade winds from the northeast. The weather is generally hot and humid throughout the year with little seasonal temperature variation. Hence, Guam is known to have equable temperatures year-round. Guam has two distinct seasons: Wet and dry season. The dry season runs from January through May and the wet season runs from July through November with an average annual rainfall between 1981 and 2010 of around . The wettest month on record at Guam Airport has been August 1997 with and the driest February 2015 with . The wettest calendar year has been 1976 with and the driest 1998 with . The most rainfall in a single day occurred on October 15, 1953, when fell.

The mean high temperature is and mean low is . Temperatures rarely exceed or fall below . The relative humidity commonly exceeds 84 percent at night throughout the year, but the average monthly humidity hovers near 66 percent. The highest temperature ever recorded in Guam was on April 18, 1971, and April 1, 1990, and the lowest temperature ever recorded was on February 8, 1973.

Guam lies in the path of typhoons and it is common for the island to be threatened by tropical storms and possible typhoons during the wet season. The highest risk of typhoons is from August through November, where typhoons and tropical storms are most probable in the northwest Pacific. They can, however, occur year round. The most powerful typhoon to pass over Guam recently was Super Typhoon Pongsona, with sustained winds of , gusts to , which struck Guam on December 8, 2002, leaving massive destruction.

Since Typhoon Pamela in 1976, wooden structures have been largely replaced by concrete structures. During the 1980s, wooden utility poles began to be replaced by typhoon-resistant concrete and steel poles. After the local Government enforced stricter construction codes, many home and business owners built their structures out of reinforced concrete with installed typhoon shutters.

According to the 2010 United States Census, the largest ethnic group are the native Chamorros, accounting for 37.3% of the total population. Other significant ethnic groups include those of Filipino (26.3%) and Chuukese (7%) ethnicities. The rest are from other Pacific Islands or of Asian ancestry. The estimated interracial marriage rate is over 40%.

The official languages of the island are English and Chamorro. Filipino is also a common language across the island. Other Pacific island languages and many Asian languages are spoken in Guam as well. Spanish, the language of administration for 300 years, is no longer commonly spoken on the island, although vestiges of the language remain in proper names, loanwords, and place names and it is studied at university and high schools.

The most common religion is the Catholic Church. According to the Pew Research Center, the religious denominations constitute of the following, in 2010:

Post-European-contact Chamorro Guamanian culture is a combination of American, Spanish, Filipino, other Micronesian Islander and Mexican traditions. Few indigenous pre-Hispanic customs remained following Spanish contact. Hispanic influences are manifested in the local language, music, dance, sea navigation, cuisine, fishing, games (such as batu, chonka, estuleks, and bayogu), songs, and fashion.

During Spanish rule (1668–1898) the majority of the population was converted to Roman Catholicism and religious festivities such as Easter and Christmas became widespread. Post-contact Chamorro cuisine is largely based on corn, and includes tortillas, tamales, atole, and chilaquiles, which are a clear influence from Mesoamerica, principally Mexico, from Spanish trade with Asia.

The modern Chamorro language has many historical parallels to modern Philippine languages in that it is an Austronesian language which has absorbed much Spanish vocabulary. The language lies within the Malayo-Polynesian languages subgroup, along with such languages as Tagalog, Indonesian, Hawaiian, and Maori. Unlike most other languages of the Pacific Islands, Chamorro does belong to the Oceanic subgroup of the Austronesian languages.

As with Filipinos, many Chamorros have Spanish surnames, although also like most Filipinos few of the inhabitants are themselves descended from the Spaniards. Instead, Spanish names and surnames became commonplace after their conversion to Roman Catholic Christianity and the historical event of the imposition of the Catálogo alfabético de apellidos in Guam and other territories of the Spanish East Indies, most notably the Philippines.

Due to foreign cultural influence from Spain, most aspects of the early indigenous culture have been lost, though there has been a resurgence in preserving any remaining pre-Hispanic culture in the last few decades. Some scholars have traveled throughout the Pacific Islands conducting research to study what the original Chamorro cultural practices such as dance, language, and canoe building may have been like.

Two aspects of indigenous pre-Hispanic culture that withstood time are chenchule' and inafa'maolek. Chenchule' is the intricate system of reciprocity at the heart of Chamorro society. It is rooted in the core value of inafa'maolek. Historian Lawrence Cunningham in 1992 wrote, "In a Chamorro sense, the land and its produce belong to everyone. , or interdependence, is the key, or central value, in Chamorro culture ... depends on a spirit of cooperation and sharing. This is the armature, or core, that everything in Chamorro culture revolves around. It is a powerful concern for mutuality rather than individualism and private property rights."

The core culture or Pengngan Chamorro is based on complex social protocol centered upon respect: from sniffing over the hands of the elders (called mangnginge in Chamorro), the passing down of legends, chants, and courtship rituals, to a person asking for permission from spiritual ancestors before entering a jungle or ancient battle grounds. Other practices predating Spanish conquest include galaide' canoe-making, making of the belembaotuyan (a string musical instrument made from a gourd), fashioning of slings and slingstones, tool manufacture, burial rituals, and preparation of herbal medicines by Suruhanu.

Master craftsmen and women specialize in weavings, including plaited work (niyok- and åkgak-leaf baskets, mats, bags, hats, and food containments), loom-woven material (kalachucha-hibiscus and banana fiber skirts, belts and burial shrouds), and body ornamentation (bead and shell necklaces, bracelets, earrings, belts, and combs made from tortoise shells and Spondylus).

While only a few masters exist to continue traditional art forms, the resurgence of interest among the Chamorros to preserve the language and culture has resulted in a growing number of young Chamorros who seek to continue the ancient ways of the Chamorro people.

The Guam national football team was founded in 1975 and joined FIFA in 1996. Guam was once considered one of FIFA's weakest teams, and experienced their first victory over a FIFA-registered side in 2009, when they defeated Mongolia in the East Asian Cup.

The national team plays at the Guam National Football Stadium and are known as the "matao" team. Matao is the definition of highest level or "noble" class; the matao team have done exceptionally well under the head coach Gary White. , the Matao is led by Darren Sawatzky, the current head coach. The top football division in Guam is the Guam Men's Soccer League. Rovers FC and Guam Shipyard are the league's most competitive and successful clubs, both have won nine championships in the past years.

Guam entered the 2018 FIFA World Cup qualification Group D. Guam hosted qualifying games on the island for the first time in 2015. During the qualifying round, Guam clinched their first FIFA World Cup Qualifying win by defeating the Turkmenistan national football team. Since then, the team has experienced moderate success in the qualifying round with a record of 2–1–1.

The Guam national basketball team is traditionally one of the top teams in the Oceania region behind the Australia men's national basketball team and the New Zealand national basketball team. , it is the reigning champion of the Pacific Games Basketball Tournament. Guam is home to various basketball organizations, including the Guam Basketball Association.

Guam is represented in rugby union by the Guam national rugby union team. The team has never qualified for a Rugby World Cup. Guam played their first match in 2005, an 8–8 draw with the India national rugby union team. Guam's biggest win was a 74–0 defeat of the Brunei national rugby union team in June 2008.

Guam hosted the Pacific Games in 1975 and 1999. At the 2007 Games, Guam finished 7th of 22 countries and 14th at the 2011 Games.

Guam's economy depends primarily on tourism, Department of Defense installations and locally owned businesses. Under the provisions of a special law by Congress, it is Guam's treasury rather than the U.S. treasury that receives the federal income taxes paid by local taxpayers (including military and civilian federal employees assigned to Guam).

Lying in the western Pacific, Guam is a popular destination for Japanese tourists. Its tourist hub, Tumon, features over 20 large hotels, a Duty Free Shoppers Galleria, Pleasure Island district, indoor aquarium, Sandcastle Las Vegas–styled shows and other shopping and entertainment venues. It is a relatively short flight from Asia or Australia compared to Hawaii, with hotels and seven public golf courses accommodating over a million tourists per year. Although 75% of the tourists are Japanese, Guam receives a sizable number of tourists from South Korea, the U.S., the Philippines, and Taiwan. Significant sources of revenue include duty-free designer shopping outlets, and the American-style malls: Micronesia Mall, Guam Premier Outlets, the Agana Shopping Center, and the world's largest Kmart.
The economy had been stable since 2000 due to increased tourism. It is expected to stabilize with the transfer of U.S. Marine Corps' 3rd Marine Expeditionary Force, currently in Okinawa, Japan, (approximately 8,000 Marines, along with their 10,000 dependents), to Guam between 2010 and 2015. In 2003, Guam had a 14% unemployment rate, and the government suffered a $314 million shortfall.

The Compacts of Free Association between the United States, the Federated States of Micronesia, the Republic of the Marshall Islands, and the Republic of Palau accorded the former entities of the Trust Territory of the Pacific Islands a political status of "free association" with the United States. The Compacts give citizens of these island nations generally no restrictions to reside in the United States (also its territories), and many were attracted to Guam due to its proximity, environmental, and cultural familiarity. Over the years, it was claimed by some in Guam that the territory has had to bear the brunt of this agreement in the form of public assistance programs and public education for those from the regions involved, and the federal government should compensate the states and territories affected by this type of migration. Over the years, Congress had appropriated "Compact Impact" aids to Guam, the Northern Mariana Islands, and Hawaii, and eventually this appropriation was written into each renewed Compact. Some, however, continue to claim the compensation is not enough or that the distribution of actual compensation received is significantly disproportionate.

In 2012 Michael Calabrese, Daniel Calarco, and Colin Richardson of "Slate" stated that the island has "tremendous bandwidth" and internet prices comparable to those of the U.S. Mainland due to being at the junction of undersea cables.

Guam is governed by a popularly elected governor and a unicameral 15-member legislature, whose members are known as senators. Its judiciary is overseen by the Supreme Court of Guam.

The District Court of Guam is the court of United States federal jurisdiction in the territory. Guam elects one delegate to the United States House of Representatives, currently Democrat Michael San Nicolas. The delegate does not have a vote on the final passage of legislation, but is accorded a vote in committee, and the privilege to speak to the House. U.S. citizens in Guam vote in a presidential straw poll for their choice in the U.S. Presidential general election, but since Guam has no votes in the Electoral College, the poll has no real effect. However, in sending delegates to the Republican and Democratic national conventions, Guam does have influence in the national presidential race. These delegates are elected by local party conventions.

In the 1980s and early 1990s, there was a significant movement in favor of this U.S. territory becoming a commonwealth, which would give it a level of self-government similar to Puerto Rico and the Northern Mariana Islands. In a 1982 plebiscite, voters indicated interest in seeking commonwealth status. However, the federal government rejected the version of a commonwealth that the government of Guam proposed, because its clauses were incompatible with the Territorial Clause (Art. IV, Sec. 3, cl. 2) of the U.S. Constitution. Other movements advocate U.S. statehood for Guam, union with the state of Hawaii, or union with the Northern Mariana Islands as a single territory, or independence.

A Commission on Decolonization was established in 1997 to educate the people of Guam about the various political status options in its relationship with the U.S.: statehood, free association and independence. The island has been considering another non-binding plebiscite on decolonization since 1998, however, the group was dormant for some years. In 2013, the Commission began seeking funding to start a public education campaign. There were few subsequent developments until late 2016. In early December 2016, the Commission scheduled a series of education sessions in various villages about the current status of Guam's relationship with the U.S. and the self-determination options that might be considered. The Commission's current Executive Director is Edward Alvarez and there are ten members. The group is also expected to release position papers on independence and statehood but the contents have not yet been completed.

The United Nations is in favor of greater self-determination for Guam and other such territories. The UN's Special Committee on Decolonization has agreed to endorse the Governor's education plan. The commission's May 2016 report states: "With academics from the University of Guam, [the Commission] was working to create and approve educational materials. The Office of the Governor was collaborating closely with the Commission" in developing educational materials for the public.

The United States Department of the Interior had approved a $300,000 grant for decolonization education, Edward Alvarez told the United Nations Pacific Regional Seminar in May 2016. "We are hopeful that this might indicate a shift in [United States] policy to its Non-Self-Governing Territories such as Guam, where they will be more willing to engage in discussions about our future and offer true support to help push us towards true self-governances and self-determination."

On 31 July 2020, the Government of Guam joined the Unrepresented Nations and Peoples Organization (UNPO).

Guam is divided into 19 municipal villages:

The U.S. military maintains jurisdiction over its bases, which cover approximately , or 29% of the island's total land area:

The U.S. military has proposed building a new aircraft carrier berth on Guam and moving 8,600 Marines, and 9,000 of their dependents, to Guam from Okinawa, Japan. Including the required construction workers, this buildup would increase Guam's population by a total of 79,000, a 49% increase over its 2010 population of 160,000. In a February 2010 letter, the United States Environmental Protection Agency sharply criticized these plans because of a water shortfall, sewage problems and the impact on coral reefs. By 2012, these plans had been cut to have only a maximum of 4,800 Marines stationed on the island, two thirds of whom would be there on a rotational basis without their dependents.

With the proposed increased military presence stemming from the upcoming preparation and relocation efforts of U.S. Marines from Okinawa, Japan to Guam slated to begin in 2010 and last for the next several years thereafter, the amount of total land that the military will control or tenant may grow to or surpass 40% of the entire landmass of Guam.

In January 2011, the Ike Skelton National Defense Authorization Act for FY2011 indicated that recent significant events will delay the deadline for realigning U.S. Marine Corps service members and their families from Okinawa to Guam. The transfer may be as late as 2020. In addition, the Defense Authorization Act cut approximately $320 million from the 2011 budget request.

Villagers and the military community are interconnected in many ways. Many villagers serve in the military or are retired. Many active duty personnel and Defense Department civilians also live in the villages outside of the military installation areas. The military and village communities have "adoption" programs where Guam's population and military personnel stationed on Guam perform community service projects.

Most of the island has state-of-the-art mobile phone services and high-speed internet widely available through either cable or DSL. Guam was added to the North American Numbering Plan (NANP) in 1997 (country code 671 became NANP area code 671), removing the barrier of high-cost international long-distance calls to the U.S. mainland.

Guam is also a major hub for submarine cables between the Western U.S., Hawaii, Australia and Asia. Guam currently serves twelve submarine cables, with most continuing to China.

In 1899, the local postage stamps were overprinted "Guam" as was done for the other former Spanish colonies, but this was discontinued shortly thereafter and regular U.S. postage stamps have been used ever since. Because Guam is also part of the U.S. Postal System (postal abbreviation: GU, ZIP code range: 96910–96932), mail to Guam from the U.S. mainland is considered domestic and no additional charges are required. Private shipping companies, such as FedEx, UPS, and DHL, however, have no obligation to do so, and do not regard Guam as domestic.

The speed of mail traveling between Guam and the states varies depending on size and time of year. Light, first-class items generally take less than a week to or from the mainland, but larger first-class or Priority items can take a week or two. Fourth-class mail, such as magazines, are transported by sea after reaching Hawaii. Most residents use post office boxes or private mail boxes, although residential delivery is becoming increasingly available. Incoming mail not from the Americas should be addressed to "Guam" instead of "USA" to avoid being routed the long way through the U.S. mainland and possibly charged a higher rate (especially from Asia).

The Commercial Port of Guam is the island's lifeline because most products must be shipped into Guam for consumers. It receives the weekly calls of the Hawaii-based shipping line Matson, Inc. whose container ships connect Guam with Honolulu, Hawaii, Los Angeles, California, Oakland, California and Seattle, Washington. The port is also the regional transhipment hub for over 500,000 customers throughout the Micronesian region. The port is the shipping and receiving point for containers designated for the island's U.S. Department of Defense installations, Andersen Air Force Base and Commander, Naval Forces Marianas and eventually the Third Marine Expeditionary Force.

Guam is served by the Antonio B. Won Pat International Airport. The island is outside the United States customs zone, so Guam is responsible for establishing and operating its own customs and quarantine agency and jurisdiction. Therefore, the U.S. Customs and Border Protection only carries out immigration (but not customs) functions. Since Guam is under federal immigration jurisdiction, passengers arriving directly from the United States skip immigration and proceed directly to Guam Customs and Quarantine.

However, due to the Guam and CNMI visa waiver program for certain countries, an eligibility pre-clearance check is carried on Guam for flights to the States. For travel from the Northern Mariana Islands to Guam, a pre-flight passport and visa check is performed before boarding the flight to Guam. On flights from Guam to the Northern Mariana Islands, no immigration check is performed. Traveling between Guam and the States through a foreign point, however, does require a passport.

Most residents travel within Guam using personally owned vehicles. The local government currently outsources the only public bus system (Guam Regional Transit Authority), and some commercial companies operate buses between tourist-frequented locations.

Believed to be a stowaway on a U.S. military transport near the end of World War II, the brown tree snake ("Boiga irregularis") was accidentally introduced to Guam, which previously had no native species of snake. It nearly eliminated the native bird population. The problem was exacerbated because the snake has no natural predators on the island. The brown tree snake, known locally as the "kulebla", is native to northern and eastern coasts of Australia, Papua New Guinea, and the Solomon Islands. It is slightly venomous, but relatively harmless to human beings; it is nocturnal. Although some studies have suggested a high density of these serpents on Guam, residents rarely see them. The United States Department of Agriculture has trained detector dogs to keep the snakes out of the island's cargo flow. The United States Geological Survey also has dogs that can detect snakes in forested environments around the region's islands.

Before the introduction of the brown tree snake, Guam was home to several endemic bird species. Among them were the Guam rail (or "ko'ko"' bird in Chamorro) and the Guam flycatcher, both were once common throughout the island. Today the flycatcher is entirely extinct and the Guam rail is critically endangered and they are bred in captivity by the Division of Aquatic and Wildlife Resources. The devastation caused by the snake has been significant over the past several decades. As many as twelve bird species are believed to have been driven to extinction. However, some of the birds still thrive and are common on other islands at the subspecies level in the Marianas, including Saipan. According to many elders, ko'ko' birds were common in Guam before World War II.

Other bird species threatened by the brown tree snake include the Mariana crow, the Mariana swiftlet, and the Micronesian starling, though populations are present on other islands, including Rota.

Guam is said to have many more insects and 40 times more spiders than neighboring islands, because their natural predators birds are severely diminished, and the forests are almost completely silent due to lack of birdsong.

An infestation of the coconut rhinoceros beetle (CRB), "Oryctes rhinoceros", was detected on Guam on September 12, 2007. CRB is not known to occur in the United States except in American Samoa. Delimiting surveys performed September 13–25, 2007, indicated that the infestation was limited to Tumon Bay and Faifai Beach, an area of approximately . Guam Department of Agriculture (GDA) placed quarantine on all properties within the Tumon area on October 5 and later expanded the quarantine to about on October 25; approximately radius in all directions from all known locations of CRB infestation. CRB is native to Southern Asia and distributed throughout Asia and the Western Pacific including Sri Lanka, Upolu, Samoa, American Samoa, Palau, New Britain, West Irian, New Ireland, Pak Island and Manus Island (New Guinea), Fiji, Cocos (Keeling) Islands, Mauritius, and Reunion.

From the seventeenth through nineteenth centuries, the Spanish introduced the Philippine deer ("Rusa mariannus"), black francolins, and carabao (a subspecies of water buffalo), which have cultural significance. Herds of carabao obstruct military base operations and harm native ecosystems. After birth control and adoption efforts were ineffective, the U.S. military began culling the herds in 2002 leading to organized protests from island residents.

Other introduced species include cane toads introduced in 1937, the giant African snail (an agricultural pest introduced during World War II by Japanese occupation troops) and more recently frog species which could threaten crops in addition to providing additional food for the brown tree snake population. Reports of loud chirping frogs native to Puerto Rico and known as coquí, that may have arrived from Hawaii, have led to fears that the noise could threaten Guam's tourism.

Guam has no native amphibian species, but now a total of eight amphibian species has been established in Guam. "Litoria fallax" (native to the eastern coast of Australia) has been present in Guam since 1968, and "Rhinella marina" (the cane toad) was brought to the island in 1937. The other 6 amphibian species, namely "Hylarana guentheri" (native to mainland Asia), "Microhyla pulchra" (native to mainland Asia), "Polypedates braueri" (endemic to Taiwan), "Eleutherodactylus planirostris" (native to the Caribbean), "Fejervarya cancrivora" (the Guam variety being most closely related to "F. cancrivora" found in Taiwan), and "Fejervarya limnocharis" (native to Southeast Asia), have been in Guam since 2003. Many species were likely inadvertently introduced via shipping cargo, especially from Taiwan, mainland China, and Southeast Asia.

Introduced feral pigs and deer, over-hunting, and habitat loss from human development are also major factors in the decline and loss of Guam's native plants and animals.

Invading animal species are not the only threat to Guam's native flora. Tinangaja, a virus affecting coconut palms, was first observed on the island in 1917 when copra production was still a major part of Guam's economy. Though coconut plantations no longer exist on the island, the dead and infected trees that have resulted from the epidemic are seen throughout the forests of Guam.

During the past century, the dense forests of northern Guam have been largely replaced by thick "tangan-tangan" brush ("Leucaena leucocephala"). Much of Guam's foliage was lost during World War II. In 1947, the U.S. military is thought to have planted tangan-tangan by seeding the island from the air to prevent erosion. Tangan-tangan was present on the island before 1905.

In southern Guam, non-native grass species dominate much of the landscape. Although the colorful and impressive flame tree ("Delonix regia") is found throughout the Marianas, the tree on Guam has been largely decimated.

The Coconut rhinoceros beetle (CRB) infestation has become an epidemic event of palm tree damage on Guam. The CRB infects palm trees by burrowing into the tips of the palms, effectively killing the plant by destroying the shoot apical meristem during the process. While the grubs and larvae of CRB do no actual harm to palms, they populate and grow within the damaged crowns of the palm trees, which is a specific mating habit of Guam CRBs.

A possible solution to the crisis, which has been ecologically analyzed in Western Samoa, would be the introduction of CRB predators. Centipedes, "Scolopendra Morsitans," have been found to seek out and eat the larvae of the beetle in logs and other breeding grounds. Manual insertion of the centipedes, or introduction of volatile attractors within the plant, may cause a significant dent in Guam's CRB mortality. The introduction of terpene, fatty acids, or other nitrogenous compound metabolites may allow palm trees to attract CRB-predators such as the centipede through an indirect defense response, which is triggered by mechanoreceptors activating secondary metabolite transcribers via an action potential pathway.

A secondary solution could be introducing metal hyperaccumulation into the palm population on Guam's beaches. Plants that accumulate extra inorganic minerals, such as iron, nickel, or zinc, indirectly deter herbivory by depositing excess materials in certain sections of the plants. While the excess inorganic materials may not directly affect adult CRBs (as minerals would not be deposited in the meristem sections of the plant), it may create an inhospitable and toxic environment for the immature CRBs that are growing in the trees.

Wildfires plague the forested areas of Guam every dry season despite the island's humid climate. Most fires are caused by humans with 80% resulting from arson. Poachers often start fires to attract deer to the new growth. Invasive grass species that rely on fire as part of their natural life cycle grow in many regularly burned areas. Grasslands and "barrens" have replaced previously forested areas leading to greater soil erosion. During the rainy season, sediment is carried by the heavy rains into the Fena Lake Reservoir and Ugum River, leading to water quality problems for southern Guam. Eroded silt also destroys the marine life in reefs around the island. Soil stabilization efforts by volunteers and forestry workers (planting trees) have had little success in preserving natural habitats.

Efforts have been made to protect Guam's coral reef habitats from pollution, eroded silt and overfishing, problems that have led to decreased fish populations. This has both ecological and economic value, as Guam is a significant vacation spot for scuba divers. In recent years, the Department of Agriculture, Division of Aquatic and Wildlife Resources has established several new marine preserves where fish populations are monitored by biologists. These are located at Pati Point, Piti Bomb Holes, Sasa Bay, Achang Reef Flat, and Tumon Bay. Before adopting U.S. Environmental Protection Agency standards, portions of Tumon Bay were dredged by the hotel chains to provide a better experience for hotel guests. Tumon Bay has since been made into a preserve. A federal Guam National Wildlife Refuge in northern Guam protects the decimated sea turtle population in addition to a small colony of Mariana fruit bats.

Harvest of sea turtle eggs was a common occurrence on Guam before World War II. The green sea turtle ("Chelonia mydas") was harvested legally on Guam before August 1978, when it was listed as threatened under the Endangered Species Act. The hawksbill sea turtle ("Eretmochelys imbricata") has been on the endangered list since 1970. In an effort to ensure protection of sea turtles on Guam, routine sightings are counted during aerial surveys and nest sites are recorded and monitored for hatchlings.

The University of Guam (UOG) and Guam Community College, both fully accredited by the Western Association of Schools and Colleges, offer courses in higher education. UOG is a member of the exclusive group of only 106 land-grant institutions in the entire United States. Pacific Islands University is a small Christian liberal arts institution nationally accredited by the Transnational Association of Christian Colleges and Schools. They offer courses at both the undergraduate and graduate levels.

The Guam Department of Education serves the entire island of Guam. In 2000, 32,000 students attended Guam's public schools, including 26 elementary schools, eight middle schools, and six high schools and alternative schools. Guam Public Schools have struggled with problems such as high dropout rates and poor test scores. Guam's educational system has always faced unique challenges as a small community located from the U.S. mainland with a very diverse student body including many students who come from backgrounds without traditional American education. An economic downturn in Guam since the mid-1990s has compounded the problems in schools.

Before September 1997, the U.S. Department of Defense partnered with Guam Board of Education. In September 1997, the DoDEA opened its own schools for children of military personnel. DoDEA schools, which also serve children of some federal civilian employees, had an attendance of 2,500 in 2000. DoDEA Guam operates three elementary/middle schools and one high school.

Guam Public Library System operates the Nieves M. Flores Memorial Library in Hagåtña and five branch libraries.

The Government of Guam maintains the island's main health care facility, Guam Memorial Hospital, in Tamuning. U.S. board certified doctors and dentists practice in all specialties. In addition, the U.S. Naval Hospital in Agana Heights serves active-duty members and dependents of the military community. There is one subscriber-based air ambulance located on the island, CareJet, which provides emergency patient transportation across Guam and surrounding islands. A private hospital, the Guam Regional Medical City, opened its doors in early 2016.

Over the years, a number of films have been shot on Guam, including "Shiro's Head" (directed by the Muna brothers) and the government-funded "" (2004). Although set on Guam, "No Man Is an Island" (1962) was not shot there, but instead shot in the Philippines. Likewise, in the 2015 film "Pixels", the scene of the first alien attack takes place at Andersen AFB.


Notes
Further reading



</doc>
<doc id="11979" url="https://en.wikipedia.org/wiki?curid=11979" title="Game Boy family">
Game Boy family

The Game Boy family is a line of handheld game consoles developed, manufactured, released and marketed by Nintendo, consisting of the Game Boy and its revisions, the Game Boy Color and the Game Boy Advance family. The product line has sold over 200 million units worldwide.

The original Game Boy (ゲームボーイ, "Gēmu Bōi") and Game Boy Color combined sold 118.69 million units worldwide. All versions of the Game Boy Advance family combined have sold 81.51 million units. All Game Boy systems combined have sold 200.20 million units worldwide.

The Game Boy line was succeeded by the Nintendo DS line. A number of Game Boy, Game Boy Color, and Game Boy Advance games have been rereleased digitally through the Virtual Console service for the Nintendo 3DS and Wii U.

Nintendo's Game Boy handheld was first released in 1989. The gaming device was the brainchild of long-time Nintendo employee Gunpei Yokoi, who was the person behind the "Ultra Hand", an expanding arm toy created and produced by Nintendo in 1970, long before Nintendo would enter the video game market. Yokoi was also responsible for the Game & Watch series of handhelds when Nintendo made the move from toys to video games.

When Yokoi designed the original Game Boy, he knew that to be successful, the system needed to be small, light, inexpensive, and durable, as well as have a varied, recognizable library of games upon its release. By following this simple mantra, the Game Boy line managed to gain a vast following despite technically superior alternatives which would have color graphics instead. This is also apparent in the name (conceived by Shigesato Itoi), which connotes a smaller "sidekick" companion to Nintendo's consoles.

Game Boy continues its success to this day and many at Nintendo have dedicated the handheld in Yokoi's memory. Game Boy celebrated its 15th anniversary in 2004, which nearly coincided with the 20-year anniversary of the original Nintendo Entertainment System (NES). To celebrate, Nintendo released the Classic NES Series and an NES controller-themed color scheme for the Game Boy Advance SP.

In 2006, Nintendo president Satoru Iwata said on the rumored demise of the Game Boy brand: "No, it's not true after all. What we are repeatedly saying is that for whichever platform, we are always conducting research and development for the new system, be it the Game Boy, or new console or whatever. And what we just told the reporter was that in thinking about the current situation where we are enjoying great sales with the DS and that we are now trying to launch the Wii, it's unthinkable for us to launch any new platform for the handheld system, including the new version of the GBA... Perhaps they misunderstood a part of this story, but as far as the handheld market is concerned [right now] we really want to focus on more sales of the DS; that's all" until Nintendo ceased the production of the Game Boy Advance games and handheld system in North America on May 15, 2010.

The original gray Game Boy was first released in Japan on April 21, 1989. Based on a Z80 processor, it has a black and green reflective LCD screen, an eight-way directional pad, two action buttons (A and B), and Start and Select buttons with the controls being identical to the NES controller. It plays games from ROM-based media contained in cartridges (sometimes called carts or Game Paks). Its graphics are 8-bit (similar to the NES).

The game that pushed the Game Boy into the upper reaches of success was "Tetris". Tetris was widely popular, and on the handheld format could be played anywhere. It came packaged with the Game Boy, and broadened its reach; adults and children alike were buying Game Boys in order to play "Tetris". Releasing "Tetris" on the Game Boy was selected as #4 on GameSpy's "25 Smartest Moments in Gaming".

The original Game Boy was one of the first cartridge-based systems that supported more than four players at one time (via the link port). In fact, it has been shown that the system could support 16 simultaneous players. However, this feature was only supported in "Faceball 2000".

In 1995, the "Play it Loud" version of the original Game Boy was released in six different colors; black, red, yellow, green, blue, white and clear. With sports variations in between.

The Game Boy Pocket is a redesigned version of the original Game Boy having the same features. It was released in 1996. Notably, this variation is smaller and lighter. It comes in seven different colors; red, yellow, green, black, clear, silver, blue, and pink.

Another notable improvement over the original Game Boy is a black-and-white display screen, rather than the green-tinted display of the original Game Boy, that also featured improved response time for less blurring during motion. The Game Boy Pocket takes two AAA batteries as opposed to four AA batteries for roughly ten hours of gameplay. The first model of the Game Boy Pocket did not have an LED to show battery levels, but the feature was added due to public demand.

In April 1998, a variant of the Game Boy Pocket named Game Boy Light was exclusively released in Japan. The differences between the original Game Boy Pocket and the Game Boy Light is that the Game Boy Light takes on two AA batteries for approximately 20 hours of gameplay (when playing without using the light), rather than two AAA batteries, and it has an electroluminescent screen that can be turned on or off. This electroluminescent screen gave games a blue-green tint and allowed the use of the unit in darkened areas. Playing with the light on would allow about 12 hours of play. The Game Boy Light also comes in six different colors; silver, gold, yellow for the "Pokémon" edition, translucent yellow, clear and translucent red for the "Astro Boy" edition. The Game Boy Light was superseded by the Game Boy Color six months later and was the only Game Boy to have a backlit screen until the release of the Game Boy Advance SP AGS-101 model in 2005.

First released in Japan on October 21, 1998, the Game Boy Color (abbreviated as GBC) added a (slightly smaller) color screen to a form factor similar in size to the Game Boy Pocket. It also has double the processor speed, three times as much memory, and an infrared communications port. Technologically, it was likened to the 8-bit NES video game console from the 1980s although the Game Boy Color has a much larger color palette (56 simultaneous colors out of 32,768 possible) which had some classic NES ports and newer titles. It comes in seven different colors; Clear purple, purple, red, blue, green, yellow and silver for the "Pokémon" edition. Like the Game Boy Light, the Game Boy Color takes on two AA batteries. It was the final handheld to have 8-bit graphics.

A major component of the Game Boy Color is its near-universal backward compatibility; that is, a Game Boy Color is able to read older Game Boy cartridges and even play them in a selectable color palette (similar to the Super Game Boy). The only black and white Game Boy games known to be incompatible are "Road Rash" and "Joshua & the Battle of Jericho". Backwards compatibility became a major feature of the Game Boy line, since it allowed each new launch to begin with a significantly larger library than any of its competitors. Some games written specifically for the Game Boy Color can be played on older model Game Boys, whereas others cannot (see the Game Paks section for more information).

In Japan, on March 21, 2001, Nintendo released a significant upgrade to the Game Boy line. The Game Boy Advance (also referred to as GBA) featured a 32 bit 16.8 MHz ARM. It included a Z80 processor and a switch activated by inserting a Game Boy or Game Boy Color game into the slot for backward compatibility, and had a larger, higher resolution screen. Controls were slightly modified with the addition of "L" and "R" shoulder buttons. Like the Game Boy Light and Game Boy Color, the Game Boy Advance takes on two AA batteries. The system was technically likened to the SNES and showed its power with successful ports of SNES titles such as "Super Mario World", "", "" and "Donkey Kong Country". There were also new titles that could be found only on the GBA, such as "", "", "Wario Land 4", "" and more. A widely criticized drawback of the Game Boy Advance is that the screen is not backlit, making viewing difficult in some conditions. The Game Paks for the GBA are roughly half the length of original Game Boy cartridges and Game Boy Color cartridges, and so older Game Paks would stick out of the top of the unit. When playing older games, the GBA provides the option to play the game at the standard equal square resolution of the original screen or the option to stretch it over the wider GBA screen. The selectable color palettes for the original Game Boy games are identical to what it was on the Game Boy Color. The only Game Boy Color games known to be incompatible are "Pocket Music" and "Chee-Chai Alien". It was the final handheld to require regular batteries.
First released in Japan on February 14, 2003, the Game Boy Advance SP—Nintendo model AGS-001—resolved several problems with the original Game Boy Advance model. It featured a new smaller clamshell design with a flip-up screen, a switchable internal frontlight, a rechargeable battery for the first time, and the only notable issue is the omission of the headphone jack, which requires a special adapter, purchased separately. In September 2005, Nintendo released the Game Boy Advance SP model AGS-101, that featured a high quality backlit screen instead of a frontlit, similar to the Game Boy Micro screen but larger. It was the final Game Boy and last handheld to have backwards compatibility with Game Boy and Game Boy Color games.
The third form of Game Boy Advance system, the Game Boy Micro is four and a half inches wide (10 cm), two inches tall (5 cm), and weighs 2.8 ounces (80 g). By far the smallest Game Boy created, it has approximately the same dimensions as an original NES controller pad. Its screen is approximately 2/3 the size of the SP and GBA screens while maintaining the same resolution (240×160 pixels) but now has a higher quality backlit display with adjustable brightness. Included with the system are two additional faceplates which can be swapped to give the system a new look; Nintendo of America sold additional faceplates on its online store. In Europe, the Game Boy Micro comes with a single faceplate. In Japan, a special "Mother 3" limited edition Game Boy Micro was released with the game in the "Mother 3 Deluxe Box". Unlike the Game Boy Advance and Game Boy Advance SP, the Game Boy Micro is unable to play any original Game Boy or Game Boy Color games, only playing Game Boy Advance titles (with the exception of the Nintendo e-Reader, discontinued in America, but still available in Japan).

Each video game is stored on a plastic cartridge, officially called a "Game Pak" by Nintendo. All cartridges, excluding those for Game Boy Advance, measure 5.8 by 6.5 cm. The cartridge provides the code and game data to the console's CPU. Some cartridges include a small battery with SRAM, flash memory chip, or EEPROM, which allows game data to be saved when the console is turned off. If the battery runs out in a cartridge, then the save data will be lost, however, it is possible to replace the battery with a new battery. To do this, the cartridge must be unscrewed, opened up, and the old battery would be removed and replaced. This may require desoldering the dead battery and soldering the replacement in place. Before 2003, Nintendo used round, flat watch batteries for saving information on the cartridges. These batteries were replaced in newer cartridges because they could only live for a certain amount of time.

The cartridge is inserted into the console cartridge slot. If the cartridge is removed while the power is on, and the Game Boy does not automatically reset, the game freezes; the Game Boy may exhibit unexpected behavior, such as rows of zeros appearing on the screen, the sound remaining at the same pitch as was emitted the instant the game was pulled out, saved data may be corrupted, and hardware may be damaged. This applies to most video game consoles that use cartridges.

The original Game Boy power switch was designed to prevent the player from being able to remove the cartridge while the power is on. Cartridges intended only for Game Boy Color (and not for the original Game Boy) lack the "notch" for the locking mechanism present in the top of the original cartridges, preventing operation on an original Game Boy (the cartridge can be inserted, but the power switch cannot be moved to the "on" position). Even if this was bypassed by using a Game Boy Pocket, Game Boy Light, or Super Game Boy (and its Japanese-only follow-up), the game would not run, and an image on the screen would inform the user that the game is only compatible with Game Boy Color systems. One exception would be the Kirby Tilt 'n' Tumble game: despite the game cartridge featuring a notch, enabling it to be inserted on the original Game Boy, the game displays an error message indicating that it only plays on Game Boy Color. "Chee Chai Alien" and "Pocket Music" are incompatible with Game Boy Advance models, displaying an error message indicating that they only play on Game Boy Color.

Game Boy Advance cartridges used a similar physical lock-out feature. Notches were located at the base of the cartridge's two back corners. One of these notches was placed as to avoid pressing a switch inside the cartridge slot to help stabilize it. When an older Game Boy or Game Boy Color game was inserted into the cartridge slot, the switch would be pressed down and the Game Boy Advance would start in Game Boy Color mode, while a Game Boy Advance cartridge would not touch the switch and the system would start in Game Boy Advance mode. The Nintendo DS replaced the switch with a solid piece of plastic that would allow Game Boy Advance cartridges to be inserted into Slot 2, but would prevent an older Game Boy or Game Boy Color cartridge from being inserted fully into the slot.

Excluding game-specific variations, there are four types of cartridges compatible with Game Boy systems:

The Game Boy, as with many other consoles, has had a number of releases from both first-party and unlicensed third-party accessories. The most notable being the Game Boy Camera (left) and the Game Boy Printer (right), which were released in 1998.
In addition to the Game Boy, special hardware has been released for various handhelds in the Game Boy line so they can be played on a television set.

In 1994, a special adapter cartridge for Nintendo's Super Nintendo Entertainment System (SNES) was released called the Super Game Boy. The Super Game Boy allows game cartridges designed for use on the Game Boy to be played on a TV display using the SNES/Super Famicom controllers. When it was released in 1994, the Super Game Boy sold for about $60 in the United States. In the United Kingdom, it retailed for £49.99. The Super Game Boy's technical architecture is similar to that of a regular Game Boy, thus Game Boy games functioned on the native hardware rather than being emulated by the SNES. It was the precursor to the Game Boy Player on the Nintendo GameCube, which functioned in a similar manner.

A follow-up of the Super Game Boy, the Super Game Boy 2 was released only in Japan in 1998. The border is similar to that of actual Game Boy Pocket hardware, but it includes an actual link cable port, and the clock speed is slowed down to match that of the Game Boy.

The Game Boy Player is a device released in 2003 by Nintendo for the GameCube which enables Game Boy (although Super Game Boy enhancements are ignored), Game Boy Color, or Game Boy Advance cartridges to be played on a television. It connects via the high speed parallel port at the bottom of the GameCube and requires use of a boot disc to access the hardware. Unlike devices such as Datel's Advance Game Port, the Game Boy Player does not use software emulation, but instead uses physical hardware nearly identical to that of a Game Boy Advance.

Approximately two thousand games are available for the Game Boy, which can be attributed in part to its sales in the amount of millions, a well-documented design, and a typically short development cycle.
The Nintendo DS and Nintendo DS Lite are able to play the large library of Game Boy Advance games (though the Nintendo DSi, Nintendo DSi XL, Nintendo 3DS, and Nintendo 2DS lack a GBA game cartridge slot). However, the DS consoles do not have a GBA game link connector, and so cannot play multiplayer GBA games (except for the few that are multiplayer on a single GBA) or link to the GameCube. The DS is not backward-compatible with Game Paks for the original Game Boy or the Game Boy Color. With homebrew development on the Nintendo DS, full speed Game Boy and Game Boy Color emulation has been achieved as well as the ability to scale the smaller Game Boy screen image to the full DS screen.

Numerous musical acts have appropriated the Game Boy as a musical instrument (Game Boy music), using software such as nanoloop or Little Sound DJ.

Certain games released for the Game Boy and Game Boy Color handheld consoles are available via the Virtual Console service on the Nintendo 3DS. Game Boy Advance games were thought to be as well due to the 3DS not being compatible, but it was just a mistranslation. However, ten Game Boy Advance games were released for Nintendo 3DS ambassadors, as in Nintendo 3DS owners who logged into the 3DS eShop before the major August 2011 price drop. The Virtual Console GBA features of releases are limited, and there are no plans to release them to the public. However, starting from April 2014, Nintendo has been releasing Game Boy Advance games as Virtual Console titles via the Nintendo eShop for the Wii U.




</doc>
<doc id="11982" url="https://en.wikipedia.org/wiki?curid=11982" title="Gemini 10">
Gemini 10

Gemini 10 (officially Gemini X) was a 1966 crewed spaceflight in NASA's Gemini program. It was the 8th crewed Gemini flight, the 16th crewed American flight, and the 24th spaceflight of all time (includes X-15 flights over ).


Jim Lovell and Buzz Aldrin had originally been named the backup crew, but after Charles Bassett and Elliot See died in a T-38 crash, they were moved to the backup crew for Gemini 9 and Alan Bean and Clifton Williams were moved to the Gemini 10 flight.




Gemini 10 was designed to achieve rendezvous and docking with an Agena Target Vehicle (ATV), and EVA. It was also planned to dock with the ATV from the Gemini 8 mission. This Agena's battery power had failed months earlier, and an approach and docking would demonstrate the ability to rendezvous with a passive object. It would be also the first mission to fire the Agena's own rocket, allowing them to reach higher orbits.

Gemini 10 established that radiation at high altitude was not a problem. After docking with their Agena booster in low orbit, Young and Collins used it to climb temporarily to . After leaving the first Agena, they then rendezvoused with the derelict Agena left over from the aborted Gemini 8 flight—thus executing the program's first double rendezvous. With no electricity on board the second Agena, the rendezvous was accomplished with eyes only—no radar.

After the rendezvous, Collins spacewalked over to the dormant Agena at the end of a tether, making him the first person to meet another spacecraft in orbit. Collins then retrieved a cosmic dust-collecting panel from the side of the Agena. As he was concentrating on keeping his tether clear of the Gemini and Agena, Collins' Hasselblad camera worked itself free and drifted away, so he was unable to take photographs during the spacewalk.

The Agena launched perfectly for the second time, after problems had occurred with the targets for Gemini 6 and 9. Gemini 10 followed 100 minutes later and entered a orbit. They were behind the Agena. Two anomalous events occurred during the launch. At liftoff, a propellant fill umbilical became snared with its release lanyard. It ripped out of the LC-19 service tower and remained attached to the second stage during ascent. Tracking camera footage also showed that the first stage oxidizer tank dome ruptured after staging and released a cloud of nitrogen tetroxide. The telemetry package on the first stage had been disabled at staging, so visual evidence was the only data available. Film review of the Titan II ICBM launches found at least seven other instances of post-staging tank ruptures, most likely caused by flying debris, second stage engine exhaust, or structural bending. NASA finally decided that this phenomenon did not pose any safety risk to the astronauts and took no corrective action.

Collins was unable to use the sextant for navigation as it did not seem to work as expected. At first he mistook airglow as the real horizon when trying to make some fixes on stars. When the image didn't seem right he tried another instrument, but this was not practical to use as it had a very small field of view.

They had a backup in the form of the computers on the ground. They made their first burn to put them into a orbit. However Young didn't realize that during the next burn, he had the spacecraft turned slightly, which meant that they introduced an out-of-plane error. This meant two extra burns were necessary, and by the time they had docked with the Agena, 60% of their fuel had been consumed. It was decided to keep the Gemini docked to the Agena as long as possible, as this would mean that they could use the fuel on board the Agena for attitude control.

The first burn of the Agena engine lasted 80 seconds and put them in a orbit. This was the highest a person had ever been, although the record was soon surpassed by Gemini 11, which went to over . This burn was quite a ride for the crew. Because the Gemini and Agena docked nose-to-nose, the forces experienced were "eyeballs out" as opposed to "eyeballs in" for a launch from Earth. The crew took a couple of pictures when they reached apogee but were more interested in what was going on in the spacecraft — checking the systems and watching the radiation dosage meter.

After this they had their sleep period which lasted for eight hours and then they were ready for another busy day. The crew's first order of business was to make a second burn with the Agena engine to put them into the same orbit as the Gemini 8 Agena. This was at 20:58 UTC on July 19 and lasted 78 seconds and took off their speed, putting them into a orbit. They made one more burn of the Agena to circularize their orbit to .

The first of two EVAs on Gemini 10 was a standup EVA, where Collins would stand in the open hatch and take photographs of stars as part of experiment S-13. They used a 70 mm general purpose camera to image the southern Milky Way in ultraviolet. After orbital sunrise Collins photographed a color plate on the side of the spacecraft (MSC-8) to see whether film reproduced colors accurately in space. He reentered the spacecraft six minutes early when both astronauts found that their eyes were irritated, which was caused by a minor leak of lithium hydroxide in the astronauts' oxygen supply. After repressurizing the cabin, they ran the oxygen at high rates and flushed the environment system.

After the exercise of the EVA Young and Collins slept in their second 'night' in space. The next 'morning' they started preparing for the second rendezvous and another EVA.

After undocking from their Agena, the crew thought they sighted the Gemini 8 Agena. It however turned out to be their own Agena away, while their target was away. It wasn't until just over away that they saw it as a faint star. After a few more correction burns, they were station-keeping away from the Gemini 8 Agena. They found the Agena to be very stable and in good condition.

At 48 hours and 41 minutes into the mission, the second EVA began. Collins' first task was to retrieve a Micrometeorite Collector (S-12) from the side of the spacecraft. This he accomplished with some difficulty (similar to that encountered by Eugene Cernan on Gemini 9A). The collector floated out of the cabin at some time during the EVA, and was lost.

Collins next traveled over to the Agena and tried to grab onto the docking cone but found this impossible as it was smooth and had no grip. He used a nitrogen-propelled Hand-Held Maneuvering Unit (HHMU) to move himself towards the Gemini and then back to the Agena. This time he was able to grab hold of some wire bundles and retrieved the Micrometeorite Collector (S-10) from the Agena. He decided against replacing it as a piece of shroud had come loose on the Agena which could have snared the umbilical, and returning to the Gemini was deemed the safest course of action.

The last tasks remaining on this EVA were to test out the HHMU, test orbital mechanics using a tether between the Gemini and Agena, and for Young in the spacecraft to translate over to a passive Collins. However, due to low propellant quantity remaining, combined with intermittent telemetry to monitor it, these fuel costly manoeuvres were abandoned and the EVA was finished after only 39 minutes. During this time, it took the crew eight minutes to close the hatch as they had some difficulty with the umbilical. It was jettisoned along with the chestpack used by Collins an hour later when they opened the hatch for the third and final time.

There were ten other experiments that the crew performed during the mission. Three were interested in radiation: MSC-3 was the Tri-Axis Magnetometer which measured levels in the South Atlantic Anomaly. There was also MSC-6, a beta spectrometer, which measured potential radiation doses for Apollo missions, and MSC-7, a bremsstrahlung spectrometer which detected radiation flux as a function of energy when the spacecraft passed through the South Atlantic Anomaly.

S-26 investigated the ion and electron wake of the spacecraft. This provided limited results due to the lack of fuel for attitude control, but found that electron and ion temperatures were higher than expected and it registered shock effects during docking and undocking.

The S-5 and S-6 experiments were performed, which were previously carried on Gemini 9A; these were Synoptic Terrain and Synoptic Weather photography respectively. There was also S-1 which was intended to image the Zodiacal light. All of these experiments were of little use as the film used was only half as sensitive as Gemini 9A and the dirty windows lowered the transmission of light by a factor of six.

The crew also tried to perform D-5, a navigation experiment. They were only able to track five stars, with six needed for accurate measurements. The last experiment, D-10, was to investigate an ion-sensing attitude control system. This experiment measured the attitude of the spacecraft from the flow of ions and electrons around the spacecraft in orbit. The results from this experiment showed the system to be accurate and responsive.

The last day of the mission was short and retrofire came at 70 hours and 10 minutes into the mission. They landed only away from the intended landing site and were recovered by .

The Gemini 10 mission was supported by the following U.S. Department of Defense resources: 9,067 personnel, 78 aircraft and 13 ships.

The patch is simple in design but highly symbolic. The main feature is a large X with a Gemini and Agena orbiting around it. The two stars have a variety of meanings: the two rendezvous attempts, Castor and Pollux in Gemini or the two crew members. This is one of the few crew patches without the crew's name. It is able to be displayed "upside down" but is correctly shown with the spacecraft to the right. It was designed by Young's first wife, Barbara.

For many years the spacecraft was the centerpiece of a space exhibition at Norsk Teknisk Museum, Oslo, Norway. It was returned on request in 2002.

The spacecraft is currently on display at the Cosmosphere in Hutchinson, Kansas.




</doc>
<doc id="11984" url="https://en.wikipedia.org/wiki?curid=11984" title="Gardening">
Gardening

Gardening is the practice of growing and cultivating plants as part of horticulture. In gardens, ornamental plants are often grown for their flowers, foliage, or overall appearance; useful plants, such as root vegetables, leaf vegetables, fruits, and herbs, are grown for consumption, for use as dyes, or for medicinal or cosmetic use. Gardening is considered by many people to be a relaxing activity.

Gardening ranges in scale from fruit orchards, to long boulevard plantings with one or more different types of shrubs, trees, and herbaceous plants, to residential back gardens including lawns and foundation plantings, and to container gardens grown inside or outside. Gardening may be very specialized, with only one type of plant grown, or involve a variety of plants in mixed plantings. It involves an active participation in the growing of plants, and tends to be labor-intensive, which differentiates it from farming or forestry.

Forest gardening, a forest-based food production system, is the world's oldest form of gardening. Forest gardens originated in prehistoric times along jungle-clad river banks and in the wet foothills of monsoon regions. In the gradual process of families improving their immediate environment, useful tree and vine species were identified, protected and improved while undesirable species were eliminated. Eventually foreign species were also selected and incorporated into the gardens.

After the emergence of the first civilizations, wealthy individuals began to create gardens for aesthetic purposes. Ancient Egyptian tomb paintings from the New Kingdom (around 1500 BC) provide some of the earliest physical evidence of ornamental horticulture and landscape design; they depict lotus ponds surrounded by symmetrical rows of acacias and palms. A notable example of ancient ornamental gardens were the Hanging Gardens of Babylon—one of the Seven Wonders of the Ancient World —while ancient Rome had dozens of gardens.

Wealthy ancient Egyptians used gardens for providing shade. Egyptians associated trees and gardens with gods, believing that their deities were pleased by gardens. Gardens in ancient Egypt were often surrounded by walls with trees planted in rows. Among the most popular species planted were date palms, sycamores, fir trees, nut trees, and willows. These gardens were a sign of higher socioeconomic status. In addition, wealthy ancient Egyptians grew vineyards, as wine was a sign of the higher social classes. Roses, poppies, daisies and irises could all also be found in the gardens of the Egyptians.

Assyria was also renowned for its beautiful gardens. These tended to be wide and large, some of them used for hunting game—rather like a game reserve today—and others as leisure gardens. Cypresses and palms were some of the most frequently planted types of trees.

Ancient Roman gardens were laid out with hedges and vines and contained a wide variety of flowers—acanthus, cornflowers, crocus, cyclamen, hyacinth, iris, ivy, lavender, lilies, myrtle, narcissus, poppy, rosemary and violets—as well as statues and sculptures. Flower beds were popular in the courtyards of rich Romans.

The Middle Ages represent a period of decline in gardens for aesthetic purposes. After the fall of Rome, gardening was done for the purpose of growing medicinal herbs and/or decorating church altars. Monasteries carried on a tradition of garden design and intense horticultural techniques during the medieval period in Europe. 
Generally, monastic garden types consisted of kitchen gardens, infirmary gardens, cemetery orchards, cloister garths and vineyards. Individual monasteries might also have had a "green court", a plot of grass and trees where horses could graze, as well as a cellarer's garden or private gardens for obedientiaries, monks who held specific posts within the monastery.

Islamic gardens were built after the model of Persian gardens and they were usually enclosed by walls and divided in four by watercourses. Commonly, the centre of the garden would have a reflecting pool or pavilion. Specific to the Islamic gardens are the mosaics and glazed tiles used to decorate the rills and fountains that were built in these gardens.

By the late 13th century, rich Europeans began to grow gardens for leisure and for medicinal herbs and vegetables. They surrounded the gardens by walls to protect them from animals and to provide seclusion. During the next two centuries, Europeans started planting lawns and raising flowerbeds and trellises of roses. Fruit trees were common in these gardens and also in some, there were turf seats. At the same time, the gardens in the monasteries were a place to grow flowers and medicinal herbs but they were also a space where the monks could enjoy nature and relax.

The gardens in the 16th and 17th century were symmetric, proportioned and balanced with a more classical appearance. Most of these gardens were built around a central axis and they were divided into different parts by hedges. Commonly, gardens had flowerbeds laid out in squares and separated by gravel paths.

Gardens in Renaissance were adorned with sculptures, topiary and fountains. In the 17th century, knot gardens became popular along with the hedge mazes. By this time, Europeans started planting new flowers such as tulips, marigolds and sunflowers.

Cottage gardens, which emerged in Elizabethan times, appear to have originated as a local source for herbs and fruits. One theory is that they arose out of the Black Death of the 1340s, when the death of so many laborers made land available for small cottages with personal gardens. According to the late 19th-century legend of origin, these gardens were originally created by the workers that lived in the cottages of the villages, to provide them with food and herbs, with flowers planted among them for decoration. Farm workers were provided with cottages that had architectural quality set in a small garden—about —where they could grow food and keep pigs and chickens.

Authentic gardens of the yeoman cottager would have included a beehive and livestock, and frequently a pig and sty, along with a well. The peasant cottager of medieval times was more interested in meat than flowers, with herbs grown for medicinal use rather than for their beauty. By Elizabethan times there was more prosperity, and thus more room to grow flowers. Even the early cottage garden flowers typically had their practical use—violets were spread on the floor (for their pleasant scent and keeping out vermin); calendulas and primroses were both attractive and used in cooking. Others, such as sweet William and hollyhocks, were grown entirely for their beauty.

In the 18th century gardens were laid out more naturally, without any walls. This style of smooth undulating grass, which would run straight to the house, clumps, belts and scattering of trees and his serpentine lakes formed by invisibly damming small rivers, were a new style within the English landscape, a "gardenless" form of landscape gardening, which swept away almost all the remnants of previous formally patterned styles. The English landscape garden usually included a lake, lawns set against groves of trees, and often contained shrubberies, grottoes, pavilions, bridges and follies such as mock temples, Gothic ruins, bridges, and other picturesque architecture, designed to recreate an idyllic pastoral landscape. This new style emerged in England in the early 18th century, and spread across Europe, replacing the more formal, symmetrical garden à la française of the 17th century as the principal gardening style of Europe. The English garden presented an idealized view of nature. They were often inspired by paintings of landscapes by Claude Lorraine and Nicolas Poussin, and some were Influenced by the classic Chinese gardens of the East, which had recently been described by European travelers. The work of Lancelot 'Capability' Brown was particularly influential. Also, in 1804 the Horticultural Society was formed. 

Gardens of the 19th century contained plants such as the monkey puzzle or Chile pine. This is also the time when the so-called "gardenesque" style of gardens evolved. These gardens displayed a wide variety of flowers in a rather small space. Rock gardens increased in popularity in the 19th century.

Residential gardening takes place near the home, in a space referred to as the garden. Although a garden typically is located on the land near a residence, it may also be located on a roof, in an atrium, on a balcony, in a windowbox, on a patio or vivarium.

Gardening also takes place in non-residential green areas, such as parks, public or semi-public gardens (botanical gardens or zoological gardens), amusement parks, along transportation corridors, and around tourist attractions and garden hotels. In these situations, a staff of gardeners or groundskeepers maintains the gardens.

People can express their political or social views in gardens, intentionally or not. The lawn vs. garden issue is played out in urban planning as the debate over the "land ethic" that is to determine urban land use and whether hyper hygienist bylaws (e.g. weed control) should apply, or whether land should generally be allowed to exist in its natural wild state. In a famous Canadian Charter of Rights case, "Sandra Bell vs. City of Toronto", 1997, the right to cultivate all native species, even most varieties deemed noxious or allergenic, was upheld as part of the right of free expression.

Community gardening comprises a wide variety of approaches to sharing land and gardens.
People often surround their house and garden with a hedge. Common hedge plants are privet, hawthorn, beech, yew, leyland cypress, hemlock, arborvitae, barberry, box, holly, oleander, forsythia and lavender. The idea of open gardens without hedges may be distasteful to those who enjoy privacy.
The Slow Food movement has sought in some countries to add an edible school yard and garden classrooms to schools, e.g. in Fergus, Ontario, where these were added to a public school to augment the kitchen classroom. Garden sharing, where urban landowners allow gardeners to grow on their property in exchange for a share of the harvest, is associated with the desire to control the quality of one's food, and reconnect with soil and community.

In US and British usage, the production of ornamental plantings around buildings is called "landscaping", "landscape maintenance" or "grounds keeping", while international usage uses the term "gardening" for these same activities.

Also gaining popularity is the concept of "Green Gardening" which involves growing plants using organic fertilizers and pesticides so that the gardening process – or the flowers and fruits produced thereby – doesn't adversely affect the environment or people's health in any manner.

Gardening for beauty is likely nearly as old as farming for food, however for most of history for the majority of people there was no real distinction since the need for food and other useful products trumped other concerns. Small-scale, subsistence agriculture (called hoe-farming) is largely indistinguishable from gardening. A patch of potatoes grown by a Peruvian peasant or an Irish smallholder for personal use could be described as either a garden or a farm. Gardening for average people evolved as a separate discipline, more concerned with aesthetics, recreation and leisure,
under the influence of the pleasure gardens of the wealthy. Meanwhile, farming has evolved (in developed countries) in the direction of commercialization, economics of scale, and monocropping.

In respect to its food-producing purpose, gardening is distinguished from farming chiefly by scale and intent. Farming occurs on a larger scale, and with the production of salable goods as a major motivation. Gardening happens on a smaller scale, primarily for pleasure and to produce goods for the gardener's own family or community. There is some overlap between the terms, particularly in that some moderate-sized vegetable growing concerns, often called market gardening, can fit in either category.

The key distinction between gardening and farming is essentially one of scale; gardening can be a hobby or an income supplement, but farming is generally understood as a full-time or commercial activity, usually involving more land and quite different practices. One distinction is that gardening is labor-intensive and employs very little infrastructural capital, sometimes no more than a few tools, e.g. a spade, hoe, basket and watering can. By contrast, larger-scale farming often involves irrigation systems, chemical fertilizers and harvesters or at least ladders, e.g. to reach up into fruit trees. However, this distinction is becoming blurred with the increasing use of power tools in even small gardens.

Monty Don has speculated on an atavistic connection between present-day gardeners and pre-modern peasantry.

The term precision agriculture is sometimes used to describe gardening using intermediate technology (more than tools, less than harvesters), especially of organic varieties. Gardening is effectively scaled up to feed entire villages of over 100 people from specialized plots. A variant is the community garden which offers plots to urban dwellers; see further in allotment (gardening).

There is a wide range of garden ornaments and accessories available in the market for both the professional gardener and the amateur to exercise their creativity. These are used to add decoration or functionality, and may be made from a wide range of materials such as copper, stone, wood, bamboo, stainless steel, clay, stained glass, concrete, or iron. Examples include trellis, garden furniture, statues, outdoor fireplaces, fountains, rain chains, urns, bird baths and feeders, wind chimes, and garden lighting such as candle lanterns and oil lamps. The use of these items can be part of the expression of a gardener's gardening personality.

Garden design is considered to be an art in most cultures, distinguished from gardening, which generally means "garden maintenance". Garden design can include different themes such as perennial, butterfly, wildlife, Japanese, water, tropical, or shade gardens. 

In Japan, Samurai and Zen monks were often required to build decorative gardens or practice related skills like flower arrangement known as "ikebana". In 18th-century Europe, country estates were refashioned by landscape gardeners into formal gardens or landscaped park lands, such as at Versailles, France, or Stowe, England. Today, landscape architects and garden designers continue to produce artistically creative designs for private garden spaces. In the US, professional landscape designers are certified by the Association of Professional Landscape Designers.

Garden pests are generally plants, fungi, or animals (frequently insects) that engage in activity that the gardener considers undesirable. A pest may crowd out desirable plants, disturb soil, stunt the growth of young seedlings, steal or damage fruit, or otherwise kill plants, hamper their growth, damage their appearance, or reduce the quality of the edible or ornamental portions of the plant. Aphids, spider mites, slugs, snails, ants, birds, and even cats are commonly considered to be garden pests.
Because gardeners may have different goals, organisms considered "garden pests" vary from gardener to gardener. "Tropaeolum speciosum", for example, may be considered a desirable and ornamental garden plant, or it may be considered a pest if it seeds and starts to grow where it is not wanted. As another example, in lawns, moss can become dominant and be impossible to eradicate. In some lawns, lichens, especially very damp lawn lichens such as "Peltigera lactucfolia" and "P. membranacea", can become difficult to control and are considered pests.

There are many ways by which unwanted pests are removed from a garden. The techniques vary depending on the pest, the gardener's goals, and the gardener's philosophy. For example, snails may be dealt with through the use of a chemical pesticide, an organic pesticide, hand-picking, barriers, or simply growing snail-resistant plants.

Pest control is often done through the use of pesticides, which may be either organic or artificially synthesized. Pesticides may affect the ecology of a garden due to their effects on the populations of both target and non-target species. For example, unintended exposure to some neonicotinoid pesticides has been proposed as a factor in the recent decline in honey bee populations. A mole vibrator can deter mole activity in a garden.

Other means of control include the removal of infected plants, using fertilizers and biostimulants to improve the health and vigour of plants so they better resist attack, practising crop rotation to prevent pest build-up, using companion planting, and practising good garden hygiene, such as disinfecting tools and clearing debris and weeds which may harbour pests.

Garden guns are smooth bore shotguns specifically made to fire .22 caliber snake shot, and are commonly used by gardeners and farmers for pest control. Garden guns are short range weapons that can do little harm past to , and they're relatively quiet when fired with snake shot, compared to a standard ammunition. These guns are especially effective inside of barns and sheds, as the snake shot will not shoot holes in the roof or walls, or more importantly injure livestock with a ricochet. They are also used for pest control at airports, warehouses, stockyards, etc.




</doc>
<doc id="11985" url="https://en.wikipedia.org/wiki?curid=11985" title="Graffiti">
Graffiti

Graffiti (both singular and plural; the singular graffito is very rare in English except in archeology) is writing or drawings made on a wall or other surface, usually without permission and within public view. Graffiti ranges from simple written words to elaborate wall paintings, and has existed since ancient times, with examples dating back to ancient Egypt, ancient Greece, and the Roman Empire.

In modern times, spray paint and marker pens have become commonly used graffiti materials, and there are many different types and styles of graffiti; it is a rapidly developing art form.

Graffiti is a controversial subject. In most countries, marking or painting property without permission is considered by property owners and civic authorities as defacement and vandalism, which is a punishable crime, citing the use of graffiti by street gangs to mark territory or to serve as an indicator of gang-related activities. Graffiti has become visualized as a growing urban "problem" for many cities in industrialized nations, spreading from the New York City subway system in the early 1970s to the rest of the United States and Europe and other world regions. On the other hand, graffiti artists, particularly marginalized artists with no access to mainstream media, resist this viewpoint to display their art or political views in public locations.

Both "graffiti" and its occasional singular form "graffito" are from the Italian word "graffiato" ("scratched"). "Graffiti" is applied in art history to works of art produced by scratching a design into a surface. A related term is "sgraffito", which involves scratching through one layer of pigment to reveal another beneath it. This technique was primarily used by potters who would glaze their wares and then scratch a design into it. In ancient times graffiti were carved on walls with a sharp object, although sometimes chalk or coal were used. The word originates from Greek —"graphein"—meaning "to write".

The term "graffiti" referred to the inscriptions, figure drawings, and such, found on the walls of ancient sepulchres or ruins, as in the Catacombs of Rome or at Pompeii. Use of the word has evolved to include any graphics applied to surfaces in a manner that constitutes vandalism.

The only known source of the Safaitic language, an ancient form of Arabic, is from graffiti: inscriptions scratched on to the surface of rocks and boulders in the predominantly basalt desert of southern Syria, eastern Jordan and northern Saudi Arabia. Safaitic dates from the first century BC to the fourth century AD.

The first known example of "modern style" graffiti survives in the ancient Greek city of Ephesus (in modern-day Turkey). Local guides say it is an advertisement for prostitution. Located near a mosaic and stone walkway, the graffiti shows a handprint that vaguely resembles a heart, along with a footprint, a number, and a carved image of a woman's head.

The ancient Romans carved graffiti on walls and monuments, examples of which also survive in Egypt. Graffiti in the classical world had different connotations than they carry in today's society concerning content. Ancient graffiti displayed phrases of love declarations, political rhetoric, and simple words of thought, compared to today's popular messages of social and political ideals.
The eruption of Vesuvius preserved graffiti in Pompeii, which includes Latin curses, magic spells, declarations of love, insults, alphabets, political slogans, and famous literary quotes, providing insight into ancient Roman street life. One inscription gives the address of a woman named Novellia Primigenia of Nuceria, a prostitute, apparently of great beauty, whose services were much in demand. Another shows a phallus accompanied by the text, "mansueta tene" ("handle with care").

Disappointed love also found its way onto walls in antiquity:

Ancient tourists visiting the 5th-century citadel at Sigiriya in Sri Lanka scribbled over 1800 individual graffiti there between the 6th and 18th centuries. Etched on the surface of the Mirror Wall, they contain pieces of prose, poetry, and commentary. The majority of these visitors appear to have been from the elite of society: royalty, officials, professions, and clergy. There were also soldiers, archers, and even some metalworkers. The topics range from love to satire, curses, wit, and lament. Many demonstrate a very high level of literacy and a deep appreciation of art and poetry. Most of the graffiti refer to the frescoes of semi-nude females found there. One reads:

Among the ancient political graffiti examples were Arab satirist poems. Yazid al-Himyari, an Umayyad Arab and Persian poet, was most known for writing his political poetry on the walls between Sajistan and Basra, manifesting a strong hatred towards the Umayyad regime and its "walis", and people used to read and circulate them very widely.

Historic forms of graffiti have helped gain understanding into the lifestyles and languages of past cultures. Errors in spelling and grammar in these graffiti offer insight into the degree of literacy in Roman times and provide clues on the pronunciation of spoken Latin. Examples are "CIL" IV, 7838: "Vettium Firmum / aed"[ilem] "quactiliar"[ii] "rog"[ant]. Here, "qu" is pronounced "co". The 83 pieces of graffiti found at "CIL" IV, 4706-85 are evidence of the ability to read and write at levels of society where literacy might not be expected. The graffiti appear on a peristyle which was being remodeled at the time of the eruption of Vesuvius by the architect Crescens. The graffiti were left by both the foreman and his workers. The brothel at "CIL" VII, 12, 18–20 contains more than 120 pieces of graffiti, some of which were the work of the prostitutes and their clients. The gladiatorial academy at "CIL" IV, 4397 was scrawled with graffiti left by the gladiator Celadus Crescens ("Suspirium puellarum Celadus thraex": "Celadus the Thracian makes the girls sigh.")

Another piece from Pompeii, written on a tavern wall about the owner of the establishment and his questionable wine:

It was not only the Greeks and Romans who produced graffiti: the Maya site of Tikal in Guatemala contains examples of ancient Maya graffiti. Viking graffiti survive in Rome and at Newgrange Mound in Ireland, and a Varangian scratched his name (Halvdan) in runes on a banister in the Hagia Sophia at Constantinople. These early forms of graffiti have contributed to the understanding of lifestyles and languages of past cultures.

Graffiti, known as Tacherons, were frequently scratched on Romanesque Scandinavian church walls.
When Renaissance artists such as Pinturicchio, Raphael, Michelangelo, Ghirlandaio, or Filippino Lippi descended into the ruins of Nero's Domus Aurea, they carved or painted their names and returned to initiate the "grottesche" style of decoration.

There are also examples of graffiti occurring in American history, such as Independence Rock, a national landmark along the Oregon Trail.

Later, French soldiers carved their names on monuments during the Napoleonic in the 1790s. Lord Byron's survives on one of the columns of the Temple of Poseidon at Cape Sounion in Attica, Greece.

Contemporary graffiti style has been heavily influenced by hip hop culture and the myriad international styles derived from Philadelphia and New York City Subway graffiti, however, there are many other traditions of notable graffiti in the twentieth century. Graffiti have long appeared on building walls, in latrines, railroad boxcars, subways, and bridges.

The oldest known example of modern graffiti are the "monikers" found on traincars created by hobos and railworkers since the late 1800s. The Bozo Texino monikers were documented by filmmaker Bill Daniel in his 2005 film, "Who is Bozo Texino?".

Some graffiti have their own poignancy. In World War II, an inscription on a wall at the fortress of Verdun was seen as an illustration of the US response twice in a generation to the wrongs of the Old World:

During World War II and for decades after, the phrase "Kilroy was here" with an accompanying illustration was widespread throughout the world, due to its use by American troops and ultimately filtering into American popular culture. Shortly after the death of Charlie Parker (nicknamed "Yardbird" or "Bird"), graffiti began appearing around New York with the words "Bird Lives". The student protests and general strike of May 1968 saw Paris bedecked in revolutionary, anarchistic, and situationist slogans such as "L'ennui est contre-révolutionnaire" ("Boredom is counterrevolutionary") expressed in painted graffiti, poster art, and stencil art. At the time in the US, other political phrases (such as "Free Huey" about Black Panther Huey Newton) became briefly popular as graffiti in limited areas, only to be forgotten. A popular graffito of the early 1970s was "Dick Nixon Before He Dicks You", reflecting the hostility of the youth culture to that US president.

Rock and roll graffiti is a significant subgenre. A famous graffito of the twentieth century was the inscription in the London tube reading "Clapton is God" in a link to the guitarist Eric Clapton. The phrase was spray-painted by an admirer on a wall in an Islington station on the Underground in the autumn of 1967. The graffito was captured in a photograph, in which a dog is urinating on the wall.

Graffiti also became associated with the anti-establishment punk rock movement beginning in the 1970s. Bands such as Black Flag and Crass (and their followers) widely stenciled their names and logos, while many punk night clubs, squats, and hangouts are famous for their graffiti. In the late 1980s the upside down Martini glass that was the tag for punk band Missing Foundation was the most ubiquitous graffito in lower Manhattan

In 1979, graffitists Lee Quiñones and Fab 5 Freddy were given a gallery opening in Rome by art dealer Claudio Bruni. For many outside of New York, it was their first encounter with their art form. Fab5 Freddy's friendship with Debbie Harry influenced Blondie's single "Rapture" (Chrysalis, 1981), the video of which featured Jean-Michel Basquiat, and offered many their first glimpse of a depiction of elements of graffiti in hip hop culture. JaJaJa toured Germany, Switzerland, Belgium, and Holland with a large graffiti canvas as a backdrop. Charlie Ahearn's independently released fiction film "Wild Style" (Wild Style, 1983), the early PBS documentary "Style Wars" (1983), hit songs such as "The Message" and "Planet Rock" and their accompanying music videos (both 1982) contributed to a growing interest outside New York in all aspects of hip hop.

"Style Wars" depicted not only famous graffitists such as Skeme, Dondi, MinOne, and ZEPHYR, but also reinforced graffiti's role within New York's emerging hip-hop culture by incorporating famous early break-dancing groups such as Rock Steady Crew into the film and featuring rap in the soundtrack. Although many officers of the New York City Police Department found this film to be controversial, Style Wars is still recognized as the most prolific film representation of what was going on within the young hip hop culture of the early 1980s. Fab5 Freddy and Futura 2000 took hip hop graffiti to Paris and London as part of the New York City Rap Tour in 1983. Hollywood also paid attention, consulting writers such as PHASE 2 as it depicted the culture and gave it international exposure in movies such as "Beat Street" (Orion, 1984).

This period also saw the emergence of the new stencil graffiti genre. Some of the first examples were created in 1981 by graffitists Blek le Rat in Paris, in 1982 by Jef Aerosol in Tours (France); by 1985 stencils had appeared in other cities including New York City, Sydney, and Melbourne, where they were documented by American photographer Charles Gatewood and Australian photographer Rennie Ellis.

With the popularity and legitimization of graffiti has come a level of commercialization. In 2001, computer giant IBM launched an advertising campaign in Chicago and San Francisco which involved people spray painting on sidewalks a peace symbol, a heart, and a penguin (Linux mascot), to represent "Peace, Love, and Linux." IBM paid Chicago and San Francisco collectively US$120,000 for punitive damages and clean-up costs.

In 2005, a similar ad campaign was launched by Sony and executed by its advertising agency in New York, Chicago, Atlanta, Philadelphia, Los Angeles, and Miami, to market its handheld PSP gaming system. In this campaign, taking notice of the legal problems of the IBM campaign, Sony paid building owners for the rights to paint on their buildings "a collection of dizzy-eyed urban kids playing with the PSP as if it were a skateboard, a paddle, or a rocking horse".

Marc Ecko, an urban clothing designer, has been an advocate of graffiti as an art form during this period, stating that "Graffiti is without question the most powerful art movement in recent history and has been a driving inspiration throughout my career."

Graffiti have become a common stepping stone for many members of both the art and design communities in North America and abroad. Within the United States graffitists such as Mike Giant, Pursue, Rime, Noah, and countless others have made careers in skateboard, apparel, and shoe design for companies such as DC Shoes, Adidas, Rebel8, Osiris, or Circa Meanwhile, there are many others such as DZINE, Daze, Blade, and The Mac who have made the switch to being gallery artists, often not even using their initial medium, spray paint.

Tristan Manco wrote that Brazil "boasts a unique and particularly rich, graffiti scene ... [earning] it an international reputation as the place to go for artistic inspiration." Graffiti "flourishes in every conceivable space in Brazil's cities." Artistic parallels "are often drawn between the energy of São Paulo today and 1970s New York." The "sprawling metropolis," of São Paulo has "become the new shrine to graffiti;" Manco alludes to "poverty and unemployment ... [and] the epic struggles and conditions of the country's marginalised peoples," and to "Brazil's chronic poverty," as the main engines that "have fuelled a vibrant graffiti culture." In world terms, Brazil has "one of the most uneven distributions of income. Laws and taxes change frequently." Such factors, Manco argues, contribute to a very fluid society, riven with those economic divisions and social tensions that underpin and feed the "folkloric vandalism and an urban sport for the disenfranchised," that is South American graffiti art.

Prominent Brazilian graffitists include Os Gêmeos, Boleta, Nunca, Nina, Speto, Tikka, and T.Freak. Their artistic success and involvement in commercial design ventures has highlighted divisions within the Brazilian graffiti community between adherents of the cruder transgressive form of "pichação" and the more conventionally artistic values of the practitioners of "grafite".

Graffiti in the Middle East is emerging slowly, with pockets of taggers operating in the various 'Emirates' of the United Arab Emirates, in Israel, and in Iran. The major Iranian newspaper "Hamshahri" has published two articles on illegal writers in the city with photographic coverage of Iranian artist A1one's works on Tehran walls. Tokyo-based design magazine, "PingMag", has interviewed A1one and featured photographs of his work. The Israeli West Bank barrier has become a site for graffiti, reminiscent in this sense of the Berlin Wall. Many graffitists in Israel come from other places around the globe, such as JUIF from Los Angeles and DEVIONE from London. The religious reference "נ נח נחמ נחמן מאומן" ("Na Nach Nachma Nachman Meuman") is commonly seen in graffiti around Israel.

There are also a large number of graffiti influences in Southeast Asian countries that mostly come from modern Western culture, such as Malaysia, where graffiti have long been a common sight in Malaysia's capital city, Kuala Lumpur. Since 2010, the country has begun hosting a street festival to encourage all generations and people from all walks of life to enjoy and encourage Malaysian street culture.

The modern-day graffitists can be found with an arsenal of various materials that allow for a successful production of a piece. This includes such techniques as scribing. However, spray paint in aerosol cans is the number one medium for graffiti. From this commodity comes different styles, technique, and abilities to form master works of graffiti. Spray paint can be found at hardware and art stores and comes in virtually every color.

Stencil graffiti is created by cutting out shapes and designs in a stiff material (such as cardboard or subject folders) to form an overall design or image. The stencil is then placed on the "canvas" gently and with quick, easy strokes of the aerosol can, the image begins to appear on the intended surface.

Modern graffiti art often incorporates additional arts and technologies. For example, Graffiti Research Lab has encouraged the use of projected images and magnetic light-emitting diodes (throwies) as new media for graffitists. Yarnbombing is another recent form of graffiti. Yarnbombers occasionally target previous graffiti for modification, which had been avoided among the majority of graffitists.

A number of recent examples of graffiti make use of hashtags.

Theories on the use of graffiti by avant-garde artists have a history dating back at least to the Asger Jorn, who in 1962 painting declared in a graffiti-like gesture "the avant-garde won't give up".

Many contemporary analysts and even art critics have begun to see artistic value in some graffiti and to recognize it as a form of public art. According to many art researchers, particularly in the Netherlands and in Los Angeles, that type of public art is, in fact an effective tool of social emancipation or, in the achievement of a political goal.

In times of conflict, such murals have offered a means of communication and self-expression for members of these socially, ethnically, or racially divided communities, and have proven themselves as effective tools in establishing dialog and thus, of addressing cleavages in the long run. The Berlin Wall was also extensively covered by graffiti reflecting social pressures relating to the oppressive Soviet rule over the GDR.

Many artists involved with graffiti are also concerned with the similar activity of stenciling. Essentially, this entails stenciling a print of one or more colors using spray-paint. Recognized while exhibiting and publishing several of her coloured stencils and paintings portraying the Sri Lankan Civil War and urban Britain in the early 2000s, graffitists Mathangi Arulpragasam, aka M.I.A., has also become known for integrating her imagery of political violence into her music videos for singles "Galang" and "Bucky Done Gun", and her cover art. Stickers of her artwork also often appear around places such as London in Brick Lane, stuck to lamp posts and street signs, she having become a muse for other graffitists and painters worldwide in cities including Seville.

Many graffitists choose to protect their identities and remain anonymous or to hinder prosecution.

With the commercialization of graffiti (and hip hop in general), in most cases, even with legally painted "graffiti" art, graffitists tend to choose anonymity. This may be attributed to various reasons or a combination of reasons. Graffiti still remains the one of four hip hop elements that is not considered "performance art" despite the image of the "singing and dancing star" that sells hip hop culture to the mainstream. Being a graphic form of art, it might also be said that many graffitists still fall in the category of the introverted archetypal artist.

Banksy is one of the world's most notorious and popular street artists who continues to remain faceless in today's society. He is known for his political, anti-war stencil art mainly in Bristol, England, but his work may be seen anywhere from Los Angeles to Palestine. In the UK, Banksy is the most recognizable icon for this cultural artistic movement and keeps his identity a secret to avoid arrest. Much of Banksy's artwork may be seen around the streets of London and surrounding suburbs, although he has painted pictures throughout the world, including the Middle East, where he has painted on Israel's controversial West Bank barrier with satirical images of life on the other side. One depicted a hole in the wall with an idyllic beach, while another shows a mountain landscape on the other side. A number of exhibitions also have taken place since 2000, and recent works of art have fetched vast sums of money. Banksy's art is a prime example of the classic controversy: vandalism vs. art. Art supporters endorse his work distributed in urban areas as pieces of art and some councils, such as Bristol and Islington, have officially protected them, while officials of other areas have deemed his work to be vandalism and have removed it.

Pixnit is another artist who chooses to keep her identity from the general public. Her work focuses on beauty and design aspects of graffiti as opposed to Banksy's anti-government shock value. Her paintings are often of flower designs above shops and stores in her local urban area of Cambridge, Massachusetts. Some store owners endorse her work and encourage others to do similar work as well. "One of the pieces was left up above Steve's Kitchen, because it looks pretty awesome"- Erin Scott, the manager of New England Comics in Allston, Massachusetts.

Graffiti often has a reputation as part of a subculture that rebels against authority, although the considerations of the practitioners often diverge and can relate to a wide range of attitudes. It can express a political practice and can form just one tool in an array of resistance techniques. One early example includes the anarcho-punk band Crass, who conducted a campaign of stenciling anti-war, anarchist, feminist, and anti-consumerist messages throughout the London Underground system during the late 1970s and early 1980s. In Amsterdam graffiti was a major part of the punk scene. The city was covered with names such as "De Zoot", "Vendex", and "Dr Rat". To document the graffiti a punk magazine was started that was called "Gallery Anus". So when hip hop came to Europe in the early 1980s there was already a vibrant graffiti culture.

The student protests and general strike of May 1968 saw Paris bedecked in revolutionary, anarchistic, and situationist slogans such as "L'ennui est contre-révolutionnaire" ("Boredom is counterrevolutionary") and "Lisez moins, vivez plus" ("Read less, live more"). While not exhaustive, the graffiti gave a sense of the 'millenarian' and rebellious spirit, tempered with a good deal of verbal wit, of the strikers.

The developments of graffiti art which took place in art galleries and colleges as well as "on the street" or "underground", contributed to the resurfacing in the 1990s of a far more overtly politicized art form in the subvertising, culture jamming, or tactical media movements. These movements or styles tend to classify the artists by their relationship to their social and economic contexts, since, in most countries, graffiti art remains illegal in many forms except when using non-permanent paint. Since the 1990s with the rise of Street Art, a growing number of artists are switching to non-permanent paints and non-traditional forms of painting. 

Contemporary practitioners, accordingly, have varied and often conflicting practices. Some individuals, such as Alexander Brener, have used the medium to politicize other art forms, and have used the prison sentences enforced on them as a means of further protest.
The practices of anonymous groups and individuals also vary widely, and practitioners by no means always agree with each other's practices. For example, the anti-capitalist art group the Space Hijackers did a piece in 2004 about the contradiction between the capitalistic elements of Banksy and his use of political imagery.

Territorial graffiti marks urban neighborhoods with tags and logos to differentiate certain groups from others. These images are meant to show outsiders a stern look at whose turf is whose. The subject matter of gang-related graffiti consists of cryptic symbols and initials strictly fashioned with unique calligraphies. Gang members use graffiti to designate membership throughout the gang, to differentiate rivals and associates and, most commonly, to mark borders which are both territorial and ideological.

Graffiti has been used as a means of advertising both legally and illegally. Bronx-based TATS CRU has made a name for themselves doing legal advertising campaigns for companies such as Coca-Cola, McDonald's, Toyota, and MTV. In the UK, Covent Garden's Boxfresh used stencil images of a Zapatista revolutionary in the hopes that cross referencing would promote their store.

Smirnoff hired artists to use reverse graffiti (the use of high pressure hoses to clean dirty surfaces to leave a clean image in the surrounding dirt) to increase awareness of their product.

Graffiti may also be used as an offensive expression. This form of graffiti may be difficult to identify, as it is mostly removed by the local authority (as councils which have adopted strategies of criminalization also strive to remove graffiti quickly). Therefore, existing racist graffiti is mostly more subtle and at first sight, not easily recognized as "racist". It can then be understood only if one knows the relevant "local code" (social, historical, political, temporal, and spatial), which is seen as heteroglot and thus a 'unique set of conditions' in a cultural context.

By making the graffiti less explicit (as adapted to social and legal constraints), these drawings are less likely to be removed, but do not lose their threatening and offensive character.

Elsewhere, activists in Russia have used painted caricatures of local officials with their mouths as potholes, to show their anger about the poor state of the roads. In Manchester, England a graffitists painted obscene images around potholes, which often resulted in their being repaired within 48 hours.

In the early 1980s, the first art galleries to show graffitists to the public were Fashion Moda in the Bronx, Now Gallery and Fun Gallery, both in the East Village, Manhattan.

A 2006 exhibition at the Brooklyn Museum displayed graffiti as an art form that began in New York's outer boroughs and reached great heights in the early 1980s with the work of Crash, Lee, Daze, Keith Haring, and Jean-Michel Basquiat. It displayed 22 works by New York graffitists, including Crash, Daze, and Lady Pink. In an article about the exhibition in the magazine "Time Out", curator Charlotta Kotik said that she hoped the exhibition would cause viewers to rethink their assumptions about graffiti.

From the 1970s onwards, Burhan Dogancay photographed urban walls all over the world; these he then archived for use as sources of inspiration for his painterly works. The project today known as "Walls of the World" grew beyond even his own expectations and comprises about 30,000 individual images. It spans a period of 40 years across five continents and 114 countries. In 1982, photographs from this project comprised a one-man exhibition titled "Les murs murmurent, ils crient, ils chantent..." (The walls whisper, shout and sing...) at the Centre Georges Pompidou in Paris.

In Australia, art historians have judged some local graffiti of sufficient creative merit to rank them firmly within the arts. Oxford University Press's art history text "Australian Painting 1788–2000" concludes with a long discussion of graffiti's key place within contemporary visual culture, including the work of several Australian practitioners.

Between March and April 2009, 150 artists exhibited 300 pieces of graffiti at the Grand Palais in Paris.

Spray paint has many negative environmental effects. The paint contains toxic chemicals, and the can uses volatile hydrocarbon gases to spray the paint onto a surface. 

Volatile organic compound (VOC) leads to ground level ozone formation and most of graffiti related emissions are VOCs. A 2010 paper estimates 4,862 tons of VOCs were released in the United States in activities related to graffiti.

In China, Mao Zedong in the 1920s used revolutionary slogans and paintings in public places to galvanise the country's communist revolution.

Based on different national conditions, many people believe that China's attitude towards Graffiti is fierce, but in fact, according to Lance Crayon's film "Spray Paint Beijing: Graffiti in the Capital of China", Graffiti is accepted by many people in Beijing, China, and even the police do not make much interference. But politically and religiously sensitive graffiti is not allowed.

In Hong Kong, Tsang Tsou Choi was known as the "King of Kowloon" for his calligraphy graffiti over many years, in which he claimed ownership of the area. Now some of his work is preserved officially.

In Taiwan, the government has made some concessions to graffitists. Since 2005 they have been allowed to freely display their work along some sections of riverside retaining walls in designated "Graffiti Zones". From 2007, Taipei's department of cultural affairs also began permitting graffiti on fences around major public construction sites. Department head Yong-ping Lee (李永萍) stated, "We will promote graffiti starting with the public sector, and then later in the private sector too. It's our goal to beautify the city with graffiti". The government later helped organize a graffiti contest in Ximending, a popular shopping district. graffitists caught working outside of these designated areas still face fines up to NT$6,000 under a department of environmental protection regulation. However, Taiwanese authorities can be relatively lenient, one veteran police officer stating anonymously, "Unless someone complains about vandalism, we won't get involved. We don't go after it proactively."

In 1993, after several expensive cars in Singapore were spray-painted, the police arrested a student from the Singapore American School, Michael P. Fay, questioned him, and subsequently charged him with vandalism. Fay pleaded guilty to vandalizing a car in addition to stealing road signs. Under the 1966 Vandalism Act of Singapore, originally passed to curb the spread of communist graffiti in Singapore, the court sentenced him to four months in jail, a fine of S$3,500 (US$2,233), and a caning. "The New York Times" ran several editorials and op-eds that condemned the punishment and called on the American public to flood the Singaporean embassy with protests. Although the Singapore government received many calls for clemency, Fay's caning took place in Singapore on 5 May 1994. Fay had originally received a sentence of six strokes of the cane, but the presiding president of Singapore, Ong Teng Cheong, agreed to reduce his caning sentence to four lashes.

In South Korea, Park Jung-soo was fined two million South Korean won by the Seoul Central District Court for spray-painting a rat on posters of the G-20 Summit a few days before the event in November 2011. Park alleged that the initial in "G-20" sounds like the Korean word for "rat", but Korean government prosecutors alleged that Park was making a derogatory statement about the president of South Korea, Lee Myung-bak, the host of the summit. This case led to public outcry and debate on the lack of government tolerance and in support of freedom of expression. The court ruled that the painting, "an ominous creature like a rat" amounts to "an organized criminal activity" and upheld the fine while denying the prosecution's request for imprisonment for Park.

In Europe, community cleaning squads have responded to graffiti, in some cases with reckless abandon, as when in 1992 in France a local Scout group, attempting to remove modern graffiti, damaged two prehistoric paintings of bison in the Cave of Mayrière supérieure near the French village of Bruniquel in Tarn-et-Garonne, earning them the 1992 Ig Nobel Prize in archeology.

In September 2006, the European Parliament directed the European Commission to create urban environment policies to prevent and eliminate dirt, litter, graffiti, animal excrement, and excessive noise from domestic and vehicular music systems in European cities, along with other concerns over urban life.

The Anti-Social Behaviour Act 2003 became Britain's latest anti-graffiti legislation. In August 2004, the Keep Britain Tidy campaign issued a press release calling for zero tolerance of graffiti and supporting proposals such as issuing "on the spot" fines to graffiti offenders and banning the sale of aerosol paint to anyone under the age of 16. The press release also condemned the use of graffiti images in advertising and in music videos, arguing that real-world experience of graffiti stood far removed from its often-portrayed 'cool' or 'edgy' image.

To back the campaign, 123 MPs (including then Prime Minister Tony Blair), signed a charter which stated: "Graffiti is not art, it's crime. On behalf of my constituents, I will do all I can to rid our community of this problem."

In the UK, city councils have the power to take action against the owner of any property that has been defaced under the Anti-social Behaviour Act 2003 (as amended by the Clean Neighbourhoods and Environment Act 2005) or, in certain cases, the Highways Act. This is often used against owners of property that are complacent in allowing protective boards to be defaced so long as the property is not damaged.

In July 2008, a conspiracy charge was used to convict graffitists for the first time. After a three-month police surveillance operation, nine members of the DPM crew were convicted of conspiracy to commit criminal damage costing at least £1 million. Five of them received prison sentences, ranging from eighteen months to two years. The unprecedented scale of the investigation and the severity of the sentences rekindled public debate over whether graffiti should be considered art or crime.

Some councils, like those of Stroud and Loerrach, provide approved areas in the town where graffitists can showcase their talents, including underpasses, car parks, and walls that might otherwise prove a target for the 'spray and run.'

In Budapest, Hungary, both a city-backed movement called "I Love Budapest" and a special police division tackle the problem, including the provision of approved areas.

In an effort to reduce vandalism, many cities in Australia have designated walls or areas exclusively for use by graffitists. One early example is the "Graffiti Tunnel" located at the Camperdown Campus of the University of Sydney, which is available for use by any student at the university to tag, advertise, poster, and create "art". Advocates of this idea suggest that this discourages petty vandalism yet encourages artists to take their time and produce great art, without worry of being caught or arrested for vandalism or trespassing. Others disagree with this approach, arguing that the presence of legal graffiti walls does not demonstrably reduce illegal graffiti elsewhere. Some local government areas throughout Australia have introduced "anti-graffiti squads", who clean graffiti in the area, and such crews as BCW (Buffers Can't Win) have taken steps to keep one step ahead of local graffiti cleaners.

Many state governments have banned the sale or possession of spray paint to those under the age of 18 (age of majority). However, a number of local governments in Victoria have taken steps to recognize the cultural heritage value of some examples of graffiti, such as prominent political graffiti. Tough new graffiti laws have been introduced in Australia with fines of up to A$26,000 and two years in prison.

Melbourne is a prominent graffiti city of Australia with many of its lanes being tourist attractions, such as Hosier Lane in particular, a popular destination for photographers, wedding photography, and backdrops for corporate print advertising. The Lonely Planet travel guide cites Melbourne's street as a major attraction. All forms of graffiti, including sticker art, poster, stencil art, and wheatpasting, can be found in many places throughout the city. Prominent street art precincts include; Fitzroy, Collingwood, Northcote, Brunswick, St. Kilda, and the CBD, where stencil and sticker art is prominent. As one moves farther away from the city, mostly along suburban train lines, graffiti tags become more prominent. Many international artists such as Banksy have left their work in Melbourne and in early 2008 a perspex screen was installed to prevent a Banksy stencil art piece from being destroyed, it has survived since 2003 through the respect of local street artists avoiding posting over it, although it has recently had paint tipped over it.

In February 2008 Helen Clark, the New Zealand prime minister at that time, announced a government crackdown on tagging and other forms of graffiti vandalism, describing it as a destructive crime representing an invasion of public and private property. New legislation subsequently adopted included a ban on the sale of paint spray cans to persons under 18 and increases in maximum fines for the offence from NZ$200 to NZ$2,000 or extended community service. The issue of tagging become a widely debated one following an incident in Auckland during January 2008 in which a middle-aged property owner stabbed one of two teenage taggers to death and was subsequently convicted of manslaughter.

Graffiti databases have increased in the past decade because they allow vandalism incidents to be fully documented against an offender and help the police and prosecution charge and prosecute offenders for multiple counts of vandalism. They also provide law enforcement the ability to rapidly search for an offender's moniker or tag in a simple, effective, and comprehensive way. These systems can also help track costs of damage to city to help allocate an anti-graffiti budget. The theory is that when an offender is caught putting up graffiti, they are not just charged with one count of vandalism; they can be held accountable for all the other damage for which they are responsible. This has two main benefits for law enforcement. One, it sends a signal to the offenders that their vandalism is being tracked. Two, a city can seek restitution from offenders for all the damage that they have committed, not merely a single incident. These systems give law enforcement personnel real-time, street-level intelligence that allows them not only to focus on the worst graffiti offenders and their damage, but also to monitor potential gang violence that is associated with the graffiti.

Many restrictions of civil gang injunctions are designed to help address and protect the physical environment and limit graffiti. Provisions of gang injunctions include things such as restricting the possession of marker pens, spray paint cans, or other sharp objects capable of defacing private or public property; spray painting, or marking with marker pens, scratching, applying stickers, or otherwise applying graffiti on any public or private property, including, but not limited to the street, alley, residences, block walls, and fences, vehicles or any other real or personal property. Some injunctions contain wording that restricts damaging or vandalizing both public and private property, including but not limited to any vehicle, light fixture, door, fence, wall, gate, window, building, street sign, utility box, telephone box, tree, or power pole.

To help address many of these issues, many local jurisdictions have set up graffiti abatement hotlines, where citizens can call in and report vandalism and have it removed. San Diego's hotline receives more than 5,000 calls per year, in addition to reporting the graffiti, callers can learn more about prevention. One of the complaints about these hotlines is the response time; there is often a lag time between a property owner calling about the graffiti and its removal. The length of delay should be a consideration for any jurisdiction planning on operating a hotline. Local jurisdictions must convince the callers that their complaint of vandalism will be a priority and cleaned off right away. If the jurisdiction does not have the resources to respond to complaints in a timely manner, the value of the hotline diminishes. Crews must be able to respond to individual service calls made to the graffiti hotline as well as focus on cleanup near schools, parks, and major intersections and transit routes to have the biggest impact. Some cities offer a reward for information leading to the arrest and prosecution of suspects for tagging or graffiti related vandalism. The amount of the reward is based on the information provided, and the action taken.

When the police use search warrants in connection with a vandalism investigation they are often seeking judicial approval to look for items such as cans of spray paint and nozzles from other kinds of aerosol sprays, etching tools, or other sharp or pointed objects used to etch or scratch glass and other hard surfaces, such as permanent marking pens and markers or paint sticks; evidence of membership or affiliation with any gang or tagging crew, paraphernalia to include any reference to "(tagger's name)," and any drawings, writings, objects, or graffiti depicting taggers' names, initials, logos, monikers, slogans, or mention of tagging crew membership; any newspaper clippings relating details of or referring to any graffiti crime.






</doc>
<doc id="11986" url="https://en.wikipedia.org/wiki?curid=11986" title="Godzilla">
Godzilla

Godzilla is depicted as an enormous, destructive, prehistoric sea monster awakened and empowered by nuclear radiation. With the nuclear bombings of Hiroshima and Nagasaki and the "Lucky Dragon 5" incident still fresh in the Japanese consciousness, Godzilla was conceived as a metaphor for nuclear weapons. Others have suggested that Godzilla is a metaphor for the United States, a giant beast woken from its slumber which then takes terrible vengeance on Japan. As the film series expanded, some stories took on less serious undertones, portraying Godzilla as an antihero, or a lesser threat who defends humanity. Several post-1984 "Godzilla" films shifted the character's portrayal to themes including Japan's forgetfulness over its imperial past, natural disasters and the human condition.

Godzilla has been featured alongside many supporting characters. It has faced human opponents such as the JSDF, or other monsters, including King Ghidorah, Mechagodzilla and Gigan. Godzilla sometimes has allies, such as Rodan, Mothra and Anguirus, and offspring, such as Minilla and Godzilla Junior. Godzilla has also fought characters from other franchises in crossover media, such as the RKO Pictures/Universal Studios movie monster King Kong, as well as various Marvel Comics characters, including S.H.I.E.L.D., the Fantastic Four and the Avengers.
 is a portmanteau of the Japanese words: and , owing to the fact that in one planning stage, Godzilla was described as "a cross between a gorilla and a whale", due to its size, power and aquatic origin. One popular story is that "Gojira" was actually the nickname of a corpulent stagehand at Toho Studio. Kimi Honda, the widow of the director, dismissed this in a 1998 BBC documentary devoted to Godzilla, "The backstage boys at Toho loved to joke around with tall stories".

Godzilla's name was written in ateji as , where the kanji are used for phonetic value and not for meaning. The Japanese pronunciation of the name is ; the Anglicized form is , with the first syllable pronounced like the word "god" and the rest rhyming with "gorilla". In the Hepburn romanization system, Godzilla's name is rendered as "Gojira", whereas in the Kunrei romanization system it is rendered as "Gozira".

During the development of the American version of "Godzilla Raids Again" (1955), Godzilla's name was changed to "Gigantis", a move initiated by producer Paul Schreibman, who wanted to create a character distinct from Godzilla.

Within the context of the Japanese films, Godzilla's exact origins vary, but it is generally depicted as an enormous, violent, prehistoric sea monster awakened and empowered by nuclear radiation. Although the specific details of Godzilla's appearance have varied slightly over the years, the overall impression has remained consistent. Inspired by the fictional "Rhedosaurus" created by animator Ray Harryhausen for the film "The Beast from 20,000 Fathoms", Godzilla's character design was conceived as that of an amphibious reptilian monster based around the loose concept of a dinosaur with an erect standing posture, scaly skin, an anthropomorphic torso with muscular arms, lobed bony plates along its back and tail, and a furrowed brow.

Art director Akira Watanabe combined attributes of a "Tyrannosaurus", an "Iguanodon", a "Stegosaurus" and an alligator to form a sort of blended chimera, inspired by illustrations from an issue of "Life" magazine. To emphasise the monster's relationship with the atomic bomb, its skin texture was inspired by the keloid scars seen on survivors in Hiroshima. The basic design has a reptilian visage, a robust build, an upright posture, a long tail and three rows of serrated plates along the back. In the original film, the plates were added for purely aesthetic purposes, in order to further differentiate Godzilla from any other living or extinct creature. Godzilla is sometimes depicted as green in comics, cartoons and movie posters, but the costumes used in the movies were usually painted charcoal grey with bone-white dorsal plates up until the film "Godzilla 2000: Millennium".

In the original Japanese films, Godzilla and all the other monsters are referred to with gender-neutral pronouns equivalent to "it", while in the English dubbed versions, Godzilla is explicitly described as a male, such as in the title of "Godzilla, King of the Monsters!". In the 1998 film "Godzilla", the monster is referred to as a male and is depicted laying eggs through parthenogenesis. In the Legendary "Godzilla" films, Godzilla is referred to as a male.

Godzilla's allegiance and motivations have changed from film to film to suit the needs of the story. Although Godzilla does not like humans, it will fight alongside humanity against common threats. However, it makes no special effort to protect human life or property and will turn against its human allies on a whim. It is not motivated to attack by predatory instinct: it does not eat people and instead sustains itself on nuclear radiation and an omnivorous diet. When inquired if Godzilla was "good or bad", producer Shogo Tomiyama likened it to a Shinto "God of Destruction" which lacks moral agency and cannot be held to human standards of good and evil. "He totally destroys everything and then there is a rebirth. Something new and fresh can begin."

Godzilla's signature weapon is its "atomic heat beam" (also known as "atomic breath"), nuclear energy that it generates inside of its body, uses electromagnetic force to concentrate it into a laser-like high velocity projectile and unleashes from its jaws in the form of a blue or red radioactive beam. Toho's special effects department has used various techniques to render the beam, from physical gas-powered flames to hand-drawn or computer-generated fire. Godzilla is shown to possess immense physical strength and muscularity. Haruo Nakajima, the actor who played Godzilla in the original films, was a black belt in judo and used his expertise to choreograph the battle sequences.

Godzilla is amphibious: it has a preference for traversing Earth's hydrosphere when in hibernation or migration, can breathe underwater and is described in the original film by the character Dr. Yamane as a transitional form between a marine and a terrestrial reptile. Godzilla is shown to have great vitality: it is immune to conventional weaponry thanks to its rugged hide and ability to regenerate, and as a result of surviving a nuclear explosion, it cannot be destroyed by anything less powerful. It is an electromagnetic pulse-producing organ in its body which generates an asymmetrical permeable shield making it impervious to all damage except for a short period when the organ recycles.

Various non-canonical films, television shows, comics and games have depicted Godzilla with additional powers, such as an atomic pulse, magnetism, precognition, fireballs, an electric bite, superhuman speed, laser beams emitted from its eyes and even flight.

Godzilla has a distinctive disyllabic roar (transcribed in several comics as "Skreeeonk!"), which was created by composer Akira Ifukube, who produced the sound by rubbing a pine-tar-resin-coated glove along the string of a contrabass and then slowing down the playback. In the American version of "Godzilla Raids Again" (1955) titled "Gigantis the Fire Monster", Godzilla's roar was mostly substituted with that of the monster Anguirus. From "The Return of Godzilla" (1984) to "Godzilla vs. King Ghidorah" (1991), Godzilla was given a deeper and more threatening-sounding roar than in previous films, though this change was reverted from "Godzilla vs. Mothra" (1992) onwards. For the 2014 American film, sound editors Ethan Van der Ryn and Erik Aadahl refused to disclose the source of the sounds used for their Godzilla's roar. Aadahl described the two syllables of the roar as representing two different emotional reactions, with the first expressing fury and the second conveying the character's soul.

Godzilla's size is inconsistent, changing from film to film, and even from scene to scene, for the sake of artistic license. The miniature sets and costumes were typically built at a – scale and filmed at 240 frames per second to create the illusion of great size. In the original 1954 film, Godzilla was scaled to be tall. This was done so Godzilla could just peer over the largest buildings in Tokyo at the time. In the 1956 American version, Godzilla is estimated to be tall, because producer Joseph E. Levine felt that 50 m did not sound "powerful enough".

As the series progressed Toho would rescale the character, eventually making Godzilla as tall as . This was done so that it would not be dwarfed by the newer, bigger buildings in Tokyo's skyline, such as the Tokyo Metropolitan Government Building which Godzilla destroyed in the film "Godzilla vs. King Ghidorah" (1991). Supplementary information, such as character profiles, would also depict Godzilla as weighing between .

In the American film "Godzilla" (2014) from Legendary Pictures, Godzilla was scaled to be and weighing , making it the largest film version at that time. Director Gareth Edwards wanted Godzilla "to be so big as to be seen from anywhere in the city, but not too big that he couldn't be obscured". For "Shin Godzilla" (2016), Godzilla was made even taller than the Legendary version, at . In "" (2017), Godzilla's height was increased further still to , the tallest height for the character to date. In "" (2019), Godzilla's height was increased to from the 2014 incarnation.

Godzilla's appearance has traditionally been portrayed in the films by an actor wearing a latex costume, though the character has also been rendered in animatronic, stop-motion and computer-generated form. Taking inspiration from "King Kong", special effects artist Eiji Tsuburaya had initially wanted Godzilla to be portrayed via stop-motion, but prohibitive deadlines and a lack of experienced animators in Japan at the time made suitmation more practical.

The first suit consisted of a body cavity made of thin wires and bamboo wrapped in chicken wire for support and covered in fabric and cushions, which were then coated in latex. The first suit was held together by small hooks on the back, though subsequent Godzilla suits incorporated a zipper. Its weight was in excess of . Prior to 1984, most Godzilla suits were made from scratch, thus resulting in slight design changes in each film appearance. The most notable changes during the 1960s-70s were the reduction in Godzilla's number of toes and the removal of the character's external ears and prominent fangs, features which would later be reincorporated in the Godzilla designs from "The Return of Godzilla" (1984) onward. The most consistent Godzilla design was maintained from "Godzilla vs. Biollante" (1989) to "Godzilla vs. Destoroyah" (1995), when the suit was given a cat-like face and double rows of teeth.

Several suit actors had difficulties in performing as Godzilla, due to the suits' weight, lack of ventilation and diminished visibility. Kenpachiro Satsuma in particular, who portrayed Godzilla from 1984 to 1995, described how the Godzilla suits he wore were even heavier and hotter than their predecessors because of the incorporation of animatronics. Satsuma himself suffered numerous medical issues during his tenure, including oxygen deprivation, near-drowning, concussions, electric shocks and lacerations to the legs from the suits' steel wire reinforcements wearing through the rubber padding.

The ventilation problem was partially solved in the suit used in 1994's "Godzilla vs. SpaceGodzilla", which was the first to include an air duct, which allowed suit actors to last longer during performances. In "The Return of Godzilla" (1984), some scenes made use of a 16-foot high robotic Godzilla (dubbed the "Cybot Godzilla") for use in close-up shots of the creature's head. The Cybot Godzilla consisted of a hydraulically-powered mechanical endoskeleton covered in urethane skin containing 3,000 computer operated parts which permitted it to tilt its head and move its lips and arms.

In "Godzilla" (1998), special effects artist Patrick Tatopoulos was instructed to redesign Godzilla as an incredibly fast runner. At one point, it was planned to use motion capture from a human to create the movements of the computer-generated Godzilla, but it was said to have ended up looking too much like a man in a suit. Tatopoulos subsequently reimagined the creature as a lean, digitigrade bipedal, iguana-like creature that stood with its back and tail parallel to the ground, rendered via CGI.

Several scenes had the monster portrayed by stuntmen in suits. The suits were similar to those used in the Toho films, with the actors' heads being located in the monster's neck region, and the facial movements controlled via animatronics. However, because of the creature's horizontal posture, the stuntmen had to wear metal leg extenders, which allowed them to stand off the ground with their feet bent forward. The film's special effects crew also built a scale animatronic Godzilla for close-up scenes, whose size outmatched that of Stan Winston's "T. rex" in "Jurassic Park". Kurt Carley performed the suitmation sequences for the adult Godzilla.

In "Godzilla" (2014), the character was portrayed entirely via CGI. Godzilla's design in the reboot was intended to stay true to that of the original series, though the film's special effects team strove to make the monster "more dynamic than a guy in a big rubber suit." To create a CG version of Godzilla, the Moving Picture Company (MPC) studied various animals such as bears, Komodo dragons, lizards, lions and wolves which helped the visual effects artists visualize Godzilla's body structure like that of its underlying bone, fat and muscle structure as well as the thickness and texture of its scales. Motion capture was also used for some of Godzilla's movements. T.J. Storm provided the performance capture for Godzilla by wearing sensors in front of a green screen. Storm reprised the role of Godzilla in "", portraying the character through performance capture. In "Shin Godzilla", a majority of the character was portrayed via CGI, with Mansai Nomura portraying Godzilla through motion capture.

Godzilla is one of the most recognizable symbols of Japanese popular culture worldwide, and remains an important facet of Japanese films, embodying the "kaiju" subset of the "tokusatsu" genre. Godzilla's vaguely humanoid appearance and strained, lumbering movements endeared it to Japanese audiences, who could relate to Godzilla as a sympathetic character, despite its wrathful nature. Audiences respond positively to the character because it acts out of rage and self-preservation and shows where science and technology can go wrong.

In 1967, the Keukdong Entertainment Company of South Korea, with production assistance from Toei Company, produced "Yongary, Monster from the Deep", a reptilian monster who invades South Korea to consume oil. The film and character has often been branded as an imitation of Godzilla.

Godzilla has been considered a filmographic metaphor for the United States, as well as an allegory of nuclear weapons in general. The earlier "Godzilla" films, especially the original, portrayed Godzilla as a frightening nuclear-spawned monster. Godzilla represented the fears that many Japanese held about the atomic bombings of Hiroshima and Nagasaki and the possibility of recurrence.

As the series progressed, so did Godzilla, changing into a less destructive and more heroic character. "Ghidorah" (1964) was the turning point in Godzilla's transformation from villain to hero, by pitting him against a greater threat to humanity, King Ghidorah. Godzilla has since been viewed as an anti-hero. Roger Ebert cites Godzilla as a notable example of a villain-turned-hero, along with King Kong, Jaws ("James Bond"), the Terminator, and "Rambo".

Godzilla is considered "the original radioactive superhero" due to his accidental radioactive origin story predating Spider-Man (1962 debut), though Godzilla did not become a hero until "Ghidorah" in 1964. By the 1970s, Godzilla came to be viewed as a superhero, with the magazine "King of the Monsters" in 1977 describing Godzilla as "Superhero of the '70s." Godzilla had surpassed Superman and Batman to become "the most universally popular superhero of 1977" according to Donald F. Glut. Godzilla was also voted the most popular movie monster in "The Monster Times" poll in 1973, beating Count Dracula, King Kong, the Wolf Man, the Mummy, the Creature From the Black Lagoon, and the Frankenstein Monster.

In 1996, Godzilla received the MTV Lifetime Achievement Award, as well as being given a star on the Hollywood Walk of Fame in 2004 to celebrate the premiere of the character's 50th anniversary film, "". Godzilla's pop-cultural impact has led to the creation of numerous parodies and tributes, as seen in media such as "Bambi Meets Godzilla", which was ranked as one of the "50 greatest cartoons", two episodes of "Mystery Science Theater 3000" and the song "Godzilla" by Blue Öyster Cult. Godzilla has also been used in advertisements, such as in a commercial for Nike, where Godzilla lost an oversized one-on-one game of basketball to a giant version of NBA player Charles Barkley. The commercial was subsequently adapted into a comic book illustrated by Jeff Butler. Godzilla has also appeared in a commercial for Snickers candy bars, which served as an indirect promo for the 2014 movie. Godzilla's success inspired the creation of numerous other monster characters, such as Gamera, Reptilicus of Denmark, Yonggary of South Korea, Pulgasari of North Korea, Gorgo of the United Kingdom and the Cloverfield monster of the United States.

Godzilla's fame and saurian appearance has influenced the scientific community. "Gojirasaurus" is a dubious genus of coelophysid dinosaur, named by paleontologist and admitted Godzilla fan Kenneth Carpenter. "Dakosaurus" is an extinct marine crocodile of the Jurassic Period, which researchers informally nicknamed "Godzilla". Paleontologists have written tongue-in-cheek speculative articles about Godzilla's biology, with Ken Carpenter tentatively classifying it as a ceratosaur based on its skull shape, four-fingered hands and dorsal scutes, and paleontologist Darren Naish expressing skepticism while commenting on Godzilla's unusual morphology.

Godzilla's ubiquity in pop-culture has led to the mistaken assumption that the character is in the public domain, resulting in litigation by Toho to protect their corporate asset from becoming a generic trademark. In April 2008, Subway depicted a giant monster in a commercial for their Five Dollar Footlong sandwich promotion. Toho filed a lawsuit against Subway for using the character without permission, demanding $150,000 in compensation. In February 2011, Toho sued Honda for depicting a fire-breathing monster in a commercial for the Honda Odyssey. The monster was never mentioned by name, being seen briefly on a video screen inside the minivan. The Sea Shepherd Conservation Society christened a vessel the "MV Gojira". Its purpose is to target and harass Japanese whalers in defense of whales in the Southern Ocean Whale Sanctuary. The "MV Gojira" was renamed the in May 2011, due to legal pressure from Toho. Gojira is the name of a French death metal band, formerly known as Godzilla; legal problems forced the band to change their name. In May 2015, Toho launched a lawsuit against Voltage Pictures over a planned picture starring Anne Hathaway. Promotional material released at the Cannes Film Festival used images of Godzilla.

Steven Spielberg cited "Godzilla" as an inspiration for "Jurassic Park" (1993), specifically "Godzilla, King of the Monsters!" (1956), which he grew up watching. Spielberg described "Godzilla" as "the most masterful of all the dinosaur movies because it made you believe it was really happening." "Godzilla" also influenced the Spielberg film "Jaws" (1975). "Godzilla" has also been cited as an inspiration by filmmakers Martin Scorsese and Tim Burton.

The main-belt asteroid 101781 Gojira, discovered by American astronomer Roy Tucker at the Goodricke-Pigott Observatory in 1999, was named in honor of the creature. The official naming citation was published by the Minor Planet Center on 11 July 2018 ().

To encourage tourism in April 2015 the central Shinjuku ward of Tokyo named Godzilla an official cultural ambassador. During an unveiling of a giant Godzilla bust at Toho headquarters, Shinjuku mayor Kenichi Yoshizumi stated "Godzilla is a character that is the pride of Japan." The mayor extended a residency certificate to an actor in a rubber suit representing Godzilla, but as the suit's hands were not designed for grasping, it was accepted on Godzilla's behalf by a Toho executive. Reporters noted that Shinjuku ward has been flattened by Godzilla in three Toho movies.





</doc>
<doc id="11988" url="https://en.wikipedia.org/wiki?curid=11988" title="King Kong vs. Godzilla">
King Kong vs. Godzilla

The project began with a story outline that featured King Kong battling a giant Frankenstein Monster, written by Willis H. O'Brien. O'Brien handed the outline to producer John Beck for development. Behind O'Brien's back and without his knowledge, Beck would eventually take the project to Toho to produce the film, replacing the giant Frankenstein Monster with Godzilla and scrapping O'Brien's original story.

"King Kong vs. Godzilla" was released theatrically in Japan on August 11, 1962. The film remains the most attended "Godzilla" film in Japan to date, and is credited with encouraging Toho to prioritize the continuation of the "Godzilla" series after seven years of dormancy. A heavily edited version was released by Universal International Inc. theatrically in the United States on June 26, 1963.

Mr. Tako, head of Pacific Pharmaceuticals, is frustrated with the television shows his company is sponsoring and wants something to boost his ratings. When a doctor tells Tako about a giant monster he discovered on the small Faro Island, Tako believes that it would be a brilliant idea to use the monster to gain publicity. Tako immediately sends two men, Osamu Sakurai and Kinsaburo Furue, to find and bring back the monster. Meanwhile, the American nuclear submarine "Seahawk" gets caught in an iceberg. The iceberg collapses, unleashing Godzilla (who, in the Japanese version, had been trapped within it since 1955), who then destroys the submarine and a nearby Arctic military base.

On Faro Island, a gigantic octopus crawls ashore and attacks the native village. The mysterious Faro monster, revealed to be King Kong, arrives and defeats the octopus. Kong then drinks some red berry juice that immediately puts him to sleep. Sakurai and Furue place Kong on a large raft and begin to transport him back to Japan. Mr. Tako arrives on the ship transporting Kong, but a JSDF ship stops them and orders them to return Kong to Faro Island. Meanwhile, Godzilla arrives in Japan and begins terrorizing the countryside. Kong wakes up and breaks free from the raft. Reaching the mainland, Kong confronts Godzilla and proceeds to throw giant rocks at Godzilla. Godzilla is not fazed by King Kong's rock attack and uses its atomic heat ray to burn him. Kong retreats after realizing that he is not yet ready to take on Godzilla and his atomic heat ray.

The JSDF digs a large pit laden with explosives and poison gas and lures Godzilla into it, but Godzilla is unharmed. They next string up a barrier of power lines around the city filled with 1,000,000 volts of electricity (50,000 volts were tried in the first film, but failed to turn the monster back), which proves effective against Godzilla. Kong then approaches Tokyo and tears through the power lines, feeding off the electricity, which seems to make him stronger. Kong then enters Tokyo and captures Fumiko, Sakurai's sister. The JSDF launches capsules full of the Faro Island berry juice in gas form, which puts Kong to sleep, and are able to rescue Fumiko. The JSDF then decides to transport Kong via balloons to Godzilla, in hopes that they will kill each other.

The next morning, Kong is dropped next to Godzilla at the summit of Mount Fuji and the two engage in a final battle. Godzilla initially has the advantage due to his atomic heat ray and nearly kills Kong. After knocking Kong out with a devastating dropkick and tail blows to Kong's head, Godzilla begins burning the foliage around Kong, trying to cremate him. Suddenly, a bolt of lightning from thunder clouds strike King Kong, reviving him and charging him up. The monsters resume their fight, making their way towards the coastline and destroying Atami Castle before falling off a cliff together into the Pacific Ocean. After an underwater battle, only Kong resurfaces from the water, and he begins to swim back toward his island home. There is no sign of Godzilla, but the JSDF speculates that it is possible he survived.

The film had its roots in an earlier concept for a new "King Kong" feature developed by Willis O'Brien, animator of the original stop-motion Kong. Around 1960, O'Brien came up with a proposed treatment, King Kong Meets Frankenstein, where Kong would fight against a giant Frankenstein Monster in San Francisco. O'Brien took the project (which consisted of some concept art and a screenplay treatment) to RKO to secure permission to use the King Kong character. During this time, the story was renamed King Kong vs. the Ginko when it was believed that Universal had the rights to the Frankenstein name (it actually only had the rights to the monster's makeup design by Jack Pierce). O'Brien was introduced to producer John Beck, who promised to find a studio to make the film (at this point in time, RKO was no longer a production company). Beck took the story treatment and had George Worthing Yates flesh it out into a screenplay. The story was slightly altered and the title changed to Prometheus vs. King Kong, returning the name to the original Frankenstein concept ("The Modern Prometheus" was the alternate title of the original novel). Unfortunately, the cost of stop-motion animation discouraged potential studios from putting the film into production. After shopping the script around overseas, Beck eventually attracted the interest of the Japanese studio Toho, which had long wanted to make a "King Kong" film. After purchasing the script, they decided to replace the giant Frankenstein Monster with Godzilla to be King Kong's opponent and would have Shinichi Sekizawa rewrite Yates' script. The studio thought that it would be the perfect way to celebrate its 30th year in production. It was one of five big banner releases for the company to celebrate the anniversary alongside "Sanjuro", "", "Lonely Lane", and "Born in Sin". John Beck's dealings with Willis O'Brien's project were done behind his back, and O'Brien was never credited for his idea. Merian C. Cooper was bitterly opposed to the project, stating in a letter addressed to his friend Douglas Burden, "I was indignant when some Japanese company made a belittling thing, to a creative mind, called "King Kong vs. Godzilla". I believe they even stooped so low as to use a man in a gorilla suit, which I have spoken out against so often in the early days of "King Kong"". In 1963, he filed a lawsuit to enjoin distribution of the movie against John Beck, as well as Toho and Universal (the film's U.S. copyright holder) claiming that he outright owned the King Kong character, but the lawsuit never went through, as it turned out he was not Kong's sole legal owner as he had previously believed.

Ishiro Honda wanted the theme of the movie to be a satire of the television industry in Japan. In April 1962, TV networks and their various sponsors started producing outrageous programming and publicity stunts to grab audiences' attention after two elderly viewers reportedly died at home while watching a violent wrestling match on TV. The various rating wars between the networks and banal programming that followed this event caused widespread debate over how TV would effect Japanese culture with Soichi Oya stating TV was creating "a nation of 100 million idiots". Honda stated "People were making a big deal out of ratings, but my own view of TV shows was that they did not take the viewer seriously, that they took the audience for granted...so I decided to show that through my movie" and "the reason I showed the monster battle through the prism of a ratings war was to depict the reality of the times". Honda addressed this by having a pharmaceutical company sponsor a TV show and going to extremes for a publicity stunt for ratings by capturing a giant monster stating "All a medicine company would have to do is just produce good medicines you know? But the company doesn't think that way. They think they will get ahead of their competitors if they use a monster to promote their product.". Honda would work with screenwriter Shinichi Sekizawa on developing the story stating that "Back then Sekizawa was working on pop songs and TV shows so he really had a clear insight into television".

Special effects director Eiji Tsuburaya was planning on working on other projects at this point in time such as a new version of a fairy tale film script called "Kaguyahime" ("Princess Kaguya"), but he postponed those to work on this project with Toho instead since he was such a huge fan of King Kong. He stated in an early 1960s interview with the Mainichi Newspaper, "But my movie company has produced a very interesting script that combined King Kong and Godzilla, so I couldn't help working on this instead of my other fantasy films. The script is special to me; it makes me emotional because it was "King Kong" that got me interested in the world of special photographic techniques when I saw it in 1933."

Eiji Tsuburaya had a stated intention to move the Godzilla series in a lighter direction. This approach was not favoured by most of the effects crew, who "couldn't believe" some of the things Tsuburaya asked them to do, such as Kong and Godzilla volleying a giant boulder back and forth. But Tsuburaya wanted to appeal to children's sensibilities and broaden the genre's audience. This approach was favoured by Toho and to this end, "King Kong vs. Godzilla" has a much lighter tone than the previous two "Godzilla" films and contains a great deal of humor within the action sequences. With the exception of the next film, "Mothra vs. Godzilla", this film began the trend to portray Godzilla and the monsters with more and more anthropomorphism as the series progressed, to appeal more to younger children. Ishirō Honda was not a fan of the dumbing down of the monsters. Years later, Honda stated in an interview. "I don't think a monster should ever be a comical character." "The public is more entertained when the great King Kong strikes fear into the hearts of the little characters." The decision was also taken to shoot the film in a (2.35:1) scope ratio (Tohoscope) and to film in color (Eastman Color), marking both monsters' first widescreen and color portrayals. Additionally, the theatrical release was accompanied by both a true 4.0 stereophonic soundtrack, and a regular monaural mix.

Toho had planned to shoot this film on location in Sri Lanka, but had to forgo that (and scale back on production costs) because it ended up paying RKO roughly ¥80 million ($220,000) for the rights to the King Kong character. The bulk of the film was shot on Izu Ōshima (an island near Japan) instead. The movie's production budget came out to ().

Suit actors Shoichi Hirose (King Kong) and Haruo Nakajima (Godzilla) were given mostly free rein by Eiji Tsuburaya to choreograph their own moves. The men would rehearse for hours and would base their moves on that from professional wrestling (a sport that was growing in popularity in Japan), in particular the movies of Toyonobori.

During pre-production, Eiji Tsuburaya had toyed with the idea of using Willis O'Brien's stop-motion technique instead of the suitmation process used in the first two "Godzilla" films, but budgetary concerns prevented him from using the process, and the more cost efficient suitmation was used instead. However, some brief stop motion was used in a couple of quick sequences. One of these sequences was animated by Koichi Takano, who was a member of Eiji Tsuburaya's crew.

A brand new Godzilla suit was designed for this film and some slight alterations were done to its overall appearance. These alterations included the removal of its tiny ears, three toes on each foot rather than four, enlarged central dorsal fins and a bulkier body. These new features gave Godzilla a more reptilian/dinosaurian appearance. Outside of the suit, a meter high model and a small puppet were also built. Another puppet (from the waist up) was also designed that had a nozzle in the mouth to spray out liquid mist simulating Godzilla's atomic breath. However the shots in the film where this prop was employed (far away shots of Godzilla breathing its atomic breath during its attack on the Arctic Military base) were ultimately cut from the film. These cut scenes can be seen in the Japanese theatrical trailer. Finally, a separate prop of Godzilla's tail was also built for close up practical shots when its tail would be used (such as the scene where Godzilla trips Kong with its tail). The tail prop would be swung offscreen by a stage hand.

Sadamasa Arikawa (who worked with Eiji Tsuburaya) said that the sculptors had a hard time coming up with a King Kong suit that appeased Tsuburaya. The first suit was rejected for being too fat with long legs giving Kong what the crew considered an almost cute look. A few other designs were done before Tsuburaya would approve the final look that was ultimately used in the film. The suit's body design was a team effort by brothers Koei Yagi and Kanji Yagi and was covered with expensive yak hair, which Eizo Kaimai hand-dyed brown. Because RKO instructed that the face must be different from the original's design, sculptor Teizo Toshimitsu based Kong's face on the Japanese macaque rather than a gorilla, and designed two separate masks. As well, two separate pairs of arms were also created. One pair were extended arms that were operated by poles inside the suit to better give Kong a gorilla-like illusion, while the other pair were at normal arms length and featured gloves that were used for scenes that required Kong to grab items and wrestle with Godzilla. Suit actor Hirose had to be sewn into the suit in order to hide the zipper. This would force him to be trapped inside the suit for large amounts of time and would cause him much physical discomfort. In the scene where Kong drinks the berry juice and falls asleep, he was trapped in the suit for three hours. Hirose stated in an interview "Sweat came pouring out like a flood and it got into my eyes too. When I came out, I was pale all over". Besides the suit with the two separate arm attachments, a meter-high model and a puppet of Kong (used for closeups) were also built. As well, a huge prop of Kong's hand was built for the scene where he grabs Mie Hama (Fumiko) and carries her off.

For the attack of the giant octopus, four live octopuses were used. They were forced to move among the miniature huts by having hot air blown onto them. After the filming of that scene was finished, three of the four octopuses were released. The fourth became special effects director Eiji Tsuburaya's dinner. These sequences were filmed on a miniature set outdoors on the Miura Coast. Along with the live animals, two rubber octopus props were built, with the larger one being covered with plastic wrap to simulate mucous. Some stop-motion tentacles were also created for the scene where the octopus grabs a native and tosses him. These sequences were shot indoors at Toho Studios.

Since King Kong was seen as the bigger draw and since Godzilla was still a villain at this point in the series, the decision was made to not only give King Kong top billing but also to present him as the winner of the climactic fight. While the ending of the film does look somewhat ambiguous, Toho confirmed that King Kong was indeed the winner in their 1962–63 English-language film program "Toho Films Vol. 8", which states in the film's plot synopsis, "A spectacular duel is arranged on the summit of Mt. Fuji and King Kong is victorious. But after he has won..."

When John Beck sold the "Prometheus vs. King Kong" script to Toho (which became "King Kong vs. Godzilla"), he was given exclusive rights to produce a version of the film for release in non-Asian territories. He was able to line up a couple of potential distributors in Warner Bros. and Universal-International even before the film began production. Beck, accompanied by two Warner Bros. representatives, attended at least two private screenings of the film on the Toho Studios lot before it was released in Japan.

John Beck enlisted the help of two Hollywood writers, Paul Mason and Bruce Howard, to write a new screenplay. After discussions with Beck, the two wrote the American version and worked with editor Peter Zinner to remove scenes, recut others, and change the sequence of several events. To give the film more of an American feel, Mason and Howard decided to insert new footage that would convey the impression that the film was actually a newscast. The television actor Michael Keith played newscaster Eric Carter, a United Nations reporter who spends much of the time commenting on the action from the U.N. Headquarters via an International Communications Satellite (ICS) broadcast. Harry Holcombe was cast as Dr. Arnold Johnson, the head of the Museum of Natural History in New York City, who tries to explain Godzilla's origin and his and Kong's motivations. The new footage, directed by Thomas Montgomery, was shot in three days.

Beck and his crew were able to obtain library music from a host of older films (music tracks that had been composed by Henry Mancini, Hans J. Salter, and even a track from Heinz Roemheld). These films include "Creature from the Black Lagoon", "Bend of the River", "Untamed Frontier", "The Golden Horde", "Frankenstein Meets the Wolf Man", "Man Made Monster", "Thunder on the Hill", "While the City Sleeps", "Against All Flags", "The Monster That Challenged the World", "The Deerslayer" and music from the TV series "Wichita Town". Cues from these scores were used to almost completely replace the original Japanese score by Akira Ifukube and give the film a more Western sound. They also obtained stock footage from the film "The Mysterians" from RKO (the film's U.S. copyright holder at the time) which was used to not only represent the ICS, but which was also utilized during the film's climax. Stock footage of a massive earthquake from "The Mysterians" was employed to make the earthquake caused by Kong and Godzilla's plummet into the ocean much more violent than the tame tremor seen in the Japanese version. This added footage features massive tidal waves, flooded valleys, and the ground splitting open swallowing up various huts.

Beck spent roughly $15,500 making his English version and sold the film to Universal-International for roughly $200,000 on April 29, 1963. The film opened in New York on June 26 of that year.

Starting in 1963, Toho's international sales booklets began advertising an English dub of "King Kong vs. Godzilla" alongside Toho-commissioned, unedited international dubs of movies such as "Giant Monster Varan" and "The Last War". By association, it is thought that this "King Kong vs. Godzilla" dub is an unedited English-language international version not known to have been released on home video.

In Japan, the film first released on August 11, 1962. The film was re-released twice as part of the "Champion Matsuri" (東宝チャンピオンまつり), a film festival that ran from 1969 through 1978 that featured numerous films packaged together and aimed at children, first in 1970, and then again in 1977, to coincide with the Japanese release of the 1976 version of "King Kong".

After its theatrical re-releases, the film was screened two more times at specialty festivals. In 1979, to celebrate Godzilla's 25th anniversary, the film was reissued as part of a triple bill festival known as "The Godzilla Movie Collection" ("Gojira Eiga Zenshu"). It played alongside "Invasion of Astro-Monster" and "Godzilla vs. Mechagodzilla". This release is known among fans for its exciting and dynamic movie poster featuring all the main kaiju from these three films engaged in battle. Then in 1983, the film was screened as part of "The Godzilla Resurrection Festival" ("Gojira no Fukkatsu"). This large festival featured 10 Godzilla/kaiju films in all ("Godzilla", "King Kong vs. Godzilla", "Mothra vs. Godzilla", "Ghidorah, the Three-Headed Monster", "Invasion of Astro-Monster", "Godzilla vs. Mechagodzilla", "Rodan", "Mothra", "Atragon", and "King Kong Escapes").

In North America, "King Kong vs. Godzilla" premiered in New York City on June 26, 1963. The film was also released in many international markets. In Germany, it was known as "Die Rückkehr des King Kong" ("The Return of King Kong") and in Italy as "Il trionfo di King Kong" ("The triumph of King Kong"). In France, it was released as "King Kong contre Godzilla" (analogous to the English-language title) in 1976.

The Japanese version of this film was released numerous times through the years by Toho on different home video formats. The film was first released on VHS in 1985 and again in 1991. It was released on LaserDisc in 1986 and 1991, and then again in 1992 in its truncated 74-minute form as part of a laserdisc box set called the "Godzilla Toho Champion Matsuri". Toho then released the film on DVD in 2001. They released it again in 2005 as part of the "Godzilla Final Box" DVD set, and again in 2010 as part of the Toho Tokusatsu DVD Collection. This release was volume #8 of the series and came packaged with a collectible magazine that featured stills, behind-the-scenes photos, interviews, and more. In the summer of 2014, the film was released for the first time on Blu-ray as part of the company releasing the entire series on the Blu-ray format for Godzilla's 60th anniversary.

The American version was released on VHS by GoodTimes Entertainment (which acquired the license of some of Universal's film catalogue) in 1987, and then on DVD to commemorate the 35th anniversary of the film's U.S release in 1998. Both of these releases were full frame. Universal Pictures released the English-language version of the film on DVD in widescreen as part of a two-pack bundle with "King Kong Escapes" in 2005, and then on its own as an individual release on September 15, 2009. They then re-released the film on Blu-ray on April 1, 2014, along with "King Kong Escapes". This release sold $749,747 worth of Blu-rays. FYE released an exclusive Limited Edition Steelbook version of this Blu-ray on September 10, 2019.

In 2019, the Japanese and American versions were included in a Blu-ray box set released by the Criterion Collection, which included all 15 films from the franchise's Shōwa era.

In Japan, this film has the highest box office attendance figures of all of the "Godzilla" films to date. It sold 11.2 million tickets during its initial theatrical run, accumulating in distribution rental earnings. The film was the fourth highest-grossing film in Japan that year, behind "The Great Wall (Shin no shikōtei)", "Sanjuro", and "" and was Toho's second biggest moneymaker. At an average 1962 Japanese ticket price, ticket sales were equivalent to estimated gross receipts of approximately ().

Including re-releases, the film accumulated a lifetime figure of 12.6 million tickets sold in Japan, with distribution rental earnings of . The 1970 re-release sold 870,000 tickets, equivalent to estimated gross receipts of approximately (). The 1977 re-release sold 480,000 tickets, equivalent to estimated gross receipts of approximately (). This adds up to total estimated Japanese gross receipts of approximately ().

In the United States, the film grossed $2.7 million, accumulating a profit (via rentals) of $1.25 million. In France, where it released in 1976, the film sold 554,695 tickets, equivalent to estimated gross receipts of approximately ($1,667,650). This adds up to total estimated gross receipts of approximately worldwide.

Adjusted for inflation, Japanese ticket sales are equivalent to at an average 2014 Japanese ticket price, while 554,695 French ticket sales are equivalent to at an average 2014 French ticket price. at 1963 US ticket prices are equivalent to at an average 2020 US ticket price. This adds up to an estimated inflation-adjusted gross of approximately worldwide.

The original Japanese version of "King Kong vs. Godzilla" is infamous for being one of the most poorly-preserved "tokusatsu" films. In 1970, director Ishiro Honda prepared an edited version of the film for the Toho Champion Festival, a children's matinee program that showcased edited re-releases of older kaiju films along with cartoons and then-new kaiju films. Honda cut 24 minutes from the film's original negative and, as a result, the highest quality source for the cut footage was lost. For years, all that was thought to remain of the uncut 1962 version was a faded, heavily damaged 16mm element from which rental prints had been made. 1980s restorations for home video integrated the 16mm deleted scenes into the 35mm Champion cut, resulting in wildly inconsistent picture quality.

In 1991, Toho issued a restored laserdisc incorporating the rediscovery of a reel of 35mm trims of the deleted footage. The resultant quality was far superior to previous reconstructions, but not perfect; an abrupt cut caused by missing frames at the beginning or end of a trim is evident whenever the master switches between the Champion cut and a 35mm trim within the same shot. This laserdisc master was utilized for Toho's 2001 DVD release with few changes.

In 2014, Toho released a new restoration of the film on Blu-Ray, which utilized the 35mm edits once again, but only those available for reels 2-7 of the film were able to be located. The remainder of video for the deleted portions was sourced from the earlier Blu-Ray of the U.S. version, in addition to the previous 480i 1991 laserdisc master. On July 14, 2016, a 4K restoration of a completely 35mm sourced version of the film aired on "The Godzilla First Impact", a series of 4K broadcasts of Godzilla films on the Nihon Eiga Senmon Channel.

Due to the film's great box office success, Toho planned to produce a sequel almost immediately. The sequel was simply called "Continuation: King Kong vs. Godzilla". However, the project was ultimately cancelled.

Also due to the great box office success of this film, Toho was convinced to build a franchise around the character of Godzilla and started producing sequels on a yearly basis. The next project was to pit Godzilla against another famous movie monster icon: a giant Frankenstein Monster. In 1963, Kaoru Mabuchi (a.k.a. Takeshi Kimura) wrote a script called Frankenshutain tai Gojira. Ultimately, Toho rejected the script and the next year pitted Mothra against Godzilla instead in the 1964 film "Mothra vs. Godzilla". This began an intra-company style crossover where kaiju from other Toho kaiju films would be brought into the Godzilla series.

Toho was eager to build a series around their version of King Kong, but were refused by RKO. They worked with the character again in 1967 though, when they helped Rankin/Bass co-produce their film "King Kong Escapes" (which was loosely based on a cartoon series R/B had produced). That film, however, was not a sequel to "King Kong vs. Godzilla".

Henry G. Saperstein (whose company UPA co-produced the 1965 film "Frankenstein vs. Baragon" (a.k.a. "Frankenstein Conquers the World" in the U.S.) and the 1966 sequel film "The War of the Gargantuas" with Toho) was so impressed with the octopus sequence that he requested the creature to appear in these two productions. The giant octopus appeared in an alternate ending in "Frankenstein Conquers the World" that was intended specifically for the American market, but was ultimately never used. The creature did reappear at the beginning of the film's sequel "The War of the Gargantuas" this time being retained in the finished film.

Even though it was only featured in this one film (although it was used for a couple of brief shots in "Mothra vs. Godzilla"), this Godzilla suit was always one of the more popular designs among fans from both sides of the Pacific. It formed the basis for some early merchandise in the U.S. in the 1960s, such as a popular model kit by Aurora Plastics Corporation, and a popular board game by Ideal Toys. This game was released alongside a King Kong game in 1963 to coincide with the U.S. theatrical release of the film.

The King Kong suit from this film was redressed into the giant monkey Goro for episode 2 ("GORO and Goro") of the television show Ultra Q. Afterwards, it was reused for the water scenes (although it was given a new mask/head) for "King Kong Escapes". Scenes of the giant octopus attack were reused in black and white for episode 23 ("Fury of the South Seas") of Ultra Q.

In 1992 (to coincide with the company's 60th anniversary), Toho wanted to remake this film as Godzilla vs. King Kong as part of the Heisei series of "Godzilla" films. However, according to the late Tomoyuki Tanaka, it proved to be difficult to obtain permission to use King Kong. Next, Toho thought to make Godzilla vs. Mechani-Kong but, (according to Koichi Kawakita), it was discovered that obtaining permission even to use the "likeness" of King Kong would be difficult. Mechani-Kong was replaced by Mechagodzilla, and the project eventually evolved into "Godzilla vs. Mechagodzilla II" in 1993.

In making "", the special effects crew was instructed to watch the giant octopus scene to get reference for the Kraken.

Through the years, the film has been referenced in various songs, advertising, television shows and comic books. It was referenced in Da Lench Mob's 1992 single "Guerillas in tha Mist". It was spoofed in advertising for a Bembos burger commercial from Peru, for Ridsect Lizard Repellant, and for the board game Connect 4. It was paid homage to in comic books by DC Comics, Bongo Comics, and Disney Comics. It was even spoofed in "The Simpsons" episode "Wedding for Disaster".

In 2015, Legendary Entertainment announced plans for a "Godzilla vs. Kong" film of their own (unrelated to Toho's version), to be released on May 21, 2021.

For many years, a popular myth has persisted that in the Japanese version of this film, Godzilla emerges as the winner. The myth originated in the pages of "Spacemen" magazine, a 1960s sister magazine to the influential publication "Famous Monsters of Filmland". In an article about the film, it is incorrectly stated that there were two endings and "If you see "King Kong vs Godzilla" in Japan, Hong Kong or some Oriental sector of the world, Godzilla wins!" The article was reprinted in various issues of "Famous Monsters of Filmland" in the years following, such as in issues #51 and #114. This misinformation would be accepted as fact and persist for decades. For example, a question in the "Genus III" edition of the popular board game "Trivial Pursuit" asked, "Who wins in the Japanese version of "King Kong vs. Godzilla"?" and stated that the correct answer was "Godzilla". Various media have repeated this falsehood, including the "Los Angeles Times".

With the rise of home video, Westerners have increasingly been able to view the original version and the myth has been dispelled. The only differences between the two endings of the film are minor:


In 1993, comic book artist Arthur Adams wrote and drew a one-page story that appeared in the anthology "Urban Legends" #1, published by Dark Horse Comics, which dispels the popular misconception about the two versions of "King Kong vs. Godzilla".




</doc>
<doc id="11992" url="https://en.wikipedia.org/wiki?curid=11992" title="Ebirah, Horror of the Deep">
Ebirah, Horror of the Deep

During its development, "Ebirah, Horror of the Deep" was intended to feature King Kong, but the character was replaced by Godzilla. The film was released to theaters in Japan on December 17, 1966, and was released directly to television in the United States in 1968 under the title "Godzilla versus the Sea Monster".

After Yata is lost at sea, his brother Ryota steals a yacht with his two friends and a bank robber. However, the crew runs afoul of Ebirah, a giant lobster-like creature, and washes ashore on Letchi Island. There the Red Bamboo, a terrorist organization, manufactures heavy water for selling weapons of mass destruction; as well as a yellow liquid that keeps Ebirah at bay, presumably controlling it. The Red Bamboo has enslaved natives from nearby Infant Island to create the yellow liquid, the natives hoping that Mothra will awaken in her winged, adult form and rescue them.

In their efforts to avoid capture, Ryota and his friends, aided by Daiyo, a native girl, come across Godzilla who previously fought Ghidorah, is now sleeping within a cliffside cavern. The group devises a plan to defeat the Red Bamboo and escape the island. In the process, they awaken Godzilla using a makeshift lightning rod. Godzilla fights Ebirah, but the huge crustacean escapes. Godzilla is then attacked by a giant condor and a squadron of Red Bamboo fighter jets. Using his atomic ray, Godzilla destroys the jets and kills the giant bird.

The humans retrieve the missing Yata and free the enslaved natives as Godzilla begins to destroy the Red Bamboo's base of operations, smashing a tower that causes a countdown that will destroy the island in a nuclear explosion. Godzilla fights Ebirah and defeats it, ripping its claws off, forcing it to retreat back into the sea. The natives await for Mothra to carry them off in a large net. However, Godzilla challenges Mothra, since the monsters previously battled in 1964, when she gets to the island. Mothra manages to repel Godzilla and save her people and the human heroes. Godzilla also escapes just before the bomb detonates and destroys the island.

The film was originally written to feature King Kong rather than Godzilla. The film's working title was Operation Robinson Crusoe: King Kong vs. Ebirah, and the project was rejected by Rankin/Bass Productions before being accepted by Toho, after which King Kong's role in the film was replaced by Godzilla. Despite the fact that Eiji Tsuburaya was given directorial credit for the special effects, Sadamasa Arikawa actually directed the special effects under the supervision of Tsuburaya, who had his own company, Tsuburaya Productions, at the time. Toho had decided to set the film on an island in order to cut back on special effects costs. Arikawa has cited the film as a frustrating experience, stating, "There were major limitations on the budget from the studio. Toho couldn't have made too many demands about the budget if Mr. Tsuburaya had been in charge. The studio knew I was also doing TV work then, so they must have figured I could produce the movie cheaply."

The underwater sequences were filmed on an indoor soundstage where the Godzilla and Ebirah suits were filmed through the glass of a water-filled aquarium, with some scenes of the Godzilla suit shot separately underwater as well. Haruo Nakajima (the suit performer for Godzilla) wore a wet suit under the Godzilla suit for every scene that required him to be in the water, which took a week to complete the water scenes, Nakajima stated, "I worked overtime until about eight o'clock everyday. Even though I wore a wet suit under the costume, I got cold. But I never got sick, because I was so tense during the filming."

This is the first of two "Godzilla" films in which a Pacific island is the primary setting, rather than a location inside Japan. The second and final one is "Son of Godzilla" (1967).

"Ebirah, Horror of the Deep" was released theatrically in Japan on December 17, 1966 where it was distributed by Toho.

The American version of the film was released directly to television by Continental Distributing in 1968 under the title "Godzilla Versus the Sea Monster". The film may have received theatrical distribution in the United States as a Walter Reade, Jr. Presentation, but this has not been confirmed.

The film was released on DVD on February 8, 2005 by Sony Pictures Home Entertainment. The film was released on Blu-ray on May 6, 2014 by Kraken Releasing. In 2019, the Japanese version was included in a Blu-ray box set released by the Criterion Collection, which included all 15 films from the franchise's Shōwa era.





</doc>
<doc id="11993" url="https://en.wikipedia.org/wiki?curid=11993" title="Son of Godzilla">
Son of Godzilla

"Son of Godzilla" received a theatrical release in Japan on December 16, 1967, and was released directly to television in the United States in 1969 through the Walter Reade Organization.

A team of scientists are trying to perfect a weather-controlling system. Their efforts are hampered by the arrival of a nosy reporter and by the sudden presence of giant praying mantises. The first test of the weather control system goes awry when the remote control for a radioactive balloon is jammed by an unexplained signal coming from the center of the island. The balloon detonates prematurely, creating a radioactive storm that causes the giant mantises to grow to enormous sizes. Investigating the mantises, which are named Kamacuras (Gimantis in the English-dubbed version), the scientists find the monstrous insects digging an egg out from under a pile of earth. The egg hatches, revealing a baby Godzilla. The scientists realize that the baby's telepathic cries for help were the cause of the interference that ruined their experiment. Shortly afterwards, Godzilla arrives on the island in response to the infant's cries, demolishing the scientist's base while rushing to defend the baby. Godzilla kills two of the Kamacuras during the battle while one manages to fly away to safety, Godzilla then adopts the baby.

The baby Godzilla, named Minilla, quickly grows to about half the size of the adult Godzilla and Godzilla instructs it on the important monster skills of roaring and using its atomic ray. At first, Minilla has difficulty producing anything more than atomic smoke rings, but Godzilla discovers that stressful conditions (i.e. stomping on his tail) or motivation produces a true radioactive blast. Minilla comes to the aid of Saeko when she is attacked by a Kamacuras, but inadvertently awakens Kumonga (Spiga in the English-dubbed version), a giant spider that was sleeping in a valley. Kumonga attacks the caves where the scientists are hiding and Minilla stumbles into the fray.

Kumonga traps Minilla and the final Kamacuras with its webbing, but as Kumonga begins to feed on the deceased Kamacuras, Godzilla arrives. Godzilla saves Minilla and they work together to defeat Kumonga by using their atomic rays on the giant spider. Hoping to keep the monsters from interfering in their attempt to escape the island, the scientists finally use their perfected weather altering device on the island and the once tropical island becomes buried in snow and ice. As the scientists are saved by an American submarine, Godzilla and Minilla begin to hibernate as they wait for the island to become tropical again.

For the second "Godzilla" film in a row, Toho produced an island themed adventure with a smaller budget than most of their monster films from this time period. While the a-list crew of talent was hired to work on that year's "King Kong Escapes", (Ishirō Honda, Eiji Tsuburaya, and Akira Ifukube), the second string crew of cheaper talent was once again tapped to work on this project as they had done with "Ebirah, Horror of the Deep". This included Jun Fukuda (director), Sadamasa Arikawa (special effects), and Masaru Sato (composer). This was the first film where Arikawa was officially listed as the director of Special Effects, although he did receive some supervision from Tsuburaya when he was available.

Toho wanted to create a baby Godzilla to appeal to the "date crowd" (a genre of films that were very popular among young couples during this time period), with the idea that girls would like a "cute" baby monster. For the idea behind Minilla, Fukuda stated, "We wanted to take a new approach, so we gave Godzilla a child. We thought it would be a little strange if we gave Godzilla a daughter, so instead we gave him a son". Fukuda also wanted to portray the monsters almost as people in regards to the father-son relationship between Godzilla and Minilla, as Fukuda stated "We focused on the relationship between Godzilla and his son throughout the course of "Son of Godzilla".

Minilla was designed to incorporate features of not only a baby Godzilla but a human baby was well. "Marchan the Dwarf" was hired to play the character due to his ability to play-act and to give the character a childlike ambiance. He was also hired because of his ability to perform athletic rolls and flips inside the thick rubber suit.

The Godzilla suit built for this film was the biggest in terms of size and girth. This was done in order to give Godzilla a "maternal" appearance and to give a parent-like stature in contrast next to Minilla. Because of the size of the suit, seasoned Godzilla suit actor Haruo Nakajima was only hired to play Godzilla in two scenes because the suit was much too big for him to wear. The smaller suit he had worn for the films "Ebirah, Horror of the Deep" and "Invasion of Astro-Monster" was used for these sequences. The much larger Seji Onaka instead played Godzilla in the film, although he was replaced midway through filming by Hiroshi Sekita after he broke his fingers.

Outside of the two monster suits, various marionettes and puppets were used to portray the Island's gigantic inhabitants. The various giant preying mantises known as Kamacuras and the huge poisonous spider Kumonga. Arikawa would usually have 20 puppeteers at a time working on the various marionettes. The massive Kumonga puppet needed 2 to 3 people at a time to operate each leg.

"Son of Godzilla" was distributed theatrically in Japan by Toho on December 16, 1967. "Son of Godzilla" was never released theatrically in the United States. The film was released directly to television by Walter Reade Sterling as well as American International Pictures (AIP-TV) in some markets in 1969. The American television version was cut to 84 minutes.

In 2005, the film was released on DVD by Sony Pictures in its original uncut length with the original Japanese audio and Toho's international English dub. In 2019, the Japanese version and export English version was included in a Blu-ray box set released by the Criterion Collection, which included all 15 films from the franchise's Shōwa era.

In a contemporary review, the "Monthly Film Bulletin" declared the film to be "out of the top drawer of the Toho Company's monster file, with the special effects department achieving their best results in monster locomotion" and that the film "has the advantage of a more soundly constructed story than most of its predecessors and a delightful vein of humor that allows for a gentle parody of the genre."






</doc>
<doc id="11994" url="https://en.wikipedia.org/wiki?curid=11994" title="Destroy All Monsters">
Destroy All Monsters

In the film, humans have achieved world peace by the year 1999, and various giant monsters are confined to an area known as Monsterland. The monsters are freed from the area and are mind-controlled by aliens known as Kilaaks, who send them to attack major cities. When the monsters are freed from the Kilaaks' influence, the aliens send King Ghidorah to challenge the other monsters.

"Destroy All Monsters" was released theatrically in Japan on August 1, 1968. The film was released by American International Pictures with an English-language dub in the United States on May 23, 1969. Contemporary American reviews were mixed, with praise mainly held for the climactic monster battle. Retrospectively, the film has received more praise, and is considered a favorite among "Godzilla" fans for its "audacious and simple story", "innovative action sequences", and a "memorably booming" score by Akira Ifukube.

At the close of the 20th century, all of the Earth's kaiju have been collected by the United Nations Science Committee and confined in an area known as Monster Island, located in the Ogasawara island chain. A special control center is constructed underneath the island to ensure that the monsters stay secure and to serve as a research facility to study them.

When communications with Monsterland are suddenly and mysteriously severed, and all of the monsters begin attacking world capitals, Dr. Yoshida of the UNSC orders Captain Yamabe and the crew of his spaceship, Moonlight SY-3, to investigate Ogasawara. There, they discover that the scientists, led by Dr. Otani, have become mind-controlled slaves of a feminine alien race identifying themselves as the Kilaaks, who reveal that they are in control of the monsters. Their leader demands that the human race surrender, or face total annihilation.

Godzilla attacks New York City, Rodan invades Moscow, Mothra (a larval offspring) lays waste to Beijing, Gorosaurus destroys Paris, (although Baragon was credited for its destruction), and Manda attacks London. These attacks were set in to motion to draw attention away from Japan, so that the aliens can establish an underground stronghold near Mt. Fuji in Japan. The Kilaaks then turn their next major attack onto Tokyo and, without serious opposition, become arrogant in their aims until the UNSC discover, after recovering the Kilaaks' monster mind-control devices from around the world, that they have switched to broadcasting the control signals from their base under the Moon's surface. In a desperate battle, the crew of the SY-3 destroys the Kilaak's lunar outpost and returns the alien control system to Earth.

With all of the monsters under the control of the UNSC, the Kilaaks unleash their secret weapon, King Ghidorah. The three-headed space dragon is dispatched to protect the alien stronghold at Mt. Fuji, and battles Godzilla, Minilla, Mothra, Rodan, Gorosaurus, Anguirus, and Kumonga. While seemingly invincible, King Ghidorah is eventually overpowered by the combined strength of the Earth monsters and is killed. Refusing to admit defeat, the Kilaaks produce their ace, a burning monster they call the Fire Dragon, which begins to torch Tokyo and destroys the control center on Ogasawara. Suddenly, Godzilla attacks and destroys the Kilaaks' underground base, revealing that the Earth's monsters instinctively know who their enemies are. Captain Yamabe then pursues the Fire Dragon in the SY-3 and narrowly achieves victory for the human race. The Fire Dragon is revealed to be a flaming Kilaak saucer and is destroyed. With the Kilaaks defeated, Godzilla and the other monsters eventually return to Monster Island to live in peace.

Per the waning popularity of the "Godzilla" series, special effects director Sadamasa Arikawa noted that Toho were going to potentially end the "Godzilla" series as "Producer Tanaka figured that all the ideas had just run out."

The film was written by Takeshi Kimura and Ishirō Honda, making it the first "Godzilla" film since "Godzilla Raids Again" not written by Shinichi Sekizawa. Takeshi Kimura is credited to the pen name Kaoru Mabuchi in the film's credits. Kimura and Honda's script developed the concept of Monsterland (referred to as Monster Island in future films). As the films has several monsters who continuously return in the films, the location was developed to be a faraway island where the monsters are pacified. This tied other films not related to the "Godzilla" series within its universe, as creatures such as Manda (from "Atragon") and Varan ("Varan the Unbelievable") exist. The film features footage from "Ghidorah, the Three-Headed Monster" (1964), specifically King Ghidorah's fiery birth scene.

New monster suits for Godzilla and Anguirus were constructed for the film, while Rodan and King Ghidorah suits were modified from previous films, with King Ghidorah having less detail than he had in previous films.

"Destroy All Monsters" was released in Japan on 1 August 1968 where it was distributed by Toho. It was released on a double bill with a reissue of the film "Atragon". The film was reissued theatrically in Japan in 1972 where it was re-edited by Honda to a 74-minute running time and released with the title "Gojira: Dengeki Taisakusen" ( "Godzilla: Lightning Fast Strategy"). "Destroy All Monsters" continued the decline in ticket sales in Japan for the "Godzilla" series, earning 2.6 million in ticket sales. In comparison, "Invasion of Astro-Monster" brought in 3.8 million and "Son of Godzilla" collected 2.5 million.

The film was released in the United States by American International Pictures with an English-language dub on 23 May 1969. The film premiered in the United States in Cincinnati. American International Pictures hired Titra Studios to dub the film into English. The American version of the film remains relatively close to the Japanese original. Among the more notable removed elements include Akira Ifukube's title theme and a brief shot of Minilla shielding his eyes and ducking when King Ghidorah drops Anguirus from the sky. "Destroy All Monsters" was shown on American television until the early 1980s. It resurfaced on cable broadcast on the Sci-Fi Channel in 1996.

"Destroy All Monsters" was released on VHS by ADV Films in 1998 which featured English-dubbed dialogue from Toho's own international version of the film. In 2011, Tokyo Shock released the film on DVD and Blu-ray and in 2014 the company re-released it on DVD and Blu-ray. In 2019, the Japanese version and export English version were included in a Blu-ray box set released by the Criterion Collection, which included all 15 films from the franchise's Shōwa era.

From contemporary reviews, both "Variety" and "Monthly Film Bulletin" noted the film's best scenes involved the monsters together, while criticising the filmmaking. "Variety" reviewed the English-dubbed version of the film stating that it may appeal to "Sci-fi addicts and monster fans" while stating that the "plot is on comic strip level, special effects depend on obvious miniatures and acting (human) is from school of "Flash Gordon"" and that the film's strength relied on its "monster rally". The "Monthly Film Bulletin" opined that "the model work is poor, and as usual the script is junior comic-strip". Both reviews mentioned the monsters' final scene with "Variety" commenting that it was "clever" and the "Monthly Film Bulletin" stating that "apart from [the monsters] statutory devastation of world capitals [...] the monsters have disappointingly little to do until they get together in the last reel for a splendid battle" The "Monthly Film Bulletin" commented that the film was "almost worth sitting through the banalities for the final confrontation on Mount Fuji" noting the son of Godzilla "endearingly applauding from a safe distance" and "the victorious monsters performing a celebratory jig".

From retrospective reviews, Steve Biodrowski of "Cinefantastique" commented that the film "is too slim in its storyline, too thin in its characterizations, to be considered a truly great film [...] But for the ten-year-old living inside us all, it is entertainment of the most awesome sort." Matt Paprocki of "Blogcritics" said the film is "far from perfect" and "can be downright boring at times" but felt that "the destruction scenes make up for everything else" and "the final battle is an epic that simply can't be matched".

The film is considered a cult favorite among fans of the "Godzilla" franchise. In Steve Ryfle and Ed Godziszewski's 2017 book covering Ishiro Honda's filmography, they expressed that "Destroy All Monsters" is now seen as the "last truly spirited entry" in Toho's initial series of "kaiju" films, due to "its audacious and simple story, a bounty of monsters and destruction, and a memorably booming soundtrack from Akira Ifukube".

"Godzilla" director Gareth Edwards previously expressed interest in making a sequel to his 2014 movie inspired by "Destroy All Monsters".





</doc>
<doc id="11998" url="https://en.wikipedia.org/wiki?curid=11998" title="Godzilla vs. Megalon">
Godzilla vs. Megalon

"Godzilla vs. Megalon" was released theatrically in Japan on March 17, 1973. It received a theatrical release in the United States in the summer of 1976 by Cinema Shares.

In the first part of 1971 (197X in the Japanese version), the most recent underground nuclear test, set off near the Aleutians, sends shockwaves as far across the globe as Monster Island in the South Pacific, disturbing monsters such as Godzilla, Anguirus, and Rodan.

For years, Seatopia, an undersea civilization, has been heavily affected by this nuclear testing conducted by the surface nations of the world. Upset by these tests, the Seatopians plan to unleash their civilization's beetle-styled god, Megalon, to destroy the surface world out of vengeance.

On the surface, an inventor named Goro Ibuki, his nephew Rokuro, and Goro's friend Hiroshi Jinkawa are off on an outing near a lake when Seatopia makes itself known to the Earth by drying up the lake the trio was relaxing nearby and using it as a base of operation. As they return home they are ambushed by agents of Seatopia who are trying to steal Jet Jaguar, a humanoid robot under construction by the trio of inventors. However the agents' first attempt is botched and they are forced to flee to safety.

Some time later, Jet Jaguar is completed but the trio of inventors are knocked unconscious by the returning Seatopian agents. The agents' plan is to use Jet Jaguar to guide and direct Megalon to destroy whatever city Seatopia commands him to do. Goro and Rokuro are sent to be killed, while Hiroshi is taken hostage. Megalon is finally released to the surface while Jet Jaguar is put under the control of the Seatopians and is used to guide Megalon to attack Tokyo with the Japan Self Defense Forces failing to defeat the monster. Eventually, the trio of heroes manage to escape their situation with the Seatopians and reunite to devise a plan to send Jet Jaguar to get Godzilla's help using Jet Jaguar's secondary control system.

After uniting with Japan's Defense Force, Goro manages to regain control of Jet Jaguar and sends the robot to Monster Island to bring Godzilla to fight Megalon. Without a guide to control his actions, Megalon flails around relentlessly and aimlessly fighting with the Defense Force and destroying the outskirts of Tokyo. The Seatopians learn of Jet Jaguar's turn and thus send out a distress call to their allies, the Space Hunter Nebula M aliens (from the previous film) to send the alien dinosaur Gigan to assist their allies.

As Godzilla journeys to fight Megalon, Jet Jaguar starts acting on his own and ignoring commands to the surprise of his inventors, and grows to gigantic proportions to face Megalon himself until Godzilla arrives. The battle is roughly at a standstill between robot and cyborg, until Gigan arrives and both Megalon and Gigan double team against Jet Jaguar. Godzilla finally arrives to assist Jet Jaguar and the odds become even. After a long and brutal fight, Gigan and Megalon both retreat and Godzilla and Jet Jaguar shake hands on a job well done. Jet Jaguar bids Godzilla farewell and Godzilla returns to his home on Monster Island. Jet Jaguar returns to his previous human-sized state and reverts back to being just a robot.

"Godzilla vs. Megalon" was originally planned as a non-"Godzilla" film, a solo vehicle for Jet Jaguar, which was the result of a contest Toho had for children in mid-to-late 1972. The winner of the contest was an elementary school student, who submitted the drawing of a robot called Red Arone. Red Arone was turned into a monster suit, but when the child was shown the suit, he became upset because the suit did not resemble his original design. The boy's original design was white but the costume was colored red, blue and yellow. Red Arone was used for publicity, but Toho had renamed the character Jet Jaguar and had special effects director Teruyoshi Nakano redesign the character, only keeping the colors from the Red Arone suit. The Red Arone suit had a different head and wings. 

However, after doing some screen tests and storyboards, Toho figured Jet Jaguar would not be able to carry the film on his own, either in screen appearance or marketing value, so they shut the project down during pre-production. Nearly a month later, producer Tomoyuki Tanaka called in screenwriter Shinichi Sekizawa to revise the script to add Godzilla and Gigan. To make up for lost production time, the film was shot in a hasty three weeks. The production time totaled nearly six months from planning to finish.

The film had three early treatments, each written by Shinichi Sekizawa, one was titled "Godzilla vs. The Megalon Brothers: The Undersea Kingdom's Annihilation Strategy" which was completed in September 1972. The second was titled "Insect Monster Megalon vs. Godzilla: Undersea Kingdom's Annihilation Strategy", which was turned in on September 5, 1972, and the third draft was submitted in September 7, 1972.

According to Teruyoshi Nakano, the Godzilla suit used in this film (nicknamed "MegaroGoji" メガロゴジ ) was made in a week, making it the fastest Godzilla suit ever made to date. They did not have time to make the eyes work correctly, something they had more time to fix for Godzilla's five appearances on Toho's superhero TV series "Zone Fighter" (1973), which was produced around the same time.

The Megalon suit was one of the heaviest suits produced since the 1954 "Godzilla" suit, which made it even more difficult to raise the Megalon suit via wires in certain scenes up to the point where Nakano almost decided to scrap those scenes altogether. Since the film was shot in the winter, Katsuhiko Sasaki stated that director Jun Fukuda gave him and Yutaka Hayashi a shot of whiskey to warm them up.

The Gigan suit is similar to the previous design, but the suit was made thinner, less bulky, the horn on the head was less pointed, and the buzzsaw didn't move, since it was made of static pieces. This suit also has different-sized back fins, a more circular visor, scales running up the back/sides of the neck and longer legs compared to the original version.

Teruyoshi Nakano recalls how the film was rushed and that it took three weeks to shoot, stating, "It went into productions without enough preparation. There was no time to ask Mr. Sekizawa to write the script, so Mr. Sekizawa kind of thought up the general story and director Fukuda wrote the screenplay. The screenplay was completed right before crank-in".

Like previous Godzilla films, "Godzilla vs. Megalon" heavily employs stock footage from previous films such as "Mothra vs. Godzilla" (1964), "The War of the Gargantuas" (1966), "Ebirah, Horror of the Deep" (1966), "Destroy All Monsters" (1968), "Godzilla vs. Hedorah" (1971), and "Godzilla vs. Gigan" (1972).

In 1976, Cinema Shares gave "Godzilla vs. Megalon" a wide theatrical release in the United States and launched a massive marketing campaign for the film, along with the poster, buttons with one of the four monsters' faces on them were released. Given away at theatrical showings was a comic that told a simplified version of the film, which incorrectly named Jet Jaguar as "Robotman" and Gigan as "Borodan". These incorrect names were also featured in the U.S. trailer.

Initially, Cinema Shares screened Toho's international English version but to ensure a G rating, several cuts were made, which resulted in the film running three minutes shorter than the original version.

"Godzilla vs. Megalon" is the first Godzilla film to receive an American prime time network television premiere, where it was broadcast nationwide at 9:00 PM on NBC on March 15, 1977. However, to accommodate commercials, the film was only shown in a one-hour time slot, which resulted in the film being cut down to 48 minutes. John Belushi hosted the broadcast where he did some skits, all in a Godzilla suit.

Mel Maron (who was president of Cinema Shares at the time) chose to release "Godzilla vs. Megalon" because he saw Godzilla as a heroic figure by that point and felt the timing was right to show children a hero who was a friendly monster and not Superman.

The U.S. rights for the film eventually fell into the public domain in the late 80s, which resulted in companies releasing poorly-cropped, fullscreen VHS tapes mastered from pan and scan sources. This also led to the film being featured in "Mystery Science Theater 3000". In 1988, New World Video intended to release the original uncut version of the English dub but, declined the project, due to a lack of budget that was required for a full release. However, despite this, the film was released uncut and in widescreen in 1992 by UK company Polygram Ltd as a double feature with "Godzilla vs. Gigan". In 1998 the film was again released by UK company, 4 Front Video. As of now it appears those are the only two VHS tapes on the film that are unedited and in high quality. It was also released on DVD by Power Multimedia in 1999 in Taiwan. Originally the Sci-Fi Channel (now SyFy) showed the cut version, until finally in 2002 as Toho regained ownership of that title alongside "Godzilla vs. Gigan" and "Godzilla vs. Mechagodzilla" (both of which also were released by Cinema Shares) and broadcast the film fully uncut for the first time in the U.S.

In Japan, "Godzilla vs. Megalon" sold approximately 980,000 tickets. It was the first "Godzilla" film to sell less than one million admissions. It earned ¥220 million in Japan distribution income (rentals).

The film was a success in American theaters, earning $383,744 in its first three days in Texas and Louisiana alone. The film grossed about worldwide.

"Godzilla vs. Megalon" was released theatrically in America on May 9, 1976, though the "San Francisco Chronicle" indicates that it opened there in June, and "The New York Times" indicates that it opened in New York City on July 11. "The New York Times" film critic Vincent Canby, who a decade before had given a negative review to "Ghidorah, the Three-Headed Monster", gave "Godzilla vs. Megalon" a generally positive review. In his review on July 12, 1976, Canby said, ""Godzilla vs. Megalon" completes the canonization of Godzilla...It's been a remarkable transformation of character - the dragon has become St. George...It's wildly preposterous, imaginative and funny (often intentionally). It demonstrates the rewards of friendship, between humans as well as monsters, and it is gentle."

"Godzilla vs. Megalon" has attracted the ire of many "Godzilla" fans in the decades since its original release. The film contributed to the reputation of "Godzilla" films in the United States as cheap children's entertainment that should not be taken seriously. It has been described as "incredibly, undeniably, mind-numbingly bad" and one of the "poorer moments" in the history of kaiju films.

In particular, the special effects of the film have been heavily criticized. One review described the Godzilla costume as appearing to be "crossed with Kermit the Frog" and another sneeringly compared it to "Godzilla vs. Gigan", stating that it did "everything wrong that "Gigan" did, and then some." However, most of the criticism is of the lack of actual special effects work, as most of it consists of stock footage from previous films, including "Godzilla vs. Gigan" and "Ghidorah, the Three-Headed Monster", but a few pieces of effects work have garnered praise, specifically a scene where Megalon breaks through a dam and the draining of the lake.

The other aspects of the film have been similarly skewered. The acting is usually described as flat and generally poor, and as not improving, or sometimes, worsening, the already weak script. One part of the film, on the other hand, has garnered almost universal praise: Godzilla's final attack on Megalon, a flying kick. It has been called the saving grace of the film, and was made famous by the mock exclamations of shock and awe displayed on "Godzilla vs. Megalon"<nowiki>'</nowiki>s appearance on "Mystery Science Theater 3000". Through the end of season three to the middle of season five, that clip would be shown at the opening of each show.

Despite all this, the film is also one of the most widely seen "Godzilla "films in the United States — it was popular in its initial theatrical release, largely due to an aggressive marketing campaign, including elaborate posters of the two title monsters battling atop New York City's World Trade Center towers, presumably to capitalize on the hype surrounding the Dino De Laurentiis remake of "King Kong", which used a similar image for its own poster.

The film was released numerous times in the VHS format, mostly as videos from bargain basement studios that featured the edited TV version (which was wrongly assumed to be in the public domain for many years), while PolyGram and 4 Front released the unedited version of the film in 1992 and 1998, respectively. Some rumors have circulated that the film's original VHS releases in the States were uncut, but there is no evidence confirming or denying this.  

Media Blasters acquired the DVD rights to both "Godzilla vs. Megalon" and "Destroy All Monsters". Both films were released under one of the company's divisions, Tokyo Shock. Media Blasters originally planned to release "Godzilla vs. Megalon" on DVD and Blu-ray on December 20, 2011; however, due to technical difficulties with the dubbing and Toho having yet to give its approval for the release, the DVD/Blu-ray release was delayed. Media Blasters finally released the film on August 14, 2012, but only on a bare-bones DVD and Blu-ray. Despite this, a manufacturing error led to several copies of the originally planned version featuring bonus content to be released by accident. These special features versions are incredibly rare and are not labelled differently from the standard version, making them nearly impossible to find. This release was commercially the first to remaster the film to its original full-length version. In 2019, the Japanese version and export English dub were included in a Blu-ray box set released by the Criterion Collection, which included all 15 films from the franchise's Shōwa era.




</doc>
<doc id="12000" url="https://en.wikipedia.org/wiki?curid=12000" title="Godzilla vs. Biollante">
Godzilla vs. Biollante

In the film, Godzilla battles a monster born from the cells of a plant and a woman, as corporations struggle for control over Godzilla cells. The idea originated from a public story-writing contest, and set a trend common to all Heisei era movies, in which Godzilla faces off against opponents capable of metamorphosing into new, progressively more powerful forms.

"Godzilla vs. Biollante" was released theatrically in Japan on December 16, 1989. It received a direct-to-video release in the United States in November 25, 1992 through HBO Video. Although it received generally positive reviews, the film was a disappointment at the Japanese box office. In Japan, it was followed by "Godzilla vs. King Ghidorah" in 1991.

In the aftermath of Godzilla's attack on Tokyo and later imprisonment at Mount Mihara, the monster's cells are secretly delivered to the Saradia Institute of Technology and Science, where they are to be merged with genetically modified plants in the hope of transforming Saradia's deserts into fertile land and ending the country's economic dependence on oil wells. Dr. Genshiro Shiragami and his daughter, Erika, are enlisted to aid with the project. However, a terrorist bombing destroys the institute's laboratory, ruining the cells and killing Erika.

In 1990, Shiragami has returned to Japan and merged some of Erika's cells with those of a rose in an attempt to preserve her soul. Scientist Kazuhito Kirishima and Lieutenant Goro Gondo of the Japan Self-Defense Forces (JSDF) are using the Godzilla cells they collected to create "Anti-Nuclear Energy Bacteria", hoping it can serve as a weapon against Godzilla should he return. They attempt to recruit Shiragami to aid them, but are rebuffed. Meanwhile, international tensions increase over the Godzilla cells, as they are coveted by both the Saradia Institute of Technology and Science and the American Bio-Major organization. An explosion from Mount Mihara causes tremors across the area, including Shiragami's home, badly damaging the roses. Shiragami agrees to join the JSDF's effort and is given access to the Godzilla cells, which he secretly merges with one of the roses. A night later, rival Bio-Major and Saradian agents break into Shiragami's lab, but are attacked by a large plant-like creature which later escapes to Lake Ashino and is named "Biollante" by Shiragami.

Bio-Major agents plant explosives around Mount Mihara and blackmails the Diet of Japan, warning the explosives will be detonated and thus free Godzilla if the cells are not handed over. Kirishima and Gondo attempt to trade, but Saradian agent SSS9 thwarts the attempt and escapes with the cells. The explosives are detonated, and Godzilla is released. He attempts to reach the nearest power plant to replenish his supply of nuclear energy, but Biollante calls out to him. Godzilla arrives at the lake to engage Biollante in a vicious battle, and emerges as the victor. Godzilla proceeds toward the power plant at Tsuruga, but psychic Miki Saegusa uses her powers to divert him toward Osaka instead. The city is quickly evacuated before Godzilla makes landfall. A team led by Gondo meet Godzilla at the central district and fire rockets infused with the anti-nuclear bacteria into his body. Gondo is killed in the process, and an unharmed Godzilla leaves.

Kirishima recovers the cells and returns them to the JSDF. Shiragami theorizes that if Godzilla's body temperature is increased, the bacteria should work against it. The JSDF erects microwave-emitting plates during an artificial thunderstorm, hitting Godzilla with lightning and heating up his body temperature during a battle in the mountains outside Osaka. Godzilla is only moderately affected, but Biollante arrives to engage him in battle once again. The fight ends after Godzilla fires an atomic heat ray inside Biollante's mouth. An exhausted Godzilla collapses on the beach, and Biollante disintegrates into the sky, forming an image of Erika among the stars. Shiragami, watching the scene, is killed by SSS9. Kirishima chases the assassin and, after a brief scuffle, SSS9 is killed by a microwave-emitting plate activated by Sho Kuroki. Godzilla reawakens and leaves for the ocean.

Tomoyuki Tanaka announced a sequel to "The Return of Godzilla" in 1985, but was skeptical of its possibilities, as the film had been of little financial benefit to Toho, and the failure of "King Kong Lives" following year convinced him that audiences were not ready for a continuation of the "Godzilla" series. He relented after the success of "Little Shop of Horrors", and proceeded to hold a public story contest for a possible script. In consideration of "The Return of Godzilla"'s marginal success in Japan, Tanaka insisted that the story focus on a classic monster vs. monster theme. Tanaka handed the five finalist entries to director Kazuki Ōmori, despite the two's initially hostile relationship; the latter had previously held Tanaka responsible for the decline in the "Godzilla" series' quality during the 1970s. Ōmori chose the entry of dentist Shinichiro Kobayashi, who wrote his story with the hypothetical death of his daughter in mind.

Kobayashi's submission was notable for its emphasis on dilemmas concerning biotechnology rather than nuclear energy, and revolved around a scientist grieving for his deceased daughter and attempting to keep her soul alive by merging her genes with those of a plant. The scientist's initial experiments would have resulted in the creation of a giant rat-like amphibian called Deutalios, which would have landed in Tokyo Bay and been killed by Godzilla. A female reporter investigating the scientist's activities would have suffered from psychic visions of plants with humanoid faces compelling her to infiltrate the scientist's laboratory. The scientist would have later confessed his intentions, and the finale would have had Godzilla battling a human-faced Biollante who defeats him by searing his flesh with acid.

Ōmori proceeded to modify the story into a workable script over a period of three years, using his background as a biologist to create a plausible plot involving genetic engineering and botany. In order to preserve the series' anti-nuclear message, he linked the creation of Biollante to the use of Godzilla cells, and replaced Kobayashi's journalist character with Miki Saegusa. He openly admitted that directing a "Godzilla" film was secondary to his desire to make a James Bond movie, and thus added elements of the spy film genre into the plot. Unlike the case with later, more committee-driven "Godzilla" films, Ōmori was given considerable leeway in writing and directing the film, which Toho staff later judged to have been an error resulting in a movie with a very narrow audience.

Koichi Kawakita, who had previously worked for Tsuburaya Productions, replaced Teruyoshi Nakano as head of the series' special effects unit after Toho became impressed at his work in "Gunhed". Kawakita made use of "Gunhead"'s special effects team Studio OX, and initially wanted to make Godzilla more animal-like, using crocodiles as references, but was berated by Tanaka, who declared Godzilla to be "a monster" rather than an animal. Kenpachiro Satsuma returned to portray Godzilla, hoping to improve his performance by making it less anthropomorphic than in previous films. Suitmaker Noboyuki Yasamaru created a Godzilla suit made specifically with Satsuma's measurements in mind, unlike the previous one which was initially built for another performer and caused Satsuma discomfort. The resulting 242 lb suit proved more comfortable than the last, having a lower center of gravity and more mobile legs. A second 176 lb suit was built for outdoor underwater scenes. The head's size was reduced, and the whites around the eyes removed. On the advice of story finalist Shinichiro Kobayashi, a double row of teeth was incorporated in the jaws. As with the previous film, animatronic models were used for close-up shots. These models were an improvement over the last, as they were made from the same molds used for the main costume, and included an articulated tongue and intricate eye motion. The suit's dorsal plates were filled with light bulbs for scenes in which Godzilla uses his atomic ray, thus lessening reliance on optical animation, though they electrocuted Satsuma the first time they were activated. Satsuma was also obliged to wear protective goggles when in the suit during scenes in which Godzilla battles the JSDF, as real explosives were used on set.

Designing and building the Biollante props proved problematic, as traditional suitmation techniques made realizing the requested design of the creature's first form difficult, and the resulting cumbersome model for Biollante's final form was met with disbelief from the special effects team. Biollante's first form was performed by Masao Takegami, who sat within the model's trunk area on a platform just above water level. While the creature's head movements were simple to operate, its vines were controlled by an intricate array of overhead wires which proved difficult for Satsuma to react to during combat scenes as they offered no tension, thus warranting Satsuma to feign receiving blows from them, despite not being able to perceive them. Biollante's final form was even more difficult to operate, as its vine network took hours to rig up on set. Visibility in both the Godzilla and final form Biollante suits was poor, thus causing difficulties for Takegami in aiming the creature's head when firing sap, which permanently stained anything it landed on.

While it was initially decided to incorporate stop motion animation into the film, the resulting sequences were scrapped, as Kawakita felt they failed to blend in with the live-action footage effectively. The film however became the first of its kind to use CGI, though its usage was limited to scenes involving computer generated schematics. The original cut of the movie had the first battle culminating in Biollante's spores falling around the hills surrounding Lake Ashino and blooming into fields of flowers, though this was removed as the flowers were out of scale.

Unlike the previous film, "Godzilla vs. Biollante" incorporates themes from Akira Ifukube's original "Godzilla" theme, though the majority of the soundtrack was composed of original themes by Koichi Sugiyama. The score was orchestrated by conductor David Howell through the Kansai Philarmonic, though Howell himself had never viewed the movie, and thus was left to interpret what the scenes would consist of when conducting the orchestra.

After the film was released in Japan, Toho commissioned a Hong Kong company named Omni Productions to dub the film into English.

In early 1990, Toho entered discussions with Miramax to distribute the film. When talks broke off, Toho filed a lawsuit in Los Angeles Federal Court, accusing Miramax of entering an oral agreement in June to pay Toho $500,000 to distribute the film. This lawsuit delayed the film's release for two years. An out of court settlement was reached with Miramax buying the rights to the film for an unreported figure. Miramax would have entertained thoughts of releasing the film in theaters, but in the end it was decided to release the film straight to home video instead. HBO released the film on VHS in 1992 and Laserdisc in 1993. Miramax utilized the uncut English international version of the film for this release.

"Godzilla vs. Biollante" was released on VHS by HBO Home Video on November 25, 1992. It was later released on Blu-ray and DVD by Miramax Echo Bridge on December 4, 2012. It was released as a double feature and 8-disk movie pack on both Blu-ray and DVD with "Mega Shark Versus Giant Octopus" (2009) by Echo Bridge Home Entertainment in 2013. It was last released by Lionsgate on Blu-ray and DVD on October 7, 2014. Sometime afterwards, Toho took the English rights away from Miramax and the future of the film's availability remains uncertain.

In Japan, the film sold approximately 2 million tickets, grossing .

"Godzilla vs. Biollante" has received positive reviews, with praise for the story, music and visuals.
Ed Godziszewski of Monster Zero said the film is "by no means a classic" but felt that "for the first time in well over 20 years, a [Godzilla] script is presented with some fresh, original ideas and themes." Joseph Savitski of Beyond Hollywood said the film's music is "a major detraction", but added that it's "not only one of the most imaginative films in the series, but also the most enjoyable to watch." Japan Hero said, "[T]his is definitely a Godzilla movie not to be missed."

In their scholarly book "Japan's Green Monsters" on kaiju cinema, Rhoads and McCorkle offer an ecocritical assessment of "Godzilla vs. Biollante". The scholars focus on the film's critique of genetic engineering and biotechnology years before the subject appeared in more popular Hollywood blockbusters like Steven Spielberg's 1993 blockbuster "Jurassic Park". Rhoads and McCorkle counter prior reviews of the film and argue that "Godzilla vs. Biollante" possesses far deeper environmental messages than the obvious ones present on the film's surface.

In July 2014, in a poll reported by the , "Godzilla vs. Biollante" was selected as the best "Godzilla" film by a group of fans and judges.

Composer Akira Ifukube, who had refused to compose the film's score, stated on interview that he disliked the way Koichi Sugiyama had modernized his Godzilla theme, and defined the Saradia theme as "ridiculous", on account of it sounding more European than Middle Eastern.





</doc>
<doc id="12001" url="https://en.wikipedia.org/wiki?curid=12001" title="Terror of Mechagodzilla">
Terror of Mechagodzilla

Terror of Mechagodzilla, released in Japan as is a 1975 Japanese "kaiju" film directed by Ishirō Honda, written by Yukiko Takayama, and produced by Tomoyuki Tanaka and Henry G. Saperstein, with special effects by Teruyoshi Nakano. The film was produced and distributed by Toho; it is the 15th film in the "Godzilla" franchise, serving as a direct sequel to the 1974 film "Godzilla vs. Mechagodzilla", and is the final film in the franchise's Shōwa period.

"Terror of Mechagodzilla" stars Katsuhiko Sasaki, Tomoko Ai, Akihiko Hirata, and Gorō Mutsumi, and features Toru Kawai, Ise Mori, and Tatsumi Nikamoto as the fictional monster characters Godzilla, Mechagodzilla 2, and Titanosaurus, respectively. The film was released theatrically in Japan on March 15, 1975. It received a limited release in the United States in 1978 by Bob Conn Enterprises under the title The Terror of Godzilla. The film remains the least financially successful entry in the "Godzilla" franchise to this day.

Continuing after the end of the events of "Godzilla vs. Mechagodzilla", Interpol agents, led by Inspector Kusaka, search for the pieces of Mechagodzilla at the bottom of the Okinawan Sea. Using the submarine "Akatsuki", they hope to gather information on the robot's builders, the alien Simeons. The "Akatsuki" is suddenly attacked by a giant aquatic dinosaur called Titanosaurus and the crew vanishes.

Interpol starts an investigation into the incident. With the help of marine biologist Akira Ichinose, they trace Titanosaurus to a reclusive, mad scientist named Shinzô Mafune, who wants to destroy all mankind. While Ichinose is visiting his old home in the seaside forest of Manazuru, they meet Mafune's lone daughter, Katsura. She tells them that not only is her father dead, but she burned all of the notes about the giant dinosaur (at her father's request). Unbeknownst to them, Mafune is still alive and well. He is visited by his friend Tsuda, who is an aide to the alien leader Mugal. He is leading the project to quickly rebuild Mechagodzilla. Mugal offers their services to Mafune, so that his Titanosaurus and their Mechagodzilla 2 (which is even more powerful than the original) will be the ultimate weapons. They hope to wipe out mankind and rebuild the world for themselves, starting with Tokyo and branching out from there.

But things become complicated for both factions when Ichinose falls in love with Katsura and unwittingly gives her Interpol's information against Titanosaurus, the new Mechagodzilla and the aliens. It is also discovered that Katsura is actually a cyborg, due to undergoing surgery after nearly being killed during one of her father's experiments when she was a child, and Mugal still has uses for her. Meanwhile, Mafune is desperate to unleash Titanosaurus without the aliens' permission, so he releases it on Yokosuka one night. By then, Interpol discovers that supersonic waves are Titanosaurus' weakness. They have a supersonic wave oscillator ready, but Katsura sabotages the machine before they can use it. Fortunately, Godzilla arrives to fight off Titanosaurus and easily defeats him, causing him to retreat back to the sea.

Later, when Ichinose visits Katsura, he is captured by the aliens. Tied up, Ichinose can only watch as Mafune and the aliens unleash Mechagodzilla 2 and Titanosaurus on Tokyo, while Interpol struggles to repair their supersonic wave oscillator and the Japanese armed forces struggle to keep the two monsters at bay. Katsura, while being controlled by Mugal, ignores Ichinose's pleas and controls both the dinosaur and the robot as they destroy the city.

Godzilla comes to the rescue, although at first he is outmatched by the two titans. While Interpol distracts Titanosaurus with the repaired supersonic wave oscillator, Godzilla is able to focus on attacking Mechagodzilla 2. Interpol agents infiltrate the aliens' hideout, rescue Ichinose, and kill Mafune and many of the aliens. The remaining aliens attempt to escape in their ships, but Godzilla shoots them down with his atomic heat ray. The wounded Katsura, while being embraced by Ichinose, shoots herself in order to destroy Mechagodzilla 2's control device (which had been implanted in her body earlier by the aliens). This allows Godzilla to destroy the robot, which short-circuits and is tossed by him into a huge chasm made by the battle, after which Godzilla blasts it with his atomic heat ray, causing it to explode into little pieces of metal again, which are then buried within the chasm at the same time. Godzilla, with the help of the oscillator, then defeats Titanosaurus and heads back to the sea.

The original screenplay that Yukiko Takayama created after winning Toho's story contest for the next installment in the Godzilla series was picked by assistant producer Kenji Tokoro and was submitted for approval on July 1, 1974, less than four months after "Godzilla vs. Mechagodzilla" was released.

The original concept is similar to the finished version of "Terror of Mechagodzilla", with many of the changes being budgetary in nature. The most obvious alteration is the removal of the two dinosaurs called the Titans, which merged to become Titanosaurus in the first draft. It was an interesting concept, although something that was also under-explained, considering the magnitude of such an occurrence of the creatures merging. Another noticeable change to the script is that of the final battle, which does not move to the countryside but instead would have reduced Tokyo to rubble during the ensuing conflict between the three monsters.

After her initial draft, Takayama submitted a revised version on October 14, 1974. This went through a third revision on December 4, and then yet another on December 28 of that same year before it was met with approval and filming began.

The film is one of the only "Godzilla" films with brief nudity. The scene occurs when Katsura is being operated on in order to have the control devise for Mechagodzilla 2 placed inside her body, at which point Katsura's breasts are exposed (although the body is actually a mannequin). This scene was cut in the U.S., both from the theatrical and TV versions of the film.

Director Ishiro Honda laments not being able to work with the story's writer, Yukiko Takayama, on other films, enjoying that a "woman's perspective was especially fresh" for the genre.

Kensho Yamashita was the chief assistant director on the project. He notes, though, that Honda never actually assigned any of the shooting to him, possibly because he was happy to be directing again after a long gap in his career and wanted to do the work himself.

Toho titled its English version of the film "Terror of Mechagodzilla" and had it dubbed into English in Hong Kong. This “international version” has never seen wide release in the United States, but has been issued on VHS in the United Kingdom by PolyGram Video Ltd. and on DVD in Taiwan by Power Multimedia.

The film was given a North American theatrical release in March 1978 by independent distributor Bob Conn Enterprises under the title "The Terror of Godzilla". Just as Cinema Shares had done with the previous three "Godzilla" movies, Bob Conn Enterprises chose to utilize the Toho-commissioned English dub instead of hiring a new crew to re-dub the film. "The Terror of Godzilla" was heavily edited to obtain a "G" rating from the MPAA. Several scenes with violent content were entirely removed, disrupting the flow of the narrative.

Henry G. Saperstein, who sold the theatrical rights to Bob Conn Enterprises, also released the film to television in late 1978, this time under Toho's international title, "Terror of Mechagodzilla". Unlike "The Terror of Godzilla", the television version remained mostly uncut, with only the shot of Katsura's naked breasts excised. Saperstein's editors also added a 10-minute prologue that served as a brief history of Godzilla, with footage from Saperstein's English versions of "Invasion of Astro-Monster" and "All Monsters Attack" (the latter of which utilized stock footage from both "Ebirah, Horror of the Deep" and "Son of Godzilla").

In the mid-1980s, the U.S. television version, "Terror of Mechagodzilla", was replaced by the theatrical edit, "The Terror of Godzilla", on television and home video. For some reason, the title was also changed to "Terror of Mechagodzilla". The 1994 Paramount release of "Terror of Mechagodzilla" listed a running time of 89 minutes on the slipcase, implying that this release would be the longer version first shown on American TV. The actual video cassette featured the edited theatrical version. In a 1995 interview with "G-Fan" magazine, Saperstein was surprised to hear about this mistake. In 1997 on Channel 4 in the U.K., three Godzilla movies were shown back to back late at night, starting with "Godzilla vs. Megalon", "Godzilla vs. Gigan" and then "Terror of Mechagodzilla"; all were dubbed versions. This showing was uncut, including the Katsura nudity scene, but it did not have the Western-made prologue.

In the mid-2000s, the television version showed up again on Monsters HD, and in 2007, it made its home video debut as the U.S. version on the Classic Media DVD. Although the added prologue was originally framed for fullscreen television, it was cropped and shown in widescreen on the disc. The rest of the movie featured the audio from Saperstein's television version synced to video from the Japanese version.

The first article about the movie's storyline was published in a 1977 issue of "Japanese Giants" (published by Brad Boyle) and was written by Richard H. Campbell, creator of "The Godzilla Fan News Letter" (a.k.a. "The Gang").

In Japan, the film sold 980,000 tickets. Despite earning positive reviews, it would be the least-attended "Godzilla" film in Japan and also one of only two "Godzilla" films to sell less than 1 million tickets. This was part of a decline in attendance for monster movies as a whole and Toho put the production of monster movies on hold. Toho had no intention of permanently ending the "Godzilla" series. Throughout the remainder of the 1970s, several new Godzilla stories were submitted by various writers and producers. None of these films, however, were ultimately made. It was not until 1984 and "Godzilla"'s 30th anniversary that Toho would start production on a new Godzilla movie.

The film has been released several times on DVD in the United States. The first release, by Simitar Entertainment, was on May 6, 1998 in a fullscreen version under the title "The Terror of Godzilla". The second release, by First Classic Media and distributed by Sony Music Entertainment, was on September 17, 2002. It was released both individually and as part of the "Ultimate Godzilla DVD Collection" box set, the latter being released on the same day.

It was then re-released by Second Classic Media, this time distributed by Genius Entertainment, on November 20, 2007 both individually and as part of the "Godzilla Collection" box set on April 29, 2008.

In 2019, both the Japanese version and the export English version were included in a Blu-ray box set released by the Criterion Collection, which included all 15 films from the franchise's Shōwa era.





</doc>
<doc id="12002" url="https://en.wikipedia.org/wiki?curid=12002" title="Godzilla vs. King Ghidorah">
Godzilla vs. King Ghidorah

The production crew of "Godzilla vs. King Ghidorah" remained largely unchanged from that of the previous film in the series, "Godzilla vs. Biollante". Because the previous installment was a box office disappointment, due to a lack of child viewership and alleged competition with the "Back to the Future" franchise, the producers of "Godzilla vs. King Ghidorah" were compelled to create a film with more fantasy elements, along with time travel.

"Godzilla vs. King Ghidorah" was the first "Godzilla" film since 1975's "Terror of Mechagodzilla" to feature a newly orchestrated score by Akira Ifukube. The film was released theatrically in Japan on December 14, 1991, and was followed by "Godzilla vs. Mothra" the following year. It was released direct-to-video in North America in 1998 by Columbia TriStar Home Entertainment. Though "Godzilla vs. King Ghidorah" was more financially successful than "Godzilla vs. Biollante", the film became somewhat controversial in the United States as a result of perceived anti-Americanism stemming from a scene where Godzilla kills several American soldiers.

In 1992, science fiction writer Kenichiro Terasawa is writing a book about Godzilla and learns of a group of Japanese soldiers stationed on Lagos Island during the Gilbert and Marshall Islands campaign. In February 1944, while threatened by American soldiers, the Japanese soldiers were saved by a mysterious dinosaur. He theorizes that the dinosaur was subsequently mutated into Godzilla in 1954 after a hydrogen bomb test on the island. Yasuaki Shindo, a wealthy businessman who commanded the Japanese soldiers on Lagos Island, confirms that the dinosaur did indeed exist.

Meanwhile, a UFO lands on Mount Fuji. When the Japanese army investigates, they are greeted by Wilson, Grenchiko, Emmy Kano and the android M-11. The visitors, known as the "Futurians", explain that they are humans from the year 2204, where Godzilla has completely destroyed Japan. The Futurians plan to travel back in time to 1944 and remove the dinosaur from Lagos Island before the island is irradiated in 1954, thus preventing the mutation of the creature into Godzilla. As proof of their story, Emmy presents a copy of Terasawa's book, which has not yet been completed in the present.

The Futurians, Terasawa, Miki Saegusa, and Professor Mazaki, board a time shuttle and travel back to 1944 to Lagos Island. There, as American forces land and engage the Japanese forces commanded by Shindo, the dinosaur attacks and kills the American soldiers. The American navy then bombs the dinosaur from the sea and gravely wounds it. After Shindo and his men leave the island, M-11 teleports the dinosaur from Lagos Island to the Bering Strait. Before returning to 1992, the Futurians secretly leave three small creatures called Dorats on Lagos Island, which are exposed to radiation from the hydrogen bomb test in 1954 and merge to become King Ghidorah, which then appears in present-day Japan. After returning to 1992, the Futurians use King Ghidorah to subjugate Japan and issue an ultimatum, but Japan refuses to surrender.

Feeling sympathy for the Japanese people, Emmy reveals to Terasawa the truth behind the Futurians' mission: in the future, Japan is an economic superpower that has surpassed the United States, Russia, and China. The Futurians traveled back in time in order to change history and prevent Japan's future economic dominance by creating King Ghidorah and using it to destroy present day Japan. At the same time, they also planned to erase Godzilla from history so it would not pose a threat to their plans. After M-11 brings Emmy back to the UFO, she reprograms the android so it will help her.

Terasawa discovers that a Russian nuclear submarine sank in the Bering Strait in the 1970s and released enough radiation to mutate the dinosaur into Godzilla. Shindo plans to use his nuclear submarine to rejuvenate Godzilla. En route to the Bering Strait, Shindo's submarine is destroyed by Godzilla, who absorbs its radiation and becomes larger and more powerful. Godzilla arrives in Japan and is met by King Ghidorah. They fight at equal strength, each immune to the other's attacks. With M-11 and Terasawa's aid, Emmy sabotages the UFO's control over King Ghidorah, causing the three-headed monster to lose focus during the battle. Godzilla eventually ends the battle by blasting off Ghidorah's middle head. Before sending King Ghidorah crashing into the ocean, Godzilla destroys the UFO, killing Wilson and Grenchiko before turning its attention on Tokyo, destroying the city and killing Shindo.

Emmy travels to the future with M-11 and returns to the present day with Mecha-King Ghidorah, a cybernetic version of King Ghidorah. The cybernetic Ghidorah blasts Godzilla with energy beams, which proves useless. Godzilla then counters by relentlessly blasting Ghidorah with its atomic breath before Ghidorah launches clamps to restrain Godzilla. Ghidorah carries Godzilla out of Japan, but Godzilla breaks from its restraints and causes Ghidorah to send both crashing into the ocean. Emmy then returns to the future but not before informing Terasawa that she is his descendant.

At the bottom of the ocean, Godzilla awakens and roars over Ghidorah's remains before swimming away.

Although the previously filmed "Godzilla vs. Biollante" had been the most expensive "Godzilla" film produced at the time, its low audience attendance and loss of revenue convinced executive producer and "Godzilla" series creator Tomoyuki Tanaka to revitalize the series by bringing back iconic monsters from pre-1984 "Godzilla" movies, specifically Godzilla's archenemy King Ghidorah.

"Godzilla vs. Biollante" director and writer Kazuki Ōmori had initially hoped to start a standalone series centered on Mothra, and was in the process of rewriting a 1990 script for the unrealized film "Mothra vs. Bagan". The film was ultimately scrapped by Toho, under the assumption that, unlike Godzilla, Mothra would have been a difficult character to market overseas. The planning stages for a sequel to "Godzilla vs. Biollante" were initially hampered by Tanaka's deteriorating health, thus prompting the takeover of Shōgo Tomiyama as producer. The new producer felt that the financial failure of "Godzilla vs. Biollante" was due to the plot being too sophisticated for child audiences, and thus intended to return some of the fantasy elements of the pre-1984 "Godzilla" films to the series. Ōmori himself blamed the lackluster performance of "Godzilla vs. Biollante" on competition with "Back to the Future Part II", and thus concluded that audiences wanted plots involving time travel. His approach to the film also differed from "Godzilla vs. Biollante" in his greater emphasis on developing the personalities of the monsters rather than the human characters.

Akira Ifukube agreed to compose the film's score on the insistence of his daughter, after as he was dissatisfied with the way his compositions had been treated in "Godzilla vs. Biollante".

The Godzilla suits used in "Godzilla vs. Biollante" were reused in "Godzilla vs. King Ghidorah", though with slight modifications. The original suit used for land-based and full body shots had its head replaced with a wider and flatter one, and the body cut in half. The upper half was used in scenes where Godzilla emerges from the sea and during close-ups during the character's first fight with King Ghidorah. The suit used previously for scenes set at sea was modified with rounder shoulders, a more prominent chest, and an enhanced face, and was used throughout the majority of the film's Godzilla scenes.

The redesigned King Ghidorah featured much more advanced wirework puppetry than its predecessors, and effects team leader Koichi Kawakita designed the "Godzillasaurus" as a more paleontologically accurate-looking dinosaur than Godzilla itself as a nod to American filmmakers aspiring to direct their own "Godzilla" films with the intention of making the monster more realistic. Ōmori's original draft specified that the dinosaur that would become Godzilla was a "Tyrannosaurus", though this was rejected by creature designer Shinji Nishikawa, who stated that he "couldn't accept that a tyrannosaur could become Godzilla". The final suit combined features of "Tyrannosaurus" with Godzilla, and real octopus blood was used during the bombardment scene. Because the Godzillasaurus' arms were much smaller than Godzilla's, suit performer Wataru Fukuda had to operate them with levers within the costume. The creature's distress calls were recycled Gamera cries.

The Columbia/TriStar Home Video DVD version was released in 1998 as a single disc double feature with "Godzilla vs. Mothra". The picture was full frame (1.33:1) [NTSC] and the audio in English (2.0). There were no subtitles. Extras included the trailer for "Godzilla vs. King Ghidorah" and "Godzilla vs. Mothra".

The Sony Blu-ray version was released on May 6, 2014 as a two-disc double feature with "Godzilla vs. Mothra". The picture was MPEG-4 AVC (1.85:1) [1080p] and the audio was in Japanese and English (DTS-HD Master Audio 2.0). Subtitles were added in English, English SDH and French. Extras included the theatrical trailer and three teasers in HD with English subtitles.

Joseph Savitski of "Beyond Hollywood" said "This entry in the popular monster series is a disappointing and flawed effort unworthy of the “Godzilla” name."

The film was considered controversial at the time of its release, being contemporary to a period of between America and Japan, but mainly due to its fictional World War II depictions. Gerald Glaubitz of the Pearl Harbor Survivors Association appeared alongside director Kazuki Ōmori on "Entertainment Tonight" and condemned the film as being in "very poor taste" and detrimental to American-Japanese relations, allegations that Ōmori denied, stating that the American extras in the film had been "happy about being crushed and squished by Godzilla." Ishirō Honda also criticized Ōmori, stating that the scene in question went "too far".



</doc>
<doc id="12003" url="https://en.wikipedia.org/wiki?curid=12003" title="Godzilla vs. Mothra">
Godzilla vs. Mothra

Originally conceived as a standalone Mothra film entitled "Mothra vs. Bagan", the film is notable for its return to a more fantasy-based, family-oriented atmosphere, evocative of older "Godzilla" films. Although he did not return as director, Ōmori continued his trend of incorporating Hollywood elements into his screenplay, in this case nods to the "Indiana Jones" franchise.

"Godzilla vs. Mothra" was released theatrically in Japan on December 12, 1992, and was followed by "Godzilla vs. Mechagodzilla II" the following year. "Godzilla vs. Mothra" was released direct-to-video in the United States in 1998 by Columbia Tristar Home Video under the title "Godzilla and Mothra: The Battle for Earth". The film was the second highest-grossing film in Japan in 1993, with "Jurassic Park" being the highest-grossing.

In mid-1992, following the events of "Godzilla vs. King Ghidorah", a meteoroid crashes in the Ogasawara Trench and awakens Godzilla. Six months later, explorer Takuya Fujito is detained after stealing an ancient artifact. Later, a representative of the Japanese Prime Minister offers to have Takuya's charges dropped if he explores Infant Island with his ex-wife, Masako Tezuka and Kenji Ando, the secretary of the rapacious Marutomo company. After the trio arrives on the island, they find a cave containing a depiction of two giant insects in battle. Further exploration leads them to a giant egg and a pair of diminutive humanoids called Cosmos, who identify the egg as belonging to Mothra.

The Cosmos tell of an ancient civilization that tried to control the Earth's climate, thus provoking the Earth into creating Battra, which became uncontrollable, and started to harm the very planet that created him. Mothra, another earth protector, fought an apocalyptic battle with Battra, who eventually lost. The Cosmos explain how the meteoroid uncovered Mothra's egg, and may have awoken Battra, who is still embittered over humanity's interference in the Earth's natural order.

The Marutomo company sends a freighter to Infant Island to pick up the egg, ostensibly to protect it. As they are sailing, Godzilla surfaces and heads toward the newly hatched Mothra larva. Battra, also as a larva, soon appears and joins the fight, allowing Mothra to retreat. The battle between Godzilla and Battra is eventually taken underwater, where the force of the battle causes a giant crack on the Philippine Sea Plate that swallows the two.

Masako and Takuya later discover Ando's true intentions when he kidnaps the Cosmos and takes them to Marutomo headquarters, where the CEO intends to use them for publicity purposes. Mothra enters Tokyo in an attempt to rescue the Cosmos, but is attacked by the JSDF. The wounded Mothra heads for the National Diet Building and starts constructing a cocoon around herself. Meanwhile, Godzilla surfaces from Mount Fuji.

Both Mothra and Battra attain their imago forms and converge at Yokohama Cosmo World. Godzilla interrupts the battle and attacks Battra. The two moths decide to join forces against Godzilla. Eventually, Mothra and Battra overwhelm Godzilla and carry him over the ocean. Godzilla bites Battra's neck and fires his atomic breath into the wound, killing Battra. A tired Mothra drops Godzilla and the lifeless Battra into the water below, sealing Godzilla below the surface by creating a mystical glyph with scales from her wings. The next morning, the Cosmos explain that Battra had been waiting many years to destroy an even larger meteoroid that would threaten the Earth in 1999. Mothra had promised she would stop the future collision if Battra were to die, and she and the Cosmos leave Earth as the humans bid farewell.

The idea of shooting a movie featuring a revamped Mothra dated back to a screenplay written in 1990 by Akira Murao entitled "Mothra vs. Bagan", which revolved around a vengeful dragon called Bagan who sought to destroy humanity for its abuse of the Earth's resources, only to be defeated by Mothra, the goddess of peace. The screenplay was revised by Kazuki Ōmori after the release of "Godzilla vs. Biollante", though the project was ultimately scrapped by Toho, under the assumption that Mothra was a character born purely out of Japanese culture, and thus would have been difficult to market overseas unlike the more internationally recognized Godzilla.

After the success of "Godzilla vs. King Ghidorah", producer Shōgo Tomiyama and "Godzilla" series creator Tomoyuki Tanaka proposed resurrecting King Ghidorah in a film entitled "Ghidorah's Counterattack", but relented when polls demonstrated that Mothra was more popular with women, who comprised the majority of Japan's population. Tomiyama replaced Ōmori with Takao Okawara as director, but maintained Ōmori as screenwriter. Hoping to maintain as much of "Mothra vs. Bagan" as possible, Ōmori reconceptualized Bagan as Badora, a dark twin to Mothra. The character was later renamed Battra (a portmanteau of "battle" and "Mothra"), as the first name was disharmonious in Japanese. Tomiyama had intended to feature "Mothra" star Frankie Sakai, but was unable to because of scheduling conflicts. The final battle between Godzilla, Mothra and Battra was originally meant to have a more elaborate conclusion; as in the final product, Godzilla would have been transported to sea, only to kill Battra and plunge into the ocean. However, the site of their fall would have been the submerged, Stonehenge-like ruins of the Cosmos civilization, which would have engulfed and trapped Godzilla with a forcefield activated by Mothra.

Ishirō Honda, who directed the first "Godzilla" film and many others, visited the set shortly before dying.

Koichi Kawakita continued his theme of giving Godzilla's opponents the ability to metamorphose, and had initially intended to have Mothra killed off, only to be reborn as the cybernetic moth "MechaMothra", though this was scrapped early in production, thus making "Godzilla vs. Mothra" the first post-1984 "Godzilla" movie to not feature a mecha contraption. The underwater scenes were filmed through an aquarium filled with fish set between the performers and the camera. Kawakita's team constructed a new Godzilla suit from previously used molds, though it was made slimmer than previous suits, the neck given more prominent ribbing, and the arrangement of the character's dorsal plates was changed so that the largest plate was placed on the middle of the back. The arms were more flexible at the biceps, and the face was given numerous cosmetic changes; the forehead was reduced and flattened, the teeth scaled down, and the eyes given a golden tint. The head was also electronically modified to allow more vertical mobility. Filming the Godzilla scenes was hampered when the suit previously used for "Godzilla vs. Biollante" and "Godzilla vs. King Ghidorah", which was needed for some stunt-work, was stolen from Toho studios, only to be recovered at Lake Okutama in bad condition. The remains of the suit were recycled for the first battle sequence. Godzilla's roar was reverted to the high-pitched shriek from pre-1984 "Godzilla" films, while Battra's sound effects were recycled from those of Rodan. In designing Battra, which the script described as a "black Mothra", artist Shinji Nishikawa sought to distance its design from Mothra's by making its adult form more similar to its larval one than is the case with Mothra, and combining Mothra's two eyes into one.

"Godzilla vs. Mothra" was released in Japan on December 12, 1992 where it was distributed by Toho. The film sold approximately 4,200,000 tickets in Japan, becoming the number one Japanese film on the domestic market in the period that included the year 1993. It earned ¥2.22 billion in distribution income, and grossed in total.

The film was released in the United States as "Godzilla and Mothra: The Battle for Earth" on April 28, 1998 on home video by Columbia TriStar Home Video.

Review aggregation website Rotten Tomatoes has a 75% approval rating from critics, based on 8 reviews with an average score of 6.3/10. Ed Godziszewski of Monster Zero said, "Rushed into production but a few months after "Godzilla vs. King Ghidorah", this film is unable to hide its hurried nature [but] effects-wise, the film makes up for the story’s shortcomings and then some." Japan Hero said, "While this movie is not the best of the Heisei series, it is still a really interesting movie. The battles are cool, and Battra was an interesting idea. If you have never seen this movie, I highly recommend it."

Stomp Tokyo said the film is "one of the better "Godzilla" movies in that the scenes in which monsters do not appear actually make some sort of sense. And for once, they are acted with some gusto, so that we as viewers can actually come to like the characters on screen, or at least be entertained by them." Mike Bogue of American Kaiju said the film "[does] not live up to its potential," but added that "[its] colorful and elaborate spectacle eventually won [him] over" and "the main story thread dealing with the eventual reconciliation of the divorced couple adequately holds the human plot together."

The film was released by Sony on Blu-ray in "The Toho Godzilla Collection" on May 6, 2014.




</doc>
