<doc id="19712" url="https://en.wikipedia.org/wiki?curid=19712" title="Methanol">
Methanol

Methanol, also known as methyl alcohol amongst other names, is a chemical with the formula CHOH (a methyl group linked to a hydroxyl group, often abbreviated MeOH). It is a light, volatile, colourless, flammable liquid with a distinctive alcoholic odour similar to that of ethanol.
A polar solvent, methanol acquired the name wood alcohol because it was once produced chiefly by the destructive distillation of wood. Today, methanol is mainly produced industrially by hydrogenation of carbon monoxide.

Methanol consists of a methyl group linked to a hydroxyl group. With more than 20 million tons produced annually, it is used as a precursor to other commodity chemicals, including formaldehyde, acetic acid, methyl tert-butyl ether, as well as a host of more specialised chemicals.
Small amounts of methanol are present in normal, healthy human individuals. One study found a mean of 4.5 ppm in the exhaled breath of test subjects. The mean endogenous methanol in humans of 0.45 g/d may be metabolized from pectin found in fruit; one kilogram of apple produces up to 1.4 g of methanol.

Methanol is produced by anaerobic bacteria and phytoplankton.

Methanol is also found in abundant quantities in star-forming regions of space and is used in astronomy as a marker for such regions. It is detected through its spectral emission lines.

In 2006, astronomers using the MERLIN array of radio telescopes at Jodrell Bank Observatory discovered a large cloud of methanol in space, 288 billion miles (463 billion km) across. In 2016, astronomers detected methanol in a planet-forming disc around the young star TW Hydrae using ALMA radio telescope.

As little as of pure methanol can cause permanent blindness by destruction of the optic nerve. is potentially fatal. The median lethal dose is , "i.e.", 1–2 mL/kg body weight of pure methanol. The reference dose for methanol is 0.5 mg/kg in a day. Toxic effects begin hours after ingestion, and antidotes can often prevent permanent damage. Because of its similarities in both appearance and odor to ethanol (the alcohol in beverages), it is difficult to differentiate between the two; such is also the case with denatured alcohol, adulterated liquors or very low quality alcoholic beverages. 

Methanol is toxic by two mechanisms. First, methanol can be fatal due to effects on the central nervous system, acting as a central nervous system depressant in the same manner as ethanol poisoning. Second, in a process of toxication, it is metabolised to formic acid (which is present as the formate ion) via formaldehyde in a process initiated by the enzyme alcohol dehydrogenase in the liver. Methanol is converted to formaldehyde via alcohol dehydrogenase (ADH) and formaldehyde is converted to formic acid (formate) via aldehyde dehydrogenase (ALDH). The conversion to formate via ALDH proceeds completely, with no detectable formaldehyde remaining. Formate is toxic because it inhibits mitochondrial cytochrome c oxidase, causing hypoxia at the cellular level, and metabolic acidosis, among a variety of other metabolic disturbances.
Outbreaks of methanol poisoning have occurred primarily due to contamination of drinking alcohol. This is more common in the developing world. In 2013 more than 1700 cases nonetheless occurred in the United States. Those affected are often adult men. Outcomes may be good with early treatment. Toxicity to methanol was described as early as 1856.

Because of its toxic properties, methanol is frequently used as a denaturant additive for ethanol manufactured for industrial uses. This addition of methanol exempts industrial ethanol (commonly known as "denatured alcohol" or "methylated spirit") from liquor excise taxation in the US and some other countries.

During the course of the COVID-19 pandemic, the U.S. Food and Drug Administration found a number of hand sanitizer products being sold that were labeled as containing ethanol but tested positive for methanol contamination. Due to the toxic effects of methanol when absorbed through the skin or ingested, in contrast to the relatively safer ethanol, the FDA ordered recalls of such hand sanitizer products containing methanol, and issued an import alert to stop these products from illegally entering the U.S. market.

Methanol is primarily converted to formaldehyde, which is widely used in many areas, especially polymers. The conversion entails oxidation:
Acetic acid can be produced from methanol.
Methanol and isobutene are combined to give methyl "tert"-butyl ether (MTBE). MTBE is a major octane booster in gasoline.

Condensation of methanol to produce hydrocarbons and even aromatic systems is the basis of several technologies related to gas to liquids. These include methanol-to-hydrocarbons (MTH), methanol to gasoline (MTG), and methanol to olefins (MTO), and methanol to propylene (MTP). These conversions are catalyzed by zeolites as heterogeneous catalysts. The MTG process was once commercialized at Motunui in New Zealand.

The European Fuel Quality Directive allows fuel producers to blend up to 3% methanol, with an equal amount of cosolvent, with gasoline sold in Europe. China uses more than 4.5 billion liters of methanol per year as a transportation fuel in low level blends for conventional vehicles, and high level blends in vehicles designed for methanol fuels.

Methanol is the precursor to most simple methylamines, methyl halides, and methyl ethers. Methyl esters are produced from methanol, including the transesterification of fats and production of biodiesel via transesterification.

Methanol is a promising energy carrier because, as a liquid, it is easier to store than hydrogen and natural gas. Its energy density is however low reflecting the fact that it represents partially combusted methane. Its energy density is 15.6 MJ/L, whereas ethanol's is 24 and gasoline's is 33 MJ/L.

Further advantages for methanol is its ready biodegradability and low toxicity. It does not persist in either aerobic (oxygen-present) or anaerobic (oxygen-absent) environments. The half-life for methanol in groundwater is just one to seven days, while many common gasoline components have half-lives in the hundreds of days (such as benzene at 10–730 days). Since methanol is miscible with water and biodegradable, it is unlikely to accumulate in groundwater, surface water, air or soil.

Methanol is occasionally used to fuel internal combustion engines. It burns forming carbon dioxide and water:
One problem with high concentrations of methanol in fuel is that alcohols corrode some metals, particularly aluminium. Methanol fuel has been proposed for ground transportation. The chief advantage of a methanol economy is that it could be adapted to gasoline internal combustion engines with minimum modification to the engines and to the infrastructure that delivers and stores liquid fuel. Its energy density is however only half that of gasoline, meaning that twice the volume of methanol would be required.

Methanol was used as a denaturant (intentional toxin) for ethanol, the product being known as "denatured alcohol" or "methylated spirit". This was commonly used during the Prohibition to discourage consumption of bootlegged liquor, and ended up causing several deaths. These types of practices are illegal in modern times, being considered homicide. 

Methanol is used as a solvent and as an antifreeze in pipelines and windshield washer fluid. Methanol was used as an automobile coolant antifreeze in the early 1900s. As of May 2019, methanol was banned in the EU for use in windscreen washing or defrosting due to its risk of human consumption.

In some wastewater treatment plants, a small amount of methanol is added to wastewater to provide a carbon food source for the denitrifying bacteria, which convert nitrates to nitrogen gas and reduce the nitrification of sensitive aquifers.

Methanol is used as a destaining agent in polyacrylamide gel electrophoresis.

Direct-methanol fuel cells are unique in their low temperature, atmospheric pressure operation, which lets them be greatly miniaturized. This, combined with the relatively easy and safe storage and handling of methanol, may open the possibility of fuel cell-powered consumer electronics, such as laptop computers and mobile phones.

Methanol is also a widely used fuel in camping and boating stoves. Methanol burns well in an unpressurized burner, so alcohol stoves are often very simple, sometimes little more than a cup to hold fuel. This lack of complexity makes them a favorite of hikers who spend extended time in the wilderness. Similarly, the alcohol can be gelled to reduce risk of leaking or spilling, as with the brand "Sterno".

Methanol is mixed with water and injected into high performance diesel and gasoline engines for an increase of power and a decrease in intake air temperature in a process known as water methanol injection.

Carbon monoxide and hydrogen react over a catalyst to produce methanol. Today, the most widely used catalyst is a mixture of copper and zinc oxides, supported on alumina, as first used by ICI in 1966. At 5–10 MPa (50–100 atm) and , the reaction is characterized by high selectivity (>99.8%):

The production of synthesis gas from methane produces three moles of hydrogen for every mole of carbon monoxide, whereas the synthesis consumes only two moles of hydrogen gas per mole of carbon monoxide. One way of dealing with the excess hydrogen is to inject carbon dioxide into the methanol synthesis reactor, where it, too, reacts to form methanol according to the equation:

In terms of mechanism, the process occurs via initial conversion of CO into CO, which is then hydrogenated:
where the HO byproduct is recycled via the water-gas shift reaction
This gives an overall reaction, which is the same as listed above.

The catalytic conversion of methane to methanol is effected by enzymes including methane monooxygenases. These enzymes are mixed-function oxygenases, i.e. oxygenation is coupled with production of water and NAD.

CH + O + NADPH + H → CHOH + HO + NAD

Both Fe- and Cu-dependent enzymes have been characterized. Intense but largely fruitless efforts have been undertaken to emulate this reactivity. Methanol is more easily oxidized than is the feedstock methane, so the reactions tend not to be selective. Some strategies exist to circumvent this problem. Examples include Shilov systems and Fe- and Cu containing zeolites. These systems do not necessarily mimick the mechanisms employed by metalloenzymes, but draw some inspiration from them. Active sites can vary substantially from those known in the enzymes. For example, a dinuclear active site is proposed in the sMMO enzyme, whereas a mononuclear iron (alpha-oxygen) is proposed in the Fe-zeolite.

Methanol is highly flammable. Its vapours are heavier than air, can travel and ignite. Methanol fires should be extinguished with dry chemical, carbon dioxide, water spray or alcohol-resistant foam.

Methanol is available commercially in various purity grades. Commercial methanol is generally classified according to ASTM purity grades A and AA. Methanol for chemical use normally corresponds to Grade AA. In addition to water, typical impurities include acetone and ethanol (which are very difficult to separate by distillation). UV-vis spectroscopy is a convenient method for detecting aromatic impurities. Water content can be determined by the Karl-Fischer titration.

In their embalming process, the ancient Egyptians used a mixture of substances, including methanol, which they obtained from the pyrolysis of wood. Pure methanol, however, was first isolated in 1661 by Robert Boyle, when he produced it via the distillation of buxus (boxwood). It later became known as "pyroxylic spirit". In 1834, the French chemists Jean-Baptiste Dumas and Eugene Peligot determined its elemental composition.

They also introduced the word "methylène" to organic chemistry, forming it from Greek "methy" = "alcoholic liquid" + "hȳlē" = "forest, wood, timber, material". "Methylène" designated a "radical" that was about 14% hydrogen by weight and contained one carbon atom. This would be CH, but at the time carbon was thought to have an atomic weight only six times that of hydrogen, so they gave the formula as CH. They then called wood alcohol (l'esprit de bois) "bihydrate de méthylène" (bihydrate because they thought the formula was CHO = (CH)(HO)). The term "methyl" was derived in about 1840 by back-formation from "methylene", and was then applied to describe "methyl alcohol". This was shortened to "methanol" in 1892 by the International Conference on Chemical Nomenclature. The suffix -yl, which, in organic chemistry, forms names of carbon groups, is from the word "methyl".

In 1923, the German chemists Alwin Mittasch and Mathias Pier, working for Badische-Anilin & Soda-Fabrik (BASF), developed a means to convert synthesis gas (a mixture of carbon monoxide, carbon dioxide, and hydrogen) into methanol. US patent 1,569,775 () was applied for on 4 Sep 1924 and issued on 12 January 1926; the process used a chromium and manganese oxide catalyst with extremely vigorous conditions: pressures ranging from 50 to 220 atm, and temperatures up to 450 °C. Modern methanol production has been made more efficient through use of catalysts (commonly copper) capable of operating at lower pressures. The modern low pressure methanol (LPM) process was developed by ICI in the late 1960s with the technology patent since long expired.

During World War II, methanol was used as a fuel in several German military rocket designs, under the name M-Stoff, and in a roughly 50/50 mixture with hydrazine, known as C-Stoff.

The use of methanol as a motor fuel received attention during the oil crises of the 1970s. By the mid-1990s, over 20,000 methanol "flexible fuel vehicles" (FFV) capable of operating on methanol or gasoline were introduced in the U.S. In addition, low levels of methanol were blended in gasoline fuels sold in Europe during much of the 1980s and early-1990s. Automakers stopped building methanol FFVs by the late-1990s, switching their attention to ethanol-fueled vehicles. While the methanol FFV program was a technical success, rising methanol pricing in the mid- to late-1990s during a period of slumping gasoline pump prices diminished interest in methanol fuels.

In the early 1970s, a process was developed by Mobil for producing gasoline fuel from methanol.

Between the 1960s and 1980s methanol emerged as a precursor to the feedstock chemicals acetic acid and acetic anhydride. These processes include the Monsanto acetic acid synthesis, Cativa process, and Tennessee Eastman acetic anhydride process.





</doc>
<doc id="19714" url="https://en.wikipedia.org/wiki?curid=19714" title="Milk">
Milk

Milk is a white, nutrient-rich liquid food produced in the mammary glands of mammals. It is the primary source of nutrition for infant mammals (including humans who are breastfed) before they are able to digest other types of food. Early-lactation milk contains colostrum, which carries the mother's antibodies to its young and can reduce the risk of many diseases. It contains many other nutrients including protein and lactose. Interspecies consumption of milk is not uncommon, particularly among humans, many of whom consume the milk of other mammals.

As an agricultural product, milk, also called "dairy milk", is extracted from farm animals during or soon after pregnancy. Dairy farms produced about 730 million tonnes of milk in 2011, from 260 million dairy cows. India is the world's largest producer of milk, and is the leading exporter of skimmed milk powder, yet it exports few other milk products. The ever-increasing rise in domestic demand for dairy products and a large demand-supply gap could lead to India being a net importer of dairy products in the future. New Zealand, Germany and the Netherlands are the largest exporters of milk products. China and Russia were the world's largest importers of milk and milk products until 2016 when both countries became self-sufficient, contributing to a worldwide glut of milk.

Throughout the world, more than six billion people consume milk and milk products. Between 750 and 900 million people live in dairy farming households.

The term "milk" comes from "Old English "meoluc" (West Saxon), "milc" (Anglian), from Proto-Germanic *"meluks" "milk" (source also of Old Norse "mjolk", Old Frisian "melok", Old Saxon "miluk", Dutch "melk", Old High German "miluh", German "Milch", Gothic "miluks")".

In food use, from 1961, the term "milk" has been defined under Codex Alimentarius standards as: "the normal mammary secretion of milking animals obtained from one or more milkings without either addition to it or extraction from it, intended for consumption as liquid milk or for further processing." The term "dairy" relates to animal milk and animal milk production.

A substance secreted by pigeons to feed their young is called "crop milk" and bears some resemblance to mammalian milk, although it is not consumed as a milk substitute.

The definition above precludes non-animal products which resemble dairy milk in color and texture, such as almond milk, coconut milk, rice milk, and soy milk. In English, the word "milk" has been used to refer to "milk-like plant juices" since 1200 AD. Traditionally a variety of non-dairy products have been described with the word "milk", including the traditional digestive remedies milk of magnesia and milk of bismuth. Latex, the complex inedible emulsion that exudes from the stems of certain plants, is generally described as "milky" and is often sold as ""rubber milk"" because of its white appearance. The word "latex" itself is deducted from the Spanish word for milk.

A 2018 survey by the International Food Information Council Foundation suggests consumers in the United States do not typically confuse plant-based milk analogues with animal milk and dairy products. In the US, (mostly plant-based) milk alternatives now command 13% of the "milk" market, leading the US dairy industry to attempt, multiple times, to sue producers of dairy milk alternatives, to have the name "milk" limited to animal milk, so far without success. The Food and Drug Administration generally supports restricting the term "milk", while the US Department of Agriculture supports the continued use of terms such as "soymilk". In the European Union, words such as milk, butter, cheese, cream and yogurt are legally restricted to animal products, with exceptions such as "coconut milk", "almond milk", "peanut butter", and "ice cream".

Production of milk substitutes from vats of brewer's yeast is under development by organizations including Impossible Foods, Muufri, and the biohacker group "Real Vegan Cheese". Some components would be chemically identical to those in animal-derived milk; others, such as lactose, to which many people are allergic, may be substituted.

Milk consumption occurs in two distinct overall types: a natural source of nutrition for all infant mammals and a food product obtained from other mammals for consumption by humans of all ages.

In almost all mammals, milk is fed to infants through breastfeeding, either directly or by expressing the milk to be stored and consumed later. The early milk from mammals is called colostrum. Colostrum contains antibodies that provide protection to the newborn baby as well as nutrients and growth factors. The makeup of the colostrum and the period of secretion varies from species to species.

For humans, the World Health Organization recommends exclusive breastfeeding for six months and breastfeeding in addition to other food for up to two years of age or more. In some cultures it is common to breastfeed children for three to five years, and the period may be longer.

Fresh goats' milk is sometimes substituted for breast milk, which introduces the risk of the child developing electrolyte imbalances, metabolic acidosis, megaloblastic anemia, and a host of allergic reactions.

In many cultures, especially in the West, humans continue to consume milk beyond infancy, using the milk of other mammals (especially cattle, goats and sheep) as a food product. Initially, the ability to digest milk was limited to children as adults did not produce lactase, an enzyme necessary for digesting the lactose in milk. People therefore converted milk to curd, cheese and other products to reduce the levels of lactose. Thousands of years ago, a chance mutation spread in human populations in Europe that enabled the production of lactase in adulthood. This mutation allowed milk to be used as a new source of nutrition which could sustain populations when other food sources failed. Milk is processed into a variety of products such as cream, butter, yogurt, kefir, ice cream, and cheese. Modern industrial processes use milk to produce casein, whey protein, lactose, condensed milk, powdered milk, and many other food-additives and industrial products.

Whole milk, butter and cream have high levels of saturated fat. The sugar lactose is found only in milk, forsythia flowers, and a few tropical shrubs. The enzyme needed to digest lactose, lactase, reaches its highest levels in the human small intestine after birth and then begins a slow decline unless milk is consumed regularly. Those groups who do continue to tolerate milk, however, often have exercised great creativity in using the milk of domesticated ungulates, not only of cattle, but also sheep, goats, yaks, water buffalo, horses, reindeer and camels. India is the largest producer and consumer of cattle and buffalo milk in the world.

Humans first learned to consume the milk of other mammals regularly following the domestication of animals during the Neolithic Revolution or the development of agriculture. This development occurred independently in several global locations from as early as 9000–7000BC in Mesopotamia to 3500–3000BC in the Americas. People first domesticated the most important dairy animals – cattle, sheep and goats – in Southwest Asia, although domestic cattle had been independently derived from wild aurochs populations several times since. Initially animals were kept for meat, and archaeologist Andrew Sherratt has suggested that dairying, along with the exploitation of domestic animals for hair and labor, began much later in a separate secondary products revolution in the fourth millennium BC. Sherratt's model is not supported by recent findings, based on the analysis of lipid residue in prehistoric pottery, that shows that dairying was practiced in the early phases of agriculture in Southwest Asia, by at least the seventh millennium BC.

From Southwest Asia domestic dairy animals spread to Europe (beginning around 7000 BC but did not reach Britain and Scandinavia until after 4000 BC), and South Asia (7000–5500 BC). The first farmers in central Europe and Britain milked their animals. Pastoral and pastoral nomadic economies, which rely predominantly or exclusively on domestic animals and their products rather than crop farming, were developed as European farmers moved into the Pontic-Caspian steppe in the fourth millennium BC, and subsequently spread across much of the Eurasian steppe. Sheep and goats were introduced to Africa from Southwest Asia, but African cattle may have been independently domesticated around 7000–6000BC. Camels, domesticated in central Arabia in the fourth millennium BC, have also been used as dairy animals in North Africa and the Arabian Peninsula. The earliest Egyptian records of burn treatments describe burn dressings using milk from mothers of male babies. In the rest of the world (i.e., East and Southeast Asia, the Americas and Australia) milk and dairy products were historically not a large part of the diet, either because they remained populated by hunter-gatherers who did not keep animals or the local agricultural economies did not include domesticated dairy species. Milk consumption became common in these regions comparatively recently, as a consequence of European colonialism and political domination over much of the world in the last 500 years.

In the Middle Ages, milk was called the "virtuous white liquor" because alcoholic beverages were safer to consume than water.

The growth in urban population, coupled with the expansion of the railway network in the mid-19th century, brought about a revolution in milk production and supply. Individual railway firms began transporting milk from rural areas to London from the 1840s and 1850s. Possibly the first such instance was in 1846, when St Thomas's Hospital in Southwark contracted with milk suppliers outside London to ship milk by rail. The Great Western Railway was an early and enthusiastic adopter, and began to transport milk into London from Maidenhead in 1860, despite much criticism. By 1900, the company was transporting over 25 million gallons annually. The milk trade grew slowly through the 1860s, but went through a period of extensive, structural change in the 1870s and 1880s.
Urban demand began to grow, as consumer purchasing power increased and milk became regarded as a required daily commodity. Over the last three decades of the 19th century, demand for milk in most parts of the country doubled or, in some cases, tripled. Legislation in 1875 made the adulteration of milk illegal– This combined with a marketing campaign to change the image of milk. The proportion of rural imports by rail as a percentage of total milk consumption in London grew from under 5% in the 1860s to over 96% by the early 20th century. By that point, the supply system for milk was the most highly organized and integrated of any food product.
The first glass bottle packaging for milk was used in the 1870s. The first company to do so may have been the New York Dairy Company in 1877. The Express Dairy Company in England began glass bottle production in 1880. In 1884, Hervey Thatcher, an American inventor from New York, invented a glass milk bottle, called "Thatcher's Common Sense Milk Jar," which was sealed with a waxed paper disk. Later, in 1932, plastic-coated paper milk cartons were introduced commercially.

In 1863, French chemist and biologist Louis Pasteur invented pasteurization, a method of killing harmful bacteria in beverages and food products. He developed this method while on summer vacation in Arbois, to remedy the frequent acidity of the local wines. He found out experimentally that it is sufficient to heat a young wine to only about for a brief time to kill the microbes, and that the wine could be nevertheless properly aged without sacrificing the final quality. In honor of Pasteur, the process became known as "pasteurization". Pasteurization was originally used as a way of preventing wine and beer from souring. Commercial pasteurizing equipment was produced in Germany in the 1880s, and producers adopted the process in Copenhagen and Stockholm by 1885.

Continued improvements in the efficiency of milk production led to a worldwide glut of milk by 2016. Russia and China became self-sufficient and stopped importing milk. Canada has tried to restrict milk production by forcing new farmers/increased capacity to "buy in" at C$24,000 per cow. Importing milk is prohibited. The European Union theoretically stopped subsidizing dairy farming in 2015. Direct subsidies were replaced by "environmental incentives" which results in the government buying milk when the price falls to €200 per . The United States has a voluntary insurance program that pays farmers depending upon the price of milk and the cost of feed.

The females of all mammal species can by definition produce milk, but cow's milk dominates commercial production. In 2011, FAO estimates 85% of all milk worldwide was produced from cows. Human milk is not produced or distributed industrially or commercially; however, human milk banks collect donated human breastmilk and redistribute it to infants who may benefit from human milk for various reasons (premature neonates, babies with allergies, metabolic diseases, etc.) but who cannot breastfeed.

In the Western world, cow's milk is produced on an industrial scale and is by far the most commonly consumed form of milk. Commercial dairy farming using automated milking equipment produces the vast majority of milk in developed countries. Dairy cattle such as the Holstein have been bred selectively for increased milk production. About 90% of the dairy cows in the United States and 85% in Great Britain are Holsteins. Other dairy cows in the United States include Ayrshire, Brown Swiss, Guernsey, Jersey and Milking Shorthorn (Dairy Shorthorn).

Aside from cattle, many kinds of livestock provide milk used by humans for dairy products. These animals include water buffalo, goat, sheep, camel, donkey, horse, reindeer and yak. The first four respectively produced about 11%, 2%, 1.4% and 0.2% of all milk worldwide in 2011.

In Russia and Sweden, small moose dairies also exist.

According to the U.S. National Bison Association, American bison (also called American buffalo) are not milked commercially; however, various sources report cows resulting from cross-breeding bison and domestic cattle are good milk producers, and have been used both during the European settlement of North America and during the development of commercial Beefalo in the 1970s and 1980s.

Swine are almost never milked, even though their milk is similar to cow's milk and perfectly suitable for human consumption. The main reasons for this are that milking a sow's numerous small teats is very cumbersome, and that sows can not store their milk as cows can. A few pig farms do sell pig cheese as a novelty item; these cheeses are exceedingly expensive.

In 2012, the largest producer of milk and milk products was India followed by the United States of America, China, Pakistan and Brazil. All 28 European Union members together produced 153.8 million tonnes of milk in 2013, the largest by any politico-economic union.

Increasing affluence in developing countries, as well as increased promotion of milk and milk products, has led to a rise in milk consumption in developing countries in recent years. In turn, the opportunities presented by these growing markets have attracted investments by multinational dairy firms. Nevertheless, in many countries production remains on a small scale and presents significant opportunities for diversification of income sources by small farms. Local milk collection centers, where milk is collected and chilled prior to being transferred to urban dairies, are a good example of where farmers have been able to work on a cooperative basis, particularly in countries such as India.

FAO reports Israel dairy farms are the most productive in the world, with a yield of milk per cow per year. This survey over 2001 and 2007 was conducted by ICAR (International Committee for Animal Recording) across 17 developed countries. The survey found that the average herd size in these developed countries increased from 74 to 99 cows per herd between 2001 and 2007. A dairy farm had an average of 19 cows per herd in Norway, and 337 in New Zealand. Annual milk production in the same period increased from per cow in these developed countries. The lowest average production was in New Zealand at per cow. The milk yield per cow depended on production systems, nutrition of the cows, and only to a minor extent different genetic potential of the animals. What the cow ate made the most impact on the production obtained. New Zealand cows with the lowest yield per year grazed all year, in contrast to Israel with the highest yield where the cows ate in barns with an energy-rich mixed diet.

The milk yield per cow in the United States, the world's largest cow milk producer, was per year in 2010. In contrast, the milk yields per cow in India and China– the second and third largest producers– were respectively and per year.

It was reported in 2007 that with increased worldwide prosperity and the competition of bio-fuel production for feed stocks, both the demand for and the price of milk had substantially increased worldwide. Particularly notable was the rapid increase of consumption of milk in China and the rise of the price of milk in the United States above the government subsidized price. In 2010 the Department of Agriculture predicted farmers would receive an average of $1.35 per U.S. gallon of cow's milk (35 cents per liter), which is down 30 cents per gallon from 2007 and below the break-even point for many cattle farmers.

The consumption of cow's milk poses numerous threats to the natural environment. Compared to plant milks, cow's milk requires the most land and water, and its production results in the greatest amount of greenhouse gas (GHG) emissions, air pollution, and water pollution. A 2010 UN report, "Assessing the Environmental Impacts of Consumption and Production", argued that animal products, including dairy, "in general require more resources and cause higher emissions than plant-based alternatives". It proposed a move away from animal products to reduce environmental damage.

The global water footprint of animal agriculture is 2,422 billion cubic meters of water (one-fourth of the total global water footprint), 19 percent of which is related to dairy cattle. A 2012 study found that 98 percent of milk's footprint can be traced back to the cows food.

A 2010 Food and Agriculture Organization report found that the global dairy sector contributes to four percent of the total global anthropogenic GHG emissions. This figure includes emissions allotted to milk production, processing and transportation, and the emissions from fattening and slaughtering dairy cows. The same report found that 52 percent of the GHGs produced by dairy cattle is methane, and nitrous oxide makes up for another 27 percent of dairy cattle's GHG emission. It is estimated that cows produce between 250 and 500 liters of methane a day. Methane has a heat-trapping potential nearly 100 times larger than carbon dioxide, and nitrous oxide has a global warming potential almost 300 times greater than carbon dioxide.

Milk is an emulsion or colloid of butterfat globules within a water-based fluid that contains dissolved carbohydrates and protein aggregates with minerals. Because it is produced as a food source for the young, all of its contents provide benefits for growth. The principal requirements are energy (lipids, lactose, and protein), biosynthesis of non-essential amino acids supplied by proteins (essential amino acids and amino groups), essential fatty acids, vitamins and inorganic elements, and water.

The pH of milk ranges from 6.4 to 6.8 and it changes over time. Milk from other bovines and non-bovine mammals varies in composition, but has a similar pH.

Initially milk fat is secreted in the form of a fat globule surrounded by a membrane. Each fat globule is composed almost entirely of triacylglycerols and is surrounded by a membrane consisting of complex lipids such as phospholipids, along with proteins. These act as emulsifiers which keep the individual globules from coalescing and protect the contents of these globules from various enzymes in the fluid portion of the milk. Although 97–98% of lipids are triacylglycerols, small amounts of di- and monoacylglycerols, free cholesterol and cholesterol esters, free fatty acids, and phospholipids are also present. Unlike protein and carbohydrates, fat composition in milk varies widely in the composition due to genetic, lactational, and nutritional factor difference between different species.

Like composition, fat globules vary in size from less than 0.2 to about 15 micrometers in diameter between different species. Diameter may also vary between animals within a species and at different times within a milking of a single animal. In unhomogenized cow's milk, the fat globules have an average diameter of two to four micrometers and with homogenization, average around 0.4 micrometers. The fat-soluble vitamins A, D, E, and K along with essential fatty acids such as linoleic and linolenic acid are found within the milk fat portion of the milk.

Normal bovine milk contains 30–35 grams of protein per liter of which about 80% is arranged in casein micelles. Total proteins in milk represent 3.2% of its composition (nutrition table).

The largest structures in the fluid portion of the milk are "casein micelles": aggregates of several thousand protein molecules with superficial resemblance to a surfactant micelle, bonded with the help of nanometer-scale particles of calcium phosphate. Each casein micelle is roughly spherical and about a tenth of a micrometer across. There are four different types of casein proteins: αs1-, αs2-, β-, and κ-caseins. Most of the casein proteins are bound into the micelles. There are several competing theories regarding the precise structure of the micelles, but they share one important feature: the outermost layer consists of strands of one type of protein, k-casein, reaching out from the body of the micelle into the surrounding fluid. These kappa-casein molecules all have a negative electrical charge and therefore repel each other, keeping the micelles separated under normal conditions and in a stable colloidal suspension in the water-based surrounding fluid.

Milk contains dozens of other types of proteins beside caseins and including enzymes. These other proteins are more water-soluble than caseins and do not form larger structures. Because the proteins remain suspended in whey remaining when caseins coagulate into curds, they are collectively known as "whey proteins". Lactoglobulin is the most common whey protein by a large margin. The ratio of caseins to whey proteins varies greatly between species; for example, it is 82:18 in cows and around 32:68 in humans.

Minerals or milk salts, are traditional names for a variety of cations and anions within bovine milk. Calcium, phosphate, magnesium, sodium, potassium, citrate, and chloride are all included as minerals and they typically occur at concentration of 5–40mM. The milk salts strongly interact with casein, most notably calcium phosphate. It is present in excess and often, much greater excess of solubility of solid calcium phosphate. In addition to calcium, milk is a good source of many other vitamins. Vitamins A, B6, B12, C, D, K, E, thiamine, niacin, biotin, riboflavin, folates, and pantothenic acid are all present in milk.

For many years the most accepted theory of the structure of a micelle was that it was composed of spherical casein aggregates, called submicelles, that were held together by calcium phosphate linkages. However, there are two recent models of the casein micelle that refute the distinct micellular structures within the micelle.

The first theory attributed to de Kruif and Holt, proposes that nanoclusters of calcium phosphate and the phosphopeptide fraction of beta-casein are the centerpiece to micellular structure. Specifically in this view, unstructured proteins organize around the calcium phosphate giving rise to their structure and thus no specific structure is formed.

The second theory proposed by Horne, the growth of calcium phosphate nanoclusters begins the process of micelle formation but is limited by binding phosphopeptide loop regions of the caseins. Once bound, protein-protein interactions are formed and polymerization occurs, in which K-casein is used as an end cap, to form micelles with trapped calcium phosphate nanoclusters.

Some sources indicate that the trapped calcium phosphate is in the form of Ca9(PO4)6;
whereas, others say it is similar to the structure of the mineral brushite CaHPO4 -2H2O.

Milk contains several different carbohydrate including lactose, glucose, galactose, and other oligosaccharides. The lactose gives milk its sweet taste and contributes approximately 40% of whole cow's milk's calories. Lactose is a disaccharide composite of two simple sugars, glucose and galactose. Bovine milk averages 4.8% anhydrous lactose, which amounts to about 50% of the total solids of skimmed milk. Levels of lactose are dependent upon the type of milk as other carbohydrates can be present at higher concentrations than lactose in milks.

Other components found in raw cow's milk are living white blood cells, mammary gland cells, various bacteria, and a large number of active enzymes.

Both the fat globules and the smaller casein micelles, which are just large enough to deflect light, contribute to the opaque white color of milk. The fat globules contain some yellow-orange carotene, enough in some breeds (such as Guernsey and Jersey cattle) to impart a golden or "creamy" hue to a glass of milk. The riboflavin in the whey portion of milk has a greenish color, which sometimes can be discerned in skimmed milk or whey products. Fat-free skimmed milk has only the casein micelles to scatter light, and they tend to scatter shorter-wavelength blue light more than they do red, giving skimmed milk a bluish tint.

In most Western countries, centralized dairy facilities process milk and products obtained from milk, such as cream, butter, and cheese. In the U.S., these dairies usually are local companies, while in the Southern Hemisphere facilities may be run by large multi-national corporations such as Fonterra.

Pasteurization is used to kill harmful pathogenic bacteria by heating the milk for a short time and then immediately cooling it. Types of pasteurized milk include full cream, reduced fat, skim milk, calcium enriched, flavored, and UHT. The standard high temperature short time (HTST) process of 72 °C for 15 seconds completely kills pathogenic bacteria in milk, rendering it safe to drink for up to three weeks if continually refrigerated. Dairies print best before dates on each container, after which stores remove any unsold milk from their shelves.

A side effect of the heating of pasteurization is that some vitamin and mineral content is lost. Soluble calcium and phosphorus decrease by 5%, thiamin and vitamin B12 by 10%, and vitamin C by 20%. Because losses are small in comparison to the large amount of the two B-vitamins present, milk continues to provide significant amounts of thiamin and vitamin B12. The loss of vitamin C is not nutritionally significant, as milk is not an important dietary source of vitamin C.

Microfiltration is a process that partially replaces pasteurization and produces milk with fewer microorganisms and longer shelf life without a change in the taste of the milk. In this process, cream is separated from the skimmed milk and is pasteurized in the usual way, but the skimmed milk is forced through ceramic microfilters that trap 99.9% of microorganisms in the milk (as compared to 99.999% killing of microorganisms in standard HTST pasteurization). The skimmed milk then is recombined with the pasteurized cream to reconstitute the original milk composition.

Ultrafiltration uses finer filters than microfiltration, which allow lactose and water to pass through while retaining fats, calcium and protein. As with microfiltration, the fat may be removed before filtration and added back in afterwards. Ultrafiltered milk is used in cheesemaking, since it has reduced volume for a given protein content, and is sold directly to consumers as a higher protein, lower sugar content, and creamier alternative to regular milk.

Upon standing for 12 to 24 hours, fresh milk has a tendency to separate into a high-fat cream layer on top of a larger, low-fat milk layer. The cream often is sold as a separate product with its own uses. Today the separation of the cream from the milk usually is accomplished rapidly in centrifugal cream separators. The fat globules rise to the top of a container of milk because fat is less dense than water.

The smaller the globules, the more other molecular-level forces prevent this from happening. The cream rises in cow's milk much more quickly than a simple model would predict: rather than isolated globules, the fat in the milk tends to form into clusters containing about a million globules, held together by a number of minor whey proteins. These clusters rise faster than individual globules can. The fat globules in milk from goats, sheep, and water buffalo do not form clusters as readily and are smaller to begin with, resulting in a slower separation of cream from these milks.

Milk often is homogenized, a treatment that prevents a cream layer from separating out of the milk. The milk is pumped at high pressures through very narrow tubes, breaking up the fat globules through turbulence and cavitation. A greater number of smaller particles possess more total surface area than a smaller number of larger ones, and the original fat globule membranes cannot completely cover them. Casein micelles are attracted to the newly exposed fat surfaces.

Nearly one-third of the micelles in the milk end up participating in this new membrane structure. The casein weighs down the globules and interferes with the clustering that accelerated separation. The exposed fat globules are vulnerable to certain enzymes present in milk, which could break down the fats and produce rancid flavors. To prevent this, the enzymes are inactivated by pasteurizing the milk immediately before or during homogenization.

Homogenized milk tastes blander but feels creamier in the mouth than unhomogenized. It is whiter and more resistant to developing off flavors. Creamline (or cream-top) milk is unhomogenized. It may or may not have been pasteurized. Milk that has undergone high-pressure homogenization, sometimes labeled as "ultra-homogenized", has a longer shelf life than milk that has undergone ordinary homogenization at lower pressures.

Ultra Heat Treatment (UHT), is a type of milk processing where all bacteria are destroyed with high heat to extend its shelf life for up to 6 months, as long as the package is not opened. Milk is firstly homogenized and then is heated to 138 degrees Celsius for 1–3seconds. The milk is immediately cooled down and packed into a sterile container. As a result of this treatment, all the pathogenic bacteria within the milk are destroyed, unlike when the milk is just pasteurised. The milk will now keep for up for 6 months if unopened. UHT milk does not need to be refrigerated until the package is opened, which makes it easier to ship and store. But in this process there is a loss of vitamin B1 and vitamin C and there is also a slight change in the taste of the milk.

The composition of milk differs widely among species. Factors such as the type of protein; the proportion of protein, fat, and sugar; the levels of various vitamins and minerals; and the size of the butterfat globules, and the strength of the curd are among those that may vary. For example:

Donkey and horse milk have the lowest fat content, while the milk of seals and whales may contain more than 50% fat.

These compositions vary by breed, animal, and point in the lactation period.

The protein range for these four breeds is 3.3% to 3.9%, while the lactose range is 4.7% to 4.9%.

Milk fat percentages may be manipulated by dairy farmers' stock diet formulation strategies. Mastitis infection can cause fat levels to decline.

Processed cow's milk was formulated to contain differing amounts of fat during the 1950s. One cup (250 mL) of 2%-fat cow's milk contains 285 mg of calcium, which represents 22% to 29% of the daily recommended intake (DRI) of calcium for an adult. Depending on its age, milk contains 8 grams of protein, and a number of other nutrients (either naturally or through fortification) including:

Overall, cow's milk can be useful to improve a diet which is otherwise of poor quality, but for a diet which is otherwise healthy, it is redundant or possibly harmful.

There is no good evidence that drinking milk helps prevent bone fractures, even though the American government recommends it for that purpose.

A 2008 review found evidence suggesting that consumption of milk is effective at promoting muscle growth. Some studies have suggested that conjugated linoleic acid, which can be found in dairy products, is an effective supplement for reducing body fat.

Calcium from dairy products has a greater bioavailability than calcium from certain vegetables, such as spinach, that contain high levels of calcium-chelating agents, but a similar or lesser bioavailability than calcium from low-oxalate vegetables such as kale, broccoli, or other vegetables in the genus "Brassica".

A 2009 meta-analysis concluded that milk consumption is associated with acne.

Lactose, the disaccharide sugar component of all milk, must be cleaved in the small intestine by the enzyme lactase, in order for its constituents, galactose and glucose, to be absorbed. Lactose intolerance is a condition in which people have symptoms due to not enough of the enzyme lactase in the small intestines. Those affected vary in the amount of lactose they can tolerate before symptoms develop. These may include abdominal pain, bloating, diarrhea, gas, and nausea. Severity depends on the amount a person eats or drinks. Those affected are usually able to drink at least one cup of milk without developing significant symptoms, with greater amounts tolerated if drunk with a meal or throughout the day.

Lactose intolerance does not cause damage to the gastrointestinal tract. There are four types: primary, secondary, developmental, and congenital. Primary lactose intolerance is when the amount of lactase decline as people age. Secondary lactose intolerance is due to injury to the small intestine such as from infection, celiac disease, inflammatory bowel disease, or other diseases. Developmental lactose intolerance may occur in premature babies and usually improves over a short period of time. Congenital lactose intolerance is an extremely rare genetic disorder in which little or no lactase is made from birth. When lactose intolerance is due to secondary lactase deficiency, treatment of the underlying disease allows lactase activity to return to normal levels. Lactose intolerance is different from a milk allergy.

The number of people with lactose intolerance is unknown. The number of adults who cannot produce enough lactase in their small intestine varies markedly in different populations. Since lactase's only function is the digestion of lactose in milk, in most mammal species the activity of the enzyme is dramatically reduced after weaning. Within most human populations, however, some individuals have developed, by natural evolution, the ability to maintain throughout their life high levels of lactose in their small intestine, as an adaptation to the consumption of nonhuman milk and dairy products beyond infancy. This ability, which allows them to digest lactose into adulthood, is called lactase persistence. The distribution of people with lactase persistence is not homogeneous in the world. For instance, those people with lactase persistence are more than 90% of the population in North Europe, and as low as 5% in parts of Asia and Africa.

Milk and dairy products have the potential for causing serious infection in newborn infants. Unpasteurized milk and cheeses can promote the growth of "Listeria" bacteria. "Listeria monocytogenes" can also cause serious infection in an infant and pregnant woman and can be transmitted to her infant in utero or after birth. The infection has the potential of seriously harming or even causing the death of a preterm infant, an infant of low or very low birth weight, or an infant with a congenital defect of the immune system. The presence of this pathogen can sometimes be determined by the symptoms that appear as a gastrointestinal illness in the mother. The mother can also acquire infection from ingesting food that contains other animal products such as hot dogs, delicatessen meats, and cheese.

Cow's milk allergy (CMA) is an immunologically mediated adverse reaction, rarely fatal, to one or more cow's milk proteins. 2.2–3.5% of the global infant population are allergic to cow's milk.

Milk must be offered at every meal if a United States school district wishes to get reimbursement from the federal government. A quarter of the largest school districts in the U.S. offer rice or soy milk and almost 17% of all U.S. school districts offer lactose-free milk. Of the milk served in U.S. school cafeterias, 71% is flavored, causing some school districts to propose a ban because flavored milk has added sugars. (Though some flavored milk products use artificial sweeteners instead.) The Boulder, Colorado, school district banned flavored milk in 2009. To keep the consumption up, the school installed a milk dispenser.

The mammary gland is thought to have derived from apocrine skin glands. It has been suggested that the original function of lactation (milk production) was keeping eggs moist. Much of the argument is based on monotremes (egg-laying mammals). The original adaptive significance of milk secretions may have been nutrition or immunological protection. This secretion gradually became more copious and accrued nutritional complexity over evolutionary time.

Tritylodontid cynodonts seem to have displayed lactation, based on their dental replacement patterns.

Since November 1993, recombinant bovine somatotropin (rbST), also called rBGH, has been sold to dairy farmers with FDA approval. Cows produce bovine growth hormone naturally, but some producers administer an additional recombinant version of BGH which is produced through genetically engineered E. coli to increase milk production. Bovine growth hormone also stimulates liver production of insulin-like growth factor 1 (IGF1). The U.S. Food and Drug Administration, the National Institutes of Health and the World Health Organization have reported that both of these compounds are safe for human consumption at the amounts present.

Milk from cows given rBST may be sold in the United States, and the FDA stated that no significant difference has been shown between milk derived from rBST-treated and that from non-rBST-treated cows. Milk that advertises that it comes from cows not treated with rBST, is required to state this finding on its label.

Cows receiving rBGH supplements may more frequently contract an udder infection known as mastitis. Problems with mastitis have led to Canada, Australia, New Zealand, and Japan banning milk from rBST treated cows. Mastitis, among other diseases, may be responsible for the fact that levels of white blood cells in milk vary naturally.

rBGH is also banned in the European Union, for reasons of animal welfare.

Vegans and some other vegetarians do not consume milk for reasons mostly related to animal rights and environmental concerns. They may object to features of dairy farming including the necessity of keeping dairy cows pregnant, the killing of almost all the male offspring of dairy cows (either by disposal soon after birth, for veal production, or for beef), the routine separation of mother and calf soon after birth, other perceived inhumane treatment of dairy cattle, and culling of cows after their productive lives.

Other people who do not drink milk are convinced that milk is not necessary for good health or that it may cause adverse health effects. Several studies have indicated that milk consumption does not result in stronger bones. Other studies have found that milk intake increases the risk of acquiring acne.

Related is the criticism of governments' promotion of milk and cheese consumption, in particular the U.S. First, the science remains unsettled as to the effect(s) of calcium-intake on human iron-processing. In addition, the U.S. government, particularly through school food-programs, motivates the (daily) consumption of milk and, especially growing since the 1970s, cheese. According to Daniel Imhoff and Christina Badracco (2019), since the 1970s the "subsidy programs have steadily supported an oversupply of milk. Although milk consumption has decreased over the same time span [...] Cheese is now the top source of saturated fat in the US diet, contributing almost 9 percent". All United States schools that are a part of the federally funded National School Lunch Act are required by the federal government to provide milk for all students. The Office of Dietary Supplements recommends that healthy adults between ages 19 and 50 get about 1,000 mg of calcium per day.

It is often argued that it is unnatural for humans to drink milk from cows (or other animals) because mammals normally do not drink milk beyond the weaning period, nor do they drink milk from another species.

Milk production is also resource intensive. On a global weighted average, for the production of a given volume of milk, a thousand times as much water has to be used.

Milk products are sold in a number of varieties based on types/degrees of:

Milk preserved by the UHT process does not need to be refrigerated before opening and has a much longer shelf life (six months) than milk in ordinary packaging. It is typically sold unrefrigerated in the UK, U.S., Europe, Latin America, and Australia.

Lactose-free milk can be produced by passing milk over lactase enzyme bound to an inert carrier. Once the molecule is cleaved, there are no lactose ill effects. Forms are available with reduced amounts of lactose (typically 30% of normal), and alternatively with nearly 0%. The only noticeable difference from regular milk is a slightly sweeter taste due to the generation of glucose by lactose cleavage. It does not, however, contain more glucose, and is nutritionally identical to regular milk.

Finland, where approximately 17% of the Finnish-speaking population has hypolactasia, has had "HYLA" (acronym for "hydrolysed lactose") products available for many years. Lactose of low-lactose level cow's milk products, ranging from ice cream to cheese, is enzymatically hydrolysed into glucose and galactose. The ultra-pasteurization process, combined with aseptic packaging, ensures a long shelf life. In 2001, Valio launched a lactose-free milk drink that is not sweet like HYLA milk but has the fresh taste of ordinary milk. Valio patented the chromatographic separation method to remove lactose. Valio also markets these products in Sweden, Estonia, Belgium, and the United States, where the company says ultrafiltration is used.

In the UK, where an estimated 4.7% of the population are affected by lactose intolerance, Lactofree produces milk, cheese, and yogurt products that contain only 0.03% lactose.

To aid digestion in those with lactose intolerance, milk with added bacterial cultures such as "Lactobacillus acidophilus" ("acidophilus milk") and bifidobacteria ("a/B milk") is available in some areas. Another milk with "Lactococcus lactis" bacteria cultures ("cultured buttermilk") often is used in cooking to replace the traditional use of naturally soured milk, which has become rare due to the ubiquity of pasteurization, which also kills the naturally occurring Lactococcus bacteria.

Lactose-free and lactose-reduced milk can also be produced via ultra filtration, which removes smaller molecules such as lactose and water while leaving calcium and proteins behind. Milk produced via these methods has a lower sugar content than regular milk.

In areas where the cattle (and often the people) live indoors, commercially sold milk commonly has vitamin D added to it to make up for lack of exposure to UVB radiation.

Reduced-fat milks often have added vitamin A palmitate to compensate for the loss of the vitamin during fat removal; in the United States this results in reduced fat milks having a higher vitamin A content than whole milk.

Milk often has flavoring added to it for better taste or as a means of improving sales. Chocolate milk has been sold for many years and has been followed more recently by strawberry milk and others. Some nutritionists have criticized flavored milk for adding sugar, usually in the form of high-fructose corn syrup, to the diets of children who are already commonly obese in the U.S.

Due to the short shelf life of normal milk, it used to be delivered to households daily in many countries; however, improved refrigeration at home, changing food shopping patterns because of supermarkets, and the higher cost of home delivery mean that daily deliveries by a milkman are no longer available in most countries.

In Australia and New Zealand, prior to metrication, milk was generally distributed in 1 pint (568mL) glass bottles. In Australia and Ireland there was a government funded "free milk for school children" program, and milk was distributed at morning recess in 1/3 pint bottles. With the conversion to metric measures, the milk industry were concerned that the replacement of the pint bottles with 500mL bottles would result in a 13.6% drop in milk consumption; hence, all pint bottles were recalled and replaced by 600mL bottles. With time, due to the steadily increasing cost of collecting, transporting, storing and cleaning glass bottles, they were replaced by cardboard cartons. A number of designs were used, including a tetrahedron which could be close-packed without waste space, and could not be knocked over accidentally. (slogan: No more crying over spilt milk.) However, the industry eventually settled on a design similar to that used in the United States.

Milk is now available in a variety of sizes in paperboard milk cartons (250 mL, 375 mL, 600 mL, 1 liter and 1.5 liters) and plastic bottles (1, 2 and 3 liters). A significant addition to the marketplace has been "long-life" milk (UHT), generally available in 1 and 2 liter rectangular cardboard cartons. In urban and suburban areas where there is sufficient demand, home delivery is still available, though in suburban areas this is often 3 times per week rather than daily. Another significant and popular addition to the marketplace has been flavored milks; for example, as mentioned above, Farmers Union Iced Coffee outsells Coca-Cola in South Australia. 

In rural India, milk is home delivered, daily, by local milkmen carrying bulk quantities in a metal container, usually on a bicycle. In other parts of metropolitan India, milk is usually bought or delivered in plastic bags or cartons via shops or supermarkets.

The current milk chain flow in India is from milk producer to milk collection agent. Then it is transported to a milk chilling center and bulk transported to the processing plant, then to the sales agent and finally to the consumer.

A 2011 survey by the Food Safety and Standards Authority of India found that nearly 70% of samples had not conformed to the standards set for milk. The study found that due to lack of hygiene and sanitation in milk handling and packaging, detergents (used during cleaning operations) were not washed properly and found their way into the milk. About 8% of samples in the survey were found to have detergents, which are hazardous to health.

In Pakistan, milk is supplied in jugs. Milk has been a staple food, especially among the pastoral tribes in this country.

Since the late 1990s, milk-buying patterns have changed drastically in the UK. The classic milkman, who travels his local milk round (route) using a milk float (often battery powered) during the early hours and delivers milk in 1 pint glass bottles with aluminium foil tops directly to households, has almost disappeared. Two of the main reasons for the decline of UK home deliveries by milkmen are household refrigerators (which lessen the need for daily milk deliveries) and private car usage (which has increased supermarket shopping). Another factor is that it is cheaper to purchase milk from a supermarket than from home delivery. In 1996, more than 2.5 billion liters of milk were still being delivered by milkmen, but by 2006 only 637 million liters (13% of milk consumed) was delivered by some 9,500 milkmen. By 2010, the estimated number of milkmen had dropped to 6,000. Assuming that delivery per milkman is the same as it was in 2006, this means milkmen deliveries now only account for 6–7% of all milk consumed by UK households (6.7 billion liters in 2008/2009).

Almost 95% of all milk in the UK is thus sold in shops today, most of it in plastic bottles of various sizes, but some also in milk cartons. Milk is hardly ever sold in glass bottles in UK shops.

In the United States, glass milk bottles have been replaced mostly with milk cartons and plastic jugs. Gallons of milk are almost always sold in jugs, while half gallons and quarts may be found in both paper cartons and plastic jugs, and smaller sizes are almost always in cartons.

The "half pint" () milk carton is the traditional unit as a component of school lunches, though some companies have replaced that unit size with a plastic bottle, which is also available at retail in 6- and 12-pack size.

Glass milk bottles are now rare. Most people purchase milk in bags, plastic bottles, or plastic-coated paper cartons. Ultraviolet (UV) light from fluorescent lighting can alter the flavor of milk, so many companies that once distributed milk in transparent or highly translucent containers are now using thicker materials that block the UV light.
Milk comes in a variety of containers with local variants:




Practically everywhere, condensed milk and evaporated milk are distributed in metal cans, 250 and 125 mL paper containers and 100 and 200 mL squeeze tubes, and powdered milk (skim and whole) is distributed in boxes or bags.

When raw milk is left standing for a while, it turns "sour". This is the result of fermentation, where lactic acid bacteria ferment the lactose in the milk into lactic acid. Prolonged fermentation may render the milk unpleasant to consume. This fermentation process is exploited by the introduction of bacterial cultures (e.g. "Lactobacilli sp., Streptococcus sp., Leuconostoc sp.", etc.) to produce a variety of fermented milk products. The reduced pH from lactic acid accumulation denatures proteins and causes the milk to undergo a variety of different transformations in appearance and texture, ranging from an aggregate to smooth consistency. Some of these products include sour cream, yogurt, cheese, buttermilk, viili, kefir, and kumis. "See Dairy product" for more information.

Pasteurization of cow's milk initially destroys any potential pathogens and increases the shelf life, but eventually results in spoilage that makes it unsuitable for consumption. This causes it to assume an unpleasant odor, and the milk is deemed non-consumable due to unpleasant taste and an increased risk of food poisoning. In raw milk, the presence of lactic acid-producing bacteria, under suitable conditions, ferments the lactose present to lactic acid. The increasing acidity in turn prevents the growth of other organisms, or slows their growth significantly. During pasteurization, however, these lactic acid bacteria are mostly destroyed.

In order to prevent spoilage, milk can be kept refrigerated and stored between in bulk tanks. Most milk is pasteurized by heating briefly and then refrigerated to allow transport from factory farms to local markets. The spoilage of milk can be forestalled by using ultra-high temperature (UHT) treatment. Milk so treated can be stored unrefrigerated for several months until opened but has a characteristic "cooked" taste. Condensed milk, made by removing most of the water, can be stored in cans for many years, unrefrigerated, as can evaporated milk. The most durable form of milk is powdered milk, which is produced from milk by removing almost all water. The moisture content is usually less than 5% in both drum- and spray-dried powdered milk.

Freezing of milk can cause fat globule aggregation upon thawing, resulting in milky layers and butterfat lumps. These can be dispersed again by warming and stirring the milk. It can change the taste by destruction of milk-fat globule membranes, releasing oxidized flavors.

Milk is used to make yogurt, cheese, ice milk, pudding, hot chocolate and french toast, among many other products. Milk is often added to dry breakfast cereal, porridge and granola. Milk is mixed with ice cream and flavoured syrups in a blender to make milkshakes. Milk is often served in coffee and tea. Frothy steamed milk is used to prepare espresso-based drinks such as cafe latte.

The importance of milk in human culture is attested to by the numerous expressions embedded in our languages, for example, "the milk of human kindness", the expression "there's no use crying over spilt milk" (which means don't "be unhappy about what cannot be undone"), "don't milk the ram" (this means "to do or attempt something futile") and "Why buy a cow when you can get milk for free?" (which means "why pay for something that you can get for free otherwise").

In Greek mythology, the Milky Way was formed after the trickster god Hermes suckled the infant Heracles at the breast of Hera, the queen of the gods, while she was asleep. When Hera awoke, she tore Heracles away from her breast and splattered her breast milk across the heavens. In another version of the story, Athena, the patron goddess of heroes, tricked Hera into suckling Heracles voluntarily, but he bit her nipple so hard that she flung him away, spraying milk everywhere.

In many African and Asian countries, butter is traditionally made from fermented milk rather than cream. It can take several hours of churning to produce workable butter grains from fermented milk.

Holy books have also mentioned milk. The Bible contains references to the "Land of Milk and Honey." In the Qur'an, there is a request to wonder on milk as follows: "And surely in the livestock there is a lesson for you, We give you to drink of that which is in their bellies from the midst of digested food and blood, pure milk palatable for the drinkers" (16-The Honeybee, 66). The Ramadan fast is traditionally broken with a glass of milk and dates.

Abhisheka is conducted by Hindu and Jain priests, by pouring libations on the idol of a deity being worshipped, amidst the chanting of mantras. Usually offerings such as milk, yogurt, ghee, honey may be poured among other offerings depending on the type of abhishekam being performed.

A milksop is an "effeminate spiritless man," an expression which is attested to in the late 14th century. Milk toast is a dish consisting of milk and toast. Its soft blandness served as inspiration for the name of the timid and ineffectual comic strip character Caspar Milquetoast, drawn by H. T. Webster from 1924 to 1952. Thus, the term "milquetoast" entered the language as the label for a timid, shrinking, apologetic person. Milk toast also appeared in Disney's "Follow Me Boys" as an undesirable breakfast for the aging main character Lem Siddons.

To "milk" someone, in the vernacular of many English-speaking countries, is to take advantage of the person, by analogy to the way a farmer "milks" a cow and takes its milk. The word "milk" has had many slang meanings over time. In the 19th century, milk was used to describe a cheap and very poisonous alcoholic drink made from methylated spirits (methanol) mixed with water. The word was also used to mean defraud, to be idle, to intercept telegrams addressed to someone else, and a weakling or "milksop." In the mid-1930s, the word was used in Australia to refer to siphoning gas from a car.

Besides serving as a beverage or source of food, milk has been described as used by farmers and gardeners as an organic fungicide and fertilizer, however, its effectiveness is debated. Diluted milk solutions have been demonstrated to provide an effective method of preventing powdery mildew on grape vines, while showing it is unlikely to harm the plant.

Milk paint is a nontoxic water-based paint. It can be made from milk and lime, generally with pigments added for color. In other recipes, borax is mixed with milk's casein protein in order to activate the casein and as a preservative.
Milk has been used for centuries as a hair and skin treatment. 

A milk bath is a bath taken in milk rather than just water. Often additives such as oatmeal, honey, and scents such as rose, daisies and essential oils are mixed in. Milk baths use lactic acid, an alpha hydroxy acid, to dissolve the proteins which hold together dead skin cells.




</doc>
<doc id="19715" url="https://en.wikipedia.org/wiki?curid=19715" title="Miss Congeniality (film)">
Miss Congeniality (film)

Miss Congeniality is a 2000 American comedy film directed by Donald Petrie, written by Marc Lawrence, Katie Ford, and Caryn Lucas, and starring Sandra Bullock, Michael Caine, Benjamin Bratt, William Shatner, Ernie Hudson, and Candice Bergen.

"Miss Congeniality" was released by Warner Bros. Pictures on December 22, 2000 and was a box office hit grossing $212 million worldwide. Bullock also garnered a Golden Globe Award for Best Actress – Motion Picture Comedy or Musical nomination. A sequel, "", was released on March 24, 2005. The film has gained a cult following since its release.

In 1982, a very young Grace Hart (Bullock) steps into a playground fight to beat up a bully who is threatening a boy she likes. However, the boy feels humiliated at being rescued "by a girl", and rejects her rudely, whereupon she punches him in the nose and leaves to sulk alone. Years later, Gracie is now a tough Special Agent for the FBI. During a sting operation against Russian mobsters, she disobeys her superior's (Hudson) orders in order to save a mob boss who appears to be choking, which causes one of the other agents to be shot. She is demoted to a desk job as punishment.

Soon after, the agency is alerted, via a letter from the notorious domestic terrorist known only as "The Citizen", to a bomb threat at the upcoming 75th annual Miss United States beauty pageant in San Antonio, Texas. Gracie's partner Eric Matthews (Bratt) is put in charge, and he relies on Gracie's suggestions, but he takes credit for them himself. One of Gracie's ideas is to plant an agent undercover at the event. When all possible candidates are deemed unfit, Eric then suggests that Gracie take on that role, replacing Miss New Jersey, who was to be disqualified. Beauty pageant coach Victor Melling (Caine) teaches Gracie how to dress, walk, and behave like a contestant. Though initially appalled, she comes to appreciate Victor's thoroughness. Gracie enters the pageant as "Gracie Lou Freebush", representing New Jersey, and becomes friends with Cheryl Frasier (Burns), who is Miss Rhode Island. As the competition begins, Gracie impresses the judges during the talent competition with her glass harp skills and self-defense techniques.

Several suspects are identified, including the current competition director and former pageant winner Kathy Morningside (Bergen), her assistant Frank Tobin (Monroe), the veteran MC Stan Fields (Shatner), and Cheryl, who has a history of being a radical animal rights activist. Gracie accompanies Cheryl and other contestants as they spend a night partying, where Gracie tries to dig into Cheryl's past, but inadvertently learns from the others that Kathy's past as a pageant contestant is suspect, including the fact that she won after the leading contestant suddenly came down with food poisoning. Gracie comes to believe Kathy is a "Citizen" copycat. When Gracie reports this to Eric and the team, she learns that "The Citizen" has been arrested on an unrelated charge, and because there is no further threat, their supervisor has pulled the mission. Gracie insists that she suspects something is wrong, and Eric returns to Texas to help her continue the investigation against orders.

In the final round, Gracie is stunned when she is named first runner up. Cheryl is named Miss United States, but as she goes to accept the tiara, Gracie realizes that Frank, who is actually Kathy's son, impersonated "The Citizen" to make the pageant bomb threat. She tries to tell Victor, however as he misunderstands her, just brightens her remarks with it. That has her then constantly attempts to whisper to Eric as she gets up on stage, but he isn't able to grasp her statements as well. She then throws the tiara up at the stage scenery, where it explodes. As Kathy and Frank are arrested, Gracie determines that the two wanted to kill the pageant winner on stage as revenge for Kathy's termination from the Miss United States organization. As the event closes down and Gracie and Eric prepare to return to headquarters with a newfound interest in each other, the other contestants name Gracie as "Miss Congeniality".

Ellen DeGeneres claims that the writer was inspired when watching her training to walk in high heels and a dress in preparation for the Oscars.

The story is set in New York City and San Antonio. Scenes showing the exterior of the St. Regis Hotel, as well as a few street scenes, were shot on location in New York, and the Alamo and River Walk scenes were shot on location in San Antonio. The majority of the film was shot in Austin, Texas: scenes depicting the interior of the St. Regis were shot in Austin's Driskill Hotel; the pageant scenes were shot at the Bass Concert Hall at the University of Texas at Austin; and scenes depicting the pageant contestants in their hotel rooms were shot in the Omni Austin at South Park.

The film was the fifth highest-grossing film in North America on its opening weekend, making $13.9 million USD. It had a 5% increase in earnings the following week—enough to make the film reach #3. Overall it was a box office hit, grossing more than $106 million in the United States, and more than $212 million worldwide.

On Rotten Tomatoes the film has an approval rating of 42% based on review from 115 critics. The critical consensus reads: "Though critics say Bullock is funny and charming, she can't overcome a bad script that makes the movie feel too much like a fluffy, unoriginal sitcom." On Metacritic the film has a score of 43 out of 100, based on reviews from 20 critics, indicating "mixed or average reviews". Audiences surveyed by CinemaScore gave the film a grade A-.

A. O. Scott of "The New York Times" described it as "a standard-issue fish-out-of-water comedy" which "seems happily, deliberately second-rate, as if its ideal audience consisted of weary airline passengers". Roger Ebert for the "Chicago Sun-Times" wrote: "It isn't bad so much as it lacks any ambition to be more than it so obviously is" although he had some praise for Sandra Bullock's performance.
It was nominated for several awards, including two Golden Globes: Sandra Bullock earned a nod for Best Performance by an Actress in a Motion Picture - Comedy/Musical, and Bosson's "One in a Million" was nominated for Best Original Song in a Motion Picture.

The film's first DVD edition, released in 2001, included two audio commentaries, some deleted scenes, the theatrical trailer, and two documentaries about the making of the film. A deluxe-edition DVD, released in 2005, featured different cover art and contained the same features as the other DVD version plus a quiz hosted by William Shatner and a sneak peek at the upcoming sequel. In 2009, a double feature edition was released that included the .

A sequel, "", was released on March 24, 2005. The film starred Sandra Bullock, Regina King, Enrique Murciano, William Shatner, Ernie Hudson, Heather Burns, Diedrich Bader, and Treat Williams. The sequel was less successful both critically and commercially, earning only $101.3 million.




</doc>
<doc id="19716" url="https://en.wikipedia.org/wiki?curid=19716" title="Magnetism">
Magnetism

Magnetism is a class of physical phenomena that are mediated by magnetic fields. Electric currents and the magnetic moments of elementary particles give rise to a magnetic field, which acts on other currents and magnetic moments. Magnetism is one aspect of the combined phenomenon of electromagnetism. The most familiar effects occur in ferromagnetic materials, which are strongly attracted by magnetic fields and can be magnetized to become permanent magnets, producing magnetic fields themselves. Demagnetizing a magnet is also possible. Only a few substances are ferromagnetic; the most common ones are iron, cobalt and nickel and their alloys. The prefix "" refers to iron, because permanent magnetism was first observed in lodestone, a form of natural iron ore called magnetite, FeO.

All substances exhibit some type of magnetism. Ferromagnetism is responsible for most of the effects of magnetism encountered in everyday life, but there are actually several types of magnetism. Paramagnetic substances, such as aluminum and oxygen, are weakly attracted to an applied magnetic field; diamagnetic substances, such as copper and carbon, are weakly repelled; while antiferromagnetic materials, such as chromium and spin glasses, have a more complex relationship with a magnetic field. The force of a magnet on paramagnetic, diamagnetic, and antiferromagnetic materials is usually too weak to be felt and can be detected only by laboratory instruments, so in everyday life, these substances are often described as non-magnetic.

The magnetic state (or magnetic phase) of a material depends on temperature, pressure, and the applied magnetic field. A material may exhibit more than one form of magnetism as these variables change.

The strength of a magnetic field almost always decreases with distance, though the exact mathematical relationship between strength and distance varies. Different configurations of magnetic moments and electric currents can result in complicated magnetic fields.

Only magnetic dipoles have been observed, although some theories predict the existence of magnetic monopoles.

Magnetism was first discovered in the ancient world, when people noticed that lodestones, naturally magnetized pieces of the mineral magnetite, could attract iron. The word "magnet" comes from the Greek term μαγνῆτις λίθος "magnētis lithos", "the Magnesian stone, lodestone." In ancient Greece, Aristotle attributed the first of what could be called a scientific discussion of magnetism to the philosopher Thales of Miletus, who lived from about 625 BC to about 545 BC. The ancient Indian medical text "Sushruta Samhita" describes using magnetite to remove arrows embedded in a person's body.
In ancient China, the earliest literary reference to magnetism lies in a 4th-century BC book named after its author, "The Sage of Ghost Valley".
The 2nd-century BC annals, "Lüshi Chunqiu", also notes:
"The lodestone makes iron approach, or it attracts it." 
The earliest mention of the attraction of a needle is in a 1st-century work "Lunheng" ("Balanced Inquiries"): "A lodestone attracts a needle." 
The 11th-century Chinese scientist Shen Kuo was the first person to write—in the "Dream Pool Essays"—of the magnetic needle compass and that it improved the accuracy of navigation by employing the astronomical concept of true north.
By the 12th century, the Chinese were known to use the lodestone compass for navigation. They sculpted a directional spoon from lodestone in such a way that the handle of the spoon always pointed south.

Alexander Neckam, by 1187, was the first in Europe to describe the compass and its use for navigation. In 1269, Peter Peregrinus de Maricourt wrote the "Epistola de magnete", the first extant treatise describing the properties of magnets. In 1282, the properties of magnets and the dry compasses were discussed by Al-Ashraf, a Yemeni physicist, astronomer, and geographer.

Leonardo Garzoni's only extant work, the "Due trattati sopra la natura, e le qualità della calamita", is the first known example of a modern treatment of magnetic phenomena. Written in years near 1580 and never published, the treatise had a wide diffusion. In particular, Garzoni is referred to as an expert in magnetism by Niccolò Cabeo, whose Philosophia Magnetica (1629) is just a re-adjustment of Garzoni's work. Garzoni's treatise was known also to Giovanni Battista Della Porta and William Gilbert. 

In 1600, William Gilbert published his "De Magnete, Magneticisque Corporibus, et de Magno Magnete Tellure" ("On the Magnet and Magnetic Bodies, and on the Great Magnet the Earth"). In this work he describes many of his experiments with his model earth called the terrella. From his experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses pointed north (previously, some believed that it was the pole star (Polaris) or a large magnetic island on the north pole that attracted the compass).

An understanding of the relationship between electricity and magnetism began in 1819 with work by Hans Christian Ørsted, a professor at the University of Copenhagen, who discovered by the accidental twitching of a compass needle near a wire that an electric current could create a magnetic field. This landmark experiment is known as Ørsted's Experiment. Several other experiments followed, with André-Marie Ampère, who in 1820 discovered that the magnetic field circulating in a closed-path was related to the current flowing through a surface enclosed by the path; Carl Friedrich Gauss; Jean-Baptiste Biot and Félix Savart, both of whom in 1820 came up with the Biot–Savart law giving an equation for the magnetic field from a current-carrying wire; Michael Faraday, who in 1831 found that a time-varying magnetic flux through a loop of wire induced a voltage, and others finding further links between magnetism and electricity. James Clerk Maxwell synthesized and expanded these insights into Maxwell's equations, unifying electricity, magnetism, and optics into the field of electromagnetism. In 1905, Albert Einstein used these laws in motivating his theory of special relativity, requiring that the laws held true in all inertial reference frames.

Electromagnetism has continued to develop into the 21st century, being incorporated into the more fundamental theories of gauge theory, quantum electrodynamics, electroweak theory, and finally the standard model.

Magnetism, at its root, arises from two sources:
The magnetic properties of materials are mainly due to the magnetic moments of their atoms' orbiting electrons. The magnetic moments of the nuclei of atoms are typically thousands of times smaller than the electrons' magnetic moments, so they are negligible in the context of the magnetization of materials. Nuclear magnetic moments are nevertheless very important in other contexts, particularly in nuclear magnetic resonance (NMR) and magnetic resonance imaging (MRI).

Ordinarily, the enormous number of electrons in a material are arranged such that their magnetic moments (both orbital and intrinsic) cancel out. This is due, to some extent, to electrons combining into pairs with opposite intrinsic magnetic moments as a result of the Pauli exclusion principle (see "electron configuration"), and combining into filled subshells with zero net orbital motion. In both cases, the electrons preferentially adopt arrangements in which the magnetic moment of each electron is canceled by the opposite moment of another electron. Moreover, even when the electron configuration "is" such that there are unpaired electrons and/or non-filled subshells, it is often the case that the various electrons in the solid will contribute magnetic moments that point in different, random directions so that the material will not be magnetic.

Sometimes, either spontaneously, or owing to an applied external magnetic field—each of the electron magnetic moments will be, on average, lined up. A suitable material can then produce a strong net magnetic field.

The magnetic behavior of a material depends on its structure, particularly its electron configuration, for the reasons mentioned above, and also on the temperature. At high temperatures, random thermal motion makes it more difficult for the electrons to maintain alignment.

Diamagnetism appears in all materials and is the tendency of a material to oppose an applied magnetic field, and therefore, to be repelled by a magnetic field. However, in a material with paramagnetic properties (that is, with a tendency to enhance an external magnetic field), the paramagnetic behavior dominates. Thus, despite its universal occurrence, diamagnetic behavior is observed only in a purely diamagnetic material. In a diamagnetic material, there are no unpaired electrons, so the intrinsic electron magnetic moments cannot produce any bulk effect. In these cases, the magnetization arises from the electrons' orbital motions, which can be understood classically as follows:

This description is meant only as a heuristic; the Bohr-van Leeuwen theorem shows that diamagnetism is impossible according to classical physics, and that a proper understanding requires a quantum-mechanical description.

All materials undergo this orbital response. However, in paramagnetic and ferromagnetic substances, the diamagnetic effect is overwhelmed by the much stronger effects caused by the unpaired electrons.

In a paramagnetic material there are "unpaired electrons"; i.e., atomic or molecular orbitals with exactly one electron in them. While paired electrons are required by the Pauli exclusion principle to have their intrinsic ('spin') magnetic moments pointing in opposite directions, causing their magnetic fields to cancel out, an unpaired electron is free to align its magnetic moment in any direction. When an external magnetic field is applied, these magnetic moments will tend to align themselves in the same direction as the applied field, thus reinforcing it.

A ferromagnet, like a paramagnetic substance, has unpaired electrons. However, in addition to the electrons' intrinsic magnetic moment's tendency to be parallel to an applied field, there is also in these materials a tendency for these magnetic moments to orient parallel to each other to maintain a lowered-energy state. Thus, even in the absence of an applied field, the magnetic moments of the electrons in the material spontaneously line up parallel to one another.

Every ferromagnetic substance has its own individual temperature, called the Curie temperature, or Curie point, above which it loses its ferromagnetic properties. This is because the thermal tendency to disorder overwhelms the energy-lowering due to ferromagnetic order.

Ferromagnetism only occurs in a few substances; common ones are iron, nickel, cobalt, their alloys, and some alloys of rare-earth metals.

The magnetic moments of atoms in a ferromagnetic material cause them to behave something like tiny permanent magnets. They stick together and align themselves into small regions of more or less uniform alignment called magnetic domains or Weiss domains. Magnetic domains can be observed with a magnetic force microscope to reveal magnetic domain boundaries that resemble white lines in the sketch. There are many scientific experiments that can physically show magnetic fields.

When a domain contains too many molecules, it becomes unstable and divides into two domains aligned in opposite directions, so that they stick together more stably, as shown at the right.

When exposed to a magnetic field, the domain boundaries move, so that the domains aligned with the magnetic field grow and dominate the structure (dotted yellow area), as shown at the left. When the magnetizing field is removed, the domains may not return to an unmagnetized state. This results in the ferromagnetic material's being magnetized, forming a permanent magnet.

When magnetized strongly enough that the prevailing domain overruns all others to result in only one single domain, the material is magnetically saturated. When a magnetized ferromagnetic material is heated to the Curie point temperature, the molecules are agitated to the point that the magnetic domains lose the organization, and the magnetic properties they cause cease. When the material is cooled, this domain alignment structure spontaneously returns, in a manner roughly analogous to how a liquid can freeze into a crystalline solid.

In an antiferromagnet, unlike a ferromagnet, there is a tendency for the intrinsic magnetic moments of neighboring valence electrons to point in "opposite" directions. When all atoms are arranged in a substance so that each neighbor is anti-parallel, the substance is antiferromagnetic. Antiferromagnets have a zero net magnetic moment, meaning that no field is produced by them. Antiferromagnets are less common compared to the other types of behaviors and are mostly observed at low temperatures. In varying temperatures, antiferromagnets can be seen to exhibit diamagnetic and ferromagnetic properties.

In some materials, neighboring electrons prefer to point in opposite directions, but there is no geometrical arrangement in which "each" pair of neighbors is anti-aligned. This is called a spin glass and is an example of geometrical frustration.

Like ferromagnetism, ferrimagnets retain their magnetization in the absence of a field. However, like antiferromagnets, neighboring pairs of electron spins tend to point in opposite directions. These two properties are not contradictory, because in the optimal geometrical arrangement, there is more magnetic moment from the sublattice of electrons that point in one direction, than from the sublattice that points in the opposite direction.

Most ferrites are ferrimagnetic. The first discovered magnetic substance, magnetite, is a ferrite and was originally believed to be a ferromagnet; Louis Néel disproved this, however, after discovering ferrimagnetism.

When a ferromagnet or ferrimagnet is sufficiently small, it acts like a single magnetic spin that is subject to Brownian motion. Its response to a magnetic field is qualitatively similar to the response of a paramagnet, but much larger.


An electromagnet is a type of magnet in which the magnetic field is produced by an electric current. The magnetic field disappears when the current is turned off. Electromagnets usually consist of a large number of closely spaced turns of wire that create the magnetic field. The wire turns are often wound around a magnetic core made from a ferromagnetic or ferrimagnetic material such as iron; the magnetic core concentrates the magnetic flux and makes a more powerful magnet.

The main advantage of an electromagnet over a permanent magnet is that the magnetic field can be quickly changed by controlling the amount of electric current in the winding. However, unlike a permanent magnet that needs no power, an electromagnet requires a continuous supply of current to maintain the magnetic field.

Electromagnets are widely used as components of other electrical devices, such as motors, generators, relays, solenoids, loudspeakers, hard disks, MRI machines, scientific instruments, and magnetic separation equipment. Electromagnets are also employed in industry for picking up and moving heavy iron objects such as scrap iron and steel. Electromagnetism was discovered in 1820.

As a consequence of Einstein's theory of special relativity, electricity and magnetism are fundamentally interlinked. Both magnetism lacking electricity, and electricity without magnetism, are inconsistent with special relativity, due to such effects as length contraction, time dilation, and the fact that the magnetic force is velocity-dependent. However, when both electricity and magnetism are taken into account, the resulting theory (electromagnetism) is fully consistent with special relativity. In particular, a phenomenon that appears purely electric or purely magnetic to one observer may be a mix of both to another, or more generally the relative contributions of electricity and magnetism are dependent on the frame of reference. Thus, special relativity "mixes" electricity and magnetism into a single, inseparable phenomenon called electromagnetism, analogous to how relativity "mixes" space and time into spacetime.

All observations on electromagnetism apply to what might be considered to be primarily magnetism, e.g. perturbations in the magnetic field are necessarily accompanied by a nonzero electric field, and propagate at the speed of light.

In a vacuum,
where is the vacuum permeability.

In a material,
The quantity is called "magnetic polarization".

If the field is small, the response of the magnetization in a diamagnet or paramagnet is approximately linear:
the constant of proportionality being called the magnetic susceptibility. If so,

In a hard magnet such as a ferromagnet, is not proportional to the field and is generally nonzero even when is zero (see Remanence).

The phenomenon of magnetism is "mediated" by the magnetic field. An electric current or magnetic dipole creates a magnetic field, and that field, in turn, imparts magnetic forces on other particles that are in the fields.

Maxwell's equations, which simplify to the Biot–Savart law in the case of steady currents, describe the origin and behavior of the fields that govern these forces. Therefore, magnetism is seen whenever electrically charged particles are in motion—for example, from movement of electrons in an electric current, or in certain cases from the orbital motion of electrons around an atom's nucleus. They also arise from "intrinsic" magnetic dipoles arising from quantum-mechanical spin.

The same situations that create magnetic fields—charge moving in a current or in an atom, and intrinsic magnetic dipoles—are also the situations in which a magnetic field has an effect, creating a force. Following is the formula for moving charge; for the forces on an intrinsic dipole, see magnetic dipole.

When a charged particle moves through a magnetic field B, it feels a Lorentz force F given by the cross product:

where

Because this is a cross product, the force is perpendicular to both the motion of the particle and the magnetic field. It follows that the magnetic force does no work on the particle; it may change the direction of the particle's movement, but it cannot cause it to speed up or slow down. The magnitude of the force is
where formula_8 is the angle between v and B.

One tool for determining the direction of the velocity vector of a moving charge, the magnetic field, and the force exerted is labeling the index finger "V", the middle finger "B", and the thumb "F" with your right hand. When making a gun-like configuration, with the middle finger crossing under the index finger, the fingers represent the velocity vector, magnetic field vector, and force vector, respectively. See also right-hand rule.

A very common source of magnetic field found in nature is a dipole, with a "South pole" and a "North pole", terms dating back to the use of magnets as compasses, interacting with the Earth's magnetic field to indicate North and South on the globe. Since opposite ends of magnets are attracted, the north pole of a magnet is attracted to the south pole of another magnet. The Earth's North Magnetic Pole (currently in the Arctic Ocean, north of Canada) is physically a south pole, as it attracts the north pole of a compass.
A magnetic field contains energy, and physical systems move toward configurations with lower energy. When diamagnetic material is placed in a magnetic field, a "magnetic dipole" tends to align itself in opposed polarity to that field, thereby lowering the net field strength. When ferromagnetic material is placed within a magnetic field, the magnetic dipoles align to the applied field, thus expanding the domain walls of the magnetic domains.

Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry "magnetic charge" analogous to electric charge. Despite systematic searches since 1931, , they have never been observed, and could very well not exist.

Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.

Certain grand unified theories predict the existence of monopoles which, unlike elementary particles, are solitons (localized energy packets). The initial results of using these models to estimate the number of monopoles created in the Big Bang contradicted cosmological observations—the monopoles would have been so plentiful and massive that they would have long since halted the expansion of the universe. However, the idea of inflation (for which this problem served as a partial motivation) was successful in solving this problem, creating models in which monopoles existed but were rare enough to be consistent with current observations.


Some organisms can detect magnetic fields, a phenomenon known as magnetoception. Some materials in living things are ferromagnetic, though it is unclear if the magnetic properties serve a special function or are merely a byproduct of containing iron. For instance, chitons, a type of marine mollusk, produce magnetite to harden their teeth, and even humans produce magnetite in bodily tissue. Magnetobiology studies the effects of magnetic fields on living organisms; fields naturally produced by an organism are known as biomagnetism. Many biological organisms are mostly made of water, and because water is diamagnetic, extremely strong magnetic fields can repel these living things.

While heuristic explanations based on classical physics can be formulated, diamagnetism, paramagnetism and ferromagnetism can only be fully explained using quantum theory.
A successful model was developed already in 1927, by Walter Heitler and Fritz London, who derived, quantum-mechanically, how hydrogen molecules are formed from hydrogen atoms, i.e. from the atomic hydrogen orbitals formula_9 and formula_10 centered at the nuclei "A" and "B", see below. That this leads to magnetism is not at all obvious, but will be explained in the following.

According to the Heitler–London theory, so-called two-body molecular formula_11-orbitals are formed, namely the resulting orbital is:

Here the last product means that a first electron, r, is in an atomic hydrogen-orbital centered at the second nucleus, whereas the second electron runs around the first nucleus. This "exchange" phenomenon is an expression for the quantum-mechanical property that particles with identical properties cannot be distinguished. It is specific not only for the formation of chemical bonds, but also for magnetism. That is, in this connection the term exchange interaction arises, a term which is essential for the origin of magnetism, and which is stronger, roughly by factors 100 and even by 1000, than the energies arising from the electrodynamic dipole-dipole interaction.

As for the "spin function" formula_13, which is responsible for the magnetism, we have the already mentioned Pauli's principle, namely that a symmetric orbital (i.e. with the + sign as above) must be multiplied with an antisymmetric spin function (i.e. with a − sign), and "vice versa". Thus:

I.e., not only formula_15 and formula_10 must be substituted by "α" and "β", respectively (the first entity means "spin up", the second one "spin down"), but also the sign + by the − sign, and finally r by the discrete values "s" (= ±½); thereby we have formula_17 and formula_18. The "singlet state", i.e. the − sign, means: the spins are "antiparallel", i.e. for the solid we have antiferromagnetism, and for two-atomic molecules one has diamagnetism. The tendency to form a (homoeopolar) chemical bond (this means: the formation of a "symmetric" molecular orbital, i.e. with the + sign) results through the Pauli principle automatically in an "antisymmetric" spin state (i.e. with the − sign). In contrast, the Coulomb repulsion of the electrons, i.e. the tendency that they try to avoid each other by this repulsion, would lead to an "antisymmetric" orbital function (i.e. with the − sign) of these two particles, and complementary to a "symmetric" spin function (i.e. with the + sign, one of the so-called "triplet functions"). Thus, now the spins would be "parallel" (ferromagnetism in a solid, paramagnetism in two-atomic gases).

The last-mentioned tendency dominates in the metals iron, cobalt and nickel, and in some rare earths, which are "ferromagnetic". Most of the other metals, where the first-mentioned tendency dominates, are "nonmagnetic" (e.g. sodium, aluminium, and magnesium) or "antiferromagnetic" (e.g. manganese). Diatomic gases are also almost exclusively diamagnetic, and not paramagnetic. However, the oxygen molecule, because of the involvement of π-orbitals, is an exception important for the life-sciences.

The Heitler-London considerations can be generalized to the Heisenberg model of magnetism (Heisenberg 1928).

The explanation of the phenomena is thus essentially based on all subtleties of quantum mechanics, whereas the electrodynamics covers mainly the phenomenology.



</doc>
<doc id="19719" url="https://en.wikipedia.org/wiki?curid=19719" title="Filter (mathematics)">
Filter (mathematics)

In mathematics, a filter is a special subset of a partially ordered set. Filters appear in order and lattice theory, but can also be found in topology, from where they originate. The dual notion of a filter is an order ideal.

Filters were introduced by Henri Cartan in 1937 and subsequently used by Bourbaki in their book "Topologie Générale" as an alternative to the similar notion of a net developed in 1922 by E. H. Moore and H. L. Smith.

Intuitively, a filter in a partially ordered set ("poset"), "P", is a subset of "P" that includes as members those elements that are large enough to satisfy some given criterion. For example, if "x" is an element of the poset, then the set of elements that are above "x" is a filter, called the principal filter at "x". (If "x" and "y" are incomparable elements of the poset, then neither of the principal filters at "x" and "y" is contained in the other one, and conversely.)

Similarly, a filter on a set contains those subsets that are sufficiently large to contain some given "thing". For example, if the set is the real line and "x" is one of its points, then the family of sets that include "x" in their interior is a filter, called the filter of neighbourhoods of "x". The "thing" in this case is slightly larger than "x", but it still doesn't contain any other specific point of the line.

The above interpretations explain conditions 1 and 3 in the section General definition: Clearly the empty set is not "large enough", and clearly the collection of "large enough" things should be "upward-closed". However, they do not really, without elaboration, explain condition 2 of the general definition. For, why should two "large enough" things contain a "common" "large enough" thing?

Alternatively, a filter can be viewed as a "locating scheme": When trying to locate something (a point or a subset) in the space "X", call a filter the collection of subsets of "X" that might contain "what is looked for". Then this "filter" should possess the following natural structure:

An ultrafilter can be viewed as a "perfect locating scheme" where "each" subset "E" of the space "X" can be used in deciding whether "what is looked for" might lie in "E".

From this interpretation, compactness (see the mathematical characterization below) can be viewed as the property that "no location scheme can end up with nothing", or, to put it another way, "always something will be found".

The mathematical notion of filter provides a precise language to treat these situations in a rigorous and general way, which is useful in analysis, general topology and logic.

A subset of a partially ordered set is a filter if the following conditions hold:


A filter is proper if it is not equal to the whole set . 
This condition is sometimes added to the definition of a filter.

While the above definition is the most general way to define a filter for arbitrary posets, it was originally defined for lattices only. In this case, the above definition can be characterized by the following equivalent statement:
A subset of a lattice is a filter, if and only if it is a non-empty upper set that is closed under finite infima (or meets), i.e., for all , it is also the case that is in "F". 
A subset of is a filter basis if the upper set generated by is all of . 

The smallest filter that contains a given element is a principal filter and is a principal element in this situation. 
The principal filter for is just given by the set formula_1 and is denoted by prefixing with an upward arrow: 

The dual notion of a filter, i.e. the concept obtained by reversing all and exchanging ∧ with ∨, is ideal. 
Because of this duality, the discussion of filters usually boils down to the discussion of ideals. 
Hence, most additional information on this topic (including the definition of maximal filters and prime filters) is to be found in the article on ideals. 
There is a separate article on ultrafilters.

There are two completing definitions of a "filter on a set," both of which require that a filter be a "dual ideal". 
One definition defines "filter" as a synonym of "dual ideal" while the other defines "filter" to mean a dual ideal that is also "proper". 

Given a set , a canonical partial ordering can be defined on the powerset by subset inclusion, turning into a lattice. 
A "dual ideal" is just a filter with respect this partial ordering. 
Note that if then there is exactly one dual ideal on , which is }. 

The article uses the following definition of "filter on a set."

The other definition of "filter on a set" is the original definition of a "filter" given by Henri Cartan, which required that a filter on a set be a dual ideal that does "not" contain the empty set: 

The only non-proper filter on is . 
Much mathematical literature, especially that related to Topology, defines "filter" to mean a "non-degenerate" dual ideal. 


A subset "B" of P("S") is called a prefilter, filter base, or filter basis if is non-empty and the intersection of any two members of includes (as a subset) a member of (" is closed under finite intersection"). 
If the empty set is not a member of , we say is a proper filter base.

Given a filter base , the filter generated or spanned by is defined as the minimum filter containing . 
It is the family of all the subsets of that include a member of . 
Every filter is also a filter base, so the process of passing from filter base to filter may be viewed as a sort of completion.

For every subset of there is a smallest (possibly nonproper) filter containing , called the filter generated or spanned by . 
It is constructed by taking all finite intersections of , which then form a filter base for . 
This filter is proper if and only if every finite intersection of elements of is non-empty, and in that case we say that is a filter subbase. 


If and are two filter bases on , one says is finer than (or that is a refinement of ) if for each , there is a such that . 
If also is finer than , one says that they are equivalent filter bases.


For every filter "F" on a set "S", the set function defined by
is finitely additive—a "measure" if that term is construed rather loosely. Therefore, the statement

can be considered somewhat analogous to the statement that φ holds "almost everywhere". 
That interpretation of membership in a filter is used (for motivation, although it is not needed for actual "proofs") in the theory of ultraproducts in model theory, a branch of mathematical logic.

In topology and analysis, filters are used to define convergence in a manner similar to the role of sequences in a metric space.

In topology and related areas of mathematics, a filter is a generalization of a net. Both nets and filters provide very general contexts to unify the various notions of limit to arbitrary topological spaces.

A sequence is usually indexed by the natural numbers, which are a totally ordered set. Thus, limits in first-countable spaces can be described by sequences. 
However, if the space is not first-countable, nets or filters must be used. Nets generalize the notion of a sequence by requiring the index set simply be a directed set. 
Filters can be thought of as sets built from multiple nets. 
Therefore, both the limit of a filter and the limit of a net are conceptually the same as the limit of a sequence.

Let "X" be a topological space and "x" a point of "X".


Let "X" be a topological space and "x" a point of "X".


Indeed:

(i) implies (ii): if "F" is a filter base satisfying the properties of (i), then the filter associated to "F" satisfies the properties of (ii).

(ii) implies (iii): if "U" is any open neighborhood of "x" then by the definition of convergence "U" contains an element of "F"; since also "Y" is an element of "F", 
"U" and "Y" have non-empty intersection.

(iii) implies (i): Define formula_8. Then "F" is a filter base satisfying the properties of (i).

Let be a topological space and "x" a point of "X".


Let "X" be a topological space.


Let and be topological spaces, let be a filter base on , and let be a function. 
The image of under , denoted by , is defined as the set }, which necessarily forms a filter base on . 


Let formula_9 be a metric space.



</doc>
<doc id="19722" url="https://en.wikipedia.org/wiki?curid=19722" title="Metallurgy">
Metallurgy

Metallurgy is a domain of materials science and engineering that studies the physical and chemical behavior of metallic elements, their inter-metallic compounds, and their mixtures, which are called alloys.
Metallurgy encompasses both the science and the technology of metals. That is, the way in which science is applied to the production of metals, and the engineering of metal components used in products for both consumers and manufacturers. Metallurgy is distinct from the craft of metalworking. Metalworking relies on metallurgy in a similar manner to how medicine relies on medical science for technical advancement. A specialist practitioner of metallurgy is known as a . 

The science of metallurgy is subdivided into two broad categories: chemical metallurgy and physical metallurgy. Chemical metallurgy is chiefly concerned with the reduction and oxidation of metals, and the chemical performance of metals. Subjects of study in chemical metallurgy include mineral processing, the extraction of metals, thermodynamics, electrochemistry, and chemical degradation (corrosion). In contrast, physical metallurgy focuses on the mechanical properties of metals, the physical properties of metals, and the physical performance of metals. Topics studied in physical metallurgy include crystallography, material characterization, mechanical metallurgy, phase transformations, and failure mechanisms. 

Historically, metallurgy has predominately focused on the production of metals. Metal production begins with the processing of ores to extract the metal, and includes the mixture of metals to make alloys. Metal alloys are often a blend of at least two different metallic elements. However, non-metallic elements are often added to alloys in order to achieve properties suitable for an application. The study of metal production is subdivided into ferrous metallurgy (also known as "black metallurgy") and non-ferrous metallurgy (also known as "colored metallurgy").
Ferrous metallurgy involves processes and alloys based on iron while non-ferrous metallurgy involves processes and alloys based on other metals. The production of ferrous metals accounts for 95 percent of world metal production.

Modern metallurgists work in both emerging and traditional areas as part of an interdisciplinary team alongside material scientists, and other engineers. Some traditional areas include mineral processing, metal production, heat treatment, failure analysis, and the joining of metals (including welding, brazing, and soldering). Emerging areas for metallurgists include nanotechnology, superconductors, composites, biomedical materials, electronic materials (semiconductors), and surface engineering.

"Metallurgy" derives from the Ancient Greek , , "worker in metal", from , , "mine, metal" + , , "work".

The word was originally an alchemist's term for the extraction of metals from minerals, the ending "-urgy" signifying a process, especially manufacturing: it was discussed in this sense in the 1797 "Encyclopædia Britannica". In the late 19th century it was extended to the more general scientific study of metals, alloys, and related processes.

In English, the pronunciation is the more common one in the UK and Commonwealth. The pronunciation is the more common one in the US, and is the first-listed variant in various American dictionaries (e.g., "Merriam-Webster Collegiate", "American Heritage").

The earliest recorded metal employed by humans appears to be gold, which can be found free or "native". Small amounts of natural gold have been found in Spanish caves dating to the late Paleolithic period, c. 40,000 BC.
Silver, copper, tin and meteoric iron can also be found in native form, allowing a limited amount of metalworking in early cultures. Egyptian weapons made from meteoric iron in about 3000 BC were highly prized as "daggers from heaven".

Certain metals, notably tin, lead, and at a higher temperature, copper, can be recovered from their ores by simply heating the rocks in a fire or blast furnace, a process known as smelting. The first evidence of this extractive metallurgy, dating from the 5th and 6th millennia BC, has been found at archaeological sites in Majdanpek, Jarmovac near Priboj and Pločnik, in present-day Serbia. To date, the earliest evidence of copper smelting is found at the Belovode site near Plocnik. This site produced a copper axe from 5500 BC, belonging to the Vinča culture.

The earliest use of lead is documented from the late neolithic settlement of Yarim Tepe in Iraq,
"The earliest lead (Pb) finds in the ancient Near East are a 6th millennium BC bangle from Yarim Tepe in northern Iraq and a slightly later conical lead piece from Halaf period Arpachiyah, near Mosul. As native lead is extremely rare, such artifacts raise the possibility that lead smelting may have begun even before copper smelting."
Copper smelting is also documented at this site at about the same time period (soon after 6000 BC), although the use of lead seems to precede copper smelting. Early metallurgy is also documented at the nearby site of Tell Maghzaliyah, which seems to be dated even earlier, and completely lacks that pottery.

The Balkans were the site of major Neolithic cultures, including Butmir, Vinča, Varna, Karanovo, and Hamangia.

The Varna Necropolis, Bulgaria, is a burial site in the western industrial zone of Varna (approximately 4 km from the city centre), internationally considered one of the key archaeological sites in world prehistory. The oldest gold treasure in the world, dating from 4,600 BC to 4,200 BC, was discovered at the site. The gold piece dating from 4,500 BC, recently founded in Durankulak, near Varna is another important example.

Other signs of early metals are found from the third millennium BC in places like Palmela (Portugal), Los Millares (Spain), and Stonehenge (United Kingdom). However, the ultimate beginnings cannot be clearly ascertained and new discoveries are both continuous and ongoing.
In the Near East, about 3500 BC, it was discovered that by combining copper and tin, a superior metal could be made, an alloy called bronze. This represented a major technological shift known as the Bronze Age.

The extraction of iron from its ore into a workable metal is much more difficult than for copper or tin. The process appears to have been invented by the Hittites in about 1200 BC, beginning the Iron Age. The secret of extracting and working iron was a key factor in the success of the Philistines.

Historical developments in ferrous metallurgy can be found in a wide variety of past cultures and civilizations. This includes the ancient and medieval kingdoms and empires of the Middle East and Near East, ancient Iran, ancient Egypt, ancient Nubia, and Anatolia (Turkey), Ancient Nok, Carthage, the Greeks and Romans of ancient Europe, medieval Europe, ancient and medieval China, ancient and medieval India, ancient and medieval Japan, amongst others. Many applications, practices, and devices associated or involved in metallurgy were established in ancient China, such as the innovation of the blast furnace, cast iron, hydraulic-powered trip hammers, and double acting piston bellows.

A 16th century book by Georg Agricola called "De re metallica" describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. Agricola has been described as the "father of metallurgy".

Extractive metallurgy is the practice of removing valuable metals from an ore and refining the extracted raw metals into a purer form. In order to convert a metal oxide or sulphide to a purer metal, the ore must be reduced physically, chemically, or electrolytically.

Extractive metallurgists are interested in three primary streams: feed, concentrate (valuable metal oxide/sulphide) and tailings(waste). After mining, large pieces of the ore feed are broken through crushing or grinding in order to obtain particles small enough where each particle is either mostly valuable or mostly waste. Concentrating the particles of value in a form supporting separation enables the desired metal to be removed from waste products.

Mining may not be necessary, if the ore body and physical environment are conducive to leaching. Leaching dissolves minerals in an ore body and results in an enriched solution. The solution is collected and processed to extract valuable metals.

Ore bodies often contain more than one valuable metal. Tailings of a previous process may be used as a feed in another process to extract a secondary product from the original ore. Additionally, a concentrate may contain more than one valuable metal. That concentrate would then be processed to separate the valuable metals into individual constituents.

Common engineering metals include aluminium, chromium, copper, iron, magnesium, nickel, titanium, zinc, and silicon. These metals are most often used as alloys with the noted exception of silicon. Much effort has been placed on understanding the iron-carbon alloy system, which includes steels and cast irons. Plain carbon steels (those that contain essentially only carbon as an alloying element) are used in low-cost, high-strength applications where neither weight nor corrosion are a major concern. Cast irons, including ductile iron, are also part of the iron-carbon system.

Stainless steel, particularly Austenitic stainless steels, galvanized steel, , titanium alloys, or occasionally copper alloys are used where resistance to corrosion is important. Aluminium alloys and magnesium alloys are commonly used when a lightweight strong part is required such as in automotive and aerospace applications. 

Copper-nickel alloys (such as Monel) are used in highly corrosive environments and for non-magnetic applications. Iron-Manganese-Chromium alloys (Hadfield-type steels) are also used in non-magnetic applications such as directional drilling. Nickel-based superalloys like Inconel are used in high-temperature applications such as gas turbines, turbochargers, pressure vessels, and heat exchangers. For extremely high temperatures, single crystal alloys are used to minimize creep. In modern electronics, high purity single crystal silicon is essential for metal-oxide-silicon transistors (MOS) and integrated circuits. 

In production engineering, metallurgy is concerned with the production of metallic components for use in consumer or engineering products. This involves the production of alloys, the shaping, the heat treatment and the surface treatment of the product. Determining the hardness of the metal using the Rockwell, Vickers, and Brinell hardness scales is a commonly used practice that helps better understand the metal's elasticity and plasticity for different applications and production processes. The task of the metallurgist is to achieve balance between material properties such as cost, weight, strength, toughness, hardness, corrosion, fatigue resistance, and performance in temperature extremes. To achieve this goal, the operating environment must be carefully considered. In a saltwater environment, most ferrous metals and some non-ferrous alloys corrode quickly. Metals exposed to cold or cryogenic conditions may undergo a ductile to brittle transition and lose their toughness, becoming more brittle and prone to cracking. Metals under continual cyclic loading can suffer from metal fatigue. Metals under constant stress at elevated temperatures can creep.

Metals are shaped by processes such as:


Cold-working processes, in which the product's shape is altered by rolling, fabrication or other processes while the product is cold, can increase the strength of the product by a process called work hardening. Work hardening creates microscopic defects in the metal, which resist further changes of shape.

Various forms of casting exist in industry and academia. These include sand casting, investment casting (also called the lost wax process), die casting, and continuous castings. Each of these forms has advantages for certain metals and applications considering factors like magnetism and corrosion.

Metals can be heat-treated to alter the properties of strength, ductility, toughness, hardness and resistance to corrosion. Common heat treatment processes include annealing, precipitation strengthening, quenching, and tempering. The annealing process softens the metal by heating it and then allowing it to cool very slowly, which gets rid of stresses in the metal and makes the grain structure large and soft-edged so that when the metal is hit or stressed it dents or perhaps bends, rather than breaking; it is also easier to sand, grind, or cut annealed metal. Quenching is the process of cooling a high-carbon steel very quickly after heating, thus "freezing" the steel's molecules in the very hard martensite form, which makes the metal harder. There is a balance between hardness and toughness in any steel; the harder the steel, the less tough or impact-resistant it is, and the more impact-resistant it is, the less hard it is. Tempering relieves stresses in the metal that were caused by the hardening process; tempering makes the metal less hard while making it better able to sustain impacts without breaking.

Often, mechanical and thermal treatments are combined in what are known as thermo-mechanical treatments for better properties and more efficient processing of materials. These processes are common to high-alloy special steels, superalloys and titanium alloys.

Electroplating is a chemical surface-treatment technique. It involves bonding a thin layer of another metal such as gold, silver, chromium or zinc to the surface of the product. This is done by selecting the coating material electrolyte solution which is the material that is going to coat the workpiece (gold, silver, zinc). There needs to be two electrodes of different materials: one the same material as the coating material and one that is receiving the coating material. Two electrodes are electrically charged and the coating material is stuck to the work piece. It is used to reduce corrosion as well as to improve the product's aesthetic appearance. It is also used to make inexpensive metals look like the more expensive ones (gold, silver).

Shot peening is a cold working process used to finish metal parts. In the process of shot peening, small round shot is blasted against the surface of the part to be finished. This process is used to prolong the product life of the part, prevent stress corrosion failures, and also prevent fatigue. The shot leaves small dimples on the surface like a peen hammer does, which cause compression stress under the dimple. As the shot media strikes the material over and over, it forms many overlapping dimples throughout the piece being treated. The compression stress in the surface of the material strengthens the part and makes it more resistant to fatigue failure, stress failures, corrosion failure, and cracking.

Thermal spraying techniques are another popular finishing option, and often have better high temperature properties than electroplated coatings.Thermal spraying, also known as a spray welding process, is an industrial coating process that consists of a heat source (flame or other) and a coating material that can be in a powder or wire form which is melted then sprayed on the surface of the material being treated at a high velocity. The spray treating process is known by many different names such as HVOF (High Velocity Oxygen Fuel), plasma spray, flame spray, arc spray, and metalizing.

Metallurgists study the microscopic and macroscopic structure of metals using metallography, a technique invented by Henry Clifton Sorby. In metallography, an alloy of interest is ground flat and polished to a mirror finish. The sample can then be etched to reveal the microstructure and macrostructure of the metal. The sample is then examined in an optical or electron microscope, and the image contrast provides details on the composition, mechanical properties, and processing history.

Crystallography, often using diffraction of x-rays or electrons, is another valuable tool available to the modern metallurgist. Crystallography allows identification of unknown materials and reveals the crystal structure of the sample. Quantitative crystallography can be used to calculate the amount of phases present as well as the degree of strain to which a sample has been subjected.



</doc>
<doc id="19723" url="https://en.wikipedia.org/wiki?curid=19723" title="MUMPS">
MUMPS

MUMPS ("Massachusetts General Hospital Utility Multi-Programming System"), or M, is a general-purpose computer programming language originally designed in 1966 for the healthcare industry. Its differentiating feature is its "built-in" database, enabling high-level access to disk storage using simple symbolic program variables and subscripted arrays; similar to the variables used by most languages to access main memory.

It continues to be used today by many large hospitals and banks to provide high-throughput transaction data processing.

MUMPS was developed by Neil Pappalardo, Robert Greenes, and Curt Marble in Dr. Octo Barnett's animal lab at the Massachusetts General Hospital (MGH) in Boston during 1966 and 1967. It was later rewritten by technical leaders Dennis "Dan" Brevik and Paul Stylos of DEC in 1970 and 1971. Evelyn Dow was the original Marketing representative. 

The original MUMPS system was, like Unix a few years later, built on a spare DEC PDP-7. Octo Barnett and Neil Pappalardo were also involved with MGH's planning for a Hospital Information System, obtained a backward compatible PDP-9, and began using MUMPS in the admissions cycle and laboratory test reporting. MUMPS was then an interpreted language, yet even then, incorporated a hierarchical database file system to standardize interaction with the data and abstract disk operations so they were only done by the MUMPS language itself.

Some aspects of MUMPS can be traced from Rand Corporation's JOSS through BBN's TELCOMP and STRINGCOMP. 

The MUMPS team deliberately chose to include portability between machines as a design goal. 

Another feature, not widely supported for machines of the era, in operating systems or in computer hardware, was multitasking, which was also built into the language itself as evidenced by the "-MPS" in the language name of "MUMPS" stands for Multi-Programming System. Although timesharing on Mainframe computers was increasingly common in systems such as Multics, most mini-computers (and they were the smallest available at that time) did not run parallel programs and threading was not available at all. Even on Mainframes, the variant of batch processing where a program was run to completion was the most common implementation for an operating system of multi-programming.

It was a few years until Unix was developed. The lack of memory management hardware also meant that all multi-processing was fraught with the possibility that a memory pointer could change some other process. MUMPS programs do not have a standard way to refer to memory directly at all, in contrast to C language, so since the multitasking was enforced by the language, not by any program written in the language it was impossible to have the risk that existed for other systems.

Dan Brevik's DEC MUMPS-15 system was adapted to a DEC PDP-15, where it lived for some time. It was first installed at Health Data Management Systems of Denver in May of 1971. The portability proved to be useful and MUMPS was awarded a government research grant, and so MUMPS was released to the public domain which was the a requirement for grants. MUMPS was soon ported to a number of other systems including the popular DEC PDP-8, the Data General Nova and on DEC PDP-11 and the Artronix PC12 minicomputer. Word about MUMPS spread mostly through the medical community, and was in widespread use, often being locally modified for their own needs. 

By the early 1970's, there were many and varied implementations of MUMPS on a range of hardware platforms. Another noteworthy platform was Paul Stylos' DEC MUMPS-11 on the PDP-11, and MEDITECH's MIIS. In the Fall of 1972, many MUMPS users attended a conference in Boston which standardized the then-fractured language, and created the MUMPS Users Group and MUMPS Development Committee (MDC) to do so. These efforts proved successful; a standard was complete by 1974, and was approved, on September 15, 1977, as ANSI standard, X11.1-1977. At about the same time DEC launched DSM-11 (Digital Standard MUMPS) for the PDP-11. This quickly dominated the market, and became the reference implementation of the time. Also, InterSystems sold ISM-11 for the PDP-11 (which was identical to DSM-11).

During the early 1980s several vendors brought MUMPS-based platforms that met the ANSI standard to market. The most significant were:

Other companies developed important MUMPS implementations:

This period also saw considerable MDC activity. The second revision of the ANSI standard for MUMPS (X11.1-1984) was approved on November 15, 1984.



The US Department of Veterans Affairs (formerly the Veterans Administration) was one of the earliest major adopters of the MUMPS language. Their development work (and subsequent contributions to the free MUMPS application codebase) was an influence on many medical users worldwide. In 1995, the Veterans Affairs' patient Admission/Tracking/Discharge system, Decentralized Hospital Computer Program (DHCP) was the recipient of the Computerworld Smithsonian Award for best use of Information Technology in Medicine. In July 2006, the Department of Veterans Affairs (VA) / Veterans Health Administration (VHA) was the recipient of the Innovations in American Government Award presented by the Ash Institute of the John F. Kennedy School of Government at Harvard University for its extension of DHCP into the Veterans Health Information Systems and Technology Architecture (VistA). Nearly the entire VA hospital system in the United States, the Indian Health Service, and major parts of the Department of Defense CHCS hospital system use MUMPS databases for clinical data tracking. In 2015 the Department of Defense awarded a 10 year contract to Leidos, Cerner, and Accenture to replace CHCS. In 2017 the Veterans Health Administration (VHA) announced that it would replace VistA with Cerner by 2024 or 2025. In 2019 the VHA announced that it would also replace its Epic Systems online appointment scheduling system with one from Cerner by 2024.

Other healthcare IT companies using MUMPS include Epic, MEDITECH, GE Healthcare (formerly IDX Systems and Centricity), AmeriPath (part of Quest Diagnostics), Care Centric, Allscripts, Coventry Healthcare, EMIS, and Sunquest Information Systems (formerly Misys Healthcare). Many reference laboratories, such as DASA, Quest Diagnostics, and Dynacare, use MUMPS software written by or based on Antrim Corporation code. Antrim was purchased by Misys Healthcare (now Sunquest Information Systems) in 2001.

MUMPS is also widely used in financial applications. MUMPS gained an early following in the financial sector and is in use at many banks and credit unions. It is used by Ameritrade, the largest online trading service in the US, with over 12 billion transactions per day, as well as by the Bank of England and Barclays Bank, among others.

Since 2005, the use of MUMPS has been either in the form of GT.M or InterSystems Caché. The latter is being aggressively marketed by InterSystems and has had success in penetrating new markets, such as telecommunications, in addition to existing markets. The European Space Agency announced on May 13, 2010 that it will use the InterSystems Caché database to support the Gaia mission. This mission aims to map the Milky Way with unprecedented precision.

MUMPS is a language intended for and designed to build database applications. Secondary language features were included to help programmers make applications using minimal computing resources. The original implementations were interpreted, though modern implementations may be fully or partially compiled. Individual "programs" run in memory "partitions". Early MUMPS memory partitions were limited to 2048 bytes so aggressive abbreviation greatly aided multi-programming on severely resource limited hardware, because more than one MUMPS job could fit into the very small memories extant in hardware at the time. The ability to provide multi-user systems was another language design feature. The word "Multi-Programming" in the acronym points to this. Even the earliest machines running MUMPS supported multiple jobs running at the same time. With the change from mini-computers to micro-computers a
few years later, even a "single user PC" with a single 8-bit CPU and 16K or 64K of memory could support multiple users, who could connect to it from (non-graphical) video display terminals.

Since memory was tight originally, the language design for MUMPS valued very terse code. Thus, every MUMPS command or function name could be abbreviated from one to three letters in length, e.g. Quit (exit program) as Q, $P = $Piece function, R = Read command, $TR = $Translate function. Spaces and end-of-line markers are significant in MUMPS because line scope promoted the same terse language design. Thus, a single line of program code could express, with few characters, an idea for which other programming languages could require 5 to 10 times as many characters. Abbreviation was a common feature of languages designed in this period (e.g., FOCAL-69, early BASICs such as Tiny BASIC, etc.). An unfortunate side effect of this, coupled with the early need to write minimalist code, was that MUMPS programmers routinely did not comment code and used extensive abbreviations. This meant that even an expert MUMPS programmer could not just skim through a page of code to see its function but would have to
analyze it line by line.

Database interaction is transparently built into the language. The MUMPS language provides a hierarchical database made up of persistent sparse arrays, which is implicitly "opened" for every MUMPS application. All variable names prefixed with the caret character ("^") use permanent (instead of RAM) storage, will maintain their values after the application exits, and will be visible to (and modifiable by) other running applications. Variables using this shared and permanent storage are called "Globals" in MUMPS, because the scoping of these variables is "globally available" to all jobs on the system. The more recent and more common use of the name "global variables" in other languages is a more limited scoping of names, coming from the fact that unscoped variables are "globally" available to any programs running in the same process, but not shared among multiple processes. The MUMPS Storage mode (i.e. Globals
stored as persistent sparse arrays), gives the MUMPS database the characteristics of a document-oriented database.

All variable names which are not prefixed with caret character ("^") are temporary and private. Like global variables, they also have a hierarchical storage model, but are only "locally available" to a single job, thus they are called "locals". Both "globals" and "locals" can have child nodes (called "subscripts" in MUMPS terminology). Subscripts are not limited to numerals—any ASCII character or group of characters can be a subscript identifier. While this is not uncommon for modern languages such as Perl or JavaScript, it was a highly unusual feature in the late 1970s. This capability was not universally implemented in MUMPS systems before the 1984 ANSI standard, as only canonically numeric subscripts were required by the standard to be allowed. Thus, the variable named 'Car' can have
subscripts "Door", "Steering Wheel" and "Engine", each of which can contain a value and have subscripts of their own. The variable ^Car("Door") could have a nested variable subscript of "Color" for example. Thus, you could say
SET ^Car("Door","Color")="BLUE"
to modify a nested child node of ^Car. In MUMPS terms, "Color" is the 2nd subscript of the variable ^Car (both the names of the child-nodes and the child-nodes themselves are likewise called subscripts). Hierarchical variables are similar to objects with properties in many object oriented languages. Additionally, the MUMPS language design requires that all subscripts of variables are automatically kept in sorted order. Numeric subscripts (including floating-point numbers) are stored from lowest to highest. All non-numeric subscripts are stored in alphabetical order following the numbers. In MUMPS terminology, this is "canonical order". By using only non-negative integer subscripts, the MUMPS programmer can emulate the arrays data type from other languages. Although MUMPS does not natively offer a full set of DBMS features such as mandatory schemas, several DBMS systems have been built on top of it that provide
application developers with flat-file, relational and network database features.

Additionally, there are built-in operators which treat a delimited string (e.g., comma-separated values) as an array. Early MUMPS programmers would often store a structure of related information as a delimited string, parsing it after it was read in; this saved disk access time and offered considerable speed advantages on some hardware.

MUMPS has no data types. Numbers can be treated as strings of digits, or strings can be treated as numbers by numeric operators ("coerced", in MUMPS terminology). Coercion can have some odd side effects, however. For example, when a string is coerced, the parser turns as much of the string (starting from the left) into a number as it can, then discards the rest. Thus the statement codice_1 is evaluated as codice_2 in MUMPS.

Other features of the language are intended to help MUMPS applications interact with each other in a multi-user environment. Database locks, process identifiers, and atomicity of database update transactions are all required of standard MUMPS implementations.

In contrast to languages in the C or Wirth traditions, some space characters between MUMPS statements are significant. A single space separates a command from its argument, and a space, or newline, separates each argument from the next MUMPS token. Commands which take no arguments (e.g., codice_3) require two following spaces. The concept is that one space separates the command from the (nonexistent) argument, the next separates the "argument" from the next command. Newlines are also significant; an codice_4, codice_3 or codice_6 command processes (or skips) everything else till the end-of-line. To make those statements control multiple lines, you must use the codice_7 command to create a code block.

A simple Hello world program in MUMPS might be:

hello()
and would be run from the MUMPS command line with the command codice_8. Since MUMPS allows commands to be strung together on the same line, and since commands can be abbreviated to a single letter, this routine could be made more compact:
hello() w "Hello, World!",! q
The 'codice_9' after the text generates a newline.

ANSI X11.1-1995 gives a complete, formal description of the language; an annotated version of this standard is available online.

Data types: There is one universal datatype, which is implicitly coerced to string, integer, or floating-point datatypes as context requires.

Booleans (called "truthvalues" in MUMPS): In IF commands and other syntax that has expressions evaluated as conditions, any string value is evaluated as a numeric value, and if that is a nonzero value, then it is interpreted as True. codice_10 sets A to "FOO" if N is less than 10; codice_11 performs PRINTERR if N is greater than 100. This construct provides a conditional whose scope is less than a full line.

Abbreviation: You can abbreviate nearly all commands and native functions to one, two, or three characters.

Reserved words: None. Since MUMPS interprets source code by context, there is no need for reserved words. You may use the names of language commands as variables, so the following is perfectly legal MUMPS code:
GREPTHIS()
THEN IF IF,SET&KILL SET SET=SET+KILL QUIT
MUMPS can be made more obfuscated by using the contracted operator syntax, as shown in this terse example derived from the example above:
GREPTHIS()
T I I,S&K S S=S+K Q
Arrays: are created dynamically, stored as B-trees, are sparse (i.e. use almost no space for missing nodes), can use any number of subscripts, and subscripts can be strings or numeric (including floating point). Arrays are always automatically stored in sorted order, so there is never any occasion to sort, pack, reorder, or otherwise reorganize the database. Built in functions such as $DATA, $ORDER, $NEXT(deprecated) and $QUERY functions provide efficient examination and traversal of the fundamental array structure, on disk or in memory.
for i=10000:1:12345 set sqtable(i)=i*i
set address("Smith","Daniel")="dpbsmith@world.std.com"
Local arrays: variable names not beginning with caret (i.e. "^") are stored in memory by process, are private to the creating process, and expire when the creating process terminates. The available storage depends on implementation. For those implementations using partitions, it is limited to the partition size (a small partition might be 32K). For other implementations, it may be several megabytes.

Global arrays: codice_12. These are stored on disk, are available to all processes, and are persistent when the creating process terminates. Very large globals (for example, hundreds of gigabytes) are practical and efficient in most implementations. This is MUMPS' main "database" mechanism. It is used instead of calling on the operating system to create, write, and read files.

Indirection: in many contexts, codice_13 can be used, and effectively substitutes the contents of VBL into another MUMPS statement. codice_14 sets the variable ABC to 123. codice_15 performs the subroutine named REPORT. This substitution allows for lazy evaluation and late binding as well as effectively the operational equivalent of "pointers" in other languages.

Piece function: This breaks variables into segmented pieces guided by a user specified separator string (sometimes called a "delimiter"). Those who know awk will find this familiar. codice_16 means the "third caret-separated piece of STRINGVAR." The piece function can also appear as an assignment (SET command) target.

codice_17 yields "std".

After

SET X="dpbsmith@world.std.com"
codice_18 causes X to become "office@world.std.com" (note that $P is equivalent to $PIECE and could be written as such).

Order function: This function treats its input as a structure, and finds the next index that exists which has the same structure except for the last subscript. It returns the sorted value that is ordered after the one given as input. (This treats the array reference as a content-addressable data rather than an address of a value.)
Set stuff(6)="xyz",stuff(10)=26,stuff(15)=""
codice_19 yields 6, codice_20 yields 10, codice_21 yields 10, codice_22 yields 15, codice_23 yields "".
Set i="" For Set i=$O(stuff(i)) Quit:i="" Write !,i,10,stuff(i)
Here, the argument-less "For" repeats until stopped by a terminating "Quit". This line prints a table of i and stuff(i) where i is successively 6, 10, and 15.

For iterating the database, the Order function returns the next key to use.
GTM>S n=""
GTM>S n=$order(^nodex(n))
GTM>zwr n
n=" building"
GTM>S n=$order(^nodex(n))
GTM>zwr n
n=" name:gd"
GTM>S n=$order(^nodex(n))
GTM>zwr n
n="%kml:guid"
Multi-User/Multi-Tasking/Multi-Processor: MUMPS supports multiple simultaneous users and processes even when the underlying operating system does not (e.g., MS-DOS). Additionally, there is the ability to specify an environment for a variable, such as by specifying a machine name in a variable (as in codice_24), which can allow you to access data on remote machines.

Some aspects of MUMPS syntax differ strongly from that of more modern languages, which can cause confusion. Whitespace is not allowed within expressions, as it ends a statement: codice_25 is an error, and must be written codice_26. All operators have the same precedence and are left-associative (codice_27 evaluates to 50). The operators for "less than or equal to" and "greater than or equal to" are codice_28 and codice_29 (that is, the boolean negation operator codice_30 plus a strict comparison operator). Periods (codice_31) are used to indent the lines in a DO block, not whitespace. The ELSE command does not need a corresponding IF, as it operates by inspecting the value in the builtin system variable codice_32.

MUMPS scoping rules are more permissive than other modern languages. Declared local variables are scoped using the stack. A routine can normally see all declared locals of the routines below it on the call stack, and routines cannot prevent routines they call from modifying their declared locals, unless the caller manually creates a new stack level (codice_33) and aliases each of the variables they wish to protect (codice_34) before calling any child routines. By contrast, undeclared variables (variables created by using them, rather than declaration) are in scope for all routines running in the same process, and remain in scope until the program exits.

Because MUMPS database references differ from to internal variable references only in the caret prefix, it is dangerously easy to unintentionally edit the database, or even to delete a database "table".

All of the following positions can be, and have been, supported by knowledgeable people at various times:

Some of the contention arose in response to strong M advocacy on the part of one commercial interest, InterSystems, whose chief executive disliked the name MUMPS and felt that it represented a serious marketing obstacle. Thus, favoring M to some extent became identified as alignment with InterSystems. The dispute also reflected rivalry between organizations (the M Technology Association, the MUMPS Development Committee, the ANSI and ISO Standards Committees) as to who determines the "official" name of the language. Some writers have attempted to defuse the issue by referring to the language as "M[UMPS]", square brackets being the customary notation for optional syntax elements. A leading authority, and the author of an open source MUMPS implementation, Professor Kevin O'Kane, uses only 'MUMPS'.

The most recent standard (ISO/IEC 11756:1999, re-affirmed on 25 June 2010), still mentions both M and MUMPS as officially accepted names.

Massachusetts General Hospital registered "MUMPS" as a trademark with the USPTO on November 28, 1971, and renewed it on November 16, 1992 - but let it expire on August 30, 2003.

MUMPS invites comparison with the Pick operating system. Similarities include:






</doc>
<doc id="19726" url="https://en.wikipedia.org/wiki?curid=19726" title="Mercury (programming language)">
Mercury (programming language)

Mercury is a functional logic programming language made for real-world uses. The first version was developed at the University of Melbourne, Computer Science department, by Fergus Henderson, Thomas Conway, and Zoltan Somogyi, under Somogyi's supervision, and released on April 8, 1995.

Mercury is a purely declarative logic programming language. It is related to both Prolog and Haskell. It features a strong, static, polymorphic type system, and a strong mode and determinism system.

The official implementation, the Melbourne Mercury Compiler, is available for most Unix and Unix-like platforms, including Linux, macOS, and for Windows.

Mercury is based on the logic programming language Prolog. It has the same syntax and the same basic concepts such as the selective linear definite clause resolution (SLD) algorithm. It can be viewed as a pure subset of Prolog with strong types and modes. As such, it is often compared to its predecessor in features and run-time efficiency.

The language is designed using software engineering principles. Unlike the original implementations of Prolog, it has a separate compilation phase, rather than being directly interpreted. This allows a much wider range of errors to be detected before running a program. It features a strict static type and mode system and a module system.

By using information obtained at compile time (such as type and mode), programs written in Mercury typically perform significantly faster than equivalent programs written in Prolog. Its authors claim that Mercury is the fastest logic language in the world, by a wide margin.

Mercury is a purely declarative language, unlike Prolog, since it lacks "extra-logical" Prolog statements such as codice_1 (cut) and imperative input/output (I/O). This enables advanced static program analysis and program optimization, including compile-time garbage collection, but it can make certain programming constructs (such as a switch over a number of options, with a default) harder to express. (While Mercury does allow impure functionality, this serves mainly as a way to call foreign language code. All impure code must be marked explicitly.) Operations which would typically be impure (such as input/output) are expressed using pure constructs in Mercury using linear types, by threading a dummy "world" value through all relevant code.

Notable programs written in Mercury include the Mercury compiler and the Prince XML formatter. Software company Mission Critical IT has also been using Mercury since 2000 to develop enterprise applications and its Ontology-Driven software development platform, ODASE.

Mercury has several back-ends, which enable compiling Mercury code into several languages, including:



Mercury also features a foreign language interface, allowing code in other languages (depending on the chosen back-end) to be linked with Mercury code. The following foreign languages are possible:
Other languages can then be interfaced to by calling them from these languages. However, this means that foreign language code may need to be written several times for the different backends, otherwise portability between backends will be lost.

The most commonly used back-end is the original low-level C back-end.

Hello World:

Calculating the 10th Fibonacci number (in the most obvious way):

codice_2 is a "state variable", which is syntactic sugar for a pair of variables which are assigned concrete names at compilation; for example, the above is desugared to something like:

Releases are named according to the year and month of release. The current stable release is 20.06 (June 30, 2020). Prior releases were numbered 0.12, 0.13, etc., and the time between stable releases can be as long as 3 years.

There is often also a snapshot "release of the day" (ROTD) consisting of the latest features and bug fixes added to the last stable release.




</doc>
<doc id="19727" url="https://en.wikipedia.org/wiki?curid=19727" title="Michael Faraday">
Michael Faraday

Michael Faraday (; 22 September 1791 – 25 August 1867) was an English scientist who contributed to the study of electromagnetism and electrochemistry. His main discoveries include the principles underlying electromagnetic induction, diamagnetism and electrolysis.

Although Faraday received little formal education, he was one of the most influential scientists in history. It was by his research on the magnetic field around a conductor carrying a direct current that Faraday established the basis for the concept of the electromagnetic field in physics. Faraday also established that magnetism could affect rays of light and that there was an underlying relationship between the two phenomena. He similarly discovered the principles of electromagnetic induction and diamagnetism, and the laws of electrolysis. His inventions of electromagnetic rotary devices formed the foundation of electric motor technology, and it was largely due to his efforts that electricity became practical for use in technology.

As a chemist, Faraday discovered benzene, investigated the clathrate hydrate of chlorine, invented an early form of the Bunsen burner and the system of oxidation numbers, and popularised terminology such as "anode", "cathode", "electrode" and "ion". Faraday ultimately became the first and foremost Fullerian Professor of Chemistry at the Royal Institution, a lifetime position.

Faraday was an excellent experimentalist who conveyed his ideas in clear and simple language; his mathematical abilities, however, did not extend as far as trigonometry and were limited to the simplest algebra. James Clerk Maxwell took the work of Faraday and others and summarized it in a set of equations which is accepted as the basis of all modern theories of electromagnetic phenomena. On Faraday's uses of lines of force, Maxwell wrote that they show Faraday "to have been in reality a mathematician of a very high order – one from whom the mathematicians of the future may derive valuable and fertile methods." The SI unit of capacitance is named in his honour: the farad.

Albert Einstein kept a picture of Faraday on his study wall, alongside pictures of Isaac Newton and James Clerk Maxwell. Physicist Ernest Rutherford stated, "When we consider the magnitude and extent of his discoveries and their influence on the progress of science and of industry, there is no honour too great to pay to the memory of Faraday, one of the greatest scientific discoverers of all time."

Michael Faraday was born on 22 September 1791 in Newington Butts, which is now part of the London Borough of Southwark but was then a suburban part of Surrey. His family was not well off. His father, James, was a member of the Glassite sect of Christianity. James Faraday moved his wife and two children to London during the winter of 1790 from Outhgill in Westmorland, where he had been an apprentice to the village blacksmith. Michael was born in the autumn of that year. The young Michael Faraday, who was the third of four children, having only the most basic school education, had to educate himself.

At the age of 14 he became an apprentice to George Riebau, a local bookbinder and bookseller in Blandford Street. During his seven-year apprenticeship Faraday read many books, including Isaac Watts's "The Improvement of the Mind", and he enthusiastically implemented the principles and suggestions contained therein. He also developed an interest in science, especially in electricity. Faraday was particularly inspired by the book "Conversations on Chemistry" by Jane Marcet.

In 1812, at the age of 20 and at the end of his apprenticeship, Faraday attended lectures by the eminent English chemist Humphry Davy of the Royal Institution and the Royal Society, and John Tatum, founder of the City Philosophical Society. Many of the tickets for these lectures were given to Faraday by William Dance, who was one of the founders of the Royal Philharmonic Society. Faraday subsequently sent Davy a 300-page book based on notes that he had taken during these lectures. Davy's reply was immediate, kind, and favourable. In 1813, when Davy damaged his eyesight in an accident with nitrogen trichloride, he decided to employ Faraday as an assistant. Coincidentally one of the Royal Institution's assistants, John Payne, was sacked and Sir Humphry Davy had been asked to find a replacement; thus he appointed Faraday as Chemical Assistant at the Royal Institution on 1 March 1813. Very soon Davy entrusted Faraday with the preparation of nitrogen trichloride samples, and they both were injured in an explosion of this very sensitive substance.

Faraday married Sarah Barnard (1800–1879) on 12 June 1821. They met through their families at the Sandemanian church, and he confessed his faith to the Sandemanian congregation the month after they were married. They had no children.

Faraday was a devout Christian; his Sandemanian denomination was an offshoot of the Church of Scotland. Well after his marriage, he served as deacon and for two terms as an elder in the meeting house of his youth. His church was located at Paul's Alley in the Barbican. This meeting house relocated in 1862 to Barnsbury Grove, Islington; this North London location was where Faraday served the final two years of his second term as elder prior to his resignation from that post. Biographers have noted that "a strong sense of the unity of God and nature pervaded Faraday's life and work."

In June 1832, the University of Oxford granted Faraday an honorary Doctor of Civil Law degree. During his lifetime, he was offered a knighthood in recognition for his services to science, which he turned down on religious grounds, believing that it was against the word of the Bible to accumulate riches and pursue worldly reward, and stating that he preferred to remain "plain Mr Faraday to the end". Elected a member of the Royal Society in 1824, he twice refused to become President. He became the first Fullerian Professor of Chemistry at the Royal Institution in 1833.

In 1832, Faraday was elected a Foreign Honorary Member of the American Academy of Arts and Sciences. He was elected a foreign member of the Royal Swedish Academy of Sciences in 1838, and was one of eight foreign members elected to the French Academy of Sciences in 1844. In 1849 he was elected as associated member to the Royal Institute of the Netherlands, which two years later became the Royal Netherlands Academy of Arts and Sciences and he was subsequently made foreign member.

Faraday suffered a nervous breakdown in 1839 but eventually returned to his investigations into electromagnetism. In 1848, as a result of representations by the Prince Consort, Faraday was awarded a grace and favour house in Hampton Court in Middlesex, free of all expenses and upkeep. This was the Master Mason's House, later called Faraday House, and now No. 37 Hampton Court Road. In 1858 Faraday retired to live there.

Having provided a number of various service projects for the British government, when asked by the government to advise on the production of chemical weapons for use in the Crimean War (1853–1856), Faraday refused to participate citing ethical reasons.

Faraday died at his house at Hampton Court on 25 August 1867, aged 75. He had some years before turned down an offer of burial in Westminster Abbey upon his death, but he has a memorial plaque there, near Isaac Newton's tomb. Faraday was interred in the dissenters' (non-Anglican) section of Highgate Cemetery.

Faraday's earliest chemical work was as an assistant to Humphry Davy. Faraday was specifically involved in the study of chlorine; he discovered two new compounds of chlorine and carbon. He also conducted the first rough experiments on the diffusion of gases, a phenomenon that was first pointed out by John Dalton. The physical importance of this phenomenon was more fully revealed by Thomas Graham and Joseph Loschmidt. Faraday succeeded in liquefying several gases, investigated the alloys of steel, and produced several new kinds of glass intended for optical purposes. A specimen of one of these heavy glasses subsequently became historically important; when the glass was placed in a magnetic field Faraday determined the rotation of the plane of polarisation of light. This specimen was also the first substance found to be repelled by the poles of a magnet.

Faraday invented an early form of what was to become the Bunsen burner, which is in practical use in science laboratories around the world as a convenient source of heat.
Faraday worked extensively in the field of chemistry, discovering chemical substances such as benzene (which he called bicarburet of hydrogen) and liquefying gases such as chlorine. The liquefying of gases helped to establish that gases are the vapours of liquids possessing a very low boiling point and gave a more solid basis to the concept of molecular aggregation. In 1820 Faraday reported the first synthesis of compounds made from carbon and chlorine, CCl and CCl, and published his results the following year. Faraday also determined the composition of the chlorine clathrate hydrate, which had been discovered by Humphry Davy in 1810. Faraday is also responsible for discovering the laws of electrolysis, and for popularizing terminology such as anode, cathode, electrode, and ion, terms proposed in large part by William Whewell.

Faraday was the first to report what later came to be called metallic nanoparticles. In 1847 he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal. This was probably the first reported observation of the effects of quantum size, and might be considered to be the birth of nanoscience.

Faraday is best known for his work regarding electricity and magnetism. His first recorded experiment was the construction of a voltaic pile with seven British halfpenny coins, stacked together with seven disks of sheet zinc, and six pieces of paper moistened with salt water. With this pile he decomposed sulfate of magnesia (first letter to Abbott, 12 July 1812).

In 1821, soon after the Danish physicist and chemist Hans Christian Ørsted discovered the phenomenon of electromagnetism, Davy and British scientist William Hyde Wollaston tried, but failed, to design an electric motor. Faraday, having discussed the problem with the two men, went on to build two devices to produce what he called "electromagnetic rotation". One of these, now known as the homopolar motor, caused a continuous circular motion that was engendered by the circular magnetic force around a wire that extended into a pool of mercury wherein was placed a magnet; the wire would then rotate around the magnet if supplied with current from a chemical battery. These experiments and inventions formed the foundation of modern electromagnetic technology. In his excitement, Faraday published results without acknowledging his work with either Wollaston or Davy. The resulting controversy within the Royal Society strained his mentor relationship with Davy and may well have contributed to Faraday's assignment to other activities, which consequently prevented his involvement in electromagnetic research for several years.

From his initial discovery in 1821, Faraday continued his laboratory work, exploring electromagnetic properties of materials and developing requisite experience. In 1824, Faraday briefly set up a circuit to study whether a magnetic field could regulate the flow of a current in an adjacent wire, but he found no such relationship. This experiment followed similar work conducted with light and magnets three years earlier that yielded identical results. During the next seven years, Faraday spent much of his time perfecting his recipe for optical quality (heavy) glass, borosilicate of lead, which he used in his future studies connecting light with magnetism. In his spare time, Faraday continued publishing his experimental work on optics and electromagnetism; he conducted correspondence with scientists whom he had met on his journeys across Europe with Davy, and who were also working on electromagnetism. Two years after the death of Davy, in 1831, he began his great series of experiments in which he discovered electromagnetic induction, recording in his laboratory diary on 28 October 1831 he was; "making many experiments with the great magnet of the Royal Society".

Faraday's breakthrough came when he wrapped two insulated coils of wire around an iron ring, and found that upon passing a current through one coil, a momentary current was induced in the other coil. This phenomenon is now known as mutual induction. The iron ring-coil apparatus is still on display at the Royal Institution. In subsequent experiments, he found that if he moved a magnet through a loop of wire an electric current flowed in that wire. The current also flowed if the loop was moved over a stationary magnet. His demonstrations established that a changing magnetic field produces an electric field; this relation was modelled mathematically by James Clerk Maxwell as Faraday's law, which subsequently became one of the four Maxwell equations, and which have in turn evolved into the generalization known today as field theory. Faraday would later use the principles he had discovered to construct the electric dynamo, the ancestor of modern power generators and the electric motor.

In 1832, he completed a series of experiments aimed at investigating the fundamental nature of electricity; Faraday used "static", batteries, and "animal electricity" to produce the phenomena of electrostatic attraction, electrolysis, magnetism, etc. He concluded that, contrary to the scientific opinion of the time, the divisions between the various "kinds" of electricity were illusory. Faraday instead proposed that only a single "electricity" exists, and the changing values of quantity and intensity (current and voltage) would produce different groups of phenomena.

Near the end of his career, Faraday proposed that electromagnetic forces extended into the empty space around the conductor. This idea was rejected by his fellow scientists, and Faraday did not live to see the eventual acceptance of his proposition by the scientific community. Faraday's concept of lines of flux emanating from charged bodies and magnets provided a way to visualize electric and magnetic fields; that conceptual model was crucial for the successful development of the electromechanical devices that dominated engineering and industry for the remainder of the 19th century.

In 1845, Faraday discovered that many materials exhibit a weak repulsion from a magnetic field: a phenomenon he termed diamagnetism.

Faraday also discovered that the plane of polarization of linearly polarized light can be rotated by the application of an external magnetic field aligned with the direction in which the light is moving. This is now termed the Faraday effect. In Sept 1845 he wrote in his notebook, "I have at last succeeded in "illuminating a magnetic curve" or "line of force" and in "magnetising a ray of light"".

Later on in his life, in 1862, Faraday used a spectroscope to search for a different alteration of light, the change of spectral lines by an applied magnetic field. The equipment available to him was, however, insufficient for a definite determination of spectral change. Pieter Zeeman later used an improved apparatus to study the same phenomenon, publishing his results in 1897 and receiving the 1902 Nobel Prize in Physics for his success. In both his 1897 paper and his Nobel acceptance speech, Zeeman made reference to Faraday's work.

In his work on static electricity, Faraday's ice pail experiment demonstrated that the charge resided only on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields emanating from them cancel one another. This shielding effect is used in what is now known as a Faraday cage.

Faraday had a long association with the Royal Institution of Great Britain. He was appointed Assistant Superintendent of the House of the Royal Institution in 1821. He was elected a member of the Royal Society in 1824. In 1825, he became Director of the Laboratory of the Royal Institution. Six years later, in 1833, Faraday became the first Fullerian Professor of Chemistry at the Royal Institution of Great Britain, a position to which he was appointed for life without the obligation to deliver lectures. His sponsor and mentor was John 'Mad Jack' Fuller, who created the position at the Royal Institution for Faraday.

Beyond his scientific research into areas such as chemistry, electricity, and magnetism at the Royal Institution, Faraday undertook numerous, and often time-consuming, service projects for private enterprise and the British government. This work included investigations of explosions in coal mines, being an expert witness in court, and along with two engineers from Chance Brothers c.1853, the preparation of high-quality optical glass, which was required by Chance for its lighthouses. In 1846, together with Charles Lyell, he produced a lengthy and detailed report on a serious explosion in the colliery at Haswell, County Durham, which killed 95 miners. Their report was a meticulous forensic investigation and indicated that coal dust contributed to the severity of the explosion. The report should have warned coal owners of the hazard of coal dust explosions, but the risk was ignored for over 60 years until the Senghenydd Colliery Disaster of 1913.

As a respected scientist in a nation with strong maritime interests, Faraday spent extensive amounts of time on projects such as the construction and operation of lighthouses and protecting the bottoms of ships from corrosion. His workshop still stands at Trinity Buoy Wharf above the Chain and Buoy Store, next to London's only lighthouse where he carried out the first experiments in electric lighting for lighthouses.

Faraday was also active in what would now be called environmental science, or engineering. He investigated industrial pollution at Swansea and was consulted on air pollution at the Royal Mint. In July 1855, Faraday wrote a letter to "The Times" on the subject of the foul condition of the River Thames, which resulted in an often-reprinted cartoon in "Punch". (See also The Great Stink).

Faraday assisted with the planning and judging of exhibits for the Great Exhibition of 1851 in London. He also advised the National Gallery on the cleaning and protection of its art collection, and served on the National Gallery Site Commission in 1857. Education was another of Faraday's areas of service; he lectured on the topic in 1854 at the Royal Institution, and in 1862 he appeared before a Public Schools Commission to give his views on education in Great Britain. Faraday also weighed in negatively on the public's fascination with table-turning, mesmerism, and seances, and in so doing chastised both the public and the nation's educational system.
Before his famous Christmas lectures, Faraday delivered chemistry lectures for the City Philosophical Society from 1816 to 1818 in order to refine the quality of his lectures. Between 1827 and 1860 at the Royal Institution in London, Faraday gave a series of nineteen Christmas lectures for young people, a series which continues today. The objective of Faraday's Christmas lectures was to present science to the general public in the hopes of inspiring them and generating revenue for the Royal Institution. They were notable events on the social calendar among London's gentry. Over the course of several letters to his close friend Benjamin Abbott, Faraday outlined his recommendations on the art of lecturing: Faraday wrote "a flame should be lighted at the commencement and kept alive with unremitting splendour to the end". His lectures were joyful and juvenile, he delighted in filling soap bubbles with various gasses (in order to determine whether or not they are magnetic) in front of his audiences and marveled at the rich colors of polarized lights, but the lectures were also deeply philosophical. In his lectures he urged his audiences to consider the mechanics of his experiments: "you know very well that ice floats upon water ... Why does the ice float? Think of that, and philosophise". His subjects consisted of Chemistry and Electricity, and included: 1841 The Rudiments of Chemistry, 1843 First Principles of Electricity, 1848 The Chemical History of a Candle, 1851 Attractive Forces, 1853 Voltaic Electricity, 1854 The Chemistry of Combustion, 1855 The Distinctive Properties of the Common Metals, 1857 Static Electricity, 1858 The Metallic Properties, 1859 The Various Forces of Matter and their Relations to Each Other.

A statue of Faraday stands in Savoy Place, London, outside the Institution of Engineering and Technology. The Michael Faraday Memorial, designed by brutalist architect Rodney Gordon and completed in 1961, is at the Elephant & Castle gyratory system, near Faraday's birthplace at Newington Butts, London. Faraday School is located on Trinity Buoy Wharf where his workshop still stands above the Chain and Buoy Store, next to London's only lighthouse. Faraday Gardens is a small park in Walworth, London, not far from his birthplace at Newington Butts. It lies within the local council ward of Faraday in the London Borough of Southwark. Michael Faraday Primary school is situated on the Aylesbury Estate in Walworth.

A building at London South Bank University, which houses the institute's electrical engineering departments is named the Faraday Wing, due to its proximity to Faraday's birthplace in Newington Butts. A hall at Loughborough University was named after Faraday in 1960. Near the entrance to its dining hall is a bronze casting, which depicts the symbol of an electrical transformer, and inside there hangs a portrait, both in Faraday's honour. An eight-story building at the University of Edinburgh's science & engineering campus is named for Faraday, as is a recently built hall of accommodation at Brunel University, the main engineering building at Swansea University, and the instructional and experimental physics building at Northern Illinois University. The former UK Faraday Station in Antarctica was named after him.
Streets named for Faraday can be found in many British cities (e.g., London, Fife, Swindon, Basingstoke, Nottingham, Whitby, Kirkby, Crawley, Newbury, Swansea, Aylesbury and Stevenage) as well as in France (Paris), Germany (Berlin-Dahlem, Hermsdorf), Canada (Quebec City, Quebec; Deep River, Ontario; Ottawa, Ontario), and the United States (Reston, Virginia).
A Royal Society of Arts blue plaque, unveiled in 1876, commemorates Faraday at 48 Blandford Street in London's Marylebone district. From 1991 until 2001, Faraday's picture featured on the reverse of Series E £20 banknotes issued by the Bank of England. He was portrayed conducting a lecture at the Royal Institution with the magneto-electric spark apparatus. In 2002, Faraday was ranked number 22 in the BBC's list of the 100 Greatest Britons following a UK-wide vote.

The Faraday Institute for Science and Religion derives its name from the scientist, who saw his faith as integral to his scientific research. The logo of the institute is also based on Faraday's discoveries. It was created in 2006 by a $2,000,000 grant from the John Templeton Foundation to carry out academic research, to foster understanding of the interaction between science and religion, and to engage public understanding in both these subject areas.

Faraday's life and contributions to electromagnetics was the principal topic of the tenth episode, titled "The Electric Boy", of the 2014 American science documentary series, "", which was broadcast on Fox and the National Geographic Channel.

Aldous Huxley, the literary giant who was also the grandson of T. H. Huxley, the grandnephew of Matthew Arnold, the brother of Julian Huxley, and the half-brother of Andrew Huxley, was well-versed in science. He wrote about Faraday in an essay entitled, "A Night in Pietramala": “He is always the natural philosopher. To discover truth is his sole aim and interest…even if I could be Shakespeare, I think I should still choose to be Faraday.”

In honor and remembrance of his great scientific contributions, several institutions have created prizes and awards in his name. This include:

Faraday's books, with the exception of "Chemical Manipulation", were collections of scientific papers or transcriptions of lectures. Since his death, Faraday's diary has been published, as have several large volumes of his letters and Faraday's journal from his travels with Davy in 1813–1815.





</doc>
<doc id="19728" url="https://en.wikipedia.org/wiki?curid=19728" title="Marriage">
Marriage

Marriage, also called matrimony or wedlock, is a culturally recognised union between people, called spouses, that establishes rights and obligations between them, as well as between them and their children, and between them and their in-laws. The definition of marriage varies around the world, not only between cultures and between religions, but also throughout the history of any given culture and religion. Over time, it has expanded and also constricted in terms of who and what is encompassed. Typically, it is an institution in which interpersonal relationships, usually sexual, are acknowledged or sanctioned. In some cultures, marriage is recommended or considered to be compulsory before pursuing any sexual activity. When defined broadly, marriage is considered a cultural universal. A marriage ceremony is called a wedding.

Individuals may marry for several reasons, including legal, social, libidinal, emotional, financial, spiritual, and religious purposes. Whom they marry may be influenced by gender, socially determined rules of incest, prescriptive marriage rules, parental choice and individual desire. In some areas of the world, arranged marriage, child marriage, polygamy, and sometimes forced marriage, may be practiced as a cultural tradition. Conversely, such practices may be outlawed and penalized in parts of the world out of concerns regarding the infringement of women's rights or children's rights (both female and male) or as a result of international law. Around the world, primarily in developed democracies, there has been a general trend towards ensuring equal rights for women within marriage and legally recognizing the marriages of interfaith, interracial, and same-sex couples. These trends coincide with the broader human rights movement.

Marriage can be recognized by a state, an organization, a religious authority, a tribal group, a local community, or peers. It is often viewed as a contract. When a marriage is performed and carried out by a government institution in accordance with the marriage laws of the jurisdiction, without religious content, it is a civil marriage. Civil marriage recognizes and creates the rights and obligations intrinsic to matrimony in the eyes of the state. When a marriage is performed with religious content under the auspices of a religious institution, it is a religious marriage. Religious marriage recognizes and creates the rights and obligations intrinsic to matrimony in the eyes of that religion. Religious marriage is known variously as sacramental marriage in Catholicism, nikah in Islam, nissuin in Judaism, and various other names in other faith traditions, each with their own constraints as to what constitutes, and who can enter into, a valid religious marriage.

Some countries do not recognize locally performed religious marriage on its own, and require a separate civil marriage for official purposes. Conversely, civil marriage does not exist in some countries governed by a religious legal system, such as Saudi Arabia, where marriages contracted abroad might not be recognized if they were contracted contrary to Saudi interpretations of Islamic religious law. In countries governed by a mixed secular-religious legal system, such as Lebanon and Israel, locally performed civil marriage does not exist within the country, which prevents interfaith and various other marriages that contradict religious laws from being entered into in the country; however, civil marriages performed abroad may be recognized by the state even if they conflict with religious laws. For example, in the case of recognition of marriage in Israel, this includes recognition of not only interfaith civil marriages performed abroad, but also overseas same-sex civil marriages.

The act of marriage usually creates normative or legal obligations between the individuals involved, and any offspring they may produce or adopt. In terms of legal recognition, most sovereign states and other jurisdictions limit marriage to opposite-sex couples and a diminishing number of these permit polygyny, child marriages, and forced marriages. In modern times, a growing number of countries, primarily developed democracies, have lifted bans on, and have established legal recognition for, the marriages of interfaith, interracial, and same-sex couples. In some areas, child marriages and polygamy may occur in spite of national laws against the practice.

Since the late twentieth century, major social changes in Western countries have led to changes in the demographics of marriage, with the age of first marriage increasing, fewer people marrying, and more couples choosing to cohabit rather than marry. For example, the number of marriages in Europe decreased by 30% from 1975 to 2005.

Historically, in most cultures, married women had very few rights of their own, being considered, along with the family's children, the property of the husband; as such, they could not own or inherit property, or represent themselves legally (see, for example, coverture). In Europe, the United States, and other places in the developed world, beginning in the late 19th century, marriage has undergone gradual legal changes, aimed at improving the rights of the wife. These changes included giving wives legal identities of their own, abolishing the right of husbands to physically discipline their wives, giving wives property rights, liberalizing divorce laws, providing wives with reproductive rights of their own, and requiring a wife's consent when sexual relations occur. These changes have occurred primarily in Western countries. In the 21st century, there continue to be controversies regarding the legal status of married women, legal acceptance of or leniency towards violence within marriage (especially sexual violence), traditional marriage customs such as dowry and bride price, forced marriage, marriageable age, and criminalization of consensual behaviors such as premarital and extramarital sex.

The word "marriage" derives from Middle English "mariage", which first appears in 1250–1300 CE. This in turn is derived from Old French, "marier" (to marry), and ultimately Latin, "marītāre", meaning to provide with a husband or wife and "marītāri" meaning to get married. The adjective "marīt-us -a, -um" meaning matrimonial or nuptial could also be used in the masculine form as a noun for "husband" and in the feminine form for "wife". The related word "matrimony" derives from the Old French word "matremoine", which appears around 1300 CE and ultimately derives from Latin "mātrimōnium", which combines the two concepts: "mater" meaning "mother" and the suffix -"monium" signifying "action, state, or condition".

Anthropologists have proposed several competing definitions of marriage in an attempt to encompass the wide variety of marital practices observed across cultures. Even within Western culture, "definitions of marriage have careened from one extreme to another and everywhere in between" (as Evan Gerstmann has put it).

In "The History of Human Marriage" (1891), Edvard Westermarck defined marriage as "a more or less durable connection between male and female lasting beyond the mere act of propagation till after the birth of the offspring." In "The Future of Marriage in Western Civilization" (1936), he rejected his earlier definition, instead provisionally defining marriage as "a relation of one or more men to one or more women that is recognized by custom or law".

The anthropological handbook "Notes and Queries" (1951) defined marriage as "a union between a man and a woman such that children born to the woman are the recognized legitimate offspring of both partners." In recognition of a practice by the Nuer people of Sudan allowing women to act as a husband in certain circumstances (the ghost marriage), Kathleen Gough suggested modifying this to "a woman and one or more other persons."

In an analysis of marriage among the Nayar, a polyandrous society in India, Gough found that the group lacked a husband role in the conventional sense; that unitary role in the west was divided between a non-resident "social father" of the woman's children, and her lovers who were the actual procreators. None of these men had legal rights to the woman's child. This forced Gough to disregard sexual access as a key element of marriage and to define it in terms of legitimacy of offspring alone: marriage is "a relationship established between a woman and one or more other persons, which provides a child born to the woman under circumstances not prohibited by the rules of relationship, is accorded full birth-status rights common to normal members of his society or social stratum."

Economic anthropologist Duran Bell has criticized the legitimacy-based definition on the basis that some societies do not require marriage for legitimacy. He argued that a legitimacy-based definition of marriage is circular in societies where illegitimacy has no other legal or social implications for a child other than the mother being unmarried.

Edmund Leach criticized Gough's definition for being too restrictive in terms of recognized legitimate offspring and suggested that marriage be viewed in terms of the different types of rights it serves to establish. In a 1955 article in "Man", Leach argued that no one definition of marriage applied to all cultures. He offered a list of ten rights associated with marriage, including sexual monopoly and rights with respect to children, with specific rights differing across cultures. Those rights, according to Leach, included:

In a 1997 article in "Current Anthropology", Duran Bell describes marriage as "a relationship between one or more men (male or female) in severalty to one or more women that provides those men with a demand-right of sexual access within a domestic group and identifies women who bear the obligation of yielding to the demands of those specific men." In referring to "men in severalty", Bell is referring to corporate kin groups such as lineages which, in having paid brideprice, retain a right in a woman's offspring even if her husband (a lineage member) deceases (Levirate marriage). In referring to "men (male or female)", Bell is referring to women within the lineage who may stand in as the "social fathers" of the wife's children born of other lovers. (See Nuer "ghost marriage".)

Monogamy is a form of marriage in which an individual has only one spouse during their lifetime or at any one time (serial monogamy).

Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found a strong correlation between intensive plough agriculture, dowry and monogamy. This pattern was found in a broad swath of Eurasian societies from Japan to Ireland. The majority of Sub-Saharan African societies that practice extensive hoe agriculture, in contrast, show a correlation between "bride price" and polygamy. A further study drawing on the Ethnographic Atlas showed a statistical correlation between increasing size of the society, the belief in "high gods" to support human morality, and monogamy.

In the countries which do not permit polygamy, a person who marries in one of those countries a person while still being lawfully married to another commits the crime of bigamy. In all cases, the second marriage is considered legally null and void. Besides the second and subsequent marriages being void, the bigamist is also liable to other penalties, which also vary between jurisdictions.

Governments that support monogamy may allow easy divorce. In a number of Western countries divorce rates approach 50%. Those who remarry do so on average three times. Divorce and remarriage can thus result in "serial monogamy", i.e. having multiple marriages but only one legal spouse at a time. This can be interpreted as a form of plural mating, as are those societies dominated by female-headed families in the Caribbean, Mauritius and Brazil where there is frequent rotation of unmarried partners. In all, these account for 16 to 24% of the "monogamous" category.

Serial monogamy creates a new kind of relative, the "ex-". The "ex-wife", for example, remains an active part of her "ex-husband's" or "ex-wife's" life, as they may be tied together by transfers of resources (alimony, child support), or shared child custody. Bob Simpson notes that in the British case, serial monogamy creates an "extended family" – a number of households tied together in this way, including mobile children (possible exes may include an ex-wife, an ex-brother-in-law, etc., but not an "ex-child"). These "unclear families" do not fit the mould of the monogamous nuclear family. As a series of connected households, they come to resemble the polygynous model of separate households maintained by mothers with children, tied by a male to whom they are married or divorced.

Polygamy is a marriage which includes more than two spouses. When a man is married to more than one wife at a time, the relationship is called polygyny, and there is no marriage bond between the wives; and when a woman is married to more than one husband at a time, it is called polyandry, and there is no marriage bond between the husbands. If a marriage includes multiple husbands or wives, it can be called group marriage.

A molecular genetic study of global human genetic diversity argued that sexual polygyny was typical of human reproductive patterns until the shift to sedentary farming communities approximately 10,000 to 5,000 years ago in Europe and Asia, and more recently in Africa and the Americas. As noted above, Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found that the majority of Sub-Saharan African societies that practice extensive hoe agriculture show a correlation between "Bride price" and polygamy. A survey of other cross-cultural samples has confirmed that the absence of the plough was the only predictor of polygamy, although other factors such as high male mortality in warfare (in non-state societies) and pathogen stress (in state societies) had some impact.

Marriages are classified according to the number of legal spouses an individual has. The suffix "-gamy" refers specifically to the number of spouses, as in bi-gamy (two spouses, generally illegal in most nations), and poly-gamy (more than one spouse).

Societies show variable acceptance of polygamy as a cultural ideal and practice. According to the Ethnographic Atlas, of 1,231 societies noted, 186 were monogamous; 453 had occasional polygyny; 588 had more frequent polygyny; and 4 had polyandry. However, as Miriam Zeitzen writes, social tolerance for polygamy is different from the practice of polygamy, since it requires wealth to establish multiple households for multiple wives. The actual practice of polygamy in a tolerant society may actually be low, with the majority of aspirant polygamists practicing monogamous marriage. Tracking the occurrence of polygamy is further complicated in jurisdictions where it has been banned, but continues to be practiced ("de facto polygamy").

Zeitzen also notes that Western perceptions of African society and marriage patterns are biased by "contradictory concerns of nostalgia for traditional African culture versus critique of polygamy as oppressive to women or detrimental to development." Polygamy has been condemned as being a form of human rights abuse, with concerns arising over domestic abuse, forced marriage, and neglect. The vast majority of the world's countries, including virtually all of the world's developed nations, do not permit polygamy. There have been calls for the abolition of polygamy in developing countries.

Polygyny usually grants wives equal status, although the husband may have personal preferences. One type of de facto polygyny is concubinage, where only one woman gets a wife's rights and status, while other women remain legal house mistresses.

Although a society may be classified as polygynous, not all marriages in it necessarily are; monogamous marriages may in fact predominate. It is to this flexibility that Anthropologist Robin Fox attributes its success as a social support system: "This has often meant – given the imbalance in the sex ratios, the higher male infant mortality, the shorter life span of males, the loss of males in wartime, etc. – that often women were left without financial support from husbands. To correct this condition, females had to be killed at birth, remain single, become prostitutes, or be siphoned off into celibate religious orders. Polygynous systems have the advantage that they can promise, as did the Mormons, a home and family for every woman."

Nonetheless, polygyny is a gender issue which offers men asymmetrical benefits. In some cases, there is a large age discrepancy (as much as a generation) between a man and his youngest wife, compounding the power differential between the two. Tensions not only exist "between" genders, but also "within" genders; senior and junior men compete for wives, and senior and junior wives in the same household may experience radically different life conditions, and internal hierarchy. Several studies have suggested that the wive's relationship with other women, including co-wives and husband's female kin, are more critical relationships than that with her husband for her productive, reproductive and personal achievement. In some societies, the co-wives are relatives, usually sisters, a practice called "sororal polygyny"; the pre-existing relationship between the co-wives is thought to decrease potential tensions within the marriage.

Fox argues that "the major difference between polygyny and monogamy could be stated thus: while plural mating occurs in both systems, under polygyny several unions may be recognized as being legal marriages while under monogamy only one of the unions is so recognized. Often, however, it is difficult to draw a hard and fast line between the two."

As polygamy in Africa is increasingly subject to legal limitations, a variant form of "de facto" (as opposed to legal or "de jure") polygyny is being practised in urban centres. Although it does not involve multiple (now illegal) formal marriages, the domestic and personal arrangements follow old polygynous patterns. The de facto form of polygyny is found in other parts of the world as well (including some Mormon sects and Muslim families in the United States).
In some societies such as the Lovedu of South Africa, or the Nuer of the Sudan, aristocratic women may become female 'husbands.' In the Lovedu case, this female husband may take a number of polygamous wives. This is not a lesbian relationship, but a means of legitimately expanding a royal lineage by attaching these wives' children to it. The relationships are considered polygynous, not polyandrous, because the female husband is in fact assuming masculine gendered political roles.

Religious groups have differing views on the legitimacy of polygyny. It is allowed in Islam and Confucianism. Judaism and Christianity have mentioned practices involving polygyny in the past, however, outright religious acceptance of such practices was not addressed until its rejection in later passages. They do explicitly prohibit polygyny today.

Polyandry is notably more rare than polygyny, though less rare than the figure commonly cited in the "Ethnographic Atlas" (1980) which listed only those polyandrous societies found in the Himalayan Mountains. More recent studies have found 53 societies outside the 28 found in the Himalayans which practice polyandry. It is most common in egalitarian societies marked by high male mortality or male absenteeism. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

The explanation for polyandry in the Himalayan Mountains is related to the scarcity of land; the marriage of all brothers in a family to the same wife ("fraternal polyandry") allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In Europe, this was prevented through the social practice of impartible inheritance (the dis-inheriting of most siblings, some of whom went on to become celibate monks and priests).

Group marriage (also known as "multi-lateral marriage") is a form of polyamory in which more than two persons form a family unit, with all the members of the group marriage being considered to be married to all the other members of the group marriage, and all members of the marriage share parental responsibility for any children arising from the marriage. No country legally condones group marriages, neither under the law nor as a common law marriage, but historically it has been practiced by some cultures of Polynesia, Asia, Papua New Guinea and the Americas – as well as in some intentional communities and alternative subcultures such as the Oneida Perfectionists in up-state New York. Of the 250 societies reported by the American anthropologist George Murdock in 1949, only the Kaingang of Brazil had any group marriages at all.

A child marriage is a marriage where one or both spouses are under the age of 18. It is related to child betrothal and teenage pregnancy.

Child marriage was common throughout history, even up until the 1900s in the United States, where in 1880 CE, in the state of Delaware, the age of consent for marriage was 7 years old. Still, in 2017, over half of the 50 United States have no explicit minimum age to marry and several states set the age as low as 14. Today it is condemned by international human rights organizations. Child marriages are often arranged between the families of the future bride and groom, sometimes as soon as the girl is born. However, in the late 1800s in England and the United States, feminist activists began calling for raised age of consent laws, which was eventually handled in the 1920s, having been raised to 16–18.

Child marriages can also occur in the context of bride kidnapping.

In the year 1552 CE, John Somerford and Jane Somerford Brereton were both married at the ages of 3 and 2, respectively. Twelve years later, in 1564, John filed for divorce.

While child marriage is observed for both boys and girls, the overwhelming majority of child spouses are girls. In many cases, only one marriage-partner is a child, usually the female, due to the importance placed upon female virginity. Causes of child marriage include poverty, bride price, dowry, laws that allow child marriages, religious and social pressures, regional customs, fear of remaining unmarried, and perceived inability of women to work for money.

Today, child marriages are widespread in parts of the world; being most common in South Asia and sub-Saharan Africa, with more than half of the girls in some countries in those regions being married before 18. The incidence of child marriage has been falling in most parts of the world. In developed countries child marriage is outlawed or restricted.

Girls who marry before 18 are at greater risk of becoming victims of domestic violence, than those who marry later, especially when they are married to a much older man.

Several kinds of same-sex marriages have been documented in Indigenous and lineage-based cultures. In the Americas, We'wha (Zuni), was a "lhamana" (male individuals who, at least some of the time, dress and live in the roles usually filled by women in that culture); a respected artist, We'wha served as an emissary of the Zuni to Washington, where he met President Grover Cleveland. We'wha had at least one husband who was generally recognized as such.

While it is a relatively new practice to grant same-sex couples the same form of legal marital recognition as commonly granted to mixed-sex couples, there is some history of recorded same-sex unions around the world. Ancient Greek same-sex relationships were like modern companionate marriages, unlike their different-sex marriages in which the spouses had few emotional ties, and the husband had freedom to engage in outside sexual liaisons. The Codex Theodosianus ("C. Th." 9.7.3) issued in 438 CE imposed severe penalties or death on same-sex relationships, but the exact intent of the law and its relation to social practice is unclear, as only a few examples of same-sex relationships in that culture exist. Same-sex unions were celebrated in some regions of China, such as Fujian. Possibly the earliest documented same-sex wedding in Latin Christendom occurred in Rome, Italy, at the San Giovanni a Porta Latina basilica in 1581.

Several cultures have practiced temporary and conditional marriages. Examples include the Celtic practice of handfasting and fixed-term marriages in the Muslim community. Pre-Islamic Arabs practiced a form of temporary marriage that carries on today in the practice of Nikah mut‘ah, a fixed-term marriage contract. The Islamic prophet Muhammad sanctioned a temporary marriage – sigheh in Iran and muta'a in Iraq – which can provide a legitimizing cover for sex workers. The same forms of temporary marriage have been used in Egypt, Lebanon and Iran to make the donation of a human ova legal for in vitro fertilisation; a woman cannot, however, use this kind of marriage to obtain a sperm donation. Muslim controversies related to Nikah Mut'ah have resulted in the practice being confined mostly to Shi'ite communities. The matrilineal Mosuo of China practice what they call "walking marriage".

In some jurisdictions cohabitation, in certain circumstances, may constitute a common-law marriage, an unregistered partnership, or otherwise provide the unmarried partners with various rights and responsibilities; and in some countries the laws recognize cohabitation in lieu of institutional marriage for taxation and social security benefits. This is the case, for example, in Australia. Cohabitation may be an option pursued as a form of resistance to traditional institutionalized marriage. However, in this context, some nations reserve the right to define the relationship as marital, or otherwise to regulate the relation, even if the relation has not been registered with the state or a religious institution.

Conversely, institutionalized marriages may not involve cohabitation. In some cases couples living together do not wish to be recognized as married. This may occur because pension or alimony rights are adversely affected; because of taxation considerations; because of immigration issues, or for other reasons. Such marriages have also been increasingly common in Beijing. Guo Jianmei, director of the center for women's studies at Beijing University, told a Newsday correspondent, "Walking marriages reflect sweeping changes in Chinese society." A "walking marriage" refers to a type of temporary marriage formed by the Mosuo of China, in which male partners live elsewhere and make nightly visits. A similar arrangement in Saudi Arabia, called misyar marriage, also involves the husband and wife living separately but meeting regularly.

There is wide cross-cultural variation in the social rules governing the selection of a partner for marriage. There is variation in the degree to which partner selection is an individual decision by the partners or a collective decision by the partners' kin groups, and there is variation in the rules regulating which partners are valid choices.

The United Nations World Fertility Report of 2003 reports that 89% of all people get married before age forty-nine. The percent of women and men who marry before age forty-nine drops to nearly 50% in some nations and reaches near 100% in other nations.

In other cultures with less strict rules governing the groups from which a partner can be chosen the selection of a marriage partner may involve either the couple going through a selection process of courtship or the marriage may be arranged by the couple's parents or an outside party, a matchmaker.

Some people want to marry a person with higher or lower status than them. Others want to marry people who have similar status. In many societies women marry men who are of higher social status. There are marriages where each party has sought a partner of similar status. There are other marriages in which the man is older than the woman.

Societies have often placed restrictions on marriage to relatives, though the degree of prohibited relationship varies widely. Marriages between parents and children, or between full siblings, with few exceptions, have been considered incest and forbidden. However, marriages between more distant relatives have been much more common, with one estimate being that 80% of all marriages in history have been between second cousins or closer. This proportion has fallen dramatically, but still more than 10% of all marriages are believed to be between people who are second cousins or more closely related. In the United States, such marriages are now highly stigmatized, and laws ban most or all first-cousin marriage in 30 states. Specifics vary: in South Korea, historically it was illegal to marry someone with the same last name and same ancestral line.

An Avunculate marriage is a marriage that occurs between an uncle and his niece or between an aunt and her nephew. Such marriages are illegal in most countries due to incest restrictions. However, a small number of countries have legalized it, including Argentina, Australia, Austria, Malaysia, and Russia.
In various societies the choice of partner is often limited to suitable persons from specific social groups. In some societies the rule is that a partner is selected from an individual's own social group – endogamy, this is often the case in class- and caste-based societies. But in other societies a partner must be chosen from a different group than one's own – exogamy, this may be the case in societies practicing totemic religion where society is divided into several exogamous totemic clans, such as most Aboriginal Australian societies. In other societies a person is expected to marry their cross-cousin, a woman must marry her father's sister's son and a man must marry his mother's brother's daughter – this is often the case if either a society has a rule of tracing kinship exclusively through patrilineal or matrilineal descent groups as among the Akan people of West Africa. Another kind of marriage selection is the levirate marriage in which widows are obligated to marry their husband's brother, mostly found in societies where kinship is based on endogamous clan groups.

Religion has commonly weighed in on the matter of which relatives, if any, are allowed to marry. Relations may be by consanguinity or affinity, meaning by blood or by marriage. On the marriage of cousins, Catholic policy has evolved from initial acceptance, through a long period of general prohibition, to the contemporary requirement for a dispensation. Islam has always allowed it, while Hindu texts vary widely.

In a wide array of lineage-based societies with a classificatory kinship system, potential spouses are sought from a specific class of relative as determined by a prescriptive marriage rule. This rule may be expressed by anthropologists using a "descriptive" kinship term, such as a "man's mother's brother's daughter" (also known as a "cross-cousin"). Such descriptive rules mask the participant's perspective: a man should marry a woman from his mother's lineage. Within the society's kinship terminology, such relatives are usually indicated by a specific term which sets them apart as potentially marriageable. Pierre Bourdieu notes, however, that very few marriages ever follow the rule, and that when they do so, it is for "practical kinship" reasons such as the preservation of family property, rather than the "official kinship" ideology.

Insofar as regular marriages following prescriptive rules occur, lineages are linked together in fixed relationships; these ties between lineages may form political alliances in kinship dominated societies. French structural anthropologist Claude Lévi-Strauss developed alliance theory to account for the "elementary" kinship structures created by the limited number of prescriptive marriage rules possible.

A pragmatic (or 'arranged') marriage is made easier by formal procedures of family or group politics. A responsible authority sets up or encourages the marriage; they may, indeed, engage a professional matchmaker to find a suitable spouse for an unmarried person. The authority figure could be parents, family, a religious official, or a group consensus. In some cases, the authority figure may choose a match for purposes other than marital harmony.

A forced marriage is a marriage in which one or both of the parties is married against their will. Forced marriages continue to be practiced in parts of the world, especially in South Asia and Africa. The line between forced marriage and consensual marriage may become blurred, because the social norms of these cultures dictate that one should never oppose the desire of one's parents/relatives in regard to the choice of a spouse; in such cultures it is not necessary for violence, threats, intimidation etc. to occur, the person simply "consents" to the marriage even if they don't want it, out of the implied social pressure and duty. The customs of bride price and dowry, that exist in parts of the world, can lead to buying and selling people into marriage.

In some societies, ranging from Central Asia to the Caucasus to Africa, the custom of bride kidnapping still exists, in which a woman is captured by a man and his friends. Sometimes this covers an elopement, but sometimes it depends on sexual violence. In previous times, "raptio" was a larger-scale version of this, with groups of women captured by groups of men, sometimes in war; the most famous example is The Rape of the Sabine Women, which provided the first citizens of Rome with their wives.

Other marriage partners are more or less imposed on an individual. For example, widow inheritance provides a widow with another man from her late husband's brothers.

In rural areas of India, child marriage is practiced, with parents often arranging the wedding, sometimes even before the child is born. This practice was made illegal under the Child Marriage Restraint Act of 1929.

The financial aspects of marriage vary between cultures and have changed over time.

In some cultures, dowries and bridewealth continue to be required today. In both cases, the financial arrangements are usually made between the groom (or his family) and the bride's family; with the bride often not being involved in the negotiations, and often not having a choice in whether to participate in the marriage.

In Early modern Britain, the social status of the couple was supposed to be equal. After the marriage, all the property (called "fortune") and expected inheritances of the wife belonged to the husband.

A dowry is "a process whereby parental property is distributed to a daughter at her marriage (i.e. "inter vivos") rather than at the holder's death ("mortis causa")… A dowry establishes some variety of conjugal fund, the nature of which may vary widely. This fund ensures her support (or endowment) in widowhood and eventually goes to provide for her sons and daughters."

In some cultures, especially in countries such as Turkey, India, Bangladesh, Pakistan, Sri Lanka, Morocco, Nepal, dowries continue to be expected. In India, thousands of dowry-related deaths have taken place on yearly basis, to counter this problem, several jurisdictions have enacted laws restricting or banning dowry (see Dowry law in India). In Nepal, dowry was made illegal in 2009. Some authors believe that the giving and receiving of dowry reflects the status and even the effort to climb high in social hierarchy.

Direct Dowry contrasts with bridewealth, which is paid by the groom or his family to the bride's parents, and with indirect dowry (or dower), which is property given to the bride herself by the groom at the time of marriage and which remains under her ownership and control.

In the Jewish tradition, the rabbis in ancient times insisted on the marriage couple entering into a prenuptial agreement, called a "ketubah". Besides other things, the "ketubah" provided for an amount to be paid by the husband in the event of a divorce or his estate in the event of his death. This amount was a replacement of the biblical dower or bride price, which was payable at the time of the marriage by the groom to the father of the bride. This innovation was put in place because the biblical bride price created a major social problem: many young prospective husbands could not raise the bride price at the time when they would normally be expected to marry. So, to enable these young men to marry, the rabbis, in effect, delayed the time that the amount would be payable, when they would be more likely to have the sum. It may also be noted that both the dower and the "ketubah" amounts served the same purpose: the protection for the wife should her support cease, either by death or divorce. The only difference between the two systems was the timing of the payment. It is the predecessor to the wife's present-day entitlement to maintenance in the event of the breakup of marriage, and family maintenance in the event of the husband not providing adequately for the wife in his will. Another function performed by the "ketubah" amount was to provide a disincentive for the husband contemplating divorcing his wife: he would need to have the amount to be able to pay to the wife.

Morning gifts, which might also be arranged by the bride's father rather than the bride, are given to the bride herself; the name derives from the Germanic tribal custom of giving them the morning after the wedding night. She might have control of this morning gift during the lifetime of her husband, but is entitled to it when widowed. If the amount of her inheritance is settled by law rather than agreement, it may be called dower. Depending on legal systems and the exact arrangement, she may not be entitled to dispose of it after her death, and may lose the property if she remarries. Morning gifts were preserved for centuries in morganatic marriage, a union where the wife's inferior social status was held to prohibit her children from inheriting a noble's titles or estates. In this case, the morning gift would support the wife and children. Another legal provision for widowhood was jointure, in which property, often land, would be held in joint tenancy, so that it would automatically go to the widow on her husband's death.

Islamic tradition has similar practices. A 'mahr', either immediate or deferred, is the woman's portion of the groom's wealth (divorce) or estate (death). These amounts are usually set on the basis of the groom's own and family wealth and incomes, but in some parts these are set very high so as to provide a disincentive for the groom exercising the divorce, or the husband's family 'inheriting' a large portion of the estate, especially if there are no male offspring from the marriage. In some countries, including Iran, the mahr or alimony can amount to more than a man can ever hope to earn, sometimes up to US$1,000,000 (4000 official Iranian gold coins). If the husband cannot pay the mahr, either in case of a divorce or on demand, according to the current laws in Iran, he will have to pay it by installments. Failure to pay the mahr might even lead to imprisonment.

Bridewealth is a common practice in parts of Southeast Asia (Thailand, Cambodia), parts of Central Asia, and in much of sub-Saharan Africa. It is also known as brideprice although this has fallen in disfavor as it implies the purchase of the bride. Bridewealth is the amount of money or property or wealth paid by the groom or his family to the parents of a woman upon the marriage of their daughter to the groom. In anthropological literature, bride price has often been explained as payment made to compensate the bride's family for the loss of her labor and fertility. In some cases, bridewealth is a means by which the groom's family's ties to the children of the union are recognized.

In some countries a married person or couple benefits from various taxation advantages not available to a single person. For example, spouses may be allowed to average their combined incomes. This is advantageous to a married couple with disparate incomes. To compensate for this, countries may provide a higher tax bracket for the averaged income of a married couple. While income averaging might still benefit a married couple with a stay-at-home spouse, such averaging would cause a married couple with roughly equal personal incomes to pay more total tax than they would as two single persons. In the United States, this is called the marriage penalty.

When the rates applied by the tax code are not based income averaging, but rather on the "sum" of individuals' incomes, higher rates will usually apply to each individual in a two-earner households in a progressive tax systems. This is most often the case with high-income taxpayers and is another situation called a marriage penalty.

Conversely, when progressive tax is levied on the individual with no consideration for the partnership, dual-income couples fare much better than single-income couples with similar household incomes. The effect can be increased when the welfare system treats the same income as a shared income thereby denying welfare access to the non-earning spouse. Such systems apply in Australia and Canada, for example.

In many Western cultures, marriage usually leads to the formation of a new household comprising the married couple, with the married couple living together in the same home, often sharing the same bed, but in some other cultures this is not the tradition. Among the Minangkabau of West Sumatra, residency after marriage is matrilocal, with the husband moving into the household of his wife's mother. Residency after marriage can also be patrilocal or avunculocal. In these cases, married couples may not form an independent household, but remain part of an extended family household.

Early theories explaining the determinants of postmarital residence connected it with the sexual division of labor. However, to date, cross-cultural tests of this hypothesis using worldwide samples have failed to find any significant relationship between these two variables. However, Korotayev's tests show that the female contribution to subsistence does correlate significantly with matrilocal residence in general. However, this correlation is masked by a general polygyny factor.

Although, in different-sex marriages, an increase in the female contribution to subsistence tends to lead to matrilocal residence, it also tends simultaneously to lead to general non-sororal polygyny which effectively destroys matrilocality. If this polygyny factor is controlled (e.g., through a multiple regression model), division of labor turns out to be a significant predictor of postmarital residence. Thus, Murdock's hypotheses regarding the relationships between the sexual division of labor and postmarital residence were basically correct, though the actual relationships between those two groups of variables are more complicated than he expected.

There has been a trend toward the neolocal residence in western societies.

Marriage laws refer to the legal requirements which determine the validity of a marriage, which vary considerably between countries.

Article 16 of the Universal Declaration of Human Rights declares that "Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution. Marriage shall be entered into only with the free and full consent of the intending spouses."

A marriage bestows rights and obligations on the married parties, and sometimes on relatives as well, being the sole mechanism for the creation of affinal ties (in-laws). These may include, depending on jurisdiction:

These rights and obligations vary considerably between societies, and between groups within society. These might include arranged marriages, family obligations, the legal establishment of a nuclear family unit, the legal protection of children and public declaration of commitment.

In many countries today, each marriage partner has the choice of keeping his or her property separate or combining properties. In the latter case, called community property, when the marriage ends by divorce each owns half. In lieu of a will or trust, property owned by the deceased generally is inherited by the surviving spouse.

In some legal systems, the partners in a marriage are "jointly liable" for the debts of the marriage. This has a basis in a traditional legal notion called the "Doctrine of Necessities" whereby, in a heterosexual marriage, a husband was responsible to provide necessary things for his wife. Where this is the case, one partner may be sued to collect a debt for which they did not expressly contract. Critics of this practice note that debt collection agencies can abuse this by claiming an unreasonably wide range of debts to be expenses of the marriage. The cost of defense and the burden of proof is then placed on the non-contracting party to prove that the expense is not a debt of the family. The respective maintenance obligations, both during and eventually after a marriage, are regulated in most jurisdictions; alimony is one such method.

Marriage is an institution that is historically filled with restrictions. From age, to race, to social status, to consanguinity, to gender, restrictions are placed on marriage by society for reasons of benefiting the children, passing on healthy genes, maintaining cultural values, or because of prejudice and fear. Almost all cultures that recognize marriage also recognize adultery as a violation of the terms of marriage.

Most jurisdictions set a minimum age for marriage, that is, a person must attain a certain age to be legally allowed to marry. This age may depend on circumstances, for instance exceptions from the general rule may be permitted if the parents of a young person express their consent and/or if a court decides that said marriage is in the best interest of the young person (often this applies in cases where a girl is pregnant). Although most age restrictions are in place in order to prevent children from being forced into marriages, especially to much older partners – marriages which can have negative education and health related consequences, and lead to child sexual abuse and other forms of violence – such child marriages remain common in parts of the world. According to the UN, child marriages are most common in rural sub-Saharan Africa and South Asia. The ten countries with the highest rates of child marriage are: Niger (75%), Chad, Central African Republic, Bangladesh, Guinea, Mozambique, Mali, Burkina Faso, South Sudan, and Malawi.

To prohibit incest and eugenic reasons, marriage laws have set restrictions for relatives to marry. Direct blood relatives are usually prohibited to marry, while for branch line relatives, laws are wary.

Laws banning "race-mixing" were enforced in certain North American jurisdictions from 1691 until 1967, in Nazi Germany (The Nuremberg Laws) from 1935 until 1945, and in South Africa during most part of the Apartheid era (1949–1985). All these laws primarily banned marriage between persons of different racially or ethnically defined groups, which was termed "amalgamation" or "miscegenation" in the U.S. The laws in Nazi Germany and many of the U.S. states, as well as South Africa, also banned sexual relations between such individuals.

In the United States, laws in some but not all of the states prohibited the marriage of whites and blacks, and in many states also the intermarriage of whites with Native Americans or Asians. In the U.S., such laws were known as anti-miscegenation laws. From 1913 until 1948, 30 out of the then 48 states enforced such laws. Although an "Anti-Miscegenation Amendment" to the United States Constitution was proposed in 1871, in 1912–1913, and in 1928, no nationwide law against racially mixed marriages was ever enacted. In 1967, the Supreme Court of the United States unanimously ruled in "Loving v. Virginia" that anti-miscegenation laws are unconstitutional. With this ruling, these laws were no longer in effect in the remaining 16 states that still had them.

The Nazi ban on interracial marriage and interracial sex was enacted in September 1935 as part of the Nuremberg Laws, the "Gesetz zum Schutze des deutschen Blutes und der deutschen Ehre" (The Law for the Protection of German Blood and German Honour). The Nuremberg Laws classified Jews as a race and forbade marriage and extramarital sexual relations at first with people of Jewish descent, but was later ended to the "Gypsies, Negroes or their bastard offspring" and people of "German or related blood". Such relations were marked as "Rassenschande" (lit. "race-disgrace") and could be punished by imprisonment (usually followed by deportation to a concentration camp) and even by death.

South Africa under apartheid also banned interracial marriage. The Prohibition of Mixed Marriages Act, 1949 prohibited marriage between persons of different races, and the Immorality Act of 1950 made sexual relations with a person of a different race a crime.

Same-sex marriage is legally performed and recognized (nationwide or in some jurisdictions) in Argentina, Australia, Austria, Belgium, Brazil, Canada, Colombia, Costa Rica, Denmark, Ecuador, Finland, France, Germany, Iceland, Ireland, Luxembourg, Malta, Mexico, the Netherlands, New Zealand, Norway, Portugal, South Africa, Spain, Sweden, Taiwan, the United Kingdom, the United States, and Uruguay. Israel recognizes same-sex marriages entered into abroad as full marriages. Furthermore, the Inter-American Court of Human Rights has issued a ruling that is expected to facilitate recognition in several countries in the Americas.

The introduction of same-sex marriage has varied by jurisdiction, being variously accomplished through legislative change to marriage law, a court ruling based on constitutional guarantees of equality, or by direct popular vote (via ballot initiative or referendum). The recognition of same-sex marriage is considered to be a human right and a civil right as well as a political, social, and religious issue. The most prominent supporters of same-sex marriage are human rights and civil rights organizations as well as the medical and scientific communities, while the most prominent opponents are religious groups. Various faith communities around the world support same-sex marriage, while many religious groups oppose it. Polls consistently show continually rising support for the recognition of same-sex marriage in all developed democracies and in some developing democracies.

The establishment of recognition in law for the marriages of same-sex couples is one of the most prominent objectives of the LGBT rights movement.

Polygyny is widely practiced in mostly Muslim and African countries. In the Middle Eastern region, Israel, Turkey and Tunisia are notable exceptions.

In most other jurisdictions, polygamy is illegal. For example, In the United States, polygamy is illegal in all 50 states.

In the late-19th century, citizens of the self-governing territory of what is present-day Utah were forced by the United States federal government to abandon the practice of polygamy through the vigorous enforcement of several Acts of Congress, and eventually complied. The Church of Jesus Christ of Latter-day Saints formally abolished the practice in 1890, in a document labeled 'The Manifesto' (see Latter Day Saint polygamy in the late-19th century). Among American Muslims, a small minority of around 50,000 to 100,000 people are estimated to live in families with a husband maintaining an illegal polygamous relationship.

Several countries such as India and Sri Lanka, permit only their Islamic citizens to practice polygamy. Some Indians have converted to Islam in order to bypass such legal restrictions. Predominantly Christian nations usually do not allow polygamous unions, with a handful of exceptions being the Republic of the Congo, Uganda, and Zambia. Myanmar (frequently referred to as Burma) is also the only predominantly Buddhist nation to allow for civil polygynous marriages, though such is rarely tolerated by the Burmese population.

In various jurisdictions, a civil marriage may take place as part of the religious marriage ceremony, although they are theoretically distinct. Some jurisdictions allow civil marriages in circumstances which are notably not allowed by particular religions, such as same-sex marriages or civil unions.

The opposite case may happen as well. Partners may not have full juridical acting capacity and churches may have less strict limits than the civil jurisdictions. This particularly applies to minimum age, or physical infirmities.

It is possible for two people to be recognised as married by a religious or other institution, but not by the state, and hence without the legal rights and obligations of marriage; or to have a civil marriage deemed invalid and sinful by a religion. Similarly, a couple may remain married in religious eyes after a civil divorce.

A marriage is usually formalized at a wedding or marriage ceremony. The ceremony may be officiated either by a religious official, by a government official or by a state approved celebrant. In various European and some Latin American countries, any religious ceremony must be held separately from the required civil ceremony. Some countries – such as Belgium, Bulgaria, France, the Netherlands, Romania and Turkey – require that a civil ceremony take place before any religious one. In some countries – notably the United States, Canada, the United Kingdom, the Republic of Ireland, Norway and Spain – both ceremonies can be held together; the officiant at the religious and civil ceremony also serving as agent of the state to perform the civil ceremony. To avoid any implication that the state is "recognizing" a religious marriage (which is prohibited in some countries) – the "civil" ceremony is said to be taking place at the same time as the religious ceremony. Often this involves simply signing a register during the religious ceremony. If the civil element of the religious ceremony is omitted, the marriage ceremony is not recognized as a marriage by government under the law.

Some countries, such as Australia, permit marriages to be held in private and at any location; others, including England and Wales, require that the civil ceremony be conducted in a place open to the public and specially sanctioned by law for the purpose. In England, the place of marriage formerly had to be a church or register office, but this was extended to any public venue with the necessary licence. An exception can be made in the case of marriage by special emergency license (UK: licence), which is normally granted only when one of the parties is terminally ill. Rules about where and when persons can marry vary from place to place. Some regulations require one of the parties to reside within the jurisdiction of the register office (formerly parish).

Each religious authority has rules for the manner in which marriages are to be conducted by their officials and members. Where religious marriages are recognised by the state, the officiator must also conform with the law of the jurisdiction.

In a small number of jurisdictions marriage relationships may be created by the operation of the law alone. Unlike the typical ceremonial marriage with legal contract, wedding ceremony, and other details, a common-law marriage may be called "marriage by habit and repute (cohabitation)." A de facto common-law marriage without a license or ceremony is legally binding in some jurisdictions but has no legal consequence in others.

A "civil union", also referred to as a "civil partnership", is a legally recognized form of partnership similar to marriage. Beginning with Denmark in 1989, civil unions under one name or another have been established by law in several countries in order to provide same-sex couples rights, benefits, and responsibilities similar (in some countries, identical) to opposite-sex civil marriage. In some jurisdictions, such as Brazil, New Zealand, Uruguay, Ecuador, France and the U.S. states of Hawaii and Illinois, civil unions are also open to opposite-sex couples.

Sometimes people marry to take advantage of a certain situation, sometimes called a marriage of convenience or a sham marriage. In 2003, over 180,000 immigrants were admitted to the U.S. as spouses of U.S. citizens; more were admitted as fiancés of US citizens for the purpose of being married within 90 days. These marriages had a diverse range of motives, including obtaining permanent residency, securing an inheritance that has a marriage clause, or to enroll in health insurance, among many others. While all marriages have a complex combination of conveniences motivating the parties to marry, a marriage of convenience is one that is devoid of normal reasons to marry. In certain countries like Singapore sham marriages are punishable criminal offences.

People have proposed arguments against marriage for reasons that include political, philosophical and religious criticisms; concerns about the divorce rate; individual liberty and gender equality; questioning the necessity of having a personal relationship sanctioned by government or religious authorities; or the promotion of celibacy for religious or philosophical reasons.

Feminist theory approaches opposite-sex marriage as an institution traditionally rooted in patriarchy that promotes male superiority and power over women. This power dynamic conceptualizes men as "the provider operating in the public sphere" and women as "the caregivers operating within the private sphere". "Theoretically, women ... [were] defined as the property of their husbands ... The adultery of a woman was always treated with more severity than that of a man." "[F]eminist demands for a wife's control over her own property were not met [in parts of Britain] until ... [laws were passed in the late 19th century]."

Traditional heterosexual marriage imposed an obligation of the wife to be sexually available for her husband and an obligation of the husband to provide material/financial support for the wife. Numerous philosophers, feminists and other academic figures have commented on this throughout history, condemning the hypocrisy of legal and religious authorities in regard to sexual issues; pointing to the lack of choice of a woman in regard to controlling her own sexuality; and drawing parallels between marriage, an institution promoted as sacred, and prostitution, widely condemned and vilified (though often tolerated as a "necessary evil"). Mary Wollstonecraft, in the 18th century, described marriage as "legal prostitution". Emma Goldman wrote in 1910: "To the moralist prostitution does not consist so much in the fact that the woman sells her body, but rather that she sells it out of wedlock". Bertrand Russell in his book Marriage and Morals wrote that: "Marriage is for woman the commonest mode of livelihood, and the total amount of undesired sex endured by women is probably greater in marriage than in prostitution." Angela Carter in Nights at the Circus wrote: "What is marriage but prostitution to one man instead of many?"

Some critics object to what they see as propaganda in relation to marriage – from the government, religious organizations, the media – which aggressively promote marriage as a solution for all social problems; such propaganda includes, for instance, marriage promotion in schools, where children, especially girls, are bombarded with positive information about marriage, being presented only with the information prepared by authorities.

The performance of dominant gender roles by men and submissive gender roles by women influence the power dynamic of a heterosexual marriage. In some American households, women internalize gender role stereotypes and often assimilate into the role of "wife", "mother", and "caretaker" in conformity to societal norms and their male partner. Author bell hooks states "within the family structure, individuals learn to accept sexist oppression as 'natural' and are primed to support other forms of oppression, including heterosexist domination." "[T]he cultural, economic, political and legal supremacy of the husband" was "[t]raditional ... under English law". This patriarchal dynamic is contrasted with a conception of egalitarian or Peer Marriage in which power and labour are divided equally, and not according to gender roles.

In the US, studies have shown that, despite egalitarian ideals being common, less than half of respondents viewed their opposite-sex relationships as equal in power, with unequal relationships being more commonly dominated by the male partner. Studies also show that married couples find the highest level of satisfaction in egalitarian relationships and lowest levels of satisfaction in wife dominate relationships. In recent years, egalitarian or Peer Marriages have been receiving increasing focus and attention politically, economically and culturally in a number of countries, including the United States.

Different societies demonstrate variable tolerance of extramarital sex. The Standard Cross-Cultural Sample describes the occurrence of extramarital sex by gender in over 50 pre-industrial cultures. The occurrence of extramarital sex by men is described as "universal" in 6 cultures, "moderate" in 29 cultures, "occasional" in 6 cultures, and "uncommon" in 10 cultures. The occurrence of extramarital sex by women is described as "universal" in 6 cultures, "moderate" in 23 cultures, "occasional" in 9 cultures, and "uncommon" in 15 cultures. Three studies using nationally representative samples in the United States found that between 10–15% of women and 20–25% of men engage in extramarital sex.

Many of the world's major religions look with disfavor on sexual relations outside marriage. There are non-secular states that sanction criminal penalties for sexual intercourse before marriage. Sexual relations by a married person with someone other than his/her spouse is known as adultery. Adultery is considered in many jurisdictions to be a crime and grounds for divorce.

In some countries, such as Saudi Arabia, Pakistan, Afghanistan, Iran, Kuwait, Maldives, Morocco, Oman, Mauritania, United Arab Emirates, Sudan, Yemen, any form of sexual activity outside marriage is illegal.

In some parts of the world, women and girls accused of having sexual relations outside marriage are at risk of becoming victims of honor killings committed by their families. In 2011 several people were sentenced to death by stoning after being accused of adultery in Iran, Somalia, Afghanistan, Sudan, Mali and Pakistan. Practices such as honor killings and stoning continue to be supported by mainstream politicians and other officials in some countries. In Pakistan, after the 2008 Balochistan honour killings in which five women were killed by tribesmen of the Umrani Tribe of Balochistan, Pakistani Federal Minister for Postal Services Israr Ullah Zehri defended the practice; he said: "These are centuries-old traditions, and I will continue to defend them. Only those who indulge in immoral acts should be afraid."

An issue that is a serious concern regarding marriage and which has been the object of international scrutiny is that of sexual violence within marriage. Throughout much of the history, in most cultures, sex in marriage was considered a 'right', that could be taken by force (often by a man from a woman), if 'denied'. As the concept of human rights started to develop in the 20th century, and with the arrival of second-wave feminism, such views have become less widely held.

The legal and social concept of marital rape has developed in most industrialized countries in the mid- to late 20th century; in many other parts of the world it is not recognized as a form of abuse, socially or legally. Several countries in Eastern Europe and Scandinavia made marital rape illegal before 1970, and other countries in Western Europe and the English-speaking Western world outlawed it in the 1980s and 1990s. In England and Wales, marital rape was made illegal in 1991. Although marital rape is being increasingly criminalized in developing countries too, cultural, religious, and traditional ideologies about "conjugal rights" remain very strong in many parts of the world; and even in many countries that have adequate laws against rape in marriage these laws are rarely enforced.

Apart from the issue of rape committed against one's spouse, marriage is, in many parts of the world, closely connected with other forms of sexual violence: in some places, like Morocco, unmarried girls and women who are raped are often forced by their families to marry their rapist. Because being the victim of rape and losing virginity carry extreme social stigma, and the victims are deemed to have their "reputation" tarnished, a marriage with the rapist is arranged. This is claimed to be in the advantage of both the victim – who does not remain unmarried and doesn't lose social status – and of the rapist, who avoids punishment. In 2012, after a Moroccan 16-year-old girl committed suicide after having been forced by her family to marry her rapist and enduring further abuse by the rapist after they married, there have been protests from activists against this practice which is common in Morocco.

In some societies, the very high social and religious importance of marital fidelity, especially female fidelity, has as result the criminalization of adultery, often with harsh penalties such as stoning or flogging; as well as leniency towards punishment of violence related to infidelity (such as honor killings). In the 21st century, criminal laws against adultery have become controversial with international organizations calling for their abolition. Opponents of adultery laws argue that these laws are a major contributor to discrimination and violence against women, as they are enforced selectively mostly against women; that they prevent women from reporting sexual violence; and that they maintain social norms which justify violent crimes committed against women by husbands, families and communities. A Joint Statement by the United Nations Working Group on discrimination against women in law and in practice states that "Adultery as a criminal offence violates women's human rights". Some human rights organizations argue that the criminalization of adultery also violates internationally recognized protections for private life, as it represents an arbitrary interference with an individual's privacy, which is not permitted under international law.

The laws surrounding heterosexual marriage in many countries have come under international scrutiny because they contradict international standards of human rights; institutionalize violence against women, child marriage and forced marriage; require the permission of a husband for his wife to work in a paid job, sign legal documents, file criminal charges against someone, sue in civil court etc.; sanction the use by husbands of violence to "discipline" their wives; and discriminate against women in divorce.

Such things were legal even in many Western countries until recently: for instance, in France, married women obtained the right to work without their husband's permission in 1965, and in West Germany women obtained this right in 1977 (by comparison women in East Germany had many more rights). In Spain, during Franco's era, a married woman needed her husband's consent, referred to as the "permiso marital", for almost all economic activities, including employment, ownership of property, and even traveling away from home; the "permiso marital" was abolished in 1975.

An absolute submission of a wife to her husband is accepted as natural in many parts of the world, for instance surveys by UNICEF have shown that the percentage of women aged 15–49 who think that a husband is justified in hitting or beating his wife under certain circumstances is as high as 90% in Afghanistan and Jordan, 87% in Mali, 86% in Guinea and Timor-Leste, 81% in Laos, 80% in Central African Republic. Detailed results from Afghanistan show that 78% of women agree with a beating if the wife "goes out without telling him [the husband]" and 76% agree "if she argues with him".

Throughout history, and still today in many countries, laws have provided for extenuating circumstances, partial or complete defenses, for men who killed their wives due to adultery, with such acts often being seen as crimes of passion and being covered by legal defenses such as provocation or defense of family honor.

While international law and conventions recognize the need for consent for entering a marriage – namely that people cannot be forced to get married against their will – the right to obtain a divorce is not recognized; therefore holding a person in a marriage against their will (if such person has consented to entering in it) is not considered a violation of human rights, with the issue of divorce being left at the appreciation of individual states. The European Court of Human Rights has repeatedly ruled that under the European Convention on Human Rights there is neither a right to apply to divorce, nor a right to obtain the divorce if applied for it; in 2017, in "Babiarz v. Poland", the Court ruled that Poland was entitled to deny a divorce because the grounds for divorce were not met, even if the marriage in question was acknowledged both by Polish courts and by the ECHR as being a legal fiction involving a long-term separation where the husband lived with another woman with whom he had an 11-year-old child.

In the EU, the last country to allow divorce was Malta, in 2011. Around the world, the only countries to forbid divorce are Philippines and Vatican City, although in practice in many countries which use a fault-based divorce system obtaining a divorce is very difficult. The ability to divorce, in law and practice, has been and continues to be a controversial issue in many countries, and public discourse involves different ideologies such as feminism, social conservatism, religious interpretations.

In recent years, the customs of dowry and bride price have received international criticism for inciting conflicts between families and clans; contributing to violence against women; promoting materialism; increasing property crimes (where men steal goods such as cattle in order to be able to pay the bride price); and making it difficult for poor people to marry. African women's rights campaigners advocate the abolishing of bride price, which they argue is based on the idea that women are a form of property which can be bought. Bride price has also been criticized for contributing to child trafficking as impoverished parents sell their young daughters to rich older men. A senior Papua New Guinea police officer has called for the abolishing of bride price arguing that it is one of the main reasons for the mistreatment of women in that country. The opposite practice of dowry has been linked to a high level of violence (see Dowry death) and to crimes such as extortion.

Historically, and still in many countries, children born outside marriage suffered severe social stigma and discrimination. In England and Wales, such children were known as bastards and whoresons.

There are significant differences between world regions in regard to the social and legal position of non-marital births, ranging from being fully accepted and uncontroversial to being severely stigmatized and discriminated.

The 1975 European Convention on the Legal Status of Children Born out of Wedlock protects the rights of children born to unmarried parents. The convention states, among others, that: "The father and mother of a child born out of wedlock shall have the same obligation to maintain the child as if it were born in wedlock" and that "A child born out of wedlock shall have the same right of succession in the estate of its father and its mother and of a member of its father's or mother's family, as if it had been born in wedlock."

While in most Western countries legal inequalities between children born inside and outside marriage have largely been abolished, this is not the case in some parts of the world.

The legal status of an unmarried father differs greatly from country to country. Without voluntary formal recognition of the child by the father, in most cases there is a need of due process of law in order to establish paternity. In some countries however, unmarried cohabitation of a couple for a specific period of time does create a presumption of paternity similar to that of formal marriage. This is the case in Australia. Under what circumstances can a paternity action be initiated, the rights and responsibilities of a father once paternity has been established (whether he can obtain parental responsibility and whether he can be forced to support the child) as well as the legal position of a father who voluntarily acknowledges the child, vary widely by jurisdiction. A special situation arises when a married woman has a child by a man other than her husband. Some countries, such as Israel, refuse to accept a legal challenge of paternity in such a circumstance, in order to avoid the stigmatization of the child (see Mamzer, a concept under Jewish law). In 2010, the European Court of Human Rights ruled in favor of a German man who had fathered twins with a married woman, granting him right of contact with the twins, despite the fact that the mother and her husband had forbidden him to see the children.

The steps that an unmarried father must take in order to obtain rights to his child vary by country. In some countries (such as the UK – since 2003 in England and Wales, 2006 in Scotland, and 2002 in Northern Ireland) it is sufficient for the father to be listed on the birth certificate for him to have parental rights; in other countries, such as Ireland, simply being listed on the birth certificate does not offer any rights, additional legal steps must be taken (if the mother agrees, the parents can both sign a "statutory declaration", but if the mother does not agree, the father has to apply to court).

Children born outside marriage have become more common, and in some countries, the majority. Recent data from Latin America showed figures for non-marital childbearing to be 74% for Colombia, 69% for Peru, 68% for Chile, 66% for Brazil, 58% for Argentina, 55% for Mexico. In 2012, in the European Union, 40% of births were outside marriage, and in the United States, in 2013, the figure was similar, at 41%. In the United Kingdom 48% of births were to unmarried women in 2012; in Ireland the figure was 35%.

During the first half of the 20th century, unmarried women in some Western countries were coerced by authorities to give their children up for adoption. This was especially the case in Australia, through the forced adoptions in Australia, with most of these adoptions taking place between the 1950s and the 1970s. In 2013, Julia Gillard, then Prime Minister of Australia, offered a national apology to those affected by the forced adoptions.

Some married couples choose not to have children. Others are unable to have children because of infertility or other factors preventing conception or the bearing of children. In some cultures, marriage imposes an "obligation" on women to bear children. In northern Ghana, for example, payment of bridewealth signifies a woman's requirement to bear children, and women using birth control face substantial threats of physical abuse and reprisals.

Religions develop in specific geographic and social milieux. Religious attitudes and practices relating to marriage vary, but have many similarities.

The Bahá'í Faith encourages marriage and views it as a mutually strengthening bond, but it is not obligatory. A Bahá'í marriage requires the couple to choose each other, and then obtain the consent of all living parents.

Modern Christianity bases its views on marriage upon the teachings of Jesus and the Paul the Apostle. Many of the largest Christian denominations regard marriage as a sacrament, sacred institution, or covenant. However, this was not the case in the Roman Catholic Church before the 1184 Council of Verona officially recognized it as such. Before then, no specific ritual was prescribed for celebrating a marriage: "Marriage vows did not have to be exchanged in a church, nor was a priest's presence required. A couple could exchange consent anywhere, anytime." The Church only formally recognized the union and it was finalized with the couple together partaking Holy Communion.

Decrees on marriage of the Roman Catholic Council of Trent (twenty-fourth session of 1563) made the validity of marriage dependent on the wedding occurring in the presence of a priest and two witnesses. The absence of a requirement of parental consent ended a debate that proceeded from the 12th century. In the case of a civil divorce, the innocent spouse had and has no right to marry again until the death of the other spouse terminates the still valid marriage, even if the other spouse was guilty of adultery.

The Christian Church performed marriages in the narthex of the church prior to the 16th century, when the emphasis was on the marital contract and betrothal. Subsequently, the ceremony moved inside the sacristy of the church.

Christians often marry for religious reasons, ranging from following the biblical injunction for a "man to leave his father and mother and cleave to his wife, and the two shall become one", to accessing the Divine grace of the Roman Catholic Sacrament.

Catholics, Eastern Orthodox, as well as many Anglicans and Methodists, consider marriage termed "holy matrimony" to be an expression of divine grace, termed a "sacrament" and "mystery" in the first two Christian traditions. In Western ritual, the ministers of the sacrament are the spouses themselves, with a bishop, priest, or deacon merely witnessing the union on behalf of the Church and blessing it. In Eastern ritual churches, the bishop or priest functions as the actual minister of the Sacred Mystery; Eastern Orthodox deacons may not perform marriages. Western Christians commonly refer to marriage as a vocation, while Eastern Christians consider it an ordination and a martyrdom, though the theological emphases indicated by the various names are not excluded by the teachings of either tradition. Marriage is commonly celebrated in the context of a Eucharistic service (a nuptial Mass or Divine Liturgy). The sacrament of marriage is indicative of the relationship between Christ and the Church.

The Roman Catholic tradition of the 12th and 13th centuries defined marriage as a sacrament ordained by God, signifying the mystical marriage of Christ to his Church.
The matrimonial covenant, by which a man and a woman establish between themselves a partnership of the whole of life, is by its nature ordered toward the good of the spouses and the procreation and education of offspring; this covenant between baptized persons has been raised by Christ the Lord to the dignity of a sacrament.
For Catholic and Methodist Christians, the mutual love between husband and wife becomes an image of the eternal love with which God loves humankind. In the United Methodist Church, the celebration of Holy Matrimony ideally occurs in the context of a Service of Worship, which includes the celebration of the Eucharist. Likewise, the celebration of marriage between two Catholics normally takes place during the public liturgical celebration of the Holy Mass, because of its sacramental connection with the unity of the Paschal mystery of Christ (Communion). Sacramental marriage confers a perpetual and exclusive bond between the spouses. By its nature, the institution of marriage and conjugal love is ordered to the procreation and upbringing of offspring. Marriage creates rights and duties in the Church between the spouses and towards their children: "[e]ntering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment". According to current Roman Catholic legislation, progeny of annulled relationships are considered legitimate. Civilly remarried persons who civilly divorced a living and lawful spouse are not separated from the Church, but they cannot receive Eucharistic Communion.

Divorce and remarriage, while generally not encouraged, are regarded differently by each Christian denomination. Most Protestant Churches allow persons to marry again after a divorce, while other require an annulment. The Eastern Orthodox Church allows divorce for a limited number of reasons, and in theory, but usually not in practice, requires that a marriage after divorce be celebrated with a penitential overtone. With respect to marriage between a Christian and a pagan, the early Church "sometimes took a more lenient view, invoking the so-called Pauline privilege of permissible separation (1 Cor. 7) as legitimate grounds for allowing a convert to divorce a pagan spouse and then marry a Christian."

The Catholic Church adheres to the proscription of Jesus in "Matthew", 19: 6 that married spouses who have consummated their marriage "are no longer two, but one flesh. Therefore, what God has joined together, no human being must separate.” Consequently, the Catholic Church understands that it is wholly without authority to terminate a sacramentally valid and consummated marriage, and its "Codex Iuris Canonici" (1983 Code of Canon Law) confirms this in Canons 1055–7. Specifically, Canon 1056 declares that "the essential properties of marriage are unity and "indissolubility"; in [C]hristian marriage they acquire a distinctive "firmness" by reason of the sacrament." Canon 1057, §2 declares that marriage is "an "irrevocable" covenant". Therefore, divorce of such a marriage is a metaphysical, moral, and legal impossibility. However, the Church has the authority to annul a presumed "marriage" by declaring it to have been invalid from the beginning, i. e., declaring it not to be and never to have been a marriage, in an annulment procedure, which is basically a fact-finding and fact-declaring effort.

For Protestant denominations, the purposes of marriage include intimate companionship, rearing children, and mutual support for both spouses to fulfill their life callings. Most Reformed Christians did not regard marriage to the status of a sacrament "because they did not regard matrimony as a necessary means of grace for salvation"; nevertheless it is considered a covenant between spouses before God. In addition, some Protestant denominations (such as the Methodist Churches) affirmed that Holy Matrimony is a "means of grace, thus, sacramental in character".
Since the 16th century, five competing models have shaped marriage in the Western tradition, as described by John Witte, Jr.:

Members of The Church of Jesus Christ of Latter-day Saints (LDS Church) believe that "marriage between a man and a woman is ordained of God and that the family is central to the Creator's plan for the eternal destiny of His children." Their view of marriage is that family relationships can endure beyond the grave. This is known as 'eternal marriage' which can be eternal only when authorized priesthood holders perform the sealing ordinance in sacred temples.

Although many Christian denominations do not currently perform same-sex marriages, many do, such as the Presbyterian Church (USA), some dioceses of the Episcopal Church, the Metropolitan Community Church, Quakers, United Church of Canada, and United Church of Christ congregations, and some Anglican dioceses, for example. Same-sex marriage is recognized by various religious denominations.

Islam also commends marriage, with the age of marriage being whenever the individuals feel ready, financially and emotionally.

In Islam, polygyny is allowed while polyandry is not, with the specific limitation that a man can have no more than four legal wives at any one time and an unlimited number of female slaves as concubines who may have rights similar wives, with the exception of not being free unless the man has children with them, with the requirement that the man is able and willing to partition his time and wealth equally among the respective wives and concubines (this practice of concubinage, as in Judaism, is not applicable in contemporary times and has been deemed by scholars as invalid due to shifts in views about the role of slavery in the world).

For a Muslim wedding to take place, the bridegroom and the guardian of the bride ("wali") must both agree on the marriage. Should the guardian disagree on the marriage, it may not legally take place. If the "wali" of the girl her father or paternal grandfather, he has the right to force her into marriage even against her proclaimed will, if it is her first marriage. A guardian who is allowed to force the bride into marriage is called "wali mujbir".

From an Islamic (Sharia) law perspective, the minimum requirements and responsibilities in a Muslim marriage are that the groom provide living expenses (housing, clothing, food, maintenance) to the bride, and in return, the bride's main responsibility is raising children to be proper Muslims. All other rights and responsibilities are to be decided between the husband and wife, and may even be included as stipulations in the marriage contract before the marriage actually takes place, so long as they do not go against the minimum requirements of the marriage.

In Sunni Islam, marriage must take place in the presence of at least two reliable witnesses, with the consent of the guardian of the bride and the consent of the groom. Following the marriage, the couple may consummate the marriage. To create an 'urf marriage, it is sufficient that a man and a woman indicate an intention to marry each other and recite the requisite words in front of a suitable Muslim. The wedding party usually follows but can be held days, or months later, whenever the couple and their families want to; however, there can be no concealment of the marriage as it is regarded as public notification due to the requirement of witnesses.

In Shia Islam, marriage may take place without the presence of witnesses as is often the case in temporary Nikah mut‘ah (prohibited in Sunni Islam), but with the consent of both the bride and the groom. Following the marriage they may consummate their marriage.

In Judaism, marriage is based on the laws of the Torah and is a contractual bond between spouses in which the spouses dedicate to be exclusive to one another. This contract is called Kiddushin. Though procreation is not the sole purpose, a Jewish marriage is also expected to fulfill the commandment to have children. The main focus centers around the relationship between the spouses. Kabbalistically, marriage is understood to mean that the spouses are merging into a single soul. This is why a man is considered "incomplete" if he is not married, as his soul is only one part of a larger whole that remains to be unified.

The Hebrew Bible (Christian Old Testament) describes a number of marriages, including those of Isaac (), Jacob () and Samson (). Polygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Hebrew Bible; another is that of concubinage (pilegesh) which was often arranged by a man and a woman who generally enjoyed the same rights as a full legal wife (other means of concubinage can be seen in Judges 19-20 where mass marriage by abduction was practiced as a form of punishment on transgressors). Today Ashkenazi Jews are prohibited to take more than one wife because of a ban instituted on this by Gershom ben Judah (Died 1040).

Among ancient Hebrews, marriage was a domestic affair and not a religious ceremony; the participation of a priest or rabbi was not required.

Betrothal ("erusin"), which refers to the time that this binding contract is made, is distinct from marriage itself ("nissu'in"), with the time between these events varying substantially.
In biblical times, a wife was regarded as personal property, belonging to her husband; the descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and men with more than one wife were expected to ensure that they continue to give the first wife food, clothing, and marital rights.

Since a wife was regarded as property, her husband was originally free to divorce her for any reason, at any time. Divorcing a woman against her will was also banned by Gershom ben Judah for Ashkenazi Jews. A divorced couple were permitted to get back together, unless the wife had married someone else after her divorce.

Hinduism sees marriage as a sacred duty that entails both religious and social obligations. Old Hindu literature in Sanskrit gives many different types of marriages and their categorization ranging from "Gandharva Vivaha" (instant marriage by mutual consent of participants only, without any need for even a single third person as witness) to normal (present day) marriages, to "Rakshasa Vivaha" ("demoniac" marriage, performed by abduction of one participant by the other participant, usually, but not always, with the help of other persons). In the Indian subcontinent, arranged marriages, the spouse's parents or an older family member choose the partner, are still predominant in comparison with so-called love marriages until nowadays. The Hindu Widow's Remarriage Act 1856 empowers a Hindu widow to remarry.

The Buddhist view of marriage considers marriage a secular affair and thus not a sacrament. Buddhists are expected to follow the civil laws regarding marriage laid out by their respective governments. Gautama Buddha, being a kshatriya was required by Shakyan tradition to pass a series of tests to prove himself as a warrior, before he was allowed to marry.

In a Sikh marriage, the couple walks around the "Guru Granth Sahib" holy book four times, and a holy man recites from it in the kirtan style. The ceremony is known as 'Anand Karaj' and represents the holy union of two souls united as one.

Wiccan marriages are commonly known as handfastings. Although handfastings vary for each Wiccan they often involve honoring Wiccan gods. Sex is considered a pious and sacred activity.

Marriages are correlated with better outcomes for the couple and their children, including higher income for men, better health and lower mortality. Part of these effects is due to the fact that those with better expectations get married more often. According to a systematic review on research literature, a significant part of the effect seems to be due to a true causal effect. The reason may be that marriages make particularly men become more future-oriented and take an economic and other responsibility of the family. The studies eliminate the effect of selectivity in numerous ways. However, much of the research is of low quality in this sense. On the other hand, the causal effect might be even higher if money, working skills and parenting practises are endogenous. Married men have less drug abuse and alcohol use and are more often at home during nights.

Marriage, like other close relationships, exerts considerable influence on health. Married people experience lower morbidity and mortality across such diverse health threats as cancer, heart attacks, and surgery. Research on marriage and health is part of the broader study of the benefits of social relationships.

Social ties provide people with a sense of identity, purpose, belonging, and support. Simply being married, as well as the quality of one's marriage, have been linked to diverse measures of health.

The health-protective effect of marriage is stronger for men than women. Marital status—the simple fact of being married—confers more health benefits to men than women.

Women's health is more strongly impacted than men's by marital conflict or satisfaction, such that unhappily married women do not enjoy better health relative to their single counterparts. Most research on marriage and health has focused on heterosexual couples; more work is needed to clarify the health impacts of same-sex marriage.

In most societies, the death of one of the partners terminates the marriage, and in monogamous societies this allows the other partner to remarry, though sometimes after a waiting or mourning period.

In some societies, a marriage can be annulled, when an authority declares that a marriage never happened. Jurisdictions often have provisions for void marriages or voidable marriages.

A marriage may also be terminated through divorce. Countries that have relatively recently legalized divorce are Italy (1970), Portugal (1975), Brazil (1977), Spain (1981), Argentina (1987), Paraguay (1991), Colombia (1991), Ireland (1996), Chile (2004) and Malta (2011). As of 2012, the Philippines and the Vatican City are the only jurisdictions which do not allow divorce (this is currently under discussion in Philippines). After divorce, one spouse may have to pay alimony. Laws concerning divorce and the ease with which a divorce can be obtained vary widely around the world. After a divorce or an annulment, the people concerned are free to remarry (or marry).

A statutory right of two married partners to mutually consent to divorce was enacted in western nations in the mid-20th century. In the United States no-fault divorce was first enacted in California in 1969 and the final state to legalize it was New York in 1989.

About 45% of marriages in Britain and, according to a 2009 study, 46% of marriages in the U.S. end in divorce.

The history of marriage is often considered under History of the family or legal history.

Many cultures have legends concerning the origins of marriage. The way in which a marriage is conducted and its rules and ramifications has changed over time, as has the institution itself, depending on the culture or demographic of the time.

The first recorded evidence of marriage ceremonies uniting a man and a woman dates back to approximately 2350 BC, in ancient Mesopotamia. Wedding ceremonies, as well as dowry and divorce, can be traced back to Mesopotamia and Babylonia.
According to ancient Hebrew tradition, a wife was seen as being property of high value and was, therefore, usually, carefully looked after. Early nomadic communities in the middle east practised a form of marriage known as "beena", in which a wife would own a tent of her own, within which she retains complete independence from her husband; this principle appears to survive in parts of early Israelite society, as some early passages of the Bible appear to portray certain wives as each owning a tent as a personal possession (specifically, Jael, Sarah, and Jacob's wives).

The husband, too, is indirectly implied to have some responsibilities to his wife. The Covenant Code orders "If he take him another; her food, her clothing, and her duty of marriage, shall he not diminish(or lessen)". If the husband does not provide the first wife with these things, she is to be divorced, without cost to her. The Talmud interprets this as a requirement for a man to provide food and clothing to, and have sex with, each of his wives. However, "duty of marriage" is also interpreted as whatever one does as a married couple, which is more than just sexual activity. And the term diminish, which means to lessen, shows the man must treat her as if he was not married to another.

As a polygynous society, the Israelites did not have any laws that imposed marital fidelity on men. However, the prophet Malachi states that none should be faithless to the wife of his youth and that God hates divorce. Adulterous married women, adulterous betrothed women, and the men who slept with them however, were subject to the death penalty by the biblical laws against adultery According to the Priestly Code of the Book of Numbers, if a pregnant woman was suspected of adultery, she was to be subjected to the Ordeal of Bitter Water, a form of trial by ordeal, but one that took a miracle to convict. The literary prophets indicate that adultery was a frequent occurrence, despite their strong protests against it, and these legal strictnesses.

In ancient Greece, no specific civil ceremony was required for the creation of a heterosexual marriage – only mutual agreement and the fact that the couple must regard each other as husband and wife accordingly. Men usually married when they were in their 20s and women in their teens. It has been suggested that these ages made sense for the Greeks because men were generally done with military service or financially established by their late 20s, and marrying a teenage girl ensured ample time for her to bear children, as life expectancies were significantly lower. Married Greek women had few rights in ancient Greek society and were expected to take care of the house and children. Time was an important factor in Greek marriage. For example, there were superstitions that being married during a full moon was good luck and, according to Robert Flacelière, Greeks married in the winter. Inheritance was more important than feelings: a woman whose father dies without male heirs could be forced to marry her nearest male relative – even if she had to divorce her husband first.

There were several types of marriages in ancient Roman society. The traditional ("conventional") form called "conventio in manum" required a ceremony with witnesses and was also dissolved with a ceremony. In this type of marriage, a woman lost her family rights of inheritance of her old family and gained them with her new one. She now was subject to the authority of her husband. There was the free marriage known as "sine manu". In this arrangement, the wife remained a member of her original family; she stayed under the authority of her father, kept her family rights of inheritance with her old family and did not gain any with the new family. The minimum age of marriage for girls was 12.

Among ancient Germanic tribes, the bride and groom were roughly the same age and generally older than their Roman counterparts, at least according to Tacitus:
The youths partake late of the pleasures of love, and hence pass the age of puberty unexhausted: nor are the virgins hurried into marriage; the same maturity, the same full growth is required: the sexes unite equally matched and robust; and the children inherit the vigor of their parents.
Where Aristotle had set the prime of life at 37 years for men and 18 for women, the Visigothic Code of law in the 7th century placed the prime of life at 20 years for both men and women, after which both presumably married. Tacitus states that ancient Germanic brides were on average about 20 and were roughly the same age as their husbands. Tacitus, however, had never visited the German-speaking lands and most of his information on Germania comes from secondary sources. In addition, Anglo-Saxon women, like those of other Germanic tribes, are marked as women from the age of 12 and older, based on archaeological finds, implying that the age of marriage coincided with puberty.

From the early Christian era (30 to 325 CE), marriage was thought of as primarily a private matter, with no uniform religious or other ceremony being required. However, bishop Ignatius of Antioch writing around 110 to bishop Polycarp of Smyrna exhorts, "[I]t becomes both men and women who marry, to form their union with the approval of the bishop, that their marriage may be according to God, and not after their own lust."

In 12th-century Europe, women took the surname of their husbands and starting in the second half of the 16th century parental consent along with the church's consent was required for marriage.

With few local exceptions, until 1545, Christian marriages in Europe were by mutual consent, declaration of intention to marry and upon the subsequent physical union of the parties. The couple would promise verbally to each other that they would be married to each other; the presence of a priest or witnesses was not required. This promise was known as the "verbum." If freely given and made in the present tense (e.g., "I marry you"), it was unquestionably binding; if made in the future tense ("I will marry you"), it would constitute a betrothal.

In 1552 a wedding took place in Zufia, Navarre, between Diego de Zufia and Mari-Miguel following the custom as it was in the realm since the Middle Ages, but the man denounced the marriage on the grounds that its validity was conditioned to "riding" her (""si te cabalgo, lo cual dixo de bascuence (...) balvin yo baneça aren senar içateko""). The tribunal of the kingdom rejected the husband's claim, validating the wedding, but the husband appealed to the tribunal in Zaragoza, and this institution annulled the marriage. According to the Charter of Navarre, the basic union consisted of a civil marriage with no priest required and at least two witnesses, and the contract could be broken using the same formula. The Church in turn lashed out at those who got married twice or thrice in a row while their formers spouses were still alive. In 1563 the Council of Trent, twenty-fourth session, required that a valid marriage must be performed by a priest before two witnesses.

One of the functions of churches from the Middle Ages was to register marriages, which was not obligatory. There was no state involvement in marriage and personal status, with these issues being adjudicated in ecclesiastical courts. During the Middle Ages marriages were arranged, sometimes as early as birth, and these early pledges to marry were often used to ensure treaties between different royal families, nobles, and heirs of fiefdoms. The church resisted these imposed unions, and increased the number of causes for nullification of these arrangements. As Christianity spread during the Roman period and the Middle Ages, the idea of free choice in selecting marriage partners increased and spread with it.

In Medieval Western Europe, later marriage and higher rates of definitive celibacy (the so-called "European marriage pattern") helped to constrain patriarchy at its most extreme level. For example, Medieval England saw marriage age as variable depending on economic circumstances, with couples delaying marriage until the early twenties when times were bad and falling to the late teens after the Black Death, when there were labor shortages; by appearances, marriage of adolescents was not the norm in England. Where the strong influence of classical Celtic and Germanic cultures (which were not rigidly patriarchal) helped to offset the Judaeo-Roman patriarchal influence, in Eastern Europe the tradition of early and universal marriage (often in early adolescence) as well as traditional Slavic patrilocal custom led to a greatly inferior status of women at all levels of society.

The average age of marriage for most of Northwestern Europe from 1500 to 1800 was around 25 years of age; as the Church dictated that both parties had to be at least 21 years of age to marry without the consent of their parents, the bride and groom were roughly the same age, with most brides in their early twenties and most grooms two or three years older, and a substantial number of women married for the first time in their thirties and forties, particularly in urban areas, with the average age at first marriage rising and falling as circumstances dictated. In better times, more people could afford to marry earlier and thus fertility rose and conversely marriages were delayed or forgone when times were bad, thus restricting family size; after the Black Death, the greater availability of profitable jobs allowed more people to marry young and have more children, but the stabilization of the population in the 16th century meant fewer job opportunities and thus more people delaying marriages.

The age of marriage was not absolute, however, as child marriages occurred throughout the Middle Ages and later, with just some of them including:

As part of the Protestant Reformation, the role of recording marriages and setting the rules for marriage passed to the state, reflecting Martin Luther's view that marriage was a "worldly thing". By the 17th century, many of the Protestant European countries had a state involvement in marriage.

In England, under the Anglican Church, marriage by consent and cohabitation was valid until the passage of Lord Hardwicke's Act in 1753. This act instituted certain requirements for marriage, including the performance of a religious ceremony observed by witnesses.

As part of the Counter-Reformation, in 1563 the Council of Trent decreed that a Roman Catholic marriage would be recognized only if the marriage ceremony was officiated by a priest with two witnesses. The Council also authorized a Catechism, issued in 1566, which defined marriage as "The conjugal union of man and woman, contracted between two qualified persons, which obliges them to live together throughout life."

In the early modern period, John Calvin and his Protestant colleagues reformulated Christian marriage by enacting the Marriage Ordinance of Geneva, which imposed "The dual requirements of state registration and church consecration to constitute marriage" for recognition.

In England and Wales, Lord Hardwicke's Marriage Act 1753 required a formal ceremony of marriage, thereby curtailing the practice of Fleet Marriage, an irregular or a clandestine marriage. These were clandestine or irregular marriages performed at Fleet Prison, and at hundreds of other places. From the 1690s until the Marriage Act of 1753 as many as 300,000 clandestine marriages were performed at Fleet Prison alone. The Act required a marriage ceremony to be officiated by an Anglican priest in the Anglican Church with two witnesses and registration. The Act did not apply to Jewish marriages or those of Quakers, whose marriages continued to be governed by their own customs.

In England and Wales, since 1837, civil marriages have been recognized as a legal alternative to church marriages under the Marriage Act 1836. In Germany, civil marriages were recognized in 1875. This law permitted a declaration of the marriage before an official clerk of the civil administration, when both spouses affirm their will to marry, to constitute a legally recognized valid and effective marriage, and allowed an optional private clerical marriage ceremony.

In contemporary English common law, a marriage is a voluntary contract by a man and a woman, in which by agreement they choose to become husband and wife. Edvard Westermarck proposed that "the institution of marriage has probably developed out of a primeval habit".

As of 2000, the average marriage age range was 25–44 years for men and 22–39 years for women.

The mythological origin of Chinese marriage is a story about Nüwa and Fu Xi who invented proper marriage procedures after becoming married. In ancient Chinese society, people of the same surname are supposed to consult with their family trees prior to marriage to reduce the potential risk of unintentional incest. Marrying one's maternal relatives was generally not thought of as incest. Families sometimes intermarried from one generation to another. Over time, Chinese people became more geographically mobile. Individuals remained members of their biological families. When a couple died, the husband and the wife were buried separately in the respective clan's graveyard. In a maternal marriage a male would become a son-in-law who lived in the wife's home.

The New Marriage Law of 1950 radically changed Chinese marriage traditions, enforcing monogamy, equality of men and women, and choice in marriage; arranged marriages were the most common type of marriage in China until then. Starting October 2003, it became legal to marry or divorce without authorization from the couple's work units. Although people with infectious diseases such as AIDS may now marry, marriage is still illegal for the mentally ill.




</doc>
<doc id="19731" url="https://en.wikipedia.org/wiki?curid=19731" title="Midgard">
Midgard

In Germanic cosmology, Midgard (an anglicised form of Old Norse ; Old English , Old Saxon , Old High German , and Gothic "Midjun-gards"; "middle yard", "middle enclosure") is the name for Earth (equivalent in meaning to the Greek term , "inhabited") inhabited by and known to humans in early Germanic cosmology. The Old Norse form plays a notable role in Norse cosmology.

This name occurs in Old Norse literature as . In Old Saxon "Heliand" it appears as and in Old High German poem "Muspilli" it appears as . The Gothic form is attested in the Gospel of Luke as a translation of the Greek word . The word is present in Old English epic and poetry as ; later transformed to or ("Middle-earth") in Middle English literature.

All these forms are from a Common Germanic "*midja-gardaz" ("*meddila-", "*medjan-"), a compound of "*midja-" "middle" and "*gardaz" "yard, enclosure".
In early Germanic cosmology, the term stands alongside "world" (Old English "weorold", Old Saxon "werold", Old High German "weralt", Old Frisian "warld" and Old Norse "verǫld"), from a Common Germanic compound "*wira-alđiz", the "age of men".

Midgard is a realm in Norse mythology. It is one of the Nine Worlds and the only one that is completely visible to mankind (the others may intersect with this visible realm but are mostly invisible). Pictured as placed somewhere in the middle of Yggdrasil, Midgard is between the land of Niflheim—the land of ice—to the north and Muspelheim—the land of fire—to the south. Midgard is surrounded by a world of water, or ocean, that is impassable. The ocean is inhabited by the great sea serpent Jörmungandr (Miðgarðsormr), who is so huge that he encircles the world entirely, grasping his own tail. The concept is similar to that of the Ouroboros. Midgard was also connected to Asgard, the home of the gods, by the Bifröst, the rainbow bridge, guarded by Heimdallr.

In Norse mythology, "Miðgarðr" became applied to the wall around the world that the gods constructed from the eyebrows of the giant Ymir as a defense against the Jotuns who lived in Jotunheim, east of "Manheimr", the "home of men", a word used to refer to the entire world. The gods slew the giant Ymir, the first created being, and put his body into the central void of the universe, creating the world out of his body: his flesh constituting the land, his blood the oceans, his bones the mountains, his teeth the cliffs, his hairs the trees, and his brains the clouds. Ymir's skull was held by four dwarfs, Nordri, Sudri, Austri, and Vestri, who represent the four points on the compass and became the dome of heaven. The sun, moon, and stars were said to be scattered sparks in the skull.
According to the Eddas, Midgard will be destroyed at Ragnarök, the battle at the end of the world. Jörmungandr will arise from the ocean, poisoning the land and sea with his venom and causing the sea to rear up and lash against the land. The final battle will take place on the plane of Vígríðr, following which Midgard and almost all life on it will be destroyed, with the earth sinking into the sea only to rise again, fertile and green when the cycle repeats and the creation begins again.

Although most surviving instances of the word Midgard refer to spiritual matters, it was also used in more mundane situations, as in the Viking Age runestone poem from the inscription Sö 56 from Fyrby:

The Danish and Swedish form or , the Norwegian or , as well as the Icelandic and Faroese form , all derive from the Old Norse term.

The name "middangeard" occurs six times in the Old English epic poem "Beowulf", and is the same word as Midgard in Old Norse. The term is equivalent in meaning to the Greek term Oikoumene, as referring to the known and inhabited world.

The concept of Midgard occurs many times in Middle English. The association with "earth" (OE "eorðe") in Middle English "middellærd", "middelerde" is by popular etymology; the continuation of "geard" "enclosure" is "yard". An early example of this transformation is from the Ormulum:

The usage of "Middle-earth" as a name for a setting was popularized by Old English scholar J. R. R. Tolkien in his "The Lord of the Rings" and other fantasy works; he was originally inspired by the references to "middangeard" and "Éarendel" in the Old English poem "Crist".

"Mittilagart" is mentioned in the 9th-century Old High German "Muspilli" (v. 54) meaning "the world" as opposed to the sea and the heavens:


</doc>
<doc id="19732" url="https://en.wikipedia.org/wiki?curid=19732" title="Mage: The Ascension">
Mage: The Ascension

Mage: The Ascension is a role-playing game based in the World of Darkness, and was published by White Wolf Game Studio. The characters portrayed in the game are referred to as mages, and are capable of feats of magic. The idea of magic in "Mage" is broadly inclusive of diverse ideas about mystical practices as well as other belief systems, such as science and religion, so that most mages do not resemble typical fantasy wizards.

In 2005, White Wolf released a new version of the game, marketed as "", for the new World of Darkness series. The new game features some of the same game mechanics but uses a substantially different premise and setting.

Following the release of "", White Wolf put out a new roleplaying game every year, each set in "Vampire"'s World of Darkness and using its Storyteller rule system. The next four games were: "" (1992), "Mage: The Ascension" (1993), "" (1994) and "" (1995). "Mage" was the first World of Darkness game that Mark Rein•Hagen
was not explicitly involved with, although it featured the Order of Hermes from his "Ars Magica" as just a single tradition among many.

The basic premise of "Mage: The Ascension" is that everyone has the capacity, at some level, to shape reality. This capacity, personified as a mysterious alter ego called the Avatar, is dormant in most people, who are known as sleepers, whereas Magi (and/or their Avatars) are said to be Awakened. Because they're awakened, Magi can consciously effect changes to reality via willpower, beliefs, and specific magical techniques.

The beliefs and techniques of Magi vary enormously, and the ability to alter reality can only exist in the context of a coherent system of belief and technique, called a paradigm. A paradigm organizes a Mage's understanding of reality, how the universe works, and what things mean. It also provides the Mage with an understanding of how to change reality, through specific magical techniques. For example, an alchemical paradigm might describe the act of wood burning as the wood "releasing its essence of elemental Fire," while modern science would describe fire as "combustion resulting from a complex chemical reaction." Paradigms tend to be idiosyncratic to the individual Mage, but the vast majority belong to broad categories of paradigm, e.g., Shamanism, Medieval Sorcery, religious miracle working, and superscience.

In the Mage setting, everyday reality is governed by commonsense rules derived from the collective beliefs of sleepers. This is called the consensus. Most Magi's paradigms differ substantially from the consensus. When a mage performs an act of magic that does not seriously violate this commonsense version of reality, in game terms this is called coincidental magic. Magic that deviates wildly from consensus is called vulgar or dynamic magic. When it is performed ineptly, or is vulgar, and especially if it is vulgar and witnessed by sleepers, magic can cause Paradox, a phenomenon in which reality tries to resolve contradictions between the consensus and the Mage's efforts. Paradox is difficult to predict and almost always bad for the mage. The most common consequences of paradox include physical damage directly to the Mage's body, and paradox flaws, magical effects which can for example turn the mage's hair green, make him mute, make him incapable of leaving a certain location, and so on. In more extreme cases paradox can cause Quiet (madness that may leak into reality), Paradox Spirits (nebulous, often powerful beings which purposefully set about resolving the contradiction, usually by directly punishing the mage), or even the removal of the Mage to a paradox realm, a pocket dimension from which it may be difficult to escape.

In Mage, there is an underlying framework to reality called the Tapestry. The Tapestry is naturally divided into various sections, including the physical realm and various levels of the spirit world, or Umbra. At the most basic level, the Tapestry is composed of Quintessence, the essence of magic and what is real. Quintessence can have distinctive characteristics, called resonance, which are broken down into three categories: dynamic, static, and entropic.

In order to understand the metaphysics of the Mage setting, it is important to remember that many of the terms used to describe magic and Magi (e.g. Avatar, Quintessence, the Umbra, Paradox, Resonance, etc.) as well as the appearance, meaning, and understanding of a character's "Spheres," the areas of magic in which their character is proficient, vary depending on the Paradigm of the Mage in question, even though they are often, in the texts of the game, described from particular paradigmatic points-of-view. In-character, only a Mage's Paradigm can explain what each of these things are, what they mean, and why it's the way it is.

In the game, Mages have always existed, though there are legends of the Pure Ones who were shards of the original, divine One. Early mages cultivated their magical beliefs alone or in small groups, generally conforming to and influencing the belief systems of their societies. Obscure myths suggest that the precursors of the modern organizations of mages originally gathered in ancient Egypt. This period of historical uncertainty also saw the rise of the Nephandi in the Near East. This set the stage for what the game's history calls the Mythic Ages.

Until the late Middle Ages, mages' fortunes waxed and waned along with their native societies. Eventually, though, mages belonging to the Order of Hermes and the Messianic Voices attained great influence over European society. However, absorbed by their pursuit of occult power and esoteric knowledge, they often neglected and even abused humanity. Frequently, they were at odds with mainstream religions, envied by noble authorities and cursed by common folk.

Mages who believed in proto-scientific theories banded together under the banner of the , declaring their aim was to create a safe world with Man as its ruler. They won the support of Sleepers by developing the useful arts of manufacturing, economics, wayfaring, and medicine. They also championed many of the values that we now associate with the Renaissance. Masses of Sleepers embraced the gifts of early Technology and the Science that accompanied them. As the masses' beliefs shifted, the Consensus changed and wizards began to lose their position as their power and influence waned.

This was intentional. The Order of Reason perceived a safe world as one devoid of heretical beliefs, ungodly practices and supernatural creatures preying upon humanity. As the defenders of the common folk, they intended to replace the dominant magical groups with a society of philosopher-scientists as shepherds, protecting and guiding humanity. In response, non-scientific mages banded together to form the where mages of all the major magical paths gathered. They fought on battlefields and in universities trying to undermine as many discoveries as they could, but to no avail – technology made the march of Science unstoppable. The Traditions' power bases were crippled, their believers mainly converted, their beliefs ridiculed all around the world. Their final counteroffensives against the Order of Reason were foiled by internal dissent and treachery in their midst.

However, from the turn of the 17th century on, the goals of the Order of Reason began to change. As their scientific paradigm unfolded, they decided that the mystical beliefs of the common people were not only backward, but dangerous, and that they should be replaced by cold, measurable and predictable physical laws and respect for human genius. They replaced long-held theologies, pantheons, and mystical traditions with ideas like rational thought and the scientific method. As more and more sleepers began to use the Order's discoveries in their everyday lives, Reason and rationality came to govern their beliefs, and the old ways came to be regarded as misguided superstition. However, the Order of Reason became less and less focused on improving the daily lives of sleepers and more concerned with eliminating any resistance to their choke-hold on the minds of humanity. Ever since a reorganization performed under Queen Victoria in the late 1800s, they call themselves .

The Technocracy espouses an authoritarian rule over Sleepers' beliefs, while suppressing the Council of Nine's attempts to reintroduce magic. The Traditions replenished their numbers (which had been diminished by the withdrawal of two Traditions, the secretive Ahl-i-Batin, and the Solificati, alchemists plagued by scandal) with former Technocrats from the Sons of Ether and Virtual Adepts factions, vying for the beliefs of sleepers and with the Technocracy, and perpetually wary of the Nephandi (who consciously embrace evil and service to a demonic or alien master) and the Marauders (who resist Paradox with a magical form of madness). While the Technocracy's propaganda campaigns were effective in turning the Consensus against mystic and heterodox science, the Traditions maintained various resources, including magical nodes, hidden schools and fortresses called Chantries, and various realms outside of the Consensus in the Umbra.

Finally, from 1997–2000, a series of metaplot events destroyed the Council of Nine's Umbral steadings, killing many of their most powerful members. This also cut the Technocracy off from their leadership. Both sides called a truce in their struggle to assess their new situation, especially since these events implied that Armageddon was soon at hand. Chief among these signs was creation of a barrier between the physical world and spirit world. This barrier was called the Avatar Storm because it affected the Avatar of the Mage. This Avatar Storm was the result of a battle in India on the so-called "Week of Nightmares."

These changes were introduced in supplements for the second edition of the game and became core material in the third edition.

Aside from common changes introduced by the World of Darkness metaplot, mages dealt with renewed conflict when the hidden Rogue Council and the Technocracy's Panopticon encouraged the Traditions and Technocracy to struggle once again. The Rogue Council only made itself known through coded missives, while Panopticon was apparently created by the leaders of the Technocracy to counter it.

This struggle eventually led to the point on the timeline occupied by the book called "". While the entire metaplot has always been meant to be altered as each play group sees fit, "Ascension" provided multiple possible endings, with none of them being definitive (though one was meant to resolve the metaplot). Thus, there is no definitive canonical ending. Since the game is meant to be adapted to a group's tastes, the importance of this and the preceding storyline is largely a matter of personal preference.

The metaplot of the game involves a four-way struggle between the technological and authoritarian Technocracy, the insane Marauders, the cosmically evil Nephandi and the nine mystical Traditions (that tread the middle path), to which the player characters are assumed to belong. (This struggle has in every edition of the game been characterized both as primarily a covert, violent war directly between factions, and primarily as an effort to sway the imaginations and beliefs of sleepers.)

The Traditions (formally called the Nine Mystic Traditions) are a fictional alliance of secret societies in the "Mage: the Ascension" role-playing game. The Traditions exist to unify users of magic under a common banner to protect reality (particularly those parts of reality that are magical) against the growing disbelief of the modern world, the spreading dominance of the , and the predations of unstable mages such as Marauders and Nephandi. Each of the Traditions are largely independent organizations unified by a broadly accepted paradigm for practicing magic. The Traditions themselves vary substantially from one another. Some have almost no structure or rules, while others have rigid rules of protocol, etiquette, and rank. Though unified in their desire to keep magic alive, the magic practiced by different Traditions are often wildly different and entirely incompatible with one another. Understanding Traditions as a whole requires understanding each Tradition separately, and then assembling them into a somewhat cohesive whole.

The nine traditions are: the Akashic Brotherhood, Celestial Chorus, Cult of Ecstasy, Dreamspeakers, Euthanatos, Order of Hermes, Sons of Ether, Verbena and Virtual Adepts. 

The Technocracy is likewise divided into groups; unlike the Traditions, however, they share a single paradigm, and instead divide themselves based upon methodologies and areas of expertise.


The Marauders are a group of mages that embody Dynamism. Marauders are chaos mages. They are completely insane. To other mages, they appear immune to paradox effects, often using vulgar magic to accomplish their insane tasks. Marauders represent the other narrative extreme, the repellent and frightening corruption of unrestrained power, of dynamism unchecked. Marauders are insane mages whose Avatars have been warped by their mental instability, and who exist in a state of permanent Quiet. While the nature of a Marauder's power may make them seem invincible, they are still severely hampered by their madness. They cannot become Archmages, as they lack sufficient insight and are incapable of appreciating truths which do not suit their madness. In the second edition of "Mage: The Ascension", Marauders were much more cogent and likely to operate in groups, with the Umbral Underground using the Umbra to infiltrate any location and wreak havoc with the aid of bygones. They were also associated heavily with other perceived agents of Dynamism, particularly the s (who equate Dynamism with the Wyld) and sometimes Changelings. For example, the Marauders chapter in "The Book of Madness" is narrated by a Corax (were-raven) named Johnny Gore, who relates his experiences running with the Butcher Street Regulars. In the revised edition, Marauders were made darker and less coherent, in keeping with the more serious treatment of madness used for Malkavians in "Vampire: The Masquerade Revised Edition". The Avatar Storm was a very convenient explanation for the Underground's loss of power and influence, though they also became more vulnerable to Paradox. In this edition, the Regulars are a cell of the Underground, and like the other cells have highly compatible Quiets.

With the Technocracy representing Stasis and the Marauders acting on behalf of Dynamism, the third part of this trifecta is Entropy, as borne by the Nephandi. While other mages may be callous or cruel, the Nephandi are morally inverted and spiritually mutilated. While a Traditionalist or Technocrat may simply fall prey to human failings or excessive zeal in their ethos, while a Marauder may well commit some true atrocities in the depth of her incurable madness; a Nephandus retains a clear moral compass, and deliberately pursues actions to worsen the world and bring about its final end. To this end, the Technocracy and Traditions have been known to set aside the ongoing war for reality to temporarily join forces to oppose the Nephandi, and even the Marauders are known to attack the Nephandi on sight. Some of their members, called "barabbi", hail from the Technocracy and Traditions, but all Nephandi have experienced the Rebirth, wherein they embrace the antithesis of everything they know to be right, and are physically and spiritually torn apart and reassembled. This metamorphosis has a sort of terrible permanence to it: while each Mage's avatar will be reborn again and again, theirs is permanently twisted as a result of their rebirth: known as Widderslainte, these mages awaken as Nephandi. While some of the background stories detail a particular mage and her teacher trying—and succeeding—at keeping her from falling again, this is very rare.

Other mystical traditions that are not part of the nine exist, and are known as Crafts. Some examples of these are the mages of Ahl-i-Batin (also known as "The Subtle Ones") who are masters of the Correspondence Sphere and former holders of the seat now held by the Virtual Adepts, as well as the djinn binding magicians known as The Taftani and the eclectic nonconformist group of willworkers known as Hollow Ones, however they are far from the only ones.

The core rules of the game are similar to those in other World of Darkness games; see Storyteller System for an explanation.

Like other storytelling games Mage emphasizes personal creativity and that ultimately the game's powers and traits should be used to tell a satisfying story. One of Mage's highlights is its system for describing magic, based on spheres, a relatively open-ended 'toolkit' approach to using game mechanics to define the bounds of a given character's magical ability. Different Mages will have differing aptitudes for spheres, and player characters' magical expertise is described by allocation of points in the spheres.

There are nine known spheres:

Deals with spatial relations, giving the Mage power over space and distances. Correspondence magic allows powers such as teleportation, seeing into distant areas, and at higher levels the Mage may also co-locate herself or even stack different spaces within each other. Correspondence can be combined with almost any other sphere to create effects that span distances.

This sphere gives the Mage power over order, chaos, fate and fortune. A mage can sense where elements of chance influence the world and manipulate them to some degree. At simple levels machines can be made to fail, plans to go off without a hitch, and games of chance heavily influenced. Advanced mages can craft self-propagating memes or curse entire family lines with blights. The only requirement of the Entropy sphere is that all interventions work within the general flow of natural entropy.

Forces concerns energies and natural forces and their negative opposites (i.e. light and shadow can both be manipulated independently with this Sphere). Essentially, anything in the material world that can be seen or felt but is not material can be controlled: electricity, gravity, magnetism, friction, heat, motion, fire, etc. At low levels the mage can control forces on a small scale, changing their direction, converting one energy into another. At high levels, storms and explosions can be conjured. Obviously, this Sphere tends to do the most damage and is the most flashy and vulgar. Along with Life and Matter, Forces is one of the three 'Pattern Spheres' which together are able to mold all aspects of the physical world.

Life deals with understanding and influencing biological systems. Generally speaking, any material object with mostly living cells falls under the influence of this sphere. Simply, this allows the mage to heal herself or metamorphose simple life-forms at lower levels, working up to healing others and controlling more complex life at higher levels. Usually, seeking to improve a complex life-form beyond natural limits causes the condition of pattern bleeding: the affected life form begins to wither and die over time. Along with Matter and Forces, Life is one of the three Pattern Spheres.

Dealing with control over one's own mind, the reading and influencing of other minds, and a variety of subtler applications such as Astral Projection and psychometry. At high levels, Mages can create new complete minds or completely rework existing ones.

Matter deals with all inanimate material. Thus, being alive protects a thing from direct manipulation by the Matter sphere. Stone, dead wood, water, gold, and the corpses of once living things are only the beginning. With this Sphere, matter can be reshaped mentally, transmuted into another substance, or given altered properties. Along with Life and Forces, Matter is one of the three Pattern Spheres.

This sphere deals directly with Quintessence, the raw material of the tapestry, which is the metaphysical structure of reality. This sphere allows Quintessence to be channeled and/or funneled in any way at higher levels, and it is necessary if the mage ever wants to conjure something out of nothing (as opposed to transforming one pattern into another). Uses of Prime include general magic senses, counter-magic, and making magical effects permanent.

This sphere is an eclectic mixture of abilities relating to dealings with the spirit world or Umbra. It includes stepping into the Near Umbra right up to traveling through outer space, contacting and controlling spirits, communing with your own or others' avatars, returning a Mage into a sleeper, returning ghosts to life, creating magical fetish items, and so forth. Unlike other Spheres, the difficulty of Spirit magic is often a factor of the Gauntlet, making these spells more difficult for the most part. The Sphere is referred to as Dimensional Science by the Technocratic Union.

This sphere deals with dilating, slowing, stopping or traveling through time. Due to game mechanics, it is simpler to travel forward in time than backwards. Time can be used to install delays into spells, view the past or future, and even pull people and objects out of linear progression. Time magic offers one means to speed up a character to get multiple actions in a combat round, a much coveted power in turn-based combat.

One of the plot hooks that the second edition books put forth were persistent rumors of a "tenth sphere". Though there were hints, it was deliberately left vague. The final book in the line, "Ascension" implies that the tenth sphere is the sphere of Ascension (in as much as spheres are practically relevant at that point in the story). As the book presents alternative resolutions for the Mage line, Chapter Two also presents an alternative interpretation that the tenth sphere is "Judgement" or "Telos" and that Anthelios (the red star in the World of Darkness metaplot) is its planet (each sphere has an associated planet and Umbral realm).

The various sphere sigils are, in whole or in part, symbols taken from alchemical texts.


The third revision of the rules, "Mage: The Ascension Revised", made significant changes to the rules and setting, mainly to update Mage with respect to its own ongoing storyline, particularly in regards to events that occurred during the run of the game's second edition. (Like other World of Darkness games, Mage uses a continuing storyline across all of its books).

Adam Tinworth of "Arcane" gave "Mage: The Ascension" second edition a score of 8/10, calling it good for those who like involving and challenging games; he noted that it could be difficult for new players to grasp the entire background and how magic works, and to develop their own style of magic, but found the gameplay system itself to be easy to understand for newcomers.

"Mage: The Ascension" was ranked 16th in the 1996 reader poll of "Arcane" magazine to determine the 50 most popular role-playing games of all time. The magazine's editor Paul Pettengale commented: "Mage is perfect for those of a philosophical bent. It's a hard game to get right, requiring a great deal of thought from players and referees alike, but its underlying theme – the nature of reality – makes it one of the most interesting and mature roleplaying games available."





</doc>
<doc id="19734" url="https://en.wikipedia.org/wiki?curid=19734" title="Malcolm Fraser">
Malcolm Fraser

John Malcolm Fraser (; 21 May 1930 – 20 March 2015) was an Australian politician who served as the 22nd Prime Minister of Australia, in office from 1975 to 1983 as leader of the Liberal Party.

Fraser was raised on his father's sheep stations, and after studying at Magdalen College, Oxford, returned to Australia to take over the family property in the Western District of Victoria. After an initial defeat in 1954, he was elected to the House of Representatives at the 1955 federal election, standing in the Division of Wannon. He was 25 at the time, making him one of the youngest people ever elected to parliament. When Harold Holt became prime minister in 1966, Fraser was appointed Minister for the Army. After Holt's disappearance and replacement by John Gorton, Fraser became Minister for Education and Science (1968–1969) and then Minister for Defence (1969–1971). In 1971, Fraser resigned from cabinet and denounced Gorton as "unfit to hold the great office of prime minister"; this precipitated the replacement of Gorton with William McMahon. He subsequently returned to his old education and science portfolio.

After the Coalition was defeated at the 1972 election, Fraser unsuccessfully stood for the Liberal leadership, losing to Billy Snedden. When the party lost the 1974 election, he began to move against Snedden, eventually mounting a successful challenge in March 1975. As Leader of the Opposition, Fraser used the Coalition's control of the Senate to block supply to the Whitlam Government, precipitating a constitutional crisis. This culminated with Gough Whitlam being dismissed as prime minister by Governor-General John Kerr, a unique occurrence in Australian history. The correctness of Fraser's actions in the crisis and the exact nature of his involvement in Kerr's decision have since been a topic of debate. Fraser remains the only Australian prime minister to ascend to the position upon the dismissal of his predecessor.

After Whitlam's dismissal, Fraser was sworn in as prime minister on an initial caretaker basis. The Coalition won a landslide victory at the 1975 election, and was re-elected in 1977 and 1980. Fraser took a keen interest in foreign affairs as prime minister, and was more active in the international sphere than many of his predecessors. He was a strong supporter of multiculturalism, and during his term in office Australia admitted significant numbers of non-white immigrants (including Vietnamese boat people) for the first time. His government also established the Special Broadcasting Service (SBS). Particularly in his final years in office, Fraser came into conflict with the economic rationalist faction of his party. His government made few major changes to economic policy.

Fraser and the Coalition lost power at the 1983 election, and he left politics a short time later. To date, he is the last Prime Minister from a country seat. In retirement, he held advisory positions with the UN and the Commonwealth of Nations, and was president of the aid agency CARE from 1990 to 1995. He resigned his membership of the Liberal Party in 2009, having been a critic of its policy direction for a number of years. Evaluations of Fraser's prime ministership have been mixed. He is generally credited with restoring stability to the country after a series of short-term leaders, but some have seen his government as a lost opportunity for economic reform. Only three Australian prime ministers have served longer terms in office – Robert Menzies, John Howard and Bob Hawke.

John Malcolm Fraser was born in Toorak, Melbourne, Victoria, on 21 May 1930. He was the second of two children born to Una Arnold (née Woolf) and John Neville Fraser; his older sister Lorraine had been born in 1928. Both he and his father were known exclusively by their middle names. His paternal grandfather, Sir Simon Fraser, was born in Nova Scotia, Canada, and arrived in Australia in 1853. He made his fortune as a railway contractor, and later acquired significant pastoral holdings, becoming a member of the "squattocracy". Fraser's maternal grandfather, Louis Woolf, was born in Dunedin, New Zealand, and arrived in Australia as a child. He was of Jewish origin, a fact which his grandson did not learn until he was an adult. A chartered accountant by trade, he married Amy Booth, who was related to the wealthy Hordern family of Sydney and was a first cousin of Sir Samuel Hordern.

Fraser had a political background on both sides of his family. His father served on the Wakool Shire Council, including as president for two years, and was an admirer of Billy Hughes and a friend of Richard Casey. Simon Fraser served in both houses of the colonial Parliament of Victoria, and represented Victoria at several of the constitutional conventions of the 1890s. He eventually become one of the inaugural members of the new federal Senate, serving from 1901 to 1913 as a member of the early conservative parties. Louis Woolf also ran for the Senate in 1901, standing as a Free Trader in Western Australia. He polled only 400 votes across the whole state, and was never again a candidate for public office.

Fraser spent most of his early life at "Balpool-Nyang", a sheep station of on the Edward River near Moulamein, New South Wales. His father had a law degree from Magdalen College, Oxford, but never practised law and preferred the life of a grazier. Fraser contracted a severe case of pneumonia when he was eight years old, which nearly proved fatal. He was home-schooled until the age of ten, when he was sent to board at Tudor House School in the Southern Highlands. He attended Tudor House from 1940 to 1943, and then completed his secondary education at Melbourne Grammar School from 1944 to 1948 where he was a member of Rusden House. While at Melbourne Grammar, he lived in a flat that his parents owned on Collins Street. In 1943, Fraser's father sold "Balpool-Nyang" – which had been prone to drought – and bought "Nareen", in the Western District of Victoria. He was devastated by the sale of his childhood home, and regarded the day he found out about it as the worst of his life.

In 1949, Fraser moved to England to study at Magdalen College, Oxford, which his father had also attended. He read Philosophy, Politics and Economics (PPE), graduating in 1952 with third-class honours. Although Fraser did not excel academically, he regarded his time at Oxford as his intellectual awakening, where he learned "how to think". His college tutor was Harry Weldon, who was a strong influence. His circle of friends at Oxford included Raymond Bonham Carter, Nicolas Browne-Wilkinson, and John Turner. In his second year, he had a relationship with Anne Reid, who as Anne Fairbairn later became a prominent poet. After graduating, Fraser considered taking a law degree or joining the British Army, but eventually decided to return to Australia and take over the running of the family property.

Fraser returned to Australia in mid-1952. He began attending meetings of the Young Liberals in Hamilton, and became acquainted with many of the local party officials. In November 1953, aged 23, Fraser unexpectedly won Liberal preselection for the Division of Wannon, which covered most of Victoria's Western District. The previous Liberal member, Dan Mackinnon, had been defeated in 1951 and moved to a different electorate. He was expected to be succeeded by Magnus Cormack, who had recently lost his place in the Senate. Fraser had put his name forward as a way of building a profile for future candidacies, but mounted a strong campaign and in the end won a narrow victory. In January 1954, he made the first of a series of weekly radio broadcasts on 3HA Hamilton and 3YB Warrnambool, titled "One Australia". His program – consisting of a pre-recorded 15-minute monologue – covered a wide range of topics, and was often reprinted in newspapers. It continued more or less uninterrupted until his retirement from politics in 1983, and helped him build a substantial personal following in his electorate.

At the 1954 election, Fraser lost to the sitting Labor member Don McLeod by just 17 votes (out of over 37,000 cast). However, he reprised his candidacy at the early 1955 election after a redistribution made Wannon notionally Liberal. McLeod concluded the reconfigured Wannon was unwinnable and retired. These factors, combined with the 1955 Labor Party split, allowed Fraser to win a landslide victory.

Fraser took his seat in parliament at the age of 25 – the youngest sitting MP by four years, and the first who had been too young to serve in World War II. He was re-elected at the 1958 election despite being restricted in his campaigning by a bout of hepatitis. Fraser was soon being touted as a future member of cabinet, but despite good relations with Robert Menzies never served in any of his ministries. This was probably due to a combination of his youth and the fact that the ministry already contained a disproportionately high number of Victorians.

Fraser spoke on a wide range of topics during his early years in parliament, but took a particular interest in foreign affairs. In 1964, he and Gough Whitlam were both awarded Leader Grants by the United States Department of State, allowing them to spend two months in Washington, D.C., getting to know American political and military leaders. The Vietnam War was the main topic of conversation, and on his return trip to Australia he spent two days in Saigon. Early in 1965, he also made a private seven-day visit to Jakarta, and with assistance from Ambassador Mick Shann secured meetings with various high-ranking officials.

After more than a decade on the backbench, Fraser was appointed to the Cabinet by the prime minister, Harold Holt, in 1966. As Minister for the Army he presided over the controversial Vietnam War conscription program.

Under the new prime minister, John Gorton, he became Minister for Education and Science and in 1969 was promoted to Minister for Defence, a particularly challenging post at the time, given the height of Australia's involvement in the Vietnam War and the protests against it.

In March 1971 Fraser abruptly resigned from the Cabinet in protest at what he called Gorton's "interference in (his) ministerial responsibilities".

This precipitated a series of events which eventually led to the downfall of Gorton and his replacement as prime minister by William McMahon. Gorton never forgave Fraser for the role he played in his downfall; to the day Gorton died in 2002, he could not bear to be in the same room with Fraser.

McMahon immediately reappointed Fraser to the Cabinet, returning him to his old position of Minister for Education and Science. When the Liberals were defeated at the 1972 election by the Labor Party under Gough Whitlam, McMahon resigned and Fraser became Shadow Minister for Labour under Billy Snedden.

After the Coalition lost the 1972 election, Fraser was one of five candidates for the Liberal leadership that had been vacated by McMahon. He outpolled John Gorton and James Killen, but was eliminated on the third ballot. Billy Snedden eventually defeated Nigel Bowen by a single vote on the fifth ballot. In the new shadow cabinet – which featured only Liberals – Fraser was given responsibility for primary industry. This was widely seen as a snub, as the new portfolio kept him mostly out of the public eye and was likely to be given to a member of the Country Party when the Coalition returned to government. In an August 1973 reshuffle, Snedden instead made him the Liberals' spokesman for industrial relations. He had hoped to be given responsibility for foreign affairs (in place of the retiring Nigel Bowen), but that role was given to Andrew Peacock. Fraser oversaw the development of the party's new industrial relations policy, which was released in April 1974. It was seen as more flexible and even-handed than the policy that the Coalition had pursued in government, and was received well by the media. According to Fraser's biographer Philip Ayres, by "putting a new policy in place, he managed to modify his public image and emerge as an excellent communicator across a traditionally hostile divide".

After the Liberals lost the 1974 election, Fraser unsuccessfully challenged Snedden for the leadership in November. Despite surviving the challenge, Snedden's position in opinion polls continued to decline and he was unable to get the better of Whitlam in the Parliament. Fraser again challenged Snedden on 21 March 1975, this time succeeding and becoming Leader of the Liberal Party and Leader of the Opposition.

Following a series of ministerial scandals engulfing the Whitlam Government later that year, Fraser began to instruct Coalition senators to delay the government's budget bills, with the objective of forcing an early election that he believed he would win. After several months of political deadlock, during which time the government secretly explored methods of obtaining supply funding outside the Parliament, the Governor-General, Sir John Kerr, controversially dismissed Whitlam as prime minister on 11 November 1975.

Fraser was immediately sworn in as caretaker prime minister on the condition that he end the political deadlock and call an immediate double dissolution election.

On 19 November 1975, shortly after the election had been called, a letter bomb was sent to Fraser, but it was intercepted and defused before it reached him. Similar devices were sent to the governor-general and the Premier of Queensland, Joh Bjelke-Petersen.

At the 1975 election, Fraser led the Liberal-Country Party Coalition to a landslide victory. The Coalition won 91 seats of a possible 127 in the election to gain a 55-seat majority, which remains to date the largest in Australian history. Fraser subsequently led the Coalition to a second victory in 1977, with only a very small decrease in their vote. The Liberals actually won a majority in their own right in both of these elections, something that Menzies and Holt had never achieved. Although Fraser thus had no need for the support of the (National) Country Party to govern, he retained the formal Coalition between the two parties.

Fraser quickly dismantled some of the programs of the Whitlam Government, such as the Ministry of the Media, and made major changes to the universal health insurance system Medibank. He initially maintained Whitlam's levels of tax and spending, but real per-person tax and spending soon began to increase. He did manage to rein in inflation, which had soared under Whitlam. His so-called "Razor Gang" implemented stringent budget cuts across many areas of the Commonwealth Public Sector, including the Australian Broadcasting Corporation (ABC).

Fraser practised Keynesian economics during his time as Prime Minister, in part demonstrated by running budget deficits throughout his term as Prime Minister. He was the Liberal Party's last Keynesian Prime Minister. Though he had long been identified with the Liberal Party's right wing, he did not carry out the radically conservative program that his political enemies had predicted, and that some of his followers wanted. Fraser's relatively moderate policies particularly disappointed the Treasurer, John Howard, as well as other ministers who were strong adherents of economic liberalism, and therefore detractors of Keynesian economics. The government's economic record was marred by rising double-digit unemployment and double-digit inflation, creating "stagflation", caused in part by the ongoing effects of the 1973 oil crisis.

Fraser was particularly active in foreign policy as prime minister. He supported the Commonwealth in campaigning to abolish apartheid in South Africa and refused permission for the aircraft carrying the Springbok rugby team to refuel on Australian territory en route to their controversial 1981 tour of New Zealand. However, an earlier tour by the South African ski boat angling team was allowed to pass through Australia on the way to New Zealand in 1977 and the transit records were suppressed by Cabinet order.

Fraser also strongly opposed white minority rule in Rhodesia. During the 1979 Commonwealth Conference, Fraser, together with his Nigerian counterpart, convinced the newly elected British prime minister, Margaret Thatcher, to withhold recognition of the internal settlement Zimbabwe Rhodesia government; Thatcher had earlier promised to recognise it. Subsequently, the Lancaster House Agreement was signed and Robert Mugabe was elected leader of an independent Zimbabwe at the inaugural 1980 election. Duncan Campbell, a former deputy secretary of the Department of Foreign Affairs and Trade has stated that Fraser was "the principal architect" in the ending of white minority rule. The President of Tanzania, Julius Nyerere, said that he considered Fraser's role "crucial in many parts" and the President of Zambia, Kenneth Kaunda, called his contribution "vital".

Under Fraser, Australia recognised Indonesia's annexation of East Timor, although many East Timorese refugees were granted asylum in Australia. Fraser was also a strong supporter of the United States and supported the boycott of the 1980 Summer Olympics in Moscow. However, although he persuaded some sporting bodies not to compete, Fraser did not try to prevent the Australian Olympic Committee sending a team to the Moscow Games.

Fraser also surprised his critics over immigration policy; according to 1977 Cabinet documents, the Fraser Government adopted a formal policy for "a humanitarian commitment to admit refugees for resettlement". Fraser's aim was to expand immigration from Asian countries and allow more refugees to enter Australia. He was a firm supporter of multiculturalism and established a government-funded multilingual radio and television network, the Special Broadcasting Service (SBS), building on their first radio stations which had been established under the Whitlam Government.

Despite Fraser's support for SBS, his government imposed stringent budget cuts on the national broadcaster, the ABC, which came under repeated attack from the Coalition for alleged "left-wing bias" and "unfair" coverage on their TV programs, including "This Day Tonight" and "Four Corners", and on the ABC's new youth-oriented radio station Double Jay. One result of the cuts was a plan to establish a national youth radio network, of which Double Jay was the first station. The network was delayed for many years and did not come to fruition until the 1990s. Fraser also legislated to give Indigenous Australians control of their traditional lands in the Northern Territory, but resisted imposing land rights laws on conservative state governments.
At the 1980 election, Fraser saw his majority more than halved, from 48 seats to 21. The Coalition also lost control of the Senate. Despite this, Fraser remained ahead of Labor leader Bill Hayden in opinion polls. However, the economy was hit by the early 1980s recession, and a protracted scandal over tax-avoidance schemes run by some high-profile Liberals also began to hurt the Government.

In April 1981, the Minister for Industrial Relations, Andrew Peacock, resigned from the Cabinet, accusing Fraser of "constant interference in his portfolio". Fraser, however, had accused former prime minister John Gorton of the same thing a decade earlier. Peacock subsequently challenged Fraser for the leadership; although Fraser defeated Peacock, these events left him politically weakened.

By early 1982, the popular former ACTU President, Bob Hawke, who had entered Parliament in 1980, was polling well ahead of both Fraser and the Labor Leader, Bill Hayden, on the question of who voters would rather see as prime minister. Fraser was well aware of the infighting this caused between Hayden and Hawke and had planned to call a snap election in autumn 1982, preventing the Labor Party changing leaders. These plans were derailed when Fraser suffered a severe back injury. Shortly after recovering from his injury, the Liberal Party narrowly won a by-election in the marginal seat of Flinders in December 1982. The failure of the Labor Party to win the seat convinced Fraser that he would be able to win an election against Hayden.

As leadership tensions began to grow in the Labor Party throughout January, Fraser subsequently resolved to call a double dissolution election at the earliest opportunity, hoping to capitalise on Labor's disunity. He knew that if the writs were issued soon enough, Labor would essentially be frozen into going into the subsequent election with Hayden as leader.

On 3 February 1983, Fraser arranged to visit the Governor-General of Australia, Ninian Stephen, intending to ask for a surprise election. However, Fraser made his run too late. Without any knowledge of Fraser's plans, Hayden resigned as Labor leader just two hours before Fraser travelled to Government House. This meant that the considerably more popular Hawke was able to replace him at almost exactly the same time that the writs were issued for the election. Although Fraser reacted to the move by saying he looked forward to "knock[ing] two Labor Leaders off in one go" at the forthcoming election, Labor immediately surged in the opinion polls.

At the election on 5 March the Coalition was heavily defeated, suffering a 24-seat swing, the worst defeat of a non-Labor government since Federation. Fraser immediately announced his resignation as Liberal leader and formally resigned as prime minister on 11 March 1983; he retired from Parliament two months later. To date, he is the last non-interim prime minister from a rural seat.

In retirement Fraser served as Chairman of the UN Panel of Eminent Persons on the Role of Transnational Corporations in South Africa 1985, as Co-Chairman of the Commonwealth Group of Eminent Persons on South Africa in 1985–86 (appointed by Prime Minister Hawke), and as Chairman of the UN Secretary-General's Expert Group on African Commodity Issues in 1989–90. He was a distinguished international fellow at the American Enterprise Institute from 1984 to 1986. Fraser helped to establish the foreign aid group CARE organisation in Australia and became the agency's international president in 1991, and worked with a number of other charitable organisations. In 2006, he was appointed Professorial Fellow at the Asia Pacific Centre for Military Law, and in October 2007 he presented his inaugural professorial lecture, "Finding Security in Terrorism's Shadow: The importance of the rule of law".

On 14 October 1986, Fraser, then the Chairman of the Commonwealth Eminent Persons Group, was found in the foyer of the Admiral Benbow Inn, a seedy Memphis hotel, wearing only a pair of underpants and confused as to where his trousers were. The hotel was an establishment popular with prostitutes and drug dealers. Though it was rumoured at the time that the former Prime Minister had been with a prostitute, his wife stated that Fraser had no recollection of the events and that she believes it more likely that he was the victim of a practical joke by his fellow delegates.

In 1993, Fraser made a bid for the Liberal Party presidency but withdrew at the last minute following opposition to his bid, which was raised due to him having been critical of then Liberal leader John Hewson for losing the election earlier that year.

After 1996, Fraser was critical of the Howard Coalition government over foreign policy issues, particularly John Howard's alignment with the foreign policy of the Bush administration, which Fraser saw as damaging Australian relationships in Asia. He opposed Howard's policy on asylum-seekers, campaigned in support of an Australian Republic and attacked what he perceived as a lack of integrity in Australian politics, together with former Labor prime minister Gough Whitlam, finding much common ground with his predecessor and his successor Bob Hawke, another republican.

The 2001 election continued his estrangement from the Liberal Party. Many Liberals criticised the Fraser years as "a decade of lost opportunity" on deregulation of the Australian economy and other issues. In early 2004, a Young Liberal convention in Hobart called for Fraser's life membership of the Liberal Party to be ended.

In 2006, Fraser criticised Howard Liberal government policies on areas such as refugees, terrorism and civil liberties, and that "if Australia continues to follow United States policies, it runs the risk of being embroiled in the conflict in Iraq for decades, and a fear of Islam in the Australian community will take years to eradicate". Fraser claimed that the way the Howard government handled the David Hicks, Cornelia Rau and Vivian Solon cases was questionable.

On 20 July 2007, Fraser sent an open letter to members of the large activist group GetUp!, encouraging members to support GetUp's campaign for a change in policy on Iraq including a clearly defined exit strategy. Fraser stated: "One of the things we should say to the Americans, quite simply, is that if the United States is not prepared to involve itself in high-level diplomacy concerning Iraq and other Middle East questions, our forces will be withdrawn before Christmas."

After the defeat of the Howard government at the 2007 federal election, Fraser claimed Howard approached him in a corridor, following a cabinet meeting in May 1977 regarding Vietnamese refugees, and said: "We don't want too many of these people. We're doing this just for show, aren't we?" The claims were made by Fraser in an interview to mark the release of the 1977 cabinet papers. Howard, through a spokesman, denied having made the comment.

In October 2007 Fraser gave a speech to Melbourne Law School on terrorism and "the importance of the rule of law," which Liberal MP Sophie Mirabella
condemned in January 2008, claiming errors and "either intellectual sloppiness or deliberate dishonesty", and claimed that he tacitly supported Islamic fundamentalism, that he should have no influence on foreign policy, and claimed his stance on the war on terror had left him open to caricature as a "frothing-at-the-mouth leftie".

Shortly after Tony Abbott won the 2009 Liberal Party leadership spill, Fraser ended his Liberal Party membership, stating the party was "no longer a liberal party but a conservative party".

In December 2011, Fraser was highly critical of the Australian government's decision (also supported by the Liberal Party Opposition) to permit the export of uranium to India, relaxing the Fraser government's policy of banning sales of uranium to countries that are not signatories of the Nuclear Non-Proliferation Treaty.

In 2012, Fraser criticised the basing of US military forces in Australia.

In late 2012, Fraser wrote a foreword for the journal "Jurisprudence" where he openly criticised the current state of human rights in Australia and the Western World. "It is a sobering thought that in recent times, freedoms hard won through centuries of struggle, in the United Kingdom and elsewhere have been whittled away. In Australia alone we have laws that allow the secret detention of the innocent. We have had a vast expansion of the power of intelligence agencies. In many cases the onus of proof has been reversed and the justice that once prevailed has been gravely diminished."

In July 2013, Fraser endorsed Australian Greens Senator Sarah Hanson-Young for re-election in a television advertisement, stating she had been a "reasonable and fair-minded voice".

Fraser's books include "Malcolm Fraser: The Political Memoirs" (with Margaret Simons – The Miegunyah Press, 2010) and "Dangerous Allies" (Melbourne University Press, 2014), which warns of "strategic dependence" on the United States. In the book and in talks promoting it, he criticised the concept of American exceptionalism and US foreign policy.

Fraser died on the early morning of 20 March 2015 shortly before his 85th birthday after a brief illness. An obituary noted that there had been "greater appreciation of the constructive and positive nature of his post-prime ministerial contribution" as his retirement years progressed.

Fraser was given a state funeral at Scots' Church in Melbourne on 27 March 2015. His ashes are interred within the 'Prime Ministers Garden' of Melbourne General Cemetery.

On 9 December 1956, Fraser married Tamara "Tamie" Beggs, who was almost six years his junior. They had met at a New Year's Eve party, and bonded over similar personal backgrounds and political views. The couple had four children together: Mark (b. 1958), Angela (b. 1959), Hugh (b. 1963), and Phoebe (b. 1966). Tamie frequently assisted her husband in campaigning, and her gregariousness was seen as complementing his more shy and reserved nature. She advised him on most of the important decisions in his career, and in retirement he observed that "if she had been prime minister in 1983, we would have won".

Fraser attended Anglican schools, although his parents were Presbyterian. In university he was inclined towards atheism, once writing that "the idea that God exists is a nonsense". However, his beliefs became less definite over time and tended towards agnosticism. During his political career, he occasionally self-described as Christian, such as in a 1975 interview with "The Catholic Weekly". Margaret Simons, the co-author of Fraser's memoirs, thought that he was "not religious, and yet thinks religion is a necessary thing". In a 2010 interview with her, he said: "I would probably like to be less logical and, you know, really able to believe there is a god, whether it is Allah, or the Christian god, or some other – but I think I studied too much philosophy ... you can never know".

In 2004, Fraser designated the University of Melbourne the official custodian of his personal papers and library to create the Malcolm Fraser Collection at the university.

Upon his death, Fraser's 1983 nemesis and often bitter opponent Hawke fondly described him as a "very significant figure in the history of Australian politics" who, in his post-Prime Ministerial years, "became an outstanding figure in the advancement of human rights issues in all respects", praised him for being "extraordinarily generous and welcoming to refugees from Indochina" and concluded that Fraser had "moved so far to the left he was almost out of sight". Andrew Peacock, who had challenged Fraser for the Liberal leadership and later succeeded him, said that he had "a deep respect and pleasurable memories of the first five years of the Fraser Government... I disagreed with him later on but during that period in the 1970s he was a very effective Prime Minister", and lamented that "despite all my arguments with him later on I am filled with admiration for his efforts on China".

In June 2018, he was honoured with the naming of the Australian Electoral Division of Fraser in the inner north-western suburbs of Melbourne.


Orders

Foreign honours

Organisations

Personal

Fellowships

Academic degrees






</doc>
<doc id="19735" url="https://en.wikipedia.org/wiki?curid=19735" title="Macquarie University">
Macquarie University

Macquarie University () is a public research university based in Sydney, Australia, in the suburb of Macquarie Park. Founded in 1964 by the New South Wales Government, it was the third university to be established in the metropolitan area of Sydney.

Established as a verdant university, Macquarie has five faculties, as well as the Macquarie University Hospital and the Macquarie Graduate School of Management, which are located on the university's main campus in suburban Sydney.

The university is the first in Australia to fully align its degree system with the Bologna Accord.

The idea of founding a third university in Sydney was flagged in the early 1960s when the New South Wales Government formed a committee of enquiry into higher education to deal with a perceived emergency in university enrollments in New South Wales. During this enquiry, the Senate of the University of Sydney put in a submission which highlighted 'the immediate need to establish a third university in the metropolitan area'. After much debate a future campus location was selected in what was then a semi-rural part of North Ryde, and it was decided that the future university be named after Lachlan Macquarie, an important early governor of the colony of New South Wales.

Macquarie University was formally established in 1964 with the passage of the Macquarie University Act 1964 by the New South Wales parliament.

The initial concept of the campus was to create a new high technology corridor, similar to the area surrounding Stanford University in Palo Alto, California, the goal being to provide for interaction between industry and the new university. The academic core was designed in the Brutalist style and developed by the renowned town planner Walter Abraham who also oversaw the next 20 years of planning and development for the university. A committee appointed to advise the state government on the establishment of the new university at North Ryde nominated Abraham as the architect-planner. The fledgling Macquarie University Council decided that planning for the campus would be done within the university, rather than by consultants, and this led to the establishment of the architect-planners office.

The first Vice-Chancellor of Macquarie University, Alexander George Mitchell, was selected by the University Council which met for the first time on 17 June 1964. Members of the first university council included: Colonel Sir Edward Ford OBE, David Paver Mellor, Rae Else-Mitchell QC and Sir Walter Scott.

The university first opened to students on 6 March 1967 with more students than anticipated. The Australian Universities Commission had allowed for 510 effective full-time students (EFTS) but Macquarie had 956 enrolments and 622 EFTS. Between 1968 and 1969, enrolment at Macquarie increased dramatically with an extra 1200 EFTS, with 100 new academic staff employed. 1969 also saw the establishment of the Macquarie Graduate School of Management (MGSM).
Macquarie grew during the seventies and eighties with rapid expansion in courses offered, student numbers and development of the site. In 1972, the university established the Macquarie Law School, the third law school in Sydney. In their book "Liberality of Opportunity", Bruce Mansfield and Mark Hutchinson describe the founding of Macquarie University as 'an act of faith and a great experiment'. An additional topic considered in this book is the science reform movement of the late 1970s that resulted in the introduction of a named science degree, thus facilitating the subsequent inclusion of other named degrees in addition to the traditional BA. An alternative view on this topic is given by theoretical physicist John Ward.

In 1973 the student union (MUSC) worked with the Builders Labourers Federation (BLF) to organise one of the first "pink bans". Similar in tactic to the green ban, the pink ban was recommended when one of the residential colleges at Macquarie University, Robert Menzies College, ordered a student to lead a celibate life and undertake therapy and confession to cure himself of his homosexuality. The BLF decided to stop all construction work at the college until the university and the college Master made statements committing to a non-discriminatory university environment. MUSC was successful in engaging with the BLF again in 1974 when a woman at Macquarie University had her NSW Department of Education scholarship cancelled on the basis that she was a lesbian and therefore unfit to be a teacher.

After over a decade of service, the first Vice Chancellor Mitchell was succeeded by Edwin Webb in December 1975. Webb was required to steer the university through one of its most difficult periods as the value of universities were debated and the governments introduced significant funding cuts. Webb left the university in 1986 and was succeeded by Di Yerbury, the first female Vice-Chancellor in Australia. Yerbury would go on to hold the position of Vice-Chancellor for nearly 20 years.

In 1990 the university absorbed the Institute of Early Childhood Studies of the Sydney College of Advanced Education, under the terms of the Higher Education (Amalgamation) Act 1989. l

Steven Schwartz replaced Di Yerbury at the beginning of 2006. Yerbury's departure was attended with much controversy, including a "bitter dispute" with Schwartz, disputed ownership of university artworks worth $13 million and Yerbury's salary package. In August 2006, Schwartz expressed concern about the actions of Yerbury in a letter to university auditors. Yerbury strongly denied any wrongdoing and claimed the artworks were hers.

During 2007, Macquarie University restructured its student organisation after an audit raised questions about management of hundreds of thousands of dollars in funds by student organisations At the centre of the investigation was Victor Ma, president of the Macquarie University Students' Council, who was previously involved in a high-profile case of student election fixing at the University of Sydney.
The university Council resolved to immediately remove Ma from his position. Vice-Chancellor Schwartz cited an urgent need to reform Macquarie's main student bodies.
However, Ma strongly denied any wrongdoing and labelled the controversy a case of 'character assassination'.
The Federal Court ordered on 23 May 2007 that Macquarie University Union Ltd be wound up.

Following the dissolution of Macquarie University Union Ltd, the outgoing student organisation was replaced with a new wholly owned subsidiary company of the university, known as U@MQ Ltd. The new student organisation originally lacked a true student representative union; however, following a complete review and authorisation from the university Council, a new student union known as Macquarie University Students Association (MUSRA) was established in 2009.

Within the first few hundred days of Schwartz's instatement as Vice-Chancellor, the 'Macquarie@50' strategic plan was launched, which positioned the university to enhance research, teaching, infrastructure and academic rankings by the university's 50th anniversary in 2014. Included in the university's plans for the future was the establishment of a sustainability office in order to more effectively manage environmental and social development at Macquarie. As part of this campaign, in 2009 Macquarie became the first Fair Trade accredited university in Australia. The beginning of 2009 also saw the introduction of a new logo for the university which retained the Sirius Star, present on both the old logo and the university crest, but now 'embedded in a stylised lotus flower'. In accordance with the university by-law, the crest continues to be used for formal purposes and is displayed on university testamurs. The by-law also prescribes the university's motto, taken from Chaucer: 'And gladly teche'.

In 2013, the university became the first in Australia to fully align its degree system with the Bologna Accord.

Macquarie's arms was assumed through a 1967 amendment of the Macquarie University Act 1964 (Confirmed by Letters Patent of the College of Arms, 16 August 1969). The escutcheon displays the Macquarie Lighthouse tower, the first major public building in the colony, as well as the Sirius star, the name of the flagship of the First Fleet. The university's founders originally wanted to base the university's arms on Lachlan Macquarie's family crest, but they decided to go for a more radical approach that represented Lachlan Macquarie as a builder and administrator. The motto chosen for the university was "And Gladly Teche." This is taken from the general Prologue of The Canterbury Tales, Geoffrey Chaucer c.1400 and symbolises the university's commitment to both learning and teaching. The coat of arms and the motto are used in a very limited number of formal communications.

Macquarie has had a number of logos in its history. In 2014, the university launched a new logo as part of its Shared Identity Project. The logo reintroduced the Macquarie Lighthouse, a popular symbol of the University within the University community and maintained the Sirus Star.

Macquarie University's main campus is located about north-west of the Sydney CBD and is set on 126 hectares of rolling lawns and natural bushland. Located within the high-technology corridor of Sydney's north-west and in close proximity to Macquarie Park and its surrounding industries, Macquarie's location has been crucial in its development as a relatively research intensive university.

Prior to the development of the campus, most of the site was cultivated with peach orchards, market gardens and poultry farms. The university's first architect-planner was Walter Abraham, one of the first six administrators appointed to Macquarie University. As the site adapted from its former rural use to a busy collegiate environment, he implemented carefully designed planting programs across the campus. Abraham established a grid design comprising lots of running north–south, with the aim of creating a compact academic core. The measure of was seen as one minute's walk, and grid design reflected the aim of having a maximum walk of 10 minutes between any two parts of the university. The main east–west walkway that runs from the Macquarie University Research Park through to the arts faculty buildings, was named Wally's Walk in recognition of Walter Abraham's contribution to the development of the university.

Apart from its centres of learning, the campus features the Macquarie University Research Park, museums, art galleries, a sculpture park, an observatory, a sport and aquatic centre and also the private Macquarie University Hospital. The campus has its own postcode, 2109.

Macquarie became the first university in Australia to own and operate a private medical facility in 2010 when it opened a $300 million hospital on its campus. The hospital is the first and only private not-for-profit teaching hospital on an Australian university campus. The Macquarie University Hospital is located to the north of the main campus area towards the university sports grounds. It comprises 183 beds, 12 operating theatres, 2 cardiac and vascular angiography suites. The hospital is co-located with the university's Australian School of Advanced Medicine.

The university hosts a number of high technology companies on its campus. Primarily designed to encourage interaction between the university and industry, commercialisation of its campus has also given the institution an additional revenue stream. Tenants are selected based on their potential to collaborate with the universities researches or their ability to provide opportunities for its students and graduates. Cochlear Limited, has its headquarters in close proximity to the Australian Hearing Hub on the southern edge of campus. Other companies that have office space at the campus include Dow Corning, Goodman Fielder, Nortel Networks, OPSM and Siemens.

The Macquarie University Observatory was originally constructed in 1978 as a research facility but, since 1997, has been accessible to the public through its Public Observing Program.

The library houses over 1.8 million items and uses the Library of Congress Classification System. The library features several collections including a Rare Book Collection, a Palaeontology Collection and the Brunner Collection of Egyptological materials. Macquarie University operated two libraries during the transition. The old library in building C7A closed at the end of July 2011 (which has since been repurposed as a student support and study space), and the new library in building C3C became fully operational on 1 August 2011. The new library was the first university library in Australia to possess an Automated Storage and Retrieval System (ASRS). The ASRS consists of an environmentally controlled vault with metal bins storing the items; robotic cranes retrieve an item on request and deliver it to the service desk for collection.

The Macquarie University Incubator is a space to research and develop ideas that can be commercialised. It was established in 2017 as a part of the Macquarie Park Innovation District (MPID) project. Macquarie University received $1 million grant from the New South Wales government to build the incubator. The University has also committed about $7 million to the incubator with financial support of the big businesses and the New South Wales government. It was officially opened by Prince Andrew, Duke of York on 25 September 2017.

Macquarie University has two residential colleges on its campus, Dunmore Lang College and Robert Menzies College, both founded in 1972. The colleges offer academic support and a wide range of social and sporting activities in a communal environment.

Separate to the colleges is the Macquarie University Village. The village has over 900 rooms in mostly town house style buildings to the north of the campus. The village encourages its students to interact in its communal spaces and has a number of social events throughout the year.

The museums and collections of Macquarie University are extensive and include nine museums and galleries. Each collection focuses on various historical, scientific or artistic interests. The most visible collection on campus is the sculpture park which is exhibited across the entire campus. At close to 100 sculptures on display, it is the largest park of its kind in the Southern Hemisphere. All museums and galleries are open to the public and offer educational programs for students at primary, secondary and tertiary levels.

Located on the western side of the campus is the Macquarie University Sport and Aquatic Centre. Previously a sports hall facility, the complex was renovated and reopened in 2007 with the addition of the new gym and aquatic centre. It houses a 50-metre FINA-compliant outdoor pool and a 25-metre indoor pool. The complex also contains a gymnasium and squash, badminton, basketball, volleyball and netball courts.

Macquarie also has seven hectares of high quality playing fields for football, cricket and tennis. Situated to the north of the campus, the playing fields are used by the university as well as a number of elite sporting teams such as Sydney FC and the Westfield Matildas.

Macquarie University is served by Macquarie University Station on the Sydney Metro Northwest line. The station opened in 2009 as part of the Epping to Chatswood Rail Link on the Sydney Trains network. Services initially ran as a shuttle between Epping and Chatswood before being incorporated into the T9 Northern Line.

In 2018, Macquarie University station closed for six months for conversion to Sydney Metro on the Sydney Metro Northwest line. Platform screen doors were installed as part of the upgrade. Macquarie is the only university in Australia with a railway station on campus. The station is served by driverless Alstom Metropolis trains every ten minutes during the off peak and every four minutes during peak hours.

There is also a major bus interchange within the campus that provides close to 800 bus services daily. The M2 Motorway runs parallel to the northern boundary of the campus and is accessible to traffic from the university.

The university currently comprises 35 departments within five faculties:

Research centres, schools and institutes that are affiliated with the university:

Macquarie University's Australian Hearing Hub is partnered with Cochlear. Cochlear Headquarters are on campus. The Australian Hearing Hub includes the head office of Australian Hearing.

The Australian Research Institute for Environment and Sustainability is a research centre that promotes change for environmental sustainability, is affiliated with the University and is located on its campus.

Access Macquarie Limited was established in 1989 as the commercial arm of the university. It facilitates and supports the commercial needs of industry, business and government organisations seeking to utilise the academic expertise of the broader University community.

The university is governed by a 17-member Council.

The University Council is the governing authority of the university under the "Macquarie University Act 1989". The Council takes primary responsibility for the control and management of the affairs of the University, and is empowered to make by-laws and rules relating to how the University is managed. Members of the Council include the University Vice-Chancellor, Academic and non-academic staff, the Vice President of the Academic Senate and a student representative. The Council is chaired by The Chancellor of the University.

The Academic Senate is the primary academic body of the university. It has certain powers delegated to it by Council, such as the approving of examination results and the completion of requirements for the award of degrees. At the same time, it makes recommendations to the Council concerning all changes to degree rules, and all proposals for new awards. While the Academic Senate is an independent body, it is required to make recommendations to the university Council in relation to matters outside its delegated authority.

Macquarie's current Vice-Chancellor, Bruce Dowton, took over from Schwartz in September 2012. Prior to his appointment Dowton served as a senior medical executive having held a range of positions in university, healthcare and consulting organisations. He also served as a pediatrician at the Massachusetts General Hospital for Children, and as Clinical Professor of Pediatrics at Harvard Medical School. There have been five Vice-Chancellors in the university's history.

The Macquarie University International College offers Foundation Studies (Pre-University) and University-level Diplomas. Upon successful completion of a MUIC Diploma, students enter the appropriate bachelor's degree as a second year student.

The Centre for Macquarie English is the English-language centre that offers a range of specialised, direct entry English programmes that are approved by Macquarie University.

The university positions itself as being research intensive. In 2012, 85% of Macquarie's broad fields of research was rated 'at or above world standard' in the Excellence in Research for Australia 2012 National report. The university is within the top 3 universities in Australia for the number of peer reviewed publications produced per academic staff member.

Researchers at Macquarie University, David Skellern and Neil Weste, and the Commonwealth Scientific and Industrial Research Organisation helped develop Wi-Fi. David Skellern has been a major donor to the University through the Skellern Family Trust. Macquarie physicists Frank Duarte and Jim Piper pioneered the laser designs adopted by researchers worldwide, in various major national programs, for atomic vapor laser isotope separation.

Macquarie University's linguistics department developed the Macquarie Dictionary. The dictionary is regarded as the standard reference on Australian English.

Macquarie University has a research partnership with the University of Hamburg in Germany and Fudan University in China. They offer dual and joint degree programs and engage in joint research.

Macquarie University (MQ) world rankings includes it being number 237 on the QS rankings, number 251+ on Times (THE), number 151+ on ARWU, and number 267= with US News. This contributes to Macquarie being the number 8 ranked Australian university overall in the world ranking systems. Macquarie University rankings within Australia include being placed at number 8 on the ERA scale (2012) and being a 4 1/2 Star AEN rated university. Macquarie also has a student survey satisfaction rating of 77.4% for business, 90.3% for health, 91.4% for arts, and 93.8% for science. Macquarie is ranked in the top 40 universities in the Asia-Pacific region and within Australia's top 12 universities according to the Academic Ranking of World Universities, the U.S. News & World Report Rankings and the QS World University Rankings. Macquarie was the highest ranked university in Australia under the age of 50 and was ranked 18th in the world (prior to its golden jubilee in 2014), according to the QS World University Rankings.

Internationally, Macquarie was ranked 239th in the world (9th in Australia) in the Academic Ranking of World Universities of 2014. Macquarie University was ranked among the top 50 universities in the world for linguistics (43rd), psychology (48th) and earth and marine sciences (48th), and was ranked in the top 5 nationally for philosophy and earth and marine sciences, according to the 2014 QS World University Rankings.

Macquarie ranked 67th in the world for Arts and Humanities (equal 5th in Australia), according to the 2015 Times Higher Education rankings by subject and 54th in the world for arts and humanities, according to the 2017 USNWR rankings by subject. Arts and Humanities is Macquarie's best discipline area in rankings. Macquarie was one of four non-Group of Eight universities ranked in the top 100 universities in the world in particular discipline areas.

The Macquarie Graduate School of Management is one of the oldest business schools in Australia. In 2014, "The Economist" ranked MGSM 5th in the Asia-Pacific, 3rd in Australia, 1st in Sydney/New South Wales and 49th in the world. It was the highest ranked business school in Australia and was ranked 68th in the world in the 2015 "Financial Times" MBA ranking.

Macquarie is the fourth largest university in Sydney (38,753 students in 2013). The university has the largest student exchange programme in Australia.

In 2012, 9,802 students from Asia were enrolled at Macquarie University (Sydney campuses and offshore programs in China, Hong Kong, Korea and Singapore).

Campus Life manages the university's non-academic services: food and retail, sport and recreation, student groups, child care, and entertainment. From late 2017 onward its Campus Hub facility has been closed for reconstruction; a 'pop-up'-style replacement, the Campus Common, has been opened for the duration.

The Global Leadership Program (GLP) is a University-funded co-curricular program that is open to all students and can be undertaken alongside any degree at Macquarie University. The GLP aims to instil leadership and innovation skills, cross-cultural understanding and a sense of global citizenship in its graduates. Upon successful completion of the GLP, students receive a formal notation on their academic transcript and a certificate.

Macquarie's GLP was the first of its kind when it launched in the Australian university sector in 2005 and is the country's flagship tertiary global leadership program with more than 4000 active participants in more than 200 academic disciplines. GLP is a co-curricular learning and engagement program that students design according to their own interests and complete at their own pace. Students are required to complete a workshop series, attend tailored keynote speaker and networking events and complete an experiential credit component. This ranges from short-term study abroad, volunteering (domestic and/or international), internships (domestic and/or international), learning a new language or attending internationally themed seminars and study tours.

The GLP won the Institute for International Education's 2017 Heiskell award for Innovation in International Education - Internationalising the Campus. Macquarie University is the first Southern Hemisphere university to receive the award in its 17-year history. 
The GLP was awarded the 2018 NSW International Student Community Engagement Award (Joint Winner) in the Education Provider category. This award recognises the innovative way in which the GLP facilitates connection and engagement with community for Macquarie University International GLP Students, and also recognises the contribution that the GLP makes to the International Student experience in New South Wales. In 2019, the GLP won the Global PIEoneer Award for International Education in the category of 'Progressive Education Delivery' in Guildhall, London. The PIEoneer Awards are the only global awards that celebrate innovation and achievement across the whole of the international education industry.

Macquarie University has its own community radio station on campus, 2SER FM. The station is jointly owned by Macquarie University and University of Technology, Sydney.

Macquarie University students celebrate Conception Day each year since 1969 to – according to legend – commemorate the date of conception of Lachlan Macquarie, as his birthday fell at the wrong time of year for a celebration. Conception Day is traditionally held on the last day of classes before the September mid-semester break.

Alumni include Rhodes and John Monash Scholars and several Fulbright Scholars.

Notable alumni include: Australian politician and former Lord Mayor of Brisbane, Jim Soorley; Australian politician, Tanya Plibersek; Australian basketball player, Lauren Jackson; Australian swimmer, Ian Thorpe; Australian water polo player, Holly Lincoln-Smith; three founding members of the Australian children's musical group The Wiggles (Murray Cook, Anthony Field, Greg Page); former Director-General of the National Library of Australia, Anne-Marie Schwirtlich AM; New Zealand conservationist, Pete Bethune.

Notable alumni in science include: Australian scientist Barry Brook, American physicist Frank Duarte, and Australian physicist Cathy Foley. Alumni notable in the business world include: Australian hedge fund manager Greg Coffey, Australian businesswoman Catherine Livingstone, founder of Freelancer.com Matt Barrie, businessman Napoleon Perdis and Australian venture capitalist Larry R. Marshall.

Notable faculty members include: Indian neurosurgeon, B. K. Misra
Australian writer and four time Miles Franklin Award winner, Thea Astley; Hungarian Australian mathematician, Esther Szekeres; Australian mathematician, Neil Trudinger; Australian composer, Phillip Wilcher; Australian environmentalist and activist, Tim Flannery; British physicist and author, Paul Davies; British-Australian physicist, John Clive Ward; Israeli-Australian mathematician, José Enrique Moyal; Australian linguist, Geoffrey Hull; Australian geologist, Fellow of the Australian Academy of Science, John Veevers; Australian climatologist, Ann Henderson-Sellers; Australian sociologist, Raewyn Connell.

Four Macquarie University academics were included in The World's Most Influential Minds 2014 report by Thomson Reuters, which identified the most highly cited researchers of the last 11 years.




</doc>
<doc id="19736" url="https://en.wikipedia.org/wiki?curid=19736" title="Muspelheim">
Muspelheim

In Norse cosmology, Muspelheim (), also called Muspell (), is a realm of fire.

The etymology of "Muspelheim" is uncertain, but may come from "Mund-spilli", "world-destroyers", "wreck of the world".

Muspelheim is described as a hot and glowing land of fire, home to the fire giants, and guarded by Surtr, with his flaming sword. It is featured in both the creation and destruction stories of Norse myth. According to the Prose Edda, A great time before the earth was made, Niflheim existed. Inside Niflheim was a well called Hvergelmer, from this well flowed numerous streams known as the Elivog. Their names were Svol, Gunnthro, Form, Finbul, Thul, Slid and Hrid, Sylg and Ylg, Vid, Leipt and Gjoll. After a time these streams had traveled far from their source at Niflheim. So far that the venom that flowed within them hardened and turned to ice. When this ice eventually settled, rain rose up from it, and froze into rime. This ice then began to layer itself over the primordial void, Ginnungagap. This made the northern portion of Ginungagap thick with ice, and storms begin to form within. However, in the southern region of Ginungagap glowing sparks were flying out of Muspelheim. When the heat and sparks from Muspelheim met the ice, it began to melt. These sparks would go on to create the Sun, Moon, and stars, and the drops would form the primeval being Ymir. "by the might of him who sent the heat, the drops quickened into life and took the likeness of a man, who got the name Ymer. But the Frost giants call him Aurgelmer"

The "Prose Edda" section "Gylfaginning" foretells that the sons of Muspell will break the Bifröst bridge as part of the events of Ragnarök:

Muspelheim appears in different kinds of Marvel Entertainment media. 

The game Puzzle & Dragons features a monster entitled Flamedragon Muspelheim and Infernodragon Muspelheim.

In the game God of War, you can travel to Muspelheim where you can complete the six Trials of Muspelheim. When completing each trial, the player will receive rewards and will advance Kratos and Atreus closer to the top of a large volcano.



</doc>
<doc id="19737" url="https://en.wikipedia.org/wiki?curid=19737" title="Maxwell's equations">
Maxwell's equations

Maxwell's equations are a set of coupled partial differential equations that, together with the Lorentz force law, form the foundation of classical electromagnetism, classical optics, and electric circuits. 
The equations provide a mathematical model for electric, optical, and radio technologies, such as power generation, electric motors, wireless communication, lenses, radar etc. They describe how electric and magnetic fields are generated by charges, currents, and changes of the fields. The equations are named after the physicist and mathematician James Clerk Maxwell, who, in 1861 and 1862, published an early form of the equations that included the Lorentz force law. Maxwell first used the equations to propose that light is an electromagnetic phenomenon.

An important consequence of Maxwell's equations is that they demonstrate how fluctuating electric and magnetic fields propagate at a constant speed ("c") in a vacuum. Known as electromagnetic radiation, these waves may occur at various wavelengths to produce a spectrum of light from radio waves to gamma rays.

The equations have two major variants. The microscopic Maxwell equations have universal applicability but are unwieldy for common calculations. They relate the electric and magnetic fields to total charge and total current, including the complicated charges and currents in materials at the atomic scale. The "macroscopic" Maxwell equations define two new auxiliary fields that describe the large-scale behaviour of matter without having to consider atomic scale charges and quantum phenomena like spins. However, their use requires experimentally determined parameters for a phenomenological description of the electromagnetic response of materials.

The term "Maxwell's equations" is often also used for equivalent alternative formulations. Versions of Maxwell's equations based on the electric and magnetic scalar potentials are preferred for explicitly solving the equations as a boundary value problem, analytical mechanics, or for use in quantum mechanics. The covariant formulation (on spacetime rather than space and time separately) makes the compatibility of Maxwell's equations with special relativity manifest. Maxwell's equations in curved spacetime, commonly used in high energy and gravitational physics, are compatible with general relativity. In fact, Albert Einstein developed special and general relativity to accommodate the invariant speed of light, a consequence of Maxwell's equations, with the principle that only relative movement has physical consequences.

The publication of the equations marked the unification of a theory for previously separately described phenomena: magnetism, electricity, light and associated radiation.
Since the mid-20th century, it has been understood that Maxwell's equations do not give an exact description of electromagnetic phenomena, but are instead a classical limit of the more precise theory of quantum electrodynamics.

Gauss's law describes the relationship between a static electric field and the electric charges that cause it: a static electric field points away from positive charges and towards negative charges, and the net outflow of the electric field through any closed surface is proportional to the charge enclosed by the surface. Picturing the electric field by its field lines, this means the field lines begin at positive electric charges and end at negative electric charges. 'Counting' the number of field lines passing through a closed surface yields the total charge (including bound charge due to polarization of material) enclosed by that surface, divided by dielectricity of free space (the vacuum permittivity).

Gauss's law for magnetism states that there are no "magnetic charges" (also called magnetic monopoles), analogous to electric charges. Instead, the magnetic field due to materials is generated by a configuration called a dipole, and the net outflow of the magnetic field through any closed surface is zero. Magnetic dipoles are best represented as loops of current but resemble positive and negative 'magnetic charges', inseparably bound together, having no net 'magnetic charge'. In terms of field lines, this equation states that magnetic field lines neither begin nor end but make loops or extend to infinity and back. In other words, any magnetic field line that enters a given volume must somewhere exit that volume. Equivalent technical statements are that the sum total magnetic flux through any Gaussian surface is zero, or that the magnetic field is a solenoidal vector field.

The Maxwell–Faraday version of Faraday's law of induction describes how a time varying magnetic field creates ("induces") an electric field. In integral form, it states that the work per unit charge required to move a charge around a closed loop equals the rate of change of the magnetic flux through the enclosed surface.

The dynamically induced electric field has closed field lines similar to a magnetic field, unless superposed by a static (charge induced) electric field. This aspect of electromagnetic induction is the operating principle behind many electric generators: for example, a rotating bar magnet creates a changing magnetic field, which in turn generates an electric field in a nearby wire.

Ampère's law with Maxwell's addition states that magnetic fields can be generated in two ways: by electric current (this was the original "Ampère's law") and by changing electric fields (this was "Maxwell's addition", which he called displacement current). In integral form, the magnetic field induced around any closed loop is proportional to the electric current plus displacement current (proportional to the rate of change of electric flux) through the enclosed surface.

Maxwell's addition to Ampère's law is particularly important: it makes the set of equations mathematically consistent for non static fields, without changing the laws of Ampere and Gauss for static fields. However, as a consequence, it predicts that a changing magnetic field induces an electric field and vice versa. Therefore, these equations allow self-sustaining "electromagnetic waves" to travel through empty space (see electromagnetic wave equation).

The speed calculated for electromagnetic waves, which could be predicted from experiments on charges and currents, matches the speed of light; indeed, light "is" one form of electromagnetic radiation (as are X-rays, radio waves, and others). Maxwell understood the connection between electromagnetic waves and light in 1861, thereby unifying the theories of electromagnetism and optics.

In the electric and magnetic field formulation there are four equations that determine the fields for given charge and current distribution. A separate law of nature, the Lorentz force law, describes how, conversely, the electric and magnetic fields act on charged particles and currents. A version of this law was included in the original equations by Maxwell but, by convention, is included no longer. The vector calculus formalism below, the work of Oliver Heaviside, has become standard. It is manifestly rotation invariant, and therefore mathematically much more transparent than Maxwell's original 20 equations in x,y,z components. The relativistic formulations are even more symmetric and manifestly Lorentz invariant. For the same equations expressed using tensor calculus or differential forms, see alternative formulations.

The differential and integral formulations are mathematically equivalent and are both useful. The integral formulation relates fields within a region of space to fields on the boundary and can often be used to simplify and directly calculate fields from symmetric distributions of charges and currents. On the other hand, the differential equations are purely "local" and are a more natural starting point for calculating the fields in more complicated (less symmetric) situations, for example using finite element analysis.

Symbols in bold represent vector quantities, and symbols in "italics" represent scalar quantities, unless otherwise indicated.
The equations introduce the electric field, , a vector field, and the magnetic field, , a pseudovector field, each generally having a time and location dependence.
The sources are

The universal constants appearing in the equations (the first two ones explicitly only in the SI units formulation) are:

In the differential equations, 

In the integral equations, 
Here a "fixed" volume or surface means that it does not change over time.
The equations are correct, complete, and a little easier to interpret with time-independent surfaces. For example, since the surface is time-independent, we can bring the differentiation under the integral sign in Faraday's law:
Maxwell's equations can be formulated with possibly time-dependent surfaces and volumes by using the differential version and using Gauss and Stokes formula appropriately. 

The definitions of charge, electric field, and magnetic field can be altered to simplify theoretical calculation, by absorbing dimensioned factors of and into the units of calculation, by convention. With a corresponding change in convention for the Lorentz force law this yields the same physics, i.e. trajectories of charged particles, or work done by an electric motor. These definitions are often preferred in theoretical and high energy physics where it is natural to take the electric and magnetic field with the same units, to simplify the appearance of the electromagnetic tensor: the Lorentz covariant object unifying electric and magnetic field would then contain components with uniform unit and dimension. Such modified definitions are conventionally used with the Gaussian (CGS) units. Using these definitions and conventions, colloquially "in Gaussian units",
the Maxwell equations become:

The equations are particularly readable when length and time are measured in compatible units like seconds and lightseconds i.e. in units such that c = 1 unit of length/unit of time. Ever since 1983 (see International System of Units), metres and seconds are compatible except for historical legacy since "by definition" c = 299 792 458 m/s (≈ 1.0 feet/nanosecond).

Further cosmetic changes, called rationalisations, are possible by absorbing factors of depending on whether we want Coulomb's law or Gauss's law to come out nicely, see Lorentz-Heaviside units (used mainly in particle physics). In theoretical physics it is often useful to choose units such that Planck's constant, the elementary charge, and even Newton's constant are 1. See Planck units.

The equivalence of the differential and integral formulations are a consequence of the Gauss divergence theorem and the Kelvin–Stokes theorem.

According to the (purely mathematical) Gauss divergence theorem, the electric flux through the 
boundary surface can be rewritten as
The integral version of Gauss's equation can thus be rewritten as 
Since is arbitrary (e.g. an arbitrary small ball with arbitrary center), this is satisfied if and only if the integrand is zero everywhere. This is 
the differential equations formulation of Gauss equation up to a trivial rearrangement.

Similarly rewriting the magnetic flux in Gauss's law for magnetism in integral form gives 
which is satisfied for all if and only if formula_9 everywhere.

By the Kelvin–Stokes theorem we can rewrite the line integrals of the fields around the closed boundary curve to an integral of the "circulation of the fields" (i.e. their curls) over a surface it bounds, i.e.

Hence the modified Ampere law in integral form can be rewritten as 
Since can be chosen arbitrarily, e.g. as an arbitrary small, arbitrary oriented, and arbitrary centered disk, we conclude that the integrand is zero iff Ampere's modified law in differential equations form is satisfied.
The equivalence of Faraday's law in differential and integral form follows likewise.

The line integrals and curls are analogous to quantities in classical fluid dynamics: the circulation of a fluid is the line integral of the fluid's flow velocity field around a closed loop, and the vorticity of the fluid is the curl of the velocity field.

The invariance of charge can be derived as a corollary of Maxwell's equations. The left-hand side of the modified Ampere's Law has zero divergence by the div–curl identity. Expanding the divergence of the right-hand side, interchanging derivatives, and applying Gauss's law gives:

i.e.

By the Gauss Divergence Theorem, this means the rate of change of charge in a fixed volume equals the net current flowing through the boundary:
In particular, in an isolated system the total charge is conserved.

In a region with no charges () and no currents (), such as in a vacuum, Maxwell's equations reduce to:

Taking the curl of the curl equations, and using the curl of the curl identity we obtain

The quantity formula_16 has the dimension of (time/length). Defining
formula_17, the equations above have the form of the standard wave equations

Already during Maxwell's lifetime, it was found that the known values for formula_19 and formula_20 give formula_21, then already known to be the speed of light in free space. This led him to propose that light and radio waves were propagating electromagnetic waves, since amply confirmed. In the old SI system of units, the values of formula_22 and formula_23 are defined constants, (which means that by definition formula_24) that define the ampere and the metre. In the new SI system, only "c" keeps its defined value, and the electron charge gets a defined value. 
In materials with relative permittivity, , and relative permeability, , the phase velocity of light becomes

which is usually less than .

In addition, and are perpendicular to each other and to the direction of wave propagation, and are in phase with each other. A sinusoidal plane wave is one special solution of these equations. Maxwell's equations explain how these waves can physically propagate through space. The changing magnetic field creates a changing electric field through Faraday's law. In turn, that electric field creates a changing magnetic field through Maxwell's addition to Ampère's law. This perpetual cycle allows these waves, now known as electromagnetic radiation, to move through space at velocity .

The above equations are the microscopic version of Maxwell's equations, expressing the electric and the magnetic fields in terms of the (possibly atomic-level) charges and currents present. This is sometimes called the "general" form, but the macroscopic version below is equally general, the difference being one of bookkeeping.

The microscopic version is sometimes called "Maxwell's equations in a vacuum": this refers to the fact that the material medium is not built into the structure of the equations, but appears only in the charge and current terms. The microscopic version was introduced by Lorentz, who tried to use it to derive the macroscopic properties of bulk matter from its microscopic constituents.

"Maxwell's macroscopic equations", also known as Maxwell's equations in matter, are more similar to those that Maxwell introduced himself.

In the macroscopic equations, the influence of bound charge and bound current is incorporated into the displacement field and the magnetizing field , while the equations depend only on the free charges and free currents . This reflects a splitting of the total electric charge "Q" and current "I" (and their densities "ρ" and J) into free and bound parts:

The cost of this splitting is that the additional fields and need to be determined through phenomenological constituent equations relating these fields to the electric field and the magnetic field , together with the bound charge and current.

See below for a detailed description of the differences between the microscopic equations, dealing with "total" charge and current including material contributions, useful in air/vacuum;
and the macroscopic equations, dealing with "free" charge and current, practical to use within materials.

When an electric field is applied to a dielectric material its molecules respond by forming microscopic electric dipoles – their atomic nuclei move a tiny distance in the direction of the field, while their electrons move a tiny distance in the opposite direction. This produces a "macroscopic" "bound charge" in the material even though all of the charges involved are bound to individual molecules. For example, if every molecule responds the same, similar to that shown in the figure, these tiny movements of charge combine to produce a layer of positive bound charge on one side of the material and a layer of negative charge on the other side. The bound charge is most conveniently described in terms of the polarization of the material, its dipole moment per unit volume. If is uniform, a macroscopic separation of charge is produced only at the surfaces where enters and leaves the material. For non-uniform , a charge is also produced in the bulk.

Somewhat similarly, in all materials the constituent atoms exhibit magnetic moments that are intrinsically linked to the angular momentum of the components of the atoms, most notably their electrons. The connection to angular momentum suggests the picture of an assembly of microscopic current loops. Outside the material, an assembly of such microscopic current loops is not different from a macroscopic current circulating around the material's surface, despite the fact that no individual charge is traveling a large distance. These "bound currents" can be described using the magnetization .

The very complicated and granular bound charges and bound currents, therefore, can be represented on the macroscopic scale in terms of and , which average these charges and currents on a sufficiently large scale so as not to see the granularity of individual atoms, but also sufficiently small that they vary with location in the material. As such, "Maxwell's macroscopic equations" ignore many details on a fine scale that can be unimportant to understanding matters on a gross scale by calculating fields that are averaged over some suitable volume.

The "definitions" (not constitutive relations) of the auxiliary fields are:

where is the polarization field and is the magnetization field, which are defined in terms of microscopic bound charges and bound currents respectively. The macroscopic bound charge density and bound current density in terms of polarization and magnetization are then defined as

If we define the total, bound, and free charge and current density by

and use the defining relations above to eliminate , and , the "macroscopic" Maxwell's equations reproduce the "microscopic" equations.

In order to apply 'Maxwell's macroscopic equations', it is necessary to specify the relations between displacement field and the electric field , as well as the magnetizing field and the magnetic field . Equivalently, we have to specify the dependence of the polarization (hence the bound charge) and the magnetization (hence the bound current) on the applied electric and magnetic field. The equations specifying this response are called constitutive relations. For real-world materials, the constitutive relations are rarely simple, except approximately, and usually determined by experiment. See the main article on constitutive relations for a fuller description.

For materials without polarization and magnetization, the constitutive relations are (by definition)
where is the permittivity of free space and the permeability of free space. Since there is no bound charge, the total and the free charge and current are equal.

An alternative viewpoint on the microscopic equations is that they are the macroscopic equations "together" with the statement that vacuum behaves like a perfect linear "material" without additional polarization and magnetization.
More generally, for linear materials the constitutive relations are
where is the permittivity and the permeability of the material. For the displacement field the linear approximation is usually excellent because for all but the most extreme electric fields or temperatures obtainable in the laboratory (high power pulsed lasers) the interatomic electric fields of materials of the order of 10 V/m are much higher than the external field. For the magnetizing field formula_32, however, the linear approximation can break down in common materials like iron leading to phenomena like hysteresis. Even the linear case can have various complications, however.

Even more generally, in the case of non-linear materials (see for example nonlinear optics), and are not necessarily proportional to , similarly or is not necessarily proportional to . In general and depend on both and , on location and time, and possibly other physical quantities.

In applications one also has to describe how the free currents and charge density behave in terms of and possibly coupled to other physical quantities like pressure, and the mass, number density, and velocity of charge-carrying particles. E.g., the original equations given by Maxwell (see History of Maxwell's equations) included Ohm's law in the form

Following is a summary of some of the numerous other mathematical formalisms to write the microscopic Maxwell's equations, with the columns separating the two homogeneous Maxwell equations from the two inhomogeneous ones involving charge and current. Each formulation has versions directly in terms of the electric and magnetic fields, and indirectly in terms of the electrical potential and the vector potential . Potentials were introduced as a convenient way to solve the homogeneous equations, but it was thought that all observable physics was contained in the electric and magnetic fields (or relativistically, the Faraday tensor). The potentials play a central role in quantum mechanics, however, and act quantum mechanically with observable consequences even when the electric and magnetic fields vanish (Aharonov–Bohm effect).

Each table describes one formalism. See the main article for details of each formulation. SI units are used throughout.

The Maxwell equations can also be formulated on a spacetime-like Minkowski space where space and time are treated on equal footing. The direct spacetime formulations make manifest that the Maxwell equations are relativistically invariant. Because of this symmetry electric and magnetic field are treated on equal footing and are recognised as components of the Faraday tensor. This reduces the four Maxwell equations to two, which simplifies the equations, although we can no longer use the familiar vector formulation. In fact the Maxwell equations in the space + time formulation are not Galileo invariant and have Lorentz invariance as a hidden symmetry. This was a major source of inspiration for the development of relativity theory. Indeed, even the formulation that treats space and time separately is not a non-relativistic approximation and describes the same physics by simply renaming variables. For this reason the relativistic invariant equations are usually called the Maxwell equations as well.

Each table describes one formalism.


Other formalisms include the geometric algebra formulation and a matrix representation of Maxwell's equations. Historically, a quaternionic formulation was used.

Maxwell's equations are partial differential equations that relate the electric and magnetic fields to each other and to the electric charges and currents. Often, the charges and currents are themselves dependent on the electric and magnetic fields via the Lorentz force equation and the constitutive relations. These all form a set of coupled partial differential equations which are often very difficult to solve: the solutions encompass all the diverse phenomena of classical electromagnetism. Some general remarks follow.

As for any differential equation, boundary conditions and initial conditions are necessary for a unique solution. For example, even with no charges and no currents anywhere in spacetime, there are the obvious solutions for which E and B are zero or constant, but there are also non-trivial solutions corresponding to electromagnetic waves. In some cases, Maxwell's equations are solved over the whole of space, and boundary conditions are given as asymptotic limits at infinity. In other cases, Maxwell's equations are solved in a finite region of space, with appropriate conditions on the boundary of that region, for example an artificial absorbing boundary representing the rest of the universe, or periodic boundary conditions, or walls that isolate a small region from the outside world (as with a waveguide or cavity resonator).

Jefimenko's equations (or the closely related Liénard–Wiechert potentials) are the explicit solution to Maxwell's equations for the electric and magnetic fields created by any given distribution of charges and currents. It assumes specific initial conditions to obtain the so-called "retarded solution", where the only fields present are the ones created by the charges. However, Jefimenko's equations are unhelpful in situations when the charges and currents are themselves affected by the fields they create.

Numerical methods for differential equations can be used to compute approximate solutions of Maxwell's equations when exact solutions are impossible. These include the finite element method and finite-difference time-domain method. For more details, see Computational electromagnetics.

Maxwell's equations "seem" overdetermined, in that they involve six unknowns (the three components of and ) but eight equations (one for each of the two Gauss's laws, three vector components each for Faraday's and Ampere's laws). (The currents and charges are not unknowns, being freely specifiable subject to charge conservation.) This is related to a certain limited kind of redundancy in Maxwell's equations: It can be proven that any system satisfying Faraday's law and Ampere's law "automatically" also satisfies the two Gauss's laws, as long as the system's initial condition does, and assuming conservation of charge and the nonexistence of magnetic monopoles. 
This explanation was first introduced by Julius Adams Stratton in 1941.

Although it is possible to simply ignore the two Gauss's laws in a numerical algorithm (apart from the initial conditions), the imperfect precision of the calculations can lead to ever-increasing violations of those laws. By introducing dummy variables characterizing these violations, the four equations become not overdetermined after all. The resulting formulation can lead to more accurate algorithms that take all four laws into account.

Both identities formula_37, which reduce eight equations to six independent ones, are the true reason of overdetermination.

Equivalently, the overdetermination can be viewed as implying conservation of electric and magnetic charge, as they are required in the derivation described above but implied by the two Gauss's laws.

For linear algebraic equations, one can make 'nice' rules to rewrite the equations and unknowns. The equations can be linearly dependent. But in differential equations, and especially PDEs, one needs appropriate boundary conditions, which depend in not so obvious ways on the equations. Even more, if one rewrites them in terms of vector and scalar potential, then the equations are underdetermined because of Gauge fixing.

Maxwell's equations and the Lorentz force law (along with the rest of classical electromagnetism) are extraordinarily successful at explaining and predicting a variety of phenomena; however they are not exact, but a classical limit of quantum electrodynamics (QED).

Some observed electromagnetic phenomena are incompatible with Maxwell's equations. These include photon–photon scattering and many other phenomena related to photons or virtual photons, "nonclassical light" and quantum entanglement of electromagnetic fields (see quantum optics). E.g. quantum cryptography cannot be described by Maxwell theory, not even approximately. The approximate nature of Maxwell's equations becomes more and more apparent when going into the extremely strong field regime (see Euler–Heisenberg Lagrangian) or to extremely small distances.

Finally, Maxwell's equations cannot explain any phenomenon involving individual photons interacting with quantum matter, such as the photoelectric effect, Planck's law, the Duane–Hunt law, and single-photon light detectors. However, many such phenomena may be approximated using a halfway theory of quantum matter coupled to a classical electromagnetic field, either as external field or with the expected value of the charge current and density on the right hand side of Maxwell's equations.

Popular variations on the Maxwell equations as a classical theory of electromagnetic fields are relatively scarce because the standard equations have stood the test of time remarkably well.

Maxwell's equations posit that there is electric charge, but no magnetic charge (also called magnetic monopoles), in the universe. Indeed, magnetic charge has never been observed, despite extensive searches, and may not exist. If they did exist, both Gauss's law for magnetism and Faraday's law would need to be modified, and the resulting four equations would be fully symmetric under the interchange of electric and magnetic fields.


The developments before relativity:





</doc>
<doc id="19738" url="https://en.wikipedia.org/wiki?curid=19738" title="Metrizable space">
Metrizable space

In topology and related areas of mathematics, a metrizable space is a topological space that is homeomorphic to a metric space. That is, a topological space formula_1 is said to be metrizable if there is a metric 

such that the topology induced by "d" is formula_3. Metrization theorems are theorems that give sufficient conditions for a topological space to be metrizable.

Metrizable spaces inherit all topological properties from metric spaces. For example, they are Hausdorff paracompact spaces (and hence normal and Tychonoff) and first-countable. However, some properties of the metric, such as completeness, cannot be said to be inherited. This is also true of other structures linked to the metric. A metrizable uniform space, for example, may have a different set of contraction maps than a metric space to which it is homeomorphic.

One of the first widely recognized metrization theorems was . This states that every Hausdorff second-countable regular space is metrizable. So, for example, every second-countable manifold is metrizable. (Historical note: The form of the theorem shown here was in fact proved by Tychonoff in 1926. What Urysohn had shown, in a paper published posthumously in 1925, was that every second-countable "normal" Hausdorff space is metrizable). The converse does not hold: there exist metric spaces that are not second countable, for example, an uncountable set endowed with the discrete metric. The Nagata–Smirnov metrization theorem, described below, provides a more specific theorem where the converse does hold.

Several other metrization theorems follow as simple corollaries to Urysohn's theorem. For example, a compact Hausdorff space is metrizable if and only if it is second-countable.

Urysohn's Theorem can be restated as: A topological space is separable and metrizable if and only if it is regular, Hausdorff and second-countable. The Nagata–Smirnov metrization theorem extends this to the non-separable case. It states that a topological space is metrizable if and only if it is regular, Hausdorff and has a σ-locally finite base. A σ-locally finite base is a base which is a union of countably many locally finite collections of open sets. For a closely related theorem see the Bing metrization theorem.

Separable metrizable spaces can also be characterized as those spaces which are homeomorphic to a subspace of the Hilbert cube formula_4, i.e. the countably infinite product of the unit interval (with its natural subspace topology from the reals) with itself, endowed with the product topology.

A space is said to be locally metrizable if every point has a metrizable neighbourhood. Smirnov proved that a locally metrizable space is metrizable if and only if it is Hausdorff and paracompact. In particular, a manifold is metrizable if and only if it is paracompact.

The group of unitary operators formula_5 on a separable Hilbert space formula_6 endowed
with the strong operator topology is metrizable (see Proposition II.1 in ).

Non-normal spaces cannot be metrizable; important examples include

The real line with the lower limit topology is not metrizable. The usual distance function is not a metric on this space because the topology it determines is the usual topology, not the lower limit topology. This space is Hausdorff, paracompact and first countable.

The long line is locally metrizable but not metrizable; in a sense it is "too long".



</doc>
<doc id="19739" url="https://en.wikipedia.org/wiki?curid=19739" title="Martin Agricola">
Martin Agricola

Martin Agricola (6 January 1486 – 10 June 1556) was a German composer of Renaissance music and a music theorist.

Agricola was born in Schwiebus in Lebusz.

From 1524 until his death he lived at Magdeburg, where he occupied the post of teacher or cantor in the Protestant school. The senator and music-printer Georg Rhau, of Wittenberg, was a close friend of Agricola, whose theoretical works, providing valuable material concerning the change from the old to the new system of notation, he published.

Among Agricola's other theoretical works is "Musica instrumentalis deudsch" (1528 and 1545), a study of musical instruments, and one of the most important works in early organology; and one of the earliest books on the rudiments of music.

Agricola was also the first to harmonize in four parts Martin Luther's chorale, "Ein feste Burg".




</doc>
<doc id="19740" url="https://en.wikipedia.org/wiki?curid=19740" title="Max August Zorn">
Max August Zorn

Max August Zorn (; June 6, 1906 – March 9, 1993) was a German mathematician. He was an algebraist, group theorist, and numerical analyst. He is best known for Zorn's lemma, a method used in set theory that is applicable to a wide range of mathematical constructs such as vector spaces, ordered sets and the like. Zorn's lemma was first postulated by Kazimierz Kuratowski in 1922, and then independently by Zorn in 1935.

Zorn was born in Krefeld, Germany. He attended the University of Hamburg. He received his Ph.D. in April 1930 for a thesis on alternative algebras. He published his findings in "Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg". Zorn showed that split-octonions could be represented by a mixed-style of matrices called Zorn's vector-matrix algebra.

Max Zorn was appointed as an assistant at the University of Halle. However, he did not have the opportunity to work there for long since he was forced to leave Germany in 1933 because of the Nazi policies. According to grandson Eric, "[Max] spoke with a raspy, airy voice most of his life. Few people knew why, because he only told the story after significant prodding, but he talked that way because pro-Hitler thugs who objected to his politics, had battered his throat in a 1933 street fight."

Zorn emigrated to the U.S. and was appointed a Sterling Fellow at Yale University. While at Yale, Zorn wrote his paper "A Remark on Method in Transfinite Algebra" that stated his Maximum Principle, later called Zorn's lemma. It requires a set that contains the union of any chain of subsets to have one chain not contained in any other, called the maximal element. He illustrated the principle with applications in ring theory and field extensions. Zorn's lemma is an alternative expression of the axiom of choice, and thus a subject of interest in axiomatic set theory.

In 1936 he moved to UCLA and remained until 1946. While at UCLA Zorn revisited his study of alternative rings and proved the existence of the nilradical of certain alternative rings. According to Angus E. Taylor, Max was his most stimulating colleague at UCLA.

In 1946 Zorn became a professor at Indiana University where he taught until retiring in 1971. He was thesis advisor for Israel Nathan Herstein.

Zorn died in Bloomington, Indiana, United States, in March 1993, of congestive heart failure, according to his obituary in "The New York Times".

Max Zorn married Alice Schlottau and they had one son, Jens, and one daughter, Liz. Jens (born June 19, 1931) is an emeritus professor of physics at the University of Michigan and an accomplished sculptor. Max Zorn's grandson Eric Zorn is a columnist for the "Chicago Tribune".





</doc>
<doc id="19745" url="https://en.wikipedia.org/wiki?curid=19745" title="Main (river)">
Main (river)

The Main () is a river in Germany. With a length of (including its 52 km long source river White Main), it is the longest right tributary of the Rhine. It is also the longest river lying entirely in Germany (if the Weser and the Werra are considered as two separate rivers; together they are longer). The largest cities along the Main are Frankfurt am Main, Offenbach am Main and Würzburg.

The mainspring of the Main River flows through the German states of Bavaria, Baden-Württemberg (forming the border with Bavaria for some distance) and Hesse. Its basin competes with the Danube for water; as a result, many of its boundaries are identical with those of the European Watershed.

The Main begins near Kulmbach in Franconia at the joining of its two headstreams, the Red Main ("Roter Main") and the White Main ("Weißer Main"). The Red Main originates in the Franconian Jura mountain range, in length, and runs through Creussen and Bayreuth. The White Main originates in the mountains of the Fichtelgebirge; it is long. In its upper and middle section, the Main runs through the valleys of the German Highlands. Its lower section crosses the Lower Main Lowlands (Hanau-Seligenstadt Basin and northern Upper Rhine Plain) to Wiesbaden, where it discharges into the Rhine. Major tributaries of the Main are the Regnitz, the Franconian Saale, the Tauber, and the Nidda.

The name ""Main"" derives from the Latin "Moenus" or "Menus". It is not related to the name of the city Mainz (Latin: "Moguntiacum").

The Main is navigable for shipping from its mouth at the Rhine close to Mainz for to Bamberg. Since 1992, the Main has been connected to the Danube via the Rhine-Main-Danube Canal and the highly regulated Altmühl river. The Main has been canalized with 34 large locks () to allow CEMT class V vessels () to navigate the total length of the river. The 16 locks in the adjacent Rhine-Main-Danube Canal and the Danube itself are of the same dimensions.

There are 34 dams and locks along the 380 km navigable portion of the Main, from the confluence with the Regnitz near Bamberg, to the Rhine.


Most of the dams along the Main also have turbines for power generation.

Tributaries from source to mouth:
Left

Right

Around Frankfurt are several large inland ports. Because the river is rather narrow on many of the upper reaches, navigation with larger vessels and push convoys requires great skill.

The largest cities along the Main are Frankfurt am Main, Offenbach am Main and Würzburg. The Main also passes the following towns: Burgkunstadt, Lichtenfels, Bad Staffelstein, Eltmann, Haßfurt, Schweinfurt, Volkach, Kitzingen, Marktbreit, Ochsenfurt, Karlstadt, Gemünden, Lohr, Marktheidenfeld, Wertheim, Miltenberg, Obernburg, Erlenbach/Main, Aschaffenburg, Seligenstadt, Hainburg, Hanau, Hattersheim, Flörsheim, and Rüsselsheim.

The river has gained enormous importance as a vital part of European "Corridor VII", the inland waterway link from the North Sea to the Black Sea.

In a historical and political sense, the Main line is referred to as the northern border of Southern Germany, with its predominantly Catholic population. The river roughly marked the southern border of the North German Federation, established in 1867 under Prussian leadership as the predecessor of the German Empire.

The river course also corresponds with the Speyer line isogloss between Central and Upper German dialects, sometimes mocked as "Weißwurstäquator".

The Main-Radweg is a major German bicycle path running along the Main River. It is approximately and was the first long-distance bicycle path to be awarded 5 stars by the General German Bicycle Club ADFC in 2008. It starts from either Creußen or Bischofsgrün and ends in Mainz.





</doc>
<doc id="19747" url="https://en.wikipedia.org/wiki?curid=19747" title="Marcus Vipsanius Agrippa">
Marcus Vipsanius Agrippa

Marcus Agrippa (; c. 63 BC – 12 BC) was a Roman general, statesman and architect. He was a close friend, son-in-law, and lieutenant to Augustus and was responsible for the construction of some of the most notable buildings in the history of Rome and for important military victories, most notably at the Battle of Actium in 31 BC against the forces of Mark Antony and Cleopatra. As a result of these victories, Octavianus became the first Roman Emperor, adopting the name of Augustus. Agrippa assisted Augustus in making Rome "a city of marble" and renovating aqueducts to give all Romans, from every social class, access to the highest quality public services. He was responsible for the creation of many baths, porticoes and gardens, as well as the original Pantheon. Agrippa was also husband to Julia the Elder (who later married the second Emperor Tiberius), maternal grandfather to Caligula, and maternal great-grandfather to the Emperor Nero.

Agrippa was born between 64 and 62 BC, in an uncertain location. His father was called Lucius Vipsanius. He had an elder brother whose name was also Lucius Vipsanius, and a sister named Vipsania Polla. His family originated in the Italian countryside, and was of humble and plebeian origins. They had not been prominent in Roman public life. According to some scholars, including Victor Gardthausen, R. E. A. Palmer and David Ridgway, Agrippa's family was originally from Pisa in Etruria.

Agrippa was about the same age as Octavian (the future emperor Augustus), and the two were educated together and became close friends. Despite Agrippa's association with the family of Julius Caesar, his elder brother chose another side in the civil wars of the 40s BC, fighting under Cato against Caesar in Africa. When Cato's forces were defeated, Agrippa's brother was taken prisoner but freed after Octavian interceded on his behalf.

It is not known whether Agrippa fought against his brother in Africa, but he probably served in Caesar's campaign of 46 to 45 BC against Gnaeus Pompeius, which culminated in the Battle of Munda. Caesar regarded him highly enough to send him with Octavius in 45 BC to study in Apollonia (on the Illyrian coast) with the Macedonian legions, while Caesar consolidated his power in Rome. In the fourth month of their stay in Apollonia the news of Julius Caesar's assassination in March 44 BC reached them. Agrippa and another friend, Quintus Salvidienus Rufus, advised Octavius to march on Rome with the troops from Macedonia, but Octavius decided to sail to Italy with a small retinue. After his arrival, he learned that Caesar had adopted him as his legal heir. Octavius at this time took Caesar's name, but modern historians refer to him as "Octavian" during this period.

After Octavian's return to Rome, he and his supporters realised they needed the support of legions. Agrippa helped Octavian to levy troops in Campania. Once Octavian had his legions, he made a pact with Mark Antony and Lepidus, legally established in 43 BC as the Second Triumvirate. Octavian and his consular colleague Quintus Pedius arranged for Caesar's assassins to be prosecuted in their absence, and Agrippa was entrusted with the case against Gaius Cassius Longinus. It may have been in the same year that Agrippa began his political career, holding the position of Tribune of the Plebs, which granted him entry to the Senate.
In 42 BC, Agrippa probably fought alongside Octavian and Antony in the Battle of Philippi. After their return to Rome, he played a major role in Octavian's war against Lucius Antonius and Fulvia, respectively the brother and wife of Mark Antony, which began in 41 BC and ended in the capture of Perusia in 40 BC. However, Salvidienus remained Octavian's main general at this time. After the Perusine war, Octavian departed for Gaul, leaving Agrippa as urban praetor in Rome with instructions to defend Italy against Sextus Pompeius, an opponent of the Triumvirate who was now occupying Sicily. In July 40, while Agrippa was occupied with the Ludi Apollinares that were the praetor's responsibility, Sextus began a raid in southern Italy. Agrippa advanced on him, forcing him to withdraw. However, the Triumvirate proved unstable, and in August 40 both Sextus and Antony invaded Italy (but not in an organized alliance). Agrippa's success in retaking Sipontum from Antony helped bring an end to the conflict. Agrippa was among the intermediaries through whom Antony and Octavian agreed once more upon peace. During the discussions Octavian learned that Salvidienus had offered to betray him to Antony, with the result that Salvidienus was prosecuted and either executed or committed suicide. Agrippa was now Octavian's leading general.
In 39 or 38 BC, Octavian appointed Agrippa governor of Transalpine Gaul, where in 38 BC he put down a rising of the Aquitanians. He also fought the Germanic tribes, becoming the next Roman general to cross the Rhine after Julius Caesar. He was summoned back to Rome by Octavian to assume the consulship for 37 BC. He was well below the usual minimum age of 43, but Octavian had suffered a humiliating naval defeat against Sextus Pompey and needed his friend to oversee the preparations for further warfare. Agrippa refused the offer of a triumph for his exploits in Gaul – on the grounds, says Dio, that he thought it improper to celebrate during a time of trouble for Octavian. Since Sextus Pompeius had command of the sea on the coasts of Italy, Agrippa's first care was to provide a safe harbour for Octavian's ships. He accomplished this by cutting through the strips of land which separated the Lacus Lucrinus from the sea, thus forming an outer harbour, while joining the lake Avernus to the Lucrinus to serve as an inner harbor. The new harbor-complex was named Portus Julius in Octavian's honour. Agrippa was also responsible for technological improvements, including larger ships and an improved form of grappling hook. About this time, he married Caecilia Pomponia Attica, daughter of Cicero's friend Titus Pomponius Atticus.

In 36 BC, Octavian and Agrippa set sail against Sextus. The fleet was badly damaged by storms and had to withdraw; Agrippa was left in charge of the second attempt. Thanks to superior technology and training, Agrippa and his men won decisive victories at Mylae and Naulochus, destroying all but seventeen of Sextus' ships and compelling most of his forces to surrender. Octavian, with his power increased, forced the triumvir Lepidus into retirement and entered Rome in triumph. Agrippa received the unprecedented honour of a naval crown decorated with the beaks of ships; as Dio remarks, this was "a decoration given to nobody before or since".

Agrippa participated in smaller military campaigns in 35 and 34 BC, but by the autumn of 34 he had returned to Rome. He rapidly set out on a campaign of public repairs and improvements, including renovation of the aqueduct known as the Aqua Marcia and an extension of its pipes to cover more of the city. He became the first water commissioner of Rome in 33 BC. Through his actions after being elected in 33 BC as one of the aediles (officials responsible for Rome's buildings and festivals), the streets were repaired and the sewers were cleaned out, while lavish public spectacles were put on. Agrippa signalled his tenure of office by effecting great improvements in the city of Rome, restoring and building aqueducts, enlarging and cleansing the Cloaca Maxima, constructing baths and porticos, and laying out gardens. He also gave a stimulus to the public exhibition of works of art. It was unusual for an ex-consul to hold the lower-ranking position of aedile, but Agrippa's success bore out this break with tradition. As emperor, Augustus would later boast that "he had found the city of brick but left it of marble", thanks in part to the great services provided by Agrippa under his reign.

Agrippa was again called away to take command of the fleet when the war with Antony and Cleopatra broke out. He captured the strategically important city of Methone at the southwest of the Peloponnese, then sailed north, raiding the Greek coast and capturing Corcyra (modern Corfu). Octavian then brought his forces to Corcyra, occupying it as a naval base. Antony drew up his ships and troops at Actium, where Octavian moved to meet him. Agrippa meanwhile defeated Antony's supporter Quintus Nasidius in a naval battle at Patrae. Dio relates that as Agrippa moved to join Octavian near Actium, he encountered Gaius Sosius, one of Antony's lieutenants, who was making a surprise attack on the squadron of Lucius Tarius, a supporter of Octavian. Agrippa's unexpected arrival turned the battle around.

As the decisive battle approached, according to Dio, Octavian received intelligence that Antony and Cleopatra planned to break past his naval blockade and escape. At first he wished to allow the flagships past, arguing that he could overtake them with his lighter vessels and that the other opposing ships would surrender when they saw their leaders' cowardice. Agrippa objected, saying that Antony's ships, although larger, could outrun Octavian's if they hoisted sails, and that Octavian ought to fight now because Antony's fleet had just been struck by storms. Octavian followed his friend's advice.

On September 2, 31 BC, the Battle of Actium was fought. Octavian's victory, which gave him the mastery of Rome and the empire, was mainly due to Agrippa. Octavian then bestowed upon him the hand of his niece Claudia Marcella Major in 28 BC. He also served a second consulship with Octavian the same year. In 27 BC, Agrippa held a third consulship with Octavian, and in that year, the senate also bestowed upon Octavian the imperial title of Augustus.

In commemoration of the Battle of Actium, Agrippa built and dedicated the building that served as the Roman Pantheon before its destruction in AD 80. Emperor Hadrian used Agrippa's design to build his own Pantheon, which survives in Rome. The inscription of the later building, which was built around 125, preserves the text of the inscription from Agrippa's building during his third consulship. The years following his third consulship, Agrippa spent in Gaul, reforming the provincial administration and taxation system, along with building an effective road system and aqueducts.

Agrippa's friendship with Augustus seems to have been clouded by the jealousy of Augustus' nephew and son-in-law Marcus Claudius Marcellus, which was probably instigated by the intrigues of Livia, the third wife of Augustus, who feared Agrippa's influence over her husband. Traditionally it is said the result of such jealousy was that Agrippa left Rome, ostensibly to take over the governorship of eastern provinces – a sort of honourable exile, but he only sent his legate to Syria, while he himself remained at Lesbos and governed by proxy, though he may have been on a secret mission to negotiate with the Parthians about the return of the Roman legions' standards which they held. On the death of Marcellus, which took place within a year of his exile, he was recalled to Rome by Augustus, who found he could not dispense with his services. However, if one places the events in the context of the crisis of 23 BC it seems unlikely that, when facing significant opposition and about to make a major political climb down, the emperor Augustus would place a man in exile in charge of the largest body of Roman troops. What is far more likely is that Agrippa's 'exile' was actually the careful political positioning of a loyal lieutenant in command of a significant army as a backup plan in case the settlement plans of 23 BC failed and Augustus needed military support. Moreover, after 23 BC as part of what became known as Augustus' "Second Constitutional Settlement", Agrippa's constitutional powers were greatly increased to provide the Principate of Augustus with greater constitutional stability by providing for a political heir or replacement for Augustus if he were to succumb to his habitual ill health or was assassinated. In the course of the year, proconsular imperium, similar to Augustus' power, was conferred upon Agrippa for five years. The exact nature of the grant is uncertain but it probably covered Augustus' imperial provinces, east and west, perhaps lacking authority over the provinces of the Senate. That was to come later, as was the jealously guarded tribunicia potestas, or powers of a tribune of the plebeians. These great powers of state are not usually heaped upon a former exile.
It is said that Maecenas advised Augustus to attach Agrippa still more closely to him by making him his son-in-law. Accordingly, by 21 BC, he induced Agrippa to divorce Marcella and marry his daughter, Julia the Elder—the widow of Marcellus, equally celebrated for her beauty, abilities, and her shameless extravagance. In 19 BC, Agrippa was employed in putting down a rising of the Cantabrians in Hispania (Cantabrian Wars).

In 18 BC, Agrippa's powers were even further increased to almost match those of Augustus. That year his proconsular imperium was augmented to cover the provinces of the Senate. More than that, he was finally granted tribunicia potestas, or powers of a tribune of the plebeians. As was the case with Augustus, Agrippa’s grant of tribunician powers was conferred without his having to actually hold that office. These powers were considerable, giving him veto power over the acts of the Senate or other magistracies, including those of other tribunes, and the power to present laws for approval by the People. Just as important, a tribune’s person was sacred, meaning that any person who harmfully touched them or impeded their actions, including political acts, could lawfully be killed. After the grant of these powers Agrippa was, on paper, almost as powerful as Augustus was. However, there was no doubt that Augustus was the man in charge.

Agrippa was appointed governor of the eastern provinces a second time in 17 BC, where his just and prudent administration won him the respect and good-will of the provincials, especially from the Jewish population. Agrippa also restored effective Roman control over the Cimmerian Chersonnese (Crimean Peninsula) during his governorship.

Agrippa’s last public service was his beginning of the conquest of the upper Danube River region, which would become the Roman province of Pannonia in 13 BC. He died at Campania in 12 BC at the age of 51. His posthumous son, Marcus Vipsanius Agrippa Postumus, was named in his honor. Augustus honoured his memory by a magnificent funeral and spent over a month in mourning. Augustus personally oversaw all of Agrippa's children’s educations. Although Agrippa had built a tomb for himself, Augustus had Agrippa's remains placed in Augustus' own mausoleum.

Agrippa was also known as a writer, especially on the subject of geography. Under his supervision, Julius Caesar's dream of having a complete survey of the Empire made was carried out. Agrippa constructed a circular chart, which was later engraved on marble by Augustus, and afterwards placed in the colonnade built by his sister Polla. Amongst his writings, an autobiography, now lost, is referenced.

Agrippa established a standard for Roman foot (Agrippa's own) in 29 BC, and thus a definition of a pace as 5 feet. An imperial Roman mile denotes 5,000 Roman feet.

The term Via Agrippa is used for any part of the network of roadways in Gaul built by Agrippa. Some of these still exist as paths or even as highways.

Agrippa had several children through his three marriages:

Through his numerous children, Agrippa would become ancestor to many subsequent members of the Julio-Claudian dynasty, whose position he helped to attain, as well as many other distinguished Romans.


There have been some attempts to assign further descendants to a number of the aforementioned figures, including two lines of Asinii descended from Gaius Asinius Pollio and Marcus Asinius Agrippa respectively. A daughter (and further descendants) named Rubellia Bassa to Julia, who may have been a daughter of Gaius Rubellius Blandus by an earlier marriage. And, finally, a series of descendants from Junia Lepida and her husband, Gaius Cassius Longinus. However, all of these lines of descent are extremely hypothetical and lack any evidence to support a connection to the descendants of Agrippa.
Agrippa is a character in William Shakespeare's play "Antony and Cleopatra".

A fictional version of Agrippa in his later life played a prominent role in the 1976 BBC Television series "I, Claudius". Agrippa was portrayed as a much older man, though he would have only been 39 years old at the time of the first episode (24/23 BC). He was played by John Paul.

Agrippa is the main character in Paul Naschy's 1980 film "Los cántabros", played by Naschy himself. It is a highly fictionalized version of the Cantabrian Wars in which Agrippa is depicted as the lover of the sister of Cantabrian leader Corocotta.

Agrippa appears in several film versions of the life of Cleopatra. He is normally portrayed as an old man rather than a young one. Among the people to portray him are Philip Locke, Alan Rowe and Andrew Keir.

Agrippa is also one of the principal characters in the British/Italian joint project "" (2003) featuring flashbacks between Augustus and Julia about Agrippa, which shows him in his youth on serving in Caesar's army up until his victory at Actium and the defeat of Cleopatra. He is portrayed by Ken Duken. In the 2005 series "Empire" the young Agrippa (played by Christopher Egan) becomes Octavian's sidekick after saving him from an attempted poisoning.

Marcus Agrippa, a highly fictional character based on Marcus Vipsanius Agrippa's early life, is part of the BBC-HBO-RAI television series "Rome". He is played by Allen Leech. He describes himself as the grandson of a slave. The series creates a romantic relationship between Agrippa and Octavian's sister Octavia Minor, for which there is no historical evidence.

Agrippa is mentioned by name in book VIII of Virgil's "The Aeneid", where Aeneas sees an image of Agrippa leading ships in the Battle of Actium on the shield forged for him by Vulcan and given to him by his mother, Venus.

Agrippa is a main character in the early part of Robert Graves' novel "I, Claudius". He is a main character in the later two novels of Colleen McCullough's Masters of Rome series. He is a featured character of prominence and importance in the historical fiction novel "Cleopatra's Daughter" by Michelle Moran. He also features prominently in John Edward Williams' historical novel "Augustus". In the backstory of "Gunpowder Empire", the first volume in Harry Turtledove's Crosstime Traffic alternate history series, Agrippa lived until AD 26, conquering all of Germania for the Empire and becoming the second Emperor when Augustus died in AD 14.





</doc>
<doc id="19757" url="https://en.wikipedia.org/wiki?curid=19757" title="Mariotto Albertinelli">
Mariotto Albertinelli

Mariotto di Bindo di Biagio Albertinelli (13 October 1474 – 5 November 1515) was an Italian Renaissance painter active in Florence. He was a close friend and collaborator of Fra Bartolomeo.

Some of his works have been described as "archaic" or "conservative"; others are considered exemplary of the grandiose classicism of High Renaissance art.

Albertinelli was born in Florence to a local gold beater. He was a pupil of Cosimo Rosselli, in whose workshop he met Baccio della Porta, later known as Fra Bartolomeo. The two were so close that in 1494 they formed a "compagnia," or partnership, in which they operating a joint studio and divided the profits of anything produced within it. The partnership lasted until 1500, when Baccio joined the Dominican order and spent two years in cloister. 
At the beginning of his career Albertinelli was placed on retainer by Alfonsina Orsini, the wife of Piero II de’ Medici and mother of Lorenzo II de' Medici. His works from this period all small-scale works executed in a minute, delicate technique and a style derived from the works of Rosselli's main pupil Piero di Cosimo as well as Lorenzo di Credi and Perugino. Like many Florentine painters, Albertinelli was also receptive to the influence of contemporary Flemish painting. 

Albertinelli's earliest works include a small triptych of the "Madonna and Child with Saints Catherine and Barbara" (1500) at the Museo Poldi Pezzoli, Milan, and another triptych of the Madonna and Child with Saints, Angels and Various Religious Scenes at the Musée des Beaux-Arts in Chartres. The several panels with "Scenes from Genesis," at the Courtauld Institute in London, Strossmayer Gallery in Zagreb, Accademia Carrara in Bergamo and Harvard Art Museums in Cambridge, probably also date from this period.

In 1503 Albertinelli signed and dated his best-known work, an altarpiece for the chapel of Sant'Elisabetta della congrega dei Preti in San Michele alle Trombe, Florence (now in the Uffizi). The central panel of this work depicts the Visitation and the predella the Annunciation, Nativity and Circumcision of Christ. The pyramidal composition, classical background architecture and pronounced contrasts of light and dark make the painting a quintessential example of High Renaissance art.
Also in 1503 Albertinelli entered a new partnership with Giuliano Bugiardini, which lasted until 1509, when Albertinelli resumed his partnership with Fra Bartolomeo. At this point Fra Bartolomeo and Albertinelli practiced similar styles and occasionally collaborated. For example, the "Kress Tondo", now in the Columbia Museum of Art, was previously attributed to Fra Bartolomeo but is now thought to be the work of Albertinelli using Fra Bartolomeo's cartoon, or scaled-preparatory drawing. The "Annunciation" at the Musée d'Art et d'Histoire in Geneva is signed and dated (1511) by both artists. The partnership was terminated in January 1513, as reported in a document stipulating the division of the workshop's properties. 

According to Giorgio Vasari's "Life" of Albertinelli, the painter lived as a libertine and was fond of good living and women. Albertinelli reportedly had experienced financial problems and operated a tavern to supplement his income as a painter. At the end of his life he was unable to repay some of his debts, including one to Raphael. His wife Antonia, whom he married in 1506, repaid some of his loans. Among his many students were Franciabigio, Jacopo da Pontormo and Innocenzo da Imola.


</doc>
<doc id="19758" url="https://en.wikipedia.org/wiki?curid=19758" title="Beijing cuisine">
Beijing cuisine

Beijing cuisine, also known as Jing cuisine, Mandarin cuisine and Peking cuisine, and formerly as Beiping cuisine, is the local cuisine of Beijing, the national capital of China.

As Beijing has been the capital of China for centuries, its cuisine is influenced by culinary traditions from all over China, but the style that has the greatest influence on Beijing cuisine is that of the eastern coastal province of Shandong. Beijing cuisine has itself, in turn, also greatly influenced other Chinese cuisines, particularly the cuisine of Liaoning, the Chinese imperial cuisine, and the Chinese aristocrat cuisine.

Another tradition that influenced Beijing cuisine (as well as influenced by the latter itself) is the Chinese imperial cuisine that originated from the "Emperor's Kitchen" (), which referred to the cooking facilities inside the Forbidden City, where thousands of cooks from different parts of China showed their best culinary skills to please the imperial family and officials. Therefore, it is sometimes difficult to determine the actual origin of a dish as the term "Mandarin" is generalised and refers not only to Beijing, but other provinces as well. However, some generalisation of Beijing cuisine can be characterised as follows: Foods that originated in Beijing are often snacks rather than main courses, and they are typically sold by small shops or street vendors. There is emphasis on dark soy paste, sesame paste, sesame oil and scallions, and fermented tofu is often served as a condiment. In terms of cooking techniques, methods relating to different ways of frying are often used. There is less emphasis on rice as an accompaniment as compared to many other regions in China, as local rice production in Beijing is limited by the relatively dry climate.

Many dishes in Beijing cuisine that are served as main courses are derived from a variety of Chinese Halal foods, particularly lamb and beef dishes, as well as from Huaiyang cuisine.

Huaiyang cuisine has been praised since ancient times in China, and it was a general practice for an official travelling to Beijing to take up a new post to bring along with him a chef specialising in Huaiyang cuisine. When these officials had completed their terms in the capital and returned to their native provinces, most of the chefs they brought along often remained in Beijing. They opened their own restaurants or were hired by wealthy locals. The imperial clan of the Ming dynasty, the House of Zhu, who had ancestry from Jiangsu Province, also contributed greatly in introducing Huaiyang cuisine to Beijing when the capital was moved from Nanjing to Beijing in the 15th century, because the imperial kitchen was mainly Huaiyang style. The element of traditional Beijing culinary and gastronomical cultures of enjoying artistic performances such as Beijing opera while dining directly developed from the similar practice in the culture of Jiangsu and Huaiyang cuisines.

Chinese Islamic cuisine is another important component of Beijing cuisine, and was first prominently introduced when Beijing became the capital of the Yuan dynasty. However, the most significant contribution to the formation of Beijing cuisine came from Shandong cuisine, as most chefs from Shandong Province came to Beijing en masse during the Qing dynasty. Unlike the earlier two cuisines, which were brought by the ruling class such as nobles, aristocrats and bureaucrats, and then spread to the general populace, the introduction of Shandong cuisine begun with serving the general populace, with much wider market segment, from wealthy merchants to the working class.

The Qing dynasty was a major period in the formation of Beijing cuisine. Before the Boxer Rebellion, the foodservice establishments in Beijing were strictly stratified by the foodservice guild. Each category of the establishment was specifically based on its ability to provide for a particular segment of the market. The top ranking foodservice establishments served nobles, aristocrats, and wealthy merchants and landlords, while lower ranking foodservice establishments served the populace of lower financial and social status. It was during this period when Beijing cuisine gained fame and became recognised by the Chinese culinary society, and the stratification of the foodservice was one of its most obvious characteristics as part of its culinary and gastronomic cultures during this first peak of its formation.

The official stratification was an integral part of the local culture of Beijing and it was not finally abolished officially after the end of the Qing dynasty, which resulted in the second peak in the formation of Beijing cuisine. Meals previously offered to nobles and aristocrats were made available to anyone who could afford them instead of being restricted only to the upper class. As chefs freely switched between jobs offered by different foodservice establishments, they brought their skills that further enriched and developed Beijing cuisine. Though the stratification of food services in Beijing was no longer effected by imperial laws, the structure more or less remained despite continuous weakening due to the financial background of the local clientele. The different classes are listed in the following subsections.

Foodservice establishments with names ending with the Chinese character "zhuang" (), or "zhuang zihao" (), were the top-ranking foodservice establishments, not only in providing foods, but entertainment as well. The form of entertainment provided was usually Beijing opera, and foodservice establishments of this class always had long-term contracts with a Beijing opera troupe to perform onsite. Moreover, foodservice establishments of this class would always have long-term contracts with famous performers, such as national-treasure-class performers, to perform onsite, though not on a daily basis. Foodservice establishments of this category did not accept any different customers on a walk-in basis, but instead, only accepted customers who came as a group and ordered banquets by appointment, and the banquets provided by foodservice establishments of this category often included most, if not all tables, at the site. The bulk of the business of foodservice of this category, however, was catering at customers' homes or other locations, and such catering was often for birthdays, marriages, funerals, promotions and other important celebrations and festivals. When catering, these foodservice establishments not only provided what was on the menu, but fulfilled customers' requests.

Foodservice establishments categorised as "leng zhuangzi" () lacked any rooms to host banquets, and thus their business was purely catering.

Foodservice establishments with names ending with the Chinese character "tang" (), or "tang zihao" (), are similar to foodservice establishments with names ending with the Chinese character "zhuang", but the business of these second-class foodservice establishments were generally evenly divided among onsite banquet hosting and catering (at customers' homes). Foodservice establishments of this class would also have long-term contracts with Beijing opera troupes to perform onsite, but they did not have long-term contracts with famous performers, such as national-treasure-class performers, to perform onsite on regular basis; however these top performers would still perform at foodservice establishments of this category occasionally. In terms of catering at the customers' sites, foodservice establishments of this category often only provided dishes strictly according to their menu, and would not provide any dishes that were not on the menu.

Foodservice establishments with names ending with the Chinese character "ting" (), or "ting zihao" () are foodservice establishments which had more business in onsite banquet hosting than catering at customers' homes. For onsite banquet hosting, entertainment was still provided, but foodservice establishments of this category did not have long-term contracts with Beijing opera troupes, so that performers varied from time to time, and top performers usually did not perform here or at any lower-ranking foodservice establishments. For catering, different foodservice establishments of this category were incapable of handling significant catering on their own, but generally had to combine resources with other foodservice establishments of the same ranking (or lower) to do the job.

Foodservice establishments with names ending with the Chinese character "yuan" (), or "yuan zihao" () did nearly all their business in hosting banquets onsite. Entertainment was not provided on a regular basis, but there were stages built onsite for Beijing opera performers. Instead of being hired by the foodservice establishments like in the previous three categories, performers at foodservice establishments of this category were usually contractors who paid the foodservice establishment to perform and split the earnings according to a certain percentage. Occasionally, foodservice establishments of this category would be called upon to help cater at customers' homes, and like foodservice establishments with names ending with the Chinese character "ting", they could not do the job on their own but had to work with others, never taking the lead as foodservice establishments with names ending with the Chinese character "ting" could.

Foodservice establishments with names ending with the Chinese character "lou" (), or "lou zihao" () did the bulk of their business hosting banquets onsite by appointment. In addition, a smaller portion of the business was in serving different customers onsite on a walk-in basis. Occasionally, when catering at customers' homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for.

Foodservice establishments with names ending with the Chinese character "ju" (), or "ju zihao" () generally divided their business evenly into two areas: serving different customers onsite on a walk-in basis, and hosting banquets by appointment for customers who came as one group. Occasionally, when catering at the customers' homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for, just like foodservice establishments with names ending with the Chinese character "lou". However, unlike those establishments, which always cooked their specialty dishes on location, foodservice establishment of this category would either cook on location or simply bring the already-cooked food to the location.

Foodservice establishments with names ending with the Chinese character "zhai" (), or "zhai zihao" () were mainly in the business of serving different customers onsite on a walk-in basis, but a small portion of their income did come from hosting banquets by appointment for customers who came as one group. Just like foodservice establishments with names ending with the Chinese character "ju", when catering at customers’ homes, foodservice establishments of this category would also only provide the few specialty dishes they are famous for, but they would mostly bring the already-cooked dishes to the location, and would only cook on location occasionally.

Foodservice establishments with names ending with the Chinese character "fang" (), or "fang zihao" (). Foodservice establishments of this category generally did not offer the service of hosting banquets made by appointment for customers who came as one group, but instead, often only offered to serve different customers onsite on a walk-in basis. Foodservice establishments of this category or lower would not be called upon to perform catering at the customers' homes for special events.

Foodservice establishments with names ending with the Chinese character "guan" (), or "guan zihao" (). Foodservice establishments of this category mainly served different customers onsite on a walk-in basis, and in addition, a portion of the income would be earned from selling to-goes.

Foodservice establishments with names ending with the Chinese character "dian" (), or "dian zihao" (). Foodservice establishments of this category had their own place, like all previous categories, but serving different customers to dine onsite on a walk-in basis only provided half of the overall income, while the other half came from selling to-goes.

Foodservice establishments with name ending with the Chinese character "pu" (), or "pu zihao" (). Foodservice establishments of this category ranked next to the last, and they were often named after the owners' last names. Foodservice establishments of this category had fixed spots of business for having their own places, but not as large as those belonging to the category of "dian", and thus did not have tables, but only seats for customers. As a result, the bulk of the income of foodservice establishments of this category was from selling to-goes, while income earned from customers dining onsite only provided a small portion of the overall income.

Foodservice establishments with names ending with the Chinese character "tan" (), or "tan zihao" (). The lowest ranking foodservice establishments without any tables, and selling to-goes was the only form of business. In addition to name the food stand after the owners' last name or the food sold, these food stands were also often named after the owners' nicknames.


Numerous traditional restaurants in Beijing are credited with great contributions in the formation of Beijing cuisine, but many of them have gone out of business. However, some of them managed to survive until today, and some of them are:



</doc>
<doc id="19760" url="https://en.wikipedia.org/wiki?curid=19760" title="Manichaeism">
Manichaeism

Manichæism (;
in New Persian "Ãyīnⁱ Mānī"; ) was a major religion founded in the 3rd century AD by the Persian prophet Mani () in the Sasanian Empire.

Manichaeism taught an elaborate dualistic cosmology describing the struggle between a good, spiritual world of light, and an evil, material world of darkness. Through an ongoing process that takes place in human history, light is gradually removed from the world of matter and returned to the world of light, whence it came. Its beliefs were based on local Mesopotamian religious movements and Gnosticism. It revered Mani as the final prophet after Zoroaster, Gautama Buddha, and Jesus.

Manichaeism was quickly successful and spread far through the Aramaic-speaking regions. It thrived between the third and seventh centuries, and at its height was one of the most widespread religions in the world. Manichaean churches and scriptures existed as far east as China and as far west as the Roman Empire. It was briefly the main rival to Christianity before the spread of Islam in the competition to replace classical paganism. Manichaeism survived longer in the east than in the west, and it appears to have finally faded away after the 14th century in south China, contemporary to the decline of the Church of the East in Ming China. While most of Manichaeism's original writings have been lost, numerous translations and fragmentary texts have survived.

An adherent of Manichaeism was called a "Manichaean" or "Manichean", or "Manichee", especially in older sources.

Mani was an Iranian born in 216 in or near Seleucia-Ctesiphon (now al-Mada'in) in the Parthian Empire. According to the "Cologne Mani-Codex", Mani's parents were members of the Jewish Christian Gnostic sect known as the Elcesaites.

Mani composed seven works, six of which were written in the Syriac language, a late variety of Aramaic. The seventh, the "Shabuhragan", was written by Mani in Middle Persian and presented by him to the Sasanian emperor, Shapur I. Although there is no proof Shapur I was a Manichaean, he tolerated the spread of Manichaeism and refrained from persecuting it within his empire's boundaries.

According to one tradition, it was Mani himself who invented the unique version of the Syriac script known as the Manichaean alphabet, which was used in all of the Manichaean works written within the Sasanian Empire, whether they were in Syriac or Middle Persian, and also for most of the works written within the Uyghur Khaganate. The primary language of Babylon (and the administrative and cultural language of the Sassanid Empire) at that time was Eastern Middle Aramaic, which included three main dialects: Jewish Babylonian Aramaic (the language of the Babylonian Talmud), Mandaean (the language of Mandaeism), and Syriac, which was the language of Mani, as well as of the Syriac Christians.
While Manichaeism was spreading, existing religions such as Zoroastrianism were still popular and Christianity was gaining social and political influence. Although having fewer adherents, Manichaeism won the support of many high-ranking political figures. With the assistance of the Sasanian Empire, Mani began missionary expeditions. After failing to win the favour of the next generation of Persian royalty, and incurring the disapproval of the Zoroastrian clergy, Mani is reported to have died in prison awaiting execution by the Persian Emperor Bahram I. The date of his death is estimated at 276–277.

Mani believed that the teachings of Gautama Buddha, Zoroaster, and Jesus were incomplete, and that his revelations were for the entire world, calling his teachings the "Religion of Light". Manichaean writings indicate that Mani received revelations when he was 12 and again when he was 24, and over this time period he grew dissatisfied with the Elcesaite sect he was born into. Mani began preaching at an early age and was possibly influenced by contemporary Babylonian-Aramaic movements such as Mandaeism, and Aramaic translations of Jewish apocalyptic writings similar to those found at Qumran (such as the book of Enoch literature), and by the Syriac dualist-gnostic writer Bardaisan (who lived a generation before Mani). With the discovery of the Mani-Codex, it also became clear that he was raised in a Jewish-Christian baptism sect, the Elcesaites, and was influenced by their writings, as well. According to biographies preserved by Ibn al-Nadim and the Persian polymath al-Biruni, he received a revelation as a youth from a spirit, whom he would later call his Twin ( , from which is also derived the name of the Thomas the Apostle, the "twin"), his "Syzygos" ( "spouse, partner", in the "Cologne Mani-Codex"), his Double, his Protective Angel or Divine Self. It taught him truths that he developed into a religion. His divine Twin or true Self brought Mani to self-realization. He claimed to be the "Paraclete of the Truth", as promised by Jesus in the New Testament.
Manichaeism's views on Jesus are described by historians:
Augustine also noted that Mani declared himself to be an "apostle of Jesus Christ". Manichaean tradition is also noted to have claimed that Mani was the reincarnation of different religious figures such as Buddha, Krishna, Zoroaster, and Jesus.

Academics also note that since much of what is known about Manichaeism comes from later 10th- and 11th-century Muslim historians like Al-Biruni and especially ibn al-Nadim (and his "Fihrist"), "Islamic authors ascribed to Mani the claim to be the Seal of the Prophets." In reality, for Mani the expression "seal of prophecy" refers to his disciples, who testify for the veracity of his message, as a seal does.
Another source of Mani's scriptures was original Aramaic writings relating to the "Book of Enoch" literature (see the Book of Enoch and the Second Book of Enoch), as well as an otherwise unknown section of the Book of Enoch called "The Book of Giants". This book was quoted directly, and expanded on by Mani, becoming one of the original six Syriac writings of the Manichaean Church. Besides brief references by non-Manichaean authors through the centuries, no original sources of "The Book of Giants" (which is actually part six of the Book of Enoch) were available until the 20th century.

Scattered fragments of both the original Aramaic "Book of Giants" (which were analyzed and published by Józef Milik in 1976) and of the Manichaean version of the same name (analyzed and published by Walter Bruno Henning in 1943) were found with the discovery in the twentieth century of the Dead Sea Scrolls in the Judaean Desert and the Manichaean writings of the Uyghur Manichaean kingdom in Turpan. Henning wrote in his analysis of them:

By comparing the cosmology in the Book of Enoch literature and the Book of Giants, alongside the description of the Manichaean myth, scholars have observed that the Manichaean cosmology can be described as being based, in part, on the description of the cosmology developed in detail in the Book of Enoch literature. This literature describes the being that the prophets saw in their ascent to heaven, as a king who sits on a throne at the highest of the heavens. In the Manichaean description, this being, the "Great King of Honor", becomes a deity who guards the entrance to the world of light, placed at the seventh of ten heavens. In the Aramaic Book of Enoch, in the Qumran writings in general, and in the original Syriac section of Manichaean scriptures quoted by Theodore bar Konai, he is called "malka raba de-ikara" (the Great King of Honor).

Mani was also influenced by writings of the Assyrian gnostic Bardaisan (154–222), who, like Mani, wrote in Syriac, and presented a dualistic interpretation of the world in terms of light and darkness, in combination with elements from Christianity.
Noting Mani's travels to the Kushan Empire (several religious paintings in Bamyan are attributed to him) at the beginning of his proselytizing career, Richard Foltz postulates Buddhist influences in Manichaeism:

The Kushan monk Lokakṣema began translating Pure Land Buddhist texts into Chinese in the century prior to Mani arriving there, and the Chinese texts of Manichaeism are full of uniquely Buddhist terms taken directly from these Chinese Pure Land scriptures, including the term "pure land" (淨土 Jìngtǔ) itself. However, the central object of veneration in Pure Land Buddhism, Amitābha, the Buddha of Infinite Light, does not appear in Chinese Manichaeism, and seems to have been replaced by another deity.

Manichaeism spread with extraordinary speed through both the East and West. It reached Rome through the apostle Psattiq by 280, who was also in Egypt in 244 and 251. It was flourishing in the Faiyum in 290.

Manichaean monasteries existed in Rome in 312 during the time of Pope Miltiades.

In 291, persecution arose in the Sasanian Empire with the murder of the apostle Sisin by Emperor Bahram II, and the slaughter of many Manichaeans. In 296, Roman Emperor Diocletian decreed against the Manichaeans: "We order that their organizers and leaders be subject to the final penalties and condemned to the fire with their abominable scriptures." This resulted in martyrdom for many in Egypt and North Africa (see Diocletian Persecution). By 354, Hilary of Poitiers wrote that Manichaeism was a significant force in Roman Gaul. In 381, Christians requested Theodosius I to strip Manichaeans of their civil rights. Starting in 382, the emperor issued a series of edicts to suppress Manichaeism and punish its followers.
Augustine of Hippo (354–430) converted to Christianity from Manichaeism in the year 387. This was shortly after the Roman emperor Theodosius I had issued a decree of death for all Manichaean monks in 382 and shortly before he declared Christianity to be the only legitimate religion for the Roman Empire in 391. Due to the heavy persecution, the religion almost disappeared from western Europe in the fifth century and from the eastern portion of the empire in the sixth century. According to his "Confessions", after nine or ten years of adhering to the Manichaean faith as a member of the group of "hearers", Augustine became a Christian and a potent adversary of Manichaeism (which he expressed in writing against his Manichaean opponent Faustus of Mileve), seeing their beliefs that knowledge was the key to salvation as too passive and not able to effect any change in one's life.

Some modern scholars have suggested that Manichaean ways of thinking influenced the development of some of Augustine's ideas, such as the nature of good and evil, the idea of hell, the separation of groups into elect, hearers, and sinners, and the hostility to the flesh and sexual activity, and his dualistic theology. These influences of Manichaeism in Augustine's Christian thinking may well have been part of the conflict between Augustine and Pelagius, a British monk whose theology, being less influenced by the Latin Church, was non-dualistic, and one that saw the created order, and mankind in particular, as having a Divine core, rather than a 'darkness' at its core.
How Manichaeism might have influenced Christianity continues to be debated. Manichaeism could have influenced the Bogomils, Paulicians, and Cathars. However, these groups left few records, and the link between them and Manichaeans is tenuous. Regardless of its accuracy, the charge of Manichaeism was leveled at them by contemporary orthodox opponents, who often tried to make contemporary heresies conform to those combatted by the church fathers. Whether the dualism of the Paulicians, Bogomils, and Cathars and their belief that the world was created by a Satanic demiurge were due to influence from Manichaeism is impossible to determine. The Cathars apparently adopted the Manichaean principles of church organization. Priscillian and his followers may also have been influenced by Manichaeism. The Manichaeans preserved many apocryphal Christian works, such as the Acts of Thomas, that would otherwise have been lost.

Manichaeism maintained a sporadic and intermittent existence in the west (Mesopotamia, Africa, Spain, France, North Italy, the Balkans) for a thousand years, and flourished for a time in Persia and even further east in Northern India, Western China, and Tibet. While it had long been thought that Manichaeism arrived in China only at the end of the seventh century, a recent archaeological discovery demonstrated that it was already known there in the second half of the 6th century.
Some Sogdians in Central Asia believed in the religion. Uyghur khagan Boku Tekin (759–780) converted to the religion in 763 after a three-day discussion with its preachers, the Babylonian headquarters sent high rank clerics to Uyghur, and Manichaeism remained the state religion for about a century before the collapse of the Uyghur Khaganate in 840. In the east it spread along trade routes as far as Chang'an, the capital of Tang China. After the Tang Dynasty, some Manichaean groups participated in peasant movements. The religion was used by many rebel leaders to mobilise followers. In the Song and Yuan dynasties of China remnants of Manichaeism continued to leave a legacy contributing to sects such as the Red Turbans. During the Song Dynasty, the Manichaeans were derogatorily referred by the Chinese as "chicai simo" (meaning that they "abstain from meat and worship demons"). An account in "Fozu Tongji", an important historiography of Buddhism in China compiled by Buddhist scholars during 1258–1269, says that the Manichaeans worshipped the "white Buddha" and their leader wore a violet headgear, while the followers wore white costumes. Many Manichaeans took part in rebellions against the Song government and were eventually quelled. After that, all governments were suppressive against Manichaeism and its followers and the religion was banned by the Ming Dynasty in 1370.

Manichaeism spread to Tibet during the Tibetan Empire. There was likely a serious attempt to introduce the religion to the Tibetans as the text "Criteria of the Authentic Scriptures" (a text attributed to Tibetan Emperor Trisong Detsen) makes a great effort to attack Manichaeism by stating that Mani was a heretic who took ideas from all faiths and blended them together into a deviating and inauthentic form.

Manichaeans in Iran tried to assimilate their religion along with Islam in the Muslim caliphates. Relatively little is known about the religion during the first century of Islamic rule. During the early caliphates, Manichaeism attracted many followers. It had a significant appeal among the Muslim society, especially among the elites. Due to the appeal of its teachings, many Muslims adopted the ideas of its theology and some even became dualists. An apologia for Manichaeism ascribed to ibn al-Muqaffa' defended its phantasmagorical cosmogony and attacked the fideism of Islam and other monotheistic religions. The Manichaeans had sufficient structure to have a head of their community.

Under the eighth-century Abbasid Caliphate, Arabic "zindīq" and the adjectival term "zandaqa" could denote many different things, though it seems primarily (or at least initially) to have signified a follower of Manichaeism however its true meaning is not known. In the ninth century, it is reported that Caliph al-Ma'mun tolerated a community of Manichaeans.

During the early Abbasid period, the Manichaeans underwent persecution. The third Abbasid caliph, al-Mahdi, persecuted the Manichaeans, establishing an inquisition against dualists who if being found guilty of heresy refused to renounce their beliefs, were executed. Their persecution was finally ended in 780s by Harun al-Rashid. During the reign of the Caliph al-Muqtadir, many Manichaeans fled from Mesopotamia to Khorasan from fear of persecution and the base of the religion was later shifted to Samarkand.
Manichaeism claimed to present the complete version of teachings that were corrupted and misinterpreted by the followers of its predecessors Adam, Zoroaster, Buddha and Jesus. Accordingly, as it spread, it adapted new deities from other religions into forms it could use for its scriptures. Its original Aramaic texts already contained stories of Jesus. When they moved eastward and were translated into Iranian languages, the names of the Manichaean deities (or angels) were often transformed into the names of Zoroastrian yazatas. Thus "Abbā dəRabbūṯā" ("The Father of Greatness", the highest Manichaean deity of Light), in Middle Persian texts might either be translated literally as "pīd ī wuzurgīh", or substituted with the name of the deity "Zurwān". Similarly, the Manichaean primal figure "Nāšā Qaḏmāyā" "The Original Man" was rendered "Ohrmazd Bay", after the Zoroastrian god Ohrmazd. This process continued in Manichaeism's meeting with Chinese Buddhism, where, for example, the original Aramaic "qaryā" (the "call" from the World of Light to those seeking rescue from the World of Darkness), becomes identified in the Chinese scriptures with Guanyin ( or Avalokiteśvara in Sanskrit, literally, "watching/perceiving sounds [of the world]", the bodhisattva of Compassion).

Manichaeism was repressed by the Sasanian Empire. In 291, persecution arose in the Persian empire with the murder of the apostle Sisin by Bahram II, and the slaughter of many Manichaeans. In 296, the Roman emperor Diocletian decreed all the Manichaean leaders to be burnt alive along with the Manichaean scriptures and many Manichaeans in Europe and North Africa were killed. This policy of persecution was also followed by his successors. Theodosius I issued a decree of death for all Manichaean monks in 382 AD. The religion was vigorously attacked and persecuted by both the Christian Church and the Roman state, and the religion almost disappeared from western Europe in the fifth century and from the eastern portion of the empire in the sixth century.

In 732, Emperor Xuanzong of Tang banned any Chinese from converting to the religion, saying it was a heretic religion that was confusing people by claiming to be Buddhism. However, the foreigners who followed the religion were allowed to practice it without punishment. After the fall of the Uyghur Khaganate in 840, which was the chief patron of Manichaeism (which was also the state religion of the Khaganate) in China, all Manichaean temples in China except in the two capitals and Taiyuan were closed down and never reopened since these temples were viewed as a symbol of foreign arrogance by the Chinese (see Cao'an). Even those that were allowed to remain open did not for long. The Manichaean temples were attacked by Chinese people who burned the images and idols of these temples. Manichaean priests were ordered to wear hanfu instead of their traditional clothing, which was viewed as un-Chinese. In 843, Emperor Wuzong of Tang gave the order to kill all Manichaean clerics as part of his Great Anti-Buddhist Persecution, and over half died. They were made to look like Buddhists by the authorities, their heads were shaved, they were made to dress like Buddhist monks and then killed. Although the religion was mostly forbidden and its followers persecuted thereafter in China, it survived till the 14th century in the country. Under the Song dynasty, its followers were derogatorily referred to with the chengyu () "vegetarian demon-worshippers".

Many Manichaeans took part in rebellions against the Song dynasty. They were quelled by Song China and were suppressed and persecuted by all successive governments before the Mongol Yuan dynasty. In 1370, the religion was banned through an edict of the Ming dynasty, whose Hongwu Emperor had a personal dislike for the religion. Its core teaching influences many religious sects in China, including the White Lotus movement.

According to Wendy Doniger, Manichaeism may have continued to exist in the modern-East Turkestan region until the Mongol conquest in the 13th century.

Manicheans also suffered persecution for some time under the Abbasid Caliphate of Baghdad. In 780, the third Abbasid Caliph, al-Mahdi, started a campaign of inquisition against those who were "dualist heretics" or "Manichaeans" called the "zindīq". He appointed a "master of the heretics" ( "ṣāhib al-zanādiqa"), an official whose task was to pursue and investigate suspected dualists, who were then examined by the Caliph. Those found guilty who refused to abjure their beliefs were executed. This persecution continued under his successor, Caliph al-Hadi, and continued for some time during reign of Harun al-Rashid, who finally abolished it and ended it. During the reign of the 18th Abbassid Caliph al-Muqtadir, many Manichaeans fled from Mesopotamia to Khorasan from fear of persecution by him and about 500 of them assembled in Samarkand. The base of the religion was later shifted to this city, which became their new Patriarchate.

Manichaean pamphlets were still in circulation in Greek in 9th century Byzantine Constantinople, as the patriarch Photios summarizes and discusses one that he has read by Agapius in his "Bibliotheca".

During the Middle Ages, several movements emerged that were collectively described as "Manichaean" by the Catholic Church, and persecuted as Christian heresies through the establishment, in 1184, of the Inquisition. They included the Cathar churches of Western Europe. Other groups sometimes referred to as "neo-Manichaean" were the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria. An example of this usage can be found in the published edition of the Latin Cathar text, the "Liber de duobus principiis" ("Book of the Two Principles"), which was described as "Neo-Manichaean" by its publishers. As there is no presence of Manichaean mythology or church terminology in the writings of these groups, there has been some dispute among historians as to whether these groups were descendants of Manichaeism.

Some sites are preserved in Xinjiang and Fujian in China. The Cao'an temple is the only fully intact Manichaean building, though it later became associated with Buddhism. Several small groups claim to continue to practice this faith.

Mani's teaching dealt with the origin of evil, by addressing a theoretical part of the problem of evil by denying the omnipotence of God and postulating two opposite powers. Manichaean theology taught a dualistic view of good and evil. A key belief in Manichaeism is that the powerful, though not omnipotent good power (God), was opposed by the eternal evil power (devil). Humanity, the world and the soul are seen as the by-product of the battle between God's proxy, Primal Man, and the devil. The human person is seen as a battle-ground for these powers: the soul defines the person, but it is under the influence of both light and dark. This contention plays out over the world as well as the human body—neither the Earth nor the flesh were seen as intrinsically evil, but rather possessed portions of both light and dark. Natural phenomena (such as rain) were seen as the physical manifestation of this spiritual contention. Therefore, the Manichaean view explained the existence of evil by positing a flawed creation in the formation of which God took no part and which constituted rather the product of a battle by the devil against God.

Manichaeism presented an elaborate description of the conflict between the spiritual world of light and the material world of darkness. The beings of both the world of darkness and the world of light have names. There are numerous sources for the details of the Manichaean belief. There are two portions of Manichaean scriptures that are probably the closest thing to the original Manichaean writings in their original languages that will ever be available. These are the Syriac-Aramaic quotation by the Nestorian Christian Theodore bar Konai, in his Syriac "Book of Scholia" ("Ketba de-Skolion"z, 8th century), and the Middle Persian sections of Mani's Shabuhragan discovered at Turpan (a summary of Mani's teachings prepared for Shapur I).

From these and other sources, it is possible to derive an almost complete description of the detailed Manichaean vision (a complete list of Manichaean deities is outlined below). According to Mani, the unfolding of the universe takes place with three "creations":

The First Creation: Originally, good and evil existed in two completely separate realms, one the "World of Light", ruled by the "Father of Greatness" together with his five "Shekhinas" (divine attributes of light), and the other the "World of Darkness", ruled by the "King of Darkness". At a certain point, the "Kingdom of Darkness" notices the "World of Light", becomes greedy for it and attacks it. The "Father of Greatness", in the first of three "creations" (or "calls"), calls to the "Mother of Life", who sends her son "Original Man" ("Nāšā Qaḏmāyā" in Aramaic), to battle with the attacking powers of Darkness, which include the "Demon of Greed". The "Original Man" is armed with five different shields of light (reflections of the five "Shekhinas"), which he loses to the forces of darkness in the ensuing battle, described as a kind of "bait" to trick the forces of darkness, as the forces of darkness greedily consume as much light as they can. When the "Original Man" comes to, he is trapped among the forces of darkness.

The Second Creation: Then the "Father of Greatness" begins the "Second Creation", calling to the "Living Spirit", who calls to his five sons, and sends a call to the "Original Man" ("Call" then becomes a Manichaean deity). An answer ("Answer" becomes another Manichaean deity) then returns from the "Original Man" to the "World of Light". The "Mother of Life", the "Living Spirit", and his five sons begin to create the universe from the bodies of the evil beings of the "World of Darkness", together with the light that they have swallowed. Ten heavens and eight earths are created, all consisting of various mixtures of the evil material beings from the "World of Darkness" and the swallowed light. The sun, moon, and stars are all created from light recovered from the "World of Darkness". The waxing and waning of the moon is described as the moon filling with light, which passes to the sun, then through the Milky Way, and eventually back to the "World of Light".
The Third Creation: Great demons (called "archons" in bar-Khonai's account) are hung out over the heavens, and then the "Father of Greatness" begins the "Third Creation". Light is recovered from out of the material bodies of the male and female evil beings and demons, by causing them to become sexually aroused in greed, towards beautiful images of the beings of light, such as the "Third Messenger" and the "Virgins of Light". However, as soon as the light is expelled from their bodies and falls to the earth (some in the form of abortions – the source of fallen angels in the Manichaean myth), the evil beings continue to swallow up as much of it as they can to keep the light inside of them. This results eventually in the evil beings swallowing huge quantities of light, copulating, and producing Adam and Eve. The "Father of Greatness" then sends the "Radiant Jesus" to awaken Adam, and to enlighten him to the true source of the light that is trapped in his material body. Adam and Eve, however, eventually copulate, and produce more human beings, trapping the light in bodies of mankind throughout human history. The appearance of the Prophet Mani was another attempt by the "World of Light" to reveal to mankind the true source of the spiritual light imprisoned within their material bodies.

Beginning with the time of its creation by Mani, the Manichaean religion had a detailed description of deities and events that took place within the Manichaean scheme of the universe. In every language and region that Manichaeism spread to, these same deities reappear, whether it is in the original Syriac quoted by Theodore bar Konai, or the Latin terminology given by Saint Augustine from Mani's "Epistola Fundamenti", or the Persian and Chinese translations found as Manichaeism spread eastward. While the original Syriac retained the original description that Mani created, the transformation of the deities through other languages and cultures produced incarnations of the deities not implied in the original Syriac writings.







The Manichaean Church was divided into the Elect, who had taken upon themselves the vows of Manicheaism, and the Hearers, those who had not, but still participated in the Church. The Elect were forbidden to consume alcohol and meat, as well as to harvest crops or prepare food, due to Mani's claim that harvesting was a form of murder against plants. The Hearers would therefore commit the sin of preparing food, and would provide it to the Elect, who would in turn pray for the Hearers and cleanse them of these sins. The terms for these divisions were already common since the days of early Christianity, however, it had a different meaning in Christianity. In Chinese writings, the Middle Persian and Parthian terms are transcribed phonetically (instead of being translated into Chinese). These were recorded by Augustine of Hippo.

Evidently from Manichaean sources, Manichaeans observed daily prayers, either four for the "hearers" or seven for the "elects". The sources differ about the exact time of prayer. The "Fihrist" by al-Nadim, points them after noon, mid-afternoon, just after sunset and at nightfall. Al-Biruni places the prayers at noon, nightfall, dawn and sunrise. The elect additionally pray at mid-afternoon, half an hour after nightfall and at midnight. Al-Nadim's account of daily prayers is probably adjusted to coincide with the public prayers for the Muslims, while Al-Birunis report may reflect an older tradition unaffected by Islam. When Al-Nadims account of daily prayers had been the only detailed source available, there was a concern, that these practises had been only adapted by Muslims during the Abbasid Caliphate. However, it is clear that the Arabic text provided by Al-Nadim corresponds with the descriptions of Egyptian texts from the fourth Century.

Every prayer started with an ablution with water or, if water is not available, with other substances comparable to Ablution in Islam and consisted of several blessings to the apostales and spirits. The prayer consisted of prostrating oneself to the ground and rising again twelve times during every prayer. During day, Manichaeans turned towards the sun and during night towards the moon. If the moon is not visible at night, when they turned towards north. Evident from Faustus of Mileve, Celestial bodies are not the subject of worship themselves, but "ships" carrying the light particles of the world to the supreme god, who can not be seen, since he exists beyond time and space, and also the dwelling places for emanations of the supreme deity, such as Jesus the Splendour. According to the writings of Augustine of Hippo, ten prayers were performed, the first devoted to the Father of Greatness, and the following to lesser deities, spirits and angels and finally towards the elect, in order to be freed from rebirth and pain and to attain peace in the realm of light. Comparable, in the Uighur confession, four prayers are directed to the supreme God ("Äzrua"), the God of the Sun and the Moon, and fivefold God and the buddhas.

Mani wrote either seven or eight books, which contained the teachings of the religion. Only scattered fragments and translations of the originals remain.

The original six Syriac writings are not preserved, although their Syriac names have been. There are also fragments and quotations from them. A long quotation, preserved by the eighth-century Nestorian Christian author Theodore Bar Konai, shows that in the original Syriac Aramaic writings of Mani there was no influence of Iranian or Zoroastrian terms. The terms for the Manichaean deities in the original Syriac writings are in Aramaic. The adaptation of Manichaeism to the Zoroastrian religion appears to have begun in Mani's lifetime however, with his writing of the Middle Persian "Shabuhragan", his book dedicated to the Sasanian emperor, Shapur I. In it, there are mentions of Zoroastrian divinities such as Ahura Mazda, Angra Mainyu, and Āz. Manichaeism is often presented as a Persian religion, mostly due to the vast number of Middle Persian, Parthian, and Sogdian (as well as Turkish) texts discovered by German researchers near Turpan in what is now Xinjiang, China, during the early 1900s. However, from the vantage point of its original Syriac descriptions (as quoted by Theodore Bar Khonai and outlined above), Manichaeism may be better described as a unique phenomenon of Aramaic Babylonia, occurring in proximity to two other new Aramaic religious phenomena, Talmudic Judaism and Mandaeism, which also appeared in Babylonia in roughly the third century.

The original, but now lost, six sacred books of Manichaeism were composed in Syriac Aramaic, and translated into other languages to help spread the religion. As they spread to the east, the Manichaean writings passed through Middle Persian, Parthian, Sogdian, Tocharian, and ultimately Uyghur and Chinese translations. As they spread to the west, they were translated into Greek, Coptic, and Latin.
Henning describes how this translation process evolved and influenced the Manichaeans of Central Asia:




In later centuries, as Manichaeism passed through eastern Persian-speaking lands and arrived at the Uyghur Khaganate (回鶻帝國), and eventually the Uyghur kingdom of Turpan (destroyed around 1335), Middle Persian and Parthian prayers ("āfrīwan" or "āfurišn") and the Parthian hymn-cycles (the "Huwīdagmān" and "Angad Rōšnan" created by Mar Ammo) were added to the Manichaean writings. A translation of a collection of these produced the "Manichaean Chinese Hymnscroll" (, which Lieu translates as "Hymns for the Lower Section [i.e. the Hearers] of the Manichaean Religion"). In addition to containing hymns attributed to Mani, it contains prayers attributed to Mani's earliest disciples, including Mār Zaku, Mār Ammo and Mār Sīsin. Another Chinese work is a complete translation of the "Sermon of the Light Nous", presented as a discussion between Mani and his disciple Adda.

Until discoveries in the 1900s of original sources, the only sources for Manichaeism were descriptions and quotations from non-Manichaean authors, either Christian, Muslim, Buddhist, or Zoroastrian. While often criticizing Manichaeism, they also quoted directly from Manichaean scriptures. This enabled Isaac de Beausobre, writing in the 18th century, to create a comprehensive work on Manichaeism, relying solely on anti-Manichaean sources. Thus quotations and descriptions in Greek and Arabic have long been known to scholars, as have the long quotations in Latin by Saint Augustine, and the extremely important quotation in Syriac by Theodore Bar Konai.

Eusebius commented as follows:
An example of how inaccurate some of these accounts could be is seen in the account of the origins of Manichaeism contained in the "Acta Archelai". This was a Greek anti-manichaean work written before 348, most well known in its Latin version, which was regarded as an accurate account of Manichaeism until refuted by Isaac de Beausobre in the 18th century:

In the time of the Apostles there lived a man named Scythianus, who is described as coming "from Scythia", and also as being "a Saracen by race" ("ex genere Saracenorum"). He settled in Egypt, where he became acquainted with "the wisdom of the Egyptians", and invented the religious system that was afterwards known as Manichaeism. Finally he emigrated to Palestine, and, when he died, his writings passed into the hands of his sole disciple, a certain Terebinthus. The latter betook himself to Babylonia, assumed the name of Budda, and endeavoured to propagate his master's teaching. But he, like Scythianus, gained only one disciple, who was an old woman. After a while he died, in consequence of a fall from the roof of a house, and the books that he had inherited from Scythianus became the property of the old woman, who, on her death, bequeathed them to a young man named Corbicius, who had been her slave. Corbicius thereupon changed his name to Manes, studied the writings of Scythianus, and began to teach the doctrines that they contained, with many additions of his own. He gained three disciples, named Thomas, Addas, and Hermas. About this time the son of the Persian king fell ill, and Manes undertook to cure him; the prince, however, died, whereupon Manes was thrown into prison. He succeeded in escaping, but eventually fell into the hands of the king, by whose order he was flayed, and his corpse was hung up at the city gate.

A. A. Bevan, who quoted this story, commented that it "has no claim to be considered historical".

According to Hegemonius' portrayal of Mani, the evil demiurge who created the world was the Jewish Jehovah. Hegemonius reports that Mani said,
In the early 1900s, original Manichaean writings started to come to light when German scholars led by Albert Grünwedel, and then by Albert von Le Coq, began excavating at Gaochang, the ancient site of the Manichaean Uyghur Kingdom near Turpan, in Chinese Turkestan (destroyed around AD 1300). While most of the writings they uncovered were in very poor condition, there were still hundreds of pages of Manichaean scriptures, written in three Iranian languages (Middle Persian, Parthian, and Sogdian) and old Uyghur. These writings were taken back to Germany, and were analyzed and published at the Prussian Academy of Sciences in Berlin, by Le Coq and others, such as Friedrich W. K. Müller and Walter Bruno Henning. While the vast majority of these writings were written in a version of the Syriac script known as Manichaean script, the German researchers, perhaps for lack of suitable fonts, published most of them using the Hebrew alphabet (which could easily be substituted for the 22 Syriac letters).

Perhaps the most comprehensive of these publications was "Manichaeische Dogmatik aus chinesischen und iranischen Texten" ("Manichaean Dogma from Chinese and Iranian texts"), by Ernst Waldschmidt and Wolfgang Lentz, published in Berlin in 1933. More than any other research work published before or since, this work printed, and then discussed, the original key Manichaean texts in the original scripts, and consists chiefly of sections from Chinese texts, and Middle Persian and Parthian texts transcribed with the Hebrew alphabet. After the Nazi Party gained power in Germany, the Manichaean writings continued to be published during the 1930s, but the publishers no longer used Hebrew letters, instead transliterating the texts into Latin letters.

Additionally, in 1930, German researchers in Egypt found a large body of Manichaean works in Coptic. Though these were also damaged, hundreds of complete pages survived and, beginning in 1933, were analyzed and published in Berlin before World War II, by German scholars such as Hans Jakob Polotsky. Some of these Coptic Manichaean writings were lost during the war.

After the success of the German researchers, French scholars visited China and discovered what is perhaps the most complete set of Manichaean writings, written in Chinese. These three Chinese writings, all found at the Mogao Caves among the Dunhuang manuscripts, and all written before the 9th century, are today kept in London, Paris, and Beijing. Some of the scholars involved with their initial discovery and publication were Édouard Chavannes, Paul Pelliot, and Aurel Stein. The original studies and analyses of these writings, along with their translations, first appeared in French, English, and German, before and after World War II. The complete Chinese texts themselves were first published in Tokyo, Japan in 1927, in the Taishō Tripiṭaka, volume 54. While in the last thirty years or so they have been republished in both Germany (with a complete translation into German, alongside the 1927 Japanese edition), and China, the Japanese publication remains the standard reference for the Chinese texts.

In Egypt, a small codex was found and became known through antique dealers in Cairo. It was purchased by the University of Cologne in 1969. Two of its scientists, Henrichs and Koenen, produced the first edition known since as the Cologne Mani-Codex, which was published in four articles in the "Zeitschrift für Papyrologie und Epigraphik". The ancient papyrus manuscript contained a Greek text describing the life of Mani. Thanks to this discovery, much more is known about the man who founded one of the most influential world religions of the past.

The terms "Manichaean" and "Manichaeism" are sometimes used figuratively as a synonym of the more general term "dualist" with respect to a philosophy, outlook or worldview. The terms are often used to suggest that the world view in question simplistically reduces the world to a struggle between good and evil. For example, Zbigniew Brzezinski used the phrase "Manichaean paranoia" in reference to U.S. President George W. Bush's world view (in "The Daily Show with Jon Stewart", 14 March 2007); Brzezinski elaborated that he meant "the notion that he [Bush] is leading the forces of good against the empire of evil". Author and journalist Glenn Greenwald followed up on the theme in describing Bush in his book "A Tragic Legacy" (2007).

The term is frequently used by critics to describe the attitudes and foreign policies of the United States and its leaders.

Philosopher Frantz Fanon frequently invoked the concept of Manicheanism in his discussions of violence between colonizers and the colonized.

In "My Secret History", author Paul Theroux's protagonist defines the word Manichaean for the protagonist's son as 'seeing that good and evil are mingled'. Prior to explaining the word to his son, the protagonist mentions Joseph Conrad's short story "The Secret Sharer" at least twice in the book, the plot of which also examines the idea of the duality of good and evil.








</doc>
<doc id="19761" url="https://en.wikipedia.org/wiki?curid=19761" title="Moroccan cuisine">
Moroccan cuisine

Moroccan cuisine is influenced by Morocco's interactions and exchanges with other cultures and nations over the centuries. Moroccan cuisine is usually a mix of Amazigh, Arab, Andalusian, and Mediterranean cuisines, with slight European (French and Spanish) and sub-Saharan influences.

Morocco produces a large range of Mediterranean fruits, vegetables and even some tropical ones. Common meats include beef, goat, mutton and lamb, chicken and seafood, which serve as a base for the cuisine. Characteristic flavorings include lemon pickle, argan oil, cold-pressed, unrefined olive oil and dried fruits. As in Mediterranean cuisine in general, the staple ingredients include wheat, used for bread and couscous, and olive oil; the third Mediterranean staple, the grape, is eaten as a dessert, though a certain amount of wine is made in the country.

Spices are used extensively in Moroccan food. Although some spices have been imported to Morocco through the Arabs for thousands of years, many ingredients—like saffron from Talaouine, mint and olives from Meknes, and oranges and lemons from Fes—are home-grown, and are being exported. Common spices include cinnamon, cumin, turmeric, ginger, paprika, coriander, saffron, mace, cloves, fennel, anise, nutmeg, cayenne pepper, fenugreek, caraway, black pepper and sesame seeds. Twenty-seven spices are combined for the famous Moroccan spice mixture "ras el hanout".

Common herbs in Moroccan cuisine include mint, parsley, coriander, oregano, peppermint, marjoram, verbena, sage and bay laurel.

A typical lunch meal begins with a series of hot and cold salads, followed by a tagine or Dwaz. Often, for a formal meal, a lamb or chicken dish is next, or couscous topped with meat and vegetables. Moroccans either eat with fork, knife and spoon or with their hands using bread as a utensil depending on the dish served. The consumption of pork and alcohol is uncommon due to religious restrictions.

The main Moroccan dish most people are familiar with is couscous; beef is the most commonly eaten red meat in Morocco, usually eaten in a tagine with a wide selection of vegetables. Chicken is also very commonly used in tagines or roasted. They also use additional ingredients such as plums, boiled eggs, and lemon. Like their national food, the tagine has a unique taste of popular spices such as saffron, cumin, cinnamon, ginger, and cilantro, as well as ground red pepper.

Since Morocco lies on two coasts, the Atlantic and the Mediterranean, Moroccan cuisine has ample seafood dishes. European pilchard is caught in large but declining quantities. Other fish species include mackerel, anchovy, sardinella, and horse mackerel.

Other famous Moroccan dishes are Pastilla (also spelled Basteeya or Bestilla), Tanjia, and Rfissa.

A big part of the daily meal is bread. Bread in Morocco is principally made from durum wheat semolina known as khobz. Bakeries are very common throughout Morocco and fresh bread is a staple in every city, town, and village. The most common is whole grain coarse ground or white flour bread or baguettes. There are also a number of flat breads and pulled unleavened pan-fried breads.

In addition, there are dried salted meats and salted preserved meats such as khlea and g'did (basically sheep bacon), which are used to flavor tagines or used in "el rghaif", a folded savory Moroccan pancake.

Harira, a typical heavy soup, eaten during winter to warm up and is usually served for dinner. It is typically eaten with plain bread or with dates during the month of Ramadan. Bissara is a broad bean-based soup that is also consumed during the colder months of the year.
Salads include both raw and cooked vegetables, served either hot or cold. Cold salads include "zaalouk," an aubergine and tomato mixture, and taktouka (a mixture of tomatoes, smoked green peppers, garlic, and spices) characteristic of the cities of Taza and Fes, in the Atlas. Another cold salad is called Bakoula, or Khoubiza. It consists of braised mallow leaves, but can also be made with spinach or arugula, with parsley, cilantro, lemon, olive oil, and olives.

Usually, seasonal fruits rather than cooked desserts are served at the close of a meal. A common dessert is "kaab el ghzal" (, "gazelle ankles"), a pastry stuffed with almond paste and topped with sugar. Another is "Halwa chebakia", pretzel-shaped dough deep-fried, soaked in honey and sprinkled with sesame seeds; it is eaten during the month of Ramadan. "Jowhara" is a delicacy typical of Fes, made with fried "waraq" pastry, cream, orange blossom water, and toasted almond slices. Coconut fudge cakes, 'Zucre Coco', are popular also.

Morocco is endowed with over 3000 km of coastline. There is an abundance of fish in these coastal waters with the sardine being commercially significant as Morocco is the world's largest exporter. Sardines were used in the production of garum in Lixus.

At Moroccan fish markets one can find sole, swordfish, tuna, tarbot, mackerel, shrimp, congre eel, skate, red snapper, spider crab, lobster and a variety of mollusks.

In Moroccan cuisine, seafood is incorporated into, among others: tajines, bastilla, briwat, and paella.

The most popular drink is green tea with mint. Traditionally, making good mint tea in Morocco is considered an art form and the drinking of it with friends and family is often a daily tradition. The pouring technique is as crucial as the quality of the tea itself. Moroccan tea pots have long, curved pouring spouts and this allows the tea to be poured evenly into tiny glasses from a height. For the best taste, glasses are filled in two stages. The Moroccans traditionally like tea with bubbles, so while pouring they hold the teapot high above the glasses. Finally, the tea is accompanied with hard sugar cones or lumps. Morocco has an abundance of oranges and tangerines, so fresh orange juice is easily found freshly squeezed and is cheap.

Selling fast food in the street has long been a tradition, and the best example is Djemaa el Fna square in Marrakech. Starting in the 1980s, new snack restaurants started serving "Bocadillo" (a Spanish word for a sandwich).

Dairy product shops locally called Mhlaba, are very prevalent all around the country. Those dairy stores generally offer all types of dairy products, juices, and local delicacies such as Bocadillos, Msemen and Harcha.

Another popular street food in Morocco is the snails, which are served in their stew in small bowls and eaten using a toothpick.

In the late 1990s, several multinational fast-food franchises opened restaurants in major cities.



</doc>
<doc id="19763" url="https://en.wikipedia.org/wiki?curid=19763" title="Martin Van Buren">
Martin Van Buren

Martin Van Buren ( ; born Maarten Van Buren ; December 5, 1782 – July 24, 1862) was an American statesman who served as the eighth president of the United States from 1837 to 1841. A founder of the Democratic Party, he had previously served as the ninth governor of New York, the tenth United States secretary of state, and the eighth vice president of the United States. He won the 1836 presidential election with the endorsement of popular outgoing President Andrew Jackson and the organizational strength of the Democratic Party. He lost his 1840 reelection bid to Whig Party nominee William Henry Harrison, thanks in part to the poor economic conditions surrounding the Panic of 1837. Later in his life, Van Buren emerged as an elder statesman and an important anti-slavery abolitionist leader who led the Free Soil Party ticket in the presidential election of 1848.

Van Buren was born into a family of Dutch Americans in Kinderhook, New York; he was the first President to have been born after the American Revolution — in which his father served as a Patriot — and is the only President to speak English as a second language. Trained as a lawyer, he quickly became involved in politics as a member of the Democratic-Republican Party, and won a seat in the New York State Senate, then the United States Senate in 1821. As the leader of the Bucktails faction, Van Buren emerged as the most influential politician from New York in the 1820s and established a political machine known as the Albany Regency. Following the 1824 presidential election, Van Buren led to re-establish a two-party system with partisan differences based on ideology rather than personalities or sectional differences; he supported Jackson's candidacy in the 1828 presidential election with this goal in mind. He ran successfully for Governor of New York in order to support Jackson's campaign, but resigned shortly after Jackson was inaugurated so he could accept appointment as Jackson's Secretary of State.

In his cabinet position, Van Buren became a key Jackson advisor, and built the organizational structure for the coalescing Democratic Party. He ultimately resigned to help resolve the Petticoat affair, and briefly served as the U.S. ambassador to the United Kingdom. At Jackson's behest, the 1832 Democratic National Convention nominated Van Buren for Vice President of the United States, and he took office after the Democratic ticket won the 1832 presidential election. With Jackson's strong support, Van Buren won the presidential nomination at the 1835 Democratic National Convention, and he defeated several Whig opponents in the 1836 presidential election. However, his presidency soon eroded with his response to the Panic of 1837, which centered on his Independent Treasury system, a plan under which the Federal government of the United States would store its funds in vaults rather than in banks; more conservative Democrats and Whigs in Congress ultimately delayed his plan from being implemented until 1840. His presidency was further marred by the costly Second Seminole War (a result of continuing Jackson's Indian removal policy); and his refusal to admit Texas to the Union as a slave state, done as an attempt to avoid heightened sectional tensions. In 1840, a surge of new voters — who nicknamed him "Martin Van Ruin" — helped turn out of office.

Van Buren was initially the leading candidate for the Democratic party's nomination again in 1844, but his continued opposition to the annexation of Texas angered Southern Democrats, leading to the nomination of James K. Polk. Van Buren led a third-party ticket in 1848, and his candidacy most likely helped Whig nominee Zachary Taylor defeat Democrat Lewis Cass. Van Buren returned to the Democrats after 1848, but grew increasingly opposed to slavery, and became one of the party's outspoken abolitionists. He supported Abraham Lincoln's policies during the American Civil War. He died in Kinderhook in July 1862, at age 79.

In historical rankings, historians and political scientists often rank Van Buren as an average or below-average U.S. president, due to his handling of the Panic of 1837. However, Van Buren is largely regarded today as a leader in the formation of the two-party system in the United States.

Van Buren was born as Maarten Van Buren on December 5, 1782, in Kinderhook, New York, about south of Albany on the Hudson River.

His father, Abraham Van Buren, was a descendant of Cornelis Maessen, a native of Buurmalsen, Netherlands who had emigrated to New Netherland in 1631 and purchased a plot of land on Manhattan Island. Abraham Van Buren had been a Patriot during the American Revolution, and he later joined the Democratic-Republican Party. He owned an inn and tavern in Kinderhook and served as Kinderhook's town clerk for several years. In 1776, he married Maria Hoes (or Goes) Van Alen (1746-1818) in the town of Kinderhook, also of Dutch extraction and the widow of Johannes Van Alen (1744-c. 1773). She had three children from her first marriage, including future U.S. Representative James I. Van Alen. Her second marriage produced five children, of which Martin was the third.

Van Buren received a basic education at the village schoolhouse, and briefly studied Latin at the Kinderhook Academy and at Washington Seminary in Claverack. Van Buren was raised speaking primarily Dutch, and learned English at school; as of 2020, he remains the only President whose first language was not English. Also during his childhood, Van Buren learned at his father's inn how to interact with people from varied ethnic, income, and societal groups, which he used to his advantage as a political organizer. His formal education ended in 1796, when he began reading law at the office of Peter Silvester and his son Francis.

Van Buren was small in stature at tall and affectionately nicknamed "Little Van". When he first began his legal studies, he wore rough, homespun clothing, causing the Silvesters to admonish him to pay greater heed to his clothing and personal appearance as an aspiring lawyer. He accepted their advice and subsequently emulated the Silvesters' clothing, appearance, bearing, and conduct. Despite Kinderhook's strong affiliation with the Federalist Party, of which the Silvesters were also strong supporters, Van Buren adopted his father's Democratic-Republican leanings. The Silvesters and Democratic-Republican political figure John Peter Van Ness suggested that Van Buren's political leanings constrained him to complete his education with a Democratic-Republican attorney, so he spent a final year of apprenticeship in the New York City office of John Van Ness's brother William P. Van Ness, a political lieutenant of Aaron Burr. Van Ness introduced Van Buren to the intricacies of New York state politics, and Van Buren observed Burr's battles for control of the state Democratic-Republican party against George Clinton and Robert R. Livingston. He returned to Kinderhook in 1803, after being admitted to the New York bar.
Van Buren married Hannah Hoes (or Goes) in Catskill, New York, on February 21, 1807. She was his childhood sweetheart, and a daughter of his maternal first cousin, Johannes Dircksen Hoes. Like Van Buren, she was raised in a Dutch home in Valatie; she spoke primarily Dutch, and spoke English with a marked accent. The couple had five children, four of whom lived to adulthood: Abraham (1807–1873), John (1810–1866), Martin Jr. (1812–1855), Winfield Scott (born and died in 1814), and Smith Thompson (1817–1876). Hannah contracted tuberculosis, and died in Kinderhook on February 5, 1819, at age 35. Van Buren never remarried.

Upon returning to Kinderhook in 1803, Van Buren formed a law partnership with his half-brother, James Van Alen, and became financially secure enough to increase his focus on politics. Van Buren had been active in politics from age 18, if not before. In 1801, he attended a Democratic-Republican Party convention in Troy, New York where he worked successfully to secure for John Peter Van Ness the party nomination in a special election for the 6th Congressional District seat. Upon returning to Kinderhook, Van Buren broke with the Burr faction, becoming an ally of both DeWitt Clinton and Daniel D. Tompkins. After the faction led by Clinton and Tompkins dominated the 1807 elections, Van Buren was appointed Surrogate of Columbia County, New York. Seeking to find a better base for his political and legal career, Van Buren and his family moved to the town of Hudson, the seat of Columbia County, in 1808. Van Buren's legal practice continued to flourish, and he traveled all over the state to represent various clients.

In 1812, Van Buren won his party's nomination for a seat in the New York State Senate. Though several Democratic-Republicans, including John Peter Van Ness, joined with the Federalists to oppose his candidacy, Van Buren won election to the state senate in mid-1812. Later in the year, the United States entered the War of 1812 against Great Britain, while Clinton launched an unsuccessful bid to defeat President James Madison in the 1812 presidential election. After the election, Van Buren became suspicious that Clinton was working with the Federalist Party, and he broke from his former political ally.

During the War of 1812, Van Buren worked with Clinton, Governor Tompkins, and Ambrose Spencer to support the Madison administration's prosecution of the war. In addition, he was a special judge advocate appointed to serve as a prosecutor of William Hull during Hull's court-martial following the surrender of Detroit. Anticipating another military campaign, he collaborated with Winfield Scott on ways to reorganize the New York Militia in the winter of 1814–1815, but their work was halted by the end of the war in early 1815. Van Buren was so favorably impressed by Scott that he named his fourth son after him. Van Buren's strong support for the war boosted his standing, and in 1815, he was elected to the position of New York Attorney General. Van Buren moved from Hudson to the state capital of Albany, where he established a legal partnership with Benjamin Butler, and shared a house with political ally Roger Skinner. In 1816, Van Buren won re-election to the state senate, and he would continue to simultaneously serve as both state senator and as the state's attorney general. In 1819, he played an active part in prosecuting the accused murderers of Richard Jennings, the first murder-for-hire case in the state of New York.

After Tompkins was elected as vice president in the 1816 presidential election, Clinton defeated Van Buren's preferred candidate, Peter Buell Porter, in the 1817 New York gubernatorial election. Clinton threw his influence behind the construction of the Erie Canal, an ambitious project designed to connect Lake Erie to the Atlantic Ocean. Though many of Van Buren's allies urged him to block Clinton's Erie Canal bill, Van Buren believed that the canal would benefit the state. His support for the bill helped it win approval from the New York legislature. Despite his support for the Erie Canal, Van Buren became the leader of an anti-Clintonian faction in New York known as the "Bucktails".

The Bucktails succeeded in emphasizing party loyalty and used it to capture and control many patronage posts throughout New York. Through his use of patronage, loyal newspapers, and connections with local party officials and leaders, Van Buren established what became known as the "Albany Regency", a political machine that emerged as an important factor in New York politics. The Regency relied on a coalition of small farmers, but also enjoyed support from the Tammany Hall machine in New York City. Van Buren largely determined Tammany Hall's political policy for the Democratic-Republicans in this era.

A New York state referendum that expanded state voting rights to all white men in 1821, and which further increased the power of Tammany Hall, was guided by Van Buren. Although Governor Clinton remained in office until late 1822, Van Buren emerged as the leader of the state's Democratic-Republicans after the 1820 elections. Van Buren was a member of the 1820 state constitutional convention, where he favored expanded voting rights, but opposed universal suffrage and tried to maintain property requirements for voting.

In February 1821, the state legislature elected Van Buren to represent New York in the United States Senate. Van Buren arrived in Washington during the "Era of Good Feelings", a period in which partisan distinctions at the national level had faded. Van Buren quickly became a prominent figure in Washington, D.C., befriending Secretary of the Treasury William H. Crawford, among others. Though not an exceptional orator, Van Buren frequently engaged in debate on the Senate floor, usually after extensively researching the subject at hand. Despite his commitments as a father and state party leader, Van Buren remained closely engaged in his legislative duties, and during his time in the Senate he served as the chairman of the Senate Finance Committee and the Senate Judiciary Committee. As he gained renown, Van Buren earned monikers like "Little Magician" and "Sly Fox".

Van Buren chose to back Crawford over John Quincy Adams, Andrew Jackson, and Henry Clay in the presidential election of 1824. Crawford shared Van Buren's affinity for Jeffersonian principles of states' rights and limited government, and Van Buren believed that Crawford was the ideal figure to lead a coalition of New York, Pennsylvania, and Virginia's "Richmond Junto". Van Buren's support for Crawford aroused strong opposition in New York in the form of the People's party, which drew support from Clintonians, Federalists, and others opposed to Van Buren. Nonetheless, Van Buren helped Crawford win the Democratic-Republican party's presidential nomination at the February 1824 congressional nominating caucus. The other Democratic-Republican candidates in the race refused to accept the poorly-attended caucus's decision, and as the Federalist Party had virtually ceased to function as a national party, the 1824 campaign became a competition among four candidates of the same party. Though Crawford suffered a severe stroke that left him in poor health, Van Buren continued to support his chosen candidate. Van Buren met with Thomas Jefferson in May 1824 in an attempt to bolster Crawford's candidacy, and though he was unsuccessful in gaining a public endorsement for Crawford, he nonetheless cherished the chance to meet with his political hero.

The 1824 elections dealt a severe blow to the Albany Regency, as Clinton returned to the governorship with the support of the People's party. By the time the state legislature convened to choose the state's presidential electors, results from other states had made it clear that no individual would win a majority of the electoral vote, necessitating a contingent election in the United States House of Representatives. While Adams and Jackson were assured of finishing in the top three, and thus being eligible for selection in the contingent election, New York's electors would help determine whether Clay or Crawford would finish third. Though most of the state's electoral votes went to Adams, Crawford won one more electoral vote than Clay in the state, and Clay's defeat in Louisiana left Crawford in third place. With Crawford still in the running, Van Buren lobbied members of the House to support him. He hoped to engineer a Crawford victory on the second ballot of the contingent election, but Adams won on the first ballot with the help of Clay and Stephen Van Rensselaer, a Congressman from New York. Despite his close ties with Van Buren, Van Rensselaer cast his vote for Adams, thus giving Adams a narrow majority of New York's delegation and a victory in the contingent election.

After the House contest, Van Buren shrewdly kept out of the controversy which followed, and began looking forward to 1828. Jackson was angered to see the presidency go to Adams despite having won more popular votes than he had, and he eagerly looked forward to a rematch. Jackson's supporters accused Adams and Clay of having engaged in a "corrupt bargain" in which Clay helped Adams win the contingent election in return for Clay's appointment as Secretary of State. Always notably courteous in his treatment of opponents, Van Buren showed no bitterness toward either Adams or Clay, and he voted to confirm Clay's nomination to the cabinet. At the same time, Van Buren opposed the Adams-Clay plans for internal improvements like roads and canals and declined to support U.S. participation in the Congress of Panama. Van Buren considered Adams's proposals to represent a return to the Hamiltonian economic model favored by Federalists, which he strongly opposed. Despite his opposition to Adams's public policies, Van Buren was able to easily secure re-election in his own divided home state in 1827.

Van Buren's overarching goal at the national level was to restore a two-party system with party cleavages based on philosophical differences, and he viewed the old divide between Federalists and Democratic-Republicans as the best state of affairs for the nation. Van Buren believed that these national parties helped ensure that elections were decided on national, rather than sectional or local, issues; as he put it, "party attachment in former times furnished a complete antidote for sectional prejudices". After the 1824 election, Van Buren was initially somewhat skeptical of Jackson, who had not taken strong positions on most policy issues. Nonetheless, he settled on Jackson as the one candidate who could beat Adams in the 1828 presidential election, and he worked to bring Crawford's former backers into line behind Jackson.

He also forged alliances with other members of Congress opposed to Adams, including Vice President John C. Calhoun, Senator Thomas Hart Benton, and Senator John Randolph. Seeking to solidify his own standing in New York and bolster Jackson's campaign, Van Buren helped arrange the passage of the Tariff of 1828, which opponents labeled as the "Tariff of Abominations". The tariff satisfied many who sought protection from foreign competition, but angered Southern cotton interests and New Englanders. Because Van Buren believed that the South would never support Adams, and New England would never support Jackson, he was willing to alienate both regions through passage of the tariff.

Meanwhile, Clinton's death from a heart attack in 1828 dramatically shook up the politics of Van Buren's home state, while the Anti-Masonic Party emerged as an increasingly important factor. After some initial reluctance, Van Buren chose to run for Governor of New York in the 1828 election. Hoping that a Jackson victory would lead to his own elevation to Secretary of State or Secretary of the Treasury, Van Buren chose Enos T. Throop as his running mate and preferred successor. Van Buren's candidacy was aided by the split between supporters of Adams, who had adopted the label of National Republicans, and the Anti-Masonic Party.

Reflecting his public association with Jackson, Van Buren accepted the gubernatorial nomination on a ticket that called itself "Jacksonian-Democrat". He campaigned on local as well as national issues, emphasizing his opposition to the policies of the Adams administration. Van Buren ran ahead of Jackson, winning the state by 30,000 votes compared to a margin of 5,000 for Jackson. Nationally, Jackson defeated Adams by a wide margin, winning nearly every state outside of New England. After the election, Van Buren resigned from the Senate to start his term as governor, which began on January 1, 1829. While his term as governor was short, he did manage to pass the Bank Safety Fund Law, an early form of deposit insurance, through the legislature. He also appointed several key supporters, including William L. Marcy and Silas Wright, to important state positions.

In February 1829, Jackson wrote to Van Buren to ask him to become Secretary of State. Van Buren quickly agreed, and he resigned as governor the following month; his tenure of forty-three days is the shortest of any Governor of New York. No serious diplomatic crises arose during Van Buren's tenure as Secretary of State, but he achieved several notable successes, such as settling long-standing claims against France and winning reparations for property that had been seized during the Napoleonic Wars. He reached an agreement with the British to open trade with the British West Indies colonies and concluded a treaty with the Ottoman Empire that gained American merchants access to the Black Sea. Items on which he did not achieve success included settling the Maine-New Brunswick boundary dispute with Great Britain, gaining settlement of the U.S. claim to the Oregon Country, concluding a commercial treaty with Russia, and persuading Mexico to sell Texas.

In addition to his foreign policy duties, Van Buren quickly emerged as an important adviser to Jackson on major domestic issues like the tariff and internal improvements. The Secretary of State was instrumental in convincing Jackson to issue the Maysville Road veto, which both reaffirmed limited government principles and also helped prevent the construction of infrastructure projects that could potentially compete with New York's Erie Canal. He also became involved in a power struggle with Calhoun over appointments and other issues, including the Petticoat Affair. The Petticoat Affair arose because Peggy Eaton, wife of Secretary of War John H. Eaton, was ostracized by the other cabinet wives due to circumstances surrounding her marriage.

Led by Floride Calhoun, wife of Vice President John Calhoun, the other cabinet wives refused to pay courtesy calls to the Eatons, receive them as visitors, or invite them to social events. As a widower, Van Buren was unaffected by the position of the cabinet wives. Van Buren initially sought to conciliate the divide in the cabinet, but most of the leading citizens in Washington continued to snub the Eatons. Jackson was personally close to Eaton, and he came to the conclusion that the allegations against Eaton arose from a plot against his administration led by Henry Clay. The Petticoat Affair, combined with a contentious debate over the tariff and Calhoun's decade-old criticisms of Jackson's actions in the First Seminole War, contributed to a split between Jackson and Calhoun. As the debate over the tariff and the proposed ability of South Carolina to nullify federal law consumed Washington, Van Buren increasingly emerged as Jackson's likely successor.

The Petticoat affair was finally resolved when Van Buren offered to resign. In April 1831, Jackson accepted and reorganized his cabinet by asking for the resignations of the anti-Eaton cabinet members. Postmaster General William T. Barry, who had sided with the Eatons in the Petticoat Affair, was the lone cabinet member to remain in office. The cabinet reorganization removed Calhoun's allies from the Jackson administration, and Van Buren had a major role in shaping the new cabinet. After leaving office, Van Buren continued to play a part in the Kitchen Cabinet, Jackson's informal circle of advisers.

In August 1831, Jackson gave Van Buren a recess appointment as the ambassador to Britain, and Van Buren arrived in London in September. He was cordially received, but in February 1832, he learned his nomination had been rejected by the Senate. The rejection of Van Buren was essentially the work of Calhoun. When the vote on Van Buren's nomination was taken, enough pro-Calhoun Jacksonians refrained from voting to produce a tie, which allowed Calhoun to cast the deciding vote against Van Buren.

Calhoun was elated, convinced that he had ended Van Buren's career. "It will kill him dead, sir, kill him dead. He will never kick, sir, never kick", Calhoun exclaimed to a friend. Calhoun's move backfired; by making Van Buren appear the victim of petty politics, Calhoun raised Van Buren in both Jackson's regard and the esteem of others in the Democratic Party. Far from ending Van Buren's career, Calhoun's action gave greater impetus to Van Buren's candidacy for vice president.

Seeking to ensure that Van Buren would replace Calhoun as his running mate, Jackson had arranged for a national convention of his supporters. The May 1832 Democratic National Convention subsequently nominated Van Buren to serve as the party's vice presidential nominee. Van Buren won the nomination over Philip Pendleton Barbour (Calhoun's favored candidate) and Richard Mentor Johnson due to the support of Jackson and the strength of the Albany Regency. Upon Van Buren's return from Europe in July 1832, he became involved in the Bank War, a struggle over the re-charter of the Second Bank of the United States.

Van Buren had long been distrustful of banks, and he viewed the Bank as an extension of the Hamiltonian economic program, so he supported Jackson's veto of the Bank's re-charter. Henry Clay, the presidential nominee of the National Republicans, made the struggle over the Bank the key issue of the presidential election of 1832. The Jackson–Van Buren ticket won the 1832 election by a landslide, and Van Buren took office as vice president in March 1833. During the Nullification Crisis, Van Buren counseled Jackson to pursue a policy of conciliation with South Carolina leaders. He played little direct role in the passage of the Tariff of 1833, but he quietly hoped that the tariff would help bring an end to the Nullification Crisis, which it did.

As Vice President, Van Buren continued to be one of Jackson's primary advisors and confidants, and accompanied Jackson on his tour of the northeastern United States in 1833. Jackson's struggle with the Second Bank of the United States continued, as the president sought to remove federal funds from the Bank. Though initially apprehensive of the removal due to congressional support for the Bank, Van Buren eventually came to support Jackson's policy. He also helped undermine a fledgling alliance between Jackson and Daniel Webster, a senator from Massachusetts who could have potentially threatened Van Buren's project to create two parties separated by policy differences rather than personalities. During Jackson's second term, the president's supporters began to refer to themselves as members of the Democratic Party. Meanwhile, those opposed to Jackson, including Clay's National Republicans, followers of Calhoun, and many members of the Anti-Masonic Party, coalesced into the Whig Party.

President Andrew Jackson declined to seek another term in the 1836 presidential election, but he remained influential within the Democratic Party as his second term came to an end. Jackson was determined to help elect Van Buren in 1836 so that the latter could continue the Jackson administration's policies. the two men-–the charismatic "Old Hickory" and the super-efficient "Sly Fox"--had entirely different personalities but had become an effective team in eight years in office together. With Jackson's support, Van Buren won the presidential nomination of the 1835 Democratic National Convention without opposition. Two names were put forward for the vice-presidential nomination: Representative Richard M. Johnson of Kentucky, and former Senator William Cabell Rives of Virginia. Southern Democrats, and Van Buren himself, strongly preferred Rives. Jackson, on the other hand, strongly preferred Johnson. Again, Jackson's considerable influence prevailed, and Johnson received the required two-thirds vote after New York Senator Silas Wright prevailed upon non-delegate Edward Rucker to cast the 15 votes of the absent Tennessee delegation in Johnson's favor.

Van Buren's competitors in the election of 1836 were three members of the Whig Party, which remained a loose coalition bound by mutual opposition to Jackson's anti-bank policies. Lacking the party unity or organizational strength to field a single ticket or define a single platform, the Whigs ran several regional candidates in hopes of sending the election to the House of Representatives. The three candidates were: Hugh Lawson White of Tennessee, Daniel Webster of Massachusetts, and William Henry Harrison of Indiana. Besides endorsing internal improvements and a national bank, the Whigs tried to tie Democrats to abolitionism and sectional tension, and attacked Jackson for "acts of aggression and usurpation of power".

Southern voters represented the biggest potential impediment in Van Buren's quest for the presidency, as many were suspicious of a Northern president. Van Buren moved to obtain the support of southerners by assuring them that he opposed abolitionism and supported the maintaining of slavery in states where it had already existed. To demonstrate consistency regarding his opinions on slavery, Van Buren cast the tie-breaking Senate vote in favor of a bill to subject abolitionist mail to state laws, thus ensuring that its circulation would be prohibited in the South. Van Buren personally considered slavery to be immoral, but sanctioned by the Constitution.

Van Buren won the election with 764,198 popular votes, 50.9% of the total, and 170 electoral votes. Harrison led the Whigs with 73 electoral votes, White receiving 26, and Webster 14. Willie Person Mangum received South Carolina's 11 electoral votes, which were awarded by the state legislature. Van Buren's victory resulted from a combination of his own attractive political and personal qualities, Jackson's popularity and endorsement, the organizational power of the Democratic party, and the inability of the Whig Party to muster an effective candidate and campaign. Virginia's presidential electors voted for Van Buren for president, but voted for William Smith for vice president, leaving Johnson one electoral vote short of election. In accordance with the Twelfth Amendment, the Senate elected Johnson vice president in a contingent vote.

The election of 1836 marked an important turning point in American political history because it saw the establishment of the Second Party System. In the early 1830s, the political party structure was still changing, rapidly, and factional and personal leaders continued to play a major role in politics. By the end of the campaign of 1836, the new party system was almost complete, as nearly every faction had been absorbed by either the Democrats or the Whigs.

Van Buren retained much of Jackson's cabinet and lower-level appointees, as he hoped that the retention of Jackson's appointees would stop Whig momentum in the South and restore confidence in the Democrats as a party of sectional unity. The cabinet holdovers represented the different regions of the country: Secretary of the Treasury Levi Woodbury came from New England, Attorney General Benjamin F. Butler and Secretary of the Navy Mahlon Dickerson hailed from mid-Atlantic states, Secretary of State John Forsyth represented the South, and Postmaster General Amos Kendall of Kentucky represented the West.

For the lone open position of Secretary of War, Van Buren first approached William Cabell Rives, who had sought the vice presidency in 1836. After Rives declined to join the cabinet, Van Buren appointed Joel Roberts Poinsett, a South Carolinian who had opposed secession during the Nullification Crisis. Van Buren's cabinet choices were criticized by Pennsylvanians such as James Buchanan, who argued that their state deserved a cabinet position as well as some Democrats who argued that Van Buren should have used his patronage powers to augment his own power. However, Van Buren saw value in avoiding contentious patronage battles, and his decision to retain Jackson's cabinet made it clear that he intended to continue the policies of his predecessor. Additionally, Van Buren had helped select Jackson's cabinet appointees and enjoyed strong working relationships with them.

Van Buren held regular formal cabinet meetings and discontinued the informal gatherings of advisers that had attracted so much attention during Jackson's presidency. He solicited advice from department heads, tolerated open and even frank exchanges between cabinet members, perceiving himself as "a mediator, and to some extent an umpire between the conflicting opinions" of his counselors. Such detachment allowed the president to reserve judgment and protect his own prerogative for making final decisions. These open discussions gave cabinet members a sense of participation and made them feel part of a functioning entity, rather than isolated executive agents. Van Buren was closely involved in foreign affairs and matters pertaining to the Treasury Department, but the Post Office, War Department, and Navy Department all had possessed high levels of autonomy under their respective cabinet secretaries.

When Van Buren entered office, the nation's economic health had taken a turn for the worse and the prosperity of the early 1830s was over. Two months into his presidency, on May 10, 1837, some important state banks in New York, running out of hard currency reserves, refused to convert paper money into gold or silver, and other financial institutions throughout the nation quickly followed suit. This financial crisis would become known as the Panic of 1837. The Panic was followed by a five-year depression in which banks failed and unemployment reached record highs.

Van Buren blamed the economic collapse on greedy American and foreign business and financial institutions, as well as the over-extension of credit by U.S. banks. Whig leaders in Congress blamed the Democrats, along with Andrew Jackson's economic policies, specifically his 1836 Specie Circular. Cries of "rescind the circular!" went up and former president Jackson sent word to Van Buren asking him not to rescind the order, believing that it had to be given enough time to work. Others, like Nicholas Biddle, believed that Jackson's dismantling of the Bank of the United States was directly responsible for the irresponsible creation of paper money by the state banks which had precipitated this panic. The Panic of 1837 loomed large over the 1838 election cycle, as the carryover effects of the economic downturn led to Whig gains in both the U.S. House and Senate. The state elections in 1837 and 1838 were also disastrous for the Democrats, and the partial economic recovery in 1838 was offset by a second commercial crisis later that year.

To address the crisis, the Whigs proposed rechartering the national bank. The president countered by proposing the establishment of an independent U.S. treasury, which he contended would take the politics out of the nation's money supply. Under the plan, the government would hold all of its money balances in the form of gold or silver, and would be restricted from printing paper money at will; both measures were designed to prevent inflation. The plan would permanently separate the government from private banks by storing government funds in government vaults rather than in private banks. Van Buren announced his proposal in September 1837, but an alliance of conservative Democrats and Whigs prevented it from becoming law until 1840. As the debate continued, conservative Democrats like Rives defected to the Whig Party, which itself grew more unified in its opposition to Van Buren. The Whigs would abolish the Independent Treasury system in 1841, but it was revived in 1846, and remained in place until the passage of the Federal Reserve Act in 1913. More important for Van Buren's immediate future, the depression would be a major issue in his upcoming re-election campaign.

Federal policy under Jackson had sought to move Indian tribes to lands west of the Mississippi River through the Indian Removal Act of 1830, and the federal government negotiated 19 treaties with Indian tribes during Van Buren's presidency. The 1835 Treaty of New Echota signed by government officials and representatives of the Cherokee tribe had established terms under which the Cherokees ceded their territory in the southeast and agreed to move west to Oklahoma. In 1838, Van Buren directed General Winfield Scott to forcibly move all those who had not yet complied with the treaty.

The Cherokees were herded violently into internment camps where they were kept for the summer of 1838. The actual transportation west was delayed by intense heat and drought, but they were forcibly marched west in the fall. Under the treaty, the government was supposed to provide wagons, rations, and even medical doctors, but it did not. Some 20,000 people were relocated against their will during the Cherokee removal, part of the Trail of Tears. Notably, Ralph Waldo Emerson, who would go on to become America's foremost man of letters, wrote Van Buren a letter protesting his treatment of the Cherokee.

The administration also contended with the Seminole Indians, who engaged the army in a prolonged conflict known as the Second Seminole War. Prior to leaving office, Jackson put General Thomas Jesup in command of all military troops in Florida to force Seminole emigration to the West. Forts were established throughout the Indian territory, and mobile columns of soldiers scoured the countryside, and many Seminoles offered to surrender, including Chief Micanopy. The Seminoles slowly gathered for emigration near Tampa, but in June they fled the detention camps, driven off by disease and the presence of slave catchers hoping to capture Black Seminoles.

In December 1837, Jesup began a massive offensive, culminating in the Battle of Lake Okeechobee, and the war entered a new phase of attrition. During this time, the government realized that it would be almost impossible to drive the remaining Seminoles from Florida, so Van Buren sent General Alexander Macomb to negotiate peace with them. It was the only time that an Indian tribe had forced the government to sue for peace. An agreement was reached allowing the Seminoles to remain in southwest Florida, but the peace was shattered in July 1839 and was not restored until 1842, after Van Buren had left office.

Just before leaving office in March 1837, Andrew Jackson extended diplomatic recognition to the Republic of Texas, which had gained de facto independence from Mexico in the Texas Revolution. By suggesting the prospect of quick annexation, Jackson raised the danger of war with Mexico and heightened sectional tensions at home. New England abolitionists charged that there was a "slaveholding conspiracy to acquire Texas", and Daniel Webster eloquently denounced annexation. Many Southern leaders, meanwhile, strongly desired the expansion of slave-holding territory in the United States.

Boldly reversing Jackson's policies, Van Buren sought peace abroad and harmony at home. He proposed a diplomatic solution to a long-standing financial dispute between American citizens and the Mexican government, rejecting Jackson's threat to settle it by force. Likewise, when the Texas minister at Washington, D.C., proposed annexation to the administration in August 1837, he was told that the proposition could not be entertained. Constitutional scruples and fear of war with Mexico were the reasons given for the rejection, but concern that it would precipitate a clash over the extension of slavery undoubtedly influenced Van Buren and continued to be the chief obstacle to annexation. Northern and Southern Democrats followed an unspoken rule: Northerners helped quash anti-slavery proposals and Southerners refrained from agitating for the annexation of Texas. Texas withdrew the annexation offer in 1838.

British subjects in Lower Canada (now Quebec) and Upper Canada (now Ontario) rose in rebellion in 1837 and 1838, protesting their lack of responsible government. While the initial insurrection in Upper Canada ended quickly (following the December 1837 Battle of Montgomery's Tavern), many of the rebels fled across the Niagara River into New York, and Canadian leader William Lyon Mackenzie began recruiting volunteers in Buffalo. Mackenzie declared the establishment of the Republic of Canada and put into motion a plan whereby volunteers would invade Upper Canada from Navy Island on the Canadian side of the Niagara River. Several hundred volunteers traveled to Navy Island in the weeks that followed. They procured the steamboat "Caroline" to deliver supplies to Navy Island from Fort Schlosser. Seeking to deter an imminent invasion, British forces crossed to the American bank of the river in late December 1837, and they burned and sank the "Caroline". In the melee, one American was killed and others were wounded.

Considerable sentiment arose within the United States to declare war, and a British ship was burned in revenge. Van Buren, looking to avoid a war with Great Britain, sent General Winfield Scott to the Canada–United States border with large discretionary powers for its protection and its peace. Scott impressed upon American citizens the need for a peaceful resolution to the crisis, and made it clear that the U.S. government would not support adventuresome Americans attacking the British. Also, in early January 1838, the president proclaimed U.S. neutrality in the Canadian independence issue, a declaration which Congress endorsed by passing a neutrality law designed to discourage the participation of American citizens in foreign conflicts.

During the Canadian rebellions, Charles Duncombe and Robert Nelson helped foment a largely American militia, the Hunters' Lodge/Frères chasseurs. This militia carried out several attacks in Upper Canada between December 1837 and December 1838, collectively known as the Patriot War. The administration followed through on its enforcement of the Neutrality Act, encouraged the prosecution of filibusters, and actively deterred U.S. citizens from subversive activities abroad. In the long term, Van Buren's opposition to the Patriot War contributed to the construction of healthy Anglo-American and Canada–United States relations in the 20th century; it also led, more immediately, to a backlash among citizens regarding the seeming overreach of federal authority, which hurt congressional Democrats in the 1838 midterm elections.

A new crisis surfaced in late 1838, in the disputed territory on the Maine–New Brunswick frontier, where Americans were settling on long-disputed land claimed by the United States and Great Britain. Jackson had been willing to drop American claims to the region in return for other concessions, but Maine was unwilling to drop its claims to the disputed territory. The British considered possession of the area vital to the defense of Canada. Both American and New Brunswick lumberjacks cut timber in the disputed territory during the winter of 1838–1839. On December 29, New Brunswick lumbermen were spotted cutting down trees on an American estate near the Aroostook River.

After American woodcutters rushed to stand guard, a shouting match, known as the Battle of Caribou, ensued. Tensions quickly boiled over into a near war with both Maine and New Brunswick arresting each other's citizens. The crisis seemed ready to turn into an armed conflict. British troops began to gather along the Saint John River. Governor John Fairfield mobilized the state militia to confront the British in the disputed territory and several forts were constructed. The American press clamored for war; "Maine and her soil, or BLOOD!" screamed one editorial. "Let the sword be drawn and the scabbard thrown away!" In June, Congress authorized 50,000 troops and a $10 million budget in the event foreign military troops crossed into United States territory.

Van Buren was unwilling to go to war over the disputed territory, though he assured Maine that he would respond to any attacks by the British. To settle the crisis, Van Buren met with the British minister to the United States, and Van Buren and the minister agreed to resolve the border issue diplomatically. Van Buren also sent General Scott to the northern border area, both to show military resolve, and more importantly, to lower the tensions. Scott successfully convinced all sides to submit the border issue to arbitration. The border dispute was put to rest a few years later, with the signing of the 1842 Webster–Ashburton Treaty.

The "Amistad" case was a freedom suit that involved international issues and parties, as well as United States law, resulting from the rebellion of Africans on board the Spanish schooner "La Amistad" in 1839. Van Buren viewed abolitionism as the greatest threat to the nation's unity, and he resisted the slightest interference with slavery in the states where it existed. His administration supported the Spanish government's demand that the ship and its cargo (including the Africans) be turned over to them. A federal district court judge ruled that the Africans were legally free and should be transported home, but Van Buren's administration appealed the case to the Supreme Court.

In February 1840, former president John Quincy Adams argued passionately for the Africans' right to freedom, and Attorney General Henry D. Gilpin presented the government's case. In March 1841, the Supreme Court issued its final verdict: the "Amistad" Africans were free people and should be allowed to return home. The unique nature of the case heightened public interest in the saga, including the participation of former president Adams, Africans testifying in federal court, and their representation by prominent lawyers. The Amistad case drew attention to the personal tragedies of slavery and attracted new support for the growing abolition movement in the North. It also transformed the courts into the principal forum for a national debate on the legal foundations of slavery.

Van Buren appointed two Associate Justices to the Supreme Court: John McKinley, confirmed September 25, 1837, and Peter Vivian Daniel, confirmed March 2, 1841. He also appointed eight other federal judges, all to United States district courts.

For the first half of his presidency, Van Buren, who had been a widower for many years, did not have a specific person fill the role of White House hostess at administration social events, but tried to assume such duties himself. When his eldest son Abraham Van Buren married Angelica Singleton in 1838, he quickly acted to install his daughter-in-law as his hostess. She solicited the advice of her distant relative, Dolley Madison, who had moved back to Washington after her husband's death, and soon the president's parties livened up. After the 1839 New Year's Eve reception, the "Boston Post" raved: "[Angelica Van Buren is a] lady of rare accomplishments, very modest yet perfectly easy and graceful in her manners and free and vivacious in her conversation ... universally admired."

As the nation endured a deep economic depression, Angelica Van Buren's receiving style at receptions was influenced by her heavy reading on European court life (and her naive delight in being received as the "Queen of the United States" when she visited the royal courts of England and France after her marriage). Newspaper coverage of this, and the claim that she intended to re-landscape the White House grounds to resemble the royal gardens of Europe, was used in a political attack on her father-in-law by a Pennsylvania Whig Congressman Charles Ogle. He referred obliquely to her as part of the presidential "household" in his famous Gold Spoon Oration. The attack was delivered in Congress and the depiction of the president as living a royal lifestyle was a primary factor in his defeat for re-election.

Van Buren easily won renomination for a second term at the 1840 Democratic National Convention, but he and his party faced a difficult election in 1840. Van Buren's presidency had been a difficult affair, with the U.S. economy mired in a severe downturn, and other divisive issues, such as slavery, western expansion, and tensions with Great Britain, providing opportunities for Van Buren's political opponents—including some of his fellow Democrats—to criticize his actions. Although Van Buren's renomination was never in doubt, Democratic strategists began to question the wisdom of keeping Johnson on the ticket. Even former president Jackson conceded that Johnson was a liability and insisted on former House Speaker James K. Polk of Tennessee as Van Buren's new running mate. Van Buren was reluctant to drop Johnson, who was popular with workers and radicals in the North and added military experience to the ticket, which might prove important against likely Whig nominee William Henry Harrison. Rather than re-nominating Johnson, the Democratic convention decided to allow state Democratic Party leaders to select the vice-presidential candidates for their states.

Van Buren hoped that the Whigs would nominate Clay for president, which would allow Van Buren to cast the 1840 campaign as a clash between Van Buren's Independent Treasury system and Clay's support for a national bank. However, rather than nominating longtime party spokesmen like Clay and Daniel Webster, the 1839 Whig National Convention nominated Harrison, who had served in various governmental positions during his career and had earned fame for his military leadership in the Battle of Tippecanoe and the War of 1812. Whig leaders like William Seward and Thaddeus Stevens believed that Harrison's war record would effectively counter the popular appeals of the Democratic Party. For vice president, the Whigs nominated former Senator John Tyler of Virginia. Clay was deeply disappointed by his defeat at the convention, but he nonetheless threw his support behind Harrison.

Whigs presented Harrison as the antithesis of the president, whom they derided as ineffective, corrupt, and effete. Whigs also depicted Van Buren as an aristocrat living in high style in the White House, while they used images of Harrison in a log cabin sipping cider to convince voters that he was a man of the people. They threw such jabs as "Van, Van, is a used-up man" and "Martin Van Ruin" and ridiculed him in newspapers and cartoons. Issues of policy were not absent from the campaign; the Whigs derided the alleged executive overreaches of Jackson and Van Buren, while also calling for a national bank and higher tariffs. Democrats attempted to campaign on the Independent Treasury system, but the onset of deflation undercut these arguments. The enthusiasm for "Tippecanoe and Tyler Too", coupled with the country's severe economic crisis, made it impossible for Van Buren to win a second term. Harrison won by a popular vote of 1,275,612 to 1,130,033, and an electoral vote margin of 234 to 60. An astonishing 80% of eligible voters went to the polls on election day. Van Buren actually won more votes than he had in 1836, but the Whig success in attracting new voters more than canceled out Democratic gains. Additionally, Whigs won majorities for the first time in both the House of Representatives and the Senate.

On the expiration of his term, Van Buren returned to his estate of Lindenwald in Kinderhook. He continued to closely watch political developments, including the battle between Clay and President Tyler, who took office after Harrison's death in April 1841. Though undecided on another presidential run, Van Buren made several moves calculated to maintain his support, including a trip to the South and West during which he met with Jackson, former Speaker of the House James K. Polk, and others. President Tyler, James Buchanan, Levi Woodbury, and others loomed as potential challengers for the 1844 Democratic nomination, but it was Calhoun who posed the most formidable obstacle.

Van Buren remained silent on major public issues like the debate over the Tariff of 1842, hoping to arrange for the appearance of a draft movement for his presidential candidacy. President John Tyler made annexation of Texas his chief foreign policy goal, and many Democrats, particularly in the South, were anxious to quickly complete the annexation of Texas. After an explosion on the killed Secretary of State Abel P. Upshur in February 1844, Tyler brought Calhoun into his cabinet to direct foreign affairs. Like Tyler, Calhoun pursued the annexation of Texas to upend the presidential race and to extend slavery into new territories.

Shortly after taking office, Secretary of State Calhoun negotiated an annexation treaty between the United States and Texas. Van Buren had hoped he would not have to take a public stand on annexation, but as the Texas question came to dominate U.S. politics, he decided to make his views on the issue public. Though he believed that his public acceptance of annexation would likely help him win the 1844 Democratic nomination, Van Buren thought that annexation would inevitably lead to an unjust war with Mexico. In a public letter published shortly after Henry Clay also announced his opposition to the annexation treaty, Van Buren articulated his views on the Texas question.

Van Buren's opposition to immediate annexation cost him the support of many pro-slavery Democrats. In the weeks before the 1844 Democratic National Convention, Van Buren's supporters anticipated that he would win a majority of the delegates on the first presidential ballot, but would not be able to win the support of the required two-thirds of delegates. Van Buren's supporters attempted to prevent the adoption of the two-thirds rule, but several Northern delegates joined with Southern delegates in implementing the two-thirds rule for the 1844 convention. Van Buren won 146 of the 266 votes on the first presidential ballot, with only 12 of his votes coming from Southern states.

Senator Lewis Cass won much of the remaining vote, and he gradually picked up support on subsequent ballots until the convention adjourned for the day. When the convention reconvened and held another ballot, James K. Polk, who shared many of Van Buren's views but favored immediate annexation, won 44 votes. On the ninth and final ballot of the convention, Van Buren's supporters withdrew the former president's name from consideration, and Polk won the Democratic presidential nomination. Although angered that his opponents had denied him in the nomination, Van Buren endorsed Polk in the interest of party unity. He also convinced Silas Wright to run for Governor of New York so that the popular Wright could help boost Polk in the state. Wright narrowly defeated Whig nominee Millard Fillmore in the 1844 gubernatorial election, and Wright's victory in the state helped Polk narrowly defeat Henry Clay in the 1844 presidential election.

After taking office, Polk used George Bancroft as an intermediary to offer Van Buren the ambassadorship to London. Van Buren declined, partly because he was upset with Polk over the treatment the Van Buren delegates had received at the 1844 convention, and partly because he was content in his retirement. Polk also consulted Van Buren in the formation of his cabinet, but offended Van Buren by offering to appoint a New Yorker only to the lesser post of Secretary of War, rather than as Secretary of State or Secretary of the Treasury. Other patronage decisions also angered Van Buren and Wright, and they became permanently alienated from the Polk administration.

Though he had previously helped maintain a balance between the Barnburners and Hunkers, the two factions of the New York Democratic Party, Van Buren moved closer to the Barnburners after the 1844 Democratic National Convention. The split in the state party worsened during the Polk's presidency, as his administration lavished patronage on the Hunkers. In his retirement, Van Buren also grew increasingly opposed to slavery.

As the Mexican–American War brought the debate over slavery in the territories to the forefront of American politics, Van Buren published an anti-slavery manifesto. In it, he refuted the notion that Congress did not have the power to regulate slavery in the territories, and argued the Founding Fathers had favored the eventual abolition of slavery. The document, which became known as the "Barnburner Manifesto," was edited at Van Buren's request by John Van Buren and Samuel Tilden, both of whom were leaders of the Barnburner faction. After the publication of the Barnburner Manifesto, many Barnburners urged the former president to seek his old office in the 1848 presidential election. The 1848 Democratic National Convention seated competing Barnburner and Hunker delegations from New York, but the Barnburners walked out of the convention when Lewis Cass, who opposed congressional regulation of slavery in the territories, was nominated on the fourth ballot.

In response to the nomination of Cass, the Barnburners began to organize as a third party. At a convention held in June 1848, in Utica, New York, the Barnburners nominated Van Buren for president. Though reluctant to bolt from the Democratic Party, Van Buren accepted the nomination to show the power of the anti-slavery movement, help defeat Cass, and weaken the Hunkers. At a convention held in Buffalo, New York in August 1848, a group of anti-slavery Democrats, Whigs, and members of the abolitionist Liberty Party met in the first national convention of what became known as the Free Soil Party.

The convention unanimously nominated Van Buren, and chose Charles Francis Adams as Van Buren's running mate. In a public message accepting the nomination, Van Buren gave his full support for the Wilmot Proviso, a proposed law that would ban slavery in all territories acquired from Mexico in the Mexican–American War. Van Buren won no electoral votes, but finished second to Whig nominee Zachary Taylor in New York, taking enough votes from Cass to give the state—and perhaps the election—to Taylor. Nationwide, Van Buren won 10.1% of the popular vote, the strongest showing by a third party presidential nominee up to that point in U.S. history.

Van Buren never sought public office again after the 1848 election, but he continued to closely follow national politics. He was deeply troubled by the stirrings of secessionism in the South and welcomed the Compromise of 1850 as a necessary conciliatory measure despite his opposition to the Fugitive Slave Act of 1850. Van Buren also worked on a history of American political parties and embarked on a tour of Europe, becoming the first former American head of state to visit Britain. Though still concerned about slavery, Van Buren and his followers returned to the Democratic fold, partly out of the fear that a continuing Democratic split would help the Whig Party. He also attempted to reconcile the Barnburners and the Hunkers, with mixed results.

Van Buren supported Franklin Pierce for president in 1852, James Buchanan in 1856, and Stephen A. Douglas in 1860. Van Buren viewed the fledgling Know Nothing movement with contempt and felt that the anti-slavery Republican Party exacerbated sectional tensions. He considered Chief Justice Roger Taney's decision in the 1857 case of "Dred Scott v. Sandford" to be a "grievous mistake" since it overturned the Missouri Compromise. He believed that the Buchanan administration handled the issue of Bleeding Kansas poorly, and saw the Lecompton Constitution as a sop to Southern extremists.

After the election of Abraham Lincoln and the secession of several Southern states in 1860, Van Buren unsuccessfully sought to call a constitutional convention. In April 1861, former president Pierce wrote to the other living former presidents and asked them to consider meeting to use their stature and influence to propose a negotiated end to the war. Pierce asked Van Buren to use his role as the senior living ex-president to issue a formal call. Van Buren's reply suggested that Buchanan should be the one to call the meeting, since he was the former president who had served most recently, or that Pierce should issue the call himself if he strongly believed in the merit of his proposal. Neither Buchanan nor Pierce was willing to make Pierce's proposal public, and nothing more resulted from it. Once the American Civil War began, Van Buren made public his support for the Union.

Van Buren's health began to fail later in 1861, and he was bedridden with pneumonia during the fall and winter of 1861–1862. He died of bronchial asthma and heart failure at his Lindenwald estate at 2:00 a.m. on July 24, 1862, at 79. He is buried in the Kinderhook Reformed Dutch Church Cemetery, as are his wife Hannah, his parents, and his son Martin Van Buren Jr.

Van Buren outlived all four of his immediate successors: Harrison, Tyler, Polk, and Taylor.

Van Buren's most lasting achievement was as a political organizer who built the Democratic Party and guided it to dominance in the Second Party System, and historians have come to regard Van Buren as integral to the development of the American political system. According to historian Robert Remini:

However, his presidency is considered to be average, at best, by historians. He was blamed for the economic troubles and was defeated for reelection. His tenure was dominated by the economic disaster of the Panic of 1837, and historians have split on the adequacy of the Independent Treasury as a response to that issue. Several writers have portrayed Van Buren as among the nation's most obscure presidents. As noted in a 2014 "Time" magazine article on the "Top 10 Forgettable Presidents":

Van Buren's home in Kinderhook, New York, which he called Lindenwald, is now the Martin Van Buren National Historic Site. Counties are named for Van Buren in Michigan, Iowa, Arkansas, and Tennessee. Mount Van Buren, , three state parks and numerous towns were named after him.

During the 1988 presidential campaign, George H. W. Bush, a Yale University graduate and member of the Skull and Bones secret society, was attempting to become the first incumbent vice president to win election to the presidency since Van Buren. In the comic strip "Doonesbury", artist Garry Trudeau depicted members of Skull and Bones as attempting to rob Van Buren's grave, apparently intending to use the relics in a ritual that would aid Bush in the election.

Van Buren is portrayed by Nigel Hawthorne in the 1997 film "Amistad". The film depicts the legal battle surrounding the status of slaves who in 1839 rebelled against their transporters on "La Amistad" slave ship. On the television show "Seinfeld", the episode "The Van Buren Boys" is about a fictional street gang that admires Van Buren and bases its rituals and symbols on him, including the hand sign of eight fingers pointing up.

Also, in an episode of "The Monkees", "Dance, Monkee, Dance", a dance instruction studio offers free lessons to anyone who can answer the question, "Who was the eighth president of the United States?" Martin Van Buren appears at the studio to claim the prize.








</doc>
<doc id="19765" url="https://en.wikipedia.org/wiki?curid=19765" title="Melbourne Cricket Ground">
Melbourne Cricket Ground

The Melbourne Cricket Ground (MCG), also known simply as "The G", is an Australian sports stadium located in Yarra Park, Melbourne, Victoria. Founded and managed by the Melbourne Cricket Club, it is the largest stadium in the Southern Hemisphere, the 11th largest globally, and the second largest cricket ground by capacity after Motera Stadium. The MCG is within walking distance of the city centre and is served by Richmond and Jolimont railway stations, as well as the route 70 tram. It is adjacent to Melbourne Park and is part of the Melbourne Sports and Entertainment Precinct.

Since it was built in 1853, the MCG has undergone numerous renovations. It served as the centrepiece stadium of the 1956 Summer Olympics, the 2006 Commonwealth Games and two Cricket World Cups: 1992 and 2015. Noted for its role in the development of international cricket, the MCG hosted both the first Test match and the first One Day International, played between Australia and England in 1877 and 1971 respectively. It has also maintained strong ties with Australian rules football since its codification in 1859, and has become the principal venue for Australian Football League (AFL) matches, including the AFL Grand Final, the world's highest attended league championship event.

Home to the National Sports Museum, the MCG has hosted other major sporting events, including international rules football matches between Australia and Ireland, international rugby union matches, State of Origin (rugby league) games, and FIFA World Cup qualifiers. Concerts and other cultural events are also held at the venue with the record attendance standing at 143,750 for a Billy Graham evangelistic crusade in 1959. Grandstand redevelopments and occupational health and safety legislation have limited the maximum seating capacity to approximately 95,000 with an additional 5,000 standing room capacity, bringing the total capacity to 100,024.

The MCG is listed on the Victorian Heritage Register and was included on the Australian National Heritage List in 2005. Journalist Greg Baum called it "a shrine, a citadel, a landmark, a totem" that "symbolises Melbourne to the world".

Founded in November 1838 the Melbourne Cricket Club (MCC) selected the current MCG site in 1853 after previously playing at several grounds around Melbourne. The club's first game was against a military team at the Old Mint site, at the corner of William and Latrobe Streets. Burial Hill (now Flagstaff Gardens) became its home ground in January 1839, but the area was already set aside for Botanical Gardens and the club was moved on in October 1846, to an area on the south bank of the Yarra about where the Herald and Weekly Times building is today. The area was subject to flooding, forcing the club to move again, this time to a ground in South Melbourne.

It was not long before the club was forced out again, this time because of the expansion of the railway. The South Melbourne ground was in the path of Victoria's first steam railway line from Melbourne to Sandridge (now Port Melbourne). Governor La Trobe offered the MCC a choice of three sites; an area adjacent to the existing ground, a site at the junction of Flinders and Spring Streets or a ten-acre (about 4 hectares) section of the Government Paddock at Richmond next to Richmond Park.

This last option, which is now Yarra Park, had been used by Aborigines until 1835. Between 1835 and the early 1860s it was known as the Government or Police Paddock and served as a large agistment area for the horses of the Mounted Police, Border Police and Native Police. The north-eastern section also housed the main barracks for the Mounted Police in the Port Phillip district. In 1850 it was part of a stretch set aside for public recreation extending from Governor La Trobe's Jolimont Estate to the Yarra River. By 1853 it had become a busy promenade for Melbourne residents.

An MCC sub-committee chose the Richmond Park option because it was level enough for cricket but sloped enough to prevent inundation. That ground was located where the Richmond, or outer, end of the current MCG is now.

At the same time the Richmond Cricket Club was given occupancy rights to six acres (2.4 hectares) for another cricket ground on the eastern side of the Government Paddock.

At the time of the land grant the Government stipulated that the ground was to be used for cricket and cricket only. This condition remained until 1933 when the State Government allowed the MCG's uses to be broadened to include other purposes when not being used for cricket.

In 1863 a corridor of land running diagonally across Yarra Park was granted to the Hobson's Bay Railway and divided Yarra Park from the river. The Mounted Police barracks were operational until the 1880s when it was subdivided into the current residential precinct bordered by Vale Street. The area closest to the river was also developed for sporting purposes in later years including Olympic venues in 1956.

The first grandstand at the MCG was the original wooden members' stand built in 1854, while the first public grandstand was a 200-metre long 6000-seat temporary structure built in 1861. Another grandstand seating 2000, facing one way to the cricket ground and the other way to the park where football was played, was built in 1876 for the 1877 visit of James Lillywhite's English cricket team. It was during this tour that the MCG hosted the world's first Test match.

In 1881 the original members' stand was sold to the Richmond Cricket Club for £55. A new brick stand, considered at the time to be the world's finest cricket facility, was built in its place. The foundation stone was laid by Prince George of Wales and Prince Albert Victor on 4 July and the stand opened in December that year. It was also in 1881 that a telephone was installed at the ground, and the wickets and goal posts were changed from an east-west orientation to north-south. In 1882 a scoreboard was built which showed details of the batsman's name and how he was dismissed.

When the Lillywhite tour stand burnt down in 1884 it was replaced by a new stand which seated 450 members and 4500 public. In 1897, second-storey wings were added to 'The Grandstand', as it was known, increasing capacity to 9,000. In 1900 it was lit with electric light.
More stands were built in the early 20th century. An open wooden stand was on the south side of the ground in 1904 and the 2084-seat Grey Smith Stand (known as the New Stand until 1912) was erected for members in 1906. The 4000-seat Harrison Stand on the ground's southern side was built in 1908 followed by the 8000-seat Wardill Stand in 1912. In the 15 years after 1897 the stand capacity at the ground increased to nearly 20,000.

In 1927 the second brick members' stand was replaced at a cost of £60,000. The Harrison and Wardill Stands were demolished in 1936 to make way for the Southern Stand which was completed in 1937. The Southern Stand seated 18,200 under cover and 13,000 in the open and was the main public area of the MCG. The maximum capacity of the ground under this configuration, as advised by the Health Department, was 84,000 seated and 94,000 standing.

The Northern Stand, also known as the Olympic Stand, was built to replace the old Grandstand for the 1956 Olympic Games. By Health Department regulations, this was to increase the stadium's capacity to 120,000; although this was revised down after the 1956 VFL Grand Final, which could not comfortably accommodate its crowd of 115,802. Ten years later, the Grey Smith Stand and the open concrete stand next to it were replaced by the Western Stand; the Duke of Edinburgh laid a foundation stone for the Western Stand on 3 March 1967, and it was completed in 1968; in 1986, it was renamed the Ponsford Stand in honour of Victorian batsman Bill Ponsford. This was the stadium's highest capacity configuration, and the all-time record crowd for a sporting event at the venue of 121,696 was set under this configuration in the 1970 VFL Grand Final.

The MCG was the home of Australia's first full colour video scoreboard, which replaced the old scoreboard in 1982, located on Level 4 of the Western Stand, which notably caught fire in 1999 and was replaced in 2000. A second video screen added in 1994 almost directly opposite, on Level 4 of the Olympic stand. In 1985, light towers were installed at the ground, allowing for night football and day-night cricket games.
In 1988 inspections of the old Southern Stand found concrete cancer and provided the opportunity to replace the increasingly run-down 50-year-old facility. The projected cost of $100 million was outside what the Melbourne Cricket Club could afford so the Victorian Football League took the opportunity to part fund the project in return for a 30-year deal to share the ground. The new Great Southern Stand was completed in 1992, in time for the 1992 Cricket World Cup, at a final cost of $150 million.
The 1928 Members' stand, the 1956 Olympic stand and the 1968 Ponsford stand were demolished one by one between late 2003 to 2005 and replaced with a new structure in time for the 2006 Commonwealth Games. Despite now standing as a single unbroken stand, the individual sections retain the names of Ponsford, Olympic and Members Stands. The redevelopment cost exceeded 400 million and pushed the ground's capacity to just above 100,000. Since redevelopment, the highest attendance was the 2018 Grand Final of the AFL with 100,022, followed by 100,021 in the 2017 Grand Final.

From 2011 until 2013, the Victorian Government and the Melbourne Cricket Club funded a $55 million refurbishment of the facilities of Great Southern Stand, including renovations to entrance gates and ticket outlets, food and beverage outlets, "etc.", without significantly modifying the stand. New scoreboards, more than twice the size of the original ones, were installed in the same positions in late 2013.

From November 2019 until February 2020 all the playing field lights, including those in the light towers, were replaced with LED sports lighting with the lighting under the roof and in two of the light towers completed in time for the Boxing Day Test against New Zealand.

The first cricket match at the venue was played on 30 September 1854, while the first inter-colonial cricket match to be played at the MCG was between Victoria and New South Wales in March 1856. Victoria had played Tasmania (then known as Van Diemen's Land) as early as 1851 but the Victorians had included two professionals in the 1853 team upsetting the Tasmanians and causing a cooling of relations between the two colonies. To replace the disgruntled Tasmanians the Melbourne Cricket Club issued a challenge to play any team in the colonies for £1000. Sydney publican William Tunks accepted the challenge on behalf of New South Wales although the Victorians were criticised for playing for money. Ethics aside, New South Wales could not afford the £1000 and only managed to travel to Melbourne after half the team's travel cost of £181 was put up by Sydney barrister Richard Driver.

The game eventually got under way on 26 March 1856. The Victorians, stung by criticism over the £1000 stake, argued over just about everything; the toss, who should bat first, whether different pitches should be used for the different innings and even what the umpires should wear.

Victoria won the toss but New South Wales captain George Gilbert successfully argued that the visiting team should decide who bats first. The MCG was a grassless desert and Gilbert, considering players fielded without boots, promptly sent Victoria into bat. Needing only 16 to win in the final innings, New South Wales collapsed to be 5 for 5 before Gilbert's batting saved the game and the visitors won by three wickets.

In subsequent years conditions at the MCG improved but the ever-ambitious Melburnians were always on the lookout for more than the usual diet of club and inter-colonial games. In 1861, Felix William Spiers and Christopher Pond, the proprietors of the Cafe de Paris in Bourke Street and caterers to the MCC, sent their agent, W.B. Mallam, to England to arrange for a cricket team to visit Australia.

Mallam found a team and, captained by Heathfield Stephenson, it arrived in Australia on Christmas Eve 1861 to be met by a crowd of more than 3000 people. The team was taken on a parade through the streets wearing white-trimmed hats with blue ribbons given to them for the occasion. Wherever they went they were mobbed and cheered by crowds to the point where the tour sponsors had to take them out of Melbourne so that they could train undisturbed.

Their first game was at the MCG on New Year's Day 1862, against a Victorian XVIII. The Englishmen also wore coloured sashes around their waists to identify each player and were presented with hats to shade them from the sun. Some estimates put the crowd at the MCG that day at 25,000. It must have been quite a picture with a new 6000 seat grandstand, coloured marquees ringing the ground and a carnival outside. Stephenson said that the ground was better than any in England. The Victorians however, were no match for the English at cricket and the visitors won by an innings and 96 runs.
Over the four days of the 'test' more than 45,000 people attended and the profits for Speirs and Pond from this game alone was enough to fund the whole tour. At that time it was the largest number of people to ever watch a cricket match anywhere in the world. Local cricket authorities went out of their way to cater for the needs of the team and the sponsors. They provided grounds and sponsors booths without charge and let the sponsors keep the gate takings. The sponsors however, were not so generous in return. They quibbled with the Melbourne Cricket Club about paying £175 for damages to the MCG despite a prior arrangement to do so.

The last match of the tour was against a Victorian XXII at the MCG after which the English team planted an elm tree outside the ground.

Following the success of this tour, a number of other English teams also visited in subsequent years. George Parr's side came out in 1863–64 and there were two tours by sides led by W.G. Grace. The fourth tour was led by James Lillywhite.

On Boxing Day 1866 an Indigenous Australian cricket team played at the MCG with 11,000 spectators against an MCC team. A few players in that match were in a later team that toured England in 1868. Some also played in three other matches at the ground before 1869.

Up until the fourth tour in 1877, led by Lillywhite, touring teams had played first-class games against the individual colonial sides, but Lillywhite felt that his side had done well enough against New South Wales to warrant a game against an All Australian team.

When Lillywhite headed off to New Zealand he left Melbourne cricketer John Conway to arrange the match for their return. Conway ignored the cricket associations in each colony and selected his own Australian team, negotiating directly with the players. Not only was the team he selected of doubtful representation but it was also probably not the strongest available as some players had declined to take part for various reasons. Demon bowler Fred Spofforth refused to play because wicket-keeper Billy Murdoch was not selected. Paceman Frank Allan was at Warnambool Agricultural Show and Australia's best all-rounder Edwin Evans could not get away from work. In the end only five Australian-born players were selected.

The same could be said for Lillywhite's team which, being selected from only four counties, meant that some of England's best players did not take part. In addition, the team had a rough voyage back across the Tasman Sea and many members had been seasick. The game was due to be played on 15 March, the day after their arrival, but most had not yet fully recovered. On top of that, wicket-keeper Ted Pooley was still in a New Zealand prison after a brawl in a Christchurch pub.

England was nonetheless favourite to win the game and the first ever Test match began with a crowd of only 1000 watching. The Australians elected Dave Gregory from New South Wales as Australia's first ever captain and on winning the toss he decided to bat.

Charles Bannerman scored an unbeaten 165 before retiring hurt. Sydney Cricket Ground curator, Ned Gregory, playing in his one and only Test for Australia, scored Test cricket's first duck. Australia racked up 245 and 104 while England scored 196 and 108 giving Australia victory by 45 runs. The win hinged on Bannerman's century and a superb bowling performance by Tom Kendall who took 7 for 55 in England's second innings.

A fortnight later there was a return game, although it was really more of a benefit for the English team. Australia included Spofforth, Murdoch and T.J.D. Cooper in the side but this time the honours went to England who won by four wickets.

Two years later Lord Harris brought another England team out and during England's first innings in the Test at the MCG, Fred Spofforth took the first hat-trick in Test cricket. He bagged two hauls of 6 for 48 and 7 for 62 in Australia's ten wicket win.

Through most of the 20th century, the Melbourne Cricket Ground was one of the two major Test venues in Australia (along with the Sydney Cricket Ground), and it would host one or two Tests in each summer in which Tests were played; since 1982, the Melbourne Cricket Ground has hosted one Test match each summer. Until 1979, the ground almost always hosted its match or one of its matches over the New Year, with the first day's play falling somewhere between 29 December and 1 January; in most years since 1980 and every year since 1995, its test has begun on Boxing Day, and it is now a standard fixture in the Australian cricket calendar and is known as the Boxing Day Test. The venue also hosts one-day international matches each year, and Twenty20 international matches most years. No other venue in Melbourne has hosted a Test, and Docklands Stadium is the only other venue to have hosted a limited-overs international.

The Victorian first-class team plays Sheffield Shield cricket at the venue during the season. Prior to Test cricket being played on Boxing Day, it was a long-standing tradition for Victoria to host New South Wales in a first-class match on Boxing Day. Victoria also played its limited overs matches at the ground. Since the introduction of the domestic Twenty20 Big Bash League (BBL) in 2011, the Melbourne Stars club has played its home matches at the ground. It is also the home ground of the Melbourne Stars Women team, which plays in the Women's Big Bash League (WBBL).

By the 1980s, the integral MCG pitch – grown from Merri Creek black soil – was considered the worst in Australia, in some matches exhibiting wildly inconsistent bounce which could see balls pass through as grubbers or rear dangerously high – a phenomenon which was put down to damage caused by footballers in winter and increased use for cricket during the summers of the 1970s. The integral pitch has since been removed and drop-in pitches have been used since 1996, generally offering consistent bounce and a fair balance between bat and ball.

The highest first class team score in history was posted at the MCG in the Boxing Day match against New South Wales in 1926–27. Victoria scored 1107 in two days, with Bill Ponsford scoring 352 and Jack Ryder scoring 295.

One of the most sensational incidents in Test cricket occurred at the MCG during the Melbourne test of the 1954–55 England tour of Australia. Big cracks had appeared in the pitch during a very hot Saturday's play and on the rest day Sunday, groundsman Jack House watered the pitch to close them up. This was illegal and the story was leaked by "The Age" newspaper. The teams agreed to finish the match and England won by 128 runs after Frank Tyson took 7 for 27 in the final innings.

An incident in the second Test of the 1960–61 series involved the West Indies player Joe Solomon being given out after his hat fell on the stumps after being bowled at by Richie Benaud. The crowd sided with the West Indies over the Australians.

Not only was the first Test match played at the MCG, the first One Day International match was also played there, on 5 January 1971, between Australia and England. The match was played on what was originally scheduled to have been the fifth day of a Test match, but the Test was abandoned after the first three days were washed out. Australia won the 40-over match by 5 wickets. The next ODI was played in August 1972, some 19 months later.

In March 1977, the Australian Cricket Board assembled 218 of the surviving 224 Australia-England players for a Test match to celebrate 100 years of Test cricket between the two nations. The match was the idea of former Australian bowler and MCC committee member Hans Ebeling who had been responsible for developing the cricket museum at the MCG. The match had everything. England's Derek Randall scored 174, Australia's Rod Marsh also got a century, Lillee took 11 wickets, and David Hookes, in his first test, smacked five fours in a row off England captain Tony Greig's bowling. Rick McCosker who opened for Australia suffered a fractured jaw after being hit by a sharply rising delivery. He left the field but came back in the second innings with his head swathed in bandages. Australia won the match by 45 runs, exactly the same margin as the first Test in 1877.
Another incident occurred on 1 February 1981 at the end of a one-day match between Australia and New Zealand. New Zealand, batting second, needed six runs off the last ball of the day to tie the game. Australian captain, Greg Chappell instructed his brother Trevor, who was bowling the last over, to send the last ball down underarm to prevent the New Zealand batsman, Brian McKechnie, from hitting the ball for six. Although not entirely in the spirit of the game, an underarm delivery was quite legal, so long as the arm was kept straight. The Laws of cricket have since been changed to prevent such a thing happening again. The incident has long been a sore point between Australia and New Zealand.

In February and March 1985 the Benson & Hedges World Championship of Cricket was played at the MCG, a One Day International tournament involving all of the then Test match playing countries to celebrate 150 years of the Australian state of Victoria. Some matches were also played at Sydney Cricket Ground.

The MCG hosted the 1992 Cricket World Cup Final between Pakistan and England with a crowd of more than 87,000. Pakistan won the match after an all-round performance by Wasim Akram who scored 33 runs and picked up 3 crucial wickets to make Pakistan cricket world champions for the first and as yet only time.

During the 1995 Boxing Day Test at the MCG, Australian umpire Darrell Hair called Sri Lankan spin bowler Muttiah Muralitharan for throwing the ball, rather than bowling it, seven times during the match. The other umpires did not call him once and this caused a controversy, although he was later called for throwing by other umpires seven other times in different matches.

The MCG is known for its great atmosphere, much of which is generated in the infamous Bay 13, situated almost directly opposite to the members stand. In the late 1980s, the crowd at Bay 13 would often mimic the warm up stretches performed by Merv Hughes. In a 1999 One-Day International, the behaviour of Bay 13 was so bad that Shane Warne, donning a helmet for protection, had to enter the ground from his dressing rooms and tell the crowd to settle down at the request of opposing England captain Alec Stewart.
The MCG hosted three pool games as part of the 2015 ICC Cricket World Cup as well as a quarter-final, and then the final on 29 March. Australia comfortably defeated New Zealand by seven wickets in front of an Australian record cricket crowd of 93,013.

In 2017-18 Ashes series, Alastair Cook scored the highest score by an English batsman and second double century since Wally Hammond at the ground. Steve Smith scored his 4th consecutive century at the ground (2014-2017) in reply, being the only player since Don Bradman (1928–31) to do so. Smith also lasted 1093 days, or scored 455 runs, between two wickets fallen. The match ended in a draw, dashing hopes of Australia achieving the third Ashes sweep in the 21st Century. The wicket used for the Boxing Day test was the first Australian wicket ever to be rated 'poor' by the ICC.

The 2020 ICC Women's T20 World Cup Final was held on International Women's Day between Australia and India. Australia won convincingly by 85 runs in front of a record crowd for women's cricket of 86 174.

Despite being called the Melbourne Cricket Ground, the stadium has been and continues to be used much more often for Australian rules football. Spectator numbers for football are larger than for any other sport in Australia, and it makes more money for the MCG than any of the other sports played there.

Although the Melbourne Cricket Club members were instrumental in founding Australian Rules Football, there were understandable concerns in the early days about the damage that might be done to the playing surface if football was allowed to be played at the MCG. Therefore, football games were often played in the parklands next to the cricket ground, and this was the case for the first documented football match to be played at the ground. The match which today is considered to be the first Australian rules football, played between Melbourne Grammar and Scotch College over three Saturdays beginning 7 August 1858 was played in this area.

It wasn't until 1869 that football was played on the MCG proper, a trial game involving a police team. It was not for another ten years, in 1879, after the formation of the Victorian Football Association, that the first official match was played on the MCG and the cricket ground itself became a regular venue for football. Two night matches were played on the ground during the year under the newly invented electric light.

In the early years, the MCG was the home ground of Melbourne Football Club, Australia's oldest club, established in 1858 by the founder of the game itself, Thomas Wills. Melbourne won five premierships during the 1870s using the MCG as its home ground.

The first of nearly 3000 Victorian Football League/Australian Football League games to be played at the MCG was on 15 May 1897, with beating 64 to 19.

Several Australian Football League (AFL) clubs later joined Melbourne in using the MCG as their home ground for matches: (1965), (1985), (1992), (started moving in 1994, became a full-time tenant in 2000) and (2000). Melbourne used the venue as its training base until 1984, before being required to move to preserve the venue's surface when North Melbourne began playing there.

Essendon and Carlton won the most VFL/AFL premierships with 16 premierships each. The VFL/AFL grand final has been played at the MCG every season since 1902, except in 1924 when no grand final was held because of the season's round-robin finals format (it hosted three of the six games in the finals series) 1942–1945, when the ground was used by the military during World War II; and in 1991 as the construction of the Great Southern Stand had temporarily reduced the ground's capacity below that of Waverley Park. All three grand final replays have been played at the MCG.
Before the MCG was fully seated, a grand final could draw attendances above 110,000. The record for the highest attendance in the history of the sport was set in the 1970 VFL Grand Final, with 121,696 in attendance.

Since being fully seated, grand final attendances are typically between 95,000 and 100,000, with the record of 100,022 in the 2018 grand final, followed by 100,021 at the 2017 AFL Grand Final.

In the modern era, most finals games held in Melbourne have been played at the MCG. Under the current contract, 10 finals (excluding the grand final) must be played at the MCG over a five-year period. Under previous contracts, the MCG was entitled to host at least one match in each week of the finals, which on several occasions required non-Victorian clubs to play "home" finals in Victoria. In 2018, the AFL, Victorian Government and Melbourne Cricket Club (MCC) announced that the MCG would continue to host the grand final until at least 2057.

All Melbourne-based teams (and most of the time Geelong) play their "home" finals at the MCG unless if four Victorian teams win the right to host a final in the first week of the finals.

For many years the VFL had an uneasy relationship with the MCG trustees and the Melbourne Cricket Club. Both needed the other, but resented the dependence. The VFL made the first move which brought things to a head by beginning the development of VFL Park at Mulgrave in the 1960s as its own home ground and as a potential venue for future grand finals. Then in 1983, president of the VFL, Allen Aylett started to pressure the MCG Trust to give the VFL a greater share of the money it made from using the ground for football.

In March 1983 the MCG trustees met to consider a submission from Aylett. Aylett said he wanted the Melbourne Cricket Club's share of revenue cut from 15 per cent to 10 per cent. He threatened to take the following day's opening game of the season, Collingwood vs Melbourne, away from the MCG. The money was held aside until an agreement could be reached.

Different deals, half deals and possible deals were done over the years, with the Premier of Victoria, John Cain, Jr., even becoming involved. Cain was said to have promised the VFL it could use the MCG for six months of the year and then hand it back to the MCC, but this never eventuated, as the MCG Trust did not approve it. In the mid-1980s, a deal was done where the VFL was given its own members area in the Southern Stand.

Against this background of political manoeuvring, in 1985 became the third club to make the MCG its home ground. In the same year, North played in the first night football match at the MCG for almost 110 years, against Collingwood on 29 March 1985.

In 1986, only a month after Ross Oakley had taken over as VFL Commissioner, VFL executives met with the MCC and took a big step towards resolving their differences. Changes in the personnel at the MCC also helped. In 1983 John Lill was appointed secretary and Don Cordner its president.

Shortly after the Southern Stand opened in 1992, the Australian Football League moved its headquarters into the complex. The AFL assisted with financing the new stand and came to an agreement that ensures at least 45 AFL games are played at the MCG each year, including the Grand Final in September. Another 45 days of cricket are also played there each year and more than 3.5 million spectators come to watch every year.

As of the end of 2011, Matthew Richardson holds the records for having scored the most goals on the MCG and Kevin Bartlett holds the record for playing the most matches. Two players have scored 14 goals for an AFL or VFL game in one match at the MCG, Gary Ablett, Sr. in 1989 and 1993 and John Longmire in 1990.

Before an AFL match between and on 27 August 1999, the city end scoreboard caught on fire due to an electrical fault, causing the start of play to be delayed by half an hour.

During World War II, the government requisitioned the MCG for military use. From 1942 until 1945 it was occupied by (in order): the United States Army Air Forces, the Royal Australian Air Force, the United States Marine Corps and again the RAAF. Over the course of the war, more than 200,000 personnel were barracked at the MCG. From April to October 1942, the US Army's Fifth Air Force occupied the ground, naming it "Camp Murphy", in honor of officer Colonel William Murphy, a senior USAAF officer killed in Java. In 1943 the MCG was home to the legendary First Regiment of the First Division of the United States Marine Corps. The First Marine Division were the heroes of the Guadalcanal campaign and used the "cricket grounds", as the marines referred to it, to rest and recuperate. On 14 March 1943 the marines hosted a giant "get together" of American and Australian troops on the arena.

In 1977, Melbourne Cricket Club president Sir Albert Chadwick and Medal of Honor recipient, Colonel Mitchell Paige, unveiled a commemorative plaque recognizing the Americans' time at the ground.

In episode 3 of the 2010 TV miniseries, "The Pacific", members of the US Marines are shown to be camped in the war-era MCG.

The MCG's most famous moment in history was as the main stadium for the 1956 Olympic Games, hosting the opening and closing ceremonies, track and field events, and the finals in field hockey and soccer. The MCG was only one of seven possible venues, including the Melbourne Showgrounds, for the Games' main arena. The MCG was the Federal Government's preferred venue but there was resistance from the MCC. The inability to decide on the central venue nearly caused the Games to be moved from Melbourne. Prime Minister Robert Menzies recognised the potential embarrassment to Australia if this happened and organised a three-day summit meeting to thrash things out. Attending was Victorian Premier John Cain, Sr., the Prime Minister, deputy opposition leader Arthur Calwell, all State political leaders, civic leaders, Olympic officials and trustees and officials of the MCC. Convening the meeting was no small effort considering the calibre of those attending and that many of the sports officials were only part-time amateurs.

As 22 November, the date of the opening ceremony, drew closer, Melbourne was gripped ever more tightly by Olympic fever. At 3 pm the day before the opening ceremony, people began to line up outside the MCG gates. That night the city was paralysed by a quarter of a million people who had come to celebrate.

The MCG's capacity was increased by the new Olympic (or Northern) Stand, and on the day itself 103,000 people filled the stadium to capacity. A young up and coming distance runner was chosen to carry the Olympic torch into the stadium for the opening ceremony.

Although Ron Clarke had a number of junior world records for distances of 1500 m, one mile (1.6 km) and two miles (3 km), he was relatively unknown in 1956. Perhaps the opportunity to carry the torch inspired him because he went on to have a career of exceptional brilliance and was without doubt the most outstanding runner of his day. At one stage he held the world record for every distance from two miles (3 km) to 20 km. His few failures came in Olympic and Commonwealth Games competition. Although favourite for the gold at Tokyo in 1964 he was placed ninth in the 5,000 metres race and the marathon and third in the 10,000 metres. He lost again in the 1966 Commonwealth Games and in 1968 at altitude in Mexico he collapsed at the end of the 10 km race. 

On that famous day in Melbourne in 1956 the torch spluttered and sparked, showering Clarke with hot magnesium, burning holes in his shirt. When he dipped the torch into the cauldron it burst into flame singeing him further. In the centre of the ground, John Landy, the fastest miler in the world, took the Olympic oath and sculler Merv Wood carried the Australian flag.

The Melbourne Games also saw the high point of Australian female sprinting with Betty Cuthbert winning three gold medals at the MCG. She won the 100 m and 200 m and anchored the winning 4 x 100 m team. Born in Merrylands in Sydney's west she was a champion schoolgirl athlete and had already broken the world record for the 200 m just before the 1956 Games. She was to be overshadowed by her Western Suburbs club member, the Marlene Matthews. When they got to the Games, Matthews was the overwhelming favourite especially for the 100 m a distance over which Cuthbert had beaten her just once.

Both Matthews and Cuthbert won their heats with Matthews setting an Olympic record of 11.5 seconds in hers. Cuthbert broke that record in the following heat with a time of 11.4 seconds. The world record of 11.3 was held by another Australian, Shirley Strickland who was eliminated in her heat. In the final Matthews felt she got a bad start and was last at the 50 metre mark. Cuthbert sensed Isabella Daniels from the USA close behind her and pulled out a little extra to win Australia's first gold at the Games in a time of 11.5 seconds, Matthews was third. The result was repeated in the 200 m final. Cuthbert won her second gold breaking Marjorie Jackson's Olympic record. Mathews was third again.

By the time the 1956 Olympics came around, Shirley Strickland was a mother of 31 years of age but managed to defend her 80 m title, which she had won in Helsinki four years before, winning gold and setting a new Olympic record.

The sensational incident of the track events was the non-selection of Marlene Matthews in the 4 x 100 m relay. Matthews trained with the relay team up until the selection was made but Cuthbert, Strickland, Fleur Mellor and Norma Croker were picked for the team. There was outrage at the selection which increased when Matthews went on to run third in both the 100 m and 200 m finals. Personally she was devastated and felt that she had been overlooked for her poor baton change. Strickland was disappointed with the way Matthews was treated and maintained it was an opinion held in New South Wales that she had baton problems. One of the selectors, Doris Magee from NSW, said that selecting Matthews increased the risk of disqualification at the change. But Cuthbert maintained that the selectors made the right choice saying that Fleur Mellor was fresh, a specialist relay runner and was better around the curves than Matthews.

The men did not fare so well. The 4 x 400 m relay team, including later IOC Committee member Kevan Gosper, won silver. Charles Porter also won silver in the high jump. Hec Hogan won bronze in the 100 m to become the first Australian man to win a medal in a sprint since the turn of the century and despite injury John Landy won bronze in the 1500 m. Allan Lawrence won bronze in the 10,000 m event.

Apart from athletics, the stadium was also used for the soccer finals, the hockey finals, the Opening and Closing Ceremonies, and an exhibition game of baseball between the Australian National Team and a US armed services team at which an estimated crowd of 114,000 attended. This was the Guinness World Record for the largest attendance for any baseball game, which stood until a 29 March 2008 exhibition game between the Boston Red Sox and Los Angeles Dodgers at the Los Angeles Coliseum (also a former Olympic venue in 1932 and 1984) drawing 115,300.

The MCG was also used for another demonstration sport, Australian Rules. The Olympics being an amateur competition meant that only amateurs could play in the demonstration game. A combined team of amateurs from the VFL and VFA were selected to play a state team from the Victorian Amateur Football Association (VAFA). The game was played 7 December 1956 with the VAFA side, wearing white jumpers, green collars and the Olympic rings on their chests, winning easily 81 to 55. One of the players chosen for the VFA side was Lindsay Gaze (although he never got off the bench) who would go on to make his mark in another sport, basketball, rather than Australian Rules.

The MCG's link with its Olympic past continues to this day. Within its walls is the IOC-endorsed Australian Gallery of Sport and Olympic Museum.

Forty-four years later at the 2000 Summer Olympics in Sydney, the ground hosted several soccer preliminaries, making it one of a few venues ever used for more than one Olympics.

The Opening and Closing Ceremonies of the 2006 Commonwealth Games were held at the MCG, as well as athletics events during the games. The games began on 15 March and ended on 26 March.

The seating capacity of the stadium during the games was 80,000. A total of 47 events were contested, of which 24 by male and 23 by female athletes. Furthermore, three men's and three women's disability events were held within the programme. All athletics events took place within the Melbourne Cricket Ground, while the marathon and racewalking events took place on the streets of Melbourne and finished at the main stadium.

The hosts Australia easily won the medals table with 16 golds and 41 medals in total. Jamaica came second with 10 golds and 22 medals, while Kenya and England were the next best performers. A total of eleven Games records were broken over the course of the seven-day competition. Six of the records were broken by Australian athletes.

The first game of Rugby Union to be played on the ground was on Saturday, 29 June 1878, when the Waratah Club of Sydney played Carlton Football Club in a return of the previous year's contests in Sydney where the clubs had competed in both codes of football. The match, watched by a crowd of between 6,000 and 7,000 resulted in a draw; one goal and one try being awarded to each team.

The next Rugby match was held on Wednesday 29 June 1881, when the Wanderers, a team organised under the auspices of the Melbourne Cricket Club, played a team representing a detached Royal Navy squadron then visiting Melbourne. The squadron team won by one goal and one try to nil.
It was not until 19 August 1899 that the MCG was again the venue for a Union match, this time Victoria v the British Lions (as they were later to be called). During the preceding week the Victorians had held several trial and practice matches there, as well as several training sessions, despite which they were defeated
30–0 on the day before a crowd of some 7,000.

Nine years later, on Monday, 10 August 1908, Victoria was again the host, this time to the Australian team en route to Great Britain and soon to be dubbed the First Wallabies. Despite being held on a working day some 1,500 spectators attended to see the visitors win by 26–6.

On Saturday, 6 July 1912 the MCG was the venue, for the only time ever, of a match between two Victorian Rugby Union clubs, Melbourne and East Melbourne, the former winning 9–5 in what was reported to be ‘... one of the finest exhibitions of the Rugby game ever seen in Victoria.' It was played before a large crowd as a curtain raiser to a State Rules match against South Australia.

On Saturday 18 June 1921, in another curtain raiser, this time to a Melbourne-Fitzroy League game, a team representing Victoria was soundly beaten 51–0 by the South African Springboks in front of a crowd of 11,214.

It was nine years later, on Saturday 13 September 1930, that the British Lions returned to play Victoria, again before a crowd of 7,000, this time defeating the home side 41–36, a surprisingly narrow winning margin. 
The first post war match at the MCG was on 21 May 1949 when the NZ Maoris outclassed a Southern States side 35–8 before a crowd of close to 10,000. A year later, on 29 July 1950, for the first and only time, Queensland travelled to Victoria to play an interstate match, defeating their hosts 31–12 before a crowd of 7,479. 
In the following year the MCG was the venue for a contest between the New Zealand All Blacks and an Australian XV . This was on 30 June 1951 before some 9,000 spectators and resulted in a convincing 56–11 win for the visitors.

Union did not return the MCG until the late 1990s, for several night time Test matches, both Australia v New Zealand All Blacks as part of the Tri Nations Series. The first, on Saturday 26 July 1997, being notable for an attendance of 90,119, the visitors winning 33–18 and the second, on Saturday 11 July 1998, for a decisive victory to Australia of 24–16. Australia and New Zealand met again at the MCG during the 2007 Tri Nations Series on 30 June, the hosts again winning, this time by 20 points to 15 in front of a crowd of 79,322.

Rugby league was first played at the ground on 15 August 1914, with the New South Wales team losing to England 15–21.

The first ever State of Origin match at the MCG (and second in Melbourne) was Game II of the 1994 series, and the attendance of 87,161 set a new record rugby league crowd in Australia. The MCG was also the venue for Game II of the 1995 State of Origin series and drew 52,994, the most of any game that series. The second game of the 1997 State of Origin series, which, due to the Super League war only featured Australian Rugby League-signed players, was played there too, but only attracted 25,105, the lowest in a series that failed to attract over 35,000 to any game.

The Melbourne Storm played two marquee games at the MCG in 2000. This was the first time that they had played outside of their normal home ground of Olympic Park Stadium which held 18,500 people. Their first game was held on 3 March 2000 against the St. George Illawarra Dragons in a rematch of the infamous 1999 NRL Grand Final. Dragons player Anthony Mundine said the Storm were 'not worthy premiers' and they responded by running in 12 tries to two, winning 70–10 in front of 23,239 fans. This was their biggest crowd they had played against until 33,427 turned up to the 2007 Preliminary Final at Docklands Stadium which saw Melbourne defeat the Parramatta Eels 26–10. The record home and away crowd record has also been overhauled, when a match at Docklands in 2010 against St George attracted 25,480 spectators. Their second game attracted only 15,535 spectators and was up against the Cronulla Sharks on 24 June 2000. Once again, the Storm won 22–16.

It was announced in June 2014 that the ground would host its first State of Origin match since 1997. Game II of the 2015 series was played at the venue, with an all-time record State of Origin crowd of 91,513 attending the match. The attendance is 19th on the all time rugby league attendance list and the 4th highest rugby league attendance in Australia.

On 9 February 2006 Victorian premier Steve Bracks and Football Federation Australia chairman Frank Lowy announced that the MCG would host a world class soccer event each year from 2006 until 2009 inclusive.

The agreement sees an annual fixture at the MCG, beginning with a clash between Australia and European champions Greece on 25 May 2006 in front of a sell-out crowd of 95,103, before Australia left to contest in the World Cup finals. Australia beat Greece 1–0. The Socceroos also hosted a match in 2007 against Argentina, losing 1–0, as well as 2010 FIFA World Cup qualification matches in 2009 against Japan, which attracted 81,872 fans as Australia beat Japan 2–1 via 2 Tim Cahill headers after falling behind 1–0 late in the 1st half. In 2010 it was announced that as a warm up to the 2010 FIFA World Cup which the Australians had qualified for, they would play fellow qualified nation New Zealand on 24 May at the MCG.
Other matches played at the MCG include the following:

In 1878 the Melbourne Cricket Club's Lawn Tennis Committee laid an asphalt court at the MCG and Victoria's first game of tennis was played there. A second court of grass was laid in 1879 and the first Victorian Championship played on it in 1880. The first inter-colonial championship was played in 1883 and the first formal inter-state match between NSW and Victoria played in 1884 with Victoria winning.

In 1889 the MCC arranged for tennis to be played at the Warehousemen's Cricket Ground (now known as the Albert Cricket Ground), at Albert Park, rather than at the MCG.

It was at the MCG in 1869 that one of Australia's first bicycle races was held. The event was for velocipedes, crude wooden machines with pedals on the front wheels. In 1898 the Austral Wheel Race was held at the MCG attracting a crowd of 30,000 to see cyclists race for a total of £200 in prize money.









The Tattersall's Parade of the Champions undertaking is a gift to the people of Australia by Tattersall's and is a focal point of the Yarra Park precinct.

The MCG is a magnet for tourists worldwide and the statues reinforce the association between the elite sportsmen and women who have competed here and the stadium that rejoiced in their performances.
In 2010, the Melbourne Cricket Club (MCC) announced an expansion to the list of sporting statues placed around the MCG precinct in partnership with Australia Post.

The Australia Post Avenue of Legends project aimed to place a minimum of five statues in Yarra Park, extending from the gate 2 MCC members entrance up the avenue towards Wellington Parade. The most recent addition of Kevin Bartlett was unveiled in March 2017.


 

 


</doc>
<doc id="19766" url="https://en.wikipedia.org/wiki?curid=19766" title="Marshall Plan">
Marshall Plan

The Marshall Plan (officially the European Recovery Program, ERP) was an American initiative passed in 1948 for foreign aid to Western Europe. The United States transferred over $12 billion (equivalent to over $129 billion as of 2020) in economic recovery programs to Western European economies after the end of World War II. Replacing an earlier proposal for a Morgenthau Plan, it operated for four years beginning on April 3, 1948. The goals of the United States were to rebuild war-torn regions, remove trade barriers, modernize industry, improve European prosperity, and prevent the spread of Communism. The Marshall Plan required a reduction of interstate barriers, a dropping of many regulations, and encouraged an increase in productivity, as well as the adoption of modern business procedures.

The Marshall Plan aid was divided amongst the participant states roughly on a per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for the general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The largest recipient of Marshall Plan money was the United Kingdom (receiving about 26% of the total), followed by France (18%) and West Germany (11%). Some eighteen European countries received Plan benefits. Although offered participation, the Soviet Union refused Plan benefits, and also blocked benefits to Eastern Bloc countries, such as Hungary and Poland. The United States provided similar aid programs in Asia, but they were not part of the Marshall Plan.

Its role in the rapid recovery has been debated. The Marshall Plan's accounting reflects that aid accounted for about 3% of the combined national income of the recipient countries between 1948 and 1951, which means an increase in GDP growth of less than half a percent.

After World War II, in 1947, industrialist Lewis H. Brown wrote (at the request of General Lucius D. Clay) "A Report on Germany", which served as a detailed recommendation for the reconstruction of post-war Germany, and served as a basis for the Marshall Plan. The initiative was named after United States Secretary of State George Marshall. The plan had bipartisan support in Washington, where the Republicans controlled Congress and the Democrats controlled the White House with Harry S. Truman as President. The Plan was largely the creation of State Department officials, especially William L. Clayton and George F. Kennan, with help from the Brookings Institution, as requested by Senator Arthur H. Vandenberg, chairman of the Senate Foreign Relations Committee. Marshall spoke of an urgent need to help the European recovery in his address at Harvard University in June 1947. The purpose of the Marshall Plan was to aid in the economic recovery of nations after World War II and to reduce the influence of Communist parties within them. To combat the effects of the Marshall Plan, the USSR developed its own economic plan, known as the Molotov Plan, in spite of the fact that large amounts of resources from the Eastern Bloc countries to the USSR were paid as reparations, for countries participating in the Axis Power during the war.

The phrase "equivalent of the Marshall Plan" is often used to describe a proposed large-scale economic rescue program.

In 1951 the Marshall Plan was largely replaced by the Mutual Security Act.

The reconstruction plan, developed at a meeting of the participating European states, was drafted on June 5, 1947. It offered the same aid to the Soviet Union and its allies, but they refused to accept it, as doing so would allow a degree of US control over the communist economies. In fact, the Soviet Union prevented its satellite states (i.e., East Germany, Poland, etc.) from accepting. Secretary Marshall became convinced Stalin had no interest in helping restore economic health in Western Europe.
President Harry Truman signed the Marshall Plan on April 3, 1948, granting $5 billion in aid to 16 European nations. During the four years the plan was in effect, the United States donated $17 billion (equivalent to $ billion in ) in economic and technical assistance to help the recovery of the European countries that joined the Organisation for European Economic Co-operation. The $17 billion was in the context of a US GDP of $258 billion in 1948, and on top of $17 billion in American aid to Europe between the end of the war and the start of the Plan that is counted separately from the Marshall Plan. The Marshall Plan was replaced by the Mutual Security Plan at the end of 1951; that new plan gave away about $7.5 billion annually until 1961 when it was replaced by another program.

The ERP addressed each of the obstacles to postwar recovery. The plan looked to the future and did not focus on the destruction caused by the war. Much more important were efforts to modernize European industrial and business practices using high-efficiency American models, reducing artificial trade barriers, and instilling a sense of hope and self-reliance.

By 1952, as the funding ended, the economy of every participant state had surpassed pre-war levels; for all Marshall Plan recipients, output in 1951 was at least 35% higher than in 1938. Over the next two decades, Western Europe enjoyed unprecedented growth and prosperity, but economists are not sure what proportion was due directly to the ERP, what proportion indirectly, and how much would have happened without it.
A common American interpretation of the program's role in European recovery was expressed by Paul Hoffman, head of the Economic Cooperation Administration, in 1949, when he told Congress Marshall aid had provided the "critical margin" on which other investment needed for European recovery depended. The Marshall Plan was one of the first elements of European integration, as it erased trade barriers and set up institutions to coordinate the economy on a continental level—that is, it stimulated the total political reconstruction of Western Europe.

Belgian economic historian Herman Van der Wee concludes the Marshall Plan was a "great success":

By the end of World War II, much of Europe was devastated. Sustained aerial bombardment during the war had badly damaged most major cities, and industrial facilities were especially hard-hit. Millions of refugees were in temporary camps. The region's trade flows had been thoroughly disrupted; millions were in refugee camps living on aid from the United States, which was provided in the guise of the United Nations Relief and Rehabilitation Administration and other agencies. Food shortages were severe, especially in the harsh winter of 1946–47. From July 1945 through June 1946, the United States shipped 16.5 million tons of food, primarily wheat, to Europe and Japan. It amounted to one-sixth of the American food supply and provided 35 trillion calories, enough to provide 400 calories a day for one year to 300 million people.

Especially damaged was transportation infrastructure, as railways, bridges, and docks had been specifically targeted by airstrikes, while much merchant shipping had been sunk. Although most small towns and villages had not suffered as much damage, the destruction of transportation left them economically isolated. None of these problems could be easily remedied, as most nations engaged in the war had exhausted their treasuries in the process.

The only major powers whose infrastructure had not been significantly harmed in World War II were the United States and Canada. They were much more prosperous than before the war but exports were a small factor in their economy. Much of the Marshall Plan aid would be used by the Europeans to buy manufactured goods and raw materials from the United States and Canada.

Most of Europe's economies were recovering slowly, as unemployment and food shortages led to strikes and unrest in several nations. Agricultural production was 83% of 1938 levels, industrial production was 88%, and exports 59%. Exceptions were the United Kingdom, the Netherlands and France, where by the end of 1947 production had already been restored to pre-war levels before the Marshall Plan. Italy and Belgium would follow by the end of 1948.

In Germany in 1945–46 housing and food conditions were bad, as the disruption of transport, markets, and finances slowed a return to normality. In the West, the bombing had destroyed 5,000,000 houses and apartments, and 12,000,000 refugees from the east had crowded in. Food production was two-thirds of the pre-war level in 1946–48, while normal grain and meat shipments no longer arrived from the East. The drop in food production can be attributed to a drought that killed a major portion of the wheat crop while a severe winter destroyed the majority of the wheat crop the following year. This caused most Europeans to rely on a 1,500 calorie per day diet. Furthermore, the large shipments of food stolen from occupied nations during the war no longer reached Germany. Industrial production fell more than half and reached pre-war levels at the end of 1949.

While Germany struggled to recover from the destruction of the War, the recovery effort began in June 1948, moving on from emergency relief. The currency reform in 1948 was headed by the military government and helped Germany to restore stability by encouraging production. The reform revalued old currency and deposits and introduced new currency. Taxes were also reduced and Germany prepared to remove economic barriers.

During the first three years of occupation of Germany, the UK and US vigorously pursued a military disarmament program in Germany, partly by removal of equipment but mainly through an import embargo on raw materials, part of the Morgenthau Plan approved by President Franklin D. Roosevelt.

Nicholas Balabkins concludes that "as long as German industrial capacity was kept idle the economic recovery of Europe was delayed." By July 1947 Washington realized that economic recovery in Europe could not go forward without the reconstruction of the German industrial base, deciding that an "orderly, prosperous Europe requires the economic contributions of a stable and productive Germany." In addition, the strength of Moscow-controlled communist parties in France and Italy worried Washington.

In the view of the State Department under President Harry S Truman, the United States needed to adopt a definite position on the world scene or fear losing credibility. The emerging doctrine of containment (as opposed to rollback) argued that the United States needed to substantially aid non-communist countries to stop the spread of Soviet influence. There was also some hope that the Eastern Bloc nations would join the plan, and thus be pulled out of the emerging Soviet bloc, but that did not happen.

In January 1947, Truman appointed retired General George Marshall as Secretary of State. In July 1947 Marshall scrapped Joint Chiefs of Staff Directive 1067, which was based on the Morgenthau Plan which had decreed "take no steps looking toward the economic rehabilitation of Germany [or] designed to maintain or strengthen the German economy." The new plan JCS 1779 stated that "an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany." The restrictions placed on German heavy industry production were partly ameliorated; permitted steel production levels were raised from 25% of pre-war capacity to a new limit placed at 50% of pre-war capacity.

With a threatening Greece, and Britain financially unable to continue its aid, the President announced his Truman Doctrine on March 12, 1947, "to support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures", with an aid request for consideration and decision, concerning Greece and Turkey. Herbert Hoover noted that "The whole economy of Europe is interlinked with German economy through the exchange of raw materials and manufactured goods. The productivity of Europe cannot be restored without the restoration of Germany as a contributor to that productivity." Hoover's report led to a realization in Washington that a new policy was needed; "almost any action would be an improvement on current policy." In Washington, the Joint Chiefs declared that the "complete revival of German industry, particularly coal mining" was now of "primary importance" to American security.

The United States was already spending a great deal to help Europe recover. Over $14 billion was spent or loaned during the postwar period through the end of 1947 and is not counted as part of the Marshall Plan. Much of this aid was designed to restore infrastructure and help refugees. Britain, for example, received an emergency loan of $3.75 billion.

The United Nations also launched a series of humanitarian and relief efforts almost wholly funded by the United States. These efforts had important effects, but they lacked any central organization and planning, and failed to meet many of Europe's more fundamental needs. Already in 1943, the United Nations Relief and Rehabilitation Administration (UNRRA) was founded to provide relief to areas liberated from Germany. UNRRA provided billions of dollars of rehabilitation aid and helped about 8 million refugees. It ceased operation of displaced persons camps in Europe in 1947; many of its functions were transferred to several UN agencies.

After Marshall's appointment in January 1947, administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets in their occupied zone. Molotov refrained from supplying accounts of Soviet assets. The Soviets took a punitive approach, pressing for a delay rather than an acceleration in economic rehabilitation, demanding unconditional fulfillment of all prior reparation claims, and pressing for progress toward nationwide socioeconomic transformation.

After six weeks of negotiations, Molotov rejected all of the American and British proposals. Molotov also rejected the counter-offer to scrap the British-American "Bizonia" and to include the Soviet zone within the newly constructed Germany. Marshall was particularly discouraged after personally meeting with Stalin to explain that the United States could not possibly abandon its position on Germany, while Stalin expressed little interest in a solution to German economic problems.

After the adjournment of the Moscow conference following six weeks of failed discussions with the Soviets regarding a potential German reconstruction, the United States concluded that a solution could not wait any longer. To clarify the American position, a major address by Secretary of State George Marshall was planned. Marshall gave the address at Harvard University on June 5, 1947. He offered American aid to promote European recovery and reconstruction. The speech described the dysfunction of the European economy and presented a rationale for US aid.

The modern system of the division of labor upon which the exchange of products is based is in danger of breaking down. ... Aside from the demoralizing effect on the world at large and the possibilities of disturbances arising as a result of the desperation of the people concerned, the consequences to the economy of the United States should be apparent to all. It is logical that the United States should do whatever it is able to do to assist in the return of normal economic health to the world, without which there can be no political stability and no assured peace. Our policy is not directed against any country, but against hunger, poverty, desperation and chaos. Any government that is willing to assist in recovery will find full co-operation on the part of the United States. Its purpose should be the revival of a working economy in the world so as to permit the emergence of political and social conditions in which free institutions can exist.

Marshall was convinced that economic stability would provide political stability in Europe. He offered aid, but the European countries had to organize the program themselves.

The speech, written by Charles Bohlen, contained virtually no details and no numbers. More a proposal than a plan, it was a challenge to European leaders to cooperate and coordinate. It asked Europeans to create their own plan for rebuilding Europe, indicating the United States would then fund this plan. The administration felt that the plan would likely be unpopular among many Americans, and the speech was mainly directed at a European audience. In an attempt to keep the speech out of American papers, journalists were not contacted, and on the same day, Truman called a press conference to take away headlines. In contrast, Dean Acheson, an Under Secretary of State, was dispatched to contact the European media, especially the British media, and the speech was read in its entirety on the BBC.

British Foreign Secretary Ernest Bevin heard Marshall's radio broadcast speech and immediately contacted French Foreign Minister Georges Bidault to begin preparing a quick European response to (and acceptance of) the offer, which led to the creation of the Committee of European Economic Co-operation. The two agreed that it would be necessary to invite the Soviets as the other major allied power. Marshall's speech had explicitly included an invitation to the Soviets, feeling that excluding them would have been a sign of distrust. State Department officials, however, knew that Stalin would almost certainly not participate and that any plan that would send large amounts of aid to the Soviets was unlikely to get Congressional approval.

Speaking at the Paris Peace Conference on October 10, 1946 Molotov had already stated Soviet fears: "If American capital was given a free hand in the small states ruined and enfeebled by the war [it] would buy up the local industries, appropriate the more attractive Rumanian, Yugoslav ... enterprises and would become the master in these small states." While the Soviet ambassador in Washington suspected that the Marshall Plan could lead to the creation of an anti-Soviet bloc, Stalin was open to the offer. He directed that—in negotiations to be held in Paris regarding the aid—countries in the Eastern Bloc should not reject economic conditions being placed upon them. Stalin only changed his outlook when he learned that (a) credit would only be extended under conditions of economic cooperation and, (b) aid would also be extended to Germany in total, an eventuality which Stalin thought would hamper the Soviets' ability to exercise influence in western Germany.

Initially, Stalin maneuvered to kill the Plan, or at least hamper it by means of destructive participation in the Paris talks regarding conditions. He quickly realized, however, that this would be impossible after Molotov reported—following his arrival in Paris in July 1947—that conditions for the credit were non-negotiable. Looming as just as large a concern was the Czechoslovak eagerness to accept the aid, as well as indications of a similar Polish attitude.

Soviet Foreign Minister Vyacheslav Molotov left Paris, rejecting the plan. Thereafter, statements were made suggesting a future confrontation with the West, calling the United States both a "fascizing" power and the "center of worldwide reaction and anti-Soviet activity", with all U.S.-aligned countries branded as enemies. The Soviets also then blamed the United States for communist losses in elections in Belgium, France and Italy months earlier, in the spring of 1947. It claimed that "marshallization" must be resisted and prevented by any means, and that French and Italian communist parties were to take maximum efforts to sabotage the implementation of the Plan. In addition, Western embassies in Moscow were isolated, with their personnel being denied contact with Soviet officials.

On July 12, a larger meeting was convened in Paris. Every country of Europe was invited, with the exceptions of Spain (a World War II neutral that had sympathized with the Axis powers) and the small states of Andorra, San Marino, Monaco, and Liechtenstein. The Soviet Union was invited with the understanding that it would likely refuse. The states of the future Eastern Bloc were also approached, and Czechoslovakia and Poland agreed to attend. In one of the clearest signs and reflections of tight Soviet control and domination over the region, Jan Masaryk, the foreign minister of Czechoslovakia, was summoned to Moscow and berated by Stalin for considering Czechoslovakia's possible involvement with and joining of the Marshall Plan. The prime minister of Poland, Józef Cyrankiewicz, was rewarded by Stalin for his country's rejection of the Plan, which came in the form of the Soviet Union's offer of a lucrative trade agreement lasting for a period of five years, a grant amounting to the approximate equivalent of $450 million (in 1948; the sum would have been $4.4 billion in 2014) in the form of long-term credit and loans and the provision of 200,000 tonnes of grain, heavy and manufacturing machinery and factories and heavy industries to Poland.

The Marshall Plan participants were not surprised when the Czechoslovakian and Polish delegations were prevented from attending the Paris meeting. The other Eastern Bloc states immediately rejected the offer. Finland also declined, to avoid antagonizing the Soviets (see also Finlandization). The Soviet Union's "alternative" to the Marshall plan, which was purported to involve Soviet subsidies and trade with western Europe, became known as the Molotov Plan, and later, the Comecon. In a 1947 speech to the United Nations, Soviet deputy foreign minister Andrei Vyshinsky said that the Marshall Plan violated the principles of the United Nations. He accused the United States of attempting to impose its will on other independent states, while at the same time using economic resources distributed as relief to needy nations as an instrument of political pressure.

Although all other Communist European Countries had deferred to Stalin and rejected the aid, the Yugoslavs, led by Josip Broz (Tito), at first went along and rejected the Marshall Plan. However, in 1948 Tito broke decisively with Stalin on other issues, making Yugoslavia an independent communist state. Yugoslavia requested American aid. American leaders were internally divided, but finally agreed and began sending money on a small scale in 1949, and on a much larger scale in 1950–53. The American aid was not part of the Marshall Plan.

In late September, the Soviet Union called a meeting of nine European Communist parties in southwest Poland. A Communist Party of the Soviet Union (CPSU) report was read at the outset to set the heavily anti-Western tone, stating now that "international politics is dominated by the ruling clique of the American imperialists" which have embarked upon the "enslavement of the weakened capitalist countries of Europe". Communist parties were to struggle against the US presence in Europe by any means necessary, including sabotage. The report further claimed that "reactionary imperialist elements throughout the world, particularly in the United States, in Britain and France, had put particular hope on Germany and Japan, primarily on Hitlerite Germany—first as a force most capable of striking a blow at the Soviet Union".

Referring to the Eastern Bloc, the report stated that "the Red Army's liberating role was complemented by an upsurge of the freedom-loving peoples' liberation struggle against the fascist predators and their hirelings." It argued that "the bosses of Wall Street" were "tak[ing] the place of Germany, Japan and Italy". The Marshall Plan was described as "the American plan for the enslavement of Europe". It described the world now breaking down "into basically two camps—the imperialist and antidemocratic camp on the one hand, and the antiimperialist and democratic camp on the other".

Although the Eastern Bloc countries except Czechoslovakia had immediately rejected Marshall Plan aid, Eastern Bloc communist parties were blamed for permitting even minor influence by non-communists in their respective countries during the run up to the Marshall Plan. The meeting's chair, Andrei Zhdanov, who was in permanent radio contact with the Kremlin from whom he received instructions, also castigated communist parties in France and Italy for collaboration with those countries' domestic agendas. Zhdanov warned that if they continued to fail to maintain international contact with Moscow to consult on all matters, "extremely harmful consequences for the development of the brother parties' work" would result.

Italian and French communist leaders were prevented by party rules from pointing out that it was actually Stalin who had directed them not to take opposition stances in 1944. The French communist party, as others, was then to redirect its mission to "destroy capitalist economy" and that the Soviet Communist Information Bureau (Cominform) would take control of the French Communist Party's activities to oppose the Marshall Plan. When they asked Zhdanov if they should prepare for armed revolt when they returned home, he did not answer. In a follow-up conversation with Stalin, he explained that an armed struggle would be impossible and that the struggle against the Marshall Plan was to be waged under the slogan of national independence.

Congress, under the control of conservative Republicans, agreed to the program for multiple reasons. The 20-member conservative isolationist Senate wing of the party, based in the rural Midwest and led by Senator Kenneth S. Wherry (R-Nebraska), was outmaneuvered by the emerging internationalist wing, led by Senator Arthur H. Vandenberg (R-Michigan). The opposition argued that it made no sense to oppose communism by supporting the socialist governments in Western Europe; and that American goods would reach Russia and increase its war potential. They called it "a wasteful 'operation rat-hole'" Vandenberg, assisted by Senator Henry Cabot Lodge, Jr. (R-Massachusetts) admitted there was no certainty that the plan would succeed, but said it would halt economic chaos, sustain Western civilization, and stop further Soviet expansion. Senator Robert A. Taft (R-Ohio) hedged on the issue. He said it was without economic justification; however, it was "absolutely necessary" in "the world battle against communism." In the end, only 17 senators voted against it on March 13, 1948 A bill granting an initial $5 billion passed Congress with strong bipartisan support. Congress eventually allocated $12.4 billion in aid over the four years of the plan.

Congress reflected public opinion, which resonated with the ideological argument that communism flourishes in poverty. Truman's own prestige and power had been greatly enhanced by his stunning victory in the 1948 election. Across America, multiple interest groups, including business, labor, farming, philanthropy, ethnic groups, and religious groups, saw the Marshall Plan as an inexpensive solution to a massive problem, noting it would also help American exports and stimulate the American economy as well. Major newspapers were highly supportive, including such conservative outlets as "Time" magazine. Vandenberg made sure of bipartisan support on the Senate Foreign Relations Committee. The Solid Democratic South was highly supportive, the upper Midwest was dubious, but heavily outnumbered. The plan was opposed by conservatives in the rural Midwest, who opposed any major government spending program and were highly suspicious of Europeans. The plan also had some opponents on the left, led by Henry A. Wallace, the former Vice President. He said the Plan was hostile to the Soviet Union, a subsidy for American exporters, and sure to polarize the world between East and West. However, opposition against the Marshall Plan was greatly reduced by the shock of the Communist coup in Czechoslovakia in February 1948. The appointment of the prominent businessman Paul G. Hoffman as director reassured conservative businessmen that the gigantic sums of money would be handled efficiently.

Turning the plan into reality required negotiations among the participating nations. Sixteen nations met in Paris to determine what form the American aid would take, and how it would be divided. The negotiations were long and complex, with each nation having its own interests. France's major concern was that Germany not be rebuilt to its previous threatening power. The Benelux countries (Belgium, Netherlands, and Luxembourg), despite also suffering under the Nazis, had long been closely linked to the German economy and felt their prosperity depended on its revival. The Scandinavian nations, especially Sweden, insisted that their long-standing trading relationships with the Eastern Bloc nations not be disrupted and that their neutrality not be infringed.

The United Kingdom insisted on special status as a longstanding belligerent during the war, concerned that if it were treated equally with the devastated continental powers it would receive virtually no aid. The Americans were pushing the importance of free trade and European unity to form a bulwark against communism. The Truman administration, represented by William L. Clayton, promised the Europeans that they would be free to structure the plan themselves, but the administration also reminded the Europeans that implementation depended on the plan's passage through Congress. A majority of Congress members were committed to free trade and European integration, and were hesitant to spend too much of the money on Germany. However, before the Marshall Plan was in effect, France, Austria, and Italy needed immediate aid. On December 17, 1947, the United States agreed to give $40 million to France, Austria, China, and Italy.

Agreement was eventually reached and the Europeans sent a reconstruction plan to Washington, which was formulated and agreed upon by the Committee of European Economic Co-operation in 1947. In the document, the Europeans asked for $22 billion in aid. Truman cut this to $17 billion in the bill he put to Congress.
On March 17, 1948, Truman addressed European security and condemned the Soviet Union before a hastily convened Joint Session of Congress. Attempting to contain spreading Soviet influence in the Eastern Bloc, Truman asked Congress to restore a peacetime military draft and to swiftly pass the Economic Cooperation Act, the name given to the Marshall Plan. Of the Soviet Union Truman said, "The situation in the world today is not primarily the result of the natural difficulties which follow a great war. It is chiefly due to the fact that one nation has not only refused to cooperate in the establishment of a just and honorable peace but—even worse—has actively sought to prevent it."

Members of the Republican-controlled 80th Congress (1947–1949) were skeptical. "In effect, he told the Nation that we have lost the peace, that our whole war effort was in vain.", noted Representative Frederick Smith of Ohio. Others thought he had not been forceful enough to contain the USSR. "What [Truman] said fell short of being tough", noted Representative Eugene Cox, a Democrat from Georgia, "there is no prospect of ever winning Russian cooperation." Despite its reservations, the 80th Congress implemented Truman's requests, further escalating the Cold War with the USSR.

Truman signed the Economic Cooperation Act into law on April 3, 1948; the Act established the Economic Cooperation Administration (ECA) to administer the program. ECA was headed by economic cooperation administrator Paul G. Hoffman. In the same year, the participating countries (Austria, Belgium, Denmark, France, West Germany, the United Kingdom, Greece, Iceland, Ireland, Italy, Luxembourg, the Netherlands, Norway, Sweden, Switzerland, Turkey, and the United States) signed an accord establishing a master financial-aid-coordinating agency, the Organisation for European Economic Co-operation (later called the Organisation for Economic Co-operation and Development or OECD), which was headed by Frenchman Robert Marjolin.

According to Armin Grunbacher:

The ECA's official mission statement was to give a boost to the European economy: to promote European production, to bolster European currency, and to facilitate international trade, especially with the United States, whose economic interest required Europe to become wealthy enough to import US goods. Another unofficial goal of ECA (and of the Marshall Plan) was the containment of growing Soviet influence in Europe, evident especially in the growing strength of communist parties in France, and Italy.

The Marshall Plan money was transferred to the governments of the European nations. The funds were jointly administered by the local governments and the ECA. Each European capital had an ECA envoy, generally a prominent American businessman, who would advise on the process. The cooperative allocation of funds was encouraged, and panels of government, business, and labor leaders were convened to examine the economy and see where aid was needed. The recipient nations were represented collectively by the Organisation for Economic Co-operation and Development (OECD), headed by British statesman Oliver Franks.

The Marshall Plan aid was mostly used for goods from the United States. The European nations had all but exhausted their foreign-exchange reserves during the war, and the Marshall Plan aid represented almost their sole means of importing goods from abroad. At the start of the plan, these imports were mainly much-needed staples such as food and fuel, but later the purchases turned towards reconstruction needs as was originally intended. In the latter years, under pressure from the United States Congress and with the outbreak of the Korean War, an increasing amount of the aid was spent on rebuilding the militaries of Western Europe. Of the some $13 billion allotted by mid-1951, $3.4 billion had been spent on imports of raw materials and semi-manufactured products; $3.2 billion on food, feed, and fertilizer; $1.9 billion on machines, vehicles, and equipment; and $1.6 billion on fuel.

Also established were counterpart funds, which used Marshall Plan aid to establish funds in the local currency. According to ECA rules, recipients had to invest 60% of these funds in industry. This was prominent in Germany, where these government-administered funds played a crucial role in lending money to private enterprises which would spend the money rebuilding. These funds played a central role in the reindustrialization of Germany. In 1949–50, for instance, 40% of the investment in the German coal industry was by these funds.

The companies were obligated to repay the loans to the government, and the money would then be lent out to another group of businesses. This process has continued to this day in the guise of the state-owned KfW bank, (Kreditanstalt für Wiederaufbau, meaning Reconstruction Credit Institute). The Special Fund, then supervised by the Federal Economics Ministry, was worth over DM 10 billion in 1971. In 1997 it was worth DM 23 billion. Through the revolving loan system, the Fund had by the end of 1995 made low-interest loans to German citizens amounting to around DM 140 billion. The other 40% of the counterpart funds were used to pay down the debt, stabilize the currency, or invest in non-industrial projects. France made the most extensive use of counterpart funds, using them to reduce the budget deficit. In France, and most other countries, the counterpart fund money was absorbed into general government revenues, and not recycled as in Germany. 

The Netherlands received US aid for economic recovery in the Netherlands Indies. However, in January 1949, the American government suspended this aid in response to the Dutch efforts to restore colonial rule in Indonesia during the Indonesian National Revolution, and it implicitly threatened to suspend Marshall aid to the Netherlands if the Dutch government continued to oppose the independence of Indonesia.

At the time the United States was a significant oil producing nation — one of the goals of the Marshall Plan was for Europe to use oil in place of coal, but the Europeans wanted to buy crude oil and use the Marshall Plan funds to build refineries instead. However, when independent American oil companies complained, the ECA denied funds for European refinery construction.

A high priority was increasing industrial productivity in Europe, which proved one of the more successful aspects of the Marshall Plan. The US Bureau of Labor Statistics (BLS) contributed heavily to the success of the Technical Assistance Program. The United States Congress passed a law on June 7, 1940 that allowed the BLS to "make continuing studies of labor productivity" and appropriated funds for the creation of a Productivity and Technological Development Division. The BLS could then use its expertise in the field of productive efficiency to implement a productivity drive in each Western European country receiving Marshall Plan aid. Counterpart funds were used to finance large-scale tours of American industry. France, for example, sent 500 missions with 4700 businessmen and experts to tour American factories, farms, stores, and offices. They were especially impressed with the prosperity of American workers, and how they could purchase an inexpensive new automobile for nine months work, compared to 30 months in France.

By implementing technological literature surveys and organized plant visits, American economists, statisticians, and engineers were able to educate European manufacturers in statistical measurement. The goal of the statistical and technical assistance from the Americans was to increase productive efficiency of European manufacturers in all industries.

To conduct this analysis, the BLS performed two types of productivity calculations. First, they used existing data to calculate how much a worker produces per hour of work—the average output rate. Second, they compared the existing output rates in a particular country to output rates in other nations. By performing these calculations across all industries, the BLS was able to identify the strengths and weaknesses of each country's manufacturing and industrial production. From that, the BLS could recommend technologies (especially statistical) that each individual nation could implement. Often, these technologies came from the United States; by the time the Technical Assistance Program began, the United States used statistical technologies "more than a generation ahead of what [the Europeans] were using".

The BLS used these statistical technologies to create Factory Performance Reports for Western European nations. The American government sent hundreds of technical advisers to Europe to observe workers in the field. This on-site analysis made the Factory Performance Reports especially helpful to the manufacturers. In addition, the Technical Assistance Program funded 24,000 European engineers, leaders, and industrialists to visit America and tour America's factories, mines, and manufacturing plants. This way, the European visitors would be able to return to their home countries and implement the technologies used in the United States. The analyses in the Factory Performance Reports and the "hands-on" experience had by the European productivity teams effectively identified productivity deficiencies in European industries; from there, it became clearer how to make European production more effective.

Before the Technical Assistance Program even went into effect, United States Secretary of Labor Maurice Tobin expressed his confidence in American productivity and technology to both American and European economic leaders. He urged that the United States play a large role in improving European productive efficiency by providing four recommendations for the program's administrators:

The effects of the Technical Assistance Program were not limited to improvements in productive efficiency. While the thousands of European leaders took their work/study trips to the United States, they were able to observe a number of aspects of American society as well. The Europeans could watch local, state, and federal governments work together with citizens in a pluralist society. They observed a democratic society with open universities and civic societies in addition to more advanced factories and manufacturing plants. The Technical Assistance Program allowed Europeans to bring home many types of American ideas.

Another important aspect of the Technical Assistance Program was its low cost. While $19.4 billion was allocated for capital costs in the Marshall Plan, the Technical Assistance Program only required $300 million. Only one-third of that $300 million cost was paid by the United States.

In the aftermath of the war Britain faced a deep financial crisis, whereas the United States enjoyed an economic boom. The United States continue to finance the British treasury after the war. Much of this aid was designed to restore infrastructure and help refugees. Britain received an emergency loan of $3.75 billion in 1946; it was a 50-year loan with a low 2% interest rate. The Marshall Plan provided a more permanent solution as it gave $3.3 billion to Britain. The Marshall money was a gift and carried requirements that Britain balance its budget, control tariffs and maintain adequate currency reserves. The British Labour government under Prime Minister Clement Attlee was an enthusiastic participant. 

The American goals for the Marshall plan were to help rebuild the postwar British economy, help modernize the economy, and minimize trade barriers. When the Soviet Union refused to participate or allow its satellites to participate, the Marshall plan became an element of the emerging Cold War. 

There were political tensions between the two nations regarding Marshall plan requirements. London was dubious about Washington's emphasis on European economic integration as the solution to postwar recovery. Integration with Europe at this point would mean cutting close ties to the emerging Commonwealth. London tried to convince Washington that that American economic aid, especially to the sterling currency area, was necessary to solve the dollar shortage. British economist argued that their position was validated by 1950 as European industrial production exceeded prewar levels. Washington demanded convertibility of sterling currency on 15 July 1947, which produced a severe financial crisis for Britain. Convertibility was suspended on 20 August 1947. However by 1950, American rearmament and heavy spending on the Korean War and Cold War finally ended the dollar shortage. the balance of payment problems the trouble the postwar government was caused less by economic decline and more by political overreach, according to Jim Tomlinson. 
The Marshall Plan was implemented in West Germany 1948-1950 as a way to modernize business procedures and utilize the best practices. The Marshall Plan made it possible for West Germany to return quickly to its traditional pattern of industrial production with a strong export sector. Without the plan, agriculture would have played a larger role in the recovery period, which itself would have been longer.

Marshall Aid in general and the counterpart funds in particular had actually quite a significant impact in Cold-War propaganda and economic matters in Western Europe, which most likely contributed to the declining appeal of communism.

The Marshall Plan aid was divided among the participant states on a roughly per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The exception was Iceland, which had been neutral during the war, but received far more on a per capita basis than the second highest recipient. The table below shows Marshall Plan aid by country and year (in millions of dollars) from "The Marshall Plan Fifty Years Later." There is no clear consensus on exact amounts, as different scholars differ on exactly what elements of American aid during this period were part of the Marshall Plan.

The Marshall Plan, just as GARIOA, consisted of aid both in the form of grants and in the form of loans. Out of the total, US$1.2 billion were loan-aid.

Ireland which received US$146.2 million through the Marshall Plan, received US$128.2 million as loans, and the remaining US$18 million as grants. By 1969 the Irish Marshall Plan debt, which was still being repaid, amounted to 31 million pounds, out of a total Irish foreign debt of 50 million pounds.

The UK received US$385 million of its Marshall Plan aid in the form of loans. Unconnected to the Marshall Plan the UK also received direct loans from the US amounting to US$4.6 billion. The proportion of Marshall Plan loans versus Marshall Plan grants was roughly 15% to 85% for both the UK and France.

Germany, which up until the 1953 Debt agreement had to work on the assumption that all the Marshall Plan aid was to be repaid, spent its funds very carefully. Payment for Marshall Plan goods, "counterpart funds", were administered by the Reconstruction Credit Institute, which used the funds for loans inside Germany. In the 1953 Debt agreement, the amount of Marshall plan aid that Germany was to repay was reduced to less than US$1 billion. This made the proportion of loans versus grants to Germany similar to that of France and the UK. The final German loan repayment was made in 1971. Since Germany chose to repay the aid debt out of the German Federal budget, leaving the German ERP fund intact, the fund was able to continue its reconstruction work. By 1996 it had accumulated a value of 23 billion Deutsche Mark.

The Central Intelligence Agency received 5% of the Marshall Plan funds (about $685 million spread over six years), which it used to finance secret operations abroad. Through the Office of Policy Coordination money was directed towards support for labor unions, newspapers, student groups, artists and intellectuals, who were countering the anti-American counterparts subsidized by the Communists. The largest sum went to the Congress for Cultural Freedom. There were no agents working among the Soviets or their satellite states. The founding conference of the Congress for Cultural Freedom was held in Berlin in June 1950. Among the leading intellectuals from the US and Western Europe were writers, philosophers, critics and historians: Franz Borkenau, Karl Jaspers, John Dewey, Ignazio Silone, James Burnham, Hugh Trevor-Roper, Arthur Schlesinger, Jr., Bertrand Russell, Ernst Reuter, Raymond Aron, Alfred Ayer, Benedetto Croce, Arthur Koestler, Richard Löwenthal, Melvin J. Lasky, Tennessee Williams, Irving Brown, and Sidney Hook. There were conservatives among the participants, but non-Communist (or former Communist) left-wingers were more numerous.

The Marshall Plan was originally scheduled to end in 1953. Any effort to extend it was halted by the growing cost of the Korean War and rearmament. American Republicans hostile to the plan had also gained seats in the 1950 Congressional elections, and conservative opposition to the plan was revived. Thus the plan ended in 1951, though various other forms of American aid to Europe continued afterwards.

The years 1948 to 1952 saw the fastest period of growth in European history. Industrial production increased by 35%. Agricultural production substantially surpassed pre-war levels. The poverty and starvation of the immediate postwar years disappeared, and Western Europe embarked upon an unprecedented two decades of growth that saw standards of living increase dramatically. Additionally, the long-term effect of economic integration raised European income levels substantially, by nearly 20 percent by the mid-1970s. There is some debate among historians over how much this should be credited to the Marshall Plan. Most reject the idea that it alone miraculously revived Europe, as evidence shows that a general recovery was already underway. Most believe that the Marshall Plan sped this recovery, but did not initiate it. Many argue that the structural adjustments that it forced were of great importance. Economic historians J. Bradford DeLong and Barry Eichengreen call it "history's most successful structural adjustment program." One effect of the plan was that it subtly "Americanized" European countries, especially Austria, through new exposure to American popular culture, including the growth in influence of Hollywood movies and rock n' roll.

The political effects of the Marshall Plan may have been just as important as the economic ones. Marshall Plan aid allowed the nations of Western Europe to relax austerity measures and rationing, reducing discontent and bringing political stability. The communist influence on Western Europe was greatly reduced, and throughout the region, communist parties faded in popularity in the years after the Marshall Plan. The trade relations fostered by the Marshall Plan helped forge the North Atlantic alliance that would persist throughout the Cold War in the form of NATO. At the same time, the nonparticipation of the states of the Eastern Bloc was one of the first clear signs that the continent was now divided.

The Marshall Plan also played an important role in European integration. Both the Americans and many of the European leaders felt that European integration was necessary to secure the peace and prosperity of Europe, and thus used Marshall Plan guidelines to foster integration. In some ways, this effort failed, as the OEEC never grew to be more than an agent of economic cooperation. Rather, it was the separate European Coal and Steel Community, which did not include Britain, that would eventually grow into the European Union. However, the OEEC served as both a testing and training ground for the structures that would later be used by the European Economic Community. The Marshall Plan, linked into the Bretton Woods system, also mandated free trade throughout the region.

While some historians today feel some of the praise for the Marshall Plan is exaggerated, it is still viewed favorably and many thus feel that a similar project would help other areas of the world. After the fall of communism, several proposed a "Marshall Plan for Eastern Europe" that would help revive that region. Others have proposed a Marshall Plan for Africa to help that continent, and US Vice President Al Gore suggested a Global Marshall Plan. "Marshall Plan" has become a metaphor for any very large-scale government program that is designed to solve a specific social problem. It is usually used when calling for federal spending to correct a perceived failure of the private sector.

Nicholas Shaxson comments: “It is widely believed that the plan worked by offsetting European countries’ yawning deficits. But its real importance ... was simply to compensate for the US failure to institute controls on inflows of hot money from Europe. ... American post-war aid was less than the money flowing in the other direction.“ European hot money inflated the US dollar, to the disadvantage of US exporters.

The Marshall Plan money was in the form of grants from the U.S. Treasury that did not have to be repaid. The Organisation for European Economic Co-operation took the leading role in allocating funds, and the OEEC arranged for the transfer of the goods. The American supplier was paid in dollars, which were credited against the appropriate European Recovery Program funds. The European recipient, however, was not given the goods as a gift but had to pay for them (usually on credit) in local currency. These payments were kept by the European government involved in a special counterpart fund. This counterpart money, in turn, could be used by the government for further investment projects. Five percent of the counterpart money was paid to the US to cover the administrative costs of the ERP. In addition to ERP grants, the Export-Import Bank (an agency of the US government) at the same time made long-term loans at low interest rates to finance major purchases in the US, all of which were repaid.

In the case of Germany, there also were 16 billion marks of debts from the 1920s which had defaulted in the 1930s, but which Germany decided to repay to restore its reputation. This money was owed to government and private banks in the US, France, and Britain. Another 16 billion marks represented postwar loans by the US. Under the London Debts Agreement of 1953, the repayable amount was reduced by 50% to about 15 billion marks and stretched out over 30 years, and compared to the fast-growing German economy were of minor impact.

Large parts of the world devastated by World War II did not benefit from the Marshall Plan. The only major Western European nation excluded was Francisco Franco's Spain, which was highly unpopular in Washington. With the escalation of the Cold War, the United States reconsidered its position, and in 1951 embraced Spain as an ally, encouraged by Franco's aggressive anti-communist policies. Over the next decade, a considerable amount of American aid would go to Spain, but less than its neighbors had received under the Marshall Plan.

The Soviet Union had been as badly affected as any part of the world by the war. The Soviets imposed large reparations payments on the Axis allies that were in its sphere of influence. Austria, Finland, Hungary, Romania, and especially East Germany were forced to pay vast sums and ship large amounts of supplies to the USSR. These reparation payments meant the Soviet Union itself received about the same as 16 European countries received in total from Marshall Plan aid.

In accordance with the agreements with the USSR, shipment of dismantled German industrial installations from the west began on March 31, 1946. Under the terms of the agreement, the Soviet Union would in return ship raw materials such as food and timber to the western zones. In view of the Soviet failure to do so, the western zones halted the shipments east, ostensibly on a temporary basis, although they were never resumed. It was later shown that the main reason for halting shipments east was not the behavior of the USSR but rather the recalcitrant behavior of France. Examples of material received by the USSR were equipment from the Kugel-Fischer ballbearing plant at Schweinfurt, the Daimler-Benz underground aircraft-engine plant at Obrigheim, the Deschimag shipyards at Bremen-Weser, and the Gendorf powerplant.

The USSR did establish COMECON as a riposte to the Marshall Plan to deliver aid for Eastern Bloc countries, but this was complicated by the Soviet efforts to manage their own recovery from the war. The members of Comecon looked to the Soviet Union for oil; in turn, they provided machinery, equipment, agricultural goods, industrial goods, and consumer goods to the Soviet Union. Economic recovery in the East was much slower than in the West, resulting in the formation of the shortage economies and a gap in wealth between East and West. Finland, which the USSR forbade to join the Marshall Plan and which was required to give large reparations to the USSR, saw its economy recover to pre-war levels in 1947. France, which received billions of dollars through the Marshall Plan, similarly saw its average income per person return to almost pre-war level by 1949. By mid-1948 industrial production in Poland, Hungary, Bulgaria, and Czechoslovakia had recovered to a level somewhat above pre-war level.

From the end of the war to the end of 1953, the US provided grants and credits amounting to $5.9 billion to Asian countries, especially China/Taiwan ($1.051 billion), India ($255 million), Indonesia ($215 million), Japan ($2.44 billion), South Korea ($894 million), Pakistan ($98 million) and the Philippines ($803 million). In addition, another $282 million went to Israel and $196 million to the rest of the Middle East. All this aid was separate from the Marshall Plan.

Canada, like the United States, was damaged little by the war and in 1945 was one of the world's richest economies. It operated its own aid program. In 1948, the US allowed ERP aid to be used in purchasing goods from Canada. Canada made over a billion dollars in sales in the first two years of operation.

The total of American grants and loans to the world from 1945 to 1953 came to $44.3 billion.

Bradford DeLong and Barry Eichengreen conclude it was "History's Most Successful Structural Adjustment Program." They state:

It was not large enough to have significantly accelerated recovery by financing investment, aiding the reconstruction of damaged infrastructure, or easing commodity bottlenecks. We argue, however, that the Marshall Plan did play a major role in setting the stage for post-World War II Western Europe's rapid growth. The conditions attached to Marshall Plan aid pushed European political economy in a direction that left its post World War II "mixed economies" with more "market" and less "controls" in the mix.

Prior to passing and enacting the Marshall Plan, President Truman and George Marshall started a domestic overhaul of public opinion from coast to coast. The purpose of this campaign was to sway public opinion in their direction and to inform the common person of what the Marshall Plan was and what the Plan would ultimately do. They spent months attempting to convince Americans that their cause was just and that they should embrace the higher taxes that would come in the foreseeable future.

A copious amount of propaganda ended up being highly effective in swaying public opinion towards supporting the Marshall Plan. During the nationwide campaign for support, "more than a million pieces of pro-Marshall Plan publications-booklets, leaflets, reprints, and fact sheets", were disseminated. Truman's and Marshall's efforts proved to be effective. A Gallup Poll taken between the months of July and December 1947 shows the percentage of Americans unaware of the Marshall Plan fell from 51% to 36% nationwide. By the time the Marshall Plan was ready to be implemented, there was a general consensus throughout the American public that this was the right policy for both America, and the countries who would be receiving aid.

During the period leading up to World War II, Americans were highly isolationist, and many called The Marshall Plan a "milestone" for American ideology. By looking at polling data over time from pre-World War II to post-World War II, one would find that there was a change in public opinion in regards to ideology. Americans swapped their isolationist ideals for a much more global internationalist ideology after World War II.

In a National Opinion Research Center (NORC) poll taken in April 1945, a cross-section of Americans were asked, "If our government keeps on sending lendlease materials, which we may not get paid for, to friendly countries for about three years after the war, do you think this will mean more jobs or fewer jobs for most Americans, or won't it make any difference?" 75% said the same or more jobs; 10% said fewer.

Before proposing anything to Congress in 1947, the Truman administration made an elaborate effort to organize public opinion in favor of the Marshall Plan spending, reaching out to numerous national organizations representing business, labor, farmers, women, and other interest groups. Political scientist Ralph Levering points out that:

Mounting large public relations campaigns and supporting private groups such as the Citizens Committee for the Marshall Plan, the administration carefully built public and bipartisan Congressional support before bringing these measures to a vote.

Public opinion polls in 1947 consistently showed strong support for the Marshall plan among Americans. Furthermore, Gallup polls in England, France, and Italy showed favorable majorities over 60%. 

Laissez-faire criticism of the Marshall Plan came from a number of economists. Wilhelm Röpke, who influenced German Minister for Economy Ludwig Erhard in his economic recovery program, believed recovery would be found in eliminating central planning and restoring a market economy in Europe, especially in those countries which had adopted more fascist and corporatist economic policies. Röpke criticized the Marshall Plan for forestalling the transition to the free market by subsidizing the current, failing systems. Erhard put Röpke's theory into practice and would later credit Röpke's influence for West Germany's preeminent success.

Henry Hazlitt criticized the Marshall Plan in his 1947 book "Will Dollars Save the World?", arguing that economic recovery comes through savings, capital accumulation, and private enterprise, and not through large cash subsidies. Austrian School economist Ludwig von Mises criticized the Marshall Plan in 1951, believing that "the American subsidies make it possible for [Europe's] governments to conceal partially the disastrous effects of the various socialist measures they have adopted". Some critics and Congressmen at the time believed that America was giving too much aid to Europe. America had already given Europe $9 billion in other forms of help in previous years. The Marshall Plan gave another $13 billion, equivalent to about $100 billion in 2010 value.

However, its role in the rapid recovery has been debated. Most reject the idea that it alone miraculously revived Europe since the evidence shows that a general recovery was already underway. The Marshall Plan grants were provided at a rate that was not much higher in terms of flow than the previous UNRRA aid and represented less than 3% of the combined national income of the recipient countries between 1948 and 1951, which would mean an increase in GDP growth of only 0.3%. In addition, there is no correlation between the amount of aid received and the speed of recovery: both France and the United Kingdom received more aid, but West Germany recovered significantly faster.

Criticism of the Marshall Plan became prominent among historians of the revisionist school, such as Walter LaFeber, during the 1960s and 1970s. They argued that the plan was American economic imperialism and that it was an attempt to gain control over Western Europe just as the Soviets controlled Eastern Europe economically through the Comecon. In a review of West Germany's economy from 1945 to 1951, German analyst Werner Abelshauser concluded that "foreign aid was not crucial in starting the recovery or in keeping it going". The economic recoveries of France, Italy, and Belgium, Cowen argues, began a few months before the flow of US money. Belgium, the country that relied earliest and most heavily on free-market economic policies after its liberation in 1944, experienced swift recovery and avoided the severe housing and food shortages seen in the rest of continental Europe.

Former US Chairman of the Federal Reserve Bank Alan Greenspan gives most credit to German Chancellor Ludwig Erhard for Europe's economic recovery. Greenspan writes in his memoir "The Age of Turbulence" that Erhard's economic policies were the most important aspect of postwar Western European recovery, even outweighing the contributions of the Marshall Plan. He states that it was Erhard's reductions in economic regulations that permitted Germany's miraculous recovery, and that these policies also contributed to the recoveries of many other European countries. Its recovery is attributed to traditional economic stimuli, such as increases in investment, fueled by a high savings rate and low taxes. Japan saw a large infusion of US investment during the Korean War.
Noam Chomsky said the Marshall Plan "set the stage for large amounts of private U.S. investment in Europe, establishing the basis for modern transnational corporations".

Alfred Friendly, press aide to the US Secretary of Commerce W. Averell Harriman, wrote a humorous operetta about the Marshall Plan during its first year; one of the lines in the operetta was: "Wines for Sale; will you swap / A little bit of steel for Chateau Neuf du Pape?"

Spanish director Luis García Berlanga co-wrote and directed the movie "Welcome Mr. Marshall!", a comedy about the residents of a small Spanish village who dream about the life of wealth and self-fulfilment the Marshall Plan will bring them. The film highlights the stereotypes held by both the Spanish and the Americans regarding the culture of the other, as well as displays social criticism of 1950s Francoist Spain.






</doc>
<doc id="19769" url="https://en.wikipedia.org/wiki?curid=19769" title="Mariculture">
Mariculture

Mariculture is a specialized branch of aquaculture involving the cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater. An example of the latter is the farming of marine fish, including finfish and shellfish like prawns, or oysters and seaweed in saltwater ponds. Non-food products produced by mariculture include: fish meal, nutrient agar, jewellery (e.g. cultured pearls), and cosmetics.

Similar to algae cultivation, shellfish can be farmed in multiple ways: on ropes, in bags or cages, or directly on (or within) the intertidal substrate. Shellfish mariculture does not require feed or fertilizer inputs, nor insecticides or antibiotics, making shellfish aquaculture (or 'mariculture') a self-supporting system. Shellfish can also be used in multi-species cultivation techniques, where shellfish can utilize waste generated by higher trophic level organisms.

After trials in 2012, a commercial "sea ranch" was set up in Flinders Bay, Western Australia to raise abalone. The ranch is based on an artificial reef made up of 5000 () separate concrete units called "abitats" (abalone habitats). The abitats can host 400 abalone each. The reef is seeded with young abalone from an onshore hatchery.

The abalone feed on seaweed that has grown naturally on the habitats; with the ecosystem enrichment of the bay also resulting in growing numbers of dhufish, pink snapper, wrasse, Samson fish among other species.

Brad Adams, from the company, has emphasised the similarity to wild abalone and the difference from shore based aquaculture. "We're not aquaculture, we're ranching, because once they're in the water they look after themselves."

Raising marine organisms under controlled conditions in exposed, high-energy ocean environments beyond significant coastal influence, is a relatively new approach to mariculture. Some attention has been paid to how open ocean mariculture can combine with offshore energy installation systems, such as wind-farms, to enable a more effective use of ocean space. Open ocean aquaculture (OOA) uses cages, nets, or long-line arrays that are moored, towed or float freely. Research and commercial open ocean aquaculture facilities are in operation or under development in Panama, Australia, Chile, China, France, Ireland, Italy, Japan, Mexico, and Norway. As of 2004, two commercial open ocean facilities were operating in U.S. waters, raising Threadfin near Hawaii and cobia near Puerto Rico. An operation targeting bigeye tuna recently received final approval. All U.S. commercial facilities are currently sited in waters under state or territorial jurisdiction. The largest deep water open ocean farm in the world is raising cobia 12 km off the northern coast of Panama in highly exposed sites.

There has been considerable discussion as to how mariculture of seaweeds can be conducted in the open ocean as a means to regenerate decimated fish populations by providing both habitat and the basis of a trophic pyramid for marine life. It has been proposed that natural seaweed ecosystems can be replicated in the open ocean by creating the conditions for their growth through artificial upwelling and through submerged tubing that provide substrate. Proponents and permaculture experts recognise that such approaches correspond to the core principles of permaculture and thereby constitute Marine Permaculture. The concept envisions using artificial upwelling and floating, submerged platforms as substrate to replicate natural seaweed ecosystems that provide habitat and the basis of a trophic pyramid for marine life. Following the principles of permaculture, seaweeds and fish from Marine Permaculture arrays can be sustainably harvested with the potential of also sequestering atmospheric carbon, should seaweeds be sunk below a depth of one kilometer. As of 2020, a number of successful trials have taken place in Hawaii, the Philippines, Puerto Rico and Tasmania. The idea has received substantial public attention, notably featuring as a key solution covered by Damon Gameau’s documentary 2040 and in the book Drawdown: The Most Comprehensive Plan Ever Proposed to Reverse Global Warming edited by Paul Hawken.

Enhanced Stocking (also known as sea ranching) is a Japanese principle based on operant conditioning and the migratory nature of certain species. The fishermen raise hatchlings in a closely knitted net in a harbor, sounding an underwater horn before each feeding. When the fish are old enough they are freed from the net to mature in the open sea. During spawning season, about 80% of these fish return to their birthplace. The fishermen sound the horn and then net those fish that respond.

In seawater pond mariculture, fish are raised in ponds which receive water from the sea. This has the benefit that the nutrition (e.g. microorganisms) present in the seawater can be used. This is a great advantage over traditional fish farms (e.g. sweet water farms) for which the farmers buy feed (which is expensive). Other advantages are that water purification plants may be planted in the ponds to eliminate the buildup of nitrogen, from fecal and other contamination. Also, the ponds can be left unprotected from natural predators, providing another kind of filtering.

Mariculture has rapidly expanded over the last two decades due to new technology, improvements in formulated feeds, greater biological understanding of farmed species, increased water quality within closed farm systems, greater demand for seafood products, site expansion and government interest. As a consequence, mariculture has been subject to some controversy regarding its social and environmental impacts. Commonly identified environmental impacts from marine farms are:


As with most farming practices, the degree of environmental impact depends on the size of the farm, the cultured species, stock density, type of feed, hydrography of the site, and husbandry methods. The adjacent diagram connects these causes and effects.

Mariculture of finfish can require a significant amount of fishmeal or other high protein food sources. Originally, a lot of fishmeal went to waste due to inefficient feeding regimes and poor digestibility of formulated feeds which resulted in poor feed conversion ratios.

In cage culture, several different methods are used for feeding farmed fish – from simple hand feeding to sophisticated computer-controlled systems with automated food dispensers coupled with "in situ" uptake sensors that detect consumption rates. In coastal fish farms, overfeeding primarily leads to increased disposition of detritus on the seafloor (potentially smothering seafloor dwelling invertebrates and altering the physical environment), while in hatcheries and land-based farms, excess food goes to waste and can potentially impact the surrounding catchment and local coastal environment. This impact is usually highly local, and depends significantly on the settling velocity of waste feed and the current velocity (which varies both spatially and temporally) and depth.

The impact of escapees from aquaculture operations depends on whether or not there are wild conspecifics or close relatives in the receiving environment, and whether or not the escapee is reproductively capable. Several different mitigation/prevention strategies are currently employed, from the development of infertile triploids to land-based farms which are completely isolated from any marine environment. Escapees can adversely impact local ecosystems through hybridization and loss of genetic diversity in native stocks, increase negative interactions within an ecosystem (such as predation and competition), disease transmission and habitat changes (from trophic cascades and ecosystem shifts to varying sediment regimes and thus turbidity).

The accidental introduction of invasive species is also of concern. Aquaculture is one of the main vectors for invasives following accidental releases of farmed stocks into the wild. One example is the Siberian sturgeon ("Acipenser baerii") which accidentally escaped from a fish farm into the Gironde Estuary (Southwest France) following a severe storm in December 1999 (5,000 individual fish escaped into the estuary which had never hosted this species before). Molluscan farming is another example whereby species can be introduced to new environments by ‘hitchhiking’ on farmed molluscs. Also, farmed molluscs themselves can become dominate predators and/or competitors, as well as potentially spread pathogens and parasites.

One of the primary concerns with mariculture is the potential for disease and parasite transfer. Farmed stocks are often selectively bred to increase disease and parasite resistance, as well as improving growth rates and quality of products. As a consequence, the genetic diversity within reared stocks decreases with every generation – meaning they can potentially reduce the genetic diversity within wild populations if they escape into those wild populations. Such genetic pollution from escaped aquaculture stock can reduce the wild population's ability to adjust to the changing natural environment. Species grown by mariculture can also harbour diseases and parasites (e.g., lice) which can be introduced to wild populations upon their escape. An example of this is the parasitic sea lice on wild and farmed Atlantic salmon in Canada. Also, non-indigenous species which are farmed may have resistance to, or carry, particular diseases (which they picked up in their native habitats) which could be spread through wild populations if they escape into those wild populations. Such ‘new’ diseases would be devastating for those wild populations because they would have no immunity to them.

With the exception of benthic habitats directly beneath marine farms, most mariculture causes minimal destruction to habitats. However, the destruction of mangrove forests from the farming of shrimps is of concern. Globally, shrimp farming activity is a small contributor to the destruction of mangrove forests; however, locally it can be devastating. Mangrove forests provide rich matrices which support a great deal of biodiversity – predominately juvenile fish and crustaceans. Furthermore, they act as buffering systems whereby they reduce coastal erosion, and improve water quality for in situ animals by processing material and ‘filtering’ sediments.

In addition, nitrogen and phosphorus compounds from food and waste may lead to blooms of phytoplankton, whose subsequent degradation can drastically reduce oxygen levels. If the algae are toxic, fish are killed and shellfish contaminated.

Mariculture development must be sustained by basic and applied research and development in major fields such as nutrition, genetics, system management, product handling, and socioeconomics. One approach is closed systems that have no direct interaction with the local environment. However, investment and operational cost are currently significantly higher than open cages, limiting them to their current role as hatcheries.

Sustainable mariculture promises economic and environmental benefits. Economies of scale imply that ranching can produce fish at lower cost than industrial fishing, leading to better human diets and the gradual elimination of unsustainable fisheries. Fish grown by mariculture are also perceived to be of higher quality than fish raised in ponds or tanks, and offer more diverse choice of species. Consistent supply and quality control has enabled integration in food market channels.






Scientific literature on mariculture can be found in the following journals:



</doc>
<doc id="19770" url="https://en.wikipedia.org/wiki?curid=19770" title="Memetics">
Memetics

Memetics is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Memetics describes how an idea can propagate successfully, but doesn't necessarily imply a concept is factual. Critics contend the theory is "untested, unsupported or incorrect". It has been labelled as pseudoscience by many scholars, making memetics unable to establish itself as a recognised research programme.

The term meme was coined in Richard Dawkins' 1976 book "The Selfish Gene," but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a "unit of culture" (an idea, belief, pattern of behaviour, etc.) which is "hosted" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.

In his book "The Selfish Gene" (1976), the evolutionary biologist Richard Dawkins used the term "meme" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. In 1975, Dr. Ted Cloak outlined the "corpuscles of culture" - an inspiring hypothesis that Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.

Dawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of "memetics" in "The Selfish Gene", but rather coined the term "meme" in a speculative spirit. Accordingly, different researchers came to define the term "unit of information" in different ways.

The evolutionary model of cultural information transfer is based on the concept that units of information, or "memes", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.

The modern memetics movement dates from the mid-1980s. A January 1983 "Metamagical Themas" column by Douglas Hofstadter, in "Scientific American", was influential – as was his 1985 book of the same name. "Memeticist" was coined as analogous to "geneticist" – originally in "The Selfish Gene." Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as "memetics" by analogy with "genetics". Dawkins' "The Selfish Gene" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of "Consciousness Explained" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay "Viruses of the Mind", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's "Snow Crash").

The idea of "language as a virus" had already been introduced by William S. Burroughs as early as 1962 in his book "The Ticket That Exploded", and later in "The Electronic Revolution", published in 1970 in "". Douglas Rushkoff explored the same concept in "Media Virus: Hidden Agendas in Popular Culture" in 1995.

However, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: "Virus of the Mind: The New Science of the Meme" by former Microsoft executive turned motivational speaker and professional poker-player Richard Brodie, and "Thought Contagion: How Belief Spreads Through Society" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not aware of "The Selfish Gene" until his book was very close to publication.

Around the same time as the publication of the books by Lynch and Brodie the e-journal "Journal of Memetics – Evolutionary Models of Information Transmission" (published electronically from 1997 to 2005) first appeared. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the "Journal of Ideas" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published "The Meme Machine", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.

The term "meme" derives from the Ancient Greek μιμητής ("mimētḗs"), meaning "imitator, pretender". The similar term "mneme" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work "Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen", translated into English in 1921 as "The Mneme". Until Daniel Schacter published "Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word "meme" independently of Semon, writing this:
"'Mimeme' comes from a suitable Greek root, but I want a monosyllable that sounds a bit like 'gene'. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to 'memory', or to the French word même."

The memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as "a unit of cultural transmission". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as "a unit of cultural information that can be copied, located in the brain". This thinking is more in line with Dawkins' second definition of the meme in his book "The Extended Phenotype". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.

These two schools became known as the "internalists" and the "externalists." Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, "i-memes", in response to external "e-memes".

An advanced statement of the internalist school came in 2002 with the publication of "The Electric Meme", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of "Darwinizing Culture: The Status of Memetics as a Science", edited by Aunger and with a foreword by Dennett, in 2001.

In 2005, the "Journal of Memetics" ceased publication and published a set of articles on the future of memetics. The website states that although "there was to be a relaunch... after several years nothing has happened". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words "meme" and "memetics" (without disowning the ideas in his book), adopting the self-description "thought contagionist". He died in 2005.

Susan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.
That is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or "memeplexes". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.

Critics contend that some proponents' assertions are "untested, unsupported or incorrect." Luis Benitez-Bribiesca, a critic of memetics, calls it "a pseudoscientific dogma" and "a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution" among other things. As factual criticism, he refers to the lack of a "code script" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in "Darwin's Dangerous Idea") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct.
Another criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.
Mary Midgley criticizes memetics for at least two reasons: 
Henry Jenkins, Joshua Green, and Sam Ford, in their book "Spreadable Media" (2013), criticize Dawkins' idea of the meme, writing that "while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture." The three authors also criticize other interpretations of memetics, especially those which describe memes as "self-replicating", because they ignore the fact that "culture is a human product and replicates through human agency."

Like other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is "heuristically trivial", being a mere redescription of what is already known without offering any useful novelty.



Research methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.

The application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.

Another application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).

Ben Cullen, in his book "Contagious Ideas", brought the idea of the meme into the discipline of archaeology. He coined the term "Cultural Virus Theory", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.

Francis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls "memetic selection criteria". These criteria opened the way to a specialized field of "applied memetics" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.

In "Selfish Sounds and Linguistic Evolution", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.

Australian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.

Swedish political scientist Mikael Sandberg argues against "Lamarckian" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active ("Lamarckian") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the "Lamarckian" IT strategy.





</doc>
<doc id="19773" url="https://en.wikipedia.org/wiki?curid=19773" title="March 25">
March 25





</doc>
<doc id="19780" url="https://en.wikipedia.org/wiki?curid=19780" title="List of islands of Michigan">
List of islands of Michigan

The following is a list of islands of Michigan. Michigan has the second longest coastline of any state after Alaska. Being bordered by four of the five Great Lakes—Erie, Huron, Michigan, and Superior—Michigan also has 64,980 inland lakes and ponds, as well as innumerable rivers, that may contain their own islands included in this list. The majority of the islands are within the Great Lakes. Other islands can also be found within other waterways of the Great Lake system, including Lake St. Clair, St. Clair River, Detroit River, and St. Marys River.

The largest of all the islands is Isle Royale in Lake Superior, which, in addition to its waters and other surrounding islands, is organized as Isle Royale National Park. Isle Royale itself is . The most populated island is Grosse Ile with approximately 10,000 residents, located in the Detroit River about south of Detroit. The majority of Michigan's islands are uninhabited and very small. Some of these otherwise unusable islands have been used for the large number of Michigan's lighthouses to aid in shipping throughout the Great Lakes, while others have been set aside as nature reserves. Many islands in Michigan have the same name, even some that are in the same municipality and body of water, such as Gull, Long, or Round islands.

Only Monroe County has territory in the westernmost portion of Lake Erie, which has a surface elevation of . The islands in the southern portion of the county are part of the North Maumee Bay Archeological District of the Detroit River International Wildlife Refuge. Turtle Island is the only island in the state of Michigan that is shared by another state. This remote and tiny island is cut in half and shared with Ohio.

Lake Huron is the second largest of the Great Lakes (after Lake Superior) with a surface area of . Michigan is the only U.S. state to border Lake Huron, while the portion of the lake on the other side of the international border belongs to the Canadian province of Ontario. The vast majority of Michigan's islands in Lake Huron are centered around Drummond Island in the northernmost portion of the state's lake territory. Drummond Island is the largest of Michigan's islands in Lake Huron and is the second largest Michigan island after Lake Superior's Isle Royale. Another large group of islands is the Les Cheneaux Islands archipelago, which itself contains dozens of small islands. Many of the lake's islands are very small and uninhabited.

As the most popular tourist destination in the state, Mackinac Island is the most well known of Lake Huron's islands. Drummond Island is the most populous of Michigan's islands in Lake Huron, with a population of 992 at the 2000 census. While Mackinac Island had a population of only about 500, there are thousands more seasonal workers and tourists during the summer months.

Michigan only has islands in Lake Michigan in the northern portion of the lake. There are no islands in the southern half of Lake Michigan. The largest and most populated of Michigan's islands in Lake Michigan is Beaver Island at and 551 residents. Some of the smaller islands surrounding Beaver Island are part of the larger Michigan Islands National Wildlife Refuge.

Lake Superior is the largest of the Great Lakes, and the coastline is sparsely populated. At , Isle Royale is the largest Michigan island and is the center of Isle Royale National Park, which itself contains over 450 islands. The following is a list of islands in Lake Superior that are "not" part of Isle Royale National Park. For those islands, see the list of islands in Isle Royale National Park.

Lake St. Clair connects Lake Huron and Lake Erie through the St. Clair River in the north and the Detroit River in the south. At , it is one of the largest non-Great Lakes in the United States, but it only contains a small number of islands near the mouth of the St. Clair River, where all of the following islands are located. The largest of these islands is Harsens Island, and all the islands are in Clay Township in St. Clair County.

The Detroit River runs for and connects Lake St. Clair to Lake Erie. For its entire length, it carries the international border between the United States and Canada. Some islands belong to Ontario in Canada and are not included in the list below. All islands on the American side belong to Wayne County. Portions of the southern portion of the river serve as wildlife refuges as part of the Detroit River International Wildlife Refuge. The largest and most populous island is Grosse Ile at and a population of around 10,000. Most of the islands are around and closely connected to Grosse Ile.

The St. Marys River connects Lake Superior and Lake Huron at the easternmost point of the Upper Peninsula. It carries the international border throughout its length, and some of the islands belong to neighboring Ontario. The largest of Michigan's islands in the river are Sugar Island and Neebish Island. Wider portions of the river are designated as Lake George, Lake Nicolet, and the Munuscong Lake. The whole length of the Michigan portion of the river is part of Chippewa County.

Michigan has numerous inland lakes and rivers that also contain their own islands. The following also lists the body of water in which these islands are located. Five islands below (<nowiki>*</nowiki> and highlighted in green) are actually islands within an island; they are contained within inland lakes in Isle Royale.

Grand Lake is a large lake in Presque Isle County. While it is not the largest inland lake in Michigan, it does contain the most inland islands that are officially named. At its shortest distance, it is located less than from Lake Huron, but the two are not connected. Grand Lake contains 14 islands, of which Grand Island is by far the largest.




</doc>
<doc id="19808" url="https://en.wikipedia.org/wiki?curid=19808" title="List of governors of Michigan">
List of governors of Michigan

The Governor of Michigan is the head of the executive branch of Michigan's state government and serves as the commander-in-chief of the state's military forces. The governor has a duty to enforce state laws; the power to either approve or veto appropriation bills passed by the Michigan Legislature; the power to convene the legislature; and the power to grant pardons, except in cases of impeachment. He or she is also empowered to reorganize the executive branch of the state government.

Michigan was originally part of French and British holdings, and administered by their colonial governors. After becoming part of the United States, numerous areas of what is today Michigan were originally part of the Northwest Territory, Indiana Territory and Illinois Territory, and administered by territorial governors. In 1805, the Michigan Territory was created, and five men served as territorial governors, until Michigan was granted statehood in 1837. Forty-eight individuals have held the position of state governor. The first female governor, Jennifer Granholm, served from 2003 to 2011.

After Michigan gained statehood, governors held the office for a 2-year term, until the 1963 Michigan Constitution changed the term to 4 years. The number of times an individual could hold the office was unlimited until a 1992 constitutional amendment imposed a lifetime term limit of 2 4-year governorships. The longest-serving governor in Michigan's history was William Milliken, who was promoted from lieutenant governor after Governor George W. Romney resigned, then was elected to three further successive terms. The only governors to serve non-consecutive terms were John S. Barry and Frank Fitzgerald.

Michigan was part of colonial New France until the Treaty of 1763 transferred ownership to the Kingdom of Great Britain. During this time, it was governed by the Lieutenants General of New France until 1627, the Governors of New France from 1627 to 1663, and the Governors General of New France until the transfer to Great Britain. The 1783 Treaty of Paris ceded the territory that is now Michigan to the United States as part of the end of the Revolutionary War, but British troops were not removed from the area until 1796. During the British ownership, their governors administrated the area as part of the Canadian territorial holdings.

Prior to becoming its own territory, parts of Michigan were administered by the governors of the Northwest Territory, the governors of the Indiana Territory and the governors of the Illinois Territory. On June 30, 1805, the Territory of Michigan was created, with General William Hull as the first territorial governor.

Michigan was admitted to the Union on January 26, 1837. The original 1835 Constitution of Michigan provided for the election of a governor and a lieutenant governor every 2 years. The fourth and current constitution of 1963 increased this term to four years. There was no term limit on governors until a constitutional amendment effective in 1993 limited governors to two terms.

Should the office of governor become vacant, the lieutenant governor becomes governor, followed in order of succession by the Secretary of State and the Attorney General. Prior to the current constitution, the duties of the office would devolve upon the lieutenant governor, without that person actually becoming governor. The term begins at noon on January 1 of the year following the election. Prior to the 1963 constitution, the governor and lieutenant governor were elected through separate votes, allowing them to be from different parties. In 1963, this was changed, so that votes are cast jointly for a governor and lieutenant governor of the same party.

Several governors also held other high positions within the state and federal governments. Eight governors served as U.S. House of Representatives members, while seven held positions in the U.S. Senate, all representing Michigan. Others have served as ambassadors, U.S. Cabinet members, and state and federal Supreme Court justices.

As of , there are four living former governors of Michigan. The most recent death of a former governor was that of William Milliken (served 1969-83) on October 18, 2019, aged 97. Milliken was also the most recently serving governor of Michigan to have died. The state's living former governors are:





</doc>
<doc id="19809" url="https://en.wikipedia.org/wiki?curid=19809" title="Moses Amyraut">
Moses Amyraut

Moïse Amyraut, Latin Moyses Amyraldus (Bourgueil, September 1596 – January 8, 1664), in English texts often Moses Amyraut, was a French Huguenot, Reformed theologian and metaphysician. He is perhaps most noted for his modifications to Calvinist theology regarding the nature of Christ's atonement, which is referred to as Amyraldism or Amyraldianism.

Born at Bourgueil, in the valley of the Changeon in the province of Anjou, his father was a lawyer, and, preparing Moses for his own profession, sent him, on the completion of his study of the humanities at Orléans to the university of Poitiers.

At the university he took the degree of licentiate (BA) of laws. On his way home from the university he passed through Saumur, and, having visited the pastor of the Protestant church there, was introduced by him to Philippe de Mornay, governor of the city. Struck with young Amyraut's ability and culture, they both urged him to change from law to theology. His father advised him to revise his philological and philosophical studies, and read over Calvin's "Institutions," before finally determining a course. He did so, and decided for theology.

He moved to the Academy of Saumur and studied under John Cameron, who ultimately regarded him as his greatest scholar. He had a brilliant course, and was in due time licensed as a minister of the French Protestant Church. The contemporary civil wars and excitements hindered his advancement. His first church was in Saint-Aignan, in the province of Maine. There he remained two years. Jean Daillé, who moved to Paris, advised the church at Saumur to secure Amyraut as his successor, praising him "as above himself." The university of Saumur at the same time had fixed its eyes on him as professor of theology. The great churches of Paris and Rouen also contended for him, and to win him sent their deputies to the provincial synod of Anjou.

Amyraut had left the choice to the synod. He was appointed to Saumur in 1633, and to the professor's chair along with the pastorate. On the occasion of his inauguration he maintained for thesis "De Sacerdotio Christi". His co-professors were Louis Cappel and Josué de la Place, who also were Cameron's pupils and lifelong friends, who collaborated in the "Theses Salmurienses", a collection of theses propounded by candidates in theology prefaced by the inaugural addresses of the three professors. Amyraut soon gave to French Protestantism a new direction.

In 1631 he published his "Traité des religions"; and from this year onward he was a foremost man in the church. Chosen to represent the provincial synod of Anjou, Touraine and Maine at the 1631 , he was appointed as orator to present to the king "The Copy of their Complaints and Grievances for the Infractions and Violations of the Edict of Nantes".

Previous deputies had addressed the king on their bent knees, whereas the representatives of the Catholics had been permitted to stand. Amyraut consented to be orator only if the assembly authorized him to stand. There was intense resistance. Cardinal Richelieu himself, preceded by lesser dignitaries, condescended to visit Amyraut privately, to persuade him to kneel; but Amyraut held resolutely to his point and carried it. His "oration" on this occasion, which was immediately published in the French "Mercure", remains a striking landmark in the history of French Protestantism. During his absence on this matter the assembly debated "whether the Lutherans who desired it, might be admitted into communion with the Reformed Churches of France at the Lord's Table." It was decided in the affirmative previous to his return; but he approved with astonishing eloquence, and thereafter was ever in the front rank in maintaining intercommunion between all churches holding the main doctrines of the Reformation.

Pierre Bayle recounts the title-pages of no fewer than thirty-two books of which Amyraut was the author. These show that he took part in all the great controversies on predestination and Arminianism which then so agitated and harassed all Europe. Substantially he held fast the Calvinism of his preceptor Cameron; but, like Richard Baxter in England, by his breadth and charity he exposed himself to all manner of misconstruction. In 1634 he published his "Traité de la predestination", in which he tried to mitigate the harsh features of predestination by his "Universalismus hypotheticus". God, he taught, predestines all men to happiness on condition of their having faith. This gave rise to a charge of heresy, of which he was acquitted at the national synod held at Alençon in 1637, and presided over by Benjamin Basnage (1580–1652). The charge was brought up again at the national synod of Charenton in 1644, when he was again acquitted. A third attack at the synod of Loudun in 1659 met with no better success. The university of Saumur became the university of French Protestantism.

Amyraut had as many as a hundred students in attendance upon his lectures. One of these was William Penn, who would later go on to found the Pennsylvania Colony in America based in part on Amyraut's notions of religious freedom . Another historic part filled by Amyraut was in the negotiations originated by Pierre le Gouz de la Berchère (1600–1653), first president of the "parlement" of Grenoble, when exiled to Saumur, for a reconciliation and reunion of the Catholics of France with the French Protestants. Very large were the concessions made by Richelieu in his personal interviews with Amyraut; but, as with the Worcester House negotiations in England between the Church of England and nonconformists, they inevitably fell through. On all sides the statesmanship and eloquence of Amyraut were conceded. His "De l'elevation de la foy et de l'abaissement de la raison en la creance des mysteres de la religion" (1641) gave him early a high place as a metaphysician. Exclusive of his controversial writings, he left behind him a very voluminous series of practical evangelical books, which have long remained the "fireside" favourites of the peasantry of French Protestantism. Amongst these are "Estat des fideles apres la mort"; "Sur l'oraison dominicale"; "Du merite des oeuvres"; "Traité de la justification"; and paraphrases of books of the Old and New Testament. His closing years were weakened by a severe fall he met with in 1657. He died on 18 January 1664.

There were a number of theologians who defended Calvinistic orthodoxy against Amyraut and Saumur, including Friedrich Spanheim (1600–1649) and Francis Turretin (1623–1687). Ultimately, the Helvetic Consensus was drafted to counteract the theology of Saumur and Amyraldism.




</doc>
<doc id="19811" url="https://en.wikipedia.org/wiki?curid=19811" title="Murray River">
Murray River

The Murray River (or River Murray) (Ngarrindjeri: "Millewa", Yorta Yorta: "Tongala") is Australia's longest river, at in length. The Murray rises in the Australian Alps, draining the western side of Australia's highest mountains, and then meanders across Australia's inland plains, forming the border between the states of New South Wales and Victoria as it flows to the northwest into South Australia. It turns south at Morgan for its final , reaching the ocean at Lake Alexandrina.

The water of the Murray flows through several terminal lakes that fluctuate in salinity (and were often fresh until recent decades) including Lake Alexandrina and The Coorong before emptying through the Murray Mouth into the southeastern portion of the Indian Ocean, often referenced on Australian maps as the Southern Ocean, near Goolwa. Despite discharging considerable volumes of water at times, particularly before the advent of large-scale river regulation, the mouth has always been comparatively small and shallow.

As of 2010, the Murray River system receives 58 percent of its natural flow. It is perhaps Australia's most important irrigated region, and it is widely known as the food bowl of the nation.

The Murray River forms part of the long combined Murray–Darling river system which drains most of inland Victoria, New South Wales, and southern Queensland. Overall the catchment area is one-seventh of Australia's total land mass. The Murray carries only a small fraction of the water of comparably-sized rivers in other parts of the world, and with a great annual variability of its flow. In its natural state it has even been known to dry up completely during extreme droughts, although that is extremely rare, with only two or three instances of this occurring since official record keeping began.

The Murray River makes up most of the border between the Australian states of Victoria and New South Wales. Where it does, the border is the top of the bank of the Victorian side of the river (i.e., none of the river itself is actually in Victoria). This was determined in a 1980 ruling by the High Court of Australia, which settled the question as to which state had jurisdiction in the unlawful death of a man who was fishing by the river's edge on the Victorian side of the river. This boundary definition can be ambiguous, since the river changes its course over time, and some of the river banks have been modified.

West of the line of longitude 141°E, the river continues as the border between Victoria and South Australia for approximately , where this is the only stretch where a state border runs down the middle of the river. This was due to a miscalculation during the 1840s, when the border was originally surveyed. Past this point, the Murray River is entirely within the state of South Australia.

The following major settlements are located along the course of the river, with population figures from the 2011 Census:

The Murray River (and associated tributaries) support a variety of river life adapted to its vagaries. This includes a variety of native fish such as the famous Murray cod, trout cod, golden perch, Macquarie perch, silver perch, eel-tailed catfish, Australian smelt, and western carp gudgeon, and other aquatic species like the Murray short-necked turtle, Murray River crayfish, broad-clawed yabbies, and the large clawed "Macrobrachium" shrimp, as well as aquatic species more widely distributed through southeastern Australia such as common longnecked turtles, common yabbies, the small claw-less "paratya" shrimp, water rats, and platypus. The Murray River also supports fringing corridors and forests of the river red gum.

The health of the Murray River has declined significantly since European settlement, particularly due to river regulation, and much of its aquatic life including native fish are now declining, rare or endangered. Recent extreme droughts (2000–07) have put significant stress on river red gum forests, with mounting concern over their long-term survival. The Murray has also flooded on occasion, the most significant of which was the flood of 1956, which inundated many towns on the lower Murray and which lasted for up to six months.

Introduced fish species such as carp, "gambusia", weather loach, redfin perch, brown trout, and rainbow trout have also had serious negative effects on native fish, while carp have contributed to environmental degradation of the Murray River and tributaries by destroying aquatic plants and permanently raising turbidity. In some segments of the Murray River, carp have become the only species found.

Between 2.5 and 0.5 million years ago the Murray River terminated in a vast freshwater lake called Lake Bungunnia. Lake Bungunnia was formed by earth movements that blocked the Murray River near Swan Reach during this period. At its maximum extent Lake Bungunnia covered , extending to near the Menindee Lakes in the north and to near Boundary Bend on the Murray in the south. The draining of Lake Bungunnia occurred approximately 600,000 years ago.

Deep clays deposited by the lake are evident in cliffs around Chowilla in South Australia. Considerably higher rainfall would have been required to keep such a lake full; the draining of Lake Bungunnia appears to mark the end of a wet phase in the history of the Murray-Darling Basin and the onset of widespread arid conditions similar to today. A species of "Neoceratodus" lungfish existed in Lake Bungunnia (McKay & Eastburn, 1990); today "Neoceratodus" lungfish are only found in several Queensland rivers.

The noted Barmah River Red Gum Forests owe their existence to the Cadell Fault. About 25,000 years ago, displacement occurred along the Cadell fault, raising the eastern edge of the fault, which runs north-south, above the floodplain. This created a complex series of events. A section of the original Murray River channel immediately behind the fault was abandoned, and it exists today as an empty channel known as Green Gully. The Goulburn River was dammed by the southern end of the fault to create a natural lake.

The Murray River flowed to the north around the Cadell Fault, creating the channel of the Edward River which exists today and through which much of the Murray River's waters still flow. Then the natural dam on the Goulburn River failed, the lake drained, and the Murray River avulsed to the south and started to flow through the smaller Goulburn River channel, creating "The Barmah Choke" and "The Narrows" (where the river channel is unusually narrow), before entering into the proper Murray River channel again.

This complex series of events, however, diverts attention from the primary result of the Cadell Fault – that the west-flowing water of the Murray River strikes the north-south fault and diverts both north and south around the fault in the two main channels (Edward and ancestral Goulburn) as well as a fan of small streams, and regularly floods a large amount of low-lying country in the area. These conditions are perfect for River Red Gums, which rapidly formed forests in the area. Thus the displacement of the Cadell Fault 25,000 BP led directly to the formation of the famous Barmah River Red Gum Forests.

The Barmah Choke and The Narrows mean the amount of water that can travel down this part of the Murray River is restricted. In times of flood and high irrigation flows the majority of the water, in addition to flooding the Red Gum forests, actually travels through the Edward River channel. The Murray River has not had enough flow power to naturally enlarge The Barmah Choke and The Narrows to increase the amount of water they can carry.

The Cadell Fault is quite noticeable as a continuous, low, earthen embankment as one drives into Barmah from the west, although to the untrained eye it may appear man-made.
The Murray Mouth is the point at which the Murray River empties into the sea, and the interaction between its shallow, shifting and variable currents and the open sea can be complex and unpredictable. During the peak period of Murray River commerce (roughly 1855 to 1920), it presented a major impediment to the passage of goods and produce between Adelaide and the Murray settlements, and many vessels foundered or were wrecked there.

Since the early 2000s, dredging machines have operated at the Murray Mouth, moving sand from the channel to maintain a minimal flow from the sea and into the Coorong's lagoon system. Without the 24-hour dredging, the mouth would silt up and close, cutting the supply of fresh sea-water into the Coorong, which would then warm up, stagnate and die.

Being one of the major river systems on one of the driest continents on Earth, the Murray has significant cultural relevance to Aboriginal Australians. According to the peoples of Lake Alexandrina, the Murray was created by the tracks of the Great Ancestor, Ngurunderi, as he pursued Pondi, the Murray Cod. The chase originated in the interior of New South Wales. Ngurunderi pursued the fish (who, like many totem animals in Aboriginal myths, is often portrayed as a man) on rafts (or "lala") made from red gums and continually launched spears at his target. But Pondi was a wily prey and carved a weaving path, carving out the river's various tributaries. Ngurunderi was forced to beach his rafts, and often create new ones as he changed from reach to reach of the river.

At Kobathatang, Ngurunderi finally got lucky and struck Pondi in the tail with a spear. However, the shock to the fish was so great it launched him forward in a straight line to a place called Peindjalang, near Tailem Bend. Eager to rectify his failure to catch his prey, the hunter and his two wives (sometimes the escaped sibling wives of Waku and Kanu) hurried on, and took positions high on the cliff on which Tailem Bend now stands. They sprung an ambush on Pondi only to fail again. Ngurunderi set off in pursuit again but lost his prey as Pondi dived into Lake Alexandrina. Ngurunderi and his women settled on the shore, only to suffer bad luck with fishing, being plagued by a water fiend known as Muldjewangk. They later moved to a more suitable spot at the site of present-day Ashville. The twin summits of Mount Misery are supposed to be the remnants of his rafts, they are known as "Lalangengall" or "the two watercraft".

This story of a hunter pursuing a Murray cod that carved out the Murray persists in numerous forms in various language groups that inhabit the enormous area spanned by the Murray system. The Wotojobaluk people of Victoria tell of Totyerguil from the area now known as Swan Hill who ran out of spears while chasing Otchtout the cod.

The first Europeans to encounter the river were Hamilton Hume and William Hovell, who crossed the river where Albury now stands in 1824: Hume named it the "Hume River" after his father. In 1830, Captain Charles Sturt reached the river after travelling down its tributary the Murrumbidgee River and named it the "Murray River" in honour of the then British Secretary of State for War and the Colonies, Sir George Murray, not realising it was the same river that Hume and Hovell had encountered further upstream.

Sturt continued down the remaining length of the Murray to finally reach Lake Alexandrina and the river's mouth. The area of the Murray Mouth was explored more thoroughly by Captain Collet Barker in 1831.

The first three settlers on the Murray River are known to have been James Collins Hawker (explorer and surveyor) along with E. J. Eyre (explorer and later Governor of Jamaica) plus E. B. Scott (onetime superintendent of Yatala Labour Prison). Hawker is known to have sold his share in the Bungaree Station which he founded with his brothers, and relocated alongside the Murray at a site near Moorundie

In 1852, Francis Cadell, in preparation for the launch of his steamer service, explored the river in a canvas boat, travelling downstream from Swan Hill.

In 1858, while acting as Minister of Land and Works for New South Wales, Irish nationalist and founder of Young Ireland, Charles Gavan Duffy, founded Carlyle Township on the Murray River, a township named after his close friend, Scottish historian and essayist Thomas Carlyle. Included in the township were "Jane Street," named in honor of Carlyle's wife Jane Carlyle and "Stuart-Mill Street" in honor of political philosopher John Stuart Mill (Duffy, 1892, "Conversations with Carlyle", pp. 202–203).

In 1858, the Government Zoologist, William Blandowski, along with Gerard Krefft, explored the lower reaches of the Murray and Darling rivers, compiling a list of birds and mammals.

George "Chinese" Morrison, then aged 18, navigated the river by canoe from Wodonga to its mouth, in 65 days, completing the 1,555-mile (2,503 km) journey in January 1881.

The lack of an estuary means that shipping cannot enter the Murray from the sea. However, in the 19th century the river supported a substantial commercial trade using shallow-draft paddle steamers, the first trips being made by two boats from South Australia on the spring flood of 1853. The "Lady Augusta", captained by Francis Cadell, reached Swan Hill while another, "Mary Ann", captained by William Randell, made it as far as Moama (near Echuca). In 1855 a steamer carrying gold-mining supplies reached Albury but Echuca was the usual turn-around point, though small boats continued to link with up-river ports such as Tocumwal, Wahgunya and Albury.

The arrival of steamboat transport was welcomed by pastoralists who had been suffering from a shortage of transport due to the demands of the gold fields. By 1860 a dozen steamers were operating in the high water season along the Murray and its tributaries. Once the railway reached Echuca in 1864, the bulk of the woolclip from the Riverina was transported via river to Echuca and then south to Melbourne.

The Murray was plagued by "snags", fallen trees submerged in the water, and considerable efforts were made to clear the river of these threats to shipping by using barges equipped with steam-driven winches. In recent times, efforts have been made to restore many of these snags by placing dead gum trees back into the river. The primary purpose of this is to provide habitat for fish species whose breeding grounds and shelter were eradicated by the removal of the snags.
The volume and value of river trade made Echuca Victoria's second port and in the decade from 1874 it underwent considerable expansion. By this time up to thirty steamers and a similar number of barges were working the river in season. River transport began to decline once the railways touched the Murray at numerous points. The unreliable levels made it impossible for boats to compete with the rail and later road transport. However, the river still carries pleasure boats along its entire length.

Today, most traffic on the river is recreational. Small private boats are used for water skiing and fishing. Houseboats are common, both commercial for hire and privately owned. There are a number of both historic paddle steamers and newer boats offering cruises ranging from half an hour to 5 days.

The Murray River has been a significant barrier to land-based travel and trade. Many of the ports for transport of goods along the Murray have also developed as places to cross the river, either by bridge or ferry. The first bridge to cross the Murray, which was built in 1869, is in the town of Murray Bridge, formerly called Edwards Crossing. Tolls applied on South Australian ferries until abolished in November 1961.

Small-scale pumping plants began drawing water from the Murray in the 1850s and the first high-volume plant was constructed at Mildura in 1887. The introduction of pumping stations along the river promoted an expansion of farming and led ultimately to the development of irrigation areas (including the Murrumbidgee Irrigation Area).

In 1915, the three Murray states – New South Wales, Victoria, and South Australia – signed the River Murray Agreement which proposed the construction of storage reservoirs in the river's headwaters as well as at Lake Victoria near the South Australian border. Along the intervening stretch of the river a series of locks and weirs were built. These were originally proposed to support navigation even in times of low water, but riverborne transport was already declining due to improved highway and railway systems.

Four large reservoirs were built along the Murray. In addition to Lake Victoria (completed late 1920s), these are Lake Hume near Albury–Wodonga (completed 1936), Lake Mulwala at Yarrawonga (completed 1939), and Lake Dartmouth, which is actually on the Mitta Mitta River upstream of Lake Hume (completed 1979). The Murray also receives water from the complex dam and pipeline system of the Snowy Mountains Scheme. An additional reservoir was proposed in the 1960s at Chowilla Dam which was to have been built in South Australia and would have flooded land mostly in Victoria and New South Wales. This reservoir was cancelled in favour of building Dartmouth Dam due to costs and concerns relating to increased salinity.

From 1935 to 1940 a series of barrages was built near the Murray Mouth to stop seawater egress into the lower part of the river during low flow periods. They are the Goolwa Barrage, located at , Mundoo Channel Barrage at , Boundary Creek Barrage at , Ewe Island Barrage at , and Tauwitchere Barrage at .
These dams inverted the patterns of the river's natural flow from the original winter-spring flood and summer-autumn dry to the present low level through winter and higher during summer. These changes ensured the availability of water for irrigation and made the Murray Valley Australia's most productive agricultural region, but have seriously disrupted the life cycles of many ecosystems both inside and outside the river, and the irrigation has led to dryland salinity that now threatens the agricultural industries.

The disruption of the river's natural flow, runoff from agriculture, and the introduction of pest species like the European carp has led to serious environmental damage along the river's length and to concerns that the river will be unusably salty in the medium to long term – a serious problem given that the Murray supplies 40 percent of the water supply for Adelaide. Efforts to alleviate the problems proceed but disagreement between various groups stalls progress.

In 2006, the state government of South Australia revealed its plan to investigate the construction of the controversial Wellington Weir.

Lock 1 was completed near Blanchetown in 1922. Torrumbarry weir downstream of Echuca began operating in December 1923. Of the numerous locks that were proposed, only thirteen were completed; Locks 1 to 11 on the stretch downstream of Mildura, Lock 15 at Euston and Lock 26 at Torrumbarry. Construction of the remaining weirs purely for navigation purposes was abandoned in 1934. The last lock to be completed was Lock 15, in 1937.
Lock 11, just downstream of Mildura, creates a long lock pool which aided irrigation pumping from Mildura and Red Cliffs.

Each lock has a navigable passage next to it through the weir, which is opened during periods of high river flow, when there is too much water for the lock. The weirs can be completely removed, and the locks completely covered by water during flood conditions. Lock 11 is unique in that the lock was built inside a bend of the river, with the weir in the bend itself. A channel was dug to the lock, creating an island between it and the weir. The weir is also of a different design, being dragged out of the river during high flow, rather than lifted out.


Major tributaries


Population centres





</doc>
<doc id="19812" url="https://en.wikipedia.org/wiki?curid=19812" title="Project Mercury">
Project Mercury

Project Mercury was the first human spaceflight program of the United States, running from 1958 through 1963. An early highlight of the Space Race, its goal was to put a man into Earth orbit and return him safely, ideally before the Soviet Union. Taken over from the US Air Force by the newly created civilian space agency NASA, it conducted twenty uncrewed developmental flights (some using animals), and six successful flights by astronauts. The program, which took its name from Roman mythology, cost $ adjusted for inflation. The astronauts were collectively known as the "Mercury Seven", and each spacecraft was given a name ending with a "7" by its pilot.

The Space Race began with the 1957 launch of the Soviet satellite Sputnik 1. This came as a shock to the American public, and led to the creation of NASA to expedite existing US space exploration efforts, and place most of them under civilian control. After the successful launch of the Explorer 1 satellite in 1958, crewed spaceflight became the next goal. The Soviet Union put the first human, cosmonaut Yuri Gagarin, into a single orbit aboard Vostok 1 on April 12, 1961. Shortly after this, on May 5, the US launched its first astronaut, Alan Shepard, on a suborbital flight. Soviet Gherman Titov followed with a day-long orbital flight in August 1961. The US reached its orbital goal on February 20, 1962, when John Glenn made three orbits around the Earth. When Mercury ended in May 1963, both nations had sent six people into space, but the Soviets led the US in total time spent in space.

The Mercury space capsule was produced by McDonnell Aircraft, and carried supplies of water, food and oxygen for about one day in a pressurized cabin. Mercury flights were launched from Cape Canaveral Air Force Station in Florida, on launch vehicles modified from the Redstone and Atlas D missiles. The capsule was fitted with a launch escape rocket to carry it safely away from the launch vehicle in case of a failure. The flight was designed to be controlled from the ground via the Manned Space Flight Network, a system of tracking and communications stations; back-up controls were outfitted on board. Small retrorockets were used to bring the spacecraft out of its orbit, after which an ablative heat shield protected it from the heat of atmospheric reentry. Finally, a parachute slowed the craft for a water landing. Both astronaut and capsule were recovered by helicopters deployed from a US Navy ship.

The Mercury project gained popularity, and its missions were followed by millions on radio and TV around the world. Its success laid the groundwork for Project Gemini, which carried two astronauts in each capsule and perfected space docking maneuvers essential for crewed lunar landings in the subsequent Apollo program announced a few weeks after the first crewed Mercury flight.

Project Mercury was officially approved on October 7, 1958 and publicly announced on December 17. Originally called Project Astronaut, President Dwight Eisenhower felt that gave too much attention to the pilot. Instead, the name "Mercury" was chosen from classical mythology, which had already lent names to rockets like the Greek "Atlas" and Roman "Jupiter" for the SM-65 and PGM-19 missiles. It absorbed military projects with the same aim, such as the Air Force Man in Space Soonest.

Following the end of World War II, a nuclear arms race evolved between the US and the Soviet Union (USSR). Since the USSR did not have bases in the western hemisphere from which to deploy bomber planes, Joseph Stalin decided to develop intercontinental ballistic missiles, which drove a missile race. The rocket technology in turn enabled both sides to develop Earth-orbiting satellites for communications, and gathering weather data and intelligence. Americans were shocked when the Soviet Union placed the first satellite into orbit in October 1957, leading to a growing fear that the US was falling into a "missile gap". A month later, the Soviets launched Sputnik 2, carrying a dog into orbit. Though the animal was not recovered alive, it was obvious their goal was human spaceflight. Unable to disclose details of military space projects, President Eisenhower ordered the creation of a civilian space agency in charge of civilian and scientific space exploration. Based on the federal research agency National Advisory Committee for Aeronautics (NACA), it was named the National Aeronautics and Space Administration (NASA). It achieved its first goal, an American satellite in space, in 1958. The next goal was to put a man there.

The limit of space (also known as the Kármán line) was defined at the time as a minimum altitude of , and the only way to reach it was by using rocket-powered boosters. This created risks for the pilot, including explosion, high g-forces and vibrations during lift off through a dense atmosphere, and temperatures of more than from air compression during reentry.

In space, pilots would require pressurized chambers or space suits to supply fresh air. While there, they would experience weightlessness, which could potentially cause disorientation. Further potential risks included radiation and micrometeoroid strikes, both of which would normally be absorbed in the atmosphere. All seemed possible to overcome: experience from satellites suggested micrometeoroid risk was negligible, and experiments in the early 1950s with simulated weightlessness, high g-forces on humans, and sending animals to the limit of space, all suggested potential problems could be overcome by known technologies. Finally, reentry was studied using the nuclear warheads of ballistic missiles, which demonstrated a blunt, forward-facing heat shield could solve the problem of heating.

T. Keith Glennan had been appointed the first Administrator of NASA, with Hugh L. Dryden (last Director of NACA) as his Deputy, at the creation of the agency on October 1, 1958. Glennan would report to the president through the National Aeronautics and Space Council. The group responsible for Project Mercury was NASA's Space Task Group, and the goals of the program were to orbit a crewed spacecraft around Earth, investigate the pilot's ability to function in space, and to recover both pilot and spacecraft safely. Existing technology and off-the-shelf equipment would be used wherever practical, the simplest and most reliable approach to system design would be followed, and an existing launch vehicle would be employed, together with a progressive test program. Spacecraft requirements included: a launch escape system to separate the spacecraft and its occupant from the launch vehicle in case of impending failure; attitude control for orientation of the spacecraft in orbit; a retrorocket system to bring the spacecraft out of orbit; drag braking blunt body for atmospheric reentry; and landing on water. To communicate with the spacecraft during an orbital mission, an extensive communications network had to be built. In keeping with his desire to keep from giving the US space program an overtly military flavor, President Eisenhower at first hesitated to give the project top national priority (DX rating under the Defense Production Act), which meant that Mercury had to wait in line behind military projects for materials; however, this rating was granted in May 1959, a little more than a year and a half after Sputnik was launched.

Twelve companies bid to build the Mercury spacecraft on a $20 million ($ adjusted for inflation) contract. In January 1959, McDonnell Aircraft Corporation was chosen to be prime contractor for the spacecraft. Two weeks earlier, North American Aviation, based in Los Angeles, was awarded a contract for Little Joe, a small rocket to be used for development of the launch escape system. The World Wide Tracking Network for communication between the ground and spacecraft during a flight was awarded to the Western Electric Company. Redstone rockets for suborbital launches were manufactured in Huntsville, Alabama, by the Chrysler Corporation and Atlas rockets by Convair in San Diego, California. For crewed launches, the Atlantic Missile Range at Cape Canaveral Air Force Station in Florida was made available by the USAF. This was also the site of the Mercury Control Center while the computing center of the communication network was in Goddard Space Center, Maryland. Little Joe rockets were launched from Wallops Island, Virginia. Astronaut training took place at Langley Research Center in Virginia, Lewis Flight Propulsion Laboratory in Cleveland, Ohio, and Naval Air Development Center Johnsville in Warminster, PA. Langley wind tunnels together with a rocket sled track at Holloman Air Force Base at Alamogordo, New Mexico were used for aerodynamic studies. Both Navy and Air Force aircraft were made available for the development of the spacecraft's landing system, and Navy ships and Navy and Marine Corps helicopters were made available for recovery. South of Cape Canaveral the town of Cocoa Beach boomed. From here, 75,000 people watched the first American orbital flight being launched in 1962.

The Mercury spacecraft's principal designer was Maxime Faget, who started research for human spaceflight during the time of the NACA. It was long and wide; with the launch escape system added, the overall length was . With of habitable volume, the capsule was just large enough for a single crew member. Inside were 120 controls: 55 electrical switches, 30 fuses and 35 mechanical levers. The heaviest spacecraft, Mercury-Atlas 9, weighed fully loaded. Its outer skin was made of René 41, a nickel alloy able to withstand high temperatures.

The spacecraft was cone shaped, with a neck at the narrow end. It had a convex base, which carried a heat shield (Item 2 in the diagram below) consisting of an aluminum honeycomb covered with multiple layers of fiberglass. Strapped to it was a retropack (1) consisting of three rockets deployed to brake the spacecraft during reentry. Between these were three minor rockets for separating the spacecraft from the launch vehicle at orbital insertion. The straps that held the package could be severed when it was no longer needed. Next to the heat shield was the pressurized crew compartment (3). Inside, an astronaut would be strapped to a form-fitting seat with instruments in front of him and with his back to the heat shield. Underneath the seat was the environmental control system supplying oxygen and heat, scrubbing the air of CO, vapor and odors, and (on orbital flights) collecting urine. The recovery compartment (4) at the narrow end of the spacecraft contained three parachutes: a drogue to stabilize free fall and two main chutes, a primary and reserve. Between the heat shield and inner wall of the crew compartment was a landing skirt, deployed by letting down the heat shield before landing. On top of the recovery compartment was the antenna section (5) containing both antennas for communication and scanners for guiding spacecraft orientation. Attached was a flap used to ensure the spacecraft was faced heat shield first during reentry. A launch escape system (6) was mounted to the narrow end of the spacecraft containing three small solid-fueled rockets which could be fired briefly in a launch failure to separate the capsule safely from its booster. It would deploy the capsule's parachute for a landing nearby at sea. (See also Mission profile for details.)

The Mercury spacecraft did not have an on-board computer, instead relying on all computation for reentry to be calculated by computers on the ground, with their results (retrofire times and firing attitude) then transmitted to the spacecraft by radio while in flight. All computer systems used in the Mercury space program were housed in NASA facilities on Earth. The computer systems were IBM 701 computers. (See also Ground control for details.)

The astronaut lay in a sitting position with his back to the heat shield, which was found to be the position that best enabled a human to withstand the high g-forces of launch and reentry. A fiberglass seat was custom-molded from each astronaut's space-suited body for maximum support. Near his left hand was a manual abort handle to activate the launch escape system if necessary prior to or during liftoff, in case the automatic trigger failed.

To supplement the onboard environmental control system, he wore a pressure suit with its own oxygen supply, which would also cool him. A cabin atmosphere of pure oxygen at a low pressure of (equivalent to an altitude of ) was chosen, rather than one with the same composition as air (nitrogen/oxygen) at sea level. This was easier to control, avoided the risk of decompression sickness ("the bends"), and also saved on spacecraft weight. Fires (which never occurred) would have to be extinguished by emptying the cabin of oxygen. In such case, or failure of the cabin pressure for any reason, the astronaut could make an emergency return to Earth, relying on his suit for survival. The astronauts normally flew with their visor up, which meant that the suit was not inflated. With the visor down and the suit inflated, the astronaut could only reach the side and bottom panels, where vital buttons and handles were placed.

The astronaut also wore electrodes on his chest to record his heart rhythm, a cuff that could take his blood pressure, and a rectal thermometer to record his temperature (this was replaced by an oral thermometer on the last flight). Data from these was sent to the ground during the flight. The astronaut normally drank water and ate food pellets.

Once in orbit, the spacecraft could be rotated in yaw, pitch, and roll: along its longitudinal axis (roll), left to right from the astronaut's point of view (yaw), and up or down (pitch). Movement was created by rocket-propelled thrusters which used hydrogen peroxide as a fuel. For orientation, the pilot could look through the window in front of him or he could look at a screen connected to a periscope with a camera which could be turned 360°.

The Mercury astronauts had taken part in the development of their spacecraft, and insisted that manual control, and a window, be elements of its design. As a result, spacecraft movement and other functions could be controlled three ways: remotely from the ground when passing over a ground station, automatically guided by onboard instruments, or manually by the astronaut, who could replace or override the two other methods. Experience validated the astronauts' insistence on manual controls. Without them, Gordon Cooper's manual reentry during the last flight would not have been possible.

The Mercury spacecraft design was modified three times by NASA between 1958 and 1959. After bidding by potential contractors had been completed, NASA selected the design submitted as "C" in November 1958. After it failed a test flight in July 1959, a final configuration, "D", emerged. The heat shield shape had been developed earlier in the 1950s through experiments with ballistic missiles, which had shown a blunt profile would create a shock wave that would lead most of the heat around the spacecraft. To further protect against heat, either a heat sink, or an ablative material, could be added to the shield. The heat sink would remove heat by the flow of the air inside the shock wave, whereas the ablative heat shield would remove heat by a controlled evaporation of the ablative material. After uncrewed tests, the latter was chosen for crewed flights. Apart from the capsule design, a rocket plane similar to the existing X-15 was considered. This approach was still too far from being able to make a spaceflight, and was consequently dropped. The heat shield and the stability of the spacecraft were tested in wind tunnels, and later in flight. The launch escape system was developed through uncrewed flights. During a period of problems with development of the landing parachutes, alternative landing systems such as the Rogallo glider wing were considered, but ultimately scrapped.

The spacecraft were produced at McDonnell Aircraft, St. Louis, Missouri, in clean rooms and tested in vacuum chambers at the McDonnell plant. The spacecraft had close to 600 subcontractors, such as Garrett AiResearch which built the spacecraft's environmental control system. Final quality control and preparations of the spacecraft were made at Hangar S at Cape Canaveral. NASA ordered 20 production spacecraft, numbered 1 through 20. Five of the 20, Nos. 10, 12, 15, 17, and 19, were not flown. Spacecraft No. 3 and No. 4 were destroyed during uncrewed test flights. Spacecraft No. 11 sank and was recovered from the bottom of the Atlantic Ocean after 38 years. Some spacecraft were modified after initial production (refurbished after launch abort, modified for longer missions, etc.). A number of Mercury boilerplate spacecraft (made from non-flight materials or lacking production spacecraft systems) were also made by NASA and McDonnell. They were designed and used to test spacecraft recovery systems and the escape tower. McDonnell also built the spacecraft simulators used by the astronauts during training.

A launch vehicle called Little Joe was used for uncrewed tests of the launch escape system, using a Mercury capsule with an escape tower mounted on it. Its main purpose was to test the system at max q, when aerodynamic forces against the spacecraft peaked, making separation of the launch vehicle and spacecraft most difficult. It was also the point at which the astronaut was subjected to the heaviest vibrations. The Little Joe rocket used solid-fuel propellant and was originally designed in 1958 by NACA for suborbital crewed flights, but was redesigned for Project Mercury to simulate an Atlas-D launch. It was produced by North American Aviation. It was not able to change direction; instead its flight depended on the angle from which it was launched. Its maximum altitude was fully loaded. A Scout launch vehicle was used for a single flight intended to evaluate the tracking network; however, it failed and was destroyed from the ground shortly after launch.

The Mercury-Redstone Launch Vehicle was an (with capsule and escape system) single-stage launch vehicle used for suborbital (ballistic) flights. It had a liquid-fueled engine that burned alcohol and liquid oxygen producing about of thrust, which was not enough for orbital missions. It was a descendant of the German V-2, and developed for the U.S. Army during the early 1950s. It was modified for Project Mercury by removing the warhead and adding a collar for supporting the spacecraft together with material for damping vibrations during launch. Its rocket motor was produced by North American Aviation and its direction could be altered during flight by its fins. They worked in two ways: by directing the air around them, or by directing the thrust by their inner parts (or both at the same time). Both the Atlas-D and Redstone launch vehicles contained an automatic abort sensing system which allowed them to abort a launch by firing the launch escape system if something went wrong. The Jupiter rocket, also developed by Von Braun's team at the Redstone Arsenal in Huntsville, was considered as well for intermediate Mercury suborbital flights at a higher speed and altitude than Redstone, but this plan was dropped when it turned out that man-rating Jupiter for the Mercury program would actually cost more than flying an Atlas due to economics of scale. Jupiter's only use other than as a missile system was for the short-lived Juno II launch vehicle, and keeping a full staff of technical personnel around solely to fly a few Mercury capsules would result in excessively high costs.

Orbital missions required use of the Atlas LV-3B, a man-rated version of the Atlas D which was originally developed as the United States' first operational intercontinental ballistic missile (ICBM) by Convair for the Air Force during the mid-1950s. The Atlas was a "one-and-one-half-stage" rocket fueled by kerosene and liquid oxygen (LOX). The rocket by itself stood high; total height of the Atlas-Mercury space vehicle at launch was .

The Atlas first stage was a booster skirt with two engines burning liquid fuel. This, together with the larger sustainer second stage, gave it sufficient power to launch a Mercury spacecraft into orbit. Both stages fired from lift-off with the thrust from the second stage sustainer engine passing through an opening in the first stage. After separation from the first stage, the sustainer stage continued alone. The sustainer also steered the rocket by thrusters guided by gyroscopes. Smaller vernier rockets were added on its sides for precise control of maneuvers.

NASA announced the following seven astronauts – known as the Mercury Seven – on April 9, 1959:

Shepard became the first American in space by making a suborbital flight in May 1961. He went on to fly in the Apollo program and became the only Mercury astronaut to walk on the Moon. Gus Grissom, who became the second American in space, also participated in the Gemini and Apollo programs, but died in January 1967 during a pre-launch test for Apollo 1. Glenn became the first American to orbit the Earth in February 1962, then quit NASA and went into politics, serving as a US Senator from 1974 to 1999, and returned to space in 1998 as a Payload Specialist aboard STS-95. Deke Slayton was grounded in 1962, but remained with NASA and was appointed Chief Astronaut at the beginning of Project Gemini. He remained in the position of senior astronaut, in charge of space crew flight assignments among many other responsibilities, until towards the end of Project Apollo, when he resigned and began training to fly on the Apollo-Soyuz Test Project in 1975, which he successfully did. Gordon Cooper became the last to fly in Mercury and made its longest flight, and also flew a Gemini mission. Carpenter's Mercury flight was his only trip into space. Schirra flew the third orbital Mercury mission, and then flew a Gemini mission. Three years later, he commanded the first crewed Apollo mission, becoming the only person to fly in all three of those programs.

One of the astronauts' tasks was publicity; they gave interviews to the press and visited project manufacturing facilities to speak with those who worked on Project Mercury. To make their travels easier, they requested and got jet fighters for personal use. The press was especially fond of John Glenn, who was considered the best speaker of the seven. They sold their personal stories to "Life" magazine which portrayed them as patriotic, God-fearing family men. "Life" was also allowed to be at home with the families while the astronauts were in space. During the project, Grissom, Carpenter, Cooper, Schirra and Slayton stayed with their families at or near Langley Air Force Base; Glenn lived at the base and visited his family in Washington DC on weekends. Shepard lived with his family at Naval Air Station Oceana in Virginia.

Other than Grissom, who was killed in the 1967 Apollo 1 fire, the other six survived past retirement and died between 1993 and 2016.

Prior to Project Mercury, there was no protocol for selecting astronauts so NASA would set a far reaching precedent with both their selection process and initial choices for astronaut. At the end of 1958, various ideas for the selection pool were discussed privately within the national government and the civilian space program, and also among the public at large. Initially, there was the idea to issue a widespread public call to volunteers. Thrill seekers such as rock climbers and acrobats would have been allowed to apply, but this idea was quickly shot down by NASA officials who understood that an undertaking such as space flight required individuals with professional training and education in flight engineering. By late 1958, NASA officials decided to move forward with test pilots being the heart of their selection pool. On President Eisenhower's insistence, the group was further narrowed down to active duty military test pilots, which set the number of candidates at 508 men who were USN or USMC naval aviation pilots (NAPs), or USAF pilots of Senior or Command rating. These men had long military records, which would give NASA officials more background information on which to base their decisions. Furthermore, these men were adept at flying the most advanced aircraft to date, giving them the best qualifications for the new position of astronaut. However, this selection excluded women since there were no female military test pilots at the time. It also excluded civilian NASA X-15 pilot Neil Armstrong, though he had been selected by the US Air Force in 1958 for its Man in Space Soonest program, which was replaced by Mercury. Although Armstrong had been a combat-experienced NAP during the Korean War, he left active duty in 1952. Armstrong became NASA's first civilian astronaut in 1962 when he was selected for NASA's second group, and became the first man on the Moon in 1969.

It was further stipulated that candidates should be between 25 and 40 years old, no taller than , and hold a college degree in a STEM subject. The college degree requirement excluded the USAF's X-1 pilot, then-Lt Col (later Brig Gen) Chuck Yeager, the first person to exceed the speed of sound. He later became a critic of the project, ridiculing the civilian space program, labeling astronauts as "spam in a can." John Glenn did not have a college degree either, but used influential friends to make the selection committee accept him. USAF Capt. (later Col.) Joseph Kittinger, a USAF fighter pilot and stratosphere balloonist, met all the requirements but preferred to stay in his contemporary project. Other potential candidates declined because they did not believe that human spaceflight had a future beyond Project Mercury. From the original 508, 110 candidates were selected for an interview, and from the interviews, 32 were selected for further physical and mental testing. Their health, vision, and hearing were examined, together with their tolerance to noise, vibrations, g-forces, personal isolation, and heat. In a special chamber, they were tested to see if they could perform their tasks under confusing conditions. The candidates had to answer more than 500 questions about themselves and describe what they saw in different images. Navy Lt (later Capt) Jim Lovell, who was later an astronaut in the Gemini and Apollo programs, did not pass the physical tests. After these tests it was intended to narrow the group down to six astronauts, but in the end it was decided to keep seven.

The astronauts went through a training program covering some of the same exercises that were used in their selection. They simulated the g-force profiles of launch and reentry in a centrifuge at the Naval Air Development Center, and were taught special breathing techniques necessary when subjected to more than 6 g. Weightlessness training took place in aircraft, first on the rear seat of a two-seater fighter and later inside converted and padded cargo aircraft. They practiced gaining control of a spinning spacecraft in a machine at the Lewis Flight Propulsion Laboratory called the Multi-Axis Spin-Test Inertia Facility (MASTIF), by using an attitude controller handle simulating the one in the spacecraft. A further measure for finding the right attitude in orbit was star and Earth recognition training in planetaria and simulators. Communication and flight procedures were practiced in flight simulators, first together with a single person assisting them and later with the Mission Control Center. Recovery was practiced in pools at Langley, and later at sea with frogmen and helicopter crews.

A Redstone rocket was used to boost the capsule for 2 minutes and 30 seconds to an altitude of ; the capsule continued ascending on a ballistic curve after booster separation. The launch escape system was jettisoned at the same time. At the top of the curve, the spacecraft's retrorockets were fired for testing purposes; they were not necessary for reentry because orbital speed had not been attained. The spacecraft landed in the Atlantic Ocean. The suborbital mission took about 15 minutes, had an apogee altitude of , and a downrange distance of . From the time of booster-spacecraft separation until reentry where air started to slow down the spacecraft, the pilot would experience weightlessness as shown on the image. The recovery procedure would be the same as an orbital mission.

Preparations for a mission started a month in advance with the selection of the primary and back-up astronaut; they would practice together for the mission. For three days prior to launch, the astronaut went through a special diet to minimize his need for defecating during the flight. On the morning of the trip he typically ate a steak breakfast. After having sensors applied to his body and being dressed in the pressure suit, he started breathing pure oxygen to prepare him for the atmosphere of the spacecraft. He arrived at the launch pad, took the elevator up the launch tower and entered the spacecraft two hours before launch. Once the astronaut was secured inside, the hatch was bolted, the launch area evacuated and the mobile tower rolled back. After this, the launch vehicle was filled with liquid oxygen. The entire procedure of preparing for launch and launching the spacecraft followed a time table called the countdown. It started a day in advance with a pre-count, in which all systems of the launch vehicle and spacecraft were checked. After that followed a 15-hour hold, during which pyrotechnics were installed. Then came the main countdown which for orbital flights started 6½ hours before launch (T – 390 min), counted backwards to launch (T = 0) and then forward until orbital insertion (T + 5 min).

On an orbital mission, the Atlas' rocket engines were ignited four seconds before lift-off. The launch vehicle was held to the ground by clamps and then released when sufficient thrust was built up at lift-off (A). After 30 seconds of flight, the point of maximum dynamic pressure against the vehicle was reached, at which the astronaut felt heavy vibrations. After 2 minutes and 10 seconds, the two outboard booster engines shut down and were released with the aft skirt, leaving the center sustainer engine running (B). At this point, the launch escape system was no longer needed, and was separated from the spacecraft by its jettison rocket (C). The space vehicle moved gradually to a horizontal attitude until, at an altitude of , the sustainer engine shut down and the spacecraft was inserted into orbit (D). This happened after 5 minutes and 10 seconds in a direction pointing east, whereby the spacecraft would gain speed from the rotation of the Earth. Here the spacecraft fired the three posigrade rockets for a second to separate it from the launch vehicle. Just before orbital insertion and sustainer engine cutoff, g-loads peaked at 8 g (6 g for a suborbital flight). In orbit, the spacecraft automatically turned 180°, pointed the retropackage forward and its nose 14.5° downward and kept this attitude for the rest of the orbital phase to facilitate communication with the ground.

Once in orbit, it was not possible for the spacecraft to change its trajectory except by initiating reentry. Each orbit would typically take 88 minutes to complete. The lowest point of the orbit, called perigee, was at about altitude, and the highest point, called apogee, was about altitude. When leaving orbit (E), the angle of retrofire was 34° downward from the flight path angle. Retrorockets fired for 10 seconds each (F) in a sequence where one started 5 seconds after the other. During reentry (G), the astronaut would experience about 8 g (11–12 g on a suborbital mission). The temperature around the heat shield rose to and at the same time, there was a two-minute radio blackout due to ionization of the air around the spacecraft.

After reentry, a small, drogue parachute (H) was deployed at for stabilizing the spacecraft's descent. The main parachute (I) was deployed at starting with a narrow opening that opened fully in a few seconds to lessen the strain on the lines. Just before hitting the water, the landing bag inflated from behind the heat shield to reduce the force of impact (J). Upon landing the parachutes were released. An antenna (K) was raised and sent out signals that could be traced by ships and helicopters. Further, a green marker dye was spread around the spacecraft to make its location more visible from the air. Frogmen brought in by helicopters inflated a collar around the craft to keep it upright in the water. The recovery helicopter hooked onto the spacecraft and the astronaut blew the escape hatch to exit the capsule. He was then hoisted aboard the helicopter that finally brought both him and the spacecraft to the ship.

The number of personnel supporting a Mercury mission was typically around 18,000, with about 15,000 people associated with recovery. Most of the others followed the spacecraft from the World Wide Tracking Network, a chain of 18 stations placed around the equator, which was based on a network used for satellites and made ready in 1960. It collected data from the spacecraft and provided two-way communication between the astronaut and the ground. Each station had a range of and a pass typically lasted 7 minutes. Mercury astronauts on the ground would take the role of Capsule Communicator, or CAPCOM, who communicated with the astronaut in orbit. Data from the spacecraft were sent to the ground, processed at the Goddard Space Center and relayed to the Mercury Control Center at Cape Canaveral. In the Control Center, the data were displayed on boards on each side of a world map, which showed the position of the spacecraft, its ground track and the place it could land in an emergency within the next 30 minutes.

The World Wide Tracking Network went on to serve subsequent space programs, until it was replaced by a satellite relay system in the 1980s. Mission Control Center was moved from Cape Canaveral to Houston in 1965.

On April 12, 1961 the Soviet cosmonaut Yuri Gagarin became the first person in space on an orbital flight. Alan Shepard became the first American in space on a suborbital flight three weeks later, on May 5, 1961. John Glenn, the third Mercury astronaut to fly, became the first American to reach orbit on February 20, 1962, but only after the Soviets had launched a second cosmonaut, Gherman Titov, into a day-long flight in August 1961. Three more Mercury orbital flights were made, ending on May 16, 1963 with a day-long, 22 orbit flight. However, the Soviet Union ended its Vostok program the next month, with the human spaceflight endurance record set by the 82-orbit, almost 5-day Vostok 5 flight.

All of the six crewed Mercury flights were successful, though some planned flights were canceled during the project (see below). The main medical problems encountered were simple personal hygiene, and post-flight symptoms of low blood pressure. The launch vehicles had been tested through uncrewed flights, therefore the numbering of crewed missions did not start with 1. Also, there were two separately numbered series: MR for "Mercury-Redstone" (suborbital flights), and MA for "Mercury-Atlas" (orbital flights). These names were not popularly used, since the astronauts followed a pilot tradition, each giving their spacecraft a name. They selected names ending with a "7" to commemorate the seven astronauts. Times given are Universal Coordinated Time, local time + 5 hours. MA = Mercury-Atlas, MR = Mercury-Redstone, LC = Launch Complex.

The 20 uncrewed flights used Little Joe, Redstone, and Atlas launch vehicles. They were used to develop the launch vehicles, launch escape system, spacecraft and tracking network. One flight of a Scout rocket attempted to launch a satellite for testing the ground tracking network, but failed to reach orbit. The Little Joe program used seven airframes for eight flights, of which three were successful. The second Little Joe flight was named Little Joe 6, because it was inserted into the program after the first 5 airframes had been allocated.

Nine of the planned flights were canceled. Suborbital flights were planned for four other astronauts but the number of flights was cut down gradually and finally all remaining were canceled after Titov's flight. Mercury-Atlas 9 was intended to be followed by more one-day flights and even a three-day flight but with the coming of the Gemini Project it seemed unnecessary. The Jupiter booster was, as mentioned above, intended to be used for different purposes.

Today the Mercury program is commemorated as the first American human space program. It did not win the race against the Soviet Union, but gave back national prestige and was scientifically a successful precursor of later programs such as Gemini, Apollo and Skylab.

During the 1950s, some experts doubted that human spaceflight was possible. Still, when John F. Kennedy was elected president, many, including he, had doubts about the project. As president he chose to support the programs a few months before the launch of "Freedom 7", which became a public success. Afterwards, a majority of the American public supported human spaceflight, and, within a few weeks, Kennedy announced a plan for a crewed mission to land on the Moon and return safely to Earth before the end of the 1960s.

The six astronauts who flew were awarded medals, driven in parades and two of them were invited to address a joint session of the US Congress. As a response to the selection criteria, which ruled out women, a private project was founded in which 13 women pilots successfully underwent the same tests as the men in Project Mercury. It was named Mercury 13 by the media Despite this effort, NASA did not select female astronauts until 1978 for the Space Shuttle.

On February 25, 2011, the Institute of Electrical and Electronics Engineers, the world's largest technical professional society, awarded Boeing (the successor company to McDonnell Aircraft) a Milestone Award for important inventions which debuted on the Mercury spacecraft.

On film, the program was portrayed in "The Right Stuff", a 1983 adaptation of Tom Wolfe's 1979 book of the same name, together with the HBO miniseries "From the Earth to the Moon" (1998), and "Hidden Figures" (2016).

In 1964, a monument commemorating Project Mercury was unveiled near Launch Complex 14 at Cape Canaveral, featuring a metal logo combining the symbol of Mercury with the number 7. In 1962, the United States Postal Service honored the Mercury-Atlas 6 flight with a Project Mercury commemorative stamp, the first US postal issue to depict a crewed spacecraft. 

The spacecraft that flew, together with some that did not, are on display in the United States. "Friendship 7" (capsule No. 13) went on a global tour, popularly known as its "fourth orbit". 

Commemorative patches were designed by entrepreneurs after the Mercury program to satisfy collectors.




</doc>
<doc id="19813" url="https://en.wikipedia.org/wiki?curid=19813" title="Gaius Maecenas">
Gaius Maecenas

Gaius Cilnius Maecenas (; c. 70 BC – 8 BC) was a friend and political advisor to Octavian, who later reigned as Augustus. He was also an important patron for the new generation of Augustan poets, including both Horace and Virgil. During the reign of Augustus, Maecenas served as a quasi-culture minister to the Emperor but in spite of his wealth and power he chose not to enter the Senate, remaining of equestrian rank.

Expressions in Propertius seem to imply that Maecenas had taken some part in the campaigns of Mutina, Philippi and Perugia. He prided himself on his ancient Etruscan lineage, and claimed descent from the princely house of the Cilnii, who excited the jealousy of their townsmen by their preponderant wealth and influence at Arretium in the 4th century BC. Horace makes reference to this in his address to Maecenas at the opening of his first books of "Odes" with the expression "atavis edite regibus" (descendant of kings). Tacitus refers to him as "Cilnius Maecenas"; it is possible that "Cilnius" was his mother's nomen – or that Maecenas was in fact a cognomen.
The Gaius Maecenas mentioned in Cicero as an influential member of the equestrian order in 91 BC may have been his grandfather, or even his father. The testimony of Horace and Maecenas's own literary tastes imply that he had profited from the highest education of his time.

His great wealth may have been in part hereditary, but he owed his position and influence to his close connection with the Emperor Augustus. He first appears in history in 40 BC, when he was employed by Octavian in arranging his marriage with Scribonia, and afterwards in assisting to negotiate the Treaty of Brundisium and the reconciliation with Mark Antony. As a close friend and advisor he had even acted as deputy for Augustus when he was abroad.

It was in 38 BC that Horace was introduced to Maecenas, who had before this received Lucius Varius Rufus and Virgil into his intimacy. In the "Journey to Brundisium," in 37, Maecenas and Marcus Cocceius Nerva – great-grandfather of the future emperor Nerva – are described as having been sent on an important mission, and they were successful in patching up, by the Treaty of Tarentum, a reconciliation between the two claimants for supreme power. During the Sicilian war against Sextus Pompeius in 36, Maecenas was sent back to Rome, and was entrusted with supreme administrative control in the city and in Italy. He was vicegerent of Octavian during the campaign that led to the Battle of Actium, when, with great promptness and secrecy, he crushed the conspiracy of Lepidus the Younger; during the subsequent absences of his chief in the provinces he again held the same position.
During the latter years of his life he fell somewhat out of favour with his master. Suetonius attributes the loss of the imperial favour to Maecenas' having indiscreetly revealed to Terentia, his beautiful but difficult wife, the discovery of the conspiracy in which her brother Lucius Licinius Varro Murena was implicated, but according to Cassius Dio (writing in the early 3rd century AD) it was due to the emperor's relations with Terentia. Maecenas died in 8 BC, leaving the emperor sole heir to his wealth.

Opinions were much divided in ancient times as to his personal character; but the testimony as to his administrative and diplomatic ability was unanimous. He enjoyed the credit of sharing largely in the establishment of the new order of things, of reconciling parties, and of carrying the new empire safely through many dangers. To his influence especially was attributed the more humane policy of Octavian after his first alliance with Antony and Lepidus. The best summary of his character as a man and a statesman, by Marcus Velleius Paterculus, describes him as "of sleepless vigilance in critical emergencies, far-seeing and knowing how to act, but in his relaxation from business more luxurious and effeminate than a woman." Expressions in the "Odes of Horace" seem to imply that Maecenas was deficient in the robustness of fibre which Romans liked to imagine was characteristic of their city.

Maecenas is most famous for his support of young poets, hence his name has become the eponym for a "patron of arts". He supported Virgil who wrote the "Georgics" in his honour. It was Virgil, impressed with examples of Horace's poetry, who introduced Horace to Maecenas. Indeed, Horace begins the first poem of his "Odes" ("Odes" I.i) by addressing his new patron. Maecenas gave him full financial support as well as an estate in the Sabine mountains. Propertius and the minor poets Varius Rufus, Plotius Tucca, Valgius Rufus and Domitius Marsus also were his protégés.

His character as a munificent patron of literature – which has made his name a household word – is gratefully acknowledged by the recipients of it and attested by the regrets of the men of letters of a later age, expressed by Martial and Juvenal. His patronage was exercised, not from vanity or a mere dilettante love of letters, but with a view to the higher interest of the state. He recognized in the genius of the poets of that time not only the truest ornament of the court, but the power of reconciling men's minds to the new order of things, and of investing the actual state of affairs with an ideal glory and majesty. The change in seriousness of purpose between the "Eclogues" and the "Georgics" of Virgil was in a great measure the result of the direction given by the statesman to the poet's genius. A similar change between the earlier odes of Horace, in which he declares his epicurean indifference to affairs of state, and the great national odes of the third book has been ascribed by some to the same guidance. However, since the organization of the Odes is not entirely chronological, and their composition followed both books of Satires and the Epodes, this argument is plainly specious; but doubtless the milieu of Maecenas's circle influenced the writing of the Roman Odes (III.1–6) and others such as the ode to Pollio, Motum ex Metello (II.1).

Maecenas endeavoured also to divert the less masculine genius of Propertius from harping continually on his love to themes of public interest, an effort which to some extent backfired in the ironic elegies of Book III. But if the motive of his patronage had been merely political, it never could have inspired the affection which it did in its recipients. The great charm of Maecenas in his relation to the men of genius who formed his circle was his simplicity, cordiality and sincerity. Although not particular in the choice of some of the associates of his pleasures, he admitted none but men of worth to his intimacy, and when once admitted they were treated like equals. Much of the wisdom of Maecenas probably lives in the "Satires" and "Epistles" of Horace. It has fallen to the lot of no other patron of literature to have his name associated with works of such lasting interest as the "Georgics" of Virgil, the first three books of Horace's "Odes," and the first book of his "Epistles."

Maecenas also wrote literature himself in both prose and verse. The some twenty fragments that remain show that he was less successful as an author than as a judge and patron of literature.
His prose works on various subjects – "Prometheus," dialogues like "Symposium" (a banquet at which Virgil, Horace and Messalla were present), "De cultu suo " (on his manner of life) and a poem "In Octaviam" ("Against Octavia") of which the content is unclear – were ridiculed by Augustus, Seneca and Quintilian for their strange style, the use of rare words and awkward transpositions.
According to Dio Cassius, Maecenas was also the inventor of a system of shorthand.

Maecenas sited his famous gardens, the first gardens in the Hellenistic-Persian garden style in Rome, on the Esquiline Hill, atop the Servian Wall and its adjoining necropolis, near the gardens of Lamia. It contained terraces, libraries and other aspects of Roman culture. Maecenas is said to have been the first to construct a swimming bath of hot water in Rome, which may have been in the gardens. The luxury of his gardens and villas incurred the displeasure of Seneca the Younger.
Though the approximate site is known, it is not easy to reconcile literary indications to determine the gardens' exact location, whether or not they lay on both sides of the Servian "ager" and both north and south of the porta Esquilina. Common graves of the archaic Esquiline necropolis have been found near the north-west corner of the modern Piazza Vittorio Emanuele, that is, outside the Esquiline gate of antiquity and north of the "via Tiburtina vetus"; most probably the "horti Maecenatiani" extended north from this gate and road on both sides of the "ager". The "Auditorium of Maecenas", a probable venue for dining and entertainment, may still be visited (upon reservation) on Largo Leopardi near Via Merulana.

The gardens became imperial property after Maecenas's death, and Tiberius lived there after his return to Rome in 2 AD. Nero connected them with the Palatine Hill via his Domus Transitoria, and viewed the burning of that from the turris Maecenatiana. This turris was probably the "molem propinquam nubibus arduis" ("the pile, among the clouds") mentioned by Horace.

Whether the "horti Maecenatiani" bought by Fronto actually were the former gardens of Maecenas is unknown, and the "domus Frontoniana" mentioned in the twelfth century by Magister Gregorius may also refer to the gardens of Maecenas.

His name has become a byword in many languages for a well-connected and wealthy patron. For instance, John Dewey, in his lectures Art as Experience, said "Economic patronage by wealthy and powerful individuals has at many times played a part in the encouragement of artistic production. Probably many a savage tribe had its Maecenas." He is celebrated for this role in two poems, the "Elegiae in Maecenatem", which were written after his death and collected in the "Appendix Vergiliana". In various languages, it has even been coined into a word for (private) patronage (mainly cultural, but sometimes wider, usually perceived as more altruistic than sponsorship). A verse of the student song "Gaudeamus igitur" wishes longevity upon the charity of the students' benefactors ("Maecenatum", genitive plural of "Maecenas").

In Poland and Western Ukraine, a lawyer would customarily be addressed with the honorific "Pan Mecenas", as lawyers were considered to be philanthropists and patrons of the arts.

In "The Great Gatsby", along with Midas and J. P. Morgan, Maecenas is one of the three famous wealthy men whose secrets narrator Nick Carraway hopes to find in the books he buys for his home library.

Maecenas was portrayed by Alex Wyndham in the second season of the 2005 HBO television series "Rome". He was portrayed by Russell Barr in the made-for-TV movie "". He is also featured in one episode of the second series of "Plebs" on ITV.





</doc>
<doc id="19814" url="https://en.wikipedia.org/wiki?curid=19814" title="Meander (disambiguation)">
Meander (disambiguation)

A meander is a bend in a river.

Meander may also refer to:








</doc>
<doc id="19818" url="https://en.wikipedia.org/wiki?curid=19818" title="March 16">
March 16





</doc>
<doc id="19820" url="https://en.wikipedia.org/wiki?curid=19820" title="Magick (Thelema)">
Magick (Thelema)

Magick, in the context of Aleister Crowley's Thelema, is a term used to show and differentiate the occult from performance magic and is defined as "the Science and Art of causing Change to occur in conformity with Will", including "mundane" acts of will as well as ritual magic. Crowley wrote that "it is theoretically possible to cause in any object any change of which that object is capable by nature". John Symonds and Kenneth Grant attach a deeper occult significance to this preference.

Crowley saw Magick as the essential method for a person to reach true understanding of the self and to act according to one's true will, which he saw as the reconciliation "between freewill and destiny." Crowley describes this process in his "Magick, Book 4":

The term itself is an Early Modern English spelling for "magic", used in works such as the 1651 translation of Heinrich Cornelius Agrippa's "De Occulta Philosophia", "Three Books of Occult Philosophy, or Of Magick". Aleister Crowley chose the spelling to differentiate his practices and rituals from stage magic (which may be more appropriately termed “illusion”) and the term has since been re-popularised by those who have adopted elements of his teachings.

Crowley defined Magick as "the science and art of causing change to occur in conformity with will." He goes on to elaborate on this, in one postulate, and twenty eight theorems. His first clarification on the matter is that of a postulate, in which he states "ANY required change may be effected by the application of the proper kind and degree of Force in the proper manner, through the proper medium to the proper object." He goes on further to state:

Crowley made many theories for the paranormal effects of Magick; however, as magicians and mystics had done before him and continue to do after him, Crowley dismissed such effects as useless:

Even so, Crowley asserted that paranormal effects and magical powers have some level of value for the individual:

There are several ways to view what Magick is. Again, at its most broad, it can be defined as any willed action leading to intended change. It can also be seen as the general set of methods used to accomplish the Great Work of mystical attainment. At the practical level, Magick most often takes several practices and forms of ritual, including banishing, invocation and evocation, eucharistic ritual, consecration and purification, astral travel, yoga, sex magic, and divination.

The professed purpose of banishing rituals is to eliminate forces that might interfere with a magical operation, and they are often performed at the beginning of an important event or ceremony (although they can be performed for their own sake as well). The area of effect can be a magick circle, a room, or the magician himself. The general theory of Magick proposes that there are various forces which are represented by the classical elements (air, earth, fire, and water), the planets, the signs of the Zodiac, and adjacent spaces in the astral world. Magick also proposes that various spirits and non-corporeal intelligences can be present. Banishings are performed in order to "clean out" these forces and presences. It is not uncommon to believe that banishings are more psychological than anything else, used to calm and balance the mind, but that the effect is ultimately the same—a sense of cleanliness within the self and the environment. There are many banishing rituals, but most are some variation on two of the most common—"The Star Ruby" and the Lesser Banishing Ritual of the Pentagram.

Crowley describes banishing in his "Magick, Book 4" (ch.13):

However, he further asserts:

Purification is similar in theme to banishing, but is a more rigorous process of preparing the self and her temple for serious spiritual work. Crowley mentions that ancient magicians would purify themselves through arduous programs, such as through special diets, fasting, sexual abstinence, keeping the body meticulously tidy, and undergoing a complicated series of prayers. He goes on to say that purification no longer requires such activity, since the magician can purify the self via willed intention. Specifically, the magician labors to purify the mind and body of all influences which may interfere with the Great Work:

Crowley recommended symbolically ritual practices, such as bathing and robing before a main ceremony: "The bath signifies the removal of all things extraneous or antagonistic to the one thought. The putting on of the robe is the positive side of the same operation. It is the assumption of the frame of mind suitable to that one thought."

Consecration is an equally important magical operation. It is essentially the dedication, usually of a ritual instrument or space, to a specific purpose. In "Magick, Book 4" (ch.13), Crowley writes:

Invocation is the bringing in or identifying with a particular deity or spirit. Crowley wrote of two keys to success in this arena: to "inflame thyself in praying" and to "invoke often". For Crowley, the single most important invocation, or any act of Magick for that matter, was the invocation of one's Holy Guardian Angel, or "secret self", which allows the adept to know his or her True Will.

Crowley describes the experience of invocation:

Crowley ("Magick, Book 4") discusses three main categories of invocation, although "in the great essentials these three methods are one. In each case the magician identifies himself with the Deity invoked."


Another invocatory technique that the magician can employ is called the "assumption of godforms"—where with "concentrated imagination of oneself in the symbolic shape of any God, one should be able to identify oneself with the idea which [the god] represents." A general method involves positioning the body in a position that is typical for a given god, imagining that the image of the god is coinciding with or enveloping the body, accompanied by the practice of "vibration" of the appropriate god-name(s).

There is a distinct difference between invocation and evocation, as Crowley explains:

Generally, evocation is used for two main purposes: to gather information and to obtain the services or obedience of a spirit or demon. Crowley believed that the most effective form of evocation was found in the grimoire on Goetia (see below), which instructs the magician in how to safely summon forth and command 72 infernal spirits. However, it is equally possible to evoke angelic beings, gods, and other intelligences related to planets, elements, and the Zodiac.

Unlike with invocation, which involves a calling in, evocation involves a calling forth, most commonly into what is called the "triangle of art."


The word "eucharist" originally comes from the Greek word for thanksgiving. However, within Magick, it takes on a special meaning—the transmutation of ordinary things (usually food and drink) into divine sacraments, which are then consumed. The object is to infuse the food and drink with certain properties, usually embodied by various deities, so that the adept takes in those properties upon consumption. Crowley describes the process of the regular practice of eucharistic ritual:

There are several eucharistic rituals within the magical canon. Two of the most well known are The Mass of the Phoenix and The Gnostic Mass. The first is a ritual designed for the individual, which involves sacrificing a "Cake of Light" (a type of bread that serves as the host) to Ra (i.e. the Sun) and infusing a second Cake with the adept's own blood (either real or symbolic, in a gesture reflecting the myth of the Pelican cutting its own breast to feed its young) and then consuming it with the words, "There is no grace: there is no guilt: This is the Law: Do what thou wilt!" The other ritual, The Gnostic Mass, is a very popular public ritual (although it can be practiced privately) that involves a team of participants, including a Priest and Priestess. This ritual is an enactment of the mystical journey that culminates with the Mystic Marriage and the consumption of a Cake of Light and a goblet of wine (a process termed "communication"). Afterwards, each Communicant declares, "There is no part of me that is not of the gods!"

Yoga, as Crowley interprets it, involves several key components. The first is Asana, which is the assumption (after eventual success) of any easy, steady and comfortable posture so as to maintain a good physique which complements the high level of enlightenment that meditation is accompanied with. Next is Pranayama, which is the control of breath. Yogis believe that the number of breaths a human takes are counted before one is even born and thus, by controlling the intake one may also be able to control the life. Mantram, the use of mantras enables the subject to use the knowledge of the Vedas "Atharva Veda" in this context adequately. Yama and Niyama are the adopted moral or behavioral codes (of the adept's choosing) that will be least likely to excite the mind. Pratyahara is the stilling of the thoughts so that the mind becomes quiet. Dharana is the beginning of concentration, usually on a single shape, like a triangle, which eventually leads to Dhyana, the loss of distinction between object and subject, which can be described as the annihilation of the ego (or sense of a separate self). The final stage is Samādhi—Union with the All; it is considered to be the utmost level of awareness that one could possibly achieve. According to Hindu mythology, one of their main three deities, Shiva, had mastered this and thus was bestowed upon with stupendous power and control.

The art of divination is generally employed for the purpose of obtaining information that can guide the adept in his Great Work. The underlying theory states that there exists intelligences (either outside of or inside the mind of the diviner) that can offer accurate information within certain limits using a language of symbols. Normally, divination within Magick is not the same as fortune telling, which is more interested in predicting future events. Rather, divination tends to be more about discovering information about the nature and condition of things that can help the magician gain insight and to make better decisions.

There are literally hundreds of different divinatory techniques in the world. However, Western occult practice mostly includes the use of astrology (calculating the influence of heavenly bodies), bibliomancy (reading random passages from a book, such as Liber Legis or the I Ching), tarot (a deck of 78 cards, each with symbolic meaning, usually laid out in a meaningful pattern), and geomancy (a method of making random marks on paper or in earth that results in a combination of sixteen patterns).

It is an accepted truism within Magick that divination is imperfect. As Crowley writes, "In estimating the ultimate value of a divinatory judgment, one must allow for more than the numerous sources of error inherent in the process itself. The judgment can do no more than the facts presented to it warrant. It is naturally impossible in most cases to make sure that some important factor has not been omitted [...] One must not assume that the oracle is omniscient."

The Tree of Life is a tool used to categorize and organize various mystical concepts. At its most simple level, it is composed of ten spheres, or emanations, called sephiroth (sing. "sephira") which are connected by twenty two paths. The sephiroth are represented by the planets and the paths by the characters of the Hebrew alphabet, which are subdivided by the four classical elements, the seven classical planets, and the twelve signs of the Zodiac. Within the western magical tradition, the Tree is used as a kind of conceptual filing cabinet. Each sephira and path is assigned various ideas, such as gods, cards of the Tarot, astrological planets and signs, elements, etc.

Crowley considered a deep understanding of the Tree of Life to be essential to the magician:

Similar to yoga, learning the Tree of Life is not so much Magick as it is a way to map out one's spiritual universe. As such, the adept may use the Tree to determine a destination for astral travel, to choose which gods to invoke for what purposes, et cetera. It also plays an important role in modeling the spiritual journey, where the adept begins in Malkuth, which is the every-day material world of phenomena, with the ultimate goal being at Kether, the sphere of Unity with the All.

A magical record is a journal or other source of documentation containing magical events, experiences, ideas, and any other information that the magician may see fit to add. There can be many purposes for such a record, such as recording evidence to verify the effectiveness of specific procedures (per the scientific method that Aleister Crowley claimed should be applied to the practice of Magick) or to ensure that data may propagate beyond the lifetime of the magician. Benefits of this process vary, but usually include future analysis and further education by the individual and/or associates with whom the magician feels comfortable in revealing such intrinsically private information.

Crowley was highly insistent upon the importance of this practice. As he writes in Liber E, "It is absolutely necessary that all experiments should be recorded in detail during, or immediately after, their performance ... The more scientific the record is, the better. Yet the emotions should be noted, as being some of the conditions. Let then the record be written with sincerity and care; thus with practice it will be found more and more to approximate to the ideal." Other items he suggests for inclusion include the physical and mental condition of the experimenter, the time and place, and environmental conditions, including the weather.

As with Magick itself, a "magical weapon" is any instrument used to bring about intentional change. As Crowley writes, "Illustration: It is my Will to inform the World of certain facts within my knowledge. I therefore take "magical weapons", pen, ink, and paper ... The composition and distribution of this book is thus an act of Magick by which I cause Changes to take place in conformity with my Will." With that said, in practice, magical weapons are usually specific, consecrated items used within ceremonial magic. There is no hard and fast rule for what is or isn't a magical weapon—if a magician considers it such a weapon, then it is. However, there does exist a set of magical weapons that have particular uses and symbolic meanings. Common weapons include the dagger (or athame in neopagan parlance), sword, wand, holy oil, cup (or graal), disk (or pentacle), oil lamp, bell, and thurible (or censer).

A magical formula is generally a name, word, or series of letters whose meaning illustrates principles and degrees of understanding that are often difficult to relay using other forms of speech or writing. It is a concise means to communicate very abstract information through the medium of a word or phrase, usually regarding a process of spiritual or mystical change. Common formulae include INRI, IAO, ShT, AUMGN, NOX, and LVX.

These words often have no intrinsic meaning in and of themselves. However, when deconstructed, each individual letter may refer to some universal concept found in the system that the formula appears. Additionally, in grouping certain letters together one is able to display meaningful sequences that are considered to be of value to the spiritual system that utilizes them (e.g. spiritual hierarchies, historiographic data, psychological stages, etc.)

In magical rituals involving the invocation of deities, a vocal technique called "vibration" is commonly used. This was a basic aspect of magical training for Crowley, who described it in "Liber O." According to that text, vibration involves a physical set of steps, starting in a standing position, breathing in through the nose while imagining the name of the god entering with the breath, imagining that breath travelling through the entire body, stepping forward with the left foot while throwing the body forward with arms outstretched, visualizing the name rushing out when spoken, ending in an upright stance, with the right forefinger placed upon the lips. According to Crowley in "Liber O", success in this technique is signaled by physical exhaustion and "though only by the student himself is it perceived, when he hears the name of the God vehemently roared forth, as if by the concourse of ten thousand thunders; and it should appear to him as if that Great Voice proceeded from the Universe, and not from himself."

In general ritual practice, "vibration" can also refer to a technique of saying a god-name or a magical formula in a long, drawn-out fashion (i.e. with a full, deep breath) that employs the nasal passages, such that the sound feels and sounds "vibrated'. This is known as Galdering.







</doc>
<doc id="19821" url="https://en.wikipedia.org/wiki?curid=19821" title="Marcus Claudius Tacitus">
Marcus Claudius Tacitus

Marcus Claudius Tacitus (; c. 200 – June 276) was Roman emperor from 275 to 276. During his short reign he campaigned against the Goths and the Heruli, for which he received the title "Gothicus Maximus".

Tacitus was born in Interamna (Terni), in Italia. He circulated copies of the historian Gaius Cornelius Tacitus' work, which was barely read at the time, perhaps contributing to the partial survival of the historian's work. Modern historiography rejects his claimed descent from the historian as a fabrication. In the course of his long life he discharged the duties of various civil offices, holding the consulship twice, once under Valerian and again in 273, earning universal respect.

After the assassination of Aurelian, the army, apparently in remorse at the effects of the previous centuries' military license, which had brought about the death of the well-liked emperor, relinquished the right of choosing his successor to the Senate. Initially, the Senate hesitated to accept the responsibility, but when the delay had gone on eight months from Aurelian's death it at last determined to settle the matter and offered the throne to the aged "Princeps Senatus", Tacitus. 

Tacitus, after ascertaining the sincerity of the Senate's regard for him, accepted their nomination on 25 September 275, and the choice was cordially ratified by the army. This was the last time the Senate elected a Roman Emperor. The interregnum between Aurelian and Tacitus had been quite long, and there is substantial evidence that Aurelian's wife, Ulpia Severina, ruled in her own right before the election of Tacitus. Tacitus had been living in Campania before his election, and returned only reluctantly to the assembly of the Senate in Rome, where he was elected.
He immediately asked the Senators to deify Aurelian, before arresting and executing Aurelian's murderers.

Amongst the highest concerns of the new reign was the restoration of the ancient Senatorial powers. He granted substantial prerogatives to the Senate, securing to them by law the appointment of the emperor, of the consuls, and the provincial governors, as well as supreme right of appeal from every court in the empire in its judicial function, and the direction of certain branches of the revenue in its long-abeyant administrative capacity. Probus respected these changes, but after the reforms of Diocletian in the succeeding decades not a vestige would be left of them.

Next he moved against the barbarian mercenaries that had been gathered by Aurelian to supplement Roman forces for his Eastern campaign. These mercenaries had plundered several towns in the Eastern Roman provinces after Aurelian had been murdered and the campaign cancelled. His half-brother, the Praetorian Prefect Florianus, and Tacitus himself won a victory against these tribes, among which were the Heruli, gaining the emperor the title "Gothicus Maximus".

On his way back to the west to deal with a Frankish and Alamannic invasion of Gaul, according to Aurelius Victor, Eutropius and the Historia Augusta, Tacitus died of fever at Tyana in Cappadocia in June 276. It was reported that he began acting strangely, declaring that he would alter the names of the months to honor himself, before succumbing to a fever. In a contrary account, Zosimus claims he was assassinated, after appointing one of his relatives to an important command in Syria.





</doc>
<doc id="19822" url="https://en.wikipedia.org/wiki?curid=19822" title="MV Tampa">
MV Tampa

MV "Tampa" was a roll-on/roll-off container ship completed in 1984 by Hyundai Heavy Industries Co., Ltd. in South Korea for the Norway-based firm, Wilhelmsen Lines Shipowning.

In August 2001, under Captain Arne Rinnan, a diplomatic dispute brewed between Australia, Norway, and Indonesia after "Tampa" rescued 438 Afghans from a distressed fishing vessel in international waters. The Afghans wanted passage to nearby Christmas Island. The Australian government sought to prevent this by refusing "Tampa" entry into Australian waters, insisting on their disembarkment elsewhere, and deploying the Special Air Service Regiment to board the ship. At the time of the incident, "Tampa" carried cargo worth , and 27 crew.

The crew of "Tampa" received the Nansen Refugee Award for 2002 from the United Nations High Commissioner for Refugees (UNHCR) for their efforts to follow international principles of saving people in distress at sea.

In October 2006, MV "Tampa" was one of two Wilhelmsen ships involved in a cocaine-smuggling operation intercepted by the New Zealand Customs Service and the Australian Federal Police. Twenty-seven kilograms of cocaine was allegedly attached to the side of the two cargo ships bound for Australia in purpose-built metal pods, although New Zealand authorities stated they did not believe the ship's crew or owners were involved.

In the 2006 episode "Whirlpool/Snowplow" of the TV show "MythBusters", a 1:550 scale model of MV "Tampa" was assembled, weighted with lead shot to simulate a full load of cargo, and used as a scale scientific test bed vehicle for determining whether ocean whirlpools are capable of sinking a large container ship.






</doc>
<doc id="19823" url="https://en.wikipedia.org/wiki?curid=19823" title="Maya numerals">
Maya numerals

The Mayan numeral system was the system to represent numbers and calendar dates in the Maya civilization. It was a vigesimal (base-20) positional numeral system. The numerals are made up of three symbols; zero (shell shape, with the plastron uppermost), one (a dot) and five (a bar). For example, thirteen is written as three dots in a horizontal row above two horizontal bars; sometimes it is also written as three vertical dots to the left of two vertical bars. With these three symbols each of the twenty vigesimal digits could be written.
Numbers after 19 were written vertically in powers of twenty. The Mayan used powers of twenty, just as the Hindu–Arabic numeral system uses powers of tens. For example, thirty-three would be written as one dot, above three dots atop two bars. The first dot represents "one twenty" or "1×20", which is added to three dots and two bars, or thirteen. Therefore, (1×20) + 13 = 33. Upon reaching 20 or 400, another row is started (20 or 8000, then 20 or 160,000, and so on). The number 429 would be written as one dot above one dot above four dots and a bar, or (1×20) + (1×20) + 9 = 429. 

Other than the bar and dot notation, Maya numerals were sometimes illustrated by face type glyphs or pictures. The face glyph for a number represents the deity associated with the number. These face number glyphs were rarely used, and are mostly seen on some of the most elaborate monumental carving.

Adding and subtracting numbers below 20 using Maya numerals is very simple.
Addition is performed by combining the numeric symbols at each level:<br>
If five or more dots result from the combination, five dots are removed and replaced by a bar. If four or more bars result, four bars are removed and a dot is added to the next higher row.

Similarly with subtraction, remove the elements of the subtrahend symbol from the minuend symbol:<br>
If there are not enough dots in a minuend position, a bar is replaced by five dots. If there are not enough bars, a dot is removed from the next higher minuend symbol in the column and four bars are added to the minuend symbol which is being worked on.

The "Long Count" portion of the Maya calendar uses a variation on the strictly vigesimal numbering. In the second position, only the digits up to 17 are used, and the place value of the third position is not 20×20 = 400, as would otherwise be expected, but 18×20 = 360, so that one dot over two zeros signifies 360. Presumably, this is because 360 is roughly the number of days in a year. (The Maya had however a quite accurate estimation of 365.2422 days for the solar year at least since the early Classic era.) Subsequent positions use all twenty digits and the place values continue as 18×20×20 = 7,200 and 18×20×20×20 = 144,000, etc.

Every known example of large numbers in the Maya system uses this 'modified vigesimal' system, with the third position representing multiples of 18×20. It is reasonable to assume, but not proven by any evidence, that the normal system in use was a pure base-20 system.

Several Mesoamerican cultures used similar numerals and base-twenty systems and the Mesoamerican Long Count calendar requiring the use of zero as a place-holder. The earliest long count date (on Stela 2 at Chiapa de Corzo, Chiapas) is from 36 BC.

Since the eight earliest Long Count dates appear outside the Maya homeland, it is assumed that the use of zero and the Long Count calendar predated the Maya, and was possibly the invention of the Olmec. Indeed, many of the earliest Long Count dates were found within the Olmec heartland. However, the Olmec civilization had come to an end by the 4th century BC, several centuries before the earliest known Long Count dates—which suggests that zero was "not" an Olmec discovery.

Mayan numerals were added to the Unicode Standard in June, 2018 with the release of version 11.0.

The Unicode block for Mayan Numerals is U+1D2E0–U+1D2FF:




</doc>
<doc id="19826" url="https://en.wikipedia.org/wiki?curid=19826" title="Michael Foot">
Michael Foot

Michael Mackintosh Foot (23 July 19133 March 2010) was a British Labour Party politician who served as Labour Leader from 1980 to 1983. Foot began his career as a journalist on "Tribune" and the "Evening Standard". He co-wrote the 1940 polemic against appeasement of Adolf Hitler, "Guilty Men", under a pseudonym.

Foot served as a Member of Parliament (MP) from 1945 to 1955 and again from 1960 until he retired in 1992. A passionate orator, and associated with the left wing of the Labour Party for most of his career, Foot was an ardent supporter of the Campaign for Nuclear Disarmament and of British withdrawal from the European Economic Community (EEC). He was appointed to the Cabinet as Secretary of State for Employment under Harold Wilson in 1974, and he later served as Leader of the House of Commons (1976–1979) under James Callaghan. He was also Deputy Leader of the Labour Party under Callaghan from 1976 to 1980.

Elected as a compromise candidate, Foot served as the Leader of the Labour Party, and Leader of the Opposition from 1980 to 1983. His strongly left-wing political positions and criticisms of vacillating leadership made him an unpopular leader. Not particularly telegenic, he was nicknamed "Worzel Gummidge" for his rumpled appearance. A centrist faction of the party broke away in 1981 to form the SDP. Foot led Labour into the 1983 general election, when the party obtained its lowest share of the vote since the 1918 general election and the fewest parliamentary seats it had had at any time since before 1945. He resigned the party leadership after the election, and was succeeded as leader by Neil Kinnock.

Books authored by Michael Foot include "Guilty Men" (1940); "The Pen and the Sword" (1957), a biography of Jonathan Swift; and a biography of Aneurin Bevan.

Foot was born in Lipson Terrace, Plymouth, Devon, the fifth of seven children of Isaac Foot (1880–1960) and Eva (née Mackintosh, died 17 May 1946), who was Scottish. 
Isaac Foot was a solicitor and founder of the Plymouth law firm Foot and Bowden (which amalgamated with another firm to become Foot Anstey). Isaac Foot was an active member of the Liberal Party and was the Liberal Member of Parliament for Bodmin in Cornwall from 1922–24 and again from 1929–35, and a Lord Mayor of Plymouth.

Michael Foot's siblings included: Sir Dingle Foot MP (1905–78), a Liberal and subsequently Labour MP; Hugh Foot, Baron Caradon (1907–90), Governor of Cyprus (1957–60) and representative of the United Kingdom at the United Nations from 1964–70; Liberal politician John Foot, later Baron Foot (1909–99); Margaret Elizabeth Foot (1911–65); Jennifer Mackintosh Highet (1916-2002); and Christopher Isaac Foot (1917–84). He was the uncle of campaigning journalist Paul Foot (1937–2004) and charity worker Oliver Foot (1946–2008).

Foot was educated at Plymouth College Preparatory School, Forres School in Swanage, and Leighton Park School in Reading. When he left Forres School, the headmaster sent a letter to his father in which he said "he has been the leading boy in the school in every way". He then went on to read Philosophy, Politics and Economics at Wadham College, Oxford. Foot was a president of the Oxford Union. He also took part in the ESU USA Tour (the debating tour of the United States run by the English-Speaking Union). Upon graduating with a second-class degree in 1934, he took a job as a shipping clerk in Birkenhead. Foot was profoundly influenced by the poverty and unemployment that he witnessed in Liverpool, which was on a different scale from anything he had seen in Plymouth. A Liberal up to this time, Foot was converted to socialism by Oxford University Labour Club president David Lewis, a Canadian Rhodes scholar, and others: "... I knew him [at Oxford] when I was a Liberal [and Lewis] played a part in converting me to socialism." Foot joined the Labour Party and first stood for parliament, aged 22, at the 1935 general election, where he contested Monmouth. During the election, Foot criticised the Prime Minister, Stanley Baldwin, for seeking rearmament. In his election address, Foot contended that "the armaments race in Europe must be stopped now". Foot also supported unilateral disarmament, after multilateral disarmament talks at Geneva had broken down in 1933.

Foot became a journalist, working briefly on the "New Statesman", before joining the left-wing weekly "Tribune" when it was set up in early 1937 to support the Unity Campaign, an attempt to secure an anti-fascist United Front between Labour and other left-wing parties. The campaign's members were Stafford Cripps's (Labour-affiliated) Socialist League, the Independent Labour Party and the Communist Party of Great Britain (CP). Foot resigned in 1938 after the paper's first editor, William Mellor, was sacked for refusing to adopt a new CP policy of backing a Popular Front, including non-socialist parties, against fascism and appeasement. In a 1955 interview, Foot ideologically identified as a libertarian socialist.

He was an avid anti-imperialist and was heavily involved in the India League. As an Oxford graduate, he was influenced by the founder of the India League, Krishna Menon. The India League was the premier UK-based organisation that fought for the 'Liberation of India'. After Indian's independence, Foot would remain close to India and eventually became Chair of the India League. 

On the recommendation of Aneurin Bevan, Foot was soon hired by Lord Beaverbrook to work as a writer on his "Evening Standard". (Bevan is supposed to have told Beaverbrook on the phone: "I've got a young bloody knight-errant here. They sacked his boss, so he resigned. Have a look at him.") At the outbreak of the Second World War, Foot volunteered for military service, but was rejected because of his chronic asthma.

In 1940, under the pen-name "Cato" he and two other Beaverbrook journalists (Frank Owen, editor of the "Standard", and Peter Howard of the "Daily Express") published "Guilty Men", which attacked the appeasement policy of the Chamberlain government; it became a runaway bestseller. (In so doing, Foot reversed his position of the 1935 election – when he had attacked the Conservatives as militaristic and demanded disarmament in the face of Nazi Germany.) Beaverbrook made Foot editor of the "Evening Standard" in 1942, when he was aged 28. During the war, Foot made a speech that was later featured in the documentary TV series "The World at War" broadcast in February 1974. Foot was speaking in defence of the "Daily Mirror", which had criticised the conduct of the war by the Churchill government. He mocked the notion that the Government would make no more territorial demands of other newspapers if they allowed the "Mirror" to be censored.

Foot left the "Standard" in 1945 to join the "Daily Herald" as a columnist. The "Daily Herald" was jointly owned by the TUC and Odhams Press, and was effectively an official Labour Party paper. He rejoined "Tribune" as editor from 1948 to 1952, and was again the paper's editor from 1955 to 1960. Throughout his political career he railed against the increasing corporate domination of the press.

Foot fought the Plymouth Devonport constituency in the 1945 general election. His election agent was Labour activist and lifelong friend Ron Lemin. He won the seat for Labour for the first time, holding it until his surprise defeat by Dame Joan Vickers at the 1955 general election. Until 1957, he was the most prominent ally of Aneurin Bevan, who had taken Cripps's place as leader of the Labour left, though Foot and Bevan fell out after Bevan renounced unilateral nuclear disarmament at the 1957 Labour Party conference.

Before the Cold War began in the late 1940s, Foot favoured a 'third way' foreign policy for Europe (he was joint author with Richard Crossman and Ian Mikardo of the pamphlet "Keep Left" in 1947), but in the wake of the communist seizure of power in Hungary and Czechoslovakia he and "Tribune" took a strongly anti-communist position, eventually embracing NATO.

Foot was however a critic of the West's handling of the Korean War, an opponent of West German rearmament in the early 1950s and a founder member of the Campaign for Nuclear Disarmament. Under his editorship, "Tribune" opposed both the British government's Suez campaign and the Soviet crushing of the Hungarian Revolution in 1956. In this period he made regular television appearances on the current affairs programmes "In The News" (BBC) and subsequently "Free Speech" (ITV). "There was certainly nothing wrong with his television technique in those days", reflected Anthony Howard shortly after Foot's death.

Foot returned to parliament at a by-election in Ebbw Vale, Monmouthshire, in 1960, the seat having been left vacant by Bevan's death. He had the Labour whip withdrawn in March 1961 after rebelling against the Labour leadership over air force estimates. He only returned to the Parliamentary Labour Group in 1963, when Harold Wilson became Labour leader after the sudden death of Hugh Gaitskell.

Harold Wilson — the subject of an enthusiastic campaign biography by Foot published by Robert Maxwell's Pergamon Press in 1964 – offered Foot a place in his first government, but Foot turned it down, instead becoming the leader of Labour's left opposition from the back benches. He opposed the government's moves to restrict immigration, join the European Communities (or "Common Market" as they were referred to) and reform the trade unions, was against the Vietnam War and Rhodesia's unilateral declaration of independence, and denounced the Soviet suppression of "socialism with a human face" in Czechoslovakia in 1968. He also famously allied with the Tory right-winger Enoch Powell to scupper the government's plan to abolish the voting rights of hereditary peers and create a House of Lords comprising only life peers – a "seraglio of eunuchs" as Foot put it.

Foot challenged James Callaghan for the post of Treasurer of the Labour Party in 1967, but failed.

After 1970, Labour moved to the left and Wilson came to an accommodation with Foot. Foot served in the Second Shadow Cabinet of Harold Wilson in various roles between 1970 and 1974. In April 1972, he stood for the Deputy Leadership of the party, along with Edward Short and Anthony Crosland. Short defeated Foot in the second ballot after Crosland had been eliminated in the first.

When, in 1974, Labour returned to office under Wilson, Foot became Secretary of State for Employment. According to Ben Pimlott, his appointment was intended to please the left of the party and the Trade Unions. In this role, he played the major part in the government's efforts to maintain the trade unions' support. He was also responsible for the Health and Safety at Work Act. Foot was one of the mainstays of the "no" campaign in the 1975 referendum on British membership of the European Communities. When Wilson retired in 1976, Foot contested the party leadership and led in the first ballot, but was ultimately defeated by James Callaghan. Later that year Foot was elected Deputy Leader, and served as Leader of the House of Commons, which gave him the unenviable task of trying to maintain the survival of the Callaghan government as its majority evaporated.

In 1975, Foot, along with Jennie Lee and others, courted controversy when they supported Indira Gandhi, the Prime Minister of India, after she prompted the declaration of a state of emergency. In December 1975, "The Times" ran an editorial titled 'Is Mr Foot a Fascist?' — their answer was that he was — after Norman Tebbit accused him of 'undiluted fascism' when Foot said that the Ferrybridge Six deserved dismissal for defying a closed shop.

During the Callaghan government Foot took a seat in Cabinet as Lord President of the Council and Leader of the House of Commons.

Following Labour's 1979 general election defeat by Margaret Thatcher, James Callaghan remained as party leader for the next 18 months before he resigned. Foot was elected Labour leader on 10 November 1980, beating Denis Healey in the second round of the leadership election (the last leadership contest to involve only Labour MPs). Foot presented himself as a compromise candidate, capable – unlike Healey – of uniting the party, which at the time was riven by the grassroots left-wing insurgency centred around Tony Benn.

The Bennites were demanding revenge for what they considered to be the betrayals of the Callaghan government. They called for MPs who had acquiesced in Callaghan's policies to be replaced by left-wingers who would support unilateral nuclear disarmament, withdrawal from the European Communities, and widespread nationalisation. Benn did not stand for the leadership; apart from Foot and Healey, the other candidates (both eliminated in the first round) were John Silkin, a Tribunite like Foot, and Peter Shore, a Eurosceptic.

When he became leader, Foot was already 67 years old; and frail. After the 1979 energy crisis, Britain went into recession in 1980, which was blamed on the Conservative government's controversial monetarist policy against inflation, which had the effect of increasing unemployment. As a result, Labour had moved ahead of the Conservatives in the opinion polls. After Foot's election as leader, opinion polls showed a double-digit lead for Labour, boosting his hopes of becoming Prime Minister at the next general election, which had to be held by May 1984.

When Foot became leader, the Conservative politician Kenneth Baker commented: "Labour was led by Dixon of Dock Green under Jim Callaghan. Now it is led by Worzel Gummidge." Foot's nickname in the press gradually became "Worzel Gummidge", or "Worzel". This became particularly common after Remembrance Day 1981, when he attended the Cenotaph observance wearing a coat that some said resembled a donkey jacket. After his tenure as leader, Foot would be "depicted as a scarecrow on ITV’s satirical puppet show "Spitting Image"."

Almost immediately after his election as leader, he was faced with a serious crisis. On 25 January 1981, four senior politicians on the right-wing of the Labour Party (Roy Jenkins, Shirley Williams, David Owen and William Rodgers, the so-called "Gang of Four") left Labour and formed the SDP, which was launched on 26 March 1981. This was largely seen as the consequence of the Labour Party's swing to the left, polarising divisions in an already divided party.

The SDP won the support of large sections of the media. For most of 1981 and early-1982 its opinion poll ratings suggested that it could at least overtake Labour and possibly win a general election. The Conservatives were then unpopular because of the economic policies of Margaret Thatcher, which had seen unemployment reach a postwar high.

The Labour left was still strong. In 1981, Benn decided to challenge Healey for the Deputy Leadership of the Labour Party, a contest Healey won, albeit narrowly. Foot struggled to make an impact, and was widely criticised for his ineffectiveness, though his performances in the Commons — most notably on the Falklands War of 1982 – won him widespread respect from other parliamentarians. He was criticised by some on the left for supporting Thatcher's immediate resort to military action. The right-wing newspapers nevertheless lambasted him consistently for what they saw as his bohemian eccentricity, attacking him for wearing what they described as a "donkey jacket" (actually he wore a type of duffel coat) at the wreath-laying ceremony at the Cenotaph on Remembrance Day in November 1981, for which he was likened to an "out-of-work navvy" by a fellow Labour MP. Foot did not make it generally known that the Queen Mother had described it as a "sensible coat for a day like this", which could be considered a slight or a compliment depending on whether irony was intended. He later donated the coat to the People's History Museum in Manchester, which holds a collection that spans Foot's entire political career from 1938 to 1990, and his personal papers dating back to 1926.

The formation of the SDP – which formed an alliance with the Liberal Party in June 1981 – contributed to a fall in Labour support. The double-digit lead which had still been intact in opinion polls at the start of 1981 was swiftly wiped out, and by the end of October the opinion polls were showing the Alliance ahead of Labour. Labour briefly regained their lead of most opinion polls in early 1982, but when the Falklands conflict ended on 14 June 1982 with a British victory over Argentina, opinion polls showed the Conservatives firmly in the lead. Their position at the top of the polls was strengthened by the return to economic growth later in the year. It was looking certain that the Conservatives would be re-elected, and the only key issue that the media were still speculating by the end of 1982 was whether it would be Labour or the Alliance who formed the next opposition.

Through late 1982 and early 1983, there was constant speculation that Labour MPs would replace Foot with Healey as leader. Such speculation increased after Labour lost the 1983 Bermondsey by-election, in which Peter Tatchell was Labour candidate, standing against a Conservative, a Liberal (eventual winner Simon Hughes) and John O'Grady, who had declared himself the Real Bermondsey Labour candidate. Critically, Labour held on in a subsequent by-election in Darlington, and Foot remained leader for the 1983 general election.

The 1983 Labour manifesto, strongly socialist in tone, advocated unilateral nuclear disarmament, higher personal taxation and a return to a more interventionist industrial policy. The manifesto also pledged that a Labour government would abolish the House of Lords, nationalise banks and immediately withdraw from the then-European Economic Community. Gerald Kaufman, once Harold Wilson's press officer and during the 1980s a prominent figure on the Labour right-wing, described the 1983 Labour manifesto as "the longest suicide note in history."

As a statement on internal democracy, Foot passed the edict that the manifesto would consist of all resolutions arrived at conference. The party also failed to master the medium of television, while Foot addressed public meetings around the country, and made some radio broadcasts, in the same manner as Clement Attlee did in 1945. Members joked that they had not expected Foot to allow the slogan "Think positive, Act positive, Vote Labour" on grammatical grounds.

The "Daily Mirror" was the only major newspaper to back Foot and the Labour Party at the 1983 general election, urging its readers to vote Labour and "Stop the waste of our nation, for your job your children and your future" in response to the mass unemployment which followed Conservative Prime Minister Margaret Thatcher's monetarist economic policies to reduce inflation. Most other newspapers urged their readers to vote Conservative.

The Labour Party led by Foot, lost to the Conservatives in a landslide – a result which had been widely predicted by the opinion polls since the previous summer. The only consolation for Foot and Labour was that they did not lose their place in opposition to the SDP-Liberal Alliance, who came close to them in terms of votes but were still a long way behind in terms of seats. Despite this, Foot was very critical of the Alliance, accusing them of "siphoning" Labour support and enabling the Tories to win more seats.

Foot resigned days after the bitter election defeat, and was succeeded as leader on 2 October by Neil Kinnock; who had been tipped from the outset to be Labour's choice of new leader.

Foot took a back seat in Labour politics after 1983 and retired from the House of Commons at the 1992 general election, when Labour lost to the Conservative Party (led by John Major) for the fourth election in succession, but remained politically active. From 1987 to 1992, he was the oldest sitting British MP (preceding former Prime Minister Sir Edward Heath). He defended Salman Rushdie, after Ayatollah Khomeini advocated killing the novelist in a fatwā, and took a strongly pro-interventionist position against Serbia during its conflict with Croatia and Bosnia, supporting NATO forces whilst citing defence of civilian populations in the latter countries. In addition, he was among the Patrons of the British-Croatian Society. "The Guardian"s political editor Michael White criticised Foot's "overgenerous" support for Croatian leader Franjo Tuđman.

Foot remained a high-profile member of the Campaign for Nuclear Disarmament (CND). He wrote several books, including highly regarded biographies of Aneurin Bevan and H. G. Wells. Indeed, he was a distinguished Vice-president of the H. G. Wells Society. Many of his friends have said publicly that they regret that he ever gave up literature for politics.

Michael Foot became a supporter of pro-Europeanism in the 1990s.

Foot was an Honorary Associate of the National Secular Society and a Distinguished Supporter of the British Humanist Association. In 1988, he was elected a Fellow of the Royal Society of Literature. 

In a poll of Labour Party activists he was voted the worst post-war Labour Party leader. Though Foot is considered by many right-wingers to be a failure as Labour leader, his biographer Mervyn Jones strongly makes the case that no one else could have held Labour together at the time, particularly in the face of the controversy over the infiltration of the party by Militant. Foot is remembered with affection in Westminster as a great parliamentarian. He was widely liked, and admired for his integrity, habitual courtesy, and generosity of spirit, by both his colleagues and opponents.

A portrait of Foot by the artist Robert Lenkiewicz now permanently hangs in Portcullis House, Westminster.

Oleg Gordievsky, a high-ranking KGB officer who defected from the Soviet Union to the UK in 1985, made allegations against Foot in his 1995 memoirs. Essentially, the allegations claimed that, up until 1968, Foot had spoken to KGB agents "dozens of times", passing information about politics and the trade unions, and Foot had been paid a total of around £150 for his information (said to be worth £37,000 in 2018). "The Sunday Times", which serialised Gordievsky's book under the headline "KGB: Michael Foot was our agent", claimed in an article of 19 February that the Soviet intelligence services regarded Foot as an "agent of influence" (and a "useful idiot"), codenamed "Agent BOOT", and in the pay of the KGB for many years. Crucially, the newspaper used material from the original manuscript of the book which had not been included in the published version.

At the time a leading article in "The Independent" newspaper asserted: "It seems extraordinary that such an unreliable figure should now be allowed, given the lack of supporting evidence, to damage the reputation of figures such as Mr Foot." In a February 1992 interview, Gordievsky declared that he had no further revelations to make about the Labour Party. Foot successfully sued the "Sunday Times", winning "substantial" damages.

However, in the "Daily Telegraph" in 2010, Charles Moore gave a "full account", which he said had been provided to him by Gordievsky shortly after Foot's death, of the extent of Foot's alleged KGB involvement. The account provides additional information concerning the allegations, but no new evidence. The evidence against Foot consists solely of Gordievsky's testimony. Moore wrote that, although the claims are difficult to corroborate without MI6 and KGB files, Gordievsky's past record in revealing KGB contacts in Britain had been shown to be reliable. However Moore did not think that Foot would have known that he was considered an agent, and he probably considered that he was simply keeping the Soviet Union well informed in the interests of peace. There is no evidence Foot gave away secrets.

Foot was a passionate supporter of Plymouth Argyle Football Club from his childhood and once remarked that he wasn't going to die until he had seen them play in the Premier League.
He served for several years as a director of the club, seeing two promotions under his tenure.

For his 90th birthday, Foot was registered with the Football League as an honorary player and given the shirt number 90. This made him the oldest registered professional player in the history of football.

Foot was married to the film-maker, author and feminist historian Jill Craigie (1911–99) from 1949 until her death fifty years later. He had no children.

In February 2007, it was revealed that Foot had an extramarital affair with a woman around 35 years his junior in the early-1970s. The affair, which lasted nearly a year, put a considerable strain on his marriage. The affair is detailed in Foot's official biography, published in March 2007.

On 23 July 2006, his 93rd birthday, Michael Foot became the longest-lived leader of a major British political party, passing Lord Callaghan's record of 92 years, 364 days.

A staunch republican (though well liked by the Royal Family on a personal level), Foot rejected honours from the Queen and the government, including a knighthood and a peerage, on more than one occasion.

He was also an atheist. , he was one of three leaders of the Labour Party to declare that they disbelieved.

Foot suffered from asthma until 1963 (which disqualified him from service in the Second World War) and eczema until middle age.

In October 1963, he was involved in a car crash, suffering pierced lungs, broken ribs, and a broken left leg. Foot used a walking stick for the rest of his life. According to former MP Tam Dalyell, Foot had up until the accident, been a chain-smoker; but gave up the habit thereafter. Jill Craigie also suffered from a crushed hand in this car crash.

In October 1976, Foot became blind in one eye following an attack of shingles. He was absent from the House of Commons for three weeks whilst he recovered.

Foot died at his Hampstead, north London home in the morning of 3 March 2010 at the age of 96. The House of Commons was informed of the news later that day by Justice Secretary Jack Straw, who told the House: "I am sure that this news will be received with great sadness not only in my own party but across the country as a whole." Foot's funeral was a non-religious service, held on 15 March 2010 at Golders Green Crematorium in North-West London.

A memorial to Foot in Plymouth was vandalised with Nazi symbols in the wake of the 2016 United Kingdom European Union membership referendum in July 2016.

Foot's involvement in the nuclear disarmament movement gave rise to the story that "The Times" ran the headline "Foot Heads Arms Body" over an article about his leadership of a nuclear-disarmament committee. Some decades later, Martyn Cornell recalled the story as true, saying he had written the headline himself as a "Times" subeditor around 1986. The headline does not, however, appear in The Times Digital Archive, which includes every day's newspaper from 1785 into the 21st century. It is found in a letter published in "The Guardian" in 1978.

Foot was portrayed by Patrick Godfrey in the 2002 BBC production of Ian Curteis's long unproduced "The Falklands Play" and by Michael Pennington in the film "The Iron Lady".

Foot was likened to the scarecrow Worzel Gummidge in "Dear Bill", a long-running series of fictional letters which appeared in the British satirical magazine "Private Eye", purportedly written by Denis Thatcher, husband of Margaret Thatcher, to his friend and golfing partner Bill Deedes, former editor of the "Daily Telegraph".





</doc>
<doc id="19828" url="https://en.wikipedia.org/wiki?curid=19828" title="Max and Moritz">
Max and Moritz

Max and Moritz: A Story of Seven Boyish Pranks (original: Max und Moritz – Eine Bubengeschichte in sieben Streichen) is a German language illustrated story in verse. This highly inventive, blackly humorous tale, told entirely in rhymed couplets, was written and illustrated by Wilhelm Busch and published in 1865. It is among the early works of Busch, yet it already featured many substantial, effectually aesthetic and formal regularities, procedures and basic patterns of Busch's later works. Many familiar with comic strip history consider it to have been the direct inspiration for the "Katzenjammer Kids" and "Quick & Flupke". The German title satirizes the German custom of giving a subtitle to the name of dramas in the form of "Ein Drama in ... Akten" ("A Drama in ... Acts"), which became dictum in colloquial usage for any event with an unpleasant or dramatic course, e.g. "Bundespräsidentenwahl - Drama in drei Akten" ("Federal Presidential Elections - Drama in Three Acts").

Busch's classic tale of the terrible duo (now in the public domain) has since become a proud part of the culture in German-speaking countries. Even today, parents usually read these tales to their not-yet-literate children. To this day in Germany, Austria, and Switzerland, a certain familiarity with the story and its rhymes is still presumed, as it is often referenced in mass communication. The two leering faces are synonymous with mischief, and appear almost logo-like in advertising and even graffiti.

During World War 1, the Red Baron, Manfred von Richthofen, named his dog Moritz, giving the name Max to another animal given to his friend.

"Max and Moritz" is the first published original foreign children's book in Japan which was translated into rōmaji by Shinjirō Shibutani and Kaname Oyaizu in 1887 as "" ("Naughty stories").

Max and Moritz became the forerunners to the comic strip. The story inspired Rudolph Dirks to create The Katzenjammer Kids.

After World War 2, German-U.S. composer Richard Mohaupt created together with choreographer Alfredo Bortoluzzi the dance burlesque ("Tanzburleske") "Max und Moritz", which premiered at Badisches Staatstheater Karlsruhe on December 18, 1949.

There have been several English translations of the original German verses over the years, but all have maintained the original trochaic tetrameter:

Ah, how oft we read or hear of <br>
Boys we almost stand in fear of!<br>
For example, take these stories<br>
Of two youths, named Max and Moritz,<br>
Who, instead of early turning<br>
Their young minds to useful learning,<br>
Often leered with horrid features<br>
At their lessons and their teachers.

Look now at the empty head: he<br>
Is for mischief always ready.<br>
Teasing creatures - climbing fences,<br>
Stealing apples, pears, and quinces,<br>
Is, of course, a deal more pleasant,<br>
And far easier for the present,<br>
Than to sit in schools or churches,<br>
Fixed like roosters on their perches

But O dear, O dear, O deary,<br>
When the end comes sad and dreary!<br>
'Tis a dreadful thing to tell<br>
That on Max and Moritz fell!<br>
All they did this book rehearses,<br>
Both in pictures and in verses.

The boys tie several crusts of bread together with thread, and lay this trap in the chicken yard of Bolte, an old widow, causing all the chickens to become fatally entangled.

This prank is remarkably similar to the eighth history of the classic German prankster tales of Till Eulenspiegel.

As the widow cooks her chickens, the boys sneak onto her roof. When she leaves her kitchen momentarily, the boys steal the chickens using a fishing pole down the chimney. The widow hears her dog barking and hurries upstairs, finds the hearth empty and beats the dog.

The boys torment Böck, a well-liked tailor who has a fast stream flowing in front of his house. They saw through the planks of his wooden bridge, making a precarious gap, then taunt him by making goat noises (a pun on his name being similar to the zoological expression 'buck'), until he runs outside. The bridge breaks; the tailor is swept away and nearly drowns (but for two geese, which he grabs a hold of and which fly high to safety).

Although Till removes the planks of the bridge instead of sawing them there are some similarities to Till Eulenspiegel (32nd History).

While their devout teacher, Lämpel, is busy at church, the boys invade his home and fill his favorite pipe with gunpowder. When he lights the pipe, the blast knocks him unconscious, blackens his skin and burns away all his hair. But: "Time that comes will quick repair; yet the pipe retains its share."

The boys collect bags full of May bugs, which they promptly deposit in their Uncle Fritz's bed. Uncle is nearly asleep when he feels the bugs walking on his nose. Horrified, he goes into a frenzy, killing them all before going back to sleep.

The boys invade a closed bakery to steal some Easter sweets. Attempting to steal pretzels, they fall into a vat of dough. The baker returns, catches the breaded pair, and bakes them. But they survive, and escape by gnawing through their crusts.

Hiding out in the grain storage area of a farmer, Mecke, the boys slit some grain sacks. Carrying away one of the sacks, farmer Mecke immediately notices the problem. He puts the boys in the sack instead, then takes it to the mill. The boys are ground to bits and devoured by the miller's ducks. Later, no one expresses regret.

"Max und Moritz" was adapted into a ballet by Richard Mohaupt and Alfredo Bortuluzzi. In 1956 Norbert Schultze adapted it into a straightforward children's film, "Max und Moritz" (1956).




 in German for a single work


</doc>
<doc id="19829" url="https://en.wikipedia.org/wiki?curid=19829" title="May Day">
May Day

May Day is a public holiday usually celebrated on 1 May or the first Monday of May. It is an ancient festival of spring and a current traditional spring holiday in many European cultures. Dances, singing, and cake are usually part of the festivities.

In 1889, May Day was chosen as the date for International Workers' Day by the Socialists and Communists of the Second International to commemorate the Haymarket affair in Chicago. International Workers' Day is also called "May Day", but it is a different celebration from the traditional May Day.

The earliest known May celebrations appeared with the "Floralia", festival of Flora, the Roman goddess of flowers, held from 27 April – 3 May during the Roman Republic era, and the "Maiouma" or "Maiuma", a festival celebrating Dionysus and Aphrodite held every three years during the month of May. The Floralia opened with theatrical performances. In the Floralia, Ovid says that hares and goats were released as part of the festivities. Persius writes that crowds were pelted with vetches, beans, and lupins. A ritual called the "Florifertum" was performed on either 27 April or 3 May, during which a bundle of wheat ears was carried into a shrine, though it is not clear if this devotion was made to Flora or Ceres. Floralia concluded with competitive events and spectacles, and a sacrifice to Flora.

Maiouma was celebrated at least as early as the 2nd century AD, when records show expenses for the month-long festival were appropriated by Emperor Commodus. According to the 6th-century chronicles of John Malalas, the Maiouma was a "nocturnal dramatic festival, held every three years and known as Orgies, that is, the Mysteries of Dionysus and Aphrodite" and that it was "known as the Maioumas because it is celebrated in the month of May-Artemisios". During this time, enough money was set aside by the government for torches, lights, and other expenses to cover a thirty-day festival of "all-night revels." The Maiouma was celebrated with splendorous banquets and offerings. Its reputation for licentiousness caused it to be suppressed during the reign of Emperor Constantine, though a less debauched version of it was briefly restored during the reigns of Arcadius and Honorius, only to be suppressed again during the same period.

A later May festival celebrated in Germanic countries, Walpurgis Night, commemorates the official canonization of Saint Walpurga on 1 May 870. In Gaelic culture, the evening of April 30th was the celebration of Beltane (which translates to "lucky fire"), the start of the summer season. First attested in 900 AD, the celebration mainly focused on the symbolic use of fire to bless cattle and other livestock as they were moved to summer pastures. This custom continued into the early 19th century, during which time cattle would be made to jump over fires to protect their milk from being stolen by fairies. People would also leap over the fires for luck.

Since the 18th century, many Roman Catholics have observed May – and May Day – with various May devotions to the Blessed Virgin Mary. In works of art, school skits, and so forth, Mary's head will often be adorned with flowers in a May crowning. 1 May is also one of two feast days of the Catholic patron saint of workers St Joseph the Worker, a carpenter, husband to Mother Mary, and surrogate father of Jesus. Replacing another feast to St. Joseph, this date was chosen by Pope Pius XII in 1955 as a counterpoint to the communist International Workers Day celebrations on May Day.

The best known modern May Day traditions, observed both in Europe and North America, include dancing around the maypole and crowning the Queen of May. Fading in popularity since the late 20th century is the tradition of giving of "May baskets," small baskets of sweets or flowers, usually left anonymously on neighbours' doorsteps.

In the late 20th century, many neopagans began reconstructing some of the older pagan festivals and combining them with more recently developed European secular and Catholic traditions, and celebrating May Day as a pagan religious festival.

On May Day, Bulgarians celebrate Irminden (or Yeremiya, Eremiya, Irima, Zamski den). The holiday is associated with snakes and lizards and rituals are made in order to protect people from them. The name of the holiday comes from the prophet Jeremiah, but its origins are most probably pagan.

It is said that on the days of the Holy Forty or Annunciation snakes come out of their burrows, and on Irminden their king comes out. Old people believe that those working in the fields on this day will be bitten by a snake in summer.

In western Bulgaria people light fires, jump over them and make noises to scare snakes. Another custom is to prepare "podnici" (special clay pots made for baking bread).

This day is especially observed by pregnant women so that their offspring do not catch "yeremiya"—an illness due to evil powers.

In Czech Republic, May Day is traditionally considered a holiday of love and May as a month of love. The celebrations of spring are held on April 30 when a maypole ("májka" in Czech) is erected—a tradition possibly connected to Beltane, since bonfires are also lit on the same day. The event is similar to German Walpurgisnacht, its public holiday on April 30.
On May 31, the maypole is taken down in an event called Maypole Felling.

On May 1st, couples in love kiss under a blooming tree. According to the ethnographer Klára Posekaná, this is not an old habit. It most likely originated around the beginning of the 20th century in an urban environment, perhaps in connection with Karel Hynek Mácha's poem Máj (which is often recited during these days) and Petřín. This is usually done under a cherry, an apple or a birch tree.

May Day or "Spring Day" ("Kevadpüha") is a national holiday in Estonia celebrating the arrival of spring.

More traditional festivities take place throughout the night before and into the early hours of 1 May, on the Walpurgis Night ("Volbriöö").

In Finland, Walpurgis night (') ("") is one of the four biggest holidays along with Christmas Eve, New Year's Eve, and Midsummer ('). Walpurgis witnesses the biggest carnival-style festival held in Finland's cities and towns. The celebrations, which begin on the evening of 30 April and continue on 1 May, typically centre on the consumption of sima, sparkling wine and other alcoholic beverages. Student traditions, particularly those of engineering students, are one of the main characteristics of '. Since the end of the 19th century, this traditional upper-class feast has been appropriated by university students. Many ' (university-preparatory high school) alumni wear the black and white student cap and many higher education students wear student coveralls. One tradition is to drink sima, a home-made low-alcohol mead, along with freshly cooked funnel cakes.

On 1 May 1561, King Charles IX of France received a lily of the valley as a lucky charm. He decided to offer a lily of the valley each year to the ladies of the court. At the beginning of the 20th century, it became custom to give a sprig of lily of the valley, a symbol of springtime, on 1 May. The government permits individuals and workers' organisations to sell them tax-free on that single day. Nowadays, people may present loved ones either with bunches of lily of the valley or dog rose flowers.

In rural regions of Germany, especially the Harz Mountains, Walpurgisnacht celebrations of pagan origin are traditionally held on the night before May Day, including bonfires and the wrapping of a "Maibaum" (maypole). Young people use this opportunity to party, while the day itself is used by many families to get some fresh air. Motto: "Tanz in den Mai" (""Dance into May"").

In the Rhineland, 1 May is also celebrated by the delivery of a maypole, a tree covered in streamers to the house of a girl the night before. The tree is typically from a love interest, though a tree wrapped only in white streamers is a sign of dislike. Women usually place roses or rice in the form of a heart at the house of their beloved one. It is common to stick the heart to a window or place it in front of the doormat. In leap years, it is the responsibility of the women to place the maypole. All the action is usually done secretly and it is an individual's choice whether to give a hint of their identity or stay anonymous.

May Day was not established as a public holiday until the Third Reich declared 1 May a “national workers’ day” in 1933. As Labour Day, many political parties and unions host activities related to work and employment.

1 May is a day that celebrates Spring.

Maios (Latin Maius), the month of May, took its name from the goddess Maia (Gr ), a Greek and Roman goddess of fertility. The day of Maios (Modern Greek Πρωτομαγιά) celebrates the final victory of the summer against winter as the victory of life against death. The celebration is similar to an ancient ritual associated with another minor demi-god Adonis which also celebrated the revival of nature. There is today some conflation with yet another tradition, the revival or marriage of Dionysus (the Greek God of theatre and wine-making). This event, however, was celebrated in ancient times not in May but in association with the Anthesteria, a festival held in February and dedicated to the goddess of agriculture Demeter and her daughter Persephone. Persephone emerged every year at the end of Winter from the Underworld. The Anthesteria was a festival of souls, plants and flowers, and Persephone's coming to earth from Hades marked the rebirth of nature, a common theme in all these traditions.

What remains of the customs today, echoes these traditions of antiquity. A common, until recently, May Day custom involved the annual revival of a youth called Adonis, or alternatively of Dionysus, or of Maios (in Modern Greek Μαγιόπουλο, the Son of Maia). In a simple theatrical ritual, the significance of which has long been forgotten, a chorus of young girls sang a song over a youth lying on the ground, representing Adonis, Dionysus or Maios. At the end of the song, the youth rose up and a flower wreath was placed on his head.

The most common aspect of modern May Day celebrations is the preparation of a flower wreath from wild flowers, although as a result of urbanisation there is an increasing trend to buy wreaths from flower shops. The flowers are placed on the wreath against a background of green leaves and the wreath is hung either on the entrance to the family house/apartment or on a balcony. It remains there until midsummer night. On that night, the flower wreaths are set alight in bonfires known as St John's fires. Youths leap over the flames consuming the flower wreaths. This custom has also practically disappeared, like the theatrical revival of Adonis/Dionysus/Maios, as a result of rising urban traffic and with no alternative public grounds in most Greek city neighbourhoods, not to mention potential conflicts with demonstrating workers.

May Day has been celebrated in Ireland since pagan times as the feast of Beltane (Bealtaine) and in latter times as Mary's day. Traditionally, bonfires were lit to mark the coming of summer and to grant luck to people and livestock. Officially Irish May Day holiday is the first Monday in May. Old traditions such as bonfires are no longer widely observed, though the practice still persists in some places across the country. Limerick, Clare and many other people in other counties still keep on this tradition, including areas in Dublin city such as Ringsend.

In Italy it is called "Calendimaggio" or "cantar maggio" a seasonal feast held to celebrate the arrival of spring. The event takes its name from the period in which it takes place, that is, the beginning of May, from the Latin "calenda maia". The Calendimaggio is a tradition still alive today in many regions of Italy as an allegory of the return to life and rebirth: among these Piedmont, Liguria, Lombardy, Emilia-Romagna (for example, is celebrated in the area of the "Quattro Province" or Piacenza, Pavia, Alessandria and Genoa), Tuscany and Umbria. This magical-propitiatory ritual is often performed during an almsgiving in which, in exchange for gifts (traditionally eggs, wine, food or sweets), the Maggi (or maggerini) sing auspicious verses to the inhabitants of the houses they visit. Throughout the Italian peninsula these "Il Maggio" couplets are very diverse—most are love songs with a strong romantic theme, that young people sang to celebrate the arrival of spring. Symbols of spring revival are the trees (alder, golden rain) and flowers (violets, roses), mentioned in the verses of the songs, and with which the maggerini adorn themselves. In particular the plant alder, which grows along the rivers, is considered the symbol of life and that's why it is often present in the ritual.

Calendimaggio can be historically noted in Tuscany as a mythical character who had a predominant role and met many of the attributes of the god Belenus. In Lucania, the "Maggi" have a clear auspicious character of pagan origin. In Syracuse, Sicily, the "Albero della Cuccagna" (cf. "Greasy pole") is held during the month of May, a feast celebrated to commemorate the victory over the Athenians led by Nicias. However, Angelo de Gubernatis, in his work "Mythology of Plants", believes that without doubt the festival was previous to that of said victory.

It is a celebration that dates back to ancient peoples, and is very integrated with the rhythms of nature, such as the Celts (celebrating Beltane), Etruscans and Ligures, in which the arrival of summer was of great importance.

In Poland, there is a state holiday on 1 May. It is currently celebrated without a specific connotation, and as such it is May Day. However, due to historical connotations, most of the celebrations are focused around Labour Day festivities. It is customary for labour activists and left-wing political parties to organize parades in cities and towns across Poland on this day. The holiday is also commonly referred to as "Labour Day" ("Święto Pracy").

In Poland, May Day is closely followed by May 3rd Constitution Day. These two dates combined often result in a long weekend called "Majówka". People often travel, and "Majówka" is unofficially considered the start of the barbecuing season in Poland. Between these two, on 2 May, though formerly a working day, there is now a patriotic holiday, the Day of the Polish Flag (), introduced by a Parliamentary Act of February 20, 2004. May Day has a public holiday, too.

"Maias" is a superstition throughout Portugal, with special focus on the northern territories and rarely elsewhere. Maias is the dominant naming in Northern Portugal, but it may be referred to by other names, including Dia das Bruxas (Witches' day), O Burro (the Donkey, referring to an evil spirit) or the last of April, as the local traditions preserved to this day occur on that evening only. People put the yellow flowers of Portuguese brooms, the bushes are known as giestas. The flowers of the bush are known as Maias, which are placed on doors or gates and every doorway of houses, windows, granaries, currently also cars, which the populace collect on the evening of 30 April when the Portuguese brooms are blooming, to defend those places from bad spirits, witches and the evil eye. The placement of the May flower or bush in the doorway must be done before midnight.

These festivities are a continuum of the "Os Maios" of Galiza. In ancient times, this was done while playing traditional night-music. In some places, children were dressed in these flowers and went from place to place begging for money or bread. On May 1, people also used to sing "Cantigas de Maio", traditional songs related to this day and the whole month of May.

The origin of this tradition can be traced to the Catholic Church story of Mary and Joseph fleeing to Egypt to protect Jesus from Herod. It was said that brooms could be found at the door of the house holding Jesus, but Herod soldiers arrived to the place, they found every door decorated with brooms.

On May Day, the Romanians celebrate the "arminden" (or "armindeni"), the beginning of summer, symbolically tied with the protection of crops and farm animals. The name comes from Slavonic "Jeremiinŭ dĭnĭ", meaning prophet Jeremiah's day, but the celebration rites and habits of this day are apotropaic and pagan (possibly originating in the cult of the god Pan).

The day is also called "ziua pelinului" ("mugwort day") or "ziua bețivilor" ("drunkards' day") and it is celebrated to ensure good wine in autumn and, for people and farm animals alike, good health and protection from the elements of nature (storms, hail, illness, pests). People would have parties in natural surroundings, with "lăutari" (fiddlers) for those who could afford it. Then it is customary to roast and eat lamb, along with new mutton cheese, and to drink mugwort-flavoured wine, or just red wine, to refresh the blood and get protection from diseases. On the way back, the men wear lilac or mugwort flowers on their hats.

Other apotropaic rites include, in some areas of the country, people washing their faces with the morning dew (for good health) and adorning the gates for good luck and abundance with green branches or with birch saplings (for the houses with maiden girls). The entries to the animals' shelters are also adorned with green branches. All branches are left in place until the wheat harvest when they are used in the fire which will bake the first bread from the new wheat.

On May Day eve, country women do not work in the field as well as in the house to avoid devastating storms and hail coming down on the village.

"Arminden" is also "ziua boilor" (oxen day) and thus the animals are not to be used for work, or else they could die or their owners could get ill.

It is said that the weather is always good on May Day to allow people to celebrate.

"Prvomajski uranak" (Reveille on May 1st) is a folk tradition and feast that consists of the fact that on 1 May, people go in the nature or even leave the day before and spend the night with a camp fire. Most of the time, a dish is cooked in a kettle or in a barbecue. Among Serbs this holiday is widespread. Almost every town in Serbia has its own traditional first-of-may excursion sites, and most often these are green areas outside the city.

May Day is celebrated throughout the country as "Los Mayos" (lit. "the Mays") often in a similar way to "Fiesta de las Cruces" in many parts of Hispanic America. By way of example, in Galicia, the festival ("os maios", in the local language) consists of different representations around a decorated tree or sculpture. People sing popular songs (also called "maios",) making mentions to social and political events during the past year, sometimes under the form of a converse, while they walk around the sculpture with the percussion of two sticks. In Lugo and in the village of Vilagarcía de Arousa it was usual to ask a tip to the attendees, which used to be a handful of dry chestnuts ("castañas maiolas"), walnuts or hazelnuts. Today the tradition became a competition where the best sculptures and songs receive a prize.

In the Galician city of Ourense this day is celebrated traditionally on 3 May, the day of the Holy Cross, that in the Christian tradition replaced the tree "where the health, life and resurrection are," according to the introit of that day's mass.

The more traditional festivities have moved to the day before, Walpurgis Night ("Valborgsmässoafton"), known in some locales as simply "Last of April" and often celebrated with bonfires and a good bit of drinking. The first of May is instead celebrated as International Workers' Day.

Traditional English May Day rites and celebrations include crowning a May Queen and celebrations involving a maypole, around which dancers often circle with ribbons. Historically, Morris dancing has been linked to May Day celebrations. The earliest records of maypole celebrations date to the 14th century, and by the 15th century the maypole tradition was well established in southern Britain.

The spring bank holiday on the first Monday in May was created in 1978; May Day itself1 Mayis, not a public holiday in England (unless it falls on a Monday). In February 2011, the UK Parliament was reported to be considering scrapping the bank holiday associated with May Day, replacing it with a bank holiday in October, possibly coinciding with Trafalgar Day (celebrated on October 21), to create a "United Kingdom Day". Similarly, attempts were made by the John Major government in 1993 to abolish the May Day holiday and replace it with Trafalgar Day.

Unlike the other Bank Holidays and common law holidays, the first Monday in May is taken off from (state) schools by itself, and not as part of a half-term or end of term holiday. This is because it has no Christian significance and does not otherwise fit into the usual school holiday pattern. (By contrast, the Easter Holiday can start as late—relative to Easter—as Good Friday, if Easter falls early in the year; or finish as early—relative to Easter—as Easter Monday, if Easter falls late in the year, because of the supreme significance of Good Friday and Easter Day to Christianity.)

May Day was abolished and its celebration banned by Puritan parliaments during the Interregnum, but reinstated with the restoration of Charles II in 1660. 1 May 1707, was the day the Act of Union came into effect, joining England and Scotland to form the Kingdom of Great Britain.
In Oxford, it is a centuries-old tradition for May Morning revellers to gather below the Great Tower of Magdalen College at 6am to listen to the college choir sing traditional madrigals as a conclusion to the previous night's celebrations. Since the 1980s some people then jump off Magdalen Bridge into the River Cherwell. For some years, the bridge has been closed on 1 May to prevent people from jumping, as the water under the bridge is only deep and jumping from the bridge has resulted in serious injury in the past. There are still people who climb the barriers and leap into the water, causing themselves injury.

In Durham, students of the University of Durham gather on Prebend's Bridge to see the sunrise and enjoy festivities, folk music, dancing, madrigal singing and a barbecue breakfast. This is an emerging Durham tradition, with patchy observance since 2001.

Kingsbury Episcopi, Somerset, has seen its yearly May Day Festival celebrations on the May bank holiday Monday burgeon in popularity in the recent years. Since it was reinstated 21 years ago it has grown in size, and on 5 May 2014 thousands of revellers were attracted from all over the south-west to enjoy the festivities, with BBC Somerset covering the celebrations. These include traditional maypole dancing and morris dancing, as well as contemporary music acts.

Whitstable, Kent, hosts a good example of more traditional May Day festivities, where the Jack in the Green festival was revived in 1976 and continues to lead an annual procession of morris dancers through the town on the May bank holiday. A separate revival occurred in Hastings in 1983 and has become a major event in the town calendar. A traditional sweeps festival is performed over the May bank holiday in Rochester, Kent, where the Jack in the Green is woken at dawn on 1 May by Morris dancers.

At 7:15 p.m. on 1 May each year, the Kettle Bridge Clogs morris dancing side dance across Barming Bridge (otherwise known as the Kettle Bridge), which spans the River Medway near Maidstone, to mark the official start of their morris dancing season.

Also known as Astoria Day in northern parts of rural Cumbria. A celebration of unity and female bonding. Although not very well known, it is often caused by a huge celebration.

The Maydayrun involves thousands of motorbikes taking a trip from Greater London (Locksbottom) to the Hastings seafront, East Sussex. The event has been taking place for almost 30 years now and has grown in interest from around the country, both commercially and publicly. The event is not officially organised; the police only manage the traffic, and volunteers manage the parking.

Padstow in Cornwall holds its annual Obby-Oss (Hobby Horse) day of festivities. This is believed to be one of the oldest fertility rites in the UK; revellers dance with the Oss through the streets of the town and even though the private gardens of the citizens, accompanied by accordion players and followers dressed in white with red or blue sashes who sing the traditional "May Day" song. The whole town is decorated with springtime greenery, and every year thousands of onlookers attend. Before the 19th century, distinctive May Day celebrations were widespread throughout West Cornwall, and are being revived in St. Ives and Penzance.

Kingsand, Cawsand and Millbrook in Cornwall celebrate Flower Boat Ritual on the May Day bank holiday. A model of the ship "The Black Prince" is covered in flowers and is taken in a procession from the Quay at Millbrook to the beach at Cawsand where it is cast adrift. The houses in the villages are decorated with flowers and people traditionally wear red and white clothes. There are further celebrations in Cawsand Square with Morris dancing and May pole dancing.

May Day has been celebrated in Scotland for centuries. It was previously closely associated with the Beltane festival. Reference to this earlier celebration is found in poem 'Peblis to the Play', contained in the Maitland Manuscripts of fifteenth- and sixteenth-century Scots poetry:

<poem style="margin-left: 2em;">At Beltane, quhen ilk bodie bownis
To Peblis to the Play,
To heir the singin and the soundis;
The solace, suth to say,
Be firth and forrest furth they found
Thay graythis tham full gay;
God wait that wald they do that stound,
For it was their feast day the day they celebrate May Day,
Thay said, [...]</poem>

The poem describes the celebration in the town of Peebles in the Scottish Borders, which continues to stage a parade and pageant each year, including the annual ‘Common Riding’, which takes place in many towns throughout the Borders. As well as the crowning of a Beltane Queen each year, it is custom to sing ‘The Beltane Song’.

John Jamieson, in his "Etymological Dictionary of the Scottish Language" (1808) describes some of the May Day/Beltane customs which persisted in the eighteenth and early nineteenth centuries in parts of Scotland, which he noted were beginning to die out. In the nineteenth century, folklorist Alexander Carmichael (1832–1912), collected the song "Am Beannachadh Bealltain" ("The Beltane Blessing") in his "Carmina Gadelica", which he heard from a crofter in South Uist.

Scottish May Day/Beltane celebrations have been somewhat revived since the late twentieth century. Both Edinburgh and Glasgow organise May Day festivals and rallies. In Edinburgh, the Beltane Fire Festival is held on the evening of May eve and into the early hours of May Day on the city's Calton Hill. An older Edinburgh tradition has it that young women who climb Arthur's Seat and wash their faces in the morning dew will have lifelong beauty. At the University of St Andrews, some of the students gather on the beach late on 30 April and run into the North Sea at sunrise on May Day, occasionally naked. This is accompanied by torchlit processions and much elated celebration.

In Wales the first day of May is known as "Calan Mai" or "Calan Haf", and parallels the festival of Beltane and other May Day traditions in Europe.

Traditions would start the night before ("Nos Galan Haf") with bonfires, and is considered a "Ysbrydnos" or "spirit night" when people would gather hawthorn ("draenen wen") and flowers to decorate their houses, celebrating new growth and fertility. While on May Day celebrations would include summer dancing ("dawnsio haf") and May carols ("carolau mai" or "carolau haf") othertimes referred to as "singing under the wall" ("canu dan y pared)," May Day was also a time for officially opening a village green (twmpath chwarae).

May Day is celebrated in some parts of the provinces of British Columbia, Quebec, New Brunswick and Ontario.

Toronto

In Toronto, on the morning of 1 May, various Morris Dancing troops from Toronto and Hamilton gather on the road by Grenadier Cafe, in High Park to "dance in the May". The dancers and crowd then gather together and sing traditional May Day songs such as Hal-An-Tow and Padstow.

British Columbia

Celebrations often take place not on 1 May but during the Victoria Day long weekend, later in the month and when the weather is likely to be better. The longest continually observed May Day in the British Commonwealth is held in the city of New Westminster, BC. There, the first May Day celebration was held on 4 May 1870.

May Day was also celebrated by some early European settlers of the American continent. In some parts of the United States, May baskets are made. These are small baskets usually filled with flowers or treats and left at someone's doorstep. The giver rings the bell and runs away.

Modern May Day ceremonies in the U.S. vary greatly from region to region and many unite both the holiday's "Green Root" (pagan) and "Red Root" (labour) traditions.

May Day celebrations were common at women's colleges and academic institutions in the late nineteenth and early twentieth century, a tradition that continues at Bryn Mawr College and Brenau University to this day.

In Minneapolis, the May Day Parade and Festival is presented annually by In the Heart of the Beast Puppet and Mask Theatre on the first Sunday in May, and draws around 50,000 people to Powderhorn Park. On 1 May itself, local Morris Dance sides converge on an overlook of the Mississippi River at dawn, and then spend the remainder of the day dancing around the metro area.

Hawaii

In Hawaii, May Day is also known as Lei Day, and it is normally set aside as a day to celebrate island culture in general and the culture of the Native Hawaiians in particular. Invented by poet and local newspaper columnist Don Blanding, the first Lei Day was celebrated on 1 May 1927 in Honolulu. Leonard "Red" and Ruth Hawk composed "May Day Is Lei Day in Hawai'i," the traditional holiday song.




</doc>
<doc id="19830" url="https://en.wikipedia.org/wiki?curid=19830" title="Maxwell–Boltzmann distribution">
Maxwell–Boltzmann distribution

In physics (in particular in statistical mechanics), the Maxwell–Boltzmann distribution is a particular probability distribution named after James Clerk Maxwell and Ludwig Boltzmann.

It was first defined and used for describing particle speeds in idealized gases, where the particles move freely inside a stationary container without interacting with one another, except for very brief collisions in which they exchange energy and momentum with each other or with their thermal environment. The term "particle" in this context refers to gaseous particles only (atoms or molecules), and the system of particles is assumed to have reached thermodynamic equilibrium. The energies of such particles follow what is known as Maxwell-Boltzmann statistics, and the statistical distribution of speeds is derived by equating particle energies with kinetic energy.

Mathematically, the Maxwell–Boltzmann distribution is the chi distribution with three degrees of freedom (the components of the velocity vector in Euclidean space), with a scale parameter measuring speeds in units proportional to the square root of formula_8 (the ratio of temperature and particle mass).

The Maxwell–Boltzmann distribution is a result of the kinetic theory of gases, which provides a simplified explanation of many fundamental gaseous properties, including pressure and diffusion. The Maxwell–Boltzmann distribution applies fundamentally to particle velocities in three dimensions, but turns out to depend only on the speed (the magnitude of the velocity) of the particles. A particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another. The kinetic theory of gases applies to the classical ideal gas, which is an idealization of real gases. In real gases, there are various effects (e.g., van der Waals interactions, vortical flow, relativistic speed limits, and quantum exchange interactions) that can make their speed distribution different from the Maxwell–Boltzmann form. However, rarefied gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. Ideal plasmas, which are ionized gases of sufficiently low density, frequently also have particle distributions that are partially or entirely Maxwellian.

The distribution was first derived by Maxwell in 1860 on heuristic grounds. Boltzmann later, in the 1870s, carried out significant investigations into the physical origins of this distribution.

The distribution can be derived on the ground that it maximizes the entropy of the system. A list of derivations are:


Assuming the system of interest contains a large number of particles, the fraction of the particles within an infinitesimal element of three-dimensional velocity space, formula_10, centered on a velocity vector of magnitude formula_11, is formula_12, in which

where formula_14 is the particle mass and formula_15 is the product of Boltzmann's constant and thermodynamic temperature.
One can write the element of velocity space as dformula_16 = dformula_17dformula_18dformula_19, for velocities in a standard Cartesian coordinate system, or as dformula_16 = formula_21dformula_11dformula_23 in a standard spherical coordinate system, where dformula_23 is an element of solid angle. Here formula_25 is given as a probability distribution function, properly normalized so that formula_26dformula_16 over all velocities equals one. In plasma physics, the probability distribution is often multiplied by the particle density, so that the integral of the resulting distribution function equals the density.

The Maxwellian distribution function for particles moving in only one direction, if this direction is formula_28, is
which can be obtained by integrating the three-dimensional form given above over formula_18 and formula_19.

Recognizing the symmetry of formula_25, one can integrate over solid angle and write a probability distribution of speeds as the function

This probability density function gives the probability, per unit speed, of finding the particle with a speed near formula_11. This equation is simply the Maxwell–Boltzmann distribution (given in the infobox) with distribution parameter formula_35. 
The Maxwell–Boltzmann distribution is equivalent to the chi distribution with three degrees of freedom and scale parameter formula_35.

The simplest ordinary differential equation satisfied by the distribution is:

or in unitless presentation:

With the Darwin–Fowler method of mean values, the Maxwell–Boltzmann distribution is obtained as an exact result.

For particles confined to move in a plane, the speed distribution is given by

formula_41

This distribution is used for describing systems in equilibrium. However, most systems do not start out in their equilibrium state. The evolution of a system towards its equilibrium state is governed by the Boltzmann equation. The equation predicts that for short range interactions, the equilibrium velocity distribution will follow a Maxwell–Boltzmann distribution. To the right is a molecular dynamics (MD) simulation in which 900 hard sphere particles are constrained to move in a rectangle. They interact via perfectly elastic collisions. The system is initialized out of equilibrium, but the velocity distribution (in blue) quickly converges to the 2D Maxwell–Boltzmann distribution (in orange).

The mean speed formula_42, 
most probable speed (mode) , 
and root-mean-square speed formula_43 
can be obtained from properties of the Maxwell distribution.

This works well for nearly ideal, monatomic gases like helium, but also for molecular gases like diatomic oxygen. 
This is because despite the larger heat capacity (larger internal energy at the same temperature) due to their larger number of degrees of freedom, their translational kinetic energy (and thus their speed) is unchanged.

with the solution:

For diatomic nitrogen (N, the primary component of air) 
at room temperature (), this gives 

In summary, the typical speeds are related as follows:

The root mean square speed is directly related to the speed of sound in the gas, by
where formula_62 is the adiabatic index, is the number of degrees of freedom of the individual gas molecule. For the example above, diatomic nitrogen (approximating air) at , formula_63 and
the true value for air can be approximated by using the average molar weight of air (), yielding 

The average relative velocity

where the three-dimensional velocity distribution is

The integral can easily be done by changing to coordinates formula_67 and formula_68

The original derivation in 1860 by James Clerk Maxwell was an argument based on molecular collisions of the Kinetic theory of gases as well as certain symmetries in the speed distribution function; Maxwell also gave an early argument that these molecular collisions entail a tendency towards equilibrium. After Maxwell, Ludwig Boltzmann in 1872 also derived the distribution on mechanical grounds and argued that gases should over time tend toward this distribution, due to collisions (see H-theorem). He later (1877) derived the distribution again under the framework of statistical thermodynamics. The derivations in this section are along the lines of Boltzmann's 1877 derivation, starting with result known as Maxwell–Boltzmann statistics (from statistical thermodynamics). Maxwell–Boltzmann statistics gives the average number of particles found in a given single-particle microstate. Under certain assumptions, the logarithm of the fraction of particles in a given microstate is proportional to the ratio of the energy of that state to the temperature of the system:
The assumptions of this equation are that the particles do not interact, and that they are classical; this means that each particle's state can be considered independently from the other particles' states. Additionally, the particles are assumed to be in thermal equilibrium.

This relation can be written as an equation by introducing a normalizing factor:
where:
The denominator in Equation () is simply a normalizing factor so that the ratios formula_70 add up to unity — in other words it is a kind of partition function (for the single-particle system, not the usual partition function of the entire system).

Because velocity and speed are related to energy, Equation () can be used to derive relationships between temperature and the speeds of gas particles. All that is needed is to discover the density of microstates in energy, which is determined by dividing up momentum space into equal sized regions.

The potential energy is taken to be zero, so that all energy is in the form of kinetic energy.
The relationship between kinetic energy and momentum for massive non-relativistic particles is

where "p" is the square of the momentum vector 
p = ["p", "p", "p"]. We may therefore rewrite Equation () as:

where "Z" is the partition function, corresponding to the denominator in Equation (). Here "m" is the molecular mass of the gas, "T" is the thermodynamic temperature and "k" is the Boltzmann constant. This distribution of formula_70 is proportional to the probability density function "f" for finding a molecule with these values of momentum components, so:

The normalizing constant can be determined by recognizing that the probability of a molecule having "some" momentum must be 1.
Integrating the exponential in () over all "p", "p", and "p" yields a factor of 

So that the normalized distribution function is:
The distribution is seen to be the product of three independent normally distributed variables formula_73, formula_74, and formula_75, with variance formula_76. Additionally, it can be seen that the magnitude of momentum will be distributed as a Maxwell–Boltzmann distribution, with formula_77.
The Maxwell–Boltzmann distribution for the momentum (or equally for the velocities) can be obtained more fundamentally using the H-theorem at equilibrium within the Kinetic theory of gases framework.

The energy distribution is found imposing

where formula_78 is the infinitesimal phase-space volume of momenta corresponding to the energy interval formula_79.
Making use of the spherical symmetry of the energy-momentum dispersion relation formula_80,
this can be expressed in terms of formula_79 as

Using then () in (), and expressing everything in terms of the energy formula_82, we get
and finally

Since the energy is proportional to the sum of the squares of the three normally distributed momentum components, this energy distribution can be written equivalently as a gamma distribution, using a shape parameter, formula_84 and a scale parameter, formula_85.

Using the equipartition theorem, given that the energy is evenly distributed among all three degrees of freedom in equilibrium, we can also split formula_86 into a set of chi-squared distributions, where the energy per degree of freedom, formula_87, is distributed as a chi-squared distribution with one degree of freedom,

At equilibrium, this distribution will hold true for any number of degrees of freedom. For example, if the particles are rigid mass dipoles of fixed dipole moment, they will have three translational degrees of freedom and two additional rotational degrees of freedom. The energy in each degree of freedom will be described according to the above chi-squared distribution with one degree of freedom, and the total energy will be distributed according to a chi-squared distribution with five degrees of freedom. This has implications in the theory of the specific heat of a gas.

The Maxwell–Boltzmann distribution can also be obtained by considering the gas to be a type of quantum gas for which the approximation "ε » k T" may be made.

Recognizing that the velocity probability density "f" is proportional to the momentum probability density function by

and using p = mv we get

which is the Maxwell–Boltzmann velocity distribution. The probability of finding a particle with velocity in the infinitesimal element ["dv", "dv", "dv"] about velocity v = ["v", "v", "v"] is

Like the momentum, this distribution is seen to be the product of three independent normally distributed variables formula_17, formula_18, and formula_19, but with variance formula_94.
It can also be seen that the Maxwell–Boltzmann velocity distribution for the vector velocity
["v", "v", "v"] is the product of the distributions for each of the three directions:

where the distribution for a single direction is

Each component of the velocity vector has a normal distribution with mean formula_97 and standard deviation formula_98, so the vector has a 3-dimensional normal distribution, a particular kind of multivariate normal distribution, with mean formula_99 and covariance formula_100, where formula_101 is the formula_102 identity matrix.

The Maxwell–Boltzmann distribution for the speed follows immediately from the distribution of the velocity vector, above. Note that the speed is

and the volume element in spherical coordinates

where formula_105 and formula_106 are the spherical coordinate angles of the velocity vector. Integration of the probability density function of the velocity over the solid angles formula_107 yields an additional factor of formula_108.
The speed distribution with substitution of the speed for the sum of the squares of the vector components:
In "n"-dimensional space, Maxwell-Boltzmann distribution becomes:

formula_109

Speed distribution becomes:

formula_110

The following integral result is useful:

formula_111

where formula_112is the Gamma function. This result can be used to calculate the moments of speed distribution function:

formula_113

which is the mean speed itself formula_114.

formula_115

which gives root-mean-square speed formula_116.

The derivative of speed distribution function:

formula_117

This yields the most probable speed (mode) formula_118.





</doc>
<doc id="19831" url="https://en.wikipedia.org/wiki?curid=19831" title="Margaret Thatcher">
Margaret Thatcher

Margaret Hilda Thatcher, Baroness Thatcher, (; 13 October 19258 April 2013) was a British stateswoman who served as prime minister of the United Kingdom from 1979 to 1990 and leader of the Conservative Party from 1975 to 1990. She was the longest-serving British prime minister of the 20th century and the first woman to hold that office. A Soviet journalist dubbed her the "Iron Lady", a nickname that became associated with her uncompromising politics and leadership style. As Prime Minister, she implemented policies known as Thatcherism.

Thatcher studied chemistry at Somerville College, Oxford, and worked briefly as a research chemist, before becoming a barrister. She was elected Member of Parliament for Finchley in 1959. Edward Heath appointed her secretary of state for education and science in his 1970–74 government. In 1975, she defeated Heath in the Conservative Party leadership election to become leader of the Opposition, the first woman to lead a major political party in the United Kingdom. On becoming prime minister after winning the 1979 general election, Thatcher introduced a series of economic policies intended to reverse high unemployment and Britain's struggles in the wake of the Winter of Discontent and an ongoing recession. Her political philosophy and economic policies emphasised deregulation (particularly of the financial sector), flexible labour markets, the privatisation of state-owned companies, and reducing the power and influence of trade unions. Her popularity in her first years in office waned amid recession and rising unemployment, until victory in the 1982 Falklands War and the recovering economy brought a resurgence of support, resulting in her landslide re-election in 1983. She survived an assassination attempt by the Provisional IRA in the 1984 Brighton hotel bombing and achieved a political victory against the National Union of Mineworkers in the 1984–85 miners' strike.

Thatcher was re-elected for a third term with another landslide in 1987, but her subsequent support for the Community Charge ("poll tax") was widely unpopular, and her increasingly Eurosceptic views on the European Community were not shared by others in her Cabinet. She resigned as prime minister and party leader in November 1990, after Michael Heseltine launched a challenge to her leadership.
After retiring from the Commons in 1992, she was given a life peerage as Baroness Thatcher (of Kesteven in the County of Lincolnshire) which entitled her to sit in the House of Lords. In 2013, she died of a stroke at the Ritz Hotel in London, at the age of 87.

Although a controversial figure in British political culture, Thatcher is nonetheless viewed favourably in historical rankings of British prime ministers. Her tenure constituted a realignment towards neoliberal policies in the United Kingdom and debate over the complicated legacy of Thatcherism persists into the 21st century.
Margaret Hilda Roberts was born on 13 October 1925, in Grantham, Lincolnshire. Her parents were Alfred Roberts (1892–1970), from Northamptonshire, and Beatrice Ethel (née Stephenson, 1888–1960), from Lincolnshire. Her father's maternal grandmother, Catherine Sullivan, was born in County Kerry, Ireland.

Thatcher spent her childhood in Grantham, where her father owned a tobacconist's and a grocery shop. In 1938, prior to the Second World War, the Roberts family briefly gave sanctuary to a teenage Jewish girl who had escaped Nazi Germany. Margaret, with her pen-friending elder sister Muriel, saved pocket money to help pay for the teenager's journey.

Alfred Roberts was an alderman and a Methodist local preacher, and brought up his daughter as a strict Wesleyan Methodist, attending the Finkin Street Methodist Church, but Margaret was more skeptical; the future scientist told a friend that she could not believe in angels, having calculated that they needed a breastbone six feet long to support wings. Alfred came from a Liberal family but stood (as was then customary in local government) as an Independent. He served as Mayor of Grantham in 1945–46 and lost his position as alderman in 1952 after the Labour Party won its first majority on Grantham Council in 1950.
Margaret Roberts attended Huntingtower Road Primary School and won a scholarship to Kesteven and Grantham Girls' School, a grammar school. Her school reports showed hard work and continual improvement; her extracurricular activities included the piano, field hockey, poetry recitals, swimming and walking. She was head girl in 1942–43. Other students thought of Roberts as the "star scientist", although mistaken advice regarding cleaning ink from parquetry almost caused chlorine gas poisoning. In her upper sixth year Roberts was accepted for a scholarship to study chemistry at Somerville College, Oxford, a women's college, starting in 1944. After another candidate withdrew, Roberts entered Oxford in October 1943.

Roberts arrived at Oxford in 1943 and graduated in 1947 with Second-Class Honours, in the four-year Chemistry Bachelor of Science degree, specialising in X-ray crystallography under the supervision of Dorothy Hodgkin. Her dissertation was on the structure of the antibiotic gramicidin. Roberts did not only study chemistry as she intended to be a chemist only for a short period of time, already thinking about law and politics. She was reportedly prouder of becoming the first prime minister with a science degree than becoming the first woman prime minister. While prime minister she attempted to preserve Somerville as a women's college.

During her time at Oxford, Roberts was noted for her isolated and serious attitude. Her first boyfriend, Tony Bray (1926–2014), recalled that she was "very thoughtful and a very good conversationalist. That's probably what interested me. She was good at general subjects". Roberts's enthusiasm for politics as a girl made him think of her as "unusual" and her parents as "slightly austere" and "very proper".

Roberts became President of the Oxford University Conservative Association in 1946. She was influenced at university by political works such as Friedrich Hayek's "The Road to Serfdom" (1944), which condemned economic intervention by government as a precursor to an authoritarian state.

After graduating, Roberts moved to Colchester in Essex to work as a research chemist for BX Plastics. In 1948 she applied for a job at Imperial Chemical Industries (ICI), but was rejected after the personnel department assessed her as "headstrong, obstinate and dangerously self-opinionated". argues that her understanding of modern scientific research would later impact her views as prime minister.

Roberts joined the local Conservative Association and attended the party conference at Llandudno, Wales, in 1948, as a representative of the University Graduate Conservative Association. Meanwhile, she became a high-ranking affiliate of the Vermin Club, a group of grassroots Conservatives formed in response to a derogatory comment made by Aneurin Bevan. One of her Oxford friends was also a friend of the Chair of the Dartford Conservative Association in Kent, who were looking for candidates. Officials of the association were so impressed by her that they asked her to apply, even though she was not on the party's approved list; she was selected in January 1950 (aged 24) and added to the approved list "post ante".

At a dinner following her formal adoption as Conservative candidate for Dartford in February 1949 she met divorcé Denis Thatcher, a successful and wealthy businessman, who drove her to her Essex train. After their first meeting she described him to Muriel as "not a very attractive creature – very reserved but quite nice". In preparation for the election Roberts moved to Dartford, where she supported herself by working as a research chemist for J. Lyons and Co. in Hammersmith, part of a team developing emulsifiers for ice cream. She married at Wesley's Chapel and her children were baptised there, but she and her husband began attending Church of England services and would later convert to Anglicanism.

In the 1950 and 1951 general elections, Roberts was the Conservative candidate for the Labour seat of Dartford. The local party selected her as its candidate because, though not a dynamic public speaker, Roberts was well-prepared and fearless in her answers; prospective candidate Bill Deedes recalled: "Once she opened her mouth, the rest of us began to look rather second-rate." She attracted media attention as the youngest and the only female candidate. She lost on both occasions to Norman Dodds, but reduced the Labour majority by 6,000, and then a further 1,000. During the campaigns, she was supported by her parents and by future husband Denis Thatcher, whom she married in December 1951. Denis funded his wife's studies for the bar; she qualified as a barrister in 1953 and specialised in taxation. Later that same year their twins Carol and Mark were born, delivered prematurely by Caesarean section.

In 1954, Thatcher was defeated when she sought selection to be the Conservative party candidate for the Orpington by-election of January 1955. She chose not to stand as a candidate in the 1955 general election, in later years stating: "I really just felt the twins were ... only two, I really felt that it was too soon. I couldn't do that." Afterwards, Thatcher began looking for a Conservative safe seat and was selected as the candidate for Finchley in April 1958 (narrowly beating Ian Montagu Fraser). She was elected as MP for the seat after a hard campaign in the 1959 election. Benefiting from her fortunate result in a lottery for backbenchers to propose new legislation, Thatcher's maiden speech was, unusually, in support of her private member's bill, the Public Bodies (Admission to Meetings) Act 1960, requiring local authorities to hold their council meetings in public; the bill was successful and became law. In 1961 she went against the Conservative Party's official position by voting for the restoration of birching as a judicial corporal punishment.

Thatcher's talent and drive caused her to be mentioned as a future prime minister in her early 20s although she herself was more pessimistic, stating as late as 1970: "There will not be a woman prime minister in my lifetime – the male population is too prejudiced." In October 1961 she was promoted to the frontbench as parliamentary undersecretary at the Ministry of Pensions and National Insurance by Harold Macmillan. Thatcher was the youngest woman in history to receive such a post, and among the first MPs elected in 1959 to be promoted. After the Conservatives lost the 1964 election she became spokeswoman on Housing and Land, in which position she advocated her party's policy of giving tenants the Right to Buy their council houses. She moved to the Shadow Treasury team in 1966 and, as Treasury spokeswoman, opposed Labour's mandatory price and income controls, arguing they would unintentionally produce effects that would distort the economy.

Jim Prior suggested Thatcher as a Shadow Cabinet member after the Conservatives' 1966 defeat, but party leader Edward Heath and Chief Whip William Whitelaw eventually chose Mervyn Pike as the Conservative Shadow Cabinet's sole woman member. At the 1966 Conservative Party conference, Thatcher criticised the high-tax policies of the Labour government as being steps "not only towards Socialism, but towards Communism", arguing that lower taxes served as an incentive to hard work. Thatcher was one of the few Conservative MPs to support Leo Abse's bill to decriminalise male homosexuality. She voted in favour of David Steel's bill to legalise abortion, as well as a ban on hare coursing. She supported the retention of capital punishment and voted against the relaxation of divorce laws.

In 1967, the United States Embassy chose Thatcher to take part in the International Visitor Leadership Program (then called the Foreign Leader Program), a professional exchange programme that gave her the opportunity to spend about six weeks visiting various US cities and political figures as well as institutions such as the International Monetary Fund. Although she was not yet a Shadow Cabinet member, the embassy reportedly described her to the State Department as a possible future prime minister. The description helped Thatcher meet with prominent people during a busy itinerary focused on economic issues, including Paul Samuelson, Walt Rostow, Pierre-Paul Schweitzer and Nelson Rockefeller. Following the visit, Heath appointed Thatcher to the Shadow Cabinet as Fuel and Power spokeswoman. Prior to the 1970 general election, she was promoted to Shadow Transport spokeswoman and later to Education.

In 1968, Enoch Powell delivered his "Rivers of Blood" speech in which he strongly criticised Commonwealth immigration to the United Kingdom and the then-proposed Race Relations Bill. When Heath telephoned Thatcher to inform her that he was going to sack Powell from the Shadow Cabinet, she recalled that she "really thought that it was better to let things cool down for the present rather than heighten the crisis". She believed that his main points about Commonwealth immigration were correct and that the selected quotations from his speech had been taken out of context. In a 1991 interview for "Today", Thatcher stated that she thought Powell had "made a valid argument, if in sometimes regrettable terms".

Around this time she gave her first Commons speech as a Shadow Transport Minister and highlighted the need for investment in British Rail. She argued: " ... if we build bigger and better roads, they would soon be saturated with more vehicles and we would be no nearer solving the problem." Thatcher made her first visit to the Soviet Union in the summer of 1969 as the Opposition Transport spokeswoman, and in October delivered a speech celebrating her ten years in Parliament. In early 1970, she told "The Finchley Press" that she would like to see a "reversal of the permissive society".

The Conservative Party led by Edward Heath won the 1970 general election, and Thatcher was appointed to the Cabinet as secretary of state for education and science. Thatcher caused controversy when after only a few days in office she withdrew Labour's Circular 10/65 which attempted to force comprehensivisation, without going through a consultation progress. She was highly criticised for the speed in which she carried this out. Consequently, she drafted her own new policy (Circular 10/70) which ensured that local authorities were not forced to go comprehensive. Her new policy was not meant to stop the development of new comprehensives; she said: "We shall ... expect plans to be based on educational considerations rather than on the comprehensive principle."

Thatcher supported Lord Rothschild's 1971 proposal for market forces to affect government funding of research. Although many scientists opposed the proposal, her research background probably made her sceptical of their claim that outsiders should not interfere with funding. The department evaluated proposals for more local education authorities to close grammar schools and to adopt comprehensive secondary education. Although Thatcher was committed to a tiered secondary modern-grammar school system of education and attempted to preserve grammar schools, during her tenure as education secretary she turned down only 326 of 3,612 proposals (roughly 9 per cent) for schools to become comprehensives; the proportion of pupils attending comprehensive schools consequently rose from 32 per cent to 62 per cent. Nevertheless, she managed to save 94 grammar schools.
During her first months in office she attracted public attention as a consequence of the government's attempts to cut spending. She gave priority to academic needs in schools, while administering public expenditure cuts on the state education system, resulting in the abolition of free milk for schoolchildren aged seven to eleven. She held that few children would suffer if schools were charged for milk, but agreed to provide younger children with ⅓ pint daily for nutritional purposes. She also argued that she was simply carrying on with what the Labour government had started since they had stopped giving free milk to secondary schools. Milk would still be provided to those children that required it on medical grounds and schools could still sell milk. The aftermath of the milk row hardened her determination, she told the editor-proprietor Harold Creighton of "The Spectator": "Don't underestimate me, I saw how they broke Keith [Joseph], but they won't break me."

Cabinet papers later revealed that she opposed the policy but had been forced into it by the Treasury. Her decision provoked a storm of protest from Labour and the press, leading to her being notoriously nicknamed "Margaret Thatcher, Milk Snatcher". She reportedly considered leaving politics in the aftermath and later wrote in her autobiography: "I learned a valuable lesson [from the experience]. I had incurred the maximum of political odium for the minimum of political benefit."

The Heath government continued to experience difficulties with oil embargoes and union demands for wage increases in 1973, subsequently losing the February 1974 general election. Labour formed a minority government and went on to win a narrow majority in the October 1974 general election. Heath's leadership of the Conservative Party looked increasingly in doubt. Thatcher was not initially seen as the obvious replacement, but she eventually became the main challenger, promising a fresh start. Her main support came from the parliamentary 1922 Committee and "The Spectator", but Thatcher's time in office gave her the reputation of a pragmatist rather than that of an ideologue. She defeated Heath on the first ballot and he resigned the leadership. In the second ballot she defeated Whitelaw, Heath's preferred successor. Thatcher's election had a polarising effect on the party; her support was stronger among MPs on the right, and also among those from southern England, and those who had not attended public schools or Oxbridge.

Thatcher became Conservative Party leader and leader of the Opposition on 11 February 1975; she appointed Whitelaw as her deputy. Heath was never reconciled to Thatcher's leadership of the party.

Television critic Clive James, writing in "The Observer" prior to her election as Conservative Party leader, compared her voice of 1973 to "a cat sliding down a blackboard". Thatcher had already begun to work on her presentation on the advice of Gordon Reece, a former television producer. By chance, Reece met the actor Laurence Olivier, who arranged lessons with the National Theatre's voice coach.

Thatcher began attending lunches regularly at the Institute of Economic Affairs (IEA), a think tank founded by Hayekian poultry magnate Antony Fisher; she had been visiting the IEA and reading its publications since the early 1960s. There she was influenced by the ideas of Ralph Harris and Arthur Seldon, and became the face of the ideological movement opposing the British welfare state. Keynesian economics, they believed, was weakening Britain. The institute's pamphlets proposed less government, lower taxes, and more freedom for business and consumers.
Thatcher intended to promote neoliberal economic ideas at home and abroad. Despite setting the direction of her foreign policy for a Conservative government, Thatcher was distressed by her repeated failure to shine in the House of Commons. Consequently, Thatcher decided that as "her voice was carrying little weight at home", she would "be heard in the wider world". Thatcher undertook visits across the Atlantic, establishing an international profile and promoting her economic and foreign policies. She toured the United States in 1975 and met President Gerald Ford, visiting again in 1977, when she met President Jimmy Carter. Among other foreign trips, she met Shah Mohammad Reza Pahlavi during a visit to Iran in 1978. Thatcher chose to travel without being accompanied by her shadow foreign secretary, Reginald Maudling, in an attempt to make a bolder personal impact.

In domestic affairs, Thatcher opposed Scottish devolution (home rule) and the creation of a Scottish Assembly. She instructed Conservative MPs to vote against the Scotland and Wales Bill in December 1976, which was successfully defeated, and then when new Bills were proposed she supported amending the legislation to allow the English to vote in the 1979 referendum on Scottish devolution.

Britain's economy during the 1970s was so weak that then foreign secretary James Callaghan warned his fellow Labour Cabinet members in 1974 of the possibility of "a breakdown of democracy", telling them: "If I were a young man, I would emigrate." In mid-1978, the economy began to recover and opinion polls showed Labour in the lead, with a general election being expected later that year and a Labour win a serious possibility. Now prime minister, Callaghan surprised many by announcing on 7 September that there would be no general election that year and he would wait until 1979 before going to the polls. Thatcher reacted to this by branding the Labour government "chickens", and Liberal Party leader David Steel joined in, criticising Labour for "running scared".

The Labour government then faced fresh public unease about the direction of the country and a damaging series of strikes during the winter of 1978–79, dubbed the "Winter of Discontent". The Conservatives attacked the Labour government's unemployment record, using advertising with the slogan "Labour Isn't Working". A general election was called after the Callaghan ministry lost a motion of no confidence in early 1979. The Conservatives won a 44-seat majority in the House of Commons and Thatcher became the first female British prime minister.

In 1976, Thatcher gave her "Britain Awake" foreign policy speech which lambasted the Soviet Union for seeking world dominance. The Soviet Army journal "Krasnaya Zvezda" ("Red Star" rebutted her stance in a piece entitled "Iron Lady Raises Fears" by Captain Yuri Gavrilov (alluding to "Iron Chancellor" Bismarck of imperial Germany). "The Sunday Times" covered the "Red Star" article the next day, and Thatcher embraced the epithet a week later; in a speech to Finchley Conservatives she compared it to the Duke of Wellington's nickname "The Iron Duke". The metaphorical sobriquet followed her throughout her political career, and has since become a generic descriptor for strong-willed female politicians.

Thatcher became prime minister on 4 May 1979. Arriving at Downing Street she said, paraphrasing the Prayer of Saint Francis:

In office throughout the 1980s, Thatcher was frequently described as the most powerful woman in the world.

Thatcher was Opposition leader and prime minister at a time of increased racial tension in Britain. Commenting on the local elections of 1977, "The Economist" noted: "The Tory tide swamped the smaller parties. That specifically includes the National Front (NF), which suffered a clear decline from last year." Her standing in the polls had risen by 11% after a 1978 interview for "World in Action" in which she said "the British character has done so much for democracy, for law and done so much throughout the world that if there is any fear that it might be swamped people are going to react and be rather hostile to those coming in", as well as "in many ways [minorities] add to the richness and variety of this country. The moment the minority threatens to become a big one, people get frightened". In the 1979 general election, the Conservatives had attracted votes from the NF, whose support almost collapsed. In a July 1979 meeting with Foreign Secretary Lord Carrington and Home Secretary William Whitelaw, Thatcher objected to the number of Asian immigrants, in the context of limiting the total of Vietnamese boat people allowed to settle in the UK to fewer than 10,000 over two years.

As prime minister, Thatcher met weekly with Queen Elizabeth II to discuss government business, and their relationship came under close scrutiny. states:

Michael Shea, the Queen's press secretary, had reportedly leaked anonymous rumours of a rift, which were officially denied by her private secretary, William Heseltine. Thatcher later wrote: "I always found the Queen's attitude towards the work of the Government absolutely correct ... stories of clashes between 'two powerful women' were just too good not to make up."

Thatcher's economic policy was influenced by monetarist thinking and economists such as Milton Friedman and Alan Walters. Together with her first Chancellor, Geoffrey Howe, she lowered direct taxes on income and increased indirect taxes. She increased interest rates to slow the growth of the money supply and thereby lower inflation, introduced cash limits on public spending, and reduced expenditure on social services such as education and housing. Cuts to higher education led to Thatcher being the first Oxford-educated, post-war incumbent without an honorary doctorate from Oxford University, after a 738–319 vote of the governing assembly and a student petition.

Some Heathite Conservatives in the Cabinet, the so-called "wets", expressed doubt over Thatcher's policies. The 1981 England riots resulted in the British media discussing the need for a policy U-turn. At the 1980 Conservative Party conference, Thatcher addressed the issue directly, with a speech written by the playwright Ronald Millar, that notably included the following lines:
Thatcher's job approval rating fell to 23% by December 1980, lower than recorded for any previous prime minister. As the recession of the early 1980s deepened, she increased taxes, despite concerns expressed in a March 1981 statement signed by 364 leading economists, which argued there was "no basis in economic theory ... for the Government's belief that by deflating demand they will bring inflation permanently under control", adding that "present policies will deepen the depression, erode the industrial base of our economy and threaten its social and political stability".
By 1982, the UK began to experience signs of economic recovery; inflation was down to 8.6% from a high of 18%, but unemployment was over 3 million for the first time since the 1930s. By 1983, overall economic growth was stronger, and inflation and mortgage rates had fallen to their lowest levels in 13 years, although manufacturing employment as a share of total employment fell to just over 30%, with total unemployment remaining high, peaking at 3.3 million in 1984.

During the 1982 Conservative Party Conference, Thatcher said: "We have done more to roll back the frontiers of socialism than any previous Conservative Government." She claimed at the Party Conference the following year that the British people had completely rejected state socialism and understood "the state has no source of money other than money which people earn themselves ... There is no such thing as public money; there is only taxpayers' money."

By 1987, unemployment was falling, the economy was stable and strong and inflation was low. Opinion polls showed a comfortable Conservative lead, and local council election results had also been successful, prompting Thatcher to call a general election for 11 June that year, despite the deadline for an election still being 12 months away. The election saw Thatcher re-elected for a third successive term.

Thatcher had been firmly opposed to British membership of the Exchange Rate Mechanism (ERM, a precursor to European Monetary Union), believing that it would constrain the British economy, despite the urging of both Chancellor of the Exchequer Nigel Lawson and Foreign Secretary Geoffrey Howe; in October 1990 she was persuaded by John Major, Lawson's successor as Chancellor, to join the ERM at what proved to be too high a rate.

Thatcher reformed local government taxes by replacing domestic rates (a tax based on the nominal rental value of a home) with the Community Charge (or poll tax) in which the same amount was charged to each adult resident. The new tax was introduced in Scotland in 1989 and in England and Wales the following year, and proved to be among the most unpopular policies of her premiership. Public disquiet culminated in a 70,000 to 200,000-strong demonstration in London in March 1990; the demonstration around Trafalgar Square deteriorated into riots, leaving 113 people injured and 340 under arrest. The Community Charge was abolished in 1991 by her successor, John Major. It has since transpired that Thatcher herself had failed to register for the tax, and was threatened with financial penalties if she did not return her form.

Thatcher believed that the trade unions were harmful to both ordinary trade unionists and the public. She was committed to reducing the power of the unions, whose leadership she accused of undermining parliamentary democracy and economic performance through strike action. Several unions launched strikes in response to legislation introduced to limit their power, but resistance eventually collapsed. Only 39% of union members voted Labour in the 1983 general election. According to the BBC in 2004, Thatcher "managed to destroy the power of the trade unions for almost a generation". The miners' strike of 1984–85 was the biggest and most devastating confrontation between the unions and the government under Thatcher.

In March 1984, the National Coal Board (NCB) proposed to close 20 of the 174 state-owned mines and cut 20,000 jobs out of 187,000. Two-thirds of the country's miners, led by the National Union of Mineworkers (NUM) under Arthur Scargill, downed tools in protest. However, Scargill refused to hold a ballot on the strike, having previously lost three ballots on a national strike (in January and October 1982, and March 1983). This led to the strike being declared illegal by the High Court of Justice.

Thatcher refused to meet the union's demands and compared the miners' dispute to the Falklands War, declaring in a speech in 1984: "We had to fight the enemy without in the Falklands. We always have to be aware of the enemy within, which is much more difficult to fight and more dangerous to liberty." Thatcher's opponents presented her words as indicating contempt for the working class, and have been employed in criticism of her ever since.

After a year out on strike, in March 1985 the NUM leadership conceded without a deal. The cost to the economy was estimated to be at least £1.5 billion, and the strike was blamed for much of the pound's fall against the US dollar. Thatcher reflected on the end of the strike in her statement that "if anyone has won" it was "the miners who stayed at work" and all those "that have kept Britain going".

The government closed 25 unprofitable coal mines in 1985, and by 1992 a total of 97 mines had been closed; those that remained were privatised in 1994. The resulting closure of 150 coal mines, some of which were not losing money, resulted in the loss of tens of thousands of jobs and had the effect of devastating entire communities. Strikes had helped bring down Heath's government, and Thatcher was determined to succeed where he had failed. Her strategy of preparing fuel stocks, appointing hardliner Ian MacGregor as NCB leader, and ensuring that police were adequately trained and equipped with riot gear, contributed to her triumph over the striking miners.

The number of stoppages across the UK peaked at 4,583 in 1979, when more than 29 million working days had been lost. In 1984, the year of the miners' strike, there were 1,221, resulting in the loss of more than 27 million working days. Stoppages then fell steadily throughout the rest of Thatcher's premiership; in 1990 there were 630 and fewer than 2 million working days lost, and they continued to fall thereafter. Thatcher's tenure also witnessed a sharp decline in trade union density, with the percentage of workers belonging to a trade union falling from 57.3% in 1979 to 49.5% in 1985. In 1979 up until Thatcher's final year in office, trade union membership also fell, from 13.5 million in 1979 to fewer than 10 million.

The policy of privatisation has been called "a crucial ingredient of Thatcherism". After the 1983 election the sale of state utilities accelerated; more than £29 billion was raised from the sale of nationalised industries, and another £18 billion from the sale of council houses. The process of privatisation, especially the preparation of nationalised industries for privatisation, was associated with marked improvements in performance, particularly in terms of labour productivity.

Some of the privatised industries, including gas, water, and electricity, were natural monopolies for which privatisation involved little increase in competition. The privatised industries that demonstrated improvement sometimes did so while still under state ownership. British Steel Corporation had made great gains in profitability while still a nationalised industry under the government-appointed MacGregor chairmanship, which faced down trade-union opposition to close plants and halve the workforce. Regulation was also significantly expanded to compensate for the loss of direct government control, with the foundation of regulatory bodies such as Oftel (1984), Ofgas (1986), and the National Rivers Authority (1989). There was no clear pattern to the degree of competition, regulation, and performance among the privatised industries.

In most cases privatisation benefited consumers in terms of lower prices and improved efficiency, but results overall have been mixed. Not all privatised companies have had successful share price trajectories in the longer term. A 2010 review by the Institute of Economic Affairs states: "it does seem to be the case that once competition and/or effective regulation was introduced, performance improved markedly ... But I hasten to emphasise again that the literature is not unanimous."

Thatcher always resisted privatising British Rail and was said to have told Transport Secretary Nicholas Ridley: "Railway privatisation will be the Waterloo of this government. Please never mention the railways to me again." Shortly before her resignation in 1990, she accepted the arguments for privatisation, which her successor John Major implemented in 1994.

The privatisation of public assets was combined with financial deregulation in an attempt to fuel economic growth. Chancellor Geoffrey Howe abolished the UK's exchange controls in 1979, which allowed more capital to be invested in foreign markets, and the Big Bang of 1986 removed many restrictions on the London Stock Exchange.

In 1980 and 1981, Provisional Irish Republican Army (PIRA) and Irish National Liberation Army (INLA) prisoners in Northern Ireland's Maze Prison carried out hunger strikes in an effort to regain the status of political prisoners that had been removed in 1976 by the preceding Labour government. Bobby Sands began the 1981 strike, saying that he would fast until death unless prison inmates won concessions over their living conditions. Thatcher refused to countenance a return to political status for the prisoners, having declared "Crime is crime is crime; it is not political", Nevertheless, the British government privately contacted republican leaders in a bid to bring the hunger strikes to an end. After the deaths of Sands and nine others, the strike ended. Some rights were restored to paramilitary prisoners, but not official recognition of political status. Violence in Northern Ireland escalated significantly during the hunger strikes.

Thatcher narrowly escaped injury in an IRA assassination attempt at a Brighton hotel early in the morning on 12 October 1984. Five people were killed, including the wife of minister John Wakeham. Thatcher was staying at the hotel to prepare for the Conservative Party conference, which she insisted should open as scheduled the following day. She delivered her speech as planned, though rewritten from her original draft, in a move that was widely supported across the political spectrum and enhanced her popularity with the public.

On 6 November 1981, Thatcher and Irish Taoiseach Garret FitzGerald had established the Anglo-Irish Inter-Governmental Council, a forum for meetings between the two governments. On 15 November 1985, Thatcher and FitzGerald signed the Hillsborough Anglo-Irish Agreement, which marked the first time a British government had given the Republic of Ireland an advisory role in the governance of Northern Ireland. In protest, the Ulster Says No movement led by Ian Paisley attracted 100,000 to a rally in Belfast, Ian Gow, later assassinated by the PIRA, resigned as Minister of State in the HM Treasury, and all 15 Unionist MPs resigned their parliamentary seats; only one was not returned in the subsequent by-elections on 23 January 1986.

Thatcher supported an active climate protection policy; she was instrumental in the passing of the Environmental Protection Act 1990, the founding of the Hadley Centre for Climate Research and Prediction, the establishment of the Intergovernmental Panel on Climate Change, and the ratification of the Montreal Protocol on preserving the ozone.

Thatcher helped to put climate change, acid rain and general pollution in the British mainstream in the late 1980s, calling for a global treaty on climate change in 1989. Her speeches included one to the Royal Society in 1988, followed by another to the UN General Assembly in 1989.

Thatcher appointed Lord Carrington, a senior member of the party and former Minister of Defence, as Foreign Minister in 1979. Although he was considered a "wet", he avoided domestic affairs and got along well with Thatcher. The first issue was what to do with Rhodesia, where the five-percent white population was determined to rule the prosperous, largely-black ex-colony in the face of overwhelming international disapproval. After the collapse of the Portuguese Empire in Africa in 1975, South Africa, which had been Rhodesia's chief supporter, realised that country was a liability. Black rule was inevitable, and Carrington brokered a peaceful solution at the Lancaster House conference in December 1979, attended by Rhodesian Prime Minister Ian Smith, as well as the key black leaders: Abel Muzorewa, Robert Mugabe, Joshua Nkomo and Josiah Tongogara. The conference ended the Rhodesian Bush War. The end result was the new nation of Zimbabwe under black rule in 1980.

Thatcher's first foreign-policy crisis came with the 1979 Soviet invasion of Afghanistan. She condemned the invasion, said it showed the bankruptcy of a détente policy, and helped convince some British athletes to boycott the 1980 Moscow Olympics. She gave weak support to US President Jimmy Carter who tried to punish the USSR with economic sanctions. Britain's economic situation was precarious, and most of NATO was reluctant to cut trade ties. Thatcher nevertheless gave the go-ahead for Whitehall to approve MI6 (along with the SAS) to undertake "disruptive action" in Afghanistan. As well working with the CIA in Operation Cyclone, they also supplied weapons, training and intelligence to the "mujaheddin".
The "Financial Times" reported that her government had secretly supplied Ba'athist Iraq under Saddam Hussein with "non-lethal" military equipment since 1981.

Having withdrawn formal recognition from the Pol Pot regime in 1979, the Thatcher government backed the Khmer Rouge keeping their UN seat after they were ousted from power in Cambodia by the Cambodian–Vietnamese War. Although Thatcher denied it at the time, it was revealed in 1991 that, while not directly training any Khmer Rouge, from 1983 the Special Air Service (SAS) was sent to secretly train "the armed forces of the Cambodian non-communist resistance" that remained loyal to Prince Norodom Sihanouk and his former prime minister Son Sann in the fight against the Vietnamese-backed puppet regime.

Thatcher was one of the first Western leaders to respond warmly to reformist Soviet leader Mikhail Gorbachev. Following Reagan–Gorbachev summit meetings and reforms enacted by Gorbachev in the USSR, she declared in November 1988 that "We're not in a Cold War now", but rather in a "new relationship much wider than the Cold War ever was". She went on a state visit to the Soviet Union in 1984 and met with Gorbachev and Council of Ministers chairman Nikolai Ryzhkov. She was decidedly cool toward the German reunification in 1990, but did not try to block it.

Despite opposite personalities, Thatcher bonded quickly with US President Ronald Reagan. She gave strong support to the Reagan administration's Cold War policies based on their shared distrust of communism. A sharp disagreement came in 1983 when Reagan did not consult with her on the invasion of Grenada.

During her first year as prime minister she supported NATO's decision to deploy US nuclear cruise and Pershing II missiles in Western Europe, permitting the US to station more than 160 cruise missiles at RAF Greenham Common, starting in November 1983 and triggering mass protests by the Campaign for Nuclear Disarmament. She bought the Trident nuclear missile submarine system from the US to replace Polaris, tripling the UK's nuclear forces at an eventual cost of more than £12 billion (at 1996–97 prices). Thatcher's preference for defence ties with the US was demonstrated in the Westland affair of 1985–86, when she acted with colleagues to allow the struggling helicopter manufacturer Westland to refuse a takeover offer from the Italian firm Agusta in favour of the management's preferred option, a link with Sikorsky Aircraft. Defence Secretary Michael Heseltine, who had supported the Agusta deal, resigned from the government in protest.

In April 1986 she permitted US F-111s to use Royal Air Force bases for the bombing of Libya in retaliation for the alleged Libyan bombing of a Berlin discothèque, citing the right of self-defence under Article 51 of the UN Charter. Polls suggested that fewer than one in three British citizens approved of her decision.

Thatcher was in the US on a state visit when Iraqi leader Saddam Hussein invaded Kuwait in August 1990. During her talks with President George H. W. Bush, who succeeded Reagan in 1989, she recommended intervention, and put pressure on Bush to deploy troops in the Middle East to drive the Iraqi Army out of Kuwait. Bush was apprehensive about the plan, prompting Thatcher to remark to him during a telephone conversation: "This was no time to go wobbly!" Thatcher's government supplied military forces to the international coalition in the build-up to the Gulf War, but she had resigned by the time hostilities began on 17 January 1991. She applauded the coalition victory as a backbencher, while warning that "the victories of peace will take longer than the battles of war". It was later disclosed that Thatcher suggested threatening Saddam with chemical weapons after the invasion of Kuwait.

On 2 April 1982 the ruling military junta in Argentina ordered the invasion of the British possessions of the Falkland Islands and South Georgia, triggering the Falklands War. The subsequent crisis was "a defining moment of [Thatcher's] premiership". At the suggestion of Harold Macmillan and Robert Armstrong, she set up and chaired a small War Cabinet (formally called ODSA, Overseas and Defence committee, South Atlantic) to oversee the conduct of the war, which by 5–6 April had authorised and dispatched a naval task force to retake the islands. Argentina surrendered on 14 June and "Operation Corporate" was hailed a success, notwithstanding the deaths of 255 British servicemen and 3 Falkland Islanders. Argentine fatalities totalled 649, half of them after the nuclear-powered submarine torpedoed and sank the cruiser on 2 May.

Thatcher was criticised for the neglect of the Falklands' defence that led to the war, and especially by Tam Dalyell in Parliament for the decision to torpedo the "General Belgrano", but overall she was considered a highly capable and committed war leader. The "Falklands factor", an economic recovery beginning early in 1982, and a bitterly divided opposition all contributed to Thatcher's second election victory in 1983. Thatcher frequently referred after the war to the "Falklands spirit"; suggests that this reflected her preference for the streamlined decision-making of her War Cabinet over the painstaking deal-making of peacetime cabinet government.

In September 1982 she visited China to discuss with Deng Xiaoping the sovereignty of Hong Kong after 1997. China was the first communist state Thatcher had visited and she was the first British prime minister to visit China. Throughout their meeting, she sought the PRC's agreement to a continued British presence in the territory. Deng insisted that the PRC's sovereignty on Hong Kong was non-negotiable, but stated his willingness to settle the sovereignty issue with the British government through formal negotiations, and both governments promised to maintain Hong Kong's stability and prosperity. After the two-year negotiations, Thatcher conceded to the PRC government and signed the Sino-British Joint Declaration in Beijing in 1984, agreeing to hand over Hong Kong's sovereignty in 1997.

Despite saying that she was in favour of "peaceful negotiations" to end apartheid, Thatcher opposed sanctions imposed on South Africa by the Commonwealth and the European Economic Community (EEC). She attempted to preserve trade with South Africa while persuading the government there to abandon apartheid. This included "[c]asting herself as President Botha's candid friend", and inviting him to visit the UK in 1984, in spite of the "inevitable demonstrations" against his government. Alan Merrydew of the Canadian broadcaster BCTV News asked Thatcher what her response was "to a reported ANC statement that they will target British firms in South Africa?", to which she later replied: " ... when the ANC says that they will target British companies ... This shows what a typical terrorist organisation it is. I fought terrorism all my life and if more people fought it, and we were all more successful, we should not have it and I hope that everyone in this hall will think it is right to go on fighting terrorism." During his visit to Britain five months after his release from prison, Nelson Mandela praised Thatcher: "She is an enemy of apartheid ... We have much to thank her for."

Thatcher and her party supported British membership of the EEC in the 1975 national referendum and the Single European Act of 1986, and obtained the UK rebate on contributions, but she believed that the role of the organisation should be limited to ensuring free trade and effective competition, and feared that the EEC approach was at odds with her views on smaller government and deregulation. Believing that the single market would result in political integration, Thatcher's opposition to further European integration became more pronounced during her premiership and particularly after her third government in 1987. In her Bruges speech in 1988, Thatcher outlined her opposition to proposals from the EEC, forerunner of the European Union, for a federal structure and increased centralisation of decision making:

Thatcher, sharing the concerns of French President François Mitterrand, was initially opposed to German reunification, telling Gorbachev that it "would lead to a change to postwar borders, and we cannot allow that because such a development would undermine the stability of the whole international situation and could endanger our security". She expressed concern that a united Germany would align itself more closely with the Soviet Union and move away from NATO. In March 1990, West German Chancellor Helmut Kohl reassured Thatcher that he would keep her "informed of all his intentions about unification", and that he was prepared to disclose "matters which even his cabinet would not know".

Thatcher was challenged for the leadership of the Conservative Party by the little-known backbench MP Sir Anthony Meyer in the 1989 leadership election. Of the 374 Conservative MPs eligible to vote, 314 voted for Thatcher and 33 for Meyer. Her supporters in the party viewed the result as a success, and rejected suggestions that there was discontent within the party.

During her premiership Thatcher had the second-lowest average approval rating (40%) of any post-war prime minister. Since the resignation of Nigel Lawson as Chancellor in October 1989, polls consistently showed that she was less popular than her party. A self-described conviction politician, Thatcher always insisted that she did not care about her poll ratings and pointed instead to her unbeaten election record.

Opinion polls in September 1990 reported that Labour had established a 14% lead over the Conservatives, and by November the Conservatives had been trailing Labour for 18 months. These ratings, together with Thatcher's combative personality and tendency to override collegiate opinion, contributed to discontent within her party.

Thatcher removed Geoffrey Howe as foreign secretary in July 1989 after he and Lawson had forced her to agree to a plan for Britain to join the European Exchange Rate Mechanism (ERM). Britain joined the ERM in October 1990. On 1 November 1990, Howe, by then the last remaining member of Thatcher's original 1979 cabinet, resigned from his position as deputy prime minister, ostensibly over her open hostility to moves towards European Monetary Union. In his resignation speech on 13 November, Howe commented on Thatcher's openly dismissive attitude to the government's proposal for a new European currency competing against existing currencies (a "hard ECU"):

On 14 November, Michael Heseltine mounted a challenge for the leadership of the Conservative Party. Opinion polls had indicated that he would give the Conservatives a national lead over Labour. Although Thatcher led on the first ballot with the votes of 204 Conservative MPs (54.8%) to 152 votes (40.9%) for Heseltine and 16 abstentions, she was four votes short of the required 15% majority. A second ballot was therefore necessary. Thatcher initially declared her intention to "fight on and fight to win" the second ballot, but consultation with her Cabinet persuaded her to withdraw. After holding an audience with the Queen, calling other world leaders, and making one final Commons speech, on 28 November she left Downing Street in tears. She reportedly regarded her ousting as a betrayal. Her resignation was a shock to many outside Britain, with such foreign observers as Henry Kissinger and Gorbachev expressing private consternation.

Thatcher was replaced as head of government and party leader by Chancellor John Major, who prevailed over Heseltine in the subsequent ballot. Major oversaw an upturn in Conservative support in the 17 months leading to the 1992 general election and led the party to a fourth successive victory on 9 April 1992. Thatcher favoured Major in the leadership contest, but her support for him waned in later years.

Thatcher returned to the backbenches as a constituency parliamentarian after leaving the premiership. Her domestic approval rating recovered after her resignation, though public opinion remained divided on whether her government had been good for the country. Aged 66, she retired from the House at the 1992 general election, saying that leaving the Commons would allow her more freedom to speak her mind.

On leaving the Commons, Thatcher became the first former British prime minister to set up a foundation; the British wing of the Margaret Thatcher Foundation was dissolved in 2005 due to financial difficulties. She wrote two volumes of memoirs, "The Downing Street Years" (1993) and "The Path to Power" (1995). In 1991 she and her husband Denis moved to a house in Chester Square, a residential garden square in central London's Belgravia district.

Thatcher was hired by the tobacco company Philip Morris as a "geopolitical consultant" in July 1992, for $250,000 per year and an annual contribution of $250,000 to her foundation. Thatcher earned $50,000 for each speech she delivered.

Thatcher became an advocate of Croatian and Slovenian independence. Commenting on the Yugoslav Wars, in a 1991 interview for Croatian Radiotelevision, she was critical of Western governments for not recognising the breakaway republics of Croatia and Slovenia as independent and for not supplying them with arms after the Serbian-led Yugoslav Army attacked.

In August 1992 she called for NATO to stop the Serbian assault on Goražde and Sarajevo, to end ethnic cleansing during the Bosnian War, comparing the situation in Bosnia–Herzegovina to "the barbarities of Hitler's and Stalin's".

She made a series of speeches in the Lords criticising the Maastricht Treaty, describing it as "a treaty too far" and stated: "I could never have signed this treaty." She cited A. V. Dicey when arguing that, as all three main parties were in favour of the treaty, the people should have their say in a referendum.

Thatcher served as honorary chancellor of the College of William & Mary in Virginia from 1993 to 2000, while also serving as chancellor of the private University of Buckingham from 1992 to 1998, a university she had formally opened in 1976 as the then education secretary.

After Tony Blair's election as Labour Party leader in 1994, Thatcher praised Blair as "probably the most formidable Labour leader since Hugh Gaitskell", adding: "I see a lot of socialism behind their front bench, but not in Mr Blair. I think he genuinely has moved." Blair responded in kind: "She was a thoroughly determined person, and that is an admirable quality."

In 1998, Thatcher called for the release of former Chilean dictator Augusto Pinochet when Spain had him arrested and sought to try him for human rights violations. She cited the help he gave Britain during the Falklands War. In 1999, she visited him while he was under house arrest near London. Pinochet was released in March 2000 on medical grounds by Home Secretary Jack Straw.
At the 2001 general election, Thatcher supported the Conservative campaign, as she had done in 1992 and 1997, and in the Conservative leadership election following its defeat, she endorsed Iain Duncan Smith over Kenneth Clarke. In 2002 she encouraged George W. Bush to aggressively tackle the "unfinished business" of Iraq under Saddam Hussein, and praised Blair for his "strong, bold leadership" in standing with Bush in the Iraq War.

She broached the same subject in her "", which was published in April 2002 and dedicated to Ronald Reagan, writing that there would be no peace in the Middle East until Saddam Hussein was toppled. Her book also said that Israel must trade land for peace, and that the European Union (EU) was a "fundamentally unreformable", "classic utopian project, a monument to the vanity of intellectuals, a programme whose inevitable destiny is failure". She argued that Britain should renegotiate its terms of membership or else leave the EU and join the North American Free Trade Area.

Following several small strokes she was advised by her doctors not to engage in further public speaking. In March 2002 she announced that on doctors' advice she would cancel all planned speaking engagements and accept no more.

On 26 June 2003, Thatcher's husband Sir Denis died of pancreatic cancer, and was cremated on 3 July at Mortlake Crematorium in London.

On 11 June 2004, Thatcher (against doctor's orders) attended the state funeral service for Ronald Reagan. She delivered her eulogy via videotape; in view of her health, the message had been pre-recorded several months earlier. Thatcher flew to California with the Reagan entourage, and attended the memorial service and interment ceremony for the president at the Ronald Reagan Presidential Library.

In 2005, Thatcher criticised the way the decision to invade Iraq had been made two years previously. Although she still supported the intervention to topple Saddam Hussein, she said that (as a scientist) she would always look for "facts, evidence and proof", before committing the armed forces. She celebrated her 80th birthday on 13 October at the Mandarin Oriental Hotel in Hyde Park, London; guests included the Queen, the Duke of Edinburgh, Princess Alexandra and Tony Blair. Lord (Geoffrey) Howe of Aberavon was also in attendance and said of Thatcher: "Her real triumph was to have transformed not just one party but two, so that when Labour did eventually return, the great bulk of Thatcherism was accepted as irreversible."
In 2006, Thatcher attended the official Washington, D.C. memorial service to commemorate the fifth anniversary of the 11 September attacks on the US. She was a guest of Vice-President Dick Cheney, and met Secretary of State Condoleezza Rice during her visit. In February 2007 Thatcher became the first living British prime minister to be honoured with a statue in the Houses of Parliament. The bronze statue stands opposite that of her political hero, Sir Winston Churchill, and was unveiled on 21 February 2007 with Thatcher in attendance; she remarked in the Members' Lobby of the Commons: "I might have preferred iron – but bronze will do ... It won't rust."

Thatcher was a public supporter of the Prague Declaration on European Conscience and Communism and the resulting Prague Process, and sent a public letter of support to its preceding conference.

After collapsing at a House of Lords dinner, Thatcher, suffering low blood pressure, was admitted to St Thomas' Hospital in central London on 7 March 2008 for tests. In 2009 she was hospitalised again when she fell and broke her arm. Thatcher returned to 10 Downing Street in late November 2009 for the unveiling of an official portrait by artist Richard Stone, an unusual honour for a living former prime minister. Stone was previously commissioned to paint portraits of the Queen and Queen Mother.

On 4 July 2011, Thatcher was to attend a ceremony for the unveiling of a statue to Ronald Reagan, outside the US Embassy in London, but was unable to attend due to her frail health. She last attended a sitting of the House of Lords on 19 July 2010, and on 30 July 2011 it was announced that her office in the Lords had been closed. Earlier that month, Thatcher was named the most competent prime minister of the past 30 years in an Ipsos MORI poll.

Thatcher's daughter Carol that her mother had dementia in 2005, saying "Mum doesn't read much any more because of her memory loss". In her 2008 memoir, Carol wrote that her mother "could hardly remember the beginning of a sentence by the time she got to the end". She later recounted how she was first struck by her mother's dementia when, in conversation, Thatcher confused the Falklands and Yugoslav conflicts; she recalled the pain of needing to tell her mother repeatedly that her husband Denis was dead.

Baroness Thatcher died on 8 April 2013, at the age of 87, after suffering a stroke. She had been staying at a suite in the Ritz Hotel in London since December 2012 after having difficulty with stairs at her Chester Square home in Belgravia. Her death certificate listed the primary causes of death as a "cerebrovascular accident" and "repeated transient ischaemic attack"; secondary causes were listed as a "carcinoma of the bladder" and dementia.

Details of Thatcher's funeral had been agreed with her in advance. She received a ceremonial funeral, including full military honours, with a church service at St Paul's Cathedral on 17 April.

Queen Elizabeth II and the Duke of Edinburgh attended her funeral, marking only the second time in the Queen's reign that she attended the funeral of any of her former prime ministers; the first and only precedent being that of Winston Churchill, who received a state funeral in 1965.

After the service at St Paul's Cathedral, Thatcher's body was cremated at Mortlake Crematorium, where her husband had been cremated. On 28 September, a service for Thatcher was held in the All Saints Chapel of the Royal Hospital Chelsea's Margaret Thatcher Infirmary. In a private ceremony, Thatcher's ashes were interred in the grounds of the hospital, next to those of her husband.

Thatcherism represented a systematic and decisive overhaul of the post-war consensus, whereby the major political parties largely agreed on the central themes of Keynesianism, the welfare state, nationalised industry, and close regulation of the economy, and high taxes. Thatcher generally supported the welfare state, while proposing to rid it of abuses.

She promised in 1982 that the highly popular National Health Service was "safe in our hands". At first she ignored the question of privatising nationalised industries. Heavily influenced by right-wing think tanks, and especially by Keith Joseph, Thatcher broadened her attack. Thatcherism came to refer to her policies as well as aspects of her ethical outlook and personal style, including moral absolutism, nationalism, interest in the individual, and an uncompromising approach to achieving political goals.

Thatcher defined her own political philosophy, in a major and controversial break with the "one-nation" conservatism of her predecessor Edward Heath, in a 1987 interview published in "Woman's Own" magazine:

The number of adults owning shares rose from 7 per cent to 25 per cent during her tenure, and more than a million families bought their council houses, giving an increase from 55 per cent to 67 per cent in owner occupiers from 1979 to 1990. The houses were sold at a discount of 33–55 per cent, leading to large profits for some new owners. Personal wealth rose by 80 per cent in real terms during the 1980s, mainly due to rising house prices and increased earnings. Shares in the privatised utilities were sold below their market value to ensure quick and wide sales, rather than maximise national income.

The "Thatcher years" were also marked by periods of high unemployment and social unrest, and many critics on the left of the political spectrum fault her economic policies for the unemployment level; many of the areas affected by mass unemployment as well as her monetarist economic policies remained blighted for decades, by such social problems as drug abuse and family breakdown. Unemployment did not fall below its May 1979 level during her tenure, only marginally falling below its April 1979 level in 1990. The long-term effects of her policies on manufacturing remain contentious.

Speaking in Scotland in 2009, Thatcher insisted she had no regrets and was right to introduce the "poll tax" and withdraw subsidies from "outdated industries, whose markets were in terminal decline", subsidies that created "the culture of dependency, which had done such damage to Britain". Political economist Susan Strange termed the neoliberal financial growth model "casino capitalism", reflecting her view that speculation and financial trading were becoming more important to the economy than industry.

Critics on the left describe her as divisive and claim she condoned greed and selfishness. Leading Welsh politician Rhodri Morgan, among others, characterised Thatcher as a "Marmite" figure. Journalist Michael White, writing in the aftermath of the 2007–08 financial crisis, challenged the view that her reforms were still a net benefit. Others consider her approach to have been "a mixed bag" and "[a] Curate's egg".

Thatcher did "little to advance the political cause of women" either within her party or the government. states that some British feminists regarded her as "an enemy". claims that, although Thatcher had struggled laboriously against the sexist prejudices of her day to rise to the top, she made no effort to ease the path for other women. Thatcher did not regard women's rights as requiring particular attention as she did not, especially during her premiership, consider that women were being deprived of their rights. She had once suggested the shortlisting of women by default for all public appointments, yet had also proposed that those with young children ought to leave the work force.
Thatcher's stance on immigration in the late 1970s was perceived as part of a rising racist public discourse, which terms "new racism". In opposition, Thatcher believed that the National Front (NF) was winning over large numbers of Conservative voters with warnings against floods of immigrants. Her strategy was to undermine the NF narrative by acknowledging that many of their voters had serious concerns in need of addressing. In 1978 she criticised Labour immigration policy with the goal of attracting voters away from the NF and to the Conservatives. Her rhetoric was followed by an increase in Conservative support at the expense of the NF. Critics on the left accused her of pandering to racism.

Many Thatcherite policies had an influence on the Labour Party, which returned to power in 1997 under Tony Blair. Blair rebranded the party "New Labour" in 1994 with the aim of increasing its appeal beyond its traditional supporters, and to attract those who had supported Thatcher, such as the "Essex man". Thatcher is said to have regarded the "New Labour" rebranding as her greatest achievement.
Shortly after Thatcher's death in 2013, Scottish First Minister Alex Salmond argued that her policies had the "unintended consequence" of encouraging Scottish devolution. Lord Foulkes of Cumnock agreed on "Scotland Tonight" that she had provided "the impetus" for devolution. Writing for "The Scotsman", Thatcher had argued against devolution on the basis that it would eventually lead to Scottish independence.

Thatcher's tenure of 11 years and 209 days as prime minister was the longest since Lord Salisbury (13 years and 252 days, in three spells) and the longest continuous period in office since Lord Liverpool (14 years and 305 days). She remains the longest-serving Prime Minister officially referred to as such, as the post was only officially given recognition in the order of precedence in 1905.

Having led the Conservative Party to victory in three consecutive general elections, twice in a landslide, she ranks among the most popular party leaders in British history in terms of votes cast for the winning party; over 40 million ballots were cast in total for the Conservatives under her leadership. Her electoral successes were dubbed a "historic hat trick" by the British press in 1987.

Thatcher ranked highest among living persons in the 2002 BBC poll "100 Greatest Britons". In 1999, "Time" deemed Thatcher one of the . In 2015 she topped a poll by "Scottish Widows", a major financial services company, as the most influential woman of the past 200 years; and in 2016 topped BBC Radio 4's "Woman's Hour Power List" of women judged to have had the biggest impact on female lives over the past 70 years. In 2020, "Time" magazine included Thatcher's name on its list of 100 Women of the Year. She was chosen as the Woman of the Year 1982, the year in which the Falklands War began under her command and resulted in the British victory.

In contrast to her relatively poor average approval rating as prime minister, Thatcher has since ranked highly in retrospective opinion polling and, according to YouGov, she is "see[n] in overall positive terms" by the British public. She was voted the fourth-greatest British prime minister of the 20th century in a poll of 139 academics organised by MORI.

According to theatre critic Michael Billington, Thatcher left an "emphatic mark" on the arts while Prime Minister. One of the earliest satires of Thatcher as prime minister involved satirist John Wells (as writer and performer), actress Janet Brown (voicing Thatcher) and future "Spitting Image" producer John Lloyd (as co-producer), who in 1979 were teamed up by producer Martin Lewis for the satirical audio album "The Iron Lady", which consisted of skits and songs satirising Thatcher's rise to power. The album was released in September 1979. Thatcher was heavily satirised on "Spitting Image", and "The Independent" labelled her "every stand-up's dream".

Thatcher was the subject or the inspiration for 1980s protest songs. Musicians Billy Bragg and Paul Weller helped to form the Red Wedge collective to support Labour in opposition to Thatcher. Known as "Maggie" by supporters and opponents alike, the chant song "Maggie Out" became a signature rallying cry among the left during the latter half of her premiership.

Thatcher was parodied by Wells in several media. He collaborated with Richard Ingrams on the spoof "Dear Bill" letters, which ran as a column in "Private Eye" magazine; they were also published in book form and became a West End stage revue titled "Anyone for Denis?", with Wells in the role of Denis Thatcher. It was followed by a 1982 TV special directed by Dick Clement, in which Thatcher was played by Angela Thorne.

Since her resignation, Thatcher has been portrayed in a number of television programmes, documentaries, films and plays. She was portrayed by Patricia Hodge in Ian Curteis's long unproduced "The Falklands Play" (2002) and by Andrea Riseborough in the TV film "The Long Walk to Finchley" (2008). She is the protagonist in two films, played by Lindsay Duncan in "Margaret" (2009) and by Meryl Streep in "The Iron Lady" (2011), in which she is depicted as suffering from dementia or Alzheimer's disease. She will be a main character in the fourth season of Netflix series "The Crown", played by Gillian Anderson.

Thatcher became a privy councillor (PC) on becoming a secretary of state in 1970. She was the first woman entitled to full membership rights as an honorary member of the Carlton Club on becoming Leader of the Conservative Party in 1975.

As prime minister, Thatcher received two honorary distinctions:
Two weeks after her resignation, Thatcher was appointed Member of the Order of Merit (OM) by the Queen. Her husband Denis was made a hereditary baronet at the same time. As his wife, Thatcher was entitled to use the honorific style "Lady", an automatically conferred title that she declined to use. She became Lady Thatcher in her own right on her ennoblement in the House of Lords.

In the Falklands, Margaret Thatcher Day has been marked each 10 January since 1992, commemorating her first visit to the Islands in January 1983, six months after the end of the Falklands War in June 1982.

Thatcher became a member of the Lords in 1992 with a life peerage as Baroness Thatcher, of Kesteven in the County of Lincolnshire. Subsequently, the College of Arms granted her usage of a personal coat of arms; she was allowed to revise these arms on her appointment as Lady of the Order of the Garter (LG) in 1995, the highest order of chivalry for women.

In the US, Thatcher received the Ronald Reagan Freedom Award, and was later designated Patron of The Heritage Foundation in 2006, where she established the Margaret Thatcher Center for Freedom.




 


</doc>
<doc id="19833" url="https://en.wikipedia.org/wiki?curid=19833" title="Metastability">
Metastability

In physics, metastability is a stable state of a dynamical system other than the system's state of least energy.
A ball resting in a hollow on a slope is a simple example of metastability. If the ball is only slightly pushed, it will settle back into its hollow, but a stronger push may start the ball rolling down the slope. Bowling pins show similar metastability by either merely wobbling for a moment or tipping over completely. A common example of metastability in science is isomerisation. Higher energy isomers are long lived because they are prevented from rearranging to their preferred ground state by (possibly large) barriers in the potential energy.

During a metastable state of finite lifetime, all state-describing parameters reach and hold stationary values. In isolation:

The metastability concept originated in the physics of first-order phase transitions. It then acquired new meaning in the study of aggregated subatomic particles (in atomic nuclei or in atoms) or in molecules, macromolecules or clusters of atoms and molecules. Later, it was borrowed for the study of decision-making and information transmission systems.

Many complex natural and man-made systems can demonstrate metastability.

Non-equilibrium thermodynamics is a branch of physics that studies the dynamics of statistical ensembles of molecules via unstable states. Being "stuck" in a thermodynamic trough without being at the lowest energy state is known as having kinetic stability or being kinetically persistent. The particular motion or kinetics of the atoms involved has resulted in getting stuck, despite there being preferable (lower-energy) alternatives.

Metastable states of matter (also referred as metastates) range from melting solids (or freezing liquids), boiling liquids (or condensing gases) and sublimating solids to supercooled liquids or superheated liquid-gas mixtures. Extremely pure, supercooled water stays liquid below 0 °C and remains so until applied vibrations or condensing seed doping initiates crystallization centers. This is a common situation for the droplets of atmospheric clouds.

Metastable phases are common in condensed matter and crystallography. Notably, this is the case for anatase, a metastable polymorph of titanium dioxide, which despite commonly being the first phase to form in many synthesis processes due to its lower surface energy, is always metastable, with rutile being the most stable phase at all temperatures and pressures .
As another example, diamond is a stable phase only at very high pressures, but is a metastable form of carbon at standard temperature and pressure. It can be converted to graphite (plus leftover kinetic energy), but only after overcoming an activation energy – an intervening hill. Martensite is a metastable phase used to control the hardness of most steel. Metastable polymorphs of silica are commonly observed. In some cases, such as in the allotropes of solid boron, acquiring a sample of the stable phase is difficult. 

The bonds between the building blocks of polymers such as DNA, RNA, and proteins are also metastable. Adenosine triphosphate is a highly metastable molecule, colloquially described as being "full of energy" that can be used in many ways in biology.

Generally speaking, emulsions/colloidal systems and glasses are metastable e.g. the metastability of silica glass is characterised by lifetimes of the order of 10 years compared with the lifetime of the Universe which is about 14·10 years. 

Sandpiles are one system which can exhibit metastability if a steep slope or tunnel is present. Sand grains form a pile due to friction. It is possible for an entire large sand pile to reach a point where it is stable, but the addition of a single grain causes large parts of it to collapse.

The avalanche is a well-known problem with large piles of snow and ice crystals on steep slopes. In dry conditions, snow slopes act similarly to sandpiles. An entire mountainside of snow can suddenly slide due to the presence of a skier, or even a loud noise or vibration.

Aggregated systems of subatomic particles described by quantum mechanics (quarks inside nucleons, nucleons inside atomic nuclei, electrons inside atoms, molecules, or atomic clusters) are found to have many distinguishable states. Of these, one (or a small degenerate set) is indefinitely stable: the ground state or global minimum.

All other states besides the ground state (or those degenerate with it) have higher energies. Of all these other states, the metastable states are the ones having lifetimes lasting at least 10 to 10 times longer than the shortest lived states of the set.

A "metastable state" is then long-lived (locally stable with respect to configurations of 'neighbouring' energies) but not eternal (as the global minimum is). Being excited – of an energy above the ground state – it will eventually decay to a more stable state, releasing energy. Indeed, above absolute zero, all states of a system have a non-zero probability to decay; that is, to spontaneously fall into another state (usually lower in energy). One mechanism for this to happen is through tunnelling.

Some energetic states of an atomic nucleus (having distinct spatial mass, charge, spin, isospin distributions) are much longer-lived than other (nuclear isomers of the same isotope), e.g. technetium-99m. The isotope tantalum-180m, although being a metastable excited state, is long-lived enough that it has never been observed to decay, with a half-life calculated to be least years, over 3 million times the current age of the universe.

Some atomic energy levels are metastable. Rydberg atoms are an example of metastable excited atomic states. Transitions from metastable excited levels are typically those forbidden by electric dipole selection rules. This means that any transitions from this level are relatively unlikely to occur. In a sense, an electron that happens to find itself in a metastable configuration is trapped there. Of course, since transitions from a metastable state are not impossible (merely less likely), the electron will eventually decay to a less energetic state, typically by an electric quadrupole transition, or often by non-radiative de-excitation (e.g., collisional de-excitation).

This slow-decay property of a metastable state is apparent in phosphorescence, the kind of photoluminescence seen in glow-in-the-dark toys that can be charged by first being exposed to bright light. Whereas spontaneous emission in atoms has a typical timescale on the order of 10 seconds, the decay of metastable states can typically take milliseconds to minutes, and so light emitted in phosphorescence is usually both weak and long-lasting.

In chemical systems, a system of atoms or molecules involving a change in chemical bond can be in a metastable state, which lasts for a relatively long period of time. Molecular vibrations and thermal motion make chemical species at the energetic equivalent of the top of a round hill very short-lived. Metastable states that persist for many seconds (or years) are found in energetic "valleys" which are not the lowest possible valley (point 1 in illustration). A common type of metastability is isomerism.

The stability or metastability of a given chemical system depends on its environment, particularly temperature and pressure. The difference between producing a stable vs. metastable entity can have important consequences. For instances, having the wrong crystal polymorph can result in failure of a drug while in storage between manufacture and administration. The map of which state is the most stable as a function of pressure, temperature and/or composition is known as a phase diagram. In regions where a particular state is not the most stable, it may still be metastable.
Reaction intermediates are relatively short-lived, and are usually thermodynamically unstable rather than metastable. The IUPAC recommends referring to these as "transient" rather than metastable.

Metastability is also used to refer to specific situations in mass spectrometry and spectrochemistry.

A digital circuit is supposed to be found in a small number of stable digital states within a certain amount of time after an input change. However if an input changes at the wrong moment a digital circuit which employs feedback (even a simple circuit such as a flip-flop) can enter a metastable state and take an unbounded length of time to finally settle into a fully stable digital state.

Metastability in the brain is a phenomenon studied in computational neuroscience to elucidate how the human brain recognizes patterns. Here, the term metastability is used rather loosely. There is no lower-energy state, but there are semi-transient signals in the brain that persist for a while and are different than the usual equilibrium state.



</doc>
<doc id="19834" url="https://en.wikipedia.org/wiki?curid=19834" title="Mary Wollstonecraft">
Mary Wollstonecraft

Mary Wollstonecraft (, ; 27 April 1759 – 10 September 1797) was an English writer, philosopher, and advocate of women's rights. Until the late 20th century, Wollstonecraft's life, which encompassed several unconventional personal relationships at the time, received more attention than her writing. Today Wollstonecraft is regarded as one of the founding feminist philosophers, and feminists often cite both her life and her works as important influences.

During her brief career, she wrote novels, treatises, a travel narrative, a history of the French Revolution, a conduct book, and a children's book. Wollstonecraft is best known for "A Vindication of the Rights of Woman" (1792), in which she argues that women are not naturally inferior to men, but appear to be only because they lack education. She suggests that both men and women should be treated as rational beings and imagines a social order founded on reason.

After Wollstonecraft's death, her widower published a "Memoir" (1798) of her life, revealing her unorthodox lifestyle, which inadvertently destroyed her reputation for almost a century. However, with the emergence of the feminist movement at the turn of the twentieth century, Wollstonecraft's advocacy of women's equality and critiques of conventional femininity became increasingly important.

After two ill-fated affairs, with Henry Fuseli and Gilbert Imlay (by whom she had a daughter, Fanny Imlay), Wollstonecraft married the philosopher William Godwin, one of the forefathers of the anarchist movement. Wollstonecraft died at the age of 38 leaving behind several unfinished manuscripts. She died eleven days after giving birth to her second daughter, Mary Shelley, who would become an accomplished writer and author of "Frankenstein".

Wollstonecraft was born on 27 April 1759 in Spitalfields, London. She was the second of the seven children of Elizabeth Dixon and Edward John Wollstonecraft. Although her family had a comfortable income when she was a child, her father gradually squandered it on speculative projects. Consequently, the family became financially unstable and they were frequently forced to move during Wollstonecraft's youth. The family's financial situation eventually became so dire that Wollstonecraft's father compelled her to turn over money that she would have inherited at her maturity. Moreover, he was apparently a violent man who would beat his wife in drunken rages. As a teenager, Wollstonecraft used to lie outside the door of her mother's bedroom to protect her. Wollstonecraft played a similar maternal role for her sisters, Everina and Eliza, throughout her life. For example, in a defining moment in 1784, she convinced Eliza, who was suffering from what was probably postpartum depression, to leave her husband and infant; Wollstonecraft made all of the arrangements for Eliza to flee, demonstrating her willingness to challenge social norms. The human costs, however, were severe: her sister suffered social condemnation and, because she could not remarry, was doomed to a life of poverty and hard work.

Two friendships shaped Wollstonecraft's early life. The first was with Jane Arden in Beverley. The two frequently read books together and attended lectures presented by Arden's father, a self-styled philosopher and scientist. Wollstonecraft revelled in the intellectual atmosphere of the Arden household and valued her friendship with Arden greatly, sometimes to the point of being emotionally possessive. Wollstonecraft wrote to her: "I have formed romantic notions of friendship ... I am a little singular in my thoughts of love and friendship; I must have the first place or none." In some of Wollstonecraft's letters to Arden, she reveals the volatile and depressive emotions that would haunt her throughout her life. The second and more important friendship was with Fanny (Frances) Blood, introduced to Wollstonecraft by the Clares, a couple in Hoxton who became parental figures to her; Wollstonecraft credited Blood with opening her mind.

Unhappy with her home life, Wollstonecraft struck out on her own in 1778 and accepted a job as a lady's companion to Sarah Dawson, a widow living in Bath. However, Wollstonecraft had trouble getting along with the irascible woman (an experience she drew on when describing the drawbacks of such a position in "Thoughts on the Education of Daughters", 1787). In 1780 she returned home upon being called back to care for her dying mother. Rather than return to Dawson's employ after the death of her mother, Wollstonecraft moved in with the Bloods. She realized during the two years she spent with the family that she had idealized Blood, who was more invested in traditional feminine values than was Wollstonecraft. But Wollstonecraft remained dedicated to Fanny and her family throughout her life (she frequently gave pecuniary assistance to Blood's brother, for example).

Wollstonecraft had envisioned living in a female utopia with Blood; they made plans to rent rooms together and support each other emotionally and financially, but this dream collapsed under economic realities. In order to make a living, Wollstonecraft, her sisters, and Blood set up a school together in Newington Green, a Dissenting community. Blood soon became engaged and after their marriage her husband, Hugh Skeys, took her to Lisbon, Portugal, to improve her health, which had always been precarious. Despite the change of surroundings Blood's health further deteriorated when she became pregnant, and in 1785 Wollstonecraft left the school and followed Blood to nurse her, but to no avail. Moreover, her abandonment of the school led to its failure. Blood's death devastated Wollstonecraft and was part of the inspiration for her first novel, "" (1788).

After Blood's death in 1785, Wollstonecraft's friends helped her obtain a position as governess to the daughters of the Anglo-Irish Kingsborough family in Ireland. Although she could not get along with Lady Kingsborough, the children found her an inspiring instructor; Margaret King would later say she 'had freed her mind from all superstitions'. Some of Wollstonecraft's experiences during this year would make their way into her only children's book, "Original Stories from Real Life" (1788).

Frustrated by the limited career options open to respectable yet poor women—an impediment which Wollstonecraft eloquently describes in the chapter of "Thoughts on the Education of Daughters" entitled 'Unfortunate Situation of Females, Fashionably Educated, and Left Without a Fortune'—she decided, after only a year as a governess, to embark upon a career as an author. This was a radical choice, since, at the time, few women could support themselves by writing. As she wrote to her sister Everina in 1787, she was trying to become 'the first of a new genus'. She moved to London and, assisted by the liberal publisher Joseph Johnson, found a place to live and work to support herself. She learned French and German and translated texts, most notably "Of the Importance of Religious Opinions" by Jacques Necker and "Elements of Morality, for the Use of Children" by Christian Gotthilf Salzmann. She also wrote reviews, primarily of novels, for Johnson's periodical, the "Analytical Review". Wollstonecraft's intellectual universe expanded during this time, not only from the reading that she did for her reviews but also from the company she kept: she attended Johnson's famous dinners and met such luminaries as the radical pamphleteer Thomas Paine and the philosopher William Godwin. The first time Godwin and Wollstonecraft met, they were disappointed in each other. Godwin had come to hear Paine, but Wollstonecraft assailed him all night long, disagreeing with him on nearly every subject. Johnson himself, however, became much more than a friend; she described him in her letters as a father and a brother.

In London, Wollstonecraft lived on Dolben Street, in Southwark; an up and coming area following the opening of the first Blackfriars Bridge in 1769.

While in London, Wollstonecraft pursued a relationship with the artist Henry Fuseli, even though he was already married. She was, she wrote, enraptured by his genius, 'the grandeur of his soul, that quickness of comprehension, and lovely sympathy'. She proposed a platonic living arrangement with Fuseli and his wife, but Fuseli's wife was appalled, and he broke off the relationship with Wollstonecraft. After Fuseli's rejection, Wollstonecraft decided to travel to France to escape the humiliation of the incident, and to participate in the revolutionary events that she had just celebrated in her recent "Vindication of the Rights of Men" (1790). She had written the "Rights of Men" in response to the Whig MP Edmund Burke's politically conservative critique of the French Revolution in "Reflections on the Revolution in France" (1790) and it made her famous overnight. "Reflections on the Revolution in France" was published on 1 November 1790, and so angered Wollstonecraft that she spent the rest of the month writing her rebuttal. "A Vindication of the Rights of Men, in a Letter to the Right Honourable Edmund Burke" was published on 29 November 1790, initially anonymously; the second edition of "A Vindication of the Rights of Men" was published on 18 December, and this time the publisher revealed Wollstonecraft as the author.

Wollstonecraft called the French Revolution a 'glorious "chance" to obtain more virtue and happiness than hitherto blessed our globe'. Against Burke's dismissal of the Third Estate as men of no account, Wollstonecraft wrote, 'Time may show, that this obscure throng knew more of the human heart and of legislation than the profligates of rank, emasculated by hereditary effeminacy'. About the events of 5–6 October 1789, when the royal family was marched from Versailles to Paris by a group of angry housewives, Burke praised Queen Marie Antoinette as a symbol of the refined elegance of the "ancien régime", who was surrounded by 'furies from hell, in the abused shape of the vilest of women'. Wollstonecraft by contrast wrote of the same event: 'Probably you [Burke] mean women who gained a livelihood by selling vegetables or fish, who never had any advantages of education'.

Wollstonecraft was compared with such leading lights as the theologian and controversialist Joseph Priestley and Paine, whose "Rights of Man" (1791) would prove to be the most popular of the responses to Burke. She pursued the ideas she had outlined in "Rights of Men" in "A Vindication of the Rights of Woman" (1792), her most famous and influential work. Wollstonecraft's fame extended across the English channel, for when the French statesmen Charles Maurice de Talleyrand-Périgord visited London in 1792, he visited her, during which she asked that French girls be given the same right to an education that French boys were being offered by the new regime in France.

Wollstonecraft left for Paris in December 1792 and arrived about a month before Louis XVI was guillotined. Britain and France were on the brink of war when she left for Paris, and many advised her not to go. France was in turmoil. She sought out other British visitors such as Helen Maria Williams and joined the circle of expatriates then in the city. During her time in Paris, Wollstonecraft associated mostly with the moderate Girondins rather than the more radical Jacobins.

On 26 December 1792, Wollstonecraft saw the former king, Louis XVI, being taken to be tried before the National Assembly, and much to her own surprise, found 'the tears flow[ing] insensibly from my eyes, when I saw Louis sitting, with more dignity than I expected from his character, in a hackney coach going to meet death, where so many of his race have triumphed'.

France declared war on Britain in February 1793. Wollstonecraft tried to leave France for Switzerland but was denied permission. In March, the Jacobin-dominated Committee of Public Safety came to power, instituting a totalitarian regime meant to mobilise France for the first 'total war'.

Life became very difficult for foreigners in France. At first, they were put under police surveillance and, to get a residency permit, had to produce six written statements from Frenchmen testifying to their loyalty to the republic. Then, on 12 April 1793, all foreigners were forbidden to leave France. Despite her sympathy for the revolution, life for Wollstonecraft become very uncomfortable, all the more so as the Girondins had lost out to the Jacobins. Some of Wollstonecraft's French friends lost their heads to the guillotine as the Jacobins set out to annihilate their enemies.

Having just written the "Rights of Woman", Wollstonecraft was determined to put her ideas to the test, and in the stimulating intellectual atmosphere of the French Revolution, she attempted her most experimental romantic attachment yet: she met and fell passionately in love with Gilbert Imlay, an American adventurer. Wollstonecraft put her own principles in practice by sleeping with Imlay even though they were not married, which was unacceptable behavior from a 'respectable' British woman. Whether or not she was interested in marriage, he was not, and she appears to have fallen in love with an idealisation of the man. Despite her rejection of the sexual component of relationships in the "Rights of Woman", Wollstonecraft discovered that Imlay awakened her interest in sex.

Wollstonecraft was to a certain extent disillusioned by what she saw in France, writing that the people under the republic still behaved slavishly to those who held power while the government remained 'venal' and 'brutal'. Despite her disenchantment, Wollstonecraft wrote:

I cannot yet give up the hope, that a fairer day is dawning on Europe, though I must hesitatingly observe, that little is to be expected from the narrow principle of commerce, which seems everywhere to be shoving aside "the point of honour" of the "noblesse" [nobility]. For the same pride of office, the same desire of power are still visible; with this aggravation, that, fearing to return to obscurity, after having but just acquired a relish for distinction, each hero, or philosopher, for all are dubbed with these new titles, endeavors to make hay while the sun shines.

Wollstonecraft was offended by the Jacobins' treatment of women. They refused to grant women equal rights, denounced 'Amazons', and made it clear that women were supposed to conform to Jean-Jacques Rousseau's ideal of helpers to men. On 16 October 1793, Marie Antoinette was guillotined; among her charges and convictions, she was found guilty of committing incest with her son. Though Wollstonecraft disliked the former queen, she was troubled that the Jacobins would make Marie Antoinette's alleged perverse sexual acts one of the central reasons for the French people to hate her.

As the daily arrests and executions of the Reign of Terror began, Wollstonecraft came under suspicion. She was, after all, a British citizen known to be a friend of leading Girondins. On 31 October 1793, most of the Girondin leaders were guillotined; when Imlay broke the news to Wollstonecraft, she fainted. By this time, Imlay was taking advantage of the British blockade of France, which had caused shortages and worsened ever-growing inflation, by chartering ships to bring food and soap from America and dodge the British Royal Navy, goods that he could sell at a premium to Frenchmen who still had money. Imlay's blockade-running gained the respect and support of some Jacobins, ensuring, as he had hoped, his freedom during the Terror. To protect Wollstonecraft from arrest, Imlay made a false statement to the U.S. embassy in Paris that he had married her, automatically making her an American citizen. Some of her friends were not so lucky; many, like Thomas Paine, were arrested, and some were even guillotined. Her sisters believed she had been imprisoned.

Wollstonecraft called life under the Jacobins 'nightmarish'. There were gigantic daytime parades requiring everyone to show themselves and lustily cheer lest they be suspected of inadequate commitment to the republic, as well as nighttime police raids to arrest 'enemies of the republic'. In a March 1794 letter to her sister Everina, Wollstonecraft wrote:

It is impossible for you to have any idea of the impression the sad scenes I have been a witness to have left on my mind ... death and misery, in every shape of terrour, haunts this devoted country—I certainly am glad that I came to France, because I never could have had else a just opinion of the most extraordinary event that has ever been recorded.

Wollstonecraft soon became pregnant by Imlay, and on 14 May 1794 she gave birth to her first child, Fanny, naming her after perhaps her closest friend. Wollstonecraft was overjoyed; she wrote to a friend, 'My little Girl begins to suck so MANFULLY that her father reckons saucily on her writing the second part of the R[igh]ts of Woman' (emphasis hers). She continued to write avidly, despite not only her pregnancy and the burdens of being a new mother alone in a foreign country, but also the growing tumult of the French Revolution. While at Le Havre in northern France, she wrote a history of the early revolution, "An Historical and Moral View of the French Revolution", which was published in London in December 1794. Imlay, unhappy with the domestic-minded and maternal Wollstonecraft, eventually left her. He promised that he would return to her and Fanny at Le Havre, but his delays in writing to her and his long absences convinced Wollstonecraft that he had found another woman. Her letters to him are full of needy expostulations, which most critics explain as the expressions of a deeply depressed woman, while others say they resulted from her circumstances—a foreign woman alone with an infant in the middle of a revolution that had seen good friends imprisoned or executed.

In July 1794, Wollstonecraft welcomed the fall of the Jacobins, predicting it would be followed with a restoration of freedom of the press in France, which led her to return to Paris. In August 1794, Imlay departed for London and promised to return soon. In 1793, the British government had begun a crackdown on radicals, suspending civil liberties, imposing drastic censorship, and trying for treason anyone suspected of sympathy with the revolution, which led Wollstonecraft to fear she would be imprisoned if she returned.

The winter of 1794–95 was the coldest winter in Europe for over a century, which reduced Wollstonecraft and her daughter Fanny to desperate circumstances. The river Seine froze that winter, which made it impossible for ships to bring food and coal to Paris, leading to widespread starvation and deaths from the cold in the city. Wollstonecraft continued to write to Imlay, asking him to return to France at once, declaring she still had faith in the revolution and did not wish to return to Britain. After she left France on 7 April 1795, she continued to refer to herself as 'Mrs Imlay', even to her sisters, in order to bestow legitimacy upon her child.

The British historian Tom Furniss called "An Historical and Moral View of the French Revolution" the most neglected of Wollstonecraft's books. It was first published in London in 1794, but a second edition did not appear until 1989. Later generations were more interested in her feminist writings than in her account of the French Revolution, which Furniss has called her 'best work'. Wollstonecraft was not trained as a historian, but she used all sorts of journals, letters and documents recounting how ordinary people in France reacted to the revolution. She was trying to counteract what Furniss called the 'hysterical' anti-revolutionary mood in Britain, which depicted the revolution as due to the entire French nation's going mad. Wollstonecraft argued instead that the revolution arose from a set of social, economic and political conditions that left no other way out of the crisis that gripped France in 1789.

"An Historical and Moral View of the French Revolution" was a difficult balancing act for Wollstonecraft. She condemned the Jacobin regime and the Reign of Terror, but at same time she argued that the revolution was a great achievement, which led her to stop her history in late 1789 rather than write about the Terror of 1793–94. Edmund Burke had ended his "Reflections on the Revolution in France" with reference to the events of 5–6 October 1789, when a group of women from Paris forced the French royal family from the Palace of Versailles to Paris. Burke called the women 'furies from hell', while Wollstonecraft defended them as ordinary housewives angry about the lack of bread to feed their families. Against Burke's idealised portrait of Marie Antoinette as a noble victim of a mob, Wollstonecraft portrayed the queen as a "femme fatale", a seductive, scheming and dangerous woman. Wollstonecraft argued that the values of the aristocracy corrupted women in a monarchy because women's main purpose in such a society was to bear sons to continue a dynasty, which essentially reduced a woman's value to only her womb. Moreover, Wollstonecraft pointed out that unless a queen was a queen regnant, most queens were queen consorts, which meant a woman had to exercise influence via her husband or son, encouraging her to become more and more manipulative. Wollstonecraft argued that aristocratic values, by emphasising a woman's body and her ability to be charming over her mind and character, had encouraged women like Marie Antoinette to be manipulative and ruthless, making the queen into a corrupted and corrupting product of the "ancien régime".

Seeking Imlay, Wollstonecraft returned to London in April 1795, but he rejected her. In May 1795 she attempted to commit suicide, probably with laudanum, but Imlay saved her life (although it is unclear how). In a last attempt to win back Imlay, she embarked upon some business negotiations for him in Scandinavia, trying to recoup some of his losses. Wollstonecraft undertook this hazardous trip with only her young daughter and a maid. She recounted her travels and thoughts in letters to Imlay, many of which were eventually published as "Letters Written During a Short Residence in Sweden, Norway, and Denmark" in 1796. When she returned to England and came to the full realization that her relationship with Imlay was over, she attempted suicide for the second time, leaving a note for Imlay:

She then went out on a rainy night and "to make her clothes heavy with water, she walked up and down about half an hour" before jumping into the River Thames, but a stranger saw her jump and rescued her. Wollstonecraft considered her suicide attempt deeply rational, writing after her rescue, I have only to lament, that, when the bitterness of death was past, I was inhumanly brought back to life and misery. But a fixed determination is not to be baffled by disappointment; nor will I allow that to be a frantic attempt, which was one of the calmest acts of reason. In this respect, I am only accountable to myself. Did I care for what is termed reputation, it is by other circumstances that I should be dishonoured.

Gradually, Wollstonecraft returned to her literary life, becoming involved with Joseph Johnson's circle again, in particular with Mary Hays, Elizabeth Inchbald, and Sarah Siddons through William Godwin. Godwin and Wollstonecraft's unique courtship began slowly, but it eventually became a passionate love affair. Godwin had read her "Letters Written in Sweden, Norway, and Denmark" and later wrote that "If ever there was a book calculated to make a man in love with its author, this appears to me to be the book. She speaks of her sorrows, in a way that fills us with melancholy, and dissolves us in tenderness, at the same time that she displays a genius which commands all our admiration." Once Wollstonecraft became pregnant, they decided to marry so that their child would be legitimate. Their marriage revealed the fact that Wollstonecraft had never been married to Imlay, and as a result she and Godwin lost many friends. Godwin received further criticism because he had advocated the abolition of marriage in his philosophical treatise "Political Justice". After their marriage on 29 March 1797, Godwin and Wollstonecraft moved to 29 The Polygon, Somers Town. Godwin rented an apartment 20 doors away at 17 Evesham Buildings in Chalton Street as a study, so that they could both still retain their independence; they often communicated by letter. By all accounts, theirs was a happy and stable, though brief, relationship.

On 30 August 1797, Wollstonecraft gave birth to her second daughter, Mary. Although the delivery seemed to go well initially, the placenta broke apart during the birth and became infected; childbed fever was a common and often fatal occurrence in the eighteenth century. After several days of agony, Wollstonecraft died of septicaemia on 10 September. Godwin was devastated: he wrote to his friend Thomas Holcroft, "I firmly believe there does not exist her equal in the world. I know from experience we were formed to make each other happy. I have not the least expectation that I can now ever know happiness again." She was buried at Old Saint Pancras Churchyard, where her tombstone reads, "Mary Wollstonecraft Godwin, Author of "A Vindication of the Rights of Woman": Born 27 April 1759: Died 10 September 1797."

In January 1798 Godwin published his "Memoirs of the Author of A Vindication of the Rights of Woman". Although Godwin felt that he was portraying his wife with love, compassion, and sincerity, many readers were shocked that he would reveal Wollstonecraft's illegitimate children, love affairs, and suicide attempts. The Romantic poet Robert Southey accused him of "the want of all feeling in stripping his dead wife naked" and vicious satires such as "The Unsex'd Females" were published. Godwin's "Memoirs" portrays Wollstonecraft as a woman deeply invested in feeling who was balanced by his reason and as more of a religious sceptic than her own writings suggest. Godwin's views of Wollstonecraft were perpetuated throughout the nineteenth century and resulted in poems such as "Wollstonecraft and Fuseli" by British poet Robert Browning and that by William Roscoe which includes the lines:
<poem>
</poem>

In 1851, Wollstonecraft's remains were moved by her grandson Percy Florence Shelley to his family tomb in St Peter's Church, Bournemouth. Her monument in the churchyard lies to the north-east of the church just north of Sir John Soane's grave. Her husband was buried with her on his death in 1836, as was his second wife, Mary Jane Godwin (1766–1841).

Wollstonecraft has what scholar Cora Kaplan labelled in 2002 a "curious" legacy that has evolved over time: "for an author-activist adept in many genres ... up until the last quarter-century Wollstonecraft's life has been read much more closely than her writing". After the devastating effect of Godwin's "Memoirs", Wollstonecraft's reputation lay in tatters for nearly a century; she was pilloried by such writers as Maria Edgeworth, who patterned the "freakish" Harriet Freke in "Belinda" (1801) after her. Other novelists such as Mary Hays, Charlotte Turner Smith, Fanny Burney, and Jane West created similar figures, all to teach a "moral lesson" to their readers. (Hays had been a close friend, and helped nurse her in her dying days.)

In contrast, there was one writer of the generation after Wollstonecraft who apparently did not share the judgmental views of her contemporaries. Jane Austen never mentioned the earlier woman by name, but several of her novels contain positive allusions to Wollstonecraft's work. The American literary scholar Anne K. Mellor notes several examples. In "Pride and Prejudice", Mr Wickham seems to be based upon the sort of man Wollstonecraft claimed that standing armies produce, while the sarcastic remarks of protagonist Elizabeth Bennet about "female accomplishments" closely echo Wollstonecraft's condemnation of these activities. The balance a woman must strike between feelings and reason in "Sense and Sensibility" follows what Wollstonecraft recommended in her novel "Mary", while the moral equivalence Austen drew in "Mansfield Park" between slavery and the treatment of women in society back home tracks one of Wollstonecraft's favorite arguments. In "Persuasion", Austen's characterization of Anne Eliot (as well as her late mother before her) as better qualified than her father to manage the family estate also echoes a Wollstonecraft thesis.

Scholar Virginia Sapiro states that few read Wollstonecraft's works during the nineteenth century as "her attackers implied or stated that no self-respecting woman would read her work". (Still, as Craciun points out, new editions of "Rights of Woman" appeared in the UK in the 1840s and in the US in the 1830s, 1840s, and 1850s.) If readers were few, then "many" were inspired; one such reader was Elizabeth Barrett Browning, who read "Rights of Woman" at age 12 and whose poem "Aurora Leigh" reflected Wollstonecraft's unwavering focus on education. Lucretia Mott, a Quaker minister, and Elizabeth Cady Stanton, Americans who met in 1840 at the World Anti-Slavery Convention in London, discovered they both had read Wollstonecraft, and they agreed upon the need for (what became) the Seneca Falls Convention, an influential women's rights meeting held in 1848. Another woman who read Wollstonecraft was George Eliot, a prolific writer of reviews, articles, novels, and translations. In 1855, she devoted an essay to the roles and rights of women, comparing Wollstonecraft and Margaret Fuller. Fuller was an American journalist, critic, and women's rights activist who, like Wollstonecraft, had travelled to the Continent and had been involved in the struggle for reform (in this case the Roman Republic)—and she had a child by a man without marrying him. Wollstonecraft's children's tales were adapted by Charlotte Mary Yonge in 1870.

Wollstonecraft's work was exhumed with the rise of the movement to give women a political voice. First was an attempt at rehabilitation in 1879 with the publication of Wollstonecraft's "Letters to Imlay, with prefatory memoir" by Charles Kegan Paul. Then followed the first full-length biography, which was by Elizabeth Robins Pennell; it appeared in 1884 as part of a series by the Roberts Brothers on famous women. Millicent Garrett Fawcett, a suffragist and later president of the National Union of Women's Suffrage Societies, wrote the introduction to the centenary edition (i.e. 1892) of the "Rights of Woman"; it cleansed the memory of Wollstonecraft and claimed her as the foremother of the struggle for the vote. By 1898, Wollstonecraft was the subject of a first doctoral thesis and its resulting book.

With the advent of the modern feminist movement, women as politically dissimilar from each other as Virginia Woolf and Emma Goldman embraced Wollstonecraft's life story. By 1929 Woolf described Wollstonecraft—her writing, arguments, and "experiments in living"—as immortal: "she is alive and active, she argues and experiments, we hear her voice and trace her influence even now among the living". Others, however, continued to decry Wollstonecraft's lifestyle. A biography published in 1932 refers to recent reprints of her works, incorporating new research, and to a "study" in 1911, a play in 1922, and another biography in 1924. Interest in her never completely died, with full-length biographies in 1937 and 1951.

With the emergence of feminist criticism in academia in the 1960s and 1970s, Wollstonecraft's works returned to prominence. Their fortunes reflected that of the second wave of the North American feminist movement itself; for example, in the early 1970s, six major biographies of Wollstonecraft were published that presented her "passionate life in apposition to [her] radical and rationalist agenda". The feminist artwork "The Dinner Party", first exhibited in 1979, features a place setting for Wollstonecraft. In the 1980s and 1990s, yet another image of Wollstonecraft emerged, one which described her as much more a creature of her time; scholars such as Claudia Johnson, Gary Kelly, and Virginia Sapiro demonstrated the continuity between Wollstonecraft's thought and other important eighteenth-century ideas regarding topics such as sensibility, economics, and political theory.

Wollstonecraft's work has also had an effect on feminism outside the academy in recent years. Ayaan Hirsi Ali, a political writer and former Muslim who is critical of Islam in general and its dictates regarding women in particular, cited the "Rights of Woman" in her autobiography "Infidel" and wrote that she was "inspired by Mary Wollstonecraft, the pioneering feminist thinker who told women they had the same ability to reason as men did and deserved the same rights". British writer Caitlin Moran, author of the best-selling "How to Be a Woman", described herself as "half Wollstonecraft" to the "New Yorker". She has also inspired more widely. Nobel Laureate Amartya Sen, the Indian economist and philosopher who first identified the missing women of Asia, draws repeatedly on Wollstonecraft as a political philosopher in "The Idea of Justice".

Several plaques have been erected to honour Wollstonecraft.

The majority of Wollstonecraft's early productions are about education; she assembled an anthology of literary extracts "for the improvement of young women" entitled "The Female Reader" and she translated two children's works, Maria Geertruida van de Werken de Cambon's "Young Grandison" and Christian Gotthilf Salzmann's "Elements of Morality". Her own writings also addressed the topic. In both her conduct book "Thoughts on the Education of Daughters" (1787) and her children's book "Original Stories from Real Life" (1788), Wollstonecraft advocates educating children into the emerging middle-class ethos: self-discipline, honesty, frugality, and social contentment. Both books also emphasize the importance of teaching children to reason, revealing Wollstonecraft's intellectual debt to the educational views of seventeenth-century philosopher John Locke. However, the prominence she affords religious faith and innate feeling distinguishes her work from his and links it to the discourse of sensibility popular at the end of the eighteenth century. Both texts also advocate the education of women, a controversial topic at the time and one which she would return to throughout her career, most notably in "A Vindication of the Rights of Woman". Wollstonecraft argues that well-educated women will be good wives and mothers and ultimately contribute positively to the nation.

Published in response to Edmund Burke's "Reflections on the Revolution in France" (1790), which was a defence of constitutional monarchy, aristocracy, and the Church of England, and an attack on Wollstonecraft's friend, the Rev Richard Price at the Newington Green Unitarian Church, Wollstonecraft's "A Vindication of the Rights of Men" (1790) attacks aristocracy and advocates republicanism. Hers was the first response in a pamphlet war that subsequently became known as the "Revolution Controversy", in which Thomas Paine's "Rights of Man" (1792) became the rallying cry for reformers and radicals.

Wollstonecraft attacked not only monarchy and hereditary privilege but also the language that Burke used to defend and elevate it. In a famous passage in the "Reflections", Burke had lamented: "I had thought ten thousand swords must have leaped from their scabbards to avenge even a look that threatened her <nowiki>[</nowiki>Marie Antoinette<nowiki>]</nowiki> with insult.—But the age of chivalry is gone." Most of Burke's detractors deplored what they viewed as theatrical pity for the French queen—a pity they felt was at the expense of the people. Wollstonecraft was unique in her attack on Burke's gendered language. By redefining the sublime and the beautiful, terms first established by Burke himself in "A Philosophical Enquiry into the Origin of Our Ideas of the Sublime and Beautiful" (1756), she undermined his rhetoric as well as his argument. Burke had associated the beautiful with weakness and femininity and the sublime with strength and masculinity; Wollstonecraft turns these definitions against him, arguing that his theatrical "tableaux" turn Burke's readers—the citizens—into weak women who are swayed by show. In her first unabashedly feminist critique, which Wollstonecraft scholar Claudia L. Johnson argues remains unsurpassed in its argumentative force, Wollstonecraft indicts Burke's defence of an unequal society founded on the passivity of women.

In her arguments for republican virtue, Wollstonecraft invokes an emerging middle-class ethos in opposition to what she views as the vice-ridden aristocratic code of manners. Influenced by Enlightenment thinkers, she believed in progress and derides Burke for relying on tradition and custom. She argues for rationality, pointing out that Burke's system would lead to the continuation of slavery, simply because it had been an ancestral tradition. She describes an idyllic country life in which each family can have a farm that will just suit its needs. Wollstonecraft contrasts her utopian picture of society, drawn with what she says is genuine feeling, to Burke's false feeling.

The "Rights of Men" was Wollstonecraft's first overtly political work, as well as her first feminist work; as Johnson contends, "it seems that in the act of writing the later portions of "Rights of Men" she discovered the subject that would preoccupy her for the rest of her career." It was this text that made her a well-known writer.

"A Vindication of the Rights of Woman" is one of the earliest works of feminist philosophy. In it, Wollstonecraft argues that women ought to have an education commensurate with their position in society and then proceeds to redefine that position, claiming that women are essential to the nation because they educate its children and because they could be "companions" to their husbands rather than mere wives. Instead of viewing women as ornaments to society or property to be traded in marriage, Wollstonecraft maintains that they are human beings deserving of the same fundamental rights as men. Large sections of the "Rights of Woman" respond vitriolically to conduct book writers such as James Fordyce and John Gregory and educational philosophers such as Jean-Jacques Rousseau, who wanted to deny women an education. (Rousseau famously argues in "" (1762) that women should be educated for the pleasure of men.)

Wollstonecraft states that currently many women are silly and superficial (she refers to them, for example, as "spaniels" and "toys"), but argues that this is not because of an innate deficiency of mind but rather because men have denied them access to education. Wollstonecraft is intent on illustrating the limitations that women's deficient educations have placed on them; she writes: "Taught from their infancy that beauty is woman's sceptre, the mind shapes itself to the body, and, roaming round its gilt cage, only seeks to adorn its prison." She implies that, without the encouragement young women receive from an early age to focus their attention on beauty and outward accomplishments, women could achieve much more.

While Wollstonecraft does call for equality between the sexes in particular areas of life, such as morality, she does not explicitly state that men and women are equal. What she does claim is that men and women are equal in the eyes of God. However, such claims of equality stand in contrast to her statements respecting the superiority of masculine strength and valour. Wollstonecraft famously and ambiguously writes: "Let it not be concluded that I wish to invert the order of things; I have already granted, that, from the constitution of their bodies, men seem to be designed by Providence to attain a greater degree of virtue. I speak collectively of the whole sex; but I see not the shadow of a reason to conclude that their virtues should differ in respect to their nature. In fact, how can they, if virtue has only one eternal standard? I must therefore, if I reason consequently, as strenuously maintain that they have the same simple direction, as that there is a God." Her ambiguous statements regarding the equality of the sexes have since made it difficult to classify Wollstonecraft as a modern feminist, particularly since the word did not come into existence until the 1890s.

One of Wollstonecraft's most scathing critiques in the "Rights of Woman" is of false and excessive sensibility, particularly in women. She argues that women who succumb to sensibility are "blown about by every momentary gust of feeling" and because they are "the prey of their senses" they cannot think rationally. In fact, she claims, they do harm not only to themselves but to the entire civilization: these are not women who can help refine a civilization—a popular eighteenth-century idea—but women who will destroy it. Wollstonecraft does not argue that reason and feeling should act independently of each other; rather, she believes that they should inform each other.

In addition to her larger philosophical arguments, Wollstonecraft also lays out a specific educational plan. In the twelfth chapter of the "Rights of Woman", "On National Education", she argues that all children should be sent to a "country day school" as well as given some education at home "to inspire a love of home and domestic pleasures." She also maintains that schooling should be co-educational, arguing that men and women, whose marriages are "the cement of society", should be "educated after the same model."

Wollstonecraft addresses her text to the middle-class, which she describes as the "most natural state", and in many ways the "Rights of Woman" is inflected by a bourgeois view of the world. It encourages modesty and industry in its readers and attacks the uselessness of the aristocracy. But Wollstonecraft is not necessarily a friend to the poor; for example, in her national plan for education, she suggests that, after the age of nine, the poor, except for those who are brilliant, should be separated from the rich and taught in another school.

Both of Wollstonecraft's novels criticize what she viewed as the patriarchal institution of marriage and its deleterious effects on women. In her first novel, "Mary: A Fiction" (1788), the eponymous heroine is forced into a loveless marriage for economic reasons; she fulfils her desire for love and affection outside of marriage with two passionate romantic friendships, one with a woman and one with a man. "Maria: or, The Wrongs of Woman" (1798), an unfinished novel published posthumously and often considered Wollstonecraft's most radical feminist work, revolves around the story of a woman imprisoned in an insane asylum by her husband; like Mary, Maria also finds fulfilment outside of marriage, in an affair with a fellow inmate and a friendship with one of her keepers. Neither of Wollstonecraft's novels depict successful marriages, although she posits such relationships in the "Rights of Woman". At the end of "Mary", the heroine believes she is going "to that world where there is neither marrying, nor giving in marriage", presumably a positive state of affairs.

Both of Wollstonecraft's novels also critique the discourse of sensibility, a moral philosophy and aesthetic that had become popular at the end of the eighteenth century. "Mary" is itself a novel of sensibility and Wollstonecraft attempts to use the tropes of that genre to undermine sentimentalism itself, a philosophy she believed was damaging to women because it encouraged them to rely overmuch on their emotions. In "The Wrongs of Woman" the heroine's indulgence on romantic fantasies fostered by novels themselves is depicted as particularly detrimental.

Female friendships are central to both of Wollstonecraft's novels, but it is the friendship between Maria and Jemima, the servant charged with watching over her in the insane asylum, that is the most historically significant. This friendship, based on a sympathetic bond of motherhood, between an upper-class woman and a lower-class woman is one of the first moments in the history of feminist literature that hints at a cross-class argument, that is, that women of different economic positions have the same interests because they are women.

Wollstonecraft's "Letters Written in Sweden, Norway, and Denmark" is a deeply personal travel narrative. The twenty-five letters cover a wide range of topics, from sociological reflections on Scandinavia and its peoples to philosophical questions regarding identity to musings on her relationship with Imlay (although he is not referred to by name in the text). Using the rhetoric of the sublime, Wollstonecraft explores the relationship between the self and society. Reflecting the strong influence of Rousseau, "Letters Written in Sweden, Norway, and Denmark" shares the themes of the French philosopher's "Reveries of a Solitary Walker" (1782): "the search for the source of human happiness, the stoic rejection of material goods, the ecstatic embrace of nature, and the essential role of sentiment in understanding". While Rousseau ultimately rejects society, however, Wollstonecraft celebrates domestic scenes and industrial progress in her text.
Wollstonecraft promotes subjective experience, particularly in relation to nature, exploring the connections between the sublime and sensibility. Many of the letters describe the breathtaking scenery of Scandinavia and Wollstonecraft's desire to create an emotional connection to that natural world. In so doing, she gives greater value to the imagination than she had in previous works. As in her previous writings, she champions the liberation and education of women. In a change from her earlier works, however, she illustrates the detrimental effects of commerce on society, contrasting the imaginative connection to the world with a commercial and mercenary one, an attitude she associates with Imlay.

"Letters Written in Sweden, Norway, and Denmark" was Wollstonecraft's most popular book in the 1790s. It sold well and was reviewed positively by most critics. Godwin wrote "if ever there was a book calculated to make a man in love with its author, this appears to me to be the book." It influenced Romantic poets such as William Wordsworth and Samuel Taylor Coleridge, who drew on its themes and its aesthetic.
This is a complete list of Mary Wollstonecraft's works; all works are the first edition unless otherwise noted.








</doc>
<doc id="19836" url="https://en.wikipedia.org/wiki?curid=19836" title="Molecular mass">
Molecular mass

The molecular mass ("m") is the mass of a given molecule: it is measured in daltons (Da or u). Different molecules of the same compound may have different molecular masses because they contain different isotopes of an element. The related quantity relative molecular mass, as defined by IUPAC, is the ratio of the mass of a molecule to the unified atomic mass unit (also known as the dalton) and is unitless. The molecular mass and relative molecular mass are distinct from but related to the molar mass. The molar mass is defined as the mass of a given substance divided by the amount of a substance and is expressed in g/mol. The molar mass is usually the more appropriate figure when dealing with macroscopic (weigh-able) quantities of a substance. 

The definition of molecular weight is most authoritatively synonymous with molecular mass; however, in common practice, it is also highly variable as are the units used in conjunction with it. Many common preparatory sources use g/mol and effectively define it as a synonym of molar mass, while more authoritative sources use Da or u and align its definition more closely with the molecular mass. Even when the molecular weight is used with the units Da or u, it is frequently as a weighted average similar to the molar mass but with different units. In molecular biology, the weight of macromolecules is referred to as their molecular weight and is expressed in kDa, although the numerical value is often approximate and representative of an average.

The terms molecular mass, molecular weight, and molar mass are often used interchangeably in areas of science where distinguishing between them is unhelpful. In other areas of science, the distinction is crucial. The molecular mass is more commonly used when referring to the mass of a single or specific well-defined molecule and less commonly than molecular weight when referring to a weighted average of a sample. Prior to the 2019 redefinition of SI base units quantities expressed in daltons (Da or u) were by definition numerically equivalent to otherwise identical quantities expressed in the units g/mol and were thus strictly numerically interchangeable. After the 20 May 2019 redefinition of units, this relationship is only nearly equivalent.

The molecular mass of small to medium size molecules, measured by mass spectrometry, can be used to determine the composition of elements in the molecule. The molecular masses of macromolecules, such as proteins, can also be determined by mass spectrometry; however, methods based on viscosity and light-scattering are also used to determine molecular mass when crystallographic or mass spectrometric data are not available.

Molecular masses are calculated from the atomic masses of each nuclide present in the molecule, while molar masses are calculated from the standard atomic weights of each element. The standard atomic weight takes into account the isotopic distribution of the element in a given sample (usually assumed to be "normal"). For example, water has a molar mass of 18.0153(3) g/mol, but individual water molecules have molecular masses which range between 18.010 564 6863(15) Da (HO) and 22.027 7364(9) Da (HO). 

Atomic and molecular masses are usually reported in daltons which is defined relative to the mass of the isotope C (carbon 12), which by definition is equal to 12 Da. For example, the molar mass and molecular mass of methane, whose molecular formula is CH, are calculated respectively as follows:

The more formally defined term is "relative molecular mass". Relative atomic and molecular mass values as defined are dimensionless. However, the adjective 'relative' is omitted in practice as it is universally assumed that atomic and molecular masses are relative to the mass of C. Additionally, the "unit" Dalton is used in common practice. The mass of 1 mol of substance is designated as molar mass. By definition, the molar mass has the units of grams per mole.

In the example above the standard atomic weight of carbon is 12.011 g/mol, not 12.00 g/mol. This is because naturally occurring carbon is a mixture of the isotopes C, C and C which have masses of 12 Da, 13.003355 Da, and 14.003242 Da respectively. Moreover, the proportion of the isotopes varies between samples, so 12.011 g/mol is an average value across different places on earth. By contrast, there is less variation in naturally occurring hydrogen so the standard atomic weight has less variance. The precision of the molar mass is limited by the highest variance standard atomic weight, in this example that of carbon. This uncertainty is not the same as the uncertainty in the molecular mass, which reflects variance (error) in measurement not the natural variance in isotopic abundances across the globe. In high-resolution mass spectrometry the mass isotopomers CH and CH are observed as distinct molecules, with molecular masses of approximately 16.031 Da and 17.035 Da, respectively. The intensity of the mass-spectrometry peaks is proportional to the isotopic abundances in the molecular species. C H H can also be observed with molecular mass of 17 Da.

In mass spectrometry, the molecular mass of a small molecule is usually reported as the monoisotopic mass, that is, the mass of the molecule containing only the most common isotope of each element. Note that this also differs subtly from the molecular mass in that the choice of isotopes is defined and thus is a single specific molecular mass of the many possibilities. The masses used to compute the monoisotopic molecular mass are found on a table of isotopic masses and are not found on a typical periodic table. The average molecular mass is often used for larger molecules since molecules with many atoms are unlikely to be composed exclusively of the most abundant isotope of each element. A theoretical average molecular mass can be calculated using the standard atomic weights found on a typical periodic table, since there is likely to be a statistical distribution of atoms representing the isotopes throughout the molecule. The average molecular mass of a sample, however, usually differs substantially from this since a single sample average is not the same as the average of many geographically distributed samples.

To a first approximation, the basis for determination of molecular mass according to Mark–Houwink relations is the fact that the intrinsic viscosity of solutions (or suspensions) of macromolecules depends on volumetric proportion of the dispersed particles in a particular solvent. Specifically, the hydrodynamic size as related to molecular mass depends on a conversion factor, describing the shape of a particular molecule. This allows the apparent molecular mass to be described from a range of techniques sensitive to hydrodynamic effects, including DLS, SEC (also known as GPC when the eluent is an organic solvent), viscometry, and diffusion ordered nuclear magnetic resonance spectroscopy (DOSY). The apparent hydrodynamic size can then be used to approximate molecular mass using a series of macromolecule-specific standards. As this requires calibration, it's frequently described as a "relative" molecular mass determination method.

It is also possible to determine absolute molecular mass directly from light scattering, traditionally using the Zimm method. This can be accomplished either via classical static light scattering or via multi-angle light scattering detectors. Molecular masses determined by this method do not require calibration, hence the term "absolute". The only external measurement required is refractive index increment, which describes the change in refractive index with concentration.




</doc>
