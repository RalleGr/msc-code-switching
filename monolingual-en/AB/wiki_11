<doc id="13574" url="https://en.wikipedia.org/wiki?curid=13574" title="Herodotus">
Herodotus

Herodotus (; , "Hēródotos", ; BC) was an ancient Greek historian who was born in Halicarnassus in the Persian Empire (modern-day Bodrum, Turkey). He is known for having written the book "The Histories" (Greek: Ἱστορίαι "Historíai"), a detailed record of his "inquiry" ( "historía") on the origins of the Greco-Persian Wars. He is widely considered to have been the first writer to have treated historical subjects using a method of systematic investigation—specifically, by collecting his materials and then critically arranging them into an historiographic narrative. On account of this, he is often referred to as "The Father of History," a title first conferred on him by the first-century BC Roman orator Cicero.

Despite Herodotus's historical significance, little is known about his personal life. His "Histories" primarily deals with the lives of Croesus, Cyrus, Cambyses, Smerdis, Darius, and Xerxes and the battles of Marathon, Thermopylae, Artemisium, Salamis, Plataea, and Mycale; however, his many cultural, ethnographical, geographical, historiographical, and other digressions form a defining and essential part of the "Histories" and contain a wealth of information. Herodotus has been criticized for the fact that his book includes many obvious legends and fanciful accounts. Many authors, starting with the late fifth-century BC historian Thucydides, have accused him of making up stories for entertainment. However, Herodotus states that he is merely reporting what he has seen and been told, on several occasions saying that he does not himself believe the story that he reports. A sizable portion of the information he provides has since been confirmed by historians and archaeologists.

Herodotus announced the purpose and scope of his work at the beginning of his "Histories:"

His record of the achievements of others was an achievement in itself, though the extent of it has been debated. Herodotus's place in history and his significance may be understood according to the traditions within which he worked. His work is the earliest Greek prose to have survived intact. However, Dionysius of Halicarnassus, a literary critic of Augustan Rome, listed seven predecessors of Herodotus, describing their works as simple, unadorned accounts of their own and other cities and people, Greek or foreign, including popular legends, sometimes melodramatic and naïve, often charming – all traits that can be found in the work of Herodotus himself.

Modern historians regard the chronology as uncertain, but according to the ancient account, these predecessors included Dionysius of Miletus, Charon of Lampsacus, Hellanicus of Lesbos, Xanthus of Lydia and, the best attested of them all, Hecataeus of Miletus. Of these, only fragments of Hecataeus's works survived, and the authenticity of these is debatable, but they provide a glimpse into the kind of tradition within which Herodotus wrote his own "Histories".

In his introduction to Hecataeus' work, "Genealogies":

This points forward to the "folksy" yet "international" outlook typical of Herodotus. However, one modern scholar has described the work of Hecataeus as "a curious false start to history," since despite his critical spirit, he failed to liberate history from myth. Herodotus mentions Hecataeus in his "Histories", on one occasion mocking him for his naive genealogy and, on another occasion, quoting Athenian complaints against his handling of their national history. It is possible that Herodotus borrowed much material from Hecataeus, as stated by Porphyry in a quote recorded by Eusebius. In particular, it is possible that he copied descriptions of the crocodile, hippopotamus, and phoenix from Hecataeus's "Circumnavigation of the Known World" ("Periegesis" / "Periodos ges"), even misrepresenting the source as "Heliopolitans" ("Histories" 2.73).

But Hecataeus did not record events that had occurred in living memory, unlike Herodotus, nor did he include the oral traditions of Greek history within the larger framework of oriental history. There is no proof that Herodotus derived the ambitious scope of his own work, with its grand theme of civilizations in conflict, from any predecessor, despite much scholarly speculation about this in modern times. Herodotus claims to be better informed than his predecessors by relying on empirical observation to correct their excessive schematism. For example, he argues for continental asymmetry as opposed to the older theory of a perfectly circular earth with Europe and Asia/Africa equal in size ("Histories" 4.36 and 4.42). However, he retains idealizing tendencies, as in his symmetrical notions of the Danube and Nile.

His debt to previous authors of prose "histories" might be questionable, but there is no doubt that Herodotus owed much to the example and inspiration of poets and story-tellers. For example, Athenian tragic poets provided him with a world-view of a balance between conflicting forces, upset by the hubris of kings, and they provided his narrative with a model of episodic structure. His familiarity with Athenian tragedy is demonstrated in a number of passages echoing Aeschylus's "Persae", including the epigrammatic observation that the defeat of the Persian navy at Salamis caused the defeat of the land army ("Histories" 8.68 ~ "Persae" 728). The debt may have been repaid by Sophocles because there appear to be echoes of "The Histories" in his plays, especially a passage in "Antigone" that resembles Herodotus's account of the death of Intaphernes ("Histories" 3.119 ~ "Antigone" 904–920). However, this point is one of the most contentious issues in modern scholarship.

Homer was another inspirational source. Just as Homer drew extensively on a tradition of oral poetry, sung by wandering minstrels, so Herodotus appears to have drawn on an Ionian tradition of story-telling, collecting and interpreting the oral histories he chanced upon in his travels. These oral histories often contained folk-tale motifs and demonstrated a moral, yet they also contained substantial facts relating to geography, anthropology, and history, all compiled by Herodotus in an entertaining style and format.

It is on account of the many strange stories and the folk-tales he reported that his critics have branded him "The Father of Lies." Even his own contemporaries found reason to scoff at his achievement. In fact, one modern scholar has wondered if Herodotus left his home in Greek Anatolia, migrating westwards to Athens and beyond, because his own countrymen had ridiculed his work, a circumstance possibly hinted at in an epitaph said to have been dedicated to Herodotus at one of his three supposed resting places, Thuria:
Yet it was in Athens where his most formidable contemporary critics could be found. In 425 BC, which is about the time that Herodotus is thought by many scholars to have died, the Athenian comic dramatist Aristophanes created "The Acharnians", in which he blames the Peloponnesian War on the abduction of some prostitutes – a mocking reference to Herodotus, who reported the Persians' account of their wars with Greece, beginning with the rapes of the mythical heroines Io, Europa, Medea, and Helen.

Similarly, the Athenian historian Thucydides dismissed Herodotus as a "logos-writer" (story-teller). Thucydides, who had been trained in rhetoric, became the model for subsequent prose-writers as an author who seeks to appear firmly in control of his material, whereas with his frequent digressions Herodotus appeared to minimize (or possibly disguise) his authorial control. Moreover, Thucydides developed a historical topic more in keeping with the Greek world-view: focused on the context of the "polis" or city-state. The interplay of civilizations was more relevant to Greeks living in Anatolia, such as Herodotus himself, for whom life within a foreign civilization was a recent memory.
Modern scholars generally turn to Herodotus's own writing for reliable information about his life, supplemented with ancient yet much later sources, such as the Byzantine "Suda", an 11th-century encyclopedia which possibly took its information from traditional accounts.

Modern accounts of his life typically go something like this: Herodotus was born at Halicarnassus around 485 BC. There is no reason to disbelieve the "Suda"'s information about his family: that it was influential and that he was the son of Lyxes and Dryo, and the brother of Theodorus, and that he was also related to Panyassis – an epic poet of the time.

The town was within the Persian Empire at that time, making Herodotus a Persian subject, and it may be that the young Herodotus heard local eyewitness accounts of events within the empire and of Persian preparations for the invasion of Greece, including the movements of the local fleet under the command of Artemisia I of Caria.

Inscriptions recently discovered at Halicarnassus indicate that her grandson Lygdamis negotiated with a local assembly to settle disputes over seized property, which is consistent with a tyrant under pressure. His name is not mentioned later in the tribute list of the Athenian Delian League, indicating that there might well have been a successful uprising against him sometime before 454 BC. 

The epic poet Panyassis – a relative of Herodotus – is reported to have taken part in a failed uprising. Herodotus expresses affection for the island of Samos (III, 39–60), and this is an indication that he might have lived there in his youth. So it is possible that his family was involved in an uprising against Lygdamis, leading to a period of exile on Samos and followed by some personal hand in the tyrant's eventual fall.

Herodotus wrote his "Histories" in the Ionian dialect, yet he was born in Halicarnassus, which was a Dorian settlement. According to the "Suda", Herodotus learned the Ionian dialect as a boy living on the island of Samos, to which he had fled with his family from the oppressions of Lygdamis, tyrant of Halicarnassus and grandson of Artemisia. 

The "Suda" also informs us that Herodotus later returned home to lead the revolt that eventually overthrew the tyrant. Due to recent discoveries of inscriptions at Halicarnassus dated to about Herodotus's time, we now know that the Ionic dialect was used in Halicarnassus in some official documents, so there is no need to assume (like the "Suda") that he must have learned the dialect elsewhere. Further, the "Suda" is the only source which we have for the role played by Herodotus as the heroic liberator of his birthplace. That itself is a good reason to doubt such a romantic account.

As Herodotus himself reveals, Halicarnassus, though a Dorian city, had ended its close relations with its Dorian neighbours after an unseemly quarrel (I, 144), and it had helped pioneer Greek trade with Egypt (II, 178). It was, therefore, an outward-looking, international-minded port within the Persian Empire, and the historian's family could well have had contacts in other countries under Persian rule, facilitating his travels and his researches.

Herodotus's eyewitness accounts indicate that he traveled in Egypt in association with Athenians, probably sometime after 454 BC or possibly earlier, after an Athenian fleet had assisted the uprising against Persian rule in 460–454 BC. He probably traveled to Tyre next and then down the Euphrates to Babylon. For some reason, possibly associated with local politics, he subsequently found himself unpopular in Halicarnassus, and sometime around 447 BC, migrated to Periclean Athens – a city whose people and democratic institutions he openly admires (V, 78). Athens was also the place where he came to know the local topography (VI, 137; VIII, 52–55), as well as leading citizens such as the Alcmaeonids, a clan whose history features frequently in his writing.

According to Eusebius and Plutarch, Herodotus was granted a financial reward by the Athenian assembly in recognition of his work. It is possible that he unsuccessfully applied for Athenian citizenship, a rare honour after 451 BC, requiring two separate votes by a well-attended assembly.

In 443 BC or shortly afterwards, he migrated to Thurium as part of an Athenian-sponsored colony. Aristotle refers to a version of "The Histories" written by "Herodotus of Thurium," and some passages in the "Histories" have been interpreted as proof that he wrote about southern Italy from personal experience there (IV, 15,99; VI, 127). Intimate knowledge of some events in the first years of the Peloponnesian War (VI, 91; VII, 133, 233; IX, 73) indicate that he might have returned to Athens, in which case it is possible that he died there during an outbreak of the plague. Possibly he died in Macedonia instead, after obtaining the patronage of the court there; or else he died back in Thurium. There is nothing in the "Histories" that can be dated to later than 430 BC with any certainty, and it is generally assumed that he died not long afterwards, possibly before his sixtieth year.

Herodotus would have made his researches known to the larger world through oral recitations to a public crowd. John Marincola writes in his introduction to the Penguin edition of "The Histories" that there are certain identifiable pieces in the early books of Herodotus's work which could be labeled as "performance pieces." These portions of the research seem independent and "almost detachable," so that they might have been set aside by the author for the purposes of an oral performance. The intellectual matrix of the 5th century, Marincola suggests, comprised many oral performances in which philosophers would dramatically recite such detachable pieces of their work. The idea was to criticize previous arguments on a topic and emphatically and enthusiastically insert their own in order to win over the audience.

It was conventional in Herodotus's day for authors to "publish" their works by reciting them at popular festivals. According to Lucian, Herodotus took his finished work straight from Anatolia to the Olympic Games and read the entire "Histories" to the assembled spectators in one sitting, receiving rapturous applause at the end of it. According to a very different account by an ancient grammarian, Herodotus refused to begin reading his work at the festival of Olympia until some clouds offered him a bit of shade – by which time the assembly had dispersed. (Hence the proverbial expression "Herodotus and his shade" to describe someone who misses an opportunity through delay.) Herodotus's recitation at Olympia was a favourite theme among ancient writers, and there is another interesting variation on the story to be found in the "Suda": that of Photius and Tzetzes, in which a young Thucydides happened to be in the assembly with his father, and burst into tears during the recital. Herodotus observed prophetically to the boy's father, "Your son's soul yearns for knowledge."

Eventually, Thucydides and Herodotus became close enough for both to be interred in Thucydides' tomb in Athens. Such at least was the opinion of Marcellinus in his "Life of Thucydides". According to the "Suda", he was buried in Macedonian Pella and in the agora in Thurium.

The accuracy of the works of Herodotus has been controversial since his own era. Kenton L. Sparks writes, "In antiquity, Herodotus had acquired the reputation of being unreliable, biased, parsimonious in his praise of heroes, and mendacious". The historian Duris of Samos called Herodotus a "myth-monger". Cicero ("On the Laws" I.5) said that his works were full of legends or "fables". The controversy was also commented on by Aristotle, Flavius Josephus and Plutarch. The Alexandrian grammarian Harpocration wrote a whole book on "the lies of Herodotus". Lucian of Samosata went as far as to deny the "father of history" a place among the famous on the Island of the Blessed in his "Verae Historiae".

The works of Thucydides were often given preference for their "truthfulness and reliability", even if Thucydides basically continued on foundations laid by Herodotus, as in his treatment of the Persian Wars. In spite of these lines of criticism, Herodotus' works were in general kept in high esteem and regarded as reliable by many. Many scholars, ancient and modern (such as Strabo, A. H. L. Heeren, etc.), routinely cited Herodotus.

To this day, some scholars regard his works as being at least partly unreliable. Detlev Fehling writes of "a problem recognized by everybody", namely that Herodotus frequently cannot be taken at face value. Fehling argues that Herodotus exaggerated the extent of his travels and invented his sources. For Fehling, the sources of many stories, as reported by Herodotus, do not appear credible in themselves. Persian and Egyptian informants tell stories that dovetail neatly into Greek myths and literature, yet show no signs of knowing their own traditions. For Fehling, the only credible explanation is that Herodotus invented these sources, and that the stories themselves were concocted by Herodotus himself.

Like many ancient historians, Herodotus preferred an element of show to purely analytic history, aiming to give pleasure with "exciting events, great dramas, bizarre exotica." As such, certain passages have been the subject of controversy and even some doubt, both in antiquity and today.

Despite the controversy, Herodotus has long served and still serves as the primary, often only, source for events in the Greek world, Persian Empire, and the broader region in the two centuries leading up to his own days. So even if the "Histories" were criticized in some regards since antiquity, modern historians and philosophers generally take a more positive view as to their source and epistemologic value. Herodotus is variously considered "father of comparative anthropology," "the father of ethnography," and "more modern than any other ancient historian in his approach to the ideal of total history."

Discoveries made since the end of the 19th century have generally added to Herodotus' credibility. He described Gelonus, located in Scythia, as a city thousands of times larger than Troy; this was widely disbelieved until it was rediscovered in 1975. The archaeological study of the now-submerged ancient Egyptian city of Heracleion and the recovery of the so-called "Naucratis stela" give credibility to Herodotus's previously unsupported claim that Heracleion was founded during the Egyptian New Kingdom.

Herodotus claimed to have visited Babylon. The absence of any mention of the Hanging Gardens of Babylon in his work has attracted further attacks on his credibility. In response, Dalley has proposed that the Hanging Gardens may have been in Nineveh rather than in Babylon.

The reliability of Herodotus's writing about Egypt is sometimes questioned. Alan B. Lloyd argues that, as a historical document, the writings of Herodotus are seriously defective, and that he was working from "inadequate sources." Nielsen writes: "Though we cannot entirely rule out the possibility of Herodotus having been in Egypt, it must be said that his narrative bears little witness to it." German historian Detlev Fehling questions whether Herodotus ever traveled up the Nile River, and considers doubtful almost everything that he says about Egypt and Ethiopia. Fehling states that "there is not the slightest bit of history behind the whole story" about the claim of Herodotus that Pharaoh Sesostris campaigned in Europe, and that he left a colony in Colchia. Fehling concludes that the works of Herodotus are intended as fiction. Boedeker concurs that much of the content of the works of Herodotus are literary devices.

However, a recent discovery of a baris (described in "The Histories") during an excavation of the sunken Egyptian port city of Thonis-Heracleion lends credence to Herodotus's travels and storytelling.

Herodotus' contribution to the history and ethnography of ancient Egypt and Africa was especially valued by various historians of the field (such as Constantin François de Chassebœuf, comte de Volney, W. E. B. Du Bois, Pierre Montet, Martin Bernal, Basil Davidson, Derek A. Welsby, Henry T. Aubin). Many scholars explicitly mention the reliability of Herodotus's work (such as on the Nile Valley) and demonstrate corroboration of Herodotus' writings by modern scholars. A. H. L. Heeren quoted Herodotus throughout his work and provided corroboration by scholars regarding several passages (source of the Nile, location of Meroë, etc.). 

Cheikh Anta Diop provides several examples (like the inundations of the Nile) which, he argues, support his view that Herodotus was "quite scrupulous, objective, scientific for his time." Diop argues that Herodotus "always distinguishes carefully between what he has seen and what he has been told." Diop also notes that Strabo corroborated Herodotus' ideas about the Black Egyptians, Ethiopians, and Colchians. Martin Bernal has relied on Herodotus "to an extraordinary degree" in his controversial book "Black Athena".

British egyptologist Derek A. Welsby said that "archaeology graphically confirms Herodotus's observations." To further his work on the Egyptians and Assyrians, historian and fiction writer Henry T. Aubin used Herodotus' accounts in various passages. For Aubin, Herodotus was "the author of the first important narrative history of the world."

Herodotus provides much information about the nature of the world and the status of science during his lifetime, often engaging in private speculation likewise. For example, he reports that the annual flooding of the Nile was said to be the result of melting snows far to the south, and he comments that he cannot understand how there can be snow in Africa, the hottest part of the known world, offering an elaborate explanation based on the way that desert winds affect the passage of the Sun over this part of the world (2:18ff). He also passes on reports from Phoenician sailors that, while circumnavigating Africa, they "saw the sun on the right side while sailing westwards", although, being unaware of the existence of the southern hemisphere, he says that he does not believe the claim. Owing to this brief mention, which is included almost as an afterthought, it has been argued that Africa was circumnavigated by ancient seafarers, for this is precisely where the sun ought to have been. His accounts of India are among the oldest records of Indian civilization by an outsider.

After journeys to India and Pakistan, French ethnologist Michel Peissel claimed to have discovered an animal species that may illuminate one of the most bizarre passages in the "Histories". In Book 3, passages 102 to 105, Herodotus reports that a species of fox-sized, furry "ants" lives in one of the far eastern, Indian provinces of the Persian Empire. This region, he reports, is a sandy desert, and the sand there contains a wealth of fine gold dust. These giant ants, according to Herodotus, would often unearth the gold dust when digging their mounds and tunnels, and the people living in this province would then collect the precious dust. Later Pliny the Elder would mention this story in the gold mining section of his "Naturalis Historia".

Peissel reports that, in an isolated region of northern Pakistan on the Deosai Plateau in Gilgit–Baltistan province, there is a species of marmot – the Himalayan marmot, a type of burrowing squirrel – that may have been what Herodotus called giant ants. The ground of the Deosai Plateau is rich in gold dust, much like the province that Herodotus describes. According to Peissel, he interviewed the Minaro tribal people who live in the Deosai Plateau, and they have confirmed that they have, for generations, been collecting the gold dust that the marmots bring to the surface when they are digging their burrows.

Peissel offers the theory that Herodotus may have confused the old Persian word for "marmot" with the word for "mountain ant." Research suggests that Herodotus probably did not know any Persian (or any other language except his native Greek) and was forced to rely on many local translators when travelling in the vast multilingual Persian Empire. Herodotus did not claim to have personally seen the creatures which he described. Herodotus did, though, follow up in passage 105 of Book 3 with the claim that the "ants" are said to chase and devour full-grown camels.

Some "calumnious fictions" were written about Herodotus in a work titled "On the Malice of Herodotus" by Plutarch, a Chaeronean by birth, (or it might have been a Pseudo-Plutarch, in this case "a great collector of slanders"), including the allegation that the historian was prejudiced against Thebes because the authorities there had denied him permission to set up a school. Similarly, in a "Corinthian Oration", Dio Chrysostom (or yet another pseudonymous author) accused the historian of prejudice against Corinth, sourcing it in personal bitterness over financial disappointments – an account also given by Marcellinus in his "Life of Thucydides". In fact, Herodotus was in the habit of seeking out information from empowered sources within communities, such as aristocrats and priests, and this also occurred at an international level, with Periclean Athens becoming his principal source of information about events in Greece. As a result, his reports about Greek events are often coloured by Athenian bias against rival states – Thebes and Corinth in particular.

It is clear from the beginning of Book 1 of the "Histories" that Herodotus utilizes (or at least claims to utilize) various sources in his narrative. K. H. Waters relates that "Herodotos did not work from a purely Hellenic standpoint; he was accused by the patriotic but somewhat imperceptive Plutarch of being "philobarbaros", a pro-barbarian or pro-foreigner."

Herodotus at times relates various accounts of the same story. For example, in Book 1 he mentions both the Phoenician and the Persian accounts of Io. However, Herodotus at times arbitrates between varying accounts: "I am not going to say that these events happened one way or the other. Rather, I will point out the man "who I know for a fact" began the wrong-doing against the Greeks." Again, later, Herodotus claims himself as an authority: "I know this is how it happened because I heard it from the Delphians myself."

Throughout his work, Herodotus attempts to explain the actions of people. Speaking about Solon the Athenian, Herodotus states "[Solon] sailed away on the pretext of seeing the world, "but it was really so that he could not be compelled to repeal any of the laws he had laid down"." Again, in the story about Croesus and his son's death, when speaking of Adrastus (the man who accidentally killed Croesus' son), Herodotus states: "Adrastus ... "believing himself to be the most ill-fated man he had ever known", cut his own throat over the grave."

Herodotus writes with the purpose of "explaining"; that is, he discusses the reason for or cause of an event. He lays this out in the preamble: "This is the publication of the research of Herodotus of Halicarnassus, so that the actions of people shall not fade with time, so that the great and admirable achievements of both Greeks and barbarians shall not go unrenowned, and, among other things, "to set forth the reasons why they waged war on each other"."

This mode of explanation traces itself all the way back to Homer, who opened the "Iliad" by asking:

Both Homer and Herodotus begin with a question of causality. In Homer's case, "who set these two at each other's throats?" In Herodotus's case, "Why did the Greeks and barbarians go to war with each other?"

Herodotus's means of explanation does not necessarily posit a simple cause; rather, his explanations cover a host of potential causes and emotions. It is notable, however, that "the obligations of gratitude and revenge are the fundamental human motives for Herodotus, just as ... they are the primary stimulus to the generation of narrative itself."

Some readers of Herodotus believe that his habit of tying events back to personal motives signifies an inability to see broader and more abstract reasons for action. Gould argues to the contrary that this is likely because Herodotus attempts to provide the rational reasons, as understood by his contemporaries, rather than providing more abstract reasons.

Herodotus attributes cause to both divine and human agents. These are not perceived as mutually exclusive, but rather mutually interconnected. This is true of Greek thinking in general, at least from Homer onward. Gould notes that invoking the supernatural in order to explain an event does not answer the question "why did this happen?" but rather "why did this happen to me?" By way of example, faulty craftsmanship is the human cause for a house collapsing. However, divine will is the reason that the house collapses at the particular moment when I am inside. It was the will of the gods that the house collapsed while a particular individual was within it, whereas it was the cause of man that the house had a weak structure and was prone to falling.

Some authors, including Geoffrey de Ste-Croix and Mabel Lang, have argued that Fate, or the belief that "this is how it had to be," is Herodotus's ultimate understanding of causality. Herodotus's explanation that an event "was going to happen" maps well on to Aristotelean and Homeric means of expression. The idea of "it was going to happen" reveals a "tragic discovery" associated with fifth-century drama. This tragic discovery can be seen in Homer's "Iliad" as well.

John Gould argues that Herodotus should be understood as falling in a long line of story-tellers, rather than thinking of his means of explanation as a "philosophy of history" or "simple causality." Thus, according to Gould, Herodotus's means of explanation is a mode of story-telling and narration that has been passed down from generations prior:

Although Herodotus considered his "inquiries" a serious pursuit of knowledge, he was not above relating entertaining tales derived from the collective body of myth, but he did so judiciously with regard for his historical method, by corroborating the stories through enquiry and testing their probability. While the gods never make personal appearances in his account of human events, Herodotus states emphatically that "many things prove to me that the gods take part in the affairs of man" (IX, 100).

In Book One, passages 23 and 24, Herodotus relates the story of Arion, the renowned harp player, "second to no man living at that time," who was saved by a dolphin. Herodotus prefaces the story by noting that "a very wonderful thing is said to have happened," and alleges its veracity by adding that the "Corinthians and the Lesbians agree in their account of the matter." Having become very rich while at the court of Periander, Arion conceived a desire to sail to Italy and Sicily. He hired a vessel crewed by Corinthians, whom he felt he could trust, but the sailors plotted to throw him overboard and seize his wealth. Arion discovered the plot and begged for his life, but the crew gave him two options: that either he kill himself on the spot or jump ship and fend for himself in the sea. Arion flung himself into the water, and a dolphin carried him to shore.

Herodotus clearly writes as both historian and teller of tales. Herodotus takes a fluid position between the artistic story-weaving of Homer and the rational data-accounting of later historians. John Herington has developed a helpful metaphor for describing Herodotus's dynamic position in the history of Western art and thought – Herodotus as centaur:

Herodotus is neither a mere gatherer of data nor a simple teller of tales – he is both. While Herodotus is certainly concerned with giving accurate accounts of events, this does not preclude for him the insertion of powerful mythological elements into his narrative, elements which will aid him in expressing the truth of matters under his study. Thus to understand what Herodotus is doing in the "Histories", we must not impose strict demarcations between the man as mythologist and the man as historian, or between the work as myth and the work as history. As James Romm has written, Herodotus worked under a common ancient Greek cultural assumption that the way events are remembered and retold (e.g. in myths or legends) produces a valid kind of understanding, even when this retelling is not entirely factual. For Herodotus, then, it takes both myth and history to produce truthful understanding.


Several English translations of "The Histories of Herodotus" are readily available in multiple editions. The most readily available are those translated by:





</doc>
<doc id="13575" url="https://en.wikipedia.org/wiki?curid=13575" title="Historian">
Historian

A historian is a person who studies and writes about the past and is regarded as an authority on it. Historians are concerned with the continuous, methodical narrative and research of past events as relating to the human race; as well as the study of all history in time. If the individual is concerned with events preceding written history, the individual is a historian of prehistory. Some historians are recognized by publications or training and experience. "Historian" became a professional occupation in the late nineteenth century as research universities were emerging in Germany and elsewhere.

During the "Irving v Penguin Books and Lipstadt" trial, it became evident that the court needed to identify what was an "objective historian" in the same vein as the reasonable person, and reminiscent of the standard traditionally used in English law of "the man on the Clapham omnibus". This was necessary so that there would be a legal bench mark to compare and contrast the scholarship of an objective historian against the illegitimate methods employed by David Irving, as before the "Irving v Penguin Books and Lipstadt" trial, there was no legal precedent for what constituted an objective historian.

Justice Gray leant heavily on the research of one of the expert witnesses, Richard J. Evans, who compared illegitimate distortion of the historical record practice by holocaust deniers with established historical methodologies.

By summarizing Gray's judgement, in an article published in the "Yale Law Journal", Wendie E. Schneider distils these seven points for what he meant by an objective historian:
Schneider uses the concept of the "objective historian" to suggest that this could be an aid in assessing what makes an historian suitable as an expert witnesses under the Daubert standard in the United States. Schneider proposed this, because, in her opinion, Irving could have passed the standard Daubert tests unless a court was given "a great deal of assistance from historians".

Schneider proposes that by testing an historian against the criteria of the "objective historian" then, even if an historian holds specific political views (and she gives an example of a well-qualified historian's testimony that was disregarded by a United States court because he was a member of a feminist group), providing the historian uses the "objective historian" standards, he or she is a "conscientious historian". It was Irving's failure as an "objective historian" not his right wing views that caused him to lose his libel case, as a "conscientious historian" would not have "deliberately misrepresented and manipulated historical evidence" to support his political views.

The process of historical analysis involves investigation and analysis of competing ideas, facts, and purported facts to create coherent narratives that explain "what happened" and "why or how it happened". Modern historical analysis usually draws upon other social sciences, including economics, sociology, politics, psychology, anthropology, philosophy, and linguistics. While ancient writers do not normally share modern historical practices, their work remains valuable for its insights within the cultural context of the times. An important part of the contribution of many modern historians is the verification or dismissal of earlier historical accounts through reviewing newly discovered sources and recent scholarship or through parallel disciplines like archaeology.

Understanding the past appears to be a universal human need, and the telling of history has emerged independently in civilizations around the world. What constitutes history is a philosophical question (see philosophy of history). The earliest chronologies date back to Mesopotamia and ancient Egypt, though no historical writers in these early civilizations were known by name.

Systematic historical thought emerged in ancient Greece, a development that became an important influence on the writing of history elsewhere around the Mediterranean region. The earliest known critical historical works were "The Histories", composed by Herodotus of Halicarnassus (484 – ) who later became known as the "father of history" (Cicero). Herodotus attempted to distinguish between more and less reliable accounts, and personally conducted research by travelling extensively, giving written accounts of various Mediterranean cultures. Although Herodotus' overall emphasis lay on the actions and characters of men, he also attributed an important role to divinity in the determination of historical events. Thucydides largely eliminated divine causality in his account of the war between Athens and Sparta, establishing a rationalistic element that set a precedent for subsequent Western historical writings. He was also the first to distinguish between cause and immediate origins of an event, while his successor Xenophon ( – ) introduced autobiographical elements and character studies in his Anabasis.
The Romans adopted the Greek tradition. While early Roman works were still written in Greek, the "Origines", composed by the Roman statesman Cato the Elder (), was written in Latin, in a conscious effort to counteract Greek cultural influence. Strabo ( – ) was an important exponent of the Greco-Roman tradition of combining geography with history, presenting a descriptive history of peoples and places known to his era. Livy ( – ) records the rise of Rome from city-state to empire. His speculation about what would have happened if Alexander the Great had marched against Rome represents the first known instance of alternate history.

In Chinese historiography, the "Classic of History" is one of the Five Classics of Chinese classic texts and one of the earliest narratives of China. The "Spring and Autumn Annals", the official chronicle of the State of Lu covering the period from , is among the earliest surviving Chinese historical texts arranged on annalistic principles. Sima Qian (around ) was the first in China to lay the groundwork for professional historical writing. His written work was the "Shiji" ("Records of the Grand Historian"), a monumental lifelong achievement in literature. Its scope extends as far back as the , and it includes many treatises on specific subjects and individual biographies of prominent people, and also explores the lives and deeds of commoners, both contemporary and those of previous eras.
Christian historiography began early, perhaps as early as Luke-Acts, which is the primary source for the Apostolic Age. Writing history was popular among Christian monks and clergy in the Middle Ages. They wrote about the history of Jesus Christ, that of the Church and that of their patrons, the dynastic history of the local rulers. In the Early Middle Ages historical writing often took the form of annals or chronicles recording events year by year, but this style tended to hamper the analysis of events and causes. An example of this type of writing is the Anglo-Saxon Chronicles, which were the work of several different writers: it was started during the reign of Alfred the Great in the late 9th century, but one copy was still being updated in 1154.

Muslim historical writings first began to develop in the 7th century, with the reconstruction of the Prophet Muhammad's life in the centuries following his death. With numerous conflicting narratives regarding Muhammad and his companions from various sources, scholars had to verify which sources were more reliable. To evaluate these sources, they developed various methodologies, such as the "science of biography", "science of hadith" and "Isnad" (chain of transmission). They later applied these methodologies to other historical figures in the Islamic civilization. Famous historians in this tradition include Urwah (d. 712), Wahb ibn Munabbih (d. 728), Ibn Ishaq (d. 761), al-Waqidi (745–822), Ibn Hisham (d. 834), Muhammad al-Bukhari (810–870) and Ibn Hajar (1372–1449).

During the Age of Enlightenment, the modern development of historiography through the application of scrupulous methods began.
French "philosophe" Voltaire (1694–1778) had an enormous influence on the art of history writing. His best-known histories are "The Age of Louis XIV" (1751), and "Essay on the Customs and the Spirit of the Nations" (1756). "My chief object," he wrote in 1739, "is not political or military history, it is the history of the arts, of commerce, of civilization – in a word, – of the human mind." He broke from the tradition of narrating diplomatic and military events, and emphasized customs, social history, and achievements in the arts and sciences. He was the first scholar to make a serious attempt to write the history of the world, eliminating theological frameworks, and emphasizing economics, culture, and political history.
At the same time, philosopher David Hume was having a similar impact on history in Great Britain. In 1754, he published the "History of England", a six-volume work that extended from the Invasion of Julius Caesar to the Revolution in 1688. Hume adopted a similar scope to Voltaire in his history; as well as the history of Kings, Parliaments, and armies, he examined the history of culture, including literature and science, as well. William Robertson, a Scottish historian, and the Historiographer Royal published the "History of Scotland 1542 - 1603", in 1759 and his most famous work, "The history of the reign of Charles V" in 1769. His scholarship was painstaking for the time and he was able to access a large number of documentary sources that had previously been unstudied. He was also one of the first historians who understood the importance of general and universally applicable ideas in the shaping of historical events.

The apex of Enlightenment history was reached with Edward Gibbon's, monumental six-volume work, "The History of the Decline and Fall of the Roman Empire", published on 17 February 1776. Because of its relative objectivity and heavy use of primary sources, at the time its methodology became a model for later historians. This has led to Gibbon being called the first "modern historian". The book sold impressively, earning its author a total of about £9000. Biographer Leslie Stephen wrote that thereafter, "His fame was as rapid as it has been lasting."

The tumultuous events surrounding the French Revolution inspired much of the historiography and analysis of the early 19th century. Interest in the 1688 Glorious Revolution was also rekindled by the Great Reform Act of 1832 in England.

Thomas Carlyle published his magnum opus, the three-volume "" in 1837. The resulting work had a passion new to historical writing. Thomas Macaulay produced his most famous work of history, "The History of England from the Accession of James the Second", in 1848. His writings are famous for their ringing prose and for their confident, sometimes dogmatic, emphasis on a progressive model of British history, according to which the country threw off superstition, autocracy and confusion to create a balanced constitution and a forward-looking culture combined with freedom of belief and expression. This model of human progress has been called the Whig interpretation of history.
In his main work "Histoire de France", French historian Jules Michelet coined the term Renaissance (meaning "Re-birth" in French language), as a period in Europe's cultural history that represented a break from the Middle Ages, creating a modern understanding of humanity and its place in the world.
The nineteen-volume work covered French history from Charlemagne to the outbreak of the Revolution. Michelet was one of the first historians to shift the emphasis of history to the common people, rather than the leaders and institutions of the country. Another important French historian of the period was Hippolyte Taine. He was the chief theoretical influence of French naturalism, a major proponent of sociological positivism and one of the first practitioners of historicist criticism. Literary historicism as a critical movement has been said to originate with him.

One of the major progenitors of the history of culture and art, was the Swiss historian Jacob Burckhardt Burckhardt's best-known work is "The Civilization of the Renaissance in Italy" (1860). According to John Lukacs, he was the first master of cultural history, which seeks to describe the spirit and the forms of expression of a particular age, a particular people, or a particular place. By the mid-19th century, scholars were beginning to analyse the history of institutional change, particularly the development of constitutional government. William Stubbs's "Constitutional History of England" (3 vols., 1874–78) was an important influence on this developing field. The work traced the development of the English constitution from the Teutonic invasions of Britain until 1485, and marked a distinct step in the advance of English historical learning.

Karl Marx introduced the concept of historical materialism into the study of world historical development. In his conception, the economic conditions and dominant modes of production determined the structure of society at that point. Previous historians had focused on cyclical events of the rise and decline of rulers and nations. Process of nationalization of history, as part of national revivals in the 19th century, resulted with separation of "one's own" history from common universal history by such way of perceiving, understanding and treating the past that constructed history as history of a nation. A new discipline, sociology, emerged in the late 19th century and analyzed and compared these perspectives on a larger scale.

The modern academic study of history and methods of historiography were pioneered in 19th-century German universities. Leopold von Ranke was a pivotal influence in this regard, and is considered as the founder of modern source-based history.

Specifically, he implemented the seminar teaching method in his classroom, and focused on archival research and analysis of historical documents. Beginning with his first book in 1824, the "History of the Latin and Teutonic Peoples from 1494 to 1514", Ranke used an unusually wide variety of sources for an historian of the age, including "memoirs, diaries, personal and formal missives, government documents, diplomatic dispatches and first-hand accounts of eye-witnesses". Over a career that spanned much of the century, Ranke set the standards for much of later historical writing, introducing such ideas as reliance on primary sources (empiricism), an emphasis on narrative history and especially international politics ("aussenpolitik"). Sources had to be hard, not speculations and rationalizations. His credo was to write history the way it was. He insisted on primary sources with proven authenticity.

The term Whig history was coined by Herbert Butterfield in his short book "The Whig Interpretation of History" in 1931, (a reference to the British Whigs, advocates of the power of Parliament) to refer to the approach to historiography that presents the past as an inevitable progression towards ever greater liberty and enlightenment, culminating in modern forms of liberal democracy and constitutional monarchy. In general, Whig historians emphasized the rise of constitutional government, personal freedoms, and scientific progress. The term has been also applied widely in historical disciplines outside of British history (the history of science, for example) to criticize any teleological (or goal-directed), hero-based, and transhistorical narrative. Butterfield's antidote to Whig history was "...to evoke a certain sensibility towards the past, the sensibility which studies the past 'for the sake of the past', which delights in the concrete and the complex, which 'goes out to meet the past', which searches for 'unlikenesses between past and present'." Butterfield's formulation received much attention, and the kind of historical writing he argued against in generalised terms is no longer academically respectable.
The French Annales School radically changed the focus of historical research in France during the 20th century by stressing long-term social history, rather than political or diplomatic themes. The school emphasized the use of quantification and the paying of special attention to geography. An eminent member of this school, Georges Duby, described his approach to history as one that relegated the sensational to the sidelines and was reluctant to give a simple accounting of events, but strived on the contrary to pose and solve problems and, neglecting surface disturbances, to observe the long and medium-term evolution of economy, society, and civilisation.

Marxist historiography developed as a school of historiography influenced by the chief tenets of Marxism, including the centrality of social class and economic constraints in determining historical outcomes. Friedrich Engels wrote "The Condition of the Working Class in England in 1844", which was salient in creating the socialist impetus in British politics from then on, e.g. the Fabian Society. R. H. Tawney's "The Agrarian Problem in the Sixteenth Century" (1912) and "Religion and the Rise of Capitalism" (1926), reflected his ethical concerns and preoccupations in economic history. A circle of historians inside the Communist Party of Great Britain (CPGB) formed in 1946 and became a highly influential cluster of British Marxist historians, who contributed to history from below and class structure in early capitalist society. Members included Christopher Hill, Eric Hobsbawm and E. P. Thompson.

World history, as a distinct field of historical study, emerged as an independent academic field in the 1980s. It focused on the examination of history from a global perspective and looked for common patterns that emerged across all cultures. Arnold J. Toynbee's ten-volume "A Study of History", written between 1933 and 1954, was an important influence on this developing field. He took a comparative topical approach to independent civilizations and demonstrated that they displayed striking parallels in their origin, growth, and decay. William H. McNeill wrote "The Rise of the West" (1965) to improve upon Toynbee by showing how the separate civilizations of Eurasia interacted from the very beginning of their history, borrowing critical skills from one another, and thus precipitating still further change as adjustment between traditional old and borrowed new knowledge and practice became necessary.

An undergraduate history degree is often used as a stepping stone to graduate studies in business or law. Many historians are employed at universities and other facilities for post-secondary education. In addition, it is normal for colleges and universities to require the PhD degree for new full-time hires. A scholarly thesis, such as a PhD, is now regarded as the baseline qualification for a professional historian. However, some historians still gain recognition based on published (academic) works and the award of fellowships by academic bodies like the Royal Historical Society. Publication is increasingly required by smaller schools, so graduate papers become journal articles and PhD dissertations become published monographs. The graduate student experience is difficult—those who finish their doctorate in the United States take on average 8 or more years; funding is scarce except at a few very rich universities. Being a teaching assistant in a course is required in some programs; in others it is a paid opportunity awarded a fraction of the students. Until the 1970s it was rare for graduate programs to teach how to teach; the assumption was that teaching was easy and that learning how to do research was the main mission. A critical experience for graduate students is having a mentor who will provide psychological, social, intellectual and professional support, while directing scholarship and providing an introduction to the profession.

Professional historians typically work in colleges and universities, archival centers, government agencies, museums, and as freelance writers and consultants. The job market for new PhDs in history is poor and getting worse, with many relegated to part-time "adjunct" teaching jobs with low pay and no benefits.






</doc>
<doc id="13577" url="https://en.wikipedia.org/wiki?curid=13577" title="Harthouse">
Harthouse

Harthouse is a German record label specializing in techno music.

The company was founded by Sven Väth in the early 1990s as a sublabel of Eye Q Records with the divisions "Harthouse Frankfurt", "Harthouse UK" and "Harthouse America".

In the beginning of 1997 the future of the label was uncertain, sales were drooping in the wake of rising commercial trance labels, and Sven Väth had left the label in January, causing further confusion. The firm moved from its office in Offenbach to Berlin. Two months later the firm was insolvent, and filed for bankruptcy.

At the beginning of 1998 the Under Cover Music Group (UCMG) took over the rights to use the brand name of the label as well as the trade mark "Harthouse". UCMG put together a "Retrospective Box", a collection of the most successful releases of Harthouse.

Between 1998 and 2003, there were only several new releases.

In early 2003, UCMG started to get into financial problems. In the middle of 2003 Harthouse planned to re-release a set of old classic singles, but after some test vinyl was pressed, UCMG was closed.

In 2004 Daredo Music took over the rights of the Harthouse brand.




</doc>
<doc id="13578" url="https://en.wikipedia.org/wiki?curid=13578" title="Hermann Hesse">
Hermann Hesse

Hermann Karl Hesse (; 2 July 1877 – 9 August 1962) was a German-born Swiss poet, novelist, and painter. His best-known works include "Demian", "Steppenwolf", "Siddhartha", and "The Glass Bead Game", each of which explores an individual's search for authenticity, self-knowledge and spirituality. In 1946, he received the Nobel Prize in Literature.

Hermann Karl Hesse was born on 2 July 1877 in the Black Forest town of Calw in Württemberg, German Empire. His grandparents served in India at a mission under the auspices of the Basel Mission, a Protestant Christian missionary society. His grandfather Hermann Gundert compiled the current grammar in Malayalam language, compiled a Malayalam-English dictionary, and also contributed to the work in translating the Bible to Malayalam. Hesse's mother, Marie Gundert, was born at such a mission in India in 1842. In describing her own childhood, she said, "A happy child I was not..." As was usual among missionaries at the time, she was left behind in Europe at the age of four when her parents returned to India.

Hesse's father, Johannes Hesse, the son of a doctor, was born in 1847 in Weissenstein, Governorate of Estonia in the Russian Empire (now Paide, Järva County, Estonia). Johannes Hesse belonged to the Baltic German minority in the Russian-ruled Baltic region: thus his son Hermann was at birth both a citizen of the German Empire and the Russian Empire. Hermann had five siblings, but two of them died in infancy. In 1873, the Hesse family moved to Calw, where Johannes worked for the Calwer Verlagsverein, a publishing house specializing in theological texts and schoolbooks. Marie's father, Hermann Gundert (also the namesake of his grandson), managed the publishing house at the time, and Johannes Hesse succeeded him in 1893.

Hesse grew up in a Swabian Pietist household, with the Pietist tendency to insulate believers into small, deeply thoughtful groups. Furthermore, Hesse described his father's Baltic German heritage as "an important and potent fact" of his developing identity. His father, Hesse stated, "always seemed like a very polite, very foreign, lonely, little-understood guest." His father's tales from Estonia instilled a contrasting sense of religion in young Hermann. "[It was] an exceedingly cheerful, and, for all its Christianity, a merry world... We wished for nothing so longingly as to be allowed to see this Estonia... where life was so paradisiacal, so colourful and happy." Hermann Hesse's sense of estrangement from the Swabian petite bourgeoisie grew further through his relationship with his maternal grandmother Julie Gundert, née Dubois, whose French-Swiss heritage kept her from ever quite fitting in among that milieu.

From childhood, Hesse was headstrong and hard for his family to handle. In a letter to her husband, Hermann's mother Marie wrote: "The little fellow has a life in him, an unbelievable strength, a powerful will, and, for his four years of age, a truly astonishing mind. How can he express all that? It truly gnaws at my life, this internal fighting against his tyrannical temperament, his passionate turbulence [...] God must shape this proud spirit, then it will become something noble and magnificent – but I shudder to think what this young and passionate person might become should his upbringing be false or weak."
Hesse showed signs of serious depression as early as his first year at school.
In his juvenilia collection "Gerbersau," Hesse vividly describes experiences and anecdotes from his childhood and youth in Calw: the atmosphere and adventures by the river, the bridge, the chapel, the houses leaning closely together, hidden nooks and crannies, as well as the inhabitants with their admirable qualities, their oddities, and their idiosyncrasies. The fictional town of Gerbersau is pseudonymous for Calw, imitating the real name of the nearby town of Hirsau. It is derived from the German words "gerber", meaning "tanner," and "aue", meaning "meadow." Calw had a centuries-old leather-working industry, and during Hesse's childhood the tanneries' influence on the town was still very much in evidence. Hesse's favourite place in Calw was the St. Nicholas-Bridge ("Nikolausbrücke"), which is why a Hesse monument was built there in 2002.

Hermann Hesse's grandfather Hermann Gundert, a doctor of philosophy and fluent in multiple languages, encouraged the boy to read widely, giving him access to his library, which was filled with the works of world literature. All this instilled a sense in Hermann Hesse that he was a citizen of the world. His family background became, he noted, "the basis of an isolation and a resistance to any sort of nationalism that so defined my life."

Young Hesse shared a love of music with his mother. Both music and poetry were important in his family. His mother wrote poetry, and his father was known for his use of language in both his sermons and the writing of religious tracts. His first role model for becoming an artist was his half-brother, Theo, who rebelled against the family by entering a music conservatory in 1885. Hesse showed a precocious ability to rhyme, and by 1889–90 had decided that he wanted to be a writer.

In 1881, when Hesse was four, the family moved to Basel, Switzerland, staying for six years and then returning to Calw. After successful attendance at the Latin School in Göppingen, Hesse entered the Evangelical Theological Seminary of Maulbronn Abbey in 1891. The pupils lived and studied at the abbey, one of Germany's most beautiful and well-preserved, attending 41 hours of classes a week. Although Hesse did well during the first months, writing in a letter that he particularly enjoyed writing essays and translating classic Greek poetry into German, his time in Maulbronn was the beginning of a serious personal crisis. In March 1892, Hesse showed his rebellious character, and, in one instance, he fled from the Seminary and was found in a field a day later. Hesse began a journey through various institutions and schools and experienced intense conflicts with his parents. In May, after an attempt at suicide, he spent time at an institution in Bad Boll under the care of theologian and minister Christoph Friedrich Blumhardt. Later, he was placed in a mental institution in Stetten im Remstal, and then a boys' institution in Basel. At the end of 1892, he attended the Gymnasium in Cannstatt, now part of Stuttgart. In 1893, he passed the One Year Examination, which concluded his schooling. The same year, he began spending time with older companions and took up drinking and smoking.

After this, Hesse began a bookshop apprenticeship in Esslingen am Neckar, but quit after three days. Then, in the early summer of 1894, he began a 14-month mechanic apprenticeship at a clock tower factory in Calw. The monotony of soldering and filing work made him turn himself toward more spiritual activities. In October 1895, he was ready to begin wholeheartedly a new apprenticeship with a bookseller in Tübingen. This experience from his youth, especially his time spent at the Seminary in Maulbronn, he returns to later in his novel "Beneath the Wheel".

On 17 October 1895, Hesse began working in the bookshop in Tübingen, which had a specialized collection in theology, philology, and law. Hesse's tasks consisted of organizing, packing, and archiving the books. After the end of each twelve-hour workday, Hesse pursued his own work, and he spent his long, idle Sundays with books rather than friends. Hesse studied theological writings and later Goethe, Lessing, Schiller, and Greek mythology. He also began reading Nietzsche in 1895, and that philosopher's ideas of "dual… impulses of passion and order" in humankind was a heavy influence on most of his novels.

By 1898, Hesse had a respectable income that enabled financial independence from his parents. During this time, he concentrated on the works of the German Romantics, including much of the work from Clemens Brentano, Joseph Freiherr von Eichendorff, Friedrich Hölderlin, and Novalis. In letters to his parents, he expressed a belief that "the morality of artists is replaced by aesthetics".

During this time, he was introduced to the home of Fräulein von Reutern, a friend of his family's. There he met with people his own age. His relationships with his contemporaries were "problematic", in that most of them were now at university. This usually left him feeling awkward in social situations.

In 1896, his poem "Madonna" appeared in a Viennese periodical and Hesse released his first small volume of poetry, "Romantic Songs." In 1897, a published poem of his, "Grand Valse", drew him a fan letter. It was from Helene Voigt, who the next year married Eugen Diederichs, a young publisher. To please his wife, Diederichs agreed to publish Hesse's collection of prose entitled "One Hour After Midnight" in 1898 (although it is dated 1899). Both works were a business failure. In two years, only 54 of the 600 printed copies of "Romantic Songs" were sold, and "One Hour After Midnight" received only one printing and sold sluggishly. Furthermore, Hesse "suffered a great shock" when his mother disapproved of "Romantic Songs" on the grounds that they were too secular and even "vaguely sinful."

From late 1899, Hesse worked in a distinguished antique book shop in Basel. Through family contacts, he stayed with the intellectual families of Basel. In this environment with rich stimuli for his pursuits, he further developed spiritually and artistically. At the same time, Basel offered the solitary Hesse many opportunities for withdrawal into a private life of artistic self-exploration, journeys and wanderings. In 1900, Hesse was exempted from compulsory military service due to an eye condition. This, along with nerve disorders and persistent headaches, affected him his entire life.

In 1901, Hesse undertook to fulfill a long-held dream and travelled for the first time to Italy. In the same year, Hesse changed jobs and began working at the antiquarium Wattenwyl in Basel. Hesse had more opportunities to release poems and small literary texts to journals. These publications now provided honorariums. His new bookstore agreed to publish his next work, "Posthumous Writings and Poems of Hermann Lauscher". In 1902, his mother died after a long and painful illness. He could not bring himself to attend her funeral, stating in a letter to his father: "I think it would be better for us both that I do not come, in spite of my love for my mother."

Due to the good notices that Hesse received for "Lauscher," the publisher Samuel Fischer became interested in Hesse and, with the novel "Peter Camenzind", which appeared first as a pre-publication in 1903 and then as a regular printing by Fischer in 1904, came a breakthrough: from now on, Hesse could make a living as a writer. The novel became popular throughout Germany. Sigmund Freud "praised "Peter Camenzind" as one of his favourite readings."

Having realised he could make a living as a writer, Hesse finally married Maria Bernoulli (of the famous family of mathematicians) in 1904, while her father, who disapproved of their relationship, was away for the weekend. The couple settled down in Gaienhofen on Lake Constance, and began a family, eventually having three sons. In Gaienhofen, he wrote his second novel, "Beneath the Wheel", which was published in 1906. In the following time, he composed primarily short stories and poems. His story "The Wolf", written in 1906–07, was "quite possibly" a foreshadowing of "Steppenwolf".

His next novel, "Gertrude", published in 1910, revealed a production crisis. He had to struggle through writing it, and he later would describe it as "a miscarriage". Gaienhofen was the place where Hesse's interest in Buddhism was re-sparked. Following a letter to Kapff in 1895 entitled "Nirvana", Hesse had ceased alluding to Buddhist references in his work. In 1904, however, Arthur Schopenhauer and his philosophical ideas started receiving attention again, and Hesse discovered theosophy. Schopenhauer and theosophy renewed Hesse's interest in India. Although it was many years before the publication of Hesse's "Siddhartha" (1922), this masterpiece was to be derived from these new influences.

During this time, there also was increased dissonance between him and Maria, and in 1911 Hesse left for a long trip to Sri Lanka and Indonesia. He also visited Sumatra, Borneo, and Burma, but "the physical experience... was to depress him." Any spiritual or religious inspiration that he was looking for eluded him, but the journey made a strong impression on his literary work. Following Hesse's return, the family moved to Bern (1912), but the change of environment could not solve the marriage problems, as he himself confessed in his novel "Rosshalde" from 1914.

At the outbreak of the First World War in 1914, Hesse registered himself as a volunteer with the Imperial army, saying that he could not sit inactively by a warm fireplace while other young authors were dying on the front. He was found unfit for combat duty, but was assigned to service involving the care of prisoners of war. While most poets and authors of the warring countries quickly became embroiled in a tirade of mutual hate, Hesse, seemingly immune to the general war enthusiasm of the time, wrote an essay titled "O Friends, Not These Tones" ("O Freunde, nicht diese Töne"), which was published in the "Neue Zürcher Zeitung", on 3 November. In this essay he appealed to his fellow intellectuals not to fall for nationalistic madness and hatred. Calling for subdued voices and a recognition of Europe's common heritage, Hesse wrote: "[...] That love is greater than hate, understanding greater than ire, peace nobler than war, this exactly is what this unholy World War should burn into our memories, more so than ever felt before." What followed from this, Hesse later indicated, was a great turning point in his life: For the first time, he found himself in the middle of a serious political conflict, attacked by the German press, the recipient of hate mail, and distanced from old friends. However, he did receive support from his friend Theodor Heuss, and the French writer Romain Rolland, who visited Hesse in August 1915. In 1917, Hesse wrote to Rolland, "The attempt...to apply love to matters political has failed."

This public controversy was not yet resolved when a deeper life crisis befell Hesse with the death of his father on 8 March 1916, the serious illness of his son Martin, and his wife's schizophrenia. He was forced to leave his military service and begin receiving psychotherapy. This began for Hesse a long preoccupation with psychoanalysis, through which he came to know Carl Jung personally, and was challenged to new creative heights. Hesse and Jung both later maintained a correspondence with Chilean author, diplomat and Nazi sympathizer Miguel Serrano, who detailed his relationship with both figures in the book "C.G. Jung & Hermann Hesse: A Record of Two Friendships". During a three-week period in September and October 1917, Hesse penned his novel "Demian", which would be published following the armistice in 1919 under the pseudonym Emil Sinclair.

By the time Hesse returned to civilian life in 1919, his marriage had fallen apart. His wife had a severe episode of psychosis, but, even after her recovery, Hesse saw no possible future with her. Their home in Bern was divided, their children were accommodated in boarding houses and by relatives, and Hesse resettled alone in the middle of April in Ticino. He occupied a small farm house near Minusio (close to Locarno), living from 25 April to 11 May in Sorengo. On 11 May, he moved to the town Montagnola and rented four small rooms in a castle-like building, the Casa Camuzzi. Here, he explored his writing projects further; he began to paint, an activity reflected in his next major story, "Klingsor's Last Summer", published in 1920. This new beginning in different surroundings brought him happiness, and Hesse later called his first year in Ticino "the fullest, most prolific, most industrious and most passionate time of my life." In 1922, Hesse's novella "Siddhartha" appeared, which showed the love for Indian culture and Buddhist philosophy that had already developed earlier in his life. In 1924, Hesse married the singer Ruth Wenger, the daughter of the Swiss writer Lisa Wenger and aunt of Méret Oppenheim. This marriage never attained any stability, however.

In 1923, Hesse was granted Swiss citizenship. His next major works, "Kurgast" (1925) and "The Nuremberg Trip" (1927), were autobiographical narratives with ironic undertones and foreshadowed Hesse's following novel, "Steppenwolf", which was published in 1927. In the year of his 50th birthday, the first biography of Hesse appeared, written by his friend Hugo Ball. Shortly after his new successful novel, he turned away from the solitude of "Steppenwolf" and married art historian Ninon Dolbin, née Ausländer. This change to companionship was reflected in the novel "Narcissus and Goldmund", appearing in 1930. In 1931, Hesse left the Casa Camuzzi and moved with Ninon to a large house (Casa Hesse) near Montagnola, which was built according to his wishes.

In 1931, Hesse began planning what would become his last major work, "The Glass Bead Game" (a.k.a. "Magister Ludi"). In 1932, as a preliminary study, he released the novella "Journey to the East". "The Glass Bead Game" was printed in 1943 in Switzerland. He was awarded the Nobel Prize in Literature in 1946.

As reflected in "Demian", and other works, he believed that "for different people, there are different ways to God"; but despite the influence he drew from Indian and Buddhist philosophies, he stated about his parents: “their Christianity, one not preached but lived, was the strongest of the powers that shaped and moulded me".

Hesse observed the rise to power of Nazism in Germany with concern. In 1933, Bertolt Brecht and Thomas Mann made their travels into exile, each aided by Hesse. In this way, Hesse attempted to work against Hitler's suppression of art and literature that protested Nazi ideology. Hesse's third wife was Jewish, and he had publicly expressed his opposition to anti-Semitism long before then. Hesse was criticized not for condemning the Nazi party, but his failure to criticize or support any political idea stemmed from his "politics of detachment [...] At no time did he openly condemn (the Nazis), although his detestation of their politics is beyond question." Nazism, with its blood sacrifice of the individual to the state and the race, represented the opposite of everything he believed in. In March 1933, seven weeks after Hitler took power, Hesse wrote to a correspondent in Germany, "It is the duty of spiritual types to stand alongside the spirit and not to sing along when the people start belting out the patriotic songs their leaders have ordered them to sing." In the nineteen-thirties, Hesse made a quiet statement of resistance by reviewing and publicizing the work of banned Jewish authors, including Franz Kafka. In the late 1930s, German journals stopped publishing Hesse's work, and the Nazis eventually banned it.

"The Glass Bead Game" was Hesse's last novel. During the last twenty years of his life, Hesse wrote many short stories (chiefly recollections of his childhood) and poems (frequently with nature as their theme). Hesse also wrote ironic essays about his alienation from writing (for instance, the mock autobiographies: "Life Story Briefly Told" and "Aus den Briefwechseln eines Dichters") and spent much time pursuing his interest in watercolours. Hesse also occupied himself with the steady stream of letters he received as a result of the Nobel Prize and as a new generation of German readers explored his work. In one essay, Hesse reflected wryly on his lifelong failure to acquire a talent for idleness and speculated that his average daily correspondence exceeded 150 pages. He died on 9 August 1962, aged 85, and was buried in the cemetery at San Abbondio in Montagnola, where Hugo Ball and the conductor Bruno Walter are also buried.

In his time, Hesse was a popular and influential author in the German-speaking world; worldwide fame only came later. Hesse's first great novel, "Peter Camenzind", was received enthusiastically by young Germans desiring a different and more "natural" way of life in this time of great economic and technological progress in the country (see also Wandervogel movement)." Demian" had a strong and enduring influence on the generation returning home from the First World War. Similarly, "The Glass Bead Game", with its disciplined intellectual world of Castalia and the powers of meditation and humanity, captivated Germans' longing for a new order amid the chaos of a broken nation following the loss in the Second World War.

Towards the end of his life, German (born Bavarian) composer Richard Strauss (1864–1949) set three of Hesse's poems to music in his song cycle "Four Last Songs" for soprano and orchestra (composed 1948, first performed posthumously in 1950): "Frühling" ("Spring"), "September", and "Beim Schlafengehen" ("On Going to Sleep").

In the 1950s, Hesse's popularity began to wane, while literature critics and intellectuals turned their attention to other subjects. In 1955, the sales of Hesse's books by his publisher Suhrkamp reached an all-time low. However, after Hesse's death in 1962, posthumously published writings, including letters and previously unknown pieces of prose, contributed to a new level of understanding and appreciation of his works.

By the time of Hesse's death in 1962, his works were still relatively little read in the United States, despite his status as a Nobel laureate. A memorial published in "The New York Times" went so far as to claim that Hesse's works were largely "inaccessible" to American readers. The situation changed in the mid-1960s, when Hesse's works suddenly became bestsellers in the United States. The revival in popularity of Hesse's works has been credited to their association with some of the popular themes of the 1960s counterculture (or hippie) movement. In particular, the quest-for-enlightenment theme of "Siddhartha", "Journey to the East", and "Narcissus and Goldmund" resonated with those espousing counter-cultural ideals. The "magic theatre" sequences in "Steppenwolf" were interpreted by some as drug-induced psychedelia although there is no evidence that Hesse ever took psychedelic drugs or recommended their use. To a large part, the Hesse boom in the United States can be traced back to enthusiastic writings by two influential counter-culture figures: Colin Wilson and Timothy Leary. From the United States, the Hesse renaissance spread to other parts of the world and even back to Germany: more than 800,000 copies were sold in the German-speaking world from 1972 to 1973. In a space of just a few years, Hesse became the most widely read and translated European author of the 20th century. Hesse was especially popular among young readers, a tendency which continues today.

There is a quote from "Demian" on the cover of Santana's 1970 album "Abraxas", revealing the source of the album's title.

Hesse's "Siddhartha" is one of the most popular Western novels set in India. An authorised translation of "Siddhartha" was published in the Malayalam language in 1990, the language that surrounded Hesse's grandfather, Hermann Gundert, for most of his life. A Hermann Hesse Society of India has also been formed. It aims to bring out authentic translations of "Siddhartha" in all Indian languages and has already prepared the Sanskrit,
Malayalam and Hindi translations of "Siddhartha".
One enduring monument to Hesse's lasting popularity in the United States is the Magic Theatre in San Francisco. Referring to "The Magic Theatre for Madmen Only" in "Steppenwolf" (a kind of spiritual and somewhat nightmarish cabaret attended by some of the characters, including Harry Haller), the Magic Theatre was founded in 1967 to perform works by new playwrights. Founded by John Lion, the Magic Theatre has fulfilled that mission for many years, including the world premieres of many plays by Sam Shepard.

There is also a theater in Chicago named after the novel, Steppenwolf Theater.

Throughout Germany, many schools are named after him. The Hermann-Hesse-Literaturpreis is a literary prize associated with the city of Karlsruhe that has been awarded since 1957. Since 1990, the Calw Hermann Hesse Prize has been awarded every two years alternately to a German-language literary journal and a translator of Hesse's work. 
The Internationale Hermann-Hesse-Gesellschaft (unofficial English name: "International Hermann Hesse Society") was founded in 2002 on the 125th birthday of Hesse and began awarding its Hermann Hesse prize in 2017.

Musician Steve Adey adapted the poem "How Heavy the Days" on his 2017 LP ""Do Me a Kindness"".

The band Steppenwolf took its name from Hesse's novel.







</doc>
<doc id="13584" url="https://en.wikipedia.org/wiki?curid=13584" title="History of the Mediterranean region">
History of the Mediterranean region

The Mediterranean Sea was the central superhighway of transport, trade and cultural exchange between diverse peoples encompassing three continents: Western Asia, North Africa, and Southern Europe. The history of the cultures and people of the Mediterranean Basin is important for understanding the origin and development of the Mesopotamian, Egyptian, Canaanite, Phoenician, Hebrew, Carthaginian, Greek, Persian, Thracian, Etruscan, Iberian, Roman, Byzantine, Bulgarian, Arab, Berber, Ottoman, Christian and Islamic cultures.

Lézignan-la-Cèbe in France, Orce in Spain, Monte Poggiolo in Italy and Kozarnika in Bulgaria are amongst the oldest Palaeolithic sites in Europe and are located around the Mediterranean basin.

There is evidence of stone tools on Crete, 130,000 years BC, which proves that early man was capable of using boats to reach the island.

The cultural stage of civilization (organised society structured around urban centers) first arises in Southwest Asia, as an extension of the Neolithic trend, from as early as the 8th millennium BC, of proto-urban centers such as Çatal Hüyük. Urban civilizations proper begin to emerge in the Chalcolithic, in 5th to 4th millennium Egypt and in Mesopotamia.

Gold artifacts in the Balkans appear from the 4th millennium BC, such as those found in a burial site from 4569–4340 BC and one of the most important archaeological sites in world prehistory - the Varna Necropolis near Lake Varna in Bulgaria, thought by one source (La Niece 2009) to be the earliest "well-dated" find of gold artifacts. As of 1990, gold artifacts found at the Wadi Qana cave cemetery of the 4th millennium BC in the West Bank were the earliest from the Levant.

The Bronze Age arises in this region during the final centuries of the 4th millennium. The urban civilizations of the Fertile Crescent now have writing systems and develop bureaucracy, by the mid-3rd millennium leading to the development of the earliest Empires. In the 2nd millennium, the eastern coastlines of the Mediterranean are dominated by the Hittite and Egyptian empires, competing for control over the city states in the Levant (Canaan).

The "Bronze Age collapse" is the transition from the Late Bronze Age to the Early Iron Age, expressed by the collapse of palace economies of the Aegean and Anatolia, which were replaced after a hiatus by the isolated village cultures of the Dark Age period in history of the ancient Near East. Some have gone so far as to call the catalyst that ended the Bronze Age a "catastrophe". The Bronze Age collapse may be seen in the context of a technological history that saw the slow, comparatively continuous spread of iron-working technology in the region, beginning with precocious iron-working in what is now Romania in the 13th and 12th centuries. The cultural collapse of the Mycenaean kingdoms, the Hittite Empire in Anatolia and Syria, and the Egyptian Empire in Syria and Israel, the scission of long-distance trade contacts and sudden eclipse of literacy occurred between 1206 and 1150 BC. In the first phase of this period, almost every city between Troy and Gaza was violently destroyed, and often left unoccupied thereafter (for example, Hattusas, Mycenae, Ugarit). The gradual end of the Dark Age that ensued saw the rise of settled Neo-Hittite Aramaean kingdoms of the mid-10th century BC, and the rise of the Neo-Assyrian Empire.

While the cultural advances during the Bronze Age had mostly been confined to the eastern parts of the Mediterranean, with the Iron Age, the entire coastal region surrounding the Mediterranean now becomes involved, significantly due to the Phoenician expansion from the Levant, beginning in ca. the 12th century. Fernand Braudel remarked in "The Perspective of the World" that Phoenicia was an early example of a "world-economy" surrounded by empires. The high point of Phoenician culture and sea power is usually placed ca. 1200–800 BC. Many of the most important Phoenician settlements had been established long before this: Byblos, Tyre, Sidon, Simyra, Arwad, and Berytus, all appear in the Amarna tablets.

The Phoenicians and the Assyrians transported elements of the Late Bronze Age culture of the Near East to Iron Age Greece and Italy, but also further afield to Northwestern Africa and to Iberia, initiating the beginning of Mediterranean history now known as Classical Antiquity.
They notably spread alphabetic writing, which would become the hallmark of the Mediterranean civilizations of the Iron Age, in contrast to the cuneiform writing of Assyria and the logographic system in the Far East (and later the abugida systems of India).

Two of the most notable Mediterranean civilizations in classical antiquity were the Greek city states and the Phoenicians. The Greeks expanded throughout the Black Sea and south through the Red Sea. The Phoenicians spread through the western Mediterranean reaching North Africa and the Iberian Peninsula. From the 6th century BC up to including the 5th century BC, many of the significant Mediterranean peoples were under Persian rule, making them dominate the Mediterranean during these years. Both the Phoenicians and some of the Greek city states in Asia Minor provided the naval forces of the Achaemenid Persian Empire. Persian dominance ended after the Greco-Persian War in the 5th century BC and Persia was crippled by Macedonia in the 4th century BC. The Odrysian Kingdom existed between the 5th century BC and the 1st century AD as the most important and powerful thracian state formation.

From the 6th century BC up to including the first half of the 4th century BC, many of the significant Mediterranean peoples came under Achaemenid Persian rule, making them dominate the Mediterranean during all these years. The empire, founded by Cyrus the Great, would include Macedonia, Thrace and the western Black sea coast (modern day southeastern and eastern Bulgaria), Egypt, Anatolia, the Phoenician lands, the Levant, and many other basin regions of the Mediterranean later on. Darius the Great (Darius I) is to be credited as the first Achaemenid king to invest in a Persian fleet. Even by then no true "imperial navy" had existed either in Greece or Egypt. Persia would become the first empire, under Darius, to inaugurate and deploy the first regular imperial navy. Both the Phoenicians and the Greeks provided the bulk of the naval forces of the Achaemenid Persian Empire, alongside the Cypriots and Egyptians. Full Persian dominance in the Mediterranean ended after the Greco-Persian War in the 5th century BC, and Persia eventually lost all her influence in the Mediterranean in the late 4th century BC following Alexander's conquests.

In the northernmost part of ancient Greece, in the ancient kingdom of Macedonia, technological and organizational skills were forged with a long history of cavalry warfare. The "hetairoi" (Companion cavalry) was considered the strongest of their time. Under Alexander the Great, this force turned east, and in a series of decisive battles, it routed the Persian forces and took over as the dominant empire of the Mediterranean. Their Macedonia empire included present-day Greece, Bulgaria, Egypt, the Phoenician lands and many other basin regions of the Mediterranean and Asia Minor.

The major centres of the Mediterranean at the time became part of Alexander's empire as a result. His empire quickly disintegrated, and the Middle East, Egypt, and Greece were soon again independent. Alexander's conquests spread Greek knowledge and ideas throughout the region.

These eastern powers soon began to be overshadowed by those farther west. In North Africa, the former Phoenician colony of Carthage rose to dominate its surroundings with an empire that contained many of the former Phoenician holdings. However, it was a city on the Italian Peninsula, Rome, that would eventually dominate the entire Mediterranean basin. Spreading first through Italy, Rome defeated Carthage in the Punic Wars, despite Hannibal's famous efforts against Rome in the Second Punic War.

After the Third Punic War, Rome then became the leading force in the Mediterranean region. The Romans soon spread east, taking Greece, and the Greek heritage played an important role in the Roman Empire. By this point the coastal trading cultures were thoroughly dominant over the inland river valleys that had once been the heart of the great powers. Egyptian power moved from the Nile cities to the coastal ones, especially Alexandria. Mesopotamia became a fringe border region between the Roman Empire and the Persians.

When Augustus founded the Roman Empire, the Mediterranean sea began to be called "Mare Nostrum" (Latin: "Our Sea") by the Romans. Their empire was centered on this sea and all the area was full of commerce and naval development. For the first time in history, an entire sea (the Mediterranean) was free of piracy. For several centuries, the Mediterranean was a "Roman Lake", surrounded on all sides by the empire.

The empire began to crumble, however, in the fifth century and Rome collapsed after 476 CE.

 and 
The Eastern Roman or Byzantine empire began its domination of the Levant during its wars with neighbouring Sassanid Persia. The rule through the 6th century CE saw climatic instability, causing inconsistent production, distribution, and a general economic decline. The Sasanians gained territory on Mediterranean land regularly, but the Eastern Romans remained superior in the Mediterranean sea for centuries. In the first quarter of the 7th century CE, the Sasanians took swaths of the Mediterranean region from the Eastern Romans during the Byzantine–Sasanian War of 602–628, though the Sasanians lost territories by the end of the war. Ultimately, Byzantine domination in the region was forever finished by invasions of the Arabs and later the Turks.

Another power was rising in the east, that of Islam, whilst the Byzantine Roman and Sassanid Persian empires were both weakened by centuries of stalemate warfare during the Roman–Persian Wars. In a series of rapid Muslim conquests, the Arab armies, motivated by Islam and led by the Caliphs and skilled military commanders such as Khalid ibn al-Walid, swept through most of the Middle East; reducing Byzantine lands by more than half and completely engulfing the Persian lands.

The Arab invasions disrupted the trade relations between Western and Eastern Europe while cutting the trade route with Oriental lands. This however had the indirect effect of promoting the trade across the Caspian Sea. The export of grains from Egypt was re-routed towards the Eastern world. Oriental goods like silk and spices were carried from Egypt to ports like Venice and Constantinople by sailors and Jewish merchants. The Viking raids further disrupted the trade in western Europe and brought it to a halt. However, the Norsemen developed the trade from Norway to the White Sea, while also trading in luxury goods from Spain and the Mediterranean. The Byzantines in the mid-8th century retook control of the area around the north-eastern part of the Mediterranean. Venetian ships from the 9th century armed themselves to counter the harassment by Arabs while concentrating trade of oriental goods at Venice.

The powerful and long-lived Bulgarian Empire was the main European rival in the region of the Mediterranean Balkan peninsula between the 7th and the 14th centuries, creating an important cultural, political, linguistic and religious legacy during the Middle Ages. 

In Anatolia, the Muslim expansion was blocked by the still capable Byzantines with the help of the Tervel of Bulgaria. The Byzantine provinces of Roman Syria, North Africa, and Sicily, however, could not mount such a resistance, and the Muslim conquerors swept through those regions. At the far west, they crossed the sea taking Visigothic Hispania before being halted in southern France by the Franks. At its greatest extent, the Arab Empire controlled 3/4 of the Mediterranean region, the only other empire besides the Roman Empire to control most of the Mediterranean Sea. Much of North Africa became a peripheral area to the main Muslim centers in the Middle East, but Al Andalus and Morocco soon broke from this distant control and became highly advanced societies in their own right.
Between 831 and 1071, the Emirate of Sicily was one of the major centres of Islamic culture in the Mediterranean. After its conquest by the Christian Normans, the island developed its own distinct culture with the fusion of Latin and Byzantine influences. Palermo remained a leading artistic and commercial centre of the Mediterranean well into the Middle Ages.

The Fatimids maintained trade relations with the Italian city-states like Amalfi and Genoa before the Crusades, according to the Cairo Geniza documents. A document dated 996 mentions Amalfian merchants living in Cairo. Another letter states that the Genoese had traded with Alexandria. The caliph al-Mustansir had allowed Amalfian merchants to reside in Jerusalem about 1060 in place of the Latin hospice.

Europe was reviving, however, as more organized and centralized states began to form in the later Middle Ages after the Renaissance of the 12th century. Motivated by religion and dreams of conquest, the kings of Europe launched a number of Crusades to try to roll back Muslim power and retake the holy land. The Crusades were unsuccessful in this goal, but they were far more effective in weakening the already tottering Byzantine Empire that began to lose increasing amounts of territory to the Seljuk Turks and later to the Ottoman Turks. They also rearranged the balance of power in the Muslim world as Egypt once again emerged as a major power in the eastern Mediterranean.

The Crusades led to flourishing of trade between Europe and the "outremer" region. Genoa, Venice and Pisa created colonies in regions controlled by the Crusaders and came to control the trade with the Orient. These colonies also allowed them to trade with the Eastern world. Though the fall of the Crusader states and attempts at banning of trade relations with Muslim states by the Popes temporarily disrupted the trade with the Orient, it however continued.

Slavery was a strategic and very important part of all Mediterranean societies during the Middle Ages. The threat of becoming a slave was a constant fear for peasants, fishermen and merchants. Those with money or who had financial backing only feared the lack of support, should they be threatened with abduction for ransom.

There were several things which could happen to people in the Mediterranean region of the Middle Ages:

Emperors would take large numbers of prisoners, parade them through the capital, hold feasts in honour of their capture and parade diplomats in front of them as a display of victory.

The "Repubbliche Marinare" (Maritime republics) of Amalfi, Gaeta, Venice, Genoa, Ancona, Pisa and Ragusa developed their own "empires" in the Mediterranean shores. The Islamic states had never been major naval powers, and trade from the east to Europe was soon in the hands of Italian traders, especially the Genoese and the Venetians, who profited immensely from it. The Republic of Pisa and later the Republic of Ragusa used diplomacy to further trade and maintained a libertarian approach in civil matters to further sentiment in its inhabitants.

The republic of Venice got to dominate the eastern mediterranean shores after the Fourth Crusade.

Between 1275 and 1344 a struggle for the control of the Strait of Gibraltar took place. Featuring the Marinid Sultanate, the Nasrid Kingdom of Granada, the Crown of Castile, the Crown of Aragón, the Kingdom of Portugal and the Republic of Genoa, it was characterized by changing alliances between the main actors. The iberian cities of Tarifa, Ceuta, Algeciras or Ronda and the African port of Ceuta were at stake.
The Western Mediterranean sea was dominated by the Crown of Aragon: thanks to their possessions of Sicily, the Kingdom of Naples, the Kingdom of Sardinia, the Balearic Islands, the Duchy of Athens the Duchy of Neopatria, and several northern African cities.

In 1347 the Black Death spread from Constantinople across the mediterranean basin.

Ottoman power continued to grow, and in 1453, the Byzantine Empire was extinguished with the fall of Constantinople. The Ottomans already controlled Greece, Bulgaria and much of the Balkans and soon also began to spread through North Africa. North Africa had grown wealthy from the trade across the Sahara Desert, but the Portuguese, who, along with other Christian powers, had been engaged in a long campaign to evict the Muslims from Iberia, had found a method to circumvent this trade by trading directly with West Africa. This was enabled by a new type of ships, the caravel, that made trade in the rough Atlantic waters profitable for the first time. The reduction in the Saharan trade weakened North Africa, and made them an easy target for the Ottomans.

Ceuta was ultimately taken by the Kingdom of Portugal in 1415, searching to undermine Castilian, Aragonese, and Genoese interests in the area.

During the Middle Ages, rival Christian and Muslim kingdoms forbade the trade of particular goods to enemy kingdoms including weaponry and other contraband items. The popes forbade the export of these commodities to the Islamic world. The Ottomans too forbade the export of weapons and other strategic items, declaring them "memnu eşya" or "memnu olan" to Christian states even in peace treaties, however friendly states could import some of the prohibited goods through capitulations. Despite these prohibitions, trade of contraband occurred on both sides. The European merchants traded in illegal goods with Muslims. The Ottomans were unable to suppress the trade with smuggling being undertaken mainly in the winter when the Ottoman Navy stationed at the Istanbul Arsenal was unable to stop Ottoman and non-Ottoman vessels from indulging in the trade.

The growing naval prowess of the European powers confronted further rapid Ottoman expansion in the region when the Battle of Lepanto checked the power of the Ottoman navy. However, as Braudel argued forcefully, this only slowed the Ottoman expansion instead of ending it. The prized island of Cyprus became Ottoman in 1571. The last resistance in Tunisia ended in 1574 and almost a generation long siege in Crete pushed Venetians out of this strategic island in 1669.

A balance of power was then established between Spain and Ottoman Empire until the 18th century, each dominating their respective half of Mediterranean, reducing Italian navies as naval powers increasingly more irrelevant. Furthermore, the Ottoman Empire had succeeded in their objective of extending Muslim rule across the North African coast.

The development of long range seafaring had an influence upon the entire Mediterranean. While once all trade from the east had passed through the region, the circumnavigation of Africa allowed gold, spices, and dyes to be imported directly to the Atlantic ports of western Europe. The Americas were also a source of extreme wealth to the western powers, from which some of the Mediterranean states were largely cut off.

The base of European power thus shifted northward and the once wealthy Italy became a peripheral area dominated by foreigners. The Ottoman Empire also began a slow decline that saw its North African possessions gain de facto independence and its European holdings gradually reduced by the increasing power of Austria and Russia.

By the nineteenth century the European States were vastly more powerful, and began to colonize North Africa. France spread its power south by taking Algeria in 1830 and later Tunisia. Britain gained control of Egypt in 1882. Italy conquered Libya from the Ottomans in 1911. Greece achieved independence in 1832. The Ottoman Empire finally collapsed in the First World War, and its holdings were carved up among France and Britain. The rump state of the wider Ottoman Empire became the independent state of Turkey in 1923. Yugoslavia was created from the former Austro-Hungarian empire at the end of the First World War.

During the first half of the twentieth century the Mediterranean was at the center of the expansion of the Kingdom of Italy, and was one of the main areas of battle during World War II between the Axis and the Allies. Post-world war period was marked by increasing activity in the Eastern Mediterranean, where naval actions formed part of ongoing Arab–Israeli conflict and Turkey had occupied the northern part of Cyprus. Cold War tensions split the Mediterranean into pro-American and pro-Soviet factions, with Turkey, Greece, Spain, Italy and France being NATO members. Syria was socialist and a pro-Soviet regime, offering the Soviets a port for their navy from an agreement in 1971. Yugoslavia was Communist but in neither the Soviet nor American camps. Egypt tilted towards the Soviets during the time of Nasser but then turned towards American influence during the time of Sadat. Israel and Egypt both received massive American military aid. American naval power made the Mediterranean a base for the United States Sixth Fleet during the Cold war.

Today, the Mediterranean Sea is the southern border of the European Union and represents one of the largest area by Trade in the World. The Maltese prime minister described the Mediterranean sea as a "cemetery" due to the large amounts of migrants who drown there. Following the 2013 Lampedusa migrant shipwreck, the Italian government, has decided to strengthen the national system for the patrolling of the Mediterranean Sea by authorizing Operation Mare Nostrum, a military and humanitarian operation in order to rescue the migrants and arrest the traffickers of immigrants.






</doc>
<doc id="13585" url="https://en.wikipedia.org/wiki?curid=13585" title="Hugo de Garis">
Hugo de Garis

Hugo de Garis (born 1947, Sydney, Australia) is a retired researcher in the sub-field of artificial intelligence (AI) known as evolvable hardware. He became known in the 1990s for his research on the use of genetic algorithms to evolve artificial neural networks using three-dimensional cellular automata inside field programmable gate arrays. He claimed that this approach would enable the creation of what he terms "artificial brains" which would quickly surpass human levels of intelligence.

He has been noted for his belief that a major war between the supporters and opponents of intelligent machines, resulting in billions of deaths, is almost inevitable before the end of the 21st century. He suggests AIs may simply eliminate the human race, and humans would be powerless to stop them because of technological singularity. This prediction has attracted debate and criticism from the AI research community, and some of its more notable members, such as Kevin Warwick, Bill Joy, Ken MacLeod, Ray Kurzweil, and Hans Moravec, have voiced their opinions on whether or not this future is likely.

De Garis originally studied theoretical physics, but he abandoned this field in favour of artificial intelligence. In 1992 he received his PhD from Université Libre de Bruxelles, Belgium. He worked as a researcher at ATR (Advanced Telecommunications Research Institute International, ), Japan from 1994–2000, a researcher at Starlab, Brussels from 2000–2001, and associate professor of computer science at Utah State University from 2001–2006. Until his retirement in late 2010 he was a professor at Xiamen University, where he taught theoretical physics and computer science, and ran the Artificial Brain Lab.

From 1993 to 2000 de Garis participated in a research project at ATR's Human Information Processing Research Laboratories (ATR-HIP) which aimed to create a billion-neuron artificial brain by the year 2001. The project was known as "cellular automata machine brain," or "CAM-Brain." During this 8-year span he and his fellow researchers published a series of papers in which they discussed the use of genetic algorithms to evolve neural structures inside 3D cellular automata. They argued that existing neural models had failed to produce intelligent behaviour because they were too small, and that in order to create "artificial brains" it was necessary to manually assemble tens of thousands of evolved neural modules together, with the billion neuron "CAM-Brain" requiring around 10 million modules; this idea was rejected by Igor Aleksander, who said "The point is that these puzzles are not puzzles because our neural models are not large enough."

Though it was initially envisaged that these cellular automata would run on special computers, such as MIT's "Cellular Automata Machine-8" (CAM-8), by 1996 it was realised that the model originally proposed, which required cellular automata with thousands of states, was too complex to be realised in hardware. The design was considerably simplified, and in 1997 the "collect and distribute 1 bit" ("CoDi-1Bit") model was published, and work began on a hardware implementation using Xilinx XC6264 FPGAs. This was to be known as the "CAM Brain Machine" (CBM).

The researchers evolved cellular automata for several tasks (using software simulation, not hardware):

Ultimately the project failed to produce a functional robot control system, and ATR terminated it along with the closure of ATR-HIP in February 2001.

The original aim of de Garis' work was to establish the field of "brain building" (a term of his invention) and to "create a trillion dollar industry within 20 years". Throughout the 90s his papers claimed that by 2001 the ATR "Robokoneko" (translation: kitten robot) project would develop a billion-neuron "cellular automata machine brain" (CAM-brain), with "computational power equivalent to 10,000 pentiums" that could simulate the brain of a real cat. de Garis received a US$0.4 million "fat brain building grant" to develop this. The first "CAM-brain" was delivered to ATR in 1999. After receiving a further US$1 million grant at Starlab de Garis failed to deliver a working "brain" before Starlab's bankruptcy. At USU de Garis announced he was establishing a "brain builder" group to create a second generation "CAM-brain".

de Garis published his last "CAM-Brain" research paper in 2002. He still works on evolvable hardware. Using a Celoxica FPGA board he says he can create up to 50,000 neural network modules for less than $3000.

Since 2002 he has co-authored several papers on evolutionary algorithms.

He believes that topological quantum computing is about to revolutionize computer science, and hopes that his teaching will help his students to understand its principles.

In 2008 de Garis received a 3 million Chinese yuan grant (around $436,000) to build an artificial brain for China (the "China-Brain Project"), as part of the "Brain Builder Group" at Wuhan University.

Hugo de Garis retired in 2010. Before that he was director of the artificial brains lab at Xiamen University in China. In 2013 he was studying Maths and Physics at PhD level and over the next 20 years plans to publish 500 graduate level free lecture videos. This is called "degarisMPC" and some lectures are already available.

de Garis's original work on "CAM-brain" machines was part of an 8-year research project, from 1993 to 2000, at the ATR Human Information Processing Research Laboratories (ATR-HIP) in Kyoto Prefecture, Japan. de Garis left in 2000, and ATR-HIP was closed on 28 February 2001. de Garis then moved to Starlab in Brussels, where he received a million dollars in funding from the government of Belgium ("over a third of the Brussels government's total budget for scientific research", according to de Garis). Starlab went bankrupt in June 2001. A few months later de Garis was employed as an associate professor at the computer science department of Utah State University. In May 2006 he became a professor at Wuhan University's international school of software, teaching graduate level pure mathematics, theoretical physics and computer science.

Since June 2006 he has been a member of the advisory board of Novamente, a commercial company which aims to create artificial general intelligence.

Hugo De Garis believes that a major war before the end of the 21st century, resulting in billions of deaths, is almost inevitable. Intelligent machines (or "artilects", a shortened form of "artificial intellects") will be far more intelligent than humans and will threaten to attain world domination, resulting in a conflict between "Cosmists", who support the artilects, and "Terrans", who oppose them (both of these are terms of his invention). He describes this conflict as a "gigadeath" war, reinforcing the point that billions of people will be killed. This scenario has been criticised by other AI researchers, including Chris Malcolm, who described it as "entertaining science fiction horror stories which happen to have caught the attention of the popular media". Kevin Warwick called it a "hellish nightmare, as portrayed in films such as the Terminator".
In 2005, de Garis published a book describing his views on this topic entitled "The Artilect War: Cosmists vs. Terrans: A Bitter Controversy Concerning Whether Humanity Should Build Godlike Massively Intelligent Machines".

Cosmism is a moral philosophy that favours building or growing strong artificial intelligence and ultimately leaving Earth to the Terrans, who oppose this path for humanity. The first half of the book describes technologies which he believes will make it possible for computers to be billions or trillions of times more intelligent than humans. He predicts that as artificial intelligence improves and becomes progressively more human-like, differing views will begin to emerge regarding how far such research should be allowed to proceed. Cosmists will foresee the massive, truly astronomical potential of substrate-independent cognition, and will therefore advocate unlimited growth in the designated fields, in the hopes that "super intelligent" machines might one day colonise the universe. It is this "cosmic" view of history, in which the fate of one single species, on one single planet, is seen as insignificant next to the fate of the known universe, that gives the Cosmists their name. Hugo identifies with that group and noted that it "would be a cosmic tragedy if humanity freezes evolution at the puny human level".

Terrans, on the other hand, will have a more "terrestrial" Earth-centred view, in which the fate of the Earth and its species (like humanity) are seen as being all-important. To Terrans, a future without humans is to be avoided at all costs, as it would represent "the" worst-case scenario. As such, Terrans will find themselves unable to ignore the possibility that super intelligent machines might one day cause the destruction of the human race—being very immensely intelligent and so cosmically inclined, these artilect machines may have no more moral or ethical difficulty in exterminating humanity than humans do in using medicines to cure diseases. So, Terrans will see themselves as living during the closing of a window of opportunity, to disable future artilects before they are built, after which humans will no longer have a say in the affairs of intelligent machines.

It is these two extreme ideologies which de Garis believes may herald a new world war, wherein one group with a "grand plan" (the Cosmists) will be rabidly opposed by another which feels itself to be under deadly threat from that plan (the Terrans). The factions, he predicts, may eventually war to the death because of this, as the Terrans will come to view the Cosmists as "arch-monsters" when they begin seriously discussing acceptable risks, and the probabilities of large percentages of Earth-based life going extinct. In response to this, the Cosmists will come to view the Terrans as being reactionary extremists, and will stop treating them and their ideas seriously, further aggravating the situation, possibly beyond reconciliation.

Throughout his book, de Garis states that he is ambivalent about which viewpoint he ultimately supports, and attempts to make convincing cases for both sides. He elaborates towards the end of the book that the more he thinks about it, the more he feels like a Cosmist, because he feels that despite the horrible possibility that humanity might ultimately be destroyed, perhaps inadvertently or at least indifferently, by the artilects, he cannot ignore the fact that the human species is just another link in the evolutionary chain, and must become extinct in their current form anyway, whereas the artilects could very well be the "next" link in that chain and therefore would be excellent candidates to carry the torch of science and exploration forward into the rest of the universe.

He relates a morally isomorphic scenario in which extraterrestrial intelligences visit the earth three billion years ago and discover two domains of life living there, one domain which is "older" but "simpler" and contemporarily dominant, but which upon closer study appears to be incapable of much further evolutionary development; and one "younger" domain which is struggling to survive, but which upon further study displays the potential to evolve into all the varieties of life existing on the Earth today, including humanity, and then queries the reader as to whether they would feel ethically compelled to destroy the dominant domain of life to ensure the survival of the younger one, or to destroy the younger one in order to ensure the survival of the older and more populous domain which was "there first". He states that he believes that, like himself, most of the public would feel torn or at least ambivalent about the outcome of artilects at first, but that as the technology advances, the issue would be forced and most would feel compelled to choose a side, and that as such the public consciousness of the coming issue should be raised now so that society can "choose", hopefully before the factions becomes irreconcilably polarised, which outcome it prefers.

He also predicts a third group that will emerge between the two. He refers to this third party as Cyborgians or "Cyborgs", because they will not be opposed to artilects as such, but desire to become artilects themselves by adding components to their own human brains, rather than falling into obsolescence. They will seek to become artilects by gradually merging themselves with machines and think that the dichotomy between the Cosmists and Terrans can be avoided because all human beings would become artilects.
The transhumanist movement are usually identified as Cyborgians.

His concept of the Cyborgians might have stemmed from a conversation with Kevin Warwick: in 2000, de Garis noted, "Just out of curiosity, I asked Kevin Warwick whether he was a Terran or a Cosmist. He said he was against the idea of artilects being built (i.e., he is Terran). I was surprised, and felt a shiver go up my spine. That moment reminded me of a biography of Lenin that I had read in my 20s in which the Bolsheviks and the Mensheviks first started debating the future government of Russia. What began as an intellectual difference ended up as a Russian civil war after 1917 between the white and the red Russians".




In recent years, De Garis has become vocal in the Masculist and Men Going Their Own Way (MGTOW) movements. He is a believer in anti-semitic conspiracy theories and has written (and presented on YouTube) a series of essays on the subject.




</doc>
<doc id="13586" url="https://en.wikipedia.org/wiki?curid=13586" title="HTTPS">
HTTPS

Hypertext Transfer Protocol Secure (HTTPS) is an extension of the Hypertext Transfer Protocol (HTTP). It is used for secure communication over a computer network, and is widely used on the Internet. In HTTPS, the communication protocol is encrypted using Transport Layer Security (TLS) or, formerly, Secure Sockets Layer (SSL). The protocol is therefore also referred to as HTTP over TLS, or HTTP over SSL.

The principal motivations for HTTPS are authentication of the accessed website, and protection of the privacy and integrity of the exchanged data while in transit. It protects against man-in-the-middle attacks, and the bidirectional encryption of communications between a client and server protects the communications against eavesdropping and tampering. In practice, this provides a reasonable assurance that one is communicating with the intended website without interference from attackers.

The authentication aspect of HTTPS requires a trusted third party to sign server-side digital certificates. This was historically an expensive operation, which meant fully authenticated HTTPS connections were usually found only on secured payment transaction services and other secured corporate information systems on the World Wide Web. In 2016, a campaign by the Electronic Frontier Foundation with the support of web browser developers led to the protocol becoming more prevalent. HTTPS is now used more often by web users than the original non-secure HTTP, primarily to protect page authenticity on all types of websites; secure accounts; and to keep user communications, identity, and web browsing private.

The Uniform Resource Identifier (URI) scheme "HTTPS" has identical usage syntax to the HTTP scheme. However, HTTPS signals the browser to use an added encryption layer of SSL/TLS to protect the traffic. SSL/TLS is especially suited for HTTP, since it can provide some protection even if only one side of the communication is authenticated. This is the case with HTTP transactions over the Internet, where typically only the server is authenticated (by the client examining the server's certificate).

HTTPS creates a secure channel over an insecure network. This ensures reasonable protection from eavesdroppers and man-in-the-middle attacks, provided that adequate cipher suites are used and that the server certificate is verified and trusted.

Because HTTPS piggybacks HTTP entirely on top of TLS, the entirety of the underlying HTTP protocol can be encrypted. This includes the request URL (which particular web page was requested), query parameters, headers, and cookies (which often contain identifying information about the user). However, because website addresses and port numbers are necessarily part of the underlying TCP/IP protocols, HTTPS cannot protect their disclosure. In practice this means that even on a correctly configured web server, eavesdroppers can infer the IP address and port number of the web server, and sometimes even the domain name (e.g. www.example.org, but not the rest of the URL) that a user is communicating with, along with the amount of data transferred and the duration of the communication, though not the content of the communication.

Web browsers know how to trust HTTPS websites based on certificate authorities that come pre-installed in their software. Certificate authorities are in this way being trusted by web browser creators to provide valid certificates. Therefore, a user should trust an HTTPS connection to a website if and only if all of the following are true:


HTTPS is especially important over insecure networks and networks that may be subject to tampering. Insecure networks, such as public Wi-Fi access points, allow anyone on the same local network to packet-sniff and discover sensitive information not protected by HTTPS. Additionally, some free-to-use and paid WLAN networks have been observed tampering with webpages by engaging in packet injection in order to serve their own ads on other websites. This practice can be exploited maliciously in many ways, such as by injecting malware onto webpages and stealing users' private information.

HTTPS is also important for connections over the Tor anonymity network, as malicious Tor nodes could otherwise damage or alter the contents passing through them in an insecure fashion and inject malware into the connection. This is one reason why the Electronic Frontier Foundation and the Tor project started the development of HTTPS Everywhere, which is included in the Tor Browser Bundle.

As more information is revealed about global mass surveillance and criminals stealing personal information, the use of HTTPS security on all websites is becoming increasingly important regardless of the type of Internet connection being used. Even though metadata about individual pages that a user visits might not be considered sensitive, when aggregated it can reveal a lot about the user and compromise the user's privacy.

Deploying HTTPS also allows the use of HTTP/2 (or its predecessor, the now-deprecated protocol SPDY), which is a new generation of HTTP designed to reduce page load times, size, and latency.

It is recommended to use HTTP Strict Transport Security (HSTS) with HTTPS to protect users from man-in-the-middle attacks, especially SSL stripping.

HTTPS should not be confused with the seldom-used Secure HTTP (S-HTTP) specified in RFC 2660.

, 33.2% of Alexa top 1,000,000 websites use HTTPS as default, 57.1% of the Internet's 137,971 most popular websites have a secure implementation of HTTPS, and 70% of page loads (measured by Firefox Telemetry) use HTTPS.

Most browsers display a warning if they receive an invalid certificate. Older browsers, when connecting to a site with an invalid certificate, would present the user with a dialog box asking whether they wanted to continue. Newer browsers display a warning across the entire window. Newer browsers also prominently display the site's security information in the address bar. Extended validation certificates turn the address bar green in newer browsers. Most browsers also display a warning to the user when visiting a site that contains a mixture of encrypted and unencrypted content. Additionally, many web filters return a security warning when visiting prohibited websites.
The Electronic Frontier Foundation, opining that "In an ideal world, every web request could be defaulted to HTTPS", has provided an add-on called HTTPS Everywhere for Mozilla Firefox, Google Chrome, Chromium, and Android, that enables HTTPS by default for hundreds of frequently used websites.

The security of HTTPS is that of the underlying TLS, which typically uses long-term public and private keys to generate a short-term session key, which is then used to encrypt the data flow between the client and the server. X.509 certificates are used to authenticate the server (and sometimes the client as well). As a consequence, certificate authorities and public key certificates are necessary to verify the relation between the certificate and its owner, as well as to generate, sign, and administer the validity of certificates. While this can be more beneficial than verifying the identities via a web of trust, the 2013 mass surveillance disclosures drew attention to certificate authorities as a potential weak point allowing man-in-the-middle attacks. An important property in this context is forward secrecy, which ensures that encrypted communications recorded in the past cannot be retrieved and decrypted should long-term secret keys or passwords be compromised in the future. Not all web servers provide forward secrecy.

For HTTPS to be effective, a site must be completely hosted over HTTPS. If some of the site's contents are loaded over HTTP (scripts or images, for example), or if only a certain page that contains sensitive information, such as a log-in page, is loaded over HTTPS while the rest of the site is loaded over plain HTTP, the user will be vulnerable to attacks and surveillance. Additionally, cookies on a site served through HTTPS must have the secure attribute enabled. On a site that has sensitive information on it, the user and the session will get exposed every time that site is accessed with HTTP instead of HTTPS.

HTTPS URLs begin with "https://" and use port 443 by default, whereas, HTTP URLs begin with "http://" and use port 80 by default.

HTTP is not encrypted and thus is vulnerable to man-in-the-middle and eavesdropping attacks, which can let attackers gain access to website accounts and sensitive information, and modify webpages to inject malware or advertisements. HTTPS is designed to withstand such attacks and is considered secure against them (with the exception of HTTPS implementations that use deprecated versions of SSL).

HTTP operates at the highest layer of the TCP/IP model, the Application layer; as does the TLS security protocol (operating as a lower sublayer of the same layer), which encrypts an HTTP message prior to transmission and decrypts a message upon arrival. Strictly speaking, HTTPS is not a separate protocol, but refers to the use of ordinary HTTP over an encrypted SSL/TLS connection.

HTTPS encrypts all message contents, including the HTTP headers and the request/response data. With the exception of the possible CCA cryptographic attack described in the limitations section below, an attacker should at most be able to discover that a connection is taking place between two parties, along with their domain names and IP addresses.

To prepare a web server to accept HTTPS connections, the administrator must create a public key certificate for the web server. This certificate must be signed by a trusted certificate authority for the web browser to accept it without warning. The authority certifies that the certificate holder is the operator of the web server that presents it. Web browsers are generally distributed with a list of signing certificates of major certificate authorities so that they can verify certificates signed by them.

A number of commercial certificate authorities exist, offering paid-for SSL/TLS certificates of a number of types, including Extended Validation Certificates.

Let's Encrypt, launched in April 2016, provides free and automated service that delivers basic SSL/TLS certificates to websites. According to the Electronic Frontier Foundation, Let's Encrypt will make switching from HTTP to HTTPS "as easy as issuing one command, or clicking one button." The majority of web hosts and cloud providers now leverage Let's Encrypt, providing free certificates to their customers.

The system can also be used for client authentication in order to limit access to a web server to authorized users. To do this, the site administrator typically creates a certificate for each user, which the user loads into their browser. Normally, the certificate contains the name and e-mail address of the authorized user and is automatically checked by the server on each connection to verify the user's identity, potentially without even requiring a password.

An important property in this context is perfect forward secrecy (PFS). Possessing one of the long-term asymmetric secret keys used to establish an HTTPS session should not make it easier to derive the short-term session key to then decrypt the conversation, even at a later time. Diffie–Hellman key exchange (DHE) and Elliptic curve Diffie–Hellman key exchange (ECDHE) are in 2013 the only schemes known to have that property. In 2013, only 30% of Firefox, Opera, and Chromium Browser sessions used it, and nearly 0% of Apple's Safari and Microsoft Internet Explorer sessions. TLS 1.3, published in August 2018, dropped support for ciphers without forward secrecy. , 96.6% of web servers surveyed support some form of forward secrecy, and 52.1% will use forward secrecy with most browsers.

A certificate may be revoked before it expires, for example because the secrecy of the private key has been compromised. Newer versions of popular browsers such as Firefox, Opera, and Internet Explorer on Windows Vista implement the Online Certificate Status Protocol (OCSP) to verify that this is not the case. The browser sends the certificate's serial number to the certificate authority or its delegate via OCSP (Online Certificate Status Protocol) and the authority responds, telling the browser whether the certificate is still valid or not.The CA may also issue a CRL to tell people that these certificates are revoked.

SSL (Secure Sockets Layer) and TLS (Transport Layer Security) encryption can be configured in two modes: "simple" and "mutual". In simple mode, authentication is only performed by the server. The mutual version requires the user to install a personal client certificate in the web browser for user authentication. In either case, the level of protection depends on the correctness of the implementation of the software and the cryptographic algorithms in use.

SSL/TLS does not prevent the indexing of the site by a web crawler, and in some cases the URI of the encrypted resource can be inferred by knowing only the intercepted request/response size. This allows an attacker to have access to the plaintext (the publicly available static content), and the encrypted text (the encrypted version of the static content), permitting a cryptographic attack.

Because TLS operates at a protocol level below that of HTTP and has no knowledge of the higher-level protocols, TLS servers can only strictly present one certificate for a particular address and port combination. In the past, this meant that it was not feasible to use name-based virtual hosting with HTTPS. A solution called Server Name Indication (SNI) exists, which sends the hostname to the server before encrypting the connection, although many old browsers do not support this extension. Support for SNI is available since Firefox 2, Opera 8, Apple Safari 2.1, Google Chrome 6, and Internet Explorer 7 on Windows Vista.

From an architectural point of view:


A sophisticated type of man-in-the-middle attack called SSL stripping was presented at the 2009 Blackhat Conference. This type of attack defeats the security provided by HTTPS by changing the link into an link, taking advantage of the fact that few Internet users actually type "https" into their browser interface: they get to a secure site by clicking on a link, and thus are fooled into thinking that they are using HTTPS when in fact they are using HTTP. The attacker then communicates in clear with the client. This prompted the development of a countermeasure in HTTP called HTTP Strict Transport Security.

HTTPS has been shown vulnerable to a range of traffic analysis attacks. Traffic analysis attacks are a type of side-channel attack that relies on variations in the timing and size of traffic in order to infer properties about the encrypted traffic itself. Traffic analysis is possible because SSL/TLS encryption changes the contents of traffic, but has minimal impact on the size and timing of traffic. In May 2010, a research paper by researchers from Microsoft Research and Indiana University discovered that detailed sensitive user data can be inferred from side channels such as packet sizes. The researchers found that, despite HTTPS protection in several high-profile, top-of-the-line web applications in healthcare, taxation, investment, and web search, an eavesdropper could infer the illnesses/medications/surgeries of the user, his/her family income, and investment secrets. Although this work demonstrated the vulnerability of HTTPS to traffic analysis, the approach presented by the authors required manual analysis and focused specifically on web applications protected by HTTPS.

The fact that most modern websites, including Google, Yahoo!, and Amazon, use HTTPS causes problems for many users trying to access public Wi-Fi hot spots, because a Wi-Fi hot spot login page fails to load if the user tries to open an HTTPS resource. Several websites, such as neverssl.com and nonhttps.com, guarantee that they will always remain accessible by HTTP.

Netscape Communications created HTTPS in 1994 for its Netscape Navigator web browser. Originally, HTTPS was used with the SSL protocol. As SSL evolved into Transport Layer Security (TLS), HTTPS was formally specified by RFC 2818 in May 2000. Google announced in February 2018 that its Chrome browser would mark HTTP sites as "Not Secure" after July 2018. This move was to encourage website owners to implement HTTPS, as an effort to make the World Wide Web more secure.




</doc>
<doc id="13588" url="https://en.wikipedia.org/wiki?curid=13588" title="History of Egypt">
History of Egypt

The history of Egypt has been long and wealthy, due to the flow of the Nile River with its fertile banks and delta, as well as the accomplishments of Egypt's native inhabitants and outside influence. Much of Egypt's ancient history was a mystery until Egyptian hieroglyphs were deciphered with the discovery and help of the Rosetta Stone. Among the Seven Wonders of the Ancient World, is the Great Pyramid of Giza. The Library of Alexandria was the only one of its kind for centuries.

One of the earliest human structures in the world was found in Egypt, dating to about 100,000 BC. Ancient Egyptian civilization coalesced around 3150 BC with the political unification of Upper and Lower Egypt under the first pharaoh of the First Dynasty, Narmer. Predominantly native Egyptian rule lasted until the conquest by the Achaemenid Empire in the sixth century BC.

In 332 BC, Macedonian ruler Alexander the Great conquered Egypt as he toppled the Achaemenids and established the Hellenistic Ptolemaic Kingdom, whose first ruler was one of Alexander's former generals, Ptolemy I Soter. The Ptolemies had to fight native rebellions and were involved in foreign and civil wars that led to the decline of the kingdom and its final annexation by Rome. The death of Cleopatra ended the nominal independence of Egypt resulting in Egypt's becoming one of the provinces of the Roman Empire.

Roman rule in Egypt (including Byzantine) lasted from 30 BC to 641 AD, with a brief interlude of control by the Sasanian Empire between 619–629, known as Sasanian Egypt. After the Muslim conquest of Egypt, parts of Egypt became provinces of successive Caliphates and other Muslim dynasties: Rashidun Caliphate (632-661), Umayyad Caliphate (661–750), Abbasid Caliphate (750–935), Fatimid Caliphate (909–1171), Ayyubid Sultanate (1171–1260), and the Mamluk Sultanate (1250–1517). In 1517, Ottoman sultan Selim I captured Cairo, absorbing Egypt into the Ottoman Empire.

Egypt remained entirely Ottoman until 1867, except during French occupation from 1798 to 1801. Starting in 1867, Egypt became a nominally autonomous tributary state called the Khedivate of Egypt. However, Khedivate Egypt fell under British control in 1882 following the Anglo-Egyptian War. After the end of World War I and following the Egyptian revolution of 1919, the Kingdom of Egypt was established. While a "de jure" independent state, the United Kingdom retained control over foreign affairs, defense, and other matters. British occupation lasted until 1954, with the Anglo-Egyptian agreement of 1954.

The modern Republic of Egypt was founded in 1953, and with the complete withdrawal of British forces from the Suez Canal in 1956, it marked the first time in 2500 years that Egypt was both fully independent and ruled by native Egyptians. President Gamal Abdel Nasser (president from 1956 to 1970) introduced many reforms and created the short-lived United Arab Republic (with Syria). His terms also saw the Six-Day War and the creation of the international Non-Aligned Movement. His successor, Anwar Sadat (president from 1970 to 1981) changed Egypt's trajectory, departing from many of the political, and economic tenets of Nasserism, re-instituting a multi-party system and launching the Infitah economic policy. He led Egypt in the Yom Kippur War of 1973 to regain Egypt's Sinai Peninsula, which Israel had occupied since the Six-Day War of 1967. This later led to the Egypt–Israel Peace Treaty.

Recent Egyptian history has been dominated by events following nearly thirty years of rule by the former president Hosni Mubarak. The Egyptian revolution of 2011 deposed Mubarak and resulted in the first democratically elected president in Egyptian history, Mohamed Morsi. Unrest after the 2011 revolution and related disputes led to the 2013 Egyptian coup d'état.

There is evidence of petroglyphs along the Nile terraces and in desert oases. In the 10th millennium BC, a culture of hunter-gatherers and fishermen was replaced by a grain-grinding culture. Climate changes and/or overgrazing around 6000 BC began to desiccate the pastoral lands of Egypt, forming the Sahara. Early tribal peoples migrated to the Nile River, where they developed a settled agricultural economy and more centralized society.

By about 6000 BC, a Neolithic culture rooted in the Nile Valley. During the Neolithic era, several predynastic cultures developed independently in Upper and Lower Egypt. The Badari culture and the successor Naqada series are generally regarded as precursors to dynastic Egypt. The earliest known Lower Egyptian site, Merimda, predates the Badarian by about seven hundred years. Contemporaneous Lower Egyptian communities coexisted with their southern counterparts for more than two thousand years, remaining culturally distinct, but maintaining frequent contact through trade. The earliest known evidence of Egyptian hieroglyphic inscriptions appeared during the predynastic period on Naqada III pottery vessels, dated to about 3200 BC.

A unified kingdom was formed in 3150 BC by King Menes, leading to a series of dynasties that ruled Egypt for the next three millennia. Egyptian culture flourished during this long period and remained distinctively Egyptian in its religion, arts, language and customs. The first two ruling dynasties of a unified Egypt set the stage for the Old Kingdom period ("c". 2700–2200 BC), which constructed many pyramids, most notably the Third Dynasty pyramid of Djoser and the Fourth Dynasty Giza Pyramids.

The First Intermediate Period ushered in a time of political upheaval for about 150 years. Stronger Nile floods and stabilization of government, however, brought back renewed prosperity for the country in the Middle Kingdom "c". 2040 BC, reaching a peak during the reign of Pharaoh Amenemhat III. A second period of disunity heralded the arrival of the first foreign ruling dynasty in Egypt, that of the Semitic-speaking Hyksos. The Hyksos invaders took over much of Lower Egypt around 1650 BC and founded a new capital at Avaris. They were driven out by an Upper Egyptian force led by Ahmose I, who founded the Eighteenth Dynasty and relocated the capital from Memphis to Thebes.

The New Kingdom ("c". 1550–1070 BC) began with the Eighteenth Dynasty, marking the rise of Egypt as an international power that expanded during its greatest extension to an empire as far south as Tombos in Nubia, and included parts of the Levant in the east. This period is noted for some of the most well known Pharaohs, including Hatshepsut, Thutmose III, Akhenaten and his wife Nefertiti, Tutankhamun and Ramesses II. The first historically attested expression of monotheism came during this period as Atenism, although some consider Atenism to be a form of monolatry rather than of monotheism. Frequent contacts with other nations brought new ideas to the New Kingdom. The country was later invaded and conquered by Libyans, Nubians and Assyrians, but native Egyptians eventually drove them out and regained control of their country.

In the sixth century BC, the Achaemenid Empire conquered Egypt. The entire Twenty-seventh Dynasty of Egypt, from 525 BC to 402 BC, save for Petubastis III, was an entirely Persian-ruled period, with the Achaemenid kings being granted the title of pharaoh. The Thirtieth Dynasty was the last native ruling dynasty during the Pharaonic epoch. It fell to the Persians again in 343 BC after the last native Pharaoh, King Nectanebo II, was defeated in battle.

The Thirty-first Dynasty of Egypt, also known as the Second Egyptian Satrapy, was effectively a short-living province of the Achaemenid Empire between 343 BC to 332 BC. After an interval of independence, during which three indigenous dynasties reigned (the 28th, 29th and 30th dynasty), Artaxerxes III (358–338 BC) reconquered the Nile valley for a brief second period (343–332 BC), which is called the Thirty-first Dynasty of Egypt, thus starting another period of pharaohs of Persian origin.

A team led by Johannes Krause managed the first reliable sequencing of the genomes of 90 mummified individuals in 2017. Whilst not conclusive, because of the non-exhaustive time frame and restricted location that the mummies represent, their study nevertheless showed that these Ancient Egyptians "closely resembled ancient and modern Near Eastern populations, especially those in the Levant, and had almost no DNA from sub-Saharan Africa. What's more, the genetics of the mummies remained remarkably consistent even as different powers—including Nubians, Greeks, and Romans—conquered the empire".

The Ptolemaic Kingdom was a powerful Hellenistic state extending from southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia. Alexandria became the capital city and a center of Greek culture and trade. To gain recognition by the native Egyptian populace, they named themselves as the successors to the Pharaohs. The later Ptolemies took on Egyptian traditions, had themselves portrayed on public monuments in Egyptian style and dress, and participated in Egyptian religious life.

The last ruler from the Ptolemaic dynasty was Cleopatra, who committed suicide following the burial of her lover Mark Antony, who had died in her arms (from a self-inflicted stab wound) after Augustus had captured Alexandria and her mercenary forces had fled.

The Ptolemies faced rebellions of native Egyptians, often caused by an unwanted regime, and were involved in foreign and civil wars that led to the decline of the kingdom and its annexation by Rome. Nevertheless, Hellenistic culture continued to thrive in Egypt well after the Muslim conquest. The native Egyptian/Coptic culture continued to exist as well (the Coptic language itself was Egypt's most widely spoken language until at least the 10th century).

Egypt quickly became the Empire's breadbasket supplying the greater portion of the Empire's grain in addition to flax, papyrus, glass, and many other finished goods. The city of Alexandria became a key trading outpost for the Roman Empire (by some accounts, the most important for a time). Shipping from Egypt regularly reached India and Ethiopia among other international destinations. It was also a leading (perhaps "the" leading) scientific and technological center of the Empire. Scholars such as Ptolemy, Hypatia, and Heron broke new ground in astronomy, mathematics, and other disciplines. Culturally, the city of Alexandria at times rivaled Rome in its importance.

Christianity reached Egypt relatively early in the evangelist period of the first century (traditionally credited to Mark the Evangelist). Alexandria, Egypt and Antioch, Syria quickly became the leading centers of Christianity. Diocletian's reign marked the transition from the classical Roman to the Late antique/Byzantine era in Egypt, when a great number of Egyptian Christians were persecuted. The New Testament had by then been translated into Egyptian. After the Council of Chalcedon in AD 451, a distinct Egyptian Coptic Church was firmly established.

Sasanian Egypt (known in Middle Persian sources as "Agiptus") refers to the brief rule of Egypt and parts of Libya by the Sasanian Empire, which lasted from 619 to 629, until the Sasanian rebel Shahrbaraz made an alliance with the Byzantine emperor Heraclius and had control over Egypt returned to him.

The Byzantines were able to regain control of the country after a brief Persian invasion early in the 7th century, until 639–42, when Egypt was invaded and conquered by the Arab Islamic Empire. The final loss of Egypt was of incalculable significance to the Byzantine Empire, which had relied on Egypt for many agricultural and manufactured goods.
When they defeated the Byzantine Armies in Egypt, the Arabs brought Sunni Islam to the country. Early in this period, Egyptians began to blend their new faith with their Christian traditions as well as other indigenous beliefs and practices, leading to various Sufi orders that have flourished to this day. These earlier rites had survived the period of Coptic Christianity.

Muslim rulers nominated by the Islamic Caliphate remained in control of Egypt for the next six centuries, with Cairo as the seat of the Caliphate under the Fatimids. With the end of the Kurdish Ayyubid dynasty, the Mamluks, a Turco-Circassian military caste, took control about AD 1250. By the late 13th century, Egypt linked the Red Sea, India, Malaya, and East Indies. The Greek and Coptic languages and cultures went into a steep decline in favor of Arabic culture (though Coptic managed to last as a spoken language until the 17th century and remains a liturgical language today). 

The Mamluks continued to govern the country until the conquest of Egypt by the Ottoman Turks in 1517, after which it became a province of the Ottoman Empire. The mid-14th-century Black Death killed about 40% of the Egypt's population.

After the 15th century, the Ottoman invasion pushed the Egyptian system into decline. The defensive militarization damaged its civil society and economic institutions. The weakening of the economic system combined with the effects of plague left Egypt vulnerable to foreign invasion. Portuguese traders took over their trade. Egypt suffered six famines between 1687 and 1731. The 1784 famine cost it roughly one-sixth of its population.

The brief French invasion of Egypt led by Napoleon Bonaparte began in 1798. The campaign eventually led to the discovery of the Rosetta Stone, creating the field of Egyptology. Despite early victories and an initially successful expedition into Syria, Napoleon and his Armée d'Orient were eventually defeated and forced to withdraw, especially after suffering the defeat of the supporting French fleet at the Battle of the Nile.

The expulsion of the French in 1801 by Ottoman, Mamluk, and British forces was followed by four years of anarchy in which Ottomans, Mamluks, and Albanians — who were nominally in the service of the Ottomans – wrestled for power. Out of this chaos, the commander of the Albanian regiment, Muhammad Ali (Kavalali Mehmed Ali Pasha) emerged as a dominant figure and in 1805 was acknowledged by the Sultan in Istanbul as his viceroy in Egypt; the title implied subordination to the Sultan but this was in fact a polite fiction: Ottoman power in Egypt was finished and Muhammad Ali, an ambitious and able leader, established a dynasty that was to rule Egypt until the revolution of 1952. After 1882 the dynasty became a British puppet.

Ali's primary focus was military: he annexed Northern Sudan (1820–1824), Syria (1833), and parts of Arabia and Anatolia; but in 1841 the European powers, fearful lest he topple the Ottoman Empire itself, forced him to return most of his conquests to the Ottomans, but he kept the Sudan and his title to Egypt was made hereditary. A more lasting result of his military ambition is that it required him to modernize the country. Eager to adopt the military (and therefore industrial) techniques of the great powers, he sent students to the West and invited training missions to Egypt. He built industries, a system of canals for irrigation and transport, and reformed the civil service.

The introduction in 1820 of long-staple cotton, the Egyptian variety of which became notable, transformed its agriculture into a cash-crop monoculture before the end of the century. The social effects of this were enormous: land ownership became concentrated and many foreigners arrived, shifting production towards international markets.

British indirect rule lasted from 1882, when the British succeeded in defeating the Egyptian Army at Tel el-Kebir in September and took control of the country, to the 1952 Egyptian revolution which made Egypt a republic and when British advisers were expelled.

Muhammad Ali was succeeded briefly by his son Ibrahim (in September 1848), then by a grandson Abbas I (in November 1848), then by Said (in 1854), and Isma'il (in 1863).

Abbas I was cautious. Said and Ismail were ambitious developers, but they spent beyond their means. The Suez Canal, built in partnership with the French, was completed in 1869. The cost of this and other projects had two effects: it led to enormous debt to European banks, and caused popular discontent because of the onerous taxation it required. In 1875 Ismail sold Egypt's 44% share in the canal to the British Government. Within three years this led to the imposition of British and French controllers who sat in the Egyptian cabinet, and, "with the financial power of the bondholders behind them, were the real power in the Government."

Local dissatisfaction with Ismail and with European intrusion led to the formation of the first nationalist groupings in 1879, with Ahmad Urabi a prominent figure. In 1882 he became head of a nationalist-dominated ministry committed to democratic reforms including parliamentary control of the budget. Fearing a reduction of their control, Britain and France intervened militarily, bombarding Alexandria and crushing the Egyptian army at the battle of Tel el-Kebir. They reinstalled Ismail's son Tewfik as figurehead of a "de facto" British protectorate.

In 1914, the Protectorate was made official, and the Ottoman Empire no longer had a role. The title for the head of state, which in 1867 had changed from "pasha" to "khedive", was changed again to "sultan". Abbas II was deposed as khedive and replaced by his uncle, Hussein Kamel, as sultan.

In 1906, the Dinshaway Incident prompted many neutral Egyptians to join the nationalist movement. After the First World War, Saad Zaghlul and the Wafd Party led the Egyptian nationalist movement to a majority at the local Legislative Assembly. When the British exiled Zaghlul and his associates to Malta on 8 March 1919, the country arose in its first modern revolution. The revolt led the UK government to issue a unilateral declaration of Egypt's independence on 22 February 1922.

The new government drafted and implemented a constitution in 1923 based on a parliamentary system. Saad Zaghlul was popularly elected as Prime Minister of Egypt in 1924. In 1936, the Anglo-Egyptian Treaty was concluded. Continued instability due to remaining British influence and increasing political involvement by the king led to the dissolution of the parliament in a military "coup d'état" known as the 1952 Revolution. The Free Officers Movement forced King Farouk to abdicate in support of his son Fuad.

British military presence in Egypt lasted until 1954.

On 18 June 1953, the Egyptian Republic was declared, with General Muhammad Naguib as the first President of the Republic. Naguib was forced to resign in 1954 by Gamal Abdel Nasserthe real architect of the 1952 movementand was later put under house arrest.

Nasser assumed power as President in June 1956. British forces completed their withdrawal from the occupied Suez Canal Zone on 13 June 1956. He nationalized the Suez Canal on 26 July 1956, prompting the 1956 Suez Crisis.

In 1958, Egypt and Syria formed a sovereign union known as the United Arab Republic. The union was short-lived, ending in 1961 when Syria seceded, thus ending the union. During most of its existence, the United Arab Republic was also in a loose confederation with North Yemen (the Mutawakkilite Kingdom of Yemen) known as the United Arab States.

In the 1967 Six-Day War, Israel invaded and occupied Egypt's Sinai Peninsula and the Gaza Strip, which Egypt had occupied since the 1948 Arab–Israeli War. Three years later (1970), President Nasser died and was succeeded by Anwar Sadat.

Sadat switched Egypt's Cold War allegiance from the Soviet Union to the United States, expelling Soviet advisors in 1972. He launched the Infitah economic reform policy, while clamping down on religious and secular opposition.

In 1973, Egypt, along with Syria, launched the October War, a surprise attack against the Israeli forces occupying the Sinai Peninsula and the Golan Heights. It was an attempt to regain part of the Sinai territory that Israel had captured six years earlier. Sadat hoped to seize some territory through military force, and then regain the rest of the peninsula by diplomacy. The conflict sparked an international crisis between the US and the USSR, both of whom intervened. The second UN-mandated ceasefire halted military action. While the war ended with a military stalemate, it presented Sadat with a political victory that later allowed him to regain the Sinai in return for peace with Israel.

Sadat made a historic visit to Israel in 1977, which led to the 1979 peace treaty in exchange for Israeli withdrawal from Sinai. Sadat's initiative sparked enormous controversy in the Arab world and led to Egypt's expulsion from the Arab League, but it was supported by most Egyptians. On 6 October 1981, Sadat and six diplomats were assassinated while observing a military parade commemorating the eighth anniversary of the October 1973 War. He was succeeded by Hosni Mubarak.

In 1980s, 1990s, and 2000s, terrorist attacks in Egypt became numerous and severe, and began to target Copts and foreign tourists as well as government officials. Some scholars and authors have credited Islamist writer Sayyid Qutb, who was executed in 1967, as the inspiration for the new wave of attacks.

The 1990s saw an Islamist group, al-Gama'a al-Islamiyya, engage in an extended campaign of violence, from the murders and attempted murders of prominent writers and intellectuals, to the repeated targeting of tourists and foreigners. Serious damage was done to the largest sector of Egypt's economy—tourism—and in turn to the government, but it also devastated the livelihoods of many of the people on whom the group depended for support.

Victims of the campaign against the Egyptian state from 1992–1997 exceeded 1,200 and included the head of the counter-terrorism police (Major General Raouf Khayrat), a speaker of parliament (Rifaat el-Mahgoub), dozens of European tourists and Egyptian bystanders, and over 100 Egyptian police. At times, travel by foreigners in parts of Upper Egypt was severely restricted and dangerous. On 17 November 1997, 62 people, mostly tourists, were killed near Luxor. The assailants trapped the people in the Mortuary Temple of Hatshepsut. During this period, Al-Gama'a al-Islamiyya was given support by the governments of Iran and Sudan, as well as al-Qaeda. The Egyptian government received support during that time from the United States.

In 2003, the "Kefaya" ("Egyptian Movement for Change"), was launched to oppose the Mubarak regime and to establish democratic reforms and greater civil liberties.
On 25 January 2011, widespread protests began against Mubarak's government. The objective of the protest was the removal of Mubarak from power. These took the form of an intensive campaign of civil resistance supported by a very large number of people and mainly consisting of continuous mass demonstrations. By 29 January, it was becoming clear that Mubarak's government had lost control when a curfew order was ignored, and the army took a semi-neutral stance on enforcing the curfew decree.

On 11 February 2011, Mubarak resigned and fled Cairo. Vice President Omar Suleiman announced that Mubarak had stepped down and that the Egyptian military would assume control of the nation's affairs in the short term. Jubilant celebrations broke out in Tahrir Square at the news. Mubarak may have left Cairo for Sharm el-Sheikh the previous night, before or shortly after the airing of a taped speech in which Mubarak vowed he would not step down or leave.

On 13 February 2011, the high level military command of Egypt announced that both the constitution and the parliament of Egypt had been dissolved. The parliamentary election was to be held in September.

A constitutional referendum was held on 19 March 2011. On 28 November 2011, Egypt held its first parliamentary election since the Mubarak regime fell. Turnout was high and there were no reports of violence, although members of some parties broke the ban on campaigning at polling places by handing out pamphlets and banners. There were, however, complaints of irregularities.

The first round of a presidential election was held in Egypt on 23 and 24 May 2012. Mohamed Morsi won 25% of the vote and Ahmed Shafik, the last prime minister under deposed leader Hosni Mubarak, 24%. A second round was held on 16 and 17 June. On 24 June 2012, the election commission announced that Mohamed Morsi had won the election, making him the first democratically elected president of Egypt. According to official results, Morsi took 51.7 percent of the vote while Shafik received 48.3 percent.

On 8 July 2012, Egypt's new president Mohamed Morsi announced he was overriding the military edict that dissolved the country's elected parliament and called lawmakers back into session.

On 10 July 2012, the Supreme Constitutional Court of Egypt negated the decision by Morsi to call the nation's parliament back into session. On 2 August 2012, Egypt's Prime Minister Hisham Qandil announced his 35-member cabinet, including 28 newcomers, of whom four came from the influential Muslim Brotherhood while six and the former interim military ruler Mohamed Hussein Tantawi as the Defence Minister came from the previous Government.

On 22 November 2012, Morsi issued a declaration immunizing his decrees from challenge and seeking to protect the work of the constituent assembly drafting the new constitution. The declaration also requires a retrial of those accused in the Mubarak-era killings of protesters, who had been acquitted, and extends the mandate of the constituent assembly by two months. Additionally, the declaration authorizes Morsi to take any measures necessary to protect the revolution. Liberal and secular groups previously walked out of the constitutional constituent assembly because they believed that it would impose strict Islamic practices, while Muslim Brotherhood backers threw their support behind Morsi.

The move was criticized by Mohamed ElBaradei, the leader of Egypt's Constitution Party, who stated "Morsi today usurped all state powers & appointed himself Egypt's new pharaoh" on his Twitter feed. The move led to massive protests and violent action throughout Egypt. On 5 December 2012, Tens of thousands of supporters and opponents of Egypt's president clashed, hurling rocks and Molotov cocktails and brawling in Cairo's streets, in what was described as the largest violent battle between Islamists and their foes since the country's revolution. Six senior advisors and three other officials resigned from the government and the country's leading Islamic institution called on Morsi to stem his powers. Protesters also clamored from coastal cities to desert towns.

Morsi offered a "national dialogue" with opposition leaders but refused to cancel a 15 December vote on a draft constitution written by an Islamist-dominated assembly that has ignited two weeks of political unrest.

A constitutional referendum was held in two rounds on 15 and 22 December 2012, with 64% support, and 33% against. It was signed into law by a presidential decree issued by Morsi on 26 December 2012. On 3 July 2013, the constitution was suspended by order of the Egyptian army.

On 30 June 2013, on the first anniversary of the election of Morsi, millions of protesters across Egypt took to the streets and demanded the immediate resignation of the president. On 1 July, the Egyptian Armed Forces issued a 48-hour ultimatum that gave the country's political parties until 3 July to meet the demands of the Egyptian people. The presidency rejected the Egyptian Army's 48-hour ultimatum, vowing that the president would pursue his own plans for national reconciliation to resolve the political crisis. On 3 July, General Abdel Fattah el-Sisi, head of the Egyptian Armed Forces, announced that he had removed Morsi from power, suspended the constitution and would be calling new presidential and Shura Council elections and named Supreme Constitutional Court's leader, Adly Mansour as acting president. Mansour was sworn in on 4 July 2013.

During the months after the coup d'état, a new constitution was prepared, which took effect on 18 January 2014. After that, presidential and parliamentary elections have to be held in June 2014. On 24 March 2014, 529 Morsi's supporters were sentenced to death, while the trial of Morsi himself was still ongoing. Having delivered a final judgement, 492 sentences were commuted to life imprisonment with 37 death sentences being upheld. On 28 April, another mass trial took place with 683 Morsi supporters sentenced to death for killing 1 police officer. In 2015, Egypt participated in the Saudi Arabian-led intervention in Yemen.

In the elections of June 2014 El-Sisi won with a percentage of 96.1%. Under President el-Sisi, Egypt has implemented a rigorous policy of controlling the border to the Gaza Strip, including the dismantling of tunnels between the Gaza strip and Sinai.




</doc>
<doc id="13590" url="https://en.wikipedia.org/wiki?curid=13590" title="House">
House

A house is a single-unit residential building, which may range in complexity from a rudimentary hut to a complex, structure of wood, masonry, concrete or other material, outfitted with plumbing, electrical, and heating, ventilation, and air conditioning systems. Houses use a range of different roofing systems to keep precipitation such as rain from getting into the dwelling space. Houses may have doors or locks to secure the dwelling space and protect its inhabitants and contents from burglars or other trespassers. Most conventional modern houses in Western cultures will contain one or more bedrooms and bathrooms, a kitchen or cooking area, and a living room. A house may have a separate dining room, or the eating area may be integrated into another room. Some large houses in North America have a recreation room. In traditional agriculture-oriented societies, domestic animals such as chickens or larger livestock (like cattle) may share part of the house with humans. 

The social unit that lives in a house is known as a household. Most commonly, a household is a family unit of some kind, although households may also be other social groups, such as roommates or, in a rooming house, unconnected individuals. Some houses only have a dwelling space for one family or similar-sized group; larger houses called townhouses or row houses may contain numerous family dwellings in the same structure. A house may be accompanied by outbuildings, such as a garage for vehicles or a shed for gardening equipment and tools. A house may have a backyard or front yard, which serve as additional areas where inhabitants can relax or eat.

The English word "house" derives directly from the Old English "hus" meaning "dwelling, shelter, home, house," which in turn derives from Proto-Germanic "husan" (reconstructed by etymological analysis) which is of unknown origin. The house itself gave rise to the letter 'B' through an early Proto-Semitic hieroglyphic symbol depicting a house. The symbol was called "bayt", "bet" or "beth" in various related languages, and became "beta", the Greek letter, before it was used by the Romans. "Beit" in Arabic means house, while in Maltese "bejt" refers to the roof of the house.

Ideally, architects of houses design rooms to meet the needs of the people who will live in the house. Feng shui, originally a Chinese method of moving houses according to such factors as rain and micro-climates, has recently expanded its scope to address the design of interior spaces, with a view to promoting harmonious effects on the people living inside the house, although no actual effect has ever been demonstrated. Feng shui can also mean the "aura" in or around a dwelling, making it comparable to the real estate sales concept of "indoor-outdoor flow".

The square footage of a house in the United States reports the area of "living space", excluding the garage and other non-living spaces. The "square metres" figure of a house in Europe reports the area of the walls enclosing the home, and thus includes any attached garage and non-living spaces. The number of floors or levels making up the house can affect the square footage of a home.
Humans often build houses for domestic or wild animals, often resembling smaller versions of human domiciles. Familiar animal houses built by humans include birdhouses, henhouses and doghouses, while housed agricultural animals more often live in barns and stables.

Many houses have several large rooms with specialized functions and several very small rooms for other various reasons. These may include a living/eating area, a sleeping area, and (if suitable facilities and services exist) separate or combined washing and lavatory areas. Some larger properties may also feature rooms such as a spa room, indoor pool, indoor basketball court, and other 'non-essential' facilities. In traditional agriculture-oriented societies, domestic animals such as chickens or larger livestock often share part of the house with humans. Most conventional modern houses will at least contain a bedroom, bathroom, kitchen or cooking area, and a living room.
The names of parts of a house often echo the names of parts of other buildings, but could typically include:

Little is known about the earliest origin of the house and its interior, however it can be traced back to the simplest form of shelters. Roman architect Vitruvius' theories have claimed the first form of architecture as a frame of timber branches finished in mud, also known as the primitive hut.
Philip Tabor later states the contribution of 17th century Dutch houses as the foundation of houses today.
In the Middle Ages, the Manor Houses facilitated different activities and events. Furthermore, the houses accommodated numerous people, including family, relatives, employees, servants and their guests. Their lifestyles were largely communal, as areas such as the Great Hall enforced the custom of dining and meetings and the Solar intended for shared sleeping beds.

During the 15th and 16th centuries, the Italian Renaissance Palazzo consisted of plentiful rooms of connectivity. Unlike the qualities and uses of the Manor Houses, most rooms of the palazzo contained no purpose, yet were given several doors. These doors adjoined rooms in which Robin Evans describes as a "matrix of discrete but thoroughly interconnected chambers." The layout allowed occupants to freely walk room to room from one door to another, thus breaking the boundaries of privacy. 

An early example of the segregation of rooms and consequent enhancement of privacy may be found in 1597 at the Beaufort House built in Chelsea, London. It was designed by English architect John Thorpe who wrote on his plans, "A Long Entry through all". The separation of the passageway from the room developed the function of the corridor. This new extension was revolutionary at the time, allowing the integration of one door per room, in which all universally connected to the same corridor. English architect Sir Roger Pratt states "the common way in the middle through the whole length of the house, [avoids] the offices from one molesting the other by continual passing through them." Social hierarchies within the 17th century were highly regarded, as architecture was able to epitomize the servants and the upper class. More privacy is offered to the occupant as Pratt further claims, "the ordinary servants may never publicly appear in passing to and fro for their occasions there." This social divide between rich and poor favored the physical integration of the corridor into housing by the 19th century.

Sociologist Witold Rybczynski wrote, "the subdivision of the house into day and night uses, and into formal and informal areas, had begun." Rooms were changed from public to private as single entryways forced notions of entering a room with a specific purpose.

Compared to the large scaled houses in England and the Renaissance, the 17th Century Dutch house was smaller, and was only inhabited by up to four to five members. This was because they embraced "self-reliance" in contrast to the dependence on servants, and a design for a lifestyle centered on the family. It was important for the Dutch to separate work from domesticity, as the home became an escape and a place of comfort. This way of living and the home has been noted as highly similar to the contemporary family and their dwellings. 

By the end of the 17th century, the house layout was transformed to become employment-free, enforcing these ideas for the future. This came in favour for the industrial revolution, gaining large-scale factory production and workers. The house layout of the Dutch and its functions are still relevant today.

 In the American context, some professions, such as doctors, in the 19th and early 20th century typically operated out of the front room or parlor or had a two-room office on their property, which was detached from the house. By the mid-20th-century, the increase in high-tech equipment created a marked shift whereby the contemporary doctor typically worked from an office or hospital.
The introduction of technology and electronic systems within the house has questioned the impressions of privacy as well as the segregation of work from home. Technological advances of surveillance and communications allow insight of personal habits and private lives. As a result, the "private becomes ever more public, [and] the desire for a protective home life increases, fuelled by the very media that undermine it," writes Jonathan Hill. Work has been altered by the increase of communications. The "deluge of information", has expressed the efforts of work, conveniently gaining access inside the house. Although commuting is reduced, the desire to separate working and living remains apparent. On the other hand, some architects have designed homes in which eating, working and living are brought together.

In many parts of the world, houses are constructed using scavenged materials. In Manila's Payatas neighborhood, slum houses are often made of material sourced from a nearby garbage dump. In Dakar, it is common to see houses made of recycled materials standing atop a mixture of garbage and sand which serves as a foundation. The garbage-sand mixture is also used to protect the house from flooding.

In the United States, modern house construction techniques include light-frame construction (in areas with access to supplies of wood) and adobe or sometimes rammed-earth construction (in arid regions with scarce wood-resources). Some areas use brick almost exclusively, and quarried stone has long provided foundations and walls. To some extent, aluminum and steel have displaced some traditional building materials. Increasingly popular alternative construction materials include insulating concrete forms (foam forms filled with concrete), structural insulated panels (foam panels faced with oriented strand board or fiber cement), light-gauge steel, and steel framing. More generally, people often build houses out of the nearest available material, and often tradition or culture govern construction-materials, so whole towns, areas, counties or even states/countries may be built out of one main type of material. For example, a large portion of American houses use wood, while most British and many European houses use stone, brick, or mud.
In the early 20th century, some house designers started using prefabrication. Sears, Roebuck & Co. first marketed their Sears Catalog Homes to the general public in 1908. Prefab techniques became popular after World War II. First small inside rooms framing, then later, whole walls were prefabricated and carried to the construction site. The original impetus was to use the labor force inside a shelter during inclement weather. More recently, builders have begun to collaborate with structural engineers who use finite element analysis to design prefabricated steel-framed homes with known resistance to high wind loads and seismic forces. These newer products provide labor savings, more consistent quality, and possibly accelerated construction processes.

Lesser-used construction methods have gained (or regained) popularity in recent years. Though not in wide use, these methods frequently appeal to homeowners who may become actively involved in the construction process. They include:
In the developed world, energy-conservation has grown in importance in house design. Housing produces a major proportion of carbon emissions (studies have show that it is 30% of the total in the United Kingdom).

Development of a number of types and techniques continues. They include the zero-energy house, the passive solar house, the autonomous buildings, the superinsulated and houses built to the "Passivhaus" standard.

Buildings with historical importance have legal restrictions. New houses in the UK are not covered by the Sale of Goods Act. When purchasing a new house the buyer has different legal protection than when buying other products. New houses in the UK are covered by a National House Building Council guarantee.

With the growth of dense settlement, humans designed ways of identifying houses and parcels of land. Individual houses sometimes acquire proper names, and those names may acquire in their turn considerable emotional connotations. For example, the house of "Howards End" or the castle of "Brideshead Revisited". A more systematic and general approach to identifying houses may use various methods of house numbering.

Houses may express the circumstances or opinions of their builders or their inhabitants. Thus, a vast and elaborate house may serve as a sign of conspicuous wealth whereas a low-profile house built of recycled materials may indicate support of energy conservation. Houses of particular historical significance (former residences of the famous, for example, or even just very old houses) may gain a protected status in town planning as examples of built heritage or of streetscape. Commemorative plaques may mark such structures. Home ownership provides a common measure of prosperity in economics. Contrast the importance of house-destruction, tent dwelling and house rebuilding in the wake of many natural disasters.









</doc>
<doc id="13593" url="https://en.wikipedia.org/wiki?curid=13593" title="Java applet">
Java applet

Java applets were small applications written in the Java programming language, or another programming language that compiles to Java bytecode, and delivered to users in the form of Java bytecode. The user launched the Java applet from a web page, and the applet was then executed within a Java virtual machine (JVM) in a process separate from the web browser itself. A Java applet could appear in a frame of the web page, a new application window, Sun's AppletViewer, or a stand-alone tool for testing applets.

Java applets were introduced in the first version of the Java language, which was released in 1995. Beginning in 2013, major web browsers began to phase out support for the underlying technology applets used to run, with applets becoming completely unable to be run by 2015–2017. Java applets were deprecated since Java 9 in 2017 and removed from Java SE 11 (18.9), released in September 2018.

Java applets were usually written in Java, but other languages such as Jython, JRuby, Pascal, Scala, or Eiffel (via SmartEiffel) could be used as well.

Java applets run at very fast speeds and until 2011, they were many times faster than JavaScript. Unlike JavaScript, Java applets had access to 3D hardware acceleration, making them well-suited for non-trivial, computation-intensive visualizations. As browsers have gained support for hardware-accelerated graphics thanks to the canvas technology (or specifically WebGL in the case of 3D graphics), as well as just-in-time compiled JavaScript, the speed difference has become less noticeable.

Since Java bytecode is cross-platform (or platform independent), Java applets can be executed by browsers (or other clients) for many platforms, including Microsoft Windows, FreeBSD, Unix, macOS and Linux. They cannot be run on modern mobile devices, which do not support Java.

The applets are used to provide interactive features to web applications that cannot be provided by HTML alone. They can capture mouse input and also have controls like buttons or check boxes. In response to user actions, an applet can change the provided graphic content. This makes applets well-suited for demonstration, visualization, and teaching. There are online applet collections for studying various subjects, from physics to heart physiology.

An applet can also be a text area only; providing, for instance, a cross-platform command-line interface to some remote system. If needed, an applet can leave the dedicated area and run as a separate window. However, applets have very little control over web page content outside the applet's dedicated area, so they are less useful for improving the site appearance in general, unlike other types of browser extensions (while applets like news tickers or WYSIWYG editors are also known). Applets can also play media in formats that are not natively supported by the browser.

Pages coded in HTML may embed parameters within them that are passed to the applet. Because of this, the same applet may have a different appearance depending on the parameters that were passed.

As applets were available before CSS and DHTML were standard, they were also widely used for trivial effects such as rollover navigation buttons. This approach, which posed major problems for accessibility and misused system resources, is no longer in use and was strongly discouraged even at the time.

Java applets are executed in a "sandbox" by most web browsers, preventing them from accessing local data like the clipboard or file system. The code of the applet is downloaded from a web server, after which the browser either embeds the applet into a web page or opens a new window showing the applet's user interface.

A Java applet extends the class , or in the case of a Swing applet, . The class which must override methods from the applet class to set up a user interface inside itself (codice_1) is a descendant of which is a descendant of . As applet inherits from container, it has largely the same user interface possibilities as an ordinary Java application, including regions with user specific visualization. 

The first implementations involved downloading an applet class by class. While classes are small files, there are often many of them, so applets got a reputation as slow-loading components. However, since .jars were introduced, an applet is usually delivered as a single file that has a size similar to an image file (hundreds of kilobytes to several megabytes).

The domain from where the applet executable has been downloaded is the only domain to which the usual (unsigned) applet is allowed to communicate. This domain can be different from the domain where the surrounding HTML document is hosted.

Java system libraries and runtimes are backwards-compatible, allowing one to write code that runs both on current and on future versions of the Java virtual machine.

Many Java developers, blogs and magazines are recommending that the Java Web Start technology be used in place of applets. Java Web Start allows the launching of unmodified applet code, which then runs in a separate window (not inside the invoking browser).

A Java Servlet is sometimes informally compared to be "like" a server-side applet, but it is different in its language, functions, and in each of the characteristics described here about applets.

The applet can be displayed on the web page by making use of the deprecated codice_2 HTML element, or the recommended codice_3 element. The codice_4 element can be used with Mozilla family browsers (codice_4 was deprecated in HTML 4 but is included in HTML 5). This specifies the applet's source and location. Both codice_3 and codice_4 tags can also download and install Java virtual machine (if required) or at least lead to the plugin page. codice_2 and codice_3 tags also support loading of the serialized applets that start in some particular (rather than initial) state. Tags also specify the message that shows up in place of the applet if the browser cannot run it due to any reason.

However, despite codice_3 being officially a recommended tag, as of 2010, the support of the codice_3 tag was not yet consistent among browsers and Sun kept recommending the older codice_2 tag for deploying in multibrowser environments, as it remained the only tag consistently supported by the most popular browsers. To support multiple browsers, the codice_3 tag currently requires JavaScript (that recognizes the browser and adjusts the tag), usage of additional browser-specific tags or delivering adapted output from the server side. Deprecating codice_2 tag has been criticized. Oracle now provides a maintained JavaScript code to launch applets with cross platform workarounds.

The Java browser plug-in relies on NPAPI, which many web browser vendors are deprecating due to its age and security issues. In January 2016, Oracle announced that Java runtime environments based on JDK 9 will discontinue the browser plug-in.

The following example illustrates the use of Java applets through the java.applet package. The example also uses classes from the Java Abstract Window Toolkit (AWT) to produce the message "Hello, world!" as output.
import java.applet.*;
import java.awt.*;

// Applet code for the "Hello, world!" example.
// This should be saved in a file named as "HelloWorld.java".
public class HelloWorld extends Applet {

Simple applets are shared freely on the Internet for customizing applications that support plugins.

After compilation, the resulting .class file can be placed on a web server and invoked within an HTML page by using an <applet> or an <object> tag. For example:
<!DOCTYPE html>
<html>
<head>
</head>
<body>
</body>
</html>
When the page is accessed it will read as follows:

To minimize download time, applets can be delivered in the form of a jar file. In the case of this example, if all necessary classes are placed in the compressed archive "example.jar", the following embedding code could be used instead:
<p>
</p>
Applet inclusion is described in detail in Sun's official page about the APPLET tag.

A Java applet can have any or all of the following advantages:


A Java applet may have any of the following disadvantages compared to other client-side web technologies:


Sun made considerable efforts to ensure compatibility is maintained between Java versions as they evolve, enforcing Java portability by law if required. Oracle seems to be continuing the same strategy.

The 1997 lawsuit, was filed after Microsoft created a modified Java Virtual Machine of their own, which shipped with Internet Explorer. Microsoft added about 50 methods and 50 fields into the classes within the "java.awt, java.lang", and "java.io" packages. Other modifications included removal of RMI capability and replacement of Java native interface from JNI to RNI, a different standard. RMI was removed because it only easily supports Java to Java communications and competes with Microsoft DCOM technology. Applets that relied on these changes or just inadvertently used them worked only within Microsoft's Java system. Sun sued for breach of trademark, as the point of Java was that there should be no proprietary extensions and that code should work everywhere. Microsoft agreed to pay Sun $20 million, and Sun agreed to grant Microsoft limited license to use Java without modifications only and for a limited time.

Microsoft continued to ship its own unmodified Java virtual machine. Over the years it became extremely outdated yet still default for Internet Explorer. A later study revealed that applets of this time often contain their own classes that mirror Swing and other newer features in a limited way. In 2002, Sun filed an antitrust lawsuit, claiming that Microsoft's attempts at illegal monopolization had harmed the Java platform. Sun demanded Microsoft distribute Sun's current, binary implementation of Java technology as part of Windows, distribute it as a recommended update for older Microsoft desktop operating systems and stop the distribution of Microsoft's Virtual Machine (as its licensing time, agreed in the prior lawsuit, had expired). Microsoft paid $700 million for pending antitrust issues, another $900 million for patent issues and a $350 million royalty fee to use Sun's software in the future.

There are two applet types with very different security models: signed applets and unsigned applets. As of Java SE 7 Update 21 (April 2013) applets and Web-Start Apps are encouraged to be signed with a trusted certificate, and warning messages appear when running unsigned applets. Further starting with Java 7 Update 51 unsigned applets are blocked by default; they can be run by creating an exception in the Java Control Panel.

Limits on unsigned applets are understood as "draconian": they have no access to the local filesystem and web access limited to the applet download site; there are also many other important restrictions. For instance, they cannot access all system properties, use their own class loader, call native code, execute external commands on a local system or redefine classes belonging to core packages included as part of a Java release. While they can run in a standalone frame, such frame contains a header, indicating that this is an untrusted applet. Successful initial call of the forbidden method does not automatically create a security hole as an access controller checks the entire stack of the calling code to be sure the call is not coming from an improper location.

As with any complex system, many security problems have been discovered and fixed since Java was first released. Some of these (like the Calendar serialization security bug) persisted for many years with nobody being aware. Others have been discovered in use by malware in the wild.

Some studies mention applets crashing the browser or overusing CPU resources but these are classified as nuisances and not as true security flaws. However, unsigned applets may be involved in combined attacks that exploit a combination of multiple severe configuration errors in other parts of the system. An unsigned applet can also be more dangerous to run directly on the server where it is hosted because while code base allows it to talk with the server, running inside it can bypass the firewall. An applet may also try DoS attacks on the server where it is hosted, but usually people who manage the web site also manage the applet, making this unreasonable. Communities may solve this problem via source code review or running applets on a dedicated domain.

The unsigned applet can also try to download malware hosted on originating server. However it could only store such file into a temporary folder (as it is transient data) and has no means to complete the attack by executing it. There were attempts to use applets for spreading Phoenix and Siberia exploits this way, but these exploits do not use Java internally and were also distributed in several other ways.

A signed applet contains a signature that the browser should verify through a remotely running, independent certificate authority server. Producing this signature involves specialized tools and interaction with the authority server maintainers. Once the signature is verified, and the user of the current machine also approves, a signed applet can get more rights, becoming equivalent to an ordinary standalone program. The rationale is that the author of the applet is now known and will be responsible for any deliberate damage. This approach allows applets to be used for many tasks that are otherwise not possible by client-side scripting. However, this approach requires more responsibility from the user, deciding whom he or she trusts. The related concerns include a non-responsive authority server, wrong evaluation of the signer identity when issuing certificates, and known applet publishers still doing something that the user would not approve of. Hence signed applets that appeared from Java 1.1 may actually have more security concerns.

Self-signed applets, which are applets signed by the developer themselves, may potentially pose a security risk; java plugins provide a warning when requesting authorization for a self-signed applet, as the function and safety of the applet is guaranteed only by the developer itself, and has not been independently confirmed. Such self-signed certificates are usually only used during development prior to release where third-party confirmation of security is unimportant, but most applet developers will seek third-party signing to ensure that users trust the applet's safety.

Java security problems are not fundamentally different from similar problems of any client-side scripting platform. In particular, all issues related to signed applets also apply to Microsoft ActiveX components.

As of 2014, self-signed and unsigned applets are no longer accepted by the commonly available Java plugins or Java Web Start. Consequently, developers who wish to deploy Java applets have no alternative but to acquire trusted certificates from commercial sources.

Alternative technologies exist (for example, JavaScript) that satisfy all or more of the scope of what is possible with an applet. JavaScript can coexist with applets in the same page, assist in launching applets (for instance, in a separate frame or providing platform workarounds) and later be called from the applet code. JavaFX is an extension of the Java platform and may also be viewed as an alternative.




</doc>
<doc id="13595" url="https://en.wikipedia.org/wiki?curid=13595" title="Heathrow Airport">
Heathrow Airport

Heathrow Airport, originally called London Airport (until 1966) and now known as London Heathrow , is a major international airport in London, United Kingdom. Heathrow is the second busiest airport in the world by international passenger traffic, as well as the busiest airport in Europe by passenger traffic, and the seventh busiest airport in the world by total passenger traffic. It is one of six international airports serving the London region. In 2019, it handled a record 80.8 million passengers, a 0.9% increase from 2018 as well as 475,861 aircraft movements, a decrease of 1,743 from 2018. The airport facility is owned and operated by Heathrow Airport Holdings.

Heathrow lies 14 miles (23 km) west of Central London, and has two parallel east–west runways along with four operational terminals on a site that covers . The airport is the primary hub for British Airways and the primary operating base for Virgin Atlantic.

In September 2012, the Government of the United Kingdom established the Airports Commission, an independent commission chaired by Sir Howard Davies to examine various options for increasing capacity at UK airports. In July 2015, the commission backed a third runway at Heathrow, which the government approved in October 2016. However, the England and Wales Court of Appeal rejected this plan for a third runway at Heathrow, due to concerns about climate change and the environmental impact of aviation.

Heathrow is west of central London, on a parcel of land that is designated part of the Metropolitan Green Belt. It is located west of the town of Hounslow, 3 miles south of Hayes, and 3 miles north-east of Staines-upon-Thames.

The airport is surrounded by the villages of Harlington, Harmondsworth, and Longford to the north and the neighbourhoods of Cranford and Hatton to the east. To the south lie Feltham, Bedfont and Stanwell while to the west Heathrow is separated from Wraysbury, Horton and Windsor in Berkshire by the M25 motorway. Heathrow falls entirely within the boundaries of the London Borough of Hillingdon, and under the Twickenham postcode area, with the postcode TW6. The airport is located within the Hayes and Harlington parliamentary constituency.

As the airport is located west of London and as its runways run east–west, an airliner's landing approach is usually directly over the conurbation of London when the wind is from the west, which is most of the time.

The airport forms part of a travel to work area with Slough, the west part of Greater London, and the north part of Surrey.

Along with Gatwick, Stansted, Luton, Southend and London City, Heathrow is one of six airports with scheduled services serving the London area.

Heathrow Airport originated in 1929 as a small airfield (Great West Aerodrome) on land south-east of the hamlet of Heathrow from which the airport takes its name. At that time the land consisted of farms, market gardens and orchards; there was a "Heathrow Farm" approximately where the modern Terminal 2 is situated, a "Heathrow Hall" and a "Heathrow House." This hamlet was largely along a country lane (Heathrow Road), which ran roughly along the east and south edges of the present central terminals area.

Development of the whole Heathrow area as a much larger airport began in 1944. It was stated to be for long-distance military aircraft bound for the Far East; by the time the airfield was nearing completion, World War II had ended, and the UK Government continued to develop the airport as a civil airport. The airport was opened on 25 March 1946 as London Airport and was renamed Heathrow Airport in 1966. The layout for the airport was designed by Sir Frederick Gibberd, who designed the original terminals and central area buildings, including the original control tower and the multi-faith Chapel of St George's.

Heathrow Airport is used by over 80 airlines flying to 185 destinations in 84 countries. The airport is the primary hub of British Airways and is a base for Virgin Atlantic. It has four passenger terminals (numbered 2 to 5) and a cargo terminal. Of Heathrow's 78 million passengers in 2017, 94% were international travellers; the remaining 6% were bound for (or arriving from) places in the UK. The busiest single destination in passenger numbers is New York, with over 3 million passengers flying between Heathrow and JFK Airport in 2013.

In the 1950s, Heathrow had six runways, arranged in three pairs at different angles in the shape of a hexagram with the permanent passenger terminal in the middle and the older terminal along the north edge of the field; two of its runways would always be within 30° of the wind direction. As the required length for runways has grown, Heathrow now has only two parallel runways running east–west. These are extended versions of the two east–west runways from the original hexagram. From the air, almost all of the original runways can still be seen, incorporated into the present system of taxiways. North of the northern runway and the former taxiway and aprons, now the site of extensive car parks, is the entrance to the access tunnel and the site of Heathrow's unofficial "gate guardian". For many years the home of a 40% scale model of a British Airways Concorde, G-CONC, the site has been occupied by a model of an Emirates Airbus A380 since 2008.

Heathrow Airport has Anglican, Catholic, Free Church, Hindu, Jewish, Muslim and Sikh chaplains. There is a multi-faith prayer room and counselling room in each terminal, in addition to St. George's Interdenominational Chapel in an underground vault adjacent to the old control tower, where Christian services take place. The chaplains organise and lead prayers at certain times in the prayer room.

The airport has its own resident press corps, consisting of six photographers and one TV crew, serving all the major newspapers and television stations around the world.

Most of Heathrow's internal roads are initial letter coded by area: N in the north (e.g. Newall Road), E in the east (e.g. Elmdon Road), S in the south (e.g. Stratford Road), W in the west (e.g. Walrus Road), C in the centre (e.g. Camborne Road).

Aircraft destined for Heathrow are usually routed to one of four holding points.

Air traffic controllers at Heathrow Approach Control (based in Swanwick, Hampshire) then guide the aircraft to their final approach, merging aircraft from the four holds into a single stream of traffic, sometimes as close as apart. Considerable use is made of continuous descent approach techniques to minimize the environmental effects of incoming aircraft, particularly at night. Once an aircraft is established on its final approach, control is handed over to Heathrow Tower.

When runway alternation was introduced, aircraft generated significantly more noise on departure than when landing, so a preference for westerly operations during daylight was introduced, which continues to this day. In this mode, aircraft take off towards the west and land from the east over London, thereby minimizing the impact of noise on the most densely populated areas. Heathrow's two runways generally operate in segregated mode, whereby landings are allocated to one runway and takeoffs to the other. To further reduce noise nuisance to people beneath the approach and departure routes, the use of runways 27R and 27L is swapped at 15:00 each day if the wind is from the west. When landings are easterly there is no alternation; 09L remains the landing runway and 09R the takeoff runway due to the legacy of the now rescinded Cranford Agreement, pending taxiway works to allow the roles to be reversed. Occasionally, landings are allowed on the nominated departure runway, to help reduce airborne delays and to position landing aircraft closer to their terminal, reducing taxi times.

Night-time flights at Heathrow are subject to restrictions. Between 23:00 and 04:00, the noisiest aircraft (rated QC/8 and QC/16) cannot be scheduled for operation. Also, during the night quota period (23:30–06:00) there are four limits:

A trial of "noise relief zones" ran from December 2012 to March 2013, which concentrated approach flight paths into defined areas compared with the existing paths which were spread out. The zones used alternated weekly, meaning residents in the "no-fly" areas received respite from aircraft noise for set periods. However, it was concluded that some residents in other areas experienced a significant disbenefit as a result of the trial and that it should therefore not be taken forward in its current form. Heathrow received more than 25,000 noise complaints in just three months over the summer of 2016, but around half were made by the same ten people.

Until it was required to sell Gatwick and Stansted Airports, Heathrow Airport Holdings held a dominant position in the London aviation market and has been heavily regulated by the Civil Aviation Authority (CAA) as to how much it can charge airlines to land. The annual increase in landing charge per passenger was capped at inflation minus 3% until 1 April 2003. From 2003 to 2007 charges increased by inflation plus 6.5% per year, taking the fee to £9.28 per passenger in 2007. In March 2008, the CAA announced that the charge would be allowed to increase by 23.5% to £12.80 from 1 April 2008 and by inflation plus 7.5% for each of the following four years. In April 2013, the CAA announced a proposal for Heathrow to charge fees calculated by inflation minus 1.3%, continuing until 2019. Whilst the cost of landing at Heathrow is determined by the CAA and Heathrow Airport Holdings, the allocation of landing slots to airlines is carried out by Airport Co-ordination Limited (ACL).

Until 2008, air traffic between Heathrow and the United States was strictly governed by the countries' bilateral Bermuda II treaty. The treaty originally allowed only British Airways, Pan Am and TWA to fly from Heathrow to the US. In 1991, Pan Am and TWA sold their rights to United Airlines and American Airlines respectively, while Virgin Atlantic was added to the list of airlines allowed to operate on these routes. The Bermuda bilateral agreement conflicted with the Right of Establishment of the United Kingdom concerning its EU membership, and as a consequence, the UK was ordered to drop the agreement in 2004. A new "open skies" agreement was signed by the United States and the European Union on 30 April 2007 and came into effect on 30 March 2008. Shortly afterward, additional US airlines, including Northwest Airlines, Continental Airlines, US Airways and Delta Air Lines started services to Heathrow.

The airport has been criticised in recent years for overcrowding and delays; according to Heathrow Airport Holdings, Heathrow's facilities were originally designed to accommodate 55 million passengers annually. The number of passengers using the airport reached a record 70 million in 2012. In 2007 the airport was voted the world's least favourite, alongside Chicago O'Hare, in a TripAdvisor survey. However, the opening of Terminal 5 in 2008 has relieved some pressure on terminal facilities, increasing the airport's terminal capacity to 90 million passengers per year. A tie-up is also in place with McLaren Applied Technologies to optimize the general procedure, reducing delays and pollution.

With only two runways, operating at over 98% of their capacity, Heathrow has little room for more flights, although the increasing use of larger aircraft such as the Airbus A380 will allow some increase in passenger numbers. It is difficult for existing airlines to obtain landing slots to enable them to increase their services from the airport, or for new airlines to start operations. To increase the number of flights, Heathrow Airport Holdings has proposed using the existing two runways in 'mixed mode' whereby aircraft would be allowed to take off and land on the same runway. This would increase the airport's capacity from its current 480,000 movements per year to as many as 550,000 according to British Airways CEO Willie Walsh. Heathrow Airport Holdings has also proposed building a third runway to the north of the airport, which would significantly increase traffic capacity.

Policing of the airport is the responsibility of the aviation security unit of the Metropolitan Police, although the army, including armoured vehicles of the Household Cavalry, has occasionally been deployed at the airport during periods of heightened security.

Full body scanners are now used at the airport, and passengers who object to their use after being selected are required to submit to a hand search in a private room. The scanners display passengers' bodies as a cartoon-style figure, with indicators showing where concealed items may be. The new imagery was introduced initially as a trial in September 2011 following complaints over privacy.

Following widespread disruption caused by reports of drone sightings at Gatwick Airport, and a subsequent incident at Heathrow, a drone detection system was installed airport-wide to combat possible future disruption caused by the illegal use of drones.

For many decades Heathrow had a reputation for theft from baggage by baggage handlers. This led to the airport being nicknamed "Thiefrow", with periodic arrests of baggage handlers.

During the COVID-19 pandemic in 2020, Heathrow Airport saw a vast reduction in services, and announced that as of 6 April 2020, the airport would be transitioning to single runway operations, which would change on a weekly basis, and that it would be closing Terminals 3 and 4, moving all remaining flights into Terminals 2 or 5. Dual runway operations were restored in August 2020.

The airport's newest terminal, officially known as the Queen's Terminal, was opened on 4 June 2014. Designed by Spanish architect Luis Vidal, it was built on the site that had been occupied by the original Terminal 2 and the Queens Building. The main complex was completed in November 2013 and underwent six months of testing before opening to passengers. It includes a satellite pier (T2B), a 1,340-space car park, an energy center and a cooling station to generate chilled water. There are 52 shops and 17 bars and restaurants.

Terminal 2 is used by all Star Alliance members which fly from Heathrow (consolidating the airlines under Star Alliance's co-location policy "Move Under One Roof"). Aer Lingus, Eurowings and Icelandair also operate from the terminal. The airlines moved from their original locations over six months, with only 10% of flights operating from there in the first six weeks (United Airlines' transatlantic flights) to avoid the opening problems seen at Terminal 5. On 4 June 2014, United Airlines became the first airline to move into Terminal 2 from Terminals 1 and 4 followed by All Nippon Airways, Air Canada and Air China from Terminal 3. Air New Zealand, Asiana Airlines, Croatia Airlines, LOT Polish Airlines, South African Airways, and TAP Air Portugal were the last airlines to move in on 22 October 2014.

The original Terminal 2 opened as the Europa Building in 1955 and was the airport's oldest terminal. It had an area of and was designed to handle around 1.2 million passengers annually. In its final years, it accommodated up to 8 million. A total of 316 million passengers passed through the terminal in its lifetime. The building was demolished in 2010, along with the Queens Building which had housed airline company offices.

Terminal 3 opened as the Oceanic Terminal on 13 November 1961 to handle flight departures for long-haul routes for foreign carriers to the United States, Asia and other Far Eastern destinations. At this time the airport had a direct helicopter service to Central London from the gardens on the roof of the terminal building. Renamed Terminal 3 in 1968, it was expanded in 1970 with the addition of an arrivals building. Other facilities added included the UK's first moving walkways. In 2006, the new £105 million Pier 6 was completed to accommodate the Airbus A380 superjumbo; Emirates and Qantas operate regular flights from Terminal 3 using the Airbus A380.

Redevelopment of Terminal 3's forecourt by the addition of a new four-lane drop-off area and a large pedestrianised plaza, complete with canopy to the front of the terminal building, was completed in 2007. These improvements were intended to improve passengers' experience, reduce traffic congestion and improve security. As part of this project, Virgin Atlantic was assigned its own dedicated check-in area, known as 'Zone A', which features a large sculpture and atrium.

, Terminal 3 has an area of and in 2011 it handled 19.8 million passengers on 104,100 flights. Terminal 3 is home to Oneworld members (with the exception of Iberia and American, which use Terminal 5 and Malaysia Airlines, Royal Air Maroc and Qatar Airways, All of which use Terminal 4), SkyTeam members Delta Air Lines and Middle East Airlines, all new airlines, and a few unaffiliated carriers.

Opened in 1986, Terminal 4 is situated to the south of the southern runway next to the cargo terminal and is connected to Terminals 2 and 3 by the Heathrow Cargo Tunnel. The terminal has an area of and is now home to the SkyTeam alliance, with the exception of Delta Air Lines and Middle East Airlines, which use Terminal 3, Oneworld carriers Malaysia Airlines and Qatar Airways, and to most unaffiliated carriers. It has undergone a £200m upgrade to enable it to accommodate 45 airlines with an upgraded forecourt to reduce traffic congestion and improve security. Most flights that go to Terminal 4 are flights coming from Central Asia, North Africa and the Middle East as well as a few flights to Europe. An extended check-in area with renovated piers and departure lounges and a new baggage system were installed, and two new stands were built to accommodate the Airbus A380; Etihad Airways, Korean Air, Malaysia Airlines and Qatar Airways operate regular A380 flights. El Al operates regular Boeing 787 flights.

Terminal 5 lies between the northern and southern runways at the western end of the Heathrow site and was opened by Queen Elizabeth II on 14 March 2008, some 19 years after its inception. It opened to the public on 27 March 2008, and British Airways and its partner company Iberia have exclusive use of this terminal. The first passenger to enter Terminal 5 was a UK ex-pat from Kenya who passed through security at 04:30 on the day. He was presented with a boarding pass by the British Airways CEO Willie Walsh for the first departing flight, BA302 to Paris. During the two weeks after its opening, operations were disrupted by problems with the terminal's IT systems, coupled with insufficient testing and staff training, which caused over 500 flights to be cancelled. Until March 2012, Terminal 5 was exclusively used by British Airways as its global hub; however, because of the merger, on 25 March Iberia's operations at Heathrow were moved to the terminal, making it the home of International Airlines Group. On 7 July 2020, American moved to terminal 5, to allow for easier connections from American's transatlantic flights to British Airways flights.

Built at £4.3 billion, the terminal consists of a four-story main terminal building (Concourse A) and two satellite buildings linked to the main terminal by an underground people mover transit system. The second satellite (Concourse C), includes dedicated aircraft stands for the Airbus A380. It became fully operational on 1 June 2011. Terminal 5 was voted Skytrax World's Best Airport Terminal 2014 in the Annual World Airport Awards.

The main terminal building (Concourse A) has an area of while Concourse B covers . It has 60 aircraft stands and capacity for 30 million passengers annually as well as more than 100 shops and restaurants. It is also home to British Airways' Flagship lounge, the Concorde Room, alongside four further British Airways branded lounges.

A further building, designated Concourse D and of similar size to Concourse C, may yet be built to the east of the existing site, providing up to another 16 stands. Following British Airways' merger with Iberia, this may become a priority since the combined business will require accommodation at Heathrow under one roof to maximise the cost savings envisaged under the deal. A proposal for Concourse D featured in Heathrow's most recent capital investment plan.

The transport network around the airport has been extended to cope with the increase in passenger numbers. New branches of both the Heathrow Express and the Underground's Piccadilly line serve a new shared Heathrow Terminal 5 station. A dedicated motorway spur links the terminal to the M25 (between junctions 14 and 15). The terminal has a 3,800 space multi-storey car park. A more distant long-stay car park for business passengers is connected to the terminal by a personal rapid transit system, the Heathrow Pod, which became operational in the spring of 2011. Within the terminal complex, an automated people mover (APM) system, known as the Transit, is used to transport passengers between the satellite buildings.

As of July 2020, Heathrow's four passenger terminals are assigned as follows:

Following the opening of Terminal 5 in March 2008, a complex programme of terminal moves was implemented. This saw many airlines move to be grouped in terminals by airline alliance as far as possible.

Following the opening of Phase 1 of the new Terminal 2 in June 2014, all Star Alliance member airlines (with the exception of new member Air India which moved in early 2017) along with Aer Lingus and Germanwings relocated to Terminal 2 in a phased process completed on 22 October 2014. Additionally, by 30 June 2015 all airlines left Terminal 1 in preparation for its demolition to make room for the construction of Phase 2 of Terminal 2. Some other airlines made further minor moves at a later point, e.g. Delta Air Lines merging all departures in Terminal 3 instead of a split between Terminals 3 and 4.

Terminal 1 opened in 1968 and was inaugurated by Queen Elizabeth II in April 1969. Terminal 1 was the Heathrow base for British Airways' (BA) domestic and European network and a few of its long haul routes before Terminal 5 opened. The acquisition of British Midland International (BMI) in 2012 by BA's owner International Airlines Group meant British Airways took over BMI's short-haul and medium-haul destinations from the terminal. Terminal 1 was also the main base for most Star Alliance members though some were also based at Terminal 3.

Terminal 1 closed at the end of June 2015, the site is now being used to extend Terminal 2 which opened in June 2014. A number of the newer gates used by Terminal 1 were built as part of the Terminal 2 development and are being retained. The last tenants along with British Airways were El Al, Icelandair (moved to Terminal 2 25 March 2015) and LATAM Brasil (the third to move in to Terminal 3 on 27 May 2015). British Airways was the last operator in Terminal 1. Two flights of this carrier, one departing to Hanover and one arriving from Baku, marked the terminal closure on 29 June 2015. British Airways operations have been relocated to Terminals 3 and 5.

The following airlines operate regular scheduled passenger flights at London Heathrow Airport:

When ranked by passenger traffic, Heathrow is the sixth busiest internationally, behind Hartsfield–Jackson Atlanta International Airport, Beijing Capital International Airport, Dubai International Airport, Chicago's O'Hare International Airport, and Tokyo Haneda Airport, for the 12 months ending December 2015.

In 2015, Heathrow was the busiest airport in Europe in total passenger traffic, with 14% more passengers than Paris–Charles de Gaulle Airport and 22% more than Istanbul Atatürk Airport. Heathrow was the fourth busiest European airport by cargo traffic in 2013, after Frankfurt Airport, Paris Charles de Gaulle and Amsterdam Airport Schiphol.

Heathrow Airport processed 80,884,310 passengers in 2019. New York's John F. Kennedy International Airport was the most popular route with 3,192,195 passengers. The table below shows the 10 busiest international routes at the airport in 2019.

The head office of Heathrow Airport Holdings (formerly BAA Limited) is located in the Compass Centre by Heathrow's northern runway, a building that previously served as a British Airways flight crew centre. The World Business Centre Heathrow consists of three buildings. 1 World Business Centre houses offices of Heathrow Airport Holdings, Heathrow Airport itself, and Scandinavian Airlines. Previously International Airlines Group had its head office in 2 World Business Centre.

At one time the British Airways head office was located within Heathrow Airport at Speedbird House before the completion of Waterside, the current BA head office in Harmondsworth, in June 1998.

To the north of the airfield lies the Northern Perimeter Road, along which most of Heathrow's car rental agencies are based, and Bath Road, which runs parallel to it, but outside the airport campus. This is nicknamed "The Strip" by locals, because of its continuous line of airport hotels.


Many buses and coaches operate from the large Heathrow Central bus station serving Terminals 2 and 3, and also from bus stations at Terminals 4 and 5.

All terminals lie within the Heathrow Free Travel Zone with free travel between the terminals. Terminals 2 and 3 are within walking distance of each other. Transfers from Terminals 2 and 3 to Terminal 4 and 5 are provided by Heathrow Express trains and the London Underground Piccadilly line. Direct transfer between Terminals 4 and 5 is provided by London Buses routes 482 and 490.

Transit passengers remaining airside are provided with free dedicated transfer buses between terminals.

The Heathrow Pod personal rapid transit system shuttles passengers between Terminal 5 and the business car park using 21 small, driverless transportation pods. The pods are battery-powered and run on-demand on a four-kilometre track, each able to carry up to four adults, two children, and their luggage. Plans exist to extend the Pod system to connect Terminals 2 and 3 to remote car parks.
An underground automated people mover system known as the "Transit" operates within Terminal 5, linking the main terminal with the satellite Terminals 5B and 5C. The Transit operates entirely airside using Bombardier Innovia APM 200 people mover vehicles.

The Hotel Hoppa bus network connects all terminals to major hotels in the area.

Taxis are available at all terminals.

Heathrow is accessible via the nearby M4 motorway or A4 road (Terminals 2–3), the M25 motorway (Terminals 4 and 5) and the A30 road (Terminal 4). There are drop-off and pick-up areas at all terminals and short- and long-stay multi-storey car parks. All the Heathrow forecourts are drop-off only. There are further car parks, not run by Heathrow Airport Holdings, just outside the airport: the most recognisable is the National Car Parks facility, although there are many other options; these car parks are connected to the terminals by shuttle buses.

Four parallel tunnels under the northern runway connect the M4 Heathrow spur and the A4 road to Terminals 2–3. The two larger tunnels are each two lanes wide and are used for motorised traffic. The two smaller tunnels were originally reserved for pedestrians and bicycles; to increase traffic capacity the cycle lanes have been modified to each take a single lane of cars, although bicycles still have priority over cars. Pedestrian access to the smaller tunnels has been discontinued, with the free bus services being used instead.

There are (mainly off-road) bicycle routes to some of the terminals. Free bicycle parking places are available in car parks 1 and 1A, at Terminal 4, and to the North and South of Terminal 5's Interchange Plaza. Cycling is not currently allowed through the main tunnel to access Terminals 2 and 3 (Terminal 1 closed in 2015).




There is a long history of expansion proposals for Heathrow since it was first designated as a civil airport. Following the cancellation of the Maplin project in 1974, a fourth terminal was proposed but expansion beyond this ruled out. However, the Airports Inquiries of 1981–83 and the 1985 Airports Policy White Paper considered further expansion and, following a four-year-long public inquiry in 1995–99, Terminal 5 was approved. In 2003, after many studies and consultations, the Future of Air Transport White Paper was published which proposed a third runway at Heathrow, as well as a second runway at Stansted Airport. In January 2009, the Transport Secretary at the time, Geoff Hoon announced that the British government supported the expansion of Heathrow by building a third runway and a sixth terminal building. This decision followed the 2003 white paper on the future of air transport in the UK, and a public consultation in November 2007. This was a controversial decision which met with widespread opposition because of the expected greenhouse gas emissions, impact on local communities, as well as noise and air pollution concerns.

Before the 2010 general election, the Conservative and Liberal Democrat parties announced that they would prevent the construction of any third runway or further material expansion of the airport's operating capacity. The Mayor of London, then Boris Johnson, took the position that London needs more airport capacity, favouring the construction of an entirely new airport in the Thames Estuary rather than expanding Heathrow. After the Conservative-Liberal Democrat coalition took power, it was announced that the third runway expansion was cancelled. Two years later, leading Conservatives were reported to have changed their minds on the subject.

Another proposal for expanding Heathrow's capacity was the Heathrow Hub, which aims to extend both runways to a total length of about 7,000 metres and divide them into four so that they each provide two, full length runways, allowing simultaneous take-offs and landings while decreasing noise levels.

In July 2013, the airport submitted three new proposals for expansion to the Airports Commission, which was established to review airport capacity in the southeast of England. The Airports Commission was chaired by Sir Howard Davies who, at the time of his appointment was in the employ of GIC Private Limited (formerly known as Government Investment Corporation of Singapore) and a member of its International Advisory Board. GIC Private Limited was then (2012), as it remains today, one of Heathrow's principal owners. Sir Howard Davies resigned these positions upon confirmation of his appointment to lead the Airports Commission, although it has been observed that he failed to identify these interests when invited to complete the Airports Commission's register of interests. Each of the three proposals that were to be considered by Sir Howard Davies's commission involved the construction of a third runway, either to the north, northwest or southwest of the airport.

The commission released its interim report in December 2013, shortlisting three options: the north-west third runway option at Heathrow, extending an existing runway at Heathrow, and a second runway at Gatwick Airport. After this report was published, the government confirmed that no options had been ruled out for airport expansion in the South-east and that a new runway would not be built at Heathrow before 2015. The full report was published on 1 July 2015, and backed a third, north-west, runway at Heathrow. Reaction to the report was generally negative, particularly from London Mayor Boris Johnson. One senior Conservative told Channel 4: "Howard Davies has dumped an utter steaming pile of poo on the Prime Minister's desk." On 25 October 2016, the government confirmed that Heathrow would be allowed to build a third runway; however, a final decision would not be taken until winter of 2017/18, after consultations and government votes. The earliest opening year would be 2025. On 5 June 2018, the UK Cabinet approved the third runway, with a full vote planned for Parliament. On 25 June 2018, the House of Commons voted, 415–119, in favour of the third runway. The bill received support from most MPs in the Conservative and Labour parties. A judicial review against the decision is being launched by four London local authorities affected by the expansion—Wandsworth, Richmond, Hillingdon and Hammersmith and Fulham—in partnership with Greenpeace and London mayor Sadiq Khan. Khan previously stated he would take legal action if it were passed by Parliament.

Currently, all rail connections with Heathrow airport run along an east–west alignment to and from central London, and a number of schemes have been proposed over the years to develop new rail transport links with other parts of London and with stations outside the city. This mainline rail service is due to be extended to central London and Essex when the Elizabeth line, currently under construction, opens.

A 2009 proposal to create a southern link with via the Waterloo–Reading line was abandoned in 2011 due to lack of funding and difficulties with a high number of level crossings on the route into London, and a plan to link Heathrow to the planned High Speed 2 (HS2) railway line (with a new station, ) was also dropped from the HS2 plans in March 2015.

Among other schemes that have been considered is a rapid transport link between Heathrow and Gatwick Airports, known as "Heathwick", which would allow the airports to operate jointly as an airline hub; In 2018, the Department for Transport began to invite proposals for privately funded rail links to Heathrow Airport. Projects being considered under this initiative include:

The Mayor of London's office and Transport for London commissioned plans in the event of Heathrow's closure—to replace it by a large built-up area. Some of the plans seem to show terminal 5, or part of it, kept as a shopping centre.





</doc>
<doc id="13600" url="https://en.wikipedia.org/wiki?curid=13600" title="Hipparchus">
Hipparchus

Hipparchus of Nicaea (; , "Hipparkhos";  ) was a Greek astronomer, geographer, and mathematician. He is considered the founder of trigonometry but is most famous for his incidental discovery of precession of the equinoxes.

Hipparchus was born in Nicaea, Bithynia (now İznik, Turkey), and probably died on the island of Rhodes, Greece. He is known to have been a working astronomer at least from 162 to 127 . Hipparchus is considered the greatest ancient astronomical observer and, by some, the greatest overall astronomer of antiquity. He was the first whose quantitative and accurate models for the motion of the Sun and Moon survive. For this he certainly made use of the observations and perhaps the mathematical techniques accumulated over centuries by the Babylonians and by Meton of Athens (5th century ), Timocharis, Aristyllus, Aristarchus of Samos and Eratosthenes, among others. He developed trigonometry and constructed trigonometric tables, and he solved several problems of spherical trigonometry. With his solar and lunar theories and his trigonometry, he may have been the first to develop a reliable method to predict solar eclipses. His other reputed achievements include the discovery and measurement of Earth's precession, the compilation of the first comprehensive star catalog of the western world, and possibly the invention of the astrolabe, also of the armillary sphere, which he used during the creation of much of the star catalogue.

Hipparchus was born in Nicaea (Greek "Νίκαια"), in the ancient district of Bithynia (modern-day Iznik in province Bursa), in what today is the country Turkey. The exact dates of his life are not known, but Ptolemy attributes astronomical observations to him in the period from 147–127 , and some of these are stated as made in Rhodes; earlier observations since 162  might also have been made by him. His birth date ( ) was calculated by Delambre based on clues in his work. Hipparchus must have lived some time after 127  because he analyzed and published his observations from that year. Hipparchus obtained information from Alexandria as well as Babylon, but it is not known when or if he visited these places. He is believed to have died on the island of Rhodes, where he seems to have spent most of his later life.

It is not known what Hipparchus's economic means were nor how he supported his scientific activities. His appearance is likewise unknown: there are no contemporary portraits. In the 2nd and 3rd centuries coins were made in his honour in Bithynia that bear his name and show him with a globe; this supports the tradition that he was born there.

Relatively little of Hipparchus's direct work survives into modern times. Although he wrote at least fourteen books, only his commentary on the popular astronomical poem by Aratus was preserved by later copyists. Most of what is known about Hipparchus comes from Strabo's "Geography" and Pliny's "Natural History" in the 1st century; Ptolemy's 2nd-century "Almagest"; and additional references to him in the 4th century by Pappus and Theon of Alexandria in their commentaries on the "Almagest".

Hipparchus was amongst the first to calculate a heliocentric system, but he abandoned his work because the calculations showed the orbits were not perfectly circular as believed to be mandatory by the science of the time. Although a contemporary of Hipparchus', Seleucus of Seleucia, remained a proponent of the heliocentric model, Hipparchus' rejection of heliocentrism, supported by ideas from Aristotle, remained dominant for nearly 2000 years until Copernican heliocentrism turned the tide of the debate.

Hipparchus's only preserved work is "Τῶν Ἀράτου καὶ Εὐδόξου φαινομένων ἐξήγησις" ("Commentary on the Phaenomena of Eudoxus and Aratus"). This is a highly critical commentary in the form of two books on a popular poem by Aratus based on the work by Eudoxus. Hipparchus also made a list of his major works, which apparently mentioned about fourteen books, but which is only known from references by later authors. His famous star catalog was incorporated into the one by Ptolemy, and may be almost perfectly reconstructed by subtraction of two and two-thirds degrees from the longitudes of Ptolemy's stars. The first trigonometric table was apparently compiled by Hipparchus, who is consequently now known as "the father of trigonometry".

Hipparchus was in the international news in 2005, when it was again proposed (as in 1898) that the data on the celestial globe of Hipparchus or in his star catalog may have been preserved in the only surviving large ancient celestial globe which depicts the constellations with moderate accuracy, the globe carried by the Farnese Atlas. There are a variety of mis-steps in the more ambitious 2005 paper, thus no specialists in the area accept its widely publicized speculation.

Lucio Russo has said that Plutarch, in his work "On the Face in the Moon", was reporting some physical theories that we consider to be Newtonian and that these may have come originally from Hipparchus; he goes on to say that Newton may have been influenced by them. According to one book review, both of these claims have been rejected by other scholars.

A line in Plutarch's "Table Talk" states that Hipparchus counted 103,049 compound propositions that can be formed from ten simple propositions. 103,049 is the tenth Schröder–Hipparchus number, which counts the number of ways of adding one or more pairs of parentheses around consecutive subsequences of two or more items in any sequence of ten symbols. This has led to speculation that Hipparchus knew about enumerative combinatorics, a field of mathematics that developed independently in modern mathematics.

Earlier Greek astronomers and mathematicians were influenced by Babylonian astronomy to some extent, for instance the period relations of the Metonic cycle and Saros cycle may have come from Babylonian sources (see "Babylonian astronomical diaries"). Hipparchus seems to have been the first to exploit Babylonian astronomical knowledge and techniques systematically. Except for Timocharis and Aristillus, he was the first Greek known to divide the circle in 360 degrees of 60 arc minutes (Eratosthenes before him used a simpler sexagesimal system dividing a circle into 60 parts); he also adopted the Babylonian astronomical "cubit" unit (Akkadian "ammatu", Greek πῆχυς "pēchys") which was equivalent to 2° or 2.5° ('large cubit').

Hipparchus probably compiled a list of Babylonian astronomical observations; G. J. Toomer, a historian of astronomy, has suggested that Ptolemy's knowledge of eclipse records and other Babylonian observations in the "Almagest" came from a list made by Hipparchus. Hipparchus's use of Babylonian sources has always been known in a general way, because of Ptolemy's statements. However, Franz Xaver Kugler demonstrated that the synodic and anomalistic periods that Ptolemy attributes to Hipparchus had already been used in Babylonian ephemerides, specifically the collection of texts nowadays called "System B" (sometimes attributed to Kidinnu).

Hipparchus's long draconitic lunar period (5,458 months = 5,923 lunar nodal periods) also appears a few times in Babylonian records. But the only such tablet explicitly dated is post-Hipparchus so the direction of transmission is not settled by the tablets.

Hipparchus's draconitic lunar motion cannot be solved by the lunar-four arguments that are sometimes proposed to explain his anomalistic motion. A solution that has produced the exact ratio is rejected by most historians though it uses the only anciently attested method of determining such ratios, and it automatically delivers the ratio's four-digit numerator and denominator. Hipparchus initially used ("Almagest" 6.9) his 141 BC eclipse with a Babylonian eclipse of 720 BC to find the less accurate ratio 7,160 synodic months = 7,770 draconitic months, simplified by him to 716 = 777 through division by 10. (He similarly found from the 345-year cycle the ratio 4267 synodic months = 4573 anomalistic months and divided by 17 to obtain the standard ratio 251 synodic months = 269 anomalistic months.) If he sought a longer time base for this draconitic investigation he could use his same 141 BC eclipse with a moonrise 1245 BC eclipse from Babylon, an interval of 13,645 synodic months = draconitic months ≈ anomalistic months. Dividing by produces 5458 synodic months = 5923 precisely. The obvious main objection is that the early eclipse is unattested though that is not surprising in itself and there is no consensus on whether Babylonian observations were recorded this remotely. Though Hipparchus's tables formally went back only to 747 BC, 600 years before his era, the tables were actually good back to before the eclipse in question because as only recently noted their use in reverse is no more difficult than forwards.

Hipparchus was recognized as the first mathematician known to have possessed a trigonometric table, which he needed when computing the eccentricity of the orbits of the Moon and Sun. He tabulated values for the chord function, which for a central angle in a circle gives the length of the straight line segment between the points where the angle intersects the circle. He computed this for a circle with a circumference of 21,600 units and a radius (rounded) of 3438 units; this circle has a unit length of 1 arc minute along its perimeter. He tabulated the chords for angles with increments of 7.5°. In modern terms, the chord subtended by a central angle in a circle of given radius equals the radius times twice the sine of half of the angle, i.e.:

The now lost work in which Hipparchus is said to have developed his chord table, is called "Tōn en kuklōi eutheiōn" ("Of Lines Inside a Circle") in Theon of Alexandria's 4th-century commentary on section I.10 of the "Almagest". Some claim the table of Hipparchus may have survived in astronomical treatises in India, like the "Surya Siddhanta". Trigonometry was a significant innovation, because it allowed Greek astronomers to solve any triangle, and made it possible to make quantitative astronomical models and predictions using their preferred geometric techniques.

Hipparchus must have used a better approximation for π than the one from Archimedes of between (3.14085) and (3.14286). Perhaps he had the one later used by Ptolemy: 3;8,30 (sexagesimal)(3.1417) ("Almagest" VI.7), but it is not known whether he computed an improved value himself.

Some scholars do not believe Āryabhaṭa's sine table has anything to do with Hipparchus's chord table. Others do not agree that Hipparchus even constructed a chord table. Bo C. Klintberg states, "With mathematical reconstructions and philosophical arguments I show that Toomer's 1973 paper never contained any conclusive evidence for his claims that Hipparchus had a 3438'-based chord table, and that the Indians used that table to compute their sine tables. Recalculating Toomer's reconstructions with a 3600' radius – i.e. the radius of the chord table in Ptolemy's Almagest, expressed in 'minutes' instead of 'degrees' – generates Hipparchan-like ratios similar to those produced by a 3438′ radius. It is therefore possible that the radius of Hipparchus's chord table was 3600′, and that the Indians independently constructed their 3438′-based sine table."

Hipparchus could have constructed his chord table using the Pythagorean theorem and a theorem known to Archimedes. He also might have developed and used the theorem called Ptolemy's theorem; this was proved by Ptolemy in his "Almagest" (I.10) (and later extended by Carnot).

Hipparchus was the first to show that the stereographic projection is conformal, and that it transforms circles on the sphere that do not pass through the center of projection to circles on the plane. This was the basis for the astrolabe.

Besides geometry, Hipparchus also used arithmetic techniques developed by the Chaldeans. He was one of the first Greek mathematicians to do this, and in this way expanded the techniques available to astronomers and geographers.

There are several indications that Hipparchus knew spherical trigonometry, but the first surviving text discussing it is by Menelaus of Alexandria in the 1st century, who on that basis is now commonly credited with its discovery. (Previous to the finding of the proofs of Menelaus a century ago, Ptolemy was credited with the invention of spherical trigonometry.) Ptolemy later used spherical trigonometry to compute things like the rising and setting points of the ecliptic, or to take account of the lunar parallax. If he did not use spherical trigonometry, Hipparchus may have used a globe for these tasks, reading values off coordinate grids drawn on it, or he may have made approximations from planar geometry, or perhaps used arithmetical approximations developed by the Chaldeans.

Aubrey Diller has shown that the clima calculations which Strabo preserved from Hipparchus could have been performed by spherical trigonometry using the only accurate obliquity known to have been used by ancient astronomers, 23°40′. All thirteen clima figures agree with Diller's proposal. Further confirming his contention is the finding that the big errors in Hipparchus's longitude of Regulus and both longitudes of Spica agree to a few minutes in all three instances with a theory that he took the wrong sign for his correction for parallax when using eclipses for determining stars' positions.

Hipparchus also studied the motion of the Moon and confirmed the accurate values for two periods of its motion that Chaldean astronomers are widely presumed to have possessed before him, whatever their ultimate origin. The traditional value (from Babylonian System B) for the mean synodic month is 29 days; 31,50,8,20 (sexagesimal) = 29.5305941... days. Expressed as 29 days + 12 hours +  hours this value has been used later in the Hebrew calendar. The Chaldeans also knew that 251 synodic months ≈ 269 anomalistic months. Hipparchus used the multiple of this period by a factor of 17, because that interval is also an eclipse period, and is also close to an integer number of years (4267 moons : 4573 anomalistic periods : 4630.53 nodal periods : 4611.98 lunar orbits : 344.996 years : 344.982 solar orbits : 126,007.003 days : 126,351.985 rotations). What was so exceptional and useful about the cycle was that all 345-year-interval eclipse pairs occur slightly over 126,007 days apart within a tight range of only about ± hour, guaranteeing (after division by 4267) an estimate of the synodic month correct to one part in order of magnitude 10 million. The 345-year periodicity is why the ancients could conceive of a "mean" month and quantify it so accurately that it is even today correct to a fraction of a second of time.

Hipparchus could confirm his computations by comparing eclipses from his own time (presumably 27 January 141  and 26 November 139  according to [Toomer 1980]), with eclipses from Babylonian records 345 years earlier ("Almagest" IV.2; [A.Jones, 2001]). Already al-Biruni ("Qanun" VII.2.II) and Copernicus ("de revolutionibus" IV.4) noted that the period of 4,267 moons is actually about 5 minutes longer than the value for the eclipse period that Ptolemy attributes to Hipparchus. However, the timing methods of the Babylonians had an error of no less than 8 minutes. Modern scholars agree that Hipparchus rounded the eclipse period to the nearest hour, and used it to confirm the validity of the traditional values, rather than try to derive an improved value from his own observations. From modern ephemerides and taking account of the change in the length of the day (see ΔT) we estimate that the error in the assumed length of the synodic month was less than 0.2 seconds in the 4th century  and less than 0.1 seconds in Hipparchus's time.

It had been known for a long time that the motion of the Moon is not uniform: its speed varies. This is called its "anomaly", and it repeats with its own period; the anomalistic month. The Chaldeans took account of this arithmetically, and used a table giving the daily motion of the Moon according to the date within a long period. The Greeks however preferred to think in geometrical models of the sky. Apollonius of Perga had at the end of the 3rd century  proposed two models for lunar and planetary motion:

Hipparchus devised a geometrical method to find the parameters from three positions of the Moon, at particular phases of its anomaly. In fact, he did this separately for the eccentric and the epicycle model. Ptolemy describes the details in the "Almagest" IV.11. Hipparchus used two sets of three lunar eclipse observations, which he carefully selected to satisfy the requirements. The eccentric model he fitted to these eclipses from his Babylonian eclipse list: 22/23 December 383 , 18/19 June 382 , and 12/13 December 382 . The epicycle model he fitted to lunar eclipse observations made in Alexandria at 22 September 201 , 19 March 200 , and 11 September 200 .
The somewhat weird numbers are due to the cumbersome unit he used in his chord table according to one group of historians, who explain their reconstruction's inability to agree with these four numbers as partly due to some sloppy rounding and calculation errors by Hipparchus, for which Ptolemy criticised him (he himself made rounding errors too). A simpler alternate reconstruction agrees with all four numbers. Anyway, Hipparchus found inconsistent results; he later used the ratio of the epicycle model ( : ), which is too small (60 : 4;45 sexagesimal). Ptolemy established a ratio of 60 : . (The maximum angular deviation producible by this geometry is the arcsin of divided by 60, or about 5° 1', a figure that is sometimes therefore quoted as the equivalent of the Moon's equation of the center in the Hipparchan model.)

Before Hipparchus, Meton, Euctemon, and their pupils at Athens had made a solstice observation (i.e., timed the moment of the summer solstice) on 27 June 432  (proleptic Julian calendar). Aristarchus of Samos is said to have done so in 280 , and Hipparchus also had an observation by Archimedes. As shown in a 1991
paper, in 158 BC Hipparchus computed a very erroneous summer solstice from Callippus's calendar. He observed the summer solstice in 146 and 135  both accurate to a few hours, but observations of the moment of equinox were simpler, and he made twenty during his lifetime. Ptolemy gives an extensive discussion of Hipparchus's work on the length of the year in the "Almagest" III.1, and quotes many observations that Hipparchus made or used, spanning 162–128 . Analysis of Hipparchus's seventeen equinox observations made at Rhodes shows that the mean error in declination is positive seven arc minutes, nearly agreeing with the sum of refraction by air and Swerdlow's parallax. The random noise is two arc minutes or more nearly one arcminute if rounding is taken into account which approximately agrees with the sharpness of the eye. Ptolemy quotes an equinox timing by Hipparchus (at 24 March 146  at dawn) that differs by 5 hours from the observation made on Alexandria's large public equatorial ring that same day (at 1 hour before noon): Hipparchus may have visited Alexandria but he did not make his equinox observations there; presumably he was on Rhodes (at nearly the same geographical longitude). He could have used the equatorial ring of his armillary sphere or another equatorial ring for these observations, but Hipparchus (and Ptolemy) knew that observations with these instruments are sensitive to a precise alignment with the equator, so if he were restricted to an armillary, it would make more sense to use its meridian ring as a transit instrument. The problem with an equatorial ring (if an observer is naive enough to trust it very near dawn or dusk) is that atmospheric refraction lifts the Sun significantly above the horizon: so for a northern hemisphere observer its apparent declination is too high, which changes the observed time when the Sun crosses the equator. (Worse, the refraction decreases as the Sun rises and increases as it sets, so it may appear to move in the wrong direction with respect to the equator in the course of the day – as Ptolemy mentions. Ptolemy and Hipparchus apparently did not realize that refraction is the cause.) However, such details have doubtful relation to the data of either man, since there is no textual, scientific, or statistical ground for believing that their equinoxes were taken on an equatorial ring, which is useless for solstices in any case. Not one of two centuries of mathematical investigations of their solar errors has claimed to have traced them to the effect of refraction on use of an equatorial ring. Ptolemy claims his solar observations were on a transit instrument set in the meridian.

Recent expert translation and analysis by Anne Tihon of papyrus P. Fouad 267 A has confirmed the 1991 finding cited above that Hipparchus obtained a summer solstice in 158 BC But the papyrus makes the date 26 June, over a day earlier than the 1991 paper's conclusion for 28 June. The earlier study's §M found that Hipparchus did not adopt 26 June solstices until 146 BC when he founded the orbit of the Sun which Ptolemy later adopted. Dovetailing these data suggests Hipparchus extrapolated the 158 BC 26 June solstice from his 145 solstice 12 years later a procedure that would cause only minuscule error. The papyrus also confirmed that Hipparchus had used Callippic solar motion in 158 BC, a new finding in 1991 but not attested directly until P. Fouad 267 A. Another table on the papyrus is perhaps for sidereal motion and a third table is for Metonic tropical motion, using a previously unknown year of – days. This was presumably found by dividing the 274 years from 432 to 158 BC, into the corresponding interval of 100077 days and hours between Meton's sunrise and Hipparchus's sunset solstices.

At the end of his career, Hipparchus wrote a book called "Peri eniausíou megéthous" ("On the Length of the Year") about his results. The established value for the tropical year, introduced by Callippus in or before 330  was days. Speculating a Babylonian origin for the Callippic year is hard to defend, since Babylon did not observe solstices thus the only extant System B year length was based on Greek solstices (see below). Hipparchus's equinox observations gave varying results, but he himself points out (quoted in "Almagest" III.1(H195)) that the observation errors by himself and his predecessors may have been as large as day. He used old solstice observations, and determined a difference of about one day in about 300 years. So he set the length of the tropical year to − days (= 365.24666... days = 365 days 5 hours 55 min, which differs from the actual value (modern estimate, including earth spin acceleration) in his time of about 365.2425 days, an error of about 6 min per year, an hour per decade, 10 hours per century.

Between the solstice observation of Meton and his own, there were 297 years spanning 108,478 days. D. Rawlins noted that this implies a tropical year of 365.24579... days = 365 days;14,44,51 (sexagesimal; = 365 days + + + ) and that this exact year length has been found on one of the few Babylonian clay tablets which explicitly specifies the System B month. This is an indication that Hipparchus's work was known to Chaldeans.

Another value for the year that is attributed to Hipparchus (by the astrologer Vettius Valens in the 1st century) is 365 + + days (= 365.25347... days = 365 days 6 hours 5 min), but this may be a corruption of another value attributed to a Babylonian source: 365 + + days (= 365.25694... days = 365 days 6 hours 10 min). It is not clear if this would be a value for the sidereal year (actual value at his time (modern estimate) about 365.2565 days), but the difference with Hipparchus's value for the tropical year is consistent with his rate of precession (see below).

Before Hipparchus, astronomers knew that the lengths of the seasons are not equal. Hipparchus made observations of equinox and solstice, and according to Ptolemy ("Almagest" III.4) determined that spring (from spring equinox to summer solstice) lasted 94½ days, and summer (from summer solstice to autumn equinox) days. This is inconsistent with a premise of the Sun moving around the Earth in a circle at uniform speed. Hipparchus's solution was to place the Earth not at the center of the Sun's motion, but at some distance from the center. This model described the apparent motion of the Sun fairly well. It is known today that the planets, including the Earth, move in approximate ellipses around the Sun, but this was not discovered until Johannes Kepler published his first two laws of planetary motion in 1609. The value for the eccentricity attributed to Hipparchus by Ptolemy is that the offset is of the radius of the orbit (which is a little too large), and the direction of the apogee would be at longitude 65.5° from the vernal equinox. Hipparchus may also have used other sets of observations, which would lead to different values. One of his two eclipse trios' solar longitudes are consistent with his having initially adopted inaccurate lengths for spring and summer of and days. His other triplet of solar positions is consistent with and days, an improvement on the results ( and days) attributed to Hipparchus by Ptolemy, which a few scholars still question the authorship of. Ptolemy made no change three centuries later, and expressed lengths for the autumn and winter seasons which were already implicit (as shown, e.g., by A. Aaboe).

Hipparchus also undertook to find the distances and sizes of the Sun and the Moon. He published his results in a work of two books called "Perí megethōn kaí apostēmátōn" ("On Sizes and Distances") by Pappus in his commentary on the "Almagest" V.11; Theon of Smyrna (2nd century) mentions the work with the addition "of the Sun and Moon".

Hipparchus measured the apparent diameters of the Sun and Moon with his "diopter". Like others before and after him, he found that the Moon's size varies as it moves on its (eccentric) orbit, but he found no perceptible variation in the apparent diameter of the Sun. He found that at the "mean" distance of the Moon, the Sun and Moon had the same apparent diameter; at that distance, the Moon's diameter fits 650 times into the circle, i.e., the mean apparent diameters are = 0°33′14″.

Like others before and after him, he also noticed that the Moon has a noticeable parallax, i.e., that it appears displaced from its calculated position (compared to the Sun or stars), and the difference is greater when closer to the horizon. He knew that this is because in the then-current models the Moon circles the center of the Earth, but the observer is at the surface—the Moon, Earth and observer form a triangle with a sharp angle that changes all the time. From the size of this parallax, the distance of the Moon as measured in Earth radii can be determined. For the Sun however, there was no observable parallax (we now know that it is about 8.8", several times smaller than the resolution of the unaided eye).

In the first book, Hipparchus assumes that the parallax of the Sun is 0, as if it is at infinite distance. He then analyzed a solar eclipse, which Toomer (against the opinion of over a century of astronomers) presumes to be the eclipse of 14 March 190 . It was total in the region of the Hellespont (and in his birthplace, Nicaea); at the time Toomer proposes the Romans were preparing for war with Antiochus III in the area, and the eclipse is mentioned by Livy in his "Ab Urbe Condita Libri" VIII.2. It was also observed in Alexandria, where the Sun was reported to be obscured 4/5ths by the Moon. Alexandria and Nicaea are on the same meridian. Alexandria is at about 31° North, and the region of the Hellespont about 40° North. (It has been contended that authors like Strabo and Ptolemy had fairly decent values for these geographical positions, so Hipparchus must have known them too. However, Strabo's Hipparchus dependent latitudes for this region are at least 1° too high, and Ptolemy appears to copy them, placing Byzantium 2° high in latitude.) Hipparchus could draw a triangle formed by the two places and the Moon, and from simple geometry was able to establish a distance of the Moon, expressed in Earth radii. Because the eclipse occurred in the morning, the Moon was not in the meridian, and it has been proposed that as a consequence the distance found by Hipparchus was a lower limit. In any case, according to Pappus, Hipparchus found that the least distance is 71 (from this eclipse), and the greatest 81 Earth radii.

In the second book, Hipparchus starts from the opposite extreme assumption: he assigns a (minimum) distance to the Sun of 490 Earth radii. This would correspond to a parallax of 7′, which is apparently the greatest parallax that Hipparchus thought would not be noticed (for comparison: the typical resolution of the human eye is about 2′; Tycho Brahe made naked eye observation with an accuracy down to 1′). In this case, the shadow of the Earth is a cone rather than a cylinder as under the first assumption. Hipparchus observed (at lunar eclipses) that at the mean distance of the Moon, the diameter of the shadow cone is lunar diameters. That apparent diameter is, as he had observed, degrees. With these values and simple geometry, Hipparchus could determine the mean distance; because it was computed for a minimum distance of the Sun, it is the maximum mean distance possible for the Moon. With his value for the eccentricity of the orbit, he could compute the least and greatest distances of the Moon too. According to Pappus, he found a least distance of 62, a mean of , and consequently a greatest distance of Earth radii. With this method, as the parallax of the Sun decreases (i.e., its distance increases), the minimum limit for the mean distance is 59 Earth radii – exactly the mean distance that Ptolemy later derived.

Hipparchus thus had the problematic result that his minimum distance (from book 1) was greater than his maximum mean distance (from book 2). He was intellectually honest about this discrepancy, and probably realized that especially the first method is very sensitive to the accuracy of the observations and parameters. (In fact, modern calculations show that the size of the 189  solar eclipse at Alexandria must have been closer to ths and not the reported ths, a fraction more closely matched by the degree of totality at Alexandria of eclipses occurring in 310 and 129  which were also nearly total in the Hellespont and are thought by many to be more likely possibilities for the eclipse Hipparchus used for his computations.)

Ptolemy later measured the lunar parallax directly ("Almagest" V.13), and used the second method of Hipparchus with lunar eclipses to compute the distance of the Sun ("Almagest" V.15). He criticizes Hipparchus for making contradictory assumptions, and obtaining conflicting results ("Almagest" V.11): but apparently he failed to understand Hipparchus's strategy to establish limits consistent with the observations, rather than a single value for the distance. His results were the best so far: the actual mean distance of the Moon is 60.3 Earth radii, within his limits from Hipparchus's second book.

Theon of Smyrna wrote that according to Hipparchus, the Sun is 1,880 times the size of the Earth, and the Earth twenty-seven times the size of the Moon; apparently this refers to volumes, not diameters. From the geometry of book 2 it follows that the Sun is at 2,550 Earth radii, and the mean distance of the Moon is radii. Similarly, Cleomedes quotes Hipparchus for the sizes of the Sun and Earth as 1050:1; this leads to a mean lunar distance of 61 radii. Apparently Hipparchus later refined his computations, and derived accurate single values that he could use for predictions of solar eclipses.

See [Toomer 1974] for a more detailed discussion.

Pliny ("Naturalis Historia" II.X) tells us that Hipparchus demonstrated that lunar eclipses can occur five months apart, and solar eclipses seven months (instead of the usual six months); and the Sun can be hidden twice in thirty days, but as seen by different nations. Ptolemy discussed this a century later at length in "Almagest" VI.6. The geometry, and the limits of the positions of Sun and Moon when a solar or lunar eclipse is possible, are explained in "Almagest" VI.5. Hipparchus apparently made similar calculations. The result that two solar eclipses can occur one month apart is important, because this can not be based on observations: one is visible on the northern and the other on the southern hemisphere – as Pliny indicates – and the latter was inaccessible to the Greek.

Prediction of a solar eclipse, i.e., exactly when and where it will be visible, requires a solid lunar theory and proper treatment of the lunar parallax. Hipparchus must have been the first to be able to do this. A rigorous treatment requires spherical trigonometry, thus those who remain certain that Hipparchus lacked it must speculate that he may have made do with planar approximations. He may have discussed these things in "Perí tēs katá plátos mēniaías tēs selēnēs kinēseōs" ("On the monthly motion of the Moon in latitude"), a work mentioned in the "Suda".

Pliny also remarks that "he also discovered for what exact reason, although the shadow causing the eclipse must from sunrise onward be below the earth, it happened once in the past that the Moon was eclipsed in the west while both luminaries were visible above the earth" (translation H. Rackham (1938), Loeb Classical Library 330 p. 207). Toomer (1980) argued that this must refer to the large total lunar eclipse of 26 November 139 , when over a clean sea horizon as seen from Rhodes, the Moon was eclipsed in the northwest just after the Sun rose in the southeast. This would be the second eclipse of the 345-year interval that Hipparchus used to verify the traditional Babylonian periods: this puts a late date to the development of Hipparchus's lunar theory. We do not know what "exact reason" Hipparchus found for seeing the Moon eclipsed while apparently it was not in exact opposition to the Sun. Parallax lowers the altitude of the luminaries; refraction raises them, and from a high point of view the horizon is lowered.

Hipparchus and his predecessors used various instruments for astronomical calculations and observations, such as the gnomon, the astrolabe, and the armillary sphere.

Hipparchus is credited with the invention or improvement of several astronomical instruments, which were used for a long time for naked-eye observations. According to Synesius of Ptolemais (4th century) he made the first "astrolabion": this may have been an armillary sphere (which Ptolemy however says he constructed, in "Almagest" V.1); or the predecessor of the planar instrument called astrolabe (also mentioned by Theon of Alexandria). With an astrolabe Hipparchus was the first to be able to measure the geographical latitude and time by observing fixed stars. Previously this was done at daytime by measuring the shadow cast by a gnomon, by recording the length of the longest day of the year or with the portable instrument known as a "scaphe".

Ptolemy mentions ("Almagest" V.14) that he used a similar instrument as Hipparchus, called "dioptra", to measure the apparent diameter of the Sun and Moon. Pappus of Alexandria described it (in his commentary on the "Almagest" of that chapter), as did Proclus ("Hypotyposis" IV). It was a 4-foot rod with a scale, a sighting hole at one end, and a wedge that could be moved along the rod to exactly obscure the disk of Sun or Moon.

Hipparchus also observed solar equinoxes, which may be done with an equatorial ring: its shadow falls on itself when the Sun is on the equator (i.e., in one of the equinoctial points on the ecliptic), but the shadow falls above or below the opposite side of the ring when the Sun is south or north of the equator. Ptolemy quotes (in "Almagest" III.1 (H195)) a description by Hipparchus of an equatorial ring in Alexandria; a little further he describes two such instruments present in Alexandria in his own time.

Hipparchus applied his knowledge of spherical angles to the problem of denoting locations on the Earth's surface. Before him a grid system had been used by Dicaearchus of Messana, but Hipparchus was the first to apply mathematical rigor to the determination of the latitude and longitude of places on the Earth. Hipparchus wrote a critique in three books on the work of the geographer Eratosthenes of Cyrene (3rd century ), called "Pròs tèn Eratosthénous geographían" ("Against the Geography of Eratosthenes"). It is known to us from Strabo of Amaseia, who in his turn criticised Hipparchus in his own "Geographia". Hipparchus apparently made many detailed corrections to the locations and distances mentioned by Eratosthenes. It seems he did not introduce many improvements in methods, but he did propose a means to determine the geographical longitudes of different cities at lunar eclipses (Strabo "Geographia" 1 January 2012). A lunar eclipse is visible simultaneously on half of the Earth, and the difference in longitude between places can be computed from the difference in local time when the eclipse is observed. His approach would give accurate results if it were correctly carried out but the limitations of timekeeping accuracy in his era made this method impractical.

Late in his career (possibly about 135 ) Hipparchus compiled his star catalog, the original of which does not survive. He also constructed a celestial globe depicting the constellations, based on his observations. His interest in the fixed stars may have been inspired by the observation of a supernova (according to Pliny), or by his discovery of precession, according to Ptolemy, who says that Hipparchus could not reconcile his data with earlier observations made by Timocharis and Aristillus. For more information see Discovery of precession. In Raphael's painting "The School of Athens", Hipparchus is depicted holding his celestial globe, as the representative figure for astronomy.

Previously, Eudoxus of Cnidus in the 4th century  had described the stars and constellations in two books called "Phaenomena" and "Entropon". Aratus wrote a poem called "Phaenomena" or "Arateia" based on Eudoxus's work. Hipparchus wrote a commentary on the "Arateia" – his only preserved work – which contains many stellar positions and times for rising, culmination, and setting of the constellations, and these are likely to have been based on his own measurements.

Hipparchus made his measurements with an armillary sphere, and obtained the positions of at least 850 stars. It is disputed which coordinate system(s) he used. Ptolemy's catalog in the "Almagest", which is derived from Hipparchus's catalog, is given in ecliptic coordinates. However Delambre in his "Histoire de l'Astronomie Ancienne" (1817) concluded that Hipparchus knew and used the equatorial coordinate system, a conclusion challenged by Otto Neugebauer in his "A History of Ancient Mathematical Astronomy" (1975). Hipparchus seems to have used a mix of ecliptic coordinates and equatorial coordinates: in his commentary on Eudoxos he provides stars' polar distance (equivalent to the declination in the equatorial system), right ascension (equatorial), longitude (ecliptical), polar longitude (hybrid), but not celestial latitude.

As with most of his work, Hipparchus's star catalog was adopted and perhaps expanded by Ptolemy. Delambre, in 1817, cast doubt on Ptolemy's work. It was disputed whether the star catalog in the "Almagest" is due to Hipparchus, but 1976–2002 statistical and spatial analyses (by R. R. Newton, Dennis Rawlins, Gerd Grasshoff, Keith Pickering and Dennis Duke) have shown conclusively that the "Almagest" star catalog is almost entirely Hipparchan. Ptolemy has even (since Brahe, 1598) been accused by astronomers of fraud for stating ("Syntaxis", book 7, chapter 4) that he observed all 1025 stars: for almost every star he used Hipparchus's data and precessed it to his own epoch centuries later by adding 2°40' to the longitude, using an erroneously small precession constant of 1° per century.

In any case the work started by Hipparchus has had a lasting heritage, and was much later updated by Al Sufi (964) and Copernicus (1543). Ulugh Beg reobserved all the Hipparchus stars he could see from Samarkand in 1437 to about the same accuracy as Hipparchus's. The catalog was superseded only in the late 16th century by Brahe and Wilhelm IV of Kassel via superior ruled instruments and spherical trigonometry, which improved accuracy by an order of magnitude even before the invention of the telescope. Hipparchus is considered the greatest observational astronomer from classical antiquity until Brahe.

Hipparchus is only conjectured to have ranked the apparent magnitudes of stars on a numerical scale from 1, the brightest, to 6, the faintest. Nevertheless, this system certainly precedes Ptolemy, who used it extensively about 150. This system was made more precise and extended by N. R. Pogson in 1856, who placed the magnitudes on a logarithmic scale, making magnitude 1 stars 100 times brighter than magnitude 6 stars, thus each magnitude is or 2.512 times brighter than the next faintest magnitude.

Hipparchus is generally recognized as discoverer of the precession of the equinoxes in 127 . His two books on precession, "On the Displacement of the Solsticial and Equinoctial Points" and "On the Length of the Year", are both mentioned in the "Almagest" of Claudius Ptolemy. According to Ptolemy, Hipparchus measured the longitude of Spica and Regulus and other bright stars. Comparing his measurements with data from his predecessors, Timocharis and Aristillus, he concluded that Spica had moved 2° relative to the autumnal equinox. He also compared the lengths of the tropical year (the time it takes the Sun to return to an equinox) and the sidereal year (the time it takes the Sun to return to a fixed star), and found a slight discrepancy. Hipparchus concluded that the equinoxes were moving ("precessing") through the zodiac, and that the rate of precession was not less than 1° in a century.

Hipparchus's treatise "Against the Geography of Eratosthenes" in three books is not preserved. 
Most of our knowledge of it comes from Strabo, according to whom Hipparchus thoroughly and often unfairly criticized Eratosthenes, mainly for internal contradictions and inaccuracy in determining positions of geographical localities. Hipparchus insists that a geographic map must be based only on astronomical measurements of latitudes and longitudes and triangulation for finding unknown distances. 
In geographic theory and methods Hipparchus introduced three main innovations.

He was the first to use the grade grid, to determine geographic latitude from star observations, and not only from the Sun's altitude, a method known long before him, and to suggest that geographic longitude could be determined by means of simultaneous observations of lunar eclipses in distant places. In the practical part of his work, the so-called "table of climata", Hipparchus listed latitudes for several tens of localities. In particular, he improved Eratosthenes' values for the latitudes of Athens, Sicily, and southern extremity of India. 
In calculating latitudes of climata (latitudes correlated with the length of the longest solstitial day), Hipparchus used an unexpectedly accurate value for the obliquity of the ecliptic, 23°40' (the actual value in the second half of the 2nd century  was approximately 23°43'), whereas all other ancient authors knew only a roughly rounded value 24°, and even Ptolemy used a less accurate value, 23°51'. 

Hipparchus opposed the view generally accepted in the Hellenistic period that the Atlantic and Indian Oceans and the Caspian Sea are parts of a single ocean. At the same time he extends the limits of the oikoumene, i.e. the inhabited part of the land, up to the equator and the Arctic Circle. 
Hipparchus' ideas found their reflection in the "Geography" of Ptolemy. In essence, Ptolemy's work is an extended attempt to realize Hipparchus' vision of what geography ought to be.

He is depicted opposite Ptolemy in Raphael's painting The School of Athens, although this figure is popularly believed to be Strabo or Zoroaster.

The rather cumbersome formal name for the ESA's Hipparcos Space Astrometry Mission was High Precision Parallax Collecting Satellite; it was deliberately named in this way to give an acronym, HiPParCoS, that echoed and commemorated the name of Hipparchus. The lunar crater Hipparchus and the asteroid 4000 Hipparchus are more directly named after him.

He was inducted into the International Space Hall of Fame in 2004.

The Astronomer's Monument at the Griffith Observatory in Los Angeles, California, United States features a relief of Hipparchus as one of six of the greatest astronomers of all time and the only one from Antiquity.




General

Precession

Celestial bodies

Star catalog


</doc>
<doc id="13601" url="https://en.wikipedia.org/wiki?curid=13601" title="Hebrew (disambiguation)">
Hebrew (disambiguation)

The Hebrew language is a language native to Israel.

Hebrew may also refer to:






</doc>
<doc id="13602" url="https://en.wikipedia.org/wiki?curid=13602" title="Huldrych Zwingli">
Huldrych Zwingli

Huldrych Zwingli or Ulrich Zwingli (1 January 1484 – 11 October 1531) was a leader of the Reformation in Switzerland, born during a time of emerging Swiss patriotism and increasing criticism of the Swiss mercenary system. He attended the University of Vienna and the University of Basel, a scholarly center of Renaissance humanism. He continued his studies while he served as a pastor in Glarus and later in Einsiedeln, where he was influenced by the writings of Erasmus.

In 1519, Zwingli became the pastor of the Grossmünster in Zürich where he began to preach ideas on reform of the Catholic Church. In his first public controversy in 1522, he attacked the custom of fasting during Lent. In his publications, he noted corruption in the ecclesiastical hierarchy, promoted clerical marriage, and attacked the use of images in places of worship. In 1525, he introduced a new communion liturgy to replace the Mass. He also clashed with the Anabaptists, which resulted in their persecution. Historians have debated whether or not he turned Zürich into a theocracy.

The Reformation spread to other parts of the Swiss Confederation, but several cantons resisted, preferring to remain Catholic. Zwingli formed an alliance of Reformed cantons which divided the Confederation along religious lines. In 1529, a war was averted at the last moment between the two sides. Meanwhile, Zwingli's ideas came to the attention of Martin Luther and other reformers. They met at the Marburg Colloquy and agreed on many points of doctrine, but they could not reach an accord on the doctrine of the Real Presence of Christ in the Eucharist.

In 1531, Zwingli's alliance applied an unsuccessful food blockade on the Catholic cantons. The cantons responded with an attack at a moment when Zürich was ill-prepared, and Zwingli died on the battlefield. His legacy lives on in the confessions, liturgy, and church orders of the Reformed churches of today.

The Swiss Confederation in Huldrych Zwingli's time consisted of thirteen states (cantons) as well as affiliated areas and common lordships. Unlike the modern state of Switzerland, which operates under a federal government, each of the thirteen cantons was nearly independent, conducting its own domestic and foreign affairs. Each canton formed its own alliances within and without the Confederation. This relative independence served as the basis for conflict during the time of the Reformation when the various cantons divided between different confessional camps. Military ambitions gained an additional impetus with the competition to acquire new territory and resources, as seen for example in the Old Zürich War of 1440–1446.

The wider political environment in Europe during the 15th and 16th centuries was also volatile. For centuries the relationship with the Confederation's powerful neighbour, France, determined the foreign policies of the Swiss. Nominally, the Confederation formed a part of the Holy Roman Empire. However, through a succession of wars culminating in the Swabian War in 1499, the Confederation had become "de facto" independent. As the two continental powers and minor regional states such as the Duchy of Milan, the Duchy of Savoy, and the Papal States competed and fought against each other, there were far-reaching political, economic, and social consequences for the Confederation. During this time the mercenary pension system became a subject of disagreement. The religious factions of Zwingli's time debated vociferously the merits of sending young Swiss men to fight in foreign wars mainly for the enrichment of the cantonal authorities.

These internal and external factors contributed to the rise of a Confederation national consciousness, in which the term "fatherland" () began to take on meaning beyond a reference to an individual canton. At the same time, Renaissance humanism, with its universal values and emphasis on scholarship (as exemplified by Erasmus (1466–1536), the "prince of humanism"), had taken root in the Confederation. Within this environment, defined by the confluence of Swiss patriotism and humanism, Zwingli was born in 1484.

Huldrych Zwingli was born on 1 January 1484 in Wildhaus, in the Toggenburg valley of Switzerland, to a family of farmers, the third child of nine. His father, Ulrich, played a leading role in the administration of the community ("Amtmann" or chief local magistrate). Zwingli's primary schooling was provided by his uncle, Bartholomew, a cleric in Weesen, where he probably met Katharina von Zimmern. At ten years old, Zwingli was sent to Basel to obtain his secondary education where he learned Latin under Magistrate Gregory Bünzli. After three years in Basel, he stayed a short time in Bern with the humanist, Henry Wölfflin. The Dominicans in Bern tried to persuade Zwingli to join their order and it is possible that he was received as a novice. However, his father and uncle disapproved of such a course and he left Bern without completing his Latin studies. He enrolled in the University of Vienna in the winter semester of 1498 but was expelled, according to the university's records. However, it is not certain that Zwingli was indeed expelled, and he re-enrolled in the summer semester of 1500; his activities in 1499 are unknown. Zwingli continued his studies in Vienna until 1502, after which he transferred to the University of Basel where he received the Master of Arts degree ("Magister") in 1506.

Zwingli was ordained in Constance, the seat of the local diocese, and he celebrated his first Mass in his hometown, Wildhaus, on 29 September 1506. As a young priest he had studied little theology, but this was not considered unusual at the time. His first ecclesiastical post was the pastorate of the town of Glarus, where he stayed for ten years. It was in Glarus, whose soldiers were used as mercenaries in Europe, that Zwingli became involved in politics. The Swiss Confederation was embroiled in various campaigns with its neighbours: the French, the Habsburgs, and the Papal States. Zwingli placed himself solidly on the side of the Roman See. In return, Pope Julius II honoured Zwingli by providing him with an annual pension. He took the role of chaplain in several campaigns in Italy, including the Battle of Novara in 1513. However, the decisive defeat of the Swiss in the Battle of Marignano caused a shift in mood in Glarus in favour of the French rather than the pope. Zwingli, the papal partisan, found himself in a difficult position and he decided to retreat to Einsiedeln in the canton of Schwyz. By this time, he had become convinced that mercenary service was immoral and that Swiss unity was indispensable for any future achievements. Some of his earliest extant writings, such as "The Ox" (1510) and "The Labyrinth" (1516), attacked the mercenary system using allegory and satire. His countrymen were presented as virtuous people within a French, imperial, and papal triangle. Zwingli stayed in Einsiedeln for two years during which he withdrew completely from politics in favour of ecclesiastical activities and personal studies.

Zwingli's time as the pastor of Glarus and Einsiedeln was characterized by inner growth and development. He perfected his Greek and he took up the study of Hebrew. His library contained over three hundred volumes from which he was able to draw upon classical, patristic, and scholastic works. He exchanged scholarly letters with a circle of Swiss humanists and began to study the writings of Erasmus. Zwingli took the opportunity to meet him while Erasmus was in Basel between August 1514 and May 1516. Zwingli's turn to relative pacifism and his focus on preaching can be traced to the influence of Erasmus.

In late 1518, the post of the "Leutpriestertum" (people's priest) of the Grossmünster at Zürich became vacant. The canons of the foundation that administered the Grossmünster recognised Zwingli's reputation as a fine preacher and writer. His connection with humanists was a decisive factor as several canons were sympathetic to Erasmian reform. In addition, his opposition to the French and to mercenary service was welcomed by Zürich politicians. On 11 December 1518, the canons elected Zwingli to become the stipendiary priest and on 27 December he moved permanently to Zürich.

On 1 January 1519, Zwingli gave his first sermon in Zürich. Deviating from the prevalent practice of basing a sermon on the Gospel lesson of a particular Sunday, Zwingli, using Erasmus' New Testament as a guide, began to read through the Gospel of Matthew, giving his interpretation during the sermon, known as the method of "lectio continua". He continued to read and interpret the book on subsequent Sundays until he reached the end and then proceeded in the same manner with the Acts of the Apostles, the New Testament epistles, and finally the Old Testament. His motives for doing this are not clear, but in his sermons he used exhortation to achieve moral and ecclesiastical improvement which were goals comparable with Erasmian reform. Sometime after 1520, Zwingli's theological model began to evolve into an idiosyncratic form that was neither Erasmian nor Lutheran. Scholars do not agree on the process of how he developed his own unique model. One view is that Zwingli was trained as an Erasmian humanist and Luther played a decisive role in changing his theology. Another view is that Zwingli did not pay much attention to Luther's theology and in fact he considered it as part of the humanist reform movement. A third view is that Zwingli was not a complete follower of Erasmus, but had diverged from him as early as 1516 and that he independently developed his theology.

Zwingli's theological stance was gradually revealed through his sermons. He attacked moral corruption and in the process he named individuals who were the targets of his denunciations. Monks were accused of indolence and high living. In 1519, Zwingli specifically rejected the veneration of saints and called for the need to distinguish between their true and fictional accounts. He cast doubts on hellfire, asserted that unbaptised children were not damned, and questioned the power of excommunication. His attack on the claim that tithing was a divine institution, however, had the greatest theological and social impact. This contradicted the immediate economic interests of the foundation. One of the elderly canons who had supported Zwingli's election, Konrad Hofmann, complained about his sermons in a letter. Some canons supported Hofmann, but the opposition never grew very large. Zwingli insisted that he was not an innovator and that the sole basis of his teachings was Scripture.

Within the diocese of Constance, Bernhardin Sanson was offering a special indulgence for contributors to the building of St Peter's in Rome. When Sanson arrived at the gates of Zürich at the end of January 1519, parishioners prompted Zwingli with questions. He responded with displeasure that the people were not being properly informed about the conditions of the indulgence and were being induced to part with their money on false pretences. This was over a year after Martin Luther published his Ninety-five theses (31 October 1517). The council of Zürich refused Sanson entry into the city. As the authorities in Rome were anxious to contain the fire started by Luther, the Bishop of Constance denied any support of Sanson and he was recalled.

In August 1519, Zürich was struck by an outbreak of the plague during which at least one in four persons died. All of those who could afford it left the city, but Zwingli remained and continued his pastoral duties. In September, he caught the disease and nearly died. He described his preparation for death in a poem, Zwingli's "Pestlied", consisting of three parts: the onset of the illness, the closeness to death, and the joy of recovery. The final verses of the first part read:

In the years following his recovery, Zwingli's opponents remained in the minority. When a vacancy occurred among the canons of the Grossmünster, Zwingli was elected to fulfill that vacancy on 29 April 1521. In becoming a canon, he became a full citizen of Zürich. He also retained his post as the people's priest of the Grossmünster.

The first public controversy regarding Zwingli's preaching broke out during the season of Lent in 1522. On the first fasting Sunday, 9 March, Zwingli and about a dozen other participants consciously transgressed the fasting rule by cutting and distributing two smoked sausages (the "Wurstessen" in Christoph Froschauer's workshop). Zwingli defended this act in a sermon which was published on 16 April, under the title "Von Erkiesen und Freiheit der Speisen" (Regarding the Choice and Freedom of Foods). He noted that no general valid rule on food can be derived from the Bible and that to transgress such a rule is not a sin. The event, which came to be referred to as the Affair of the Sausages, is considered to be the start of the Reformation in Switzerland. Even before the publication of this treatise, the diocese of Constance reacted by sending a delegation to Zürich. The city council condemned the fasting violation, but assumed responsibility over ecclesiastical matters and requested the religious authorities clarify the issue. The bishop responded on 24 May by admonishing the Grossmünster and city council and repeating the traditional position.

Following this event, Zwingli and other humanist friends petitioned the bishop on 2 July to abolish the requirement of celibacy on the clergy. Two weeks later the petition was reprinted for the public in German as "Eine freundliche Bitte und Ermahnung an die Eidgenossen" (A Friendly Petition and Admonition to the Confederates). The issue was not just an abstract problem for Zwingli, as he had secretly married a widow, Anna Reinhart, earlier in the year. Their cohabitation was well-known and their public wedding took place on 2 April 1524, three months before the birth of their first child. They would eventually have four children: Regula, William, Huldrych, and Anna. As the petition was addressed to the secular authorities, the bishop responded at the same level by notifying the Zürich government to maintain the ecclesiastical order. Other Swiss clergymen joined in Zwingli's cause which encouraged him to make his first major statement of faith, "Apologeticus Archeteles" (The First and Last Word). He defended himself against charges of inciting unrest and heresy. He denied the ecclesiastical hierarchy any right to judge on matters of church order because of its corrupted state.

The events of 1522 brought no clarification on the issues. Not only did the unrest between Zürich and the bishop continue, tensions were growing among Zürich's Confederation partners in the Swiss Diet. On 22 December, the Diet recommended that its members prohibit the new teachings, a strong indictment directed at Zürich. The city council felt obliged to take the initiative and find its own solution.

On 3 January 1523, the Zürich city council invited the clergy of the city and outlying region to a meeting to allow the factions to present their opinions. The bishop was invited to attend or to send a representative. The council would render a decision on who would be allowed to continue to proclaim their views. This meeting, the first Zürich disputation, took place on 29 January 1523.

The meeting attracted a large crowd of approximately six hundred participants. The bishop sent a delegation led by his vicar general, Johannes Fabri. Zwingli summarised his position in the "Schlussreden" (Concluding Statements or the Sixty-seven Articles). Fabri, who had not envisaged an academic disputation in the manner Zwingli had prepared for, was forbidden to discuss high theology before laymen, and simply insisted on the necessity of the ecclesiastical authority. The decision of the council was that Zwingli would be allowed to continue his preaching and that all other preachers should teach only in accordance with Scripture.

In September 1523, Leo Jud, Zwingli's closest friend and colleague and pastor of St. Peterskirche, publicly called for the removal of statues of saints and other icons. This led to demonstrations and iconoclastic activities. The city council decided to work out the matter of images in a second disputation. The essence of the mass and its sacrificial character was also included as a subject of discussion. Supporters of the mass claimed that the eucharist was a true sacrifice, while Zwingli claimed that it was a commemorative meal. As in the first disputation, an invitation was sent out to the Zürich clergy and the bishop of Constance. This time, however, the lay people of Zürich, the dioceses of Chur and Basel, the University of Basel, and the twelve members of the Confederation were also invited. About nine hundred persons attended this meeting, but neither the bishop nor the Confederation sent representatives. The disputation started on 26 October 1523 and lasted two days.

Zwingli again took the lead in the disputation. His opponent was the aforementioned canon, Konrad Hofmann, who had initially supported Zwingli's election. Also taking part was a group of young men demanding a much faster pace of reformation, who among other things pleaded for replacing infant baptism with adult baptism. This group was led by Conrad Grebel, one of the initiators of the Anabaptist movement. During the first three days of dispute, although the controversy of images and the mass were discussed, the arguments led to the question of whether the city council or the ecclesiastical government had the authority to decide on these issues. At this point, Konrad Schmid, a priest from Aargau and follower of Zwingli, made a pragmatic suggestion. As images were not yet considered to be valueless by everyone, he suggested that pastors preach on this subject under threat of punishment. He believed the opinions of the people would gradually change and the voluntary removal of images would follow. Hence, Schmid rejected the radicals and their iconoclasm, but supported Zwingli's position. In November the council passed ordinances in support of Schmid's motion. Zwingli wrote a booklet on the evangelical duties of a minister, "Kurze, christliche Einleitung" (Short Christian Introduction), and the council sent it out to the clergy and the members of the Confederation.

In December 1523, the council set a deadline of Pentecost in 1524 for a solution to the elimination of the mass and images. Zwingli gave a formal opinion in "Vorschlag wegen der Bilder und der Messe" (Proposal Concerning Images and the Mass). He did not urge an immediate, general abolition. The council decided on the orderly removal of images within Zürich, but rural congregations were granted the right to remove them based on majority vote. The decision on the mass was postponed.

Evidence of the effect of the Reformation was seen in early 1524. Candlemas was not celebrated, processions of robed clergy ceased, worshippers did not go with palms or relics on Palm Sunday to the Lindenhof, and triptychs remained covered and closed after Lent. Opposition to the changes came from Konrad Hofmann and his followers, but the council decided in favour of keeping the government mandates. When Hofmann left the city, opposition from pastors hostile to the Reformation broke down. The bishop of Constance tried to intervene in defending the mass and the veneration of images. Zwingli wrote an official response for the council and the result was the severance of all ties between the city and the diocese.

Although the council had hesitated in abolishing the mass, the decrease in the exercise of traditional piety allowed pastors to be unofficially released from the requirement of celebrating mass. As individual pastors altered their practices as each saw fit, Zwingli was prompted to address this disorganised situation by designing a communion liturgy in the German language. This was published in "Aktion oder Brauch des Nachtmahls" (Act or Custom of the Supper). Shortly before Easter, Zwingli and his closest associates requested the council to cancel the mass and to introduce the new public order of worship. On Maundy Thursday, 13 April 1525, Zwingli celebrated communion under his new liturgy. Wooden cups and plates were used to avoid any outward displays of formality. The congregation sat at set tables to emphasise the meal aspect of the sacrament. The sermon was the focal point of the service and there was no organ music or singing. The importance of the sermon in the worship service was underlined by Zwingli's proposal to limit the celebration of communion to four times a year.

For some time Zwingli had accused mendicant orders of hypocrisy and demanded their abolition in order to support the truly poor. He suggested the monasteries be changed into hospitals and welfare institutions and incorporate their wealth into a welfare fund. This was done by reorganising the foundations of the Grossmünster and Fraumünster and pensioning off remaining nuns and monks. The council secularised the church properties (Fraumünster handed over by Zwingli's acquaintance Katharina von Zimmern) and established new welfare programs for the poor. Zwingli requested permission to establish a Latin school, the "Prophezei" (Prophecy) or "Carolinum", at the Grossmünster. The council agreed and it was officially opened on 19 June 1525 with Zwingli and Jud as teachers. It served to retrain and re-educate the clergy. The Zürich Bible translation, traditionally attributed to Zwingli and printed by Christoph Froschauer, bears the mark of teamwork from the Prophecy school. Scholars have not yet attempted to clarify Zwingli's share of the work based on external and stylistic evidence.

Shortly after the second Zürich disputation, many in the radical wing of the Reformation became convinced that Zwingli was making too many concessions to the Zürich council. They rejected the role of civil government and demanded the immediate establishment of a congregation of the faithful. Conrad Grebel, the leader of the radicals and the emerging Anabaptist movement, spoke disparagingly of Zwingli in private. On 15 August 1524 the council insisted on the obligation to baptise all newborn infants. Zwingli secretly conferred with Grebel's group and late in 1524, the council called for official discussions. When talks were broken off, Zwingli published "Wer Ursache gebe zu Aufruhr" (Whoever Causes Unrest) clarifying the opposing points-of-view. On 17 January 1525 a public debate was held and the council decided in favour of Zwingli. Anyone refusing to have their children baptised was required to leave Zürich. The radicals ignored these measures and on 21 January, they met at the house of the mother of another radical leader, Felix Manz. Grebel and a third leader, George Blaurock, performed the first recorded Anabaptist adult baptisms.

On 2 February, the council repeated the requirement on the baptism of all babies and some who failed to comply were arrested and fined, Manz and Blaurock among them. Zwingli and Jud interviewed them and more debates were held before the Zürich council. Meanwhile, the new teachings continued to spread to other parts of the Confederation as well as a number of Swabian towns. On 6–8 November, the last debate on the subject of baptism took place in the Grossmünster. Grebel, Manz, and Blaurock defended their cause before Zwingli, Jud, and other reformers. There was no serious exchange of views as each side would not move from their positions and the debates degenerated into an uproar, each side shouting abuse at the other.

The Zürich council decided that no compromise was possible. On 7 March 1526 it released the notorious mandate that no one shall rebaptise another under the penalty of death. Although Zwingli, technically, had nothing to do with the mandate, there is no indication that he disapproved. Felix Manz, who had sworn to leave Zürich and not to baptise any more, had deliberately returned and continued the practice. After he was arrested and tried, he was executed on 5 January 1527 by being drowned in the Limmat. He was the first Anabaptist martyr; three more were to follow, after which all others either fled or were expelled from Zürich.

On 8 April 1524, five cantons, Lucerne, Uri, Schwyz, Unterwalden, and Zug, formed an alliance, "die fünf Orte" (the Five States) to defend themselves from Zwingli's Reformation. They contacted the opponents of Martin Luther including John Eck, who had debated Luther in the Leipzig Disputation of 1519. Eck offered to dispute Zwingli and he accepted. However, they could not agree on the selection of the judging authority, the location of the debate, and the use of the Swiss Diet as a court. Because of the disagreements, Zwingli decided to boycott the disputation. On 19 May 1526, all the cantons sent delegates to Baden. Although Zürich's representatives were present, they did not participate in the sessions. Eck led the Catholic party while the reformers were represented by Johannes Oecolampadius of Basel, a theologian from Württemberg who had carried on an extensive and friendly correspondence with Zwingli. While the debate proceeded, Zwingli was kept informed of the proceedings and printed pamphlets giving his opinions. It was of little use as the Diet decided against Zwingli. He was to be banned and his writings were no longer to be distributed. Of the thirteen Confederation members, Glarus, Solothurn, Fribourg, and Appenzell as well as the Five States voted against Zwingli. Bern, Basel, Schaffhausen, and Zürich supported him.

The Baden disputation exposed a deep rift in the Confederation on matters of religion. The Reformation was now emerging in other states. The city of St Gallen, an affiliated state to the Confederation, was led by a reformed mayor, Joachim Vadian, and the city abolished the mass in 1527, just two years after Zürich. In Basel, although Zwingli had a close relationship with Oecolampadius, the government did not officially sanction any reformatory changes until 1 April 1529 when the mass was prohibited. Schaffhausen, which had closely followed Zürich's example, formally adopted the Reformation in September 1529. In the case of Bern, Berchtold Haller, the priest at St Vincent Münster, and Niklaus Manuel, the poet, painter, and politician, had campaigned for the reformed cause. But it was only after another disputation that Bern counted itself as a canton of the Reformation. Four hundred and fifty persons participated, including pastors from Bern and other cantons as well as theologians from outside the Confederation such as Martin Bucer and Wolfgang Capito from Strasbourg, Ambrosius Blarer from Constance, and Andreas Althamer from Nuremberg. Eck and Fabri refused to attend and the Catholic cantons did not send representatives. The meeting started on 6 January 1528 and lasted nearly three weeks. Zwingli assumed the main burden of defending the Reformation and he preached twice in the Münster. On 7 February 1528 the council decreed that the Reformation be established in Bern.

Even before the Bern disputation, Zwingli was canvassing for an alliance of reformed cities. Once Bern officially accepted the Reformation, a new alliance, "das Christliche Burgrecht" (the Christian Civic Union) was created. The first meetings were held in Bern between representatives of Bern, Constance, and Zürich on 5–6 January 1528. Other cities, including Basel, Biel, Mülhausen, Schaffhausen, and St Gallen, eventually joined the alliance. The Five (Catholic) States felt encircled and isolated, so they searched for outside allies. After two months of negotiations, the Five States formed "die Christliche Vereinigung" (the Christian Alliance) with Ferdinand of Austria on 22 April 1529.
Soon after the Austrian treaty was signed, a reformed preacher, Jacob Kaiser, was captured in Uznach and executed in Schwyz. This triggered a strong reaction from Zwingli; he drafted "Ratschlag über den Krieg" (Advice About the War) for the government. He outlined justifications for an attack on the Catholic states and other measures to be taken. Before Zürich could implement his plans, a delegation from Bern that included Niklaus Manuel arrived in Zürich. The delegation called on Zürich to settle the matter peacefully. Manuel added that an attack would expose Bern to further dangers as Catholic Valais and the Duchy of Savoy bordered its southern flank. He then noted, "You cannot really bring faith by means of spears and halberds." Zürich, however, decided that it would act alone, knowing that Bern would be obliged to acquiesce. War was declared on 8 June 1529. Zürich was able to raise an army of 30,000 men. The Five States were abandoned by Austria and could raise only 9,000 men. The two forces met near Kappel, but war was averted due to the intervention of Hans Aebli, a relative of Zwingli, who pleaded for an armistice.

Zwingli was obliged to state the terms of the armistice. He demanded the dissolution of the Christian Alliance; unhindered preaching by reformers in the Catholic states; prohibition of the pension system; payment of war reparations; and compensation to the children of Jacob Kaiser. Manuel was involved in the negotiations. Bern was not prepared to insist on the unhindered preaching or the prohibition of the pension system. Zürich and Bern could not agree and the Five (Catholic) States pledged only to dissolve their alliance with Austria. This was a bitter disappointment for Zwingli and it marked his decline in political influence. The first Land Peace of Kappel, "der erste Landfriede", ended the war on 24 June.

While Zwingli carried on the political work of the Swiss Reformation, he developed his theological views with his colleagues. The famous disagreement between Luther and Zwingli on the interpretation of the eucharist originated when Andreas Karlstadt, Luther's former colleague from Wittenberg, published three pamphlets on the Lord's Supper in which Karlstadt rejected the idea of a real presence in the elements. These pamphlets, published in Basel in 1524, received the approval of Oecolampadius and Zwingli. Luther rejected Karlstadt's arguments and considered Zwingli primarily to be a partisan of Karlstadt. Zwingli began to express his thoughts on the eucharist in several publications including "de Eucharistia" (On the Eucharist). Understanding that Christ had ascended to heaven and was sitting at the Father's right hand, Zwingli criticized the idea that Christ's humanity could be in two places at once. Unlike his divinity, Christ's human body was not omnipresent and so could not be in heaven and at the same time be present in the elements. Timothy George, evangelical author, editor of Christianity Today and professor of Historical Theology at Beeson Divinity School at Samford University, has firmly refuted a long-standing misreading of Zwingli that erroneously claimed the Reformer denied all notions of real presence and believed in a memorial view of the Supper, where it was purely symbolic.

By spring 1527, Luther reacted strongly to Zwingli's views in the treatise "Dass Diese Worte Christi "Das ist mein Leib etc." noch fest stehen wider die Schwarmgeister" (That These Words of Christ "This is My Body etc." Still Stand Firm Against the Fanatics). The controversy continued until 1528 when efforts to build bridges between the Lutheran and the Zwinglian views began. Martin Bucer tried to mediate while Philip of Hesse, who wanted to form a political coalition of all Protestant forces, invited the two parties to Marburg to discuss their differences. This event became known as the Marburg Colloquy.

Zwingli accepted Philip's invitation fully believing that he would be able to convince Luther. In contrast, Luther did not expect anything to come out of the meeting and had to be urged by Philip to attend. Zwingli, accompanied by Oecolampadius, arrived on 28 September 1529, with Luther and Philipp Melanchthon arriving shortly thereafter. Other theologians also participated including Martin Bucer, Andreas Osiander, Johannes Brenz, and Justus Jonas. The debates were held from 1–4 October and the results were published in the fifteen "Marburg Articles". The participants were able to agree on fourteen of the articles, but the fifteenth article established the differences in their views on the presence of Christ in the eucharist. Professor George summarized the incompatible views, "On this issue, they parted without having reached an agreement. Both Luther and Zwingli agreed that the bread in the Supper was a sign. For Luther, however, that which the bread signified, namely the body of Christ, was present “in, with, and under” the sign itself. For Zwingli, though, sign and thing signified were separated by a distance—the width between heaven and earth."

The failure to find agreement resulted in strong emotions on both sides. “When the two sides departed, Zwingli cried out in tears, “There are no people on earth with whom I would rather be at one than the [Lutheran] Wittenbergers.”” Because of the differences, Luther initially refused to acknowledge Zwingli and his followers as Christians,

With the failure of the Marburg Colloquy and the split of the Confederation, Zwingli set his goal on an alliance with Philip of Hesse. He kept up a lively correspondence with Philip. Bern refused to participate, but after a long process, Zürich, Basel, and Strasbourg signed a mutual defence treaty with Philip in November 1530. Zwingli also personally negotiated with France's diplomatic representative, but the two sides were too far apart. France wanted to maintain good relations with the Five States. Approaches to Venice and Milan also failed.

As Zwingli was working on establishing these political alliances, Charles V, the Holy Roman Emperor, invited Protestants to the Augsburg Diet to present their views so that he could make a verdict on the issue of faith. The Lutherans presented the Augsburg Confession. Under the leadership of Martin Bucer, the cities of Strasbourg, Constance, Memmingen, and Lindau produced the Tetrapolitan Confession. This document attempted to take a middle position between the Lutherans and Zwinglians. It was too late for the "Burgrecht" cities to produce a confession of their own. Zwingli then produced his own private confession, "Fidei ratio" (Account of Faith) in which he explained his faith in twelve articles conforming to the articles of the Apostles' Creed. The tone was strongly anti-Catholic as well as anti-Lutheran. The Lutherans did not react officially, but criticised it privately. Zwingli's and Luther's old opponent, Johann Eck, counter-attacked with a publication, "Refutation of the Articles Zwingli Submitted to the Emperor".

When Philip of Hesse formed the Schmalkaldic League at the end of 1530, the four cities of the Tetrapolitan Confession joined on the basis of a Lutheran interpretation of that confession. Given the flexibility of the league's entrance requirements, Zürich, Basel, and Bern also considered joining. However, Zwingli could not reconcile the Tetrapolitan Confession with his own beliefs and wrote a harsh refusal to Bucer and Capito. This offended Philip to the point where relations with the League were severed. The "Burgrecht" cities now had no external allies to help deal with internal Confederation religious conflicts.

The peace treaty of the First Kappel War did not define the right of unhindered preaching in the Catholic states. Zwingli interpreted this to mean that preaching should be permitted, but the Five States suppressed any attempts to reform. The "Burgrecht" cities considered different means of applying pressure to the Five States. Basel and Schaffhausen preferred quiet diplomacy while Zürich wanted armed conflict. Zwingli and Jud unequivocally advocated an attack on the Five States. Bern took a middle position which eventually prevailed. In May 1531, Zürich reluctantly agreed to impose a food blockade. It failed to have any effect and in October, Bern decided to withdraw the blockade. Zürich urged its continuation and the "Burgrecht" cities began to quarrel among themselves.

On 9 October 1531, in a surprise move, the Five States declared war on Zürich. Zürich's mobilisation was slow due to internal squabbling and on 11 October, 3500 poorly deployed men encountered a Five States force nearly double their size near Kappel. Many pastors, including Zwingli, were among the soldiers. The battle lasted less than one hour and Zwingli was among the 500 casualties in the Zürich army.

Zwingli had considered himself first and foremost a soldier of Christ; second a defender of his country, the Confederation; and third a leader of his city, Zürich, where he had lived for the previous twelve years. Ironically, he died at the age of 47, not for Christ nor for the Confederation, but for Zürich. 
In Tabletalk, Luther is recorded saying: "They say that Zwingli recently died thus; if his error had prevailed, we would have perished, and our church with us. It was a judgment of God. That was always a proud people. The others, the papists, will probably also be dealt with by our Lord God." Erasmus wrote, "We are freed from great fear by the death of the two preachers, Zwingli and Oecolampadius, whose fate has wrought an incredible change in the mind of many. This is the wonderful hand of God on high." Oecolampadius had died on 24 November. Erasmus also wrote, "If Bellona had favoured them, it would have been all over with us."

According to Zwingli, the cornerstone of theology is the Bible. Zwingli appealed to scripture constantly in his writings. He placed its authority above other sources such as the ecumenical councils or the Church Fathers, although he did not hesitate to use other sources to support his arguments. The principles that guide Zwingli's interpretations are derived from his rationalist humanist education and his Reformed understanding of the Bible. He rejected literalist interpretations of a passage, such as those of the Anabaptists, and used synecdoche and analogies, methods he describes in "A Friendly Exegesis" (1527). Two analogies that he used quite effectively were between baptism and circumcision and between the eucharist and Passover. He also paid attention to the immediate context and attempted to understand the purpose behind it, comparing passages of scripture with each other.
Zwingli rejected the word "sacrament" in the popular usage of his time. For ordinary people, the word meant some kind of holy action of which there is inherent power to free the conscience from sin. For Zwingli, a sacrament was an initiatory ceremony or a pledge, pointing out that the word was derived from "sacramentum" meaning an oath. (However, the word is also translated "mystery".) In his early writings on baptism, he noted that baptism was an example of such a pledge. He challenged Catholics by accusing them of superstition when they ascribed the water of baptism a certain power to wash away sin. Later, in his conflict with the Anabaptists, he defended the practice of infant baptism, noting that there is no law forbidding the practice. He argued that baptism was a sign of a covenant with God, thereby replacing circumcision in the Old Testament.

Zwingli approached the eucharist in a similar manner to baptism. During the first Zürich disputation in 1523, he denied that an actual sacrifice occurred during the mass, arguing that Christ made the sacrifice only once and for all eternity. Hence, the eucharist was "a memorial of the sacrifice". Following this argument, he further developed his view, coming to the conclusion of the "signifies" interpretation for the words of the institution. He used various passages of scripture to argue against transubstantiation as well as Luther's views, the key text being John 6:63, "It is the Spirit who gives life, the flesh is of no avail". Zwingli's approach and interpretation of scripture to understand the meaning of the eucharist was one reason he could not reach a consensus with Luther.

The impact of Luther on Zwingli's theological development has long been a source of interest and discussion among Lutheran scholars, who seek to firmly establish Luther as the first Reformer. Zwingli himself asserted vigorously his independence of Luther and the most recent studies have lent credibility to this claim. Zwingli appears to have read Luther's books in search of confirmation from Luther for his own views. He agreed with the stand Luther took against the pope. Like Luther, Zwingli was also a student and admirer of Augustine.

Zwingli enjoyed music and could play several instruments, including the violin, harp, flute, dulcimer and hunting horn. He would sometimes amuse the children of his congregation on his lute and was so well known for his playing that his enemies mocked him as "the evangelical lute-player and fifer". Three of Zwingli's "Lieder" or hymns have been preserved: the "Pestlied" mentioned above, an adaptation of Psalm 65 (c. 1525), and the "Kappeler Lied", which is believed to have been composed during the campaign of the first war of Kappel (1529). These songs were not meant to be sung during worship services and are not identified as hymns of the Reformation, though they were published in some 16th-century hymnals.

Zwingli criticised the practice of priestly chanting and monastic choirs. The criticism dates from 1523 when he attacked certain worship practices. His arguments are detailed in the Conclusions of 1525, in which, Conclusions 44, 45 and 46 are concerned with musical practices under the rubric of "prayer". He associated music with images and vestments, all of which he felt diverted people's attention from true spiritual worship. It is not known what he thought of the musical practices in early Lutheran churches. Zwingli, however, eliminated instrumental music from worship in the church, stating that God had not commanded it in worship. The organist of the People's Church in Zürich is recorded as weeping upon seeing the great organ broken up. Although Zwingli did not express an opinion on congregational singing, he made no effort to encourage it. Nevertheless, scholars have found that Zwingli was supportive of a role for music in the church. Gottfried W. Locher writes, "The old assertion 'Zwingli was against church singing' holds good no longer ... Zwingli's polemic is concerned exclusively with the medieval Latin choral and priestly chanting and not with the hymns of evangelical congregations or choirs". Locher goes on to say that "Zwingli freely allowed vernacular psalm or choral singing. In addition, he even seems to have striven for lively, antiphonal, unison recitative". Locher then summarizes his comments on Zwingli's view of church music as follows: "The chief thought in his conception of worship was always 'conscious attendance and understanding'—'devotion', yet with the lively participation of all concerned".

Today's Musikabteilung (literally: music departement), located in the choir of the "Predigern" church in Zürich was founded in 1971, and forms a scientific music collection of European importance. It publishes the materials entrusted to it at irregular intervals as CDs. The repertoire ranges from the early 16th-century spiritual music of Huldrych Zwingli to music of the late 20th century, published under the label "Musik aus der Zentralbibliothek Zürich".

Zwingli was a humanist and a scholar with many devoted friends and disciples. He communicated as easily with the ordinary people of his congregation as with rulers such as Philip of Hesse. His reputation as a stern, stolid reformer is counterbalanced by the fact that he had an excellent sense of humour and used satiric fables, spoofing, and puns in his writings. He was more conscious of social obligations than was Luther, and he genuinely believed that the masses would accept a government guided by God's word. He tirelessly promoted assistance to the poor, who he believed should be cared for by a truly Christian community.

In December 1531 the Zürich council selected Heinrich Bullinger (1504-1575) as Zwingli's successor. Bullinger immediately removed any doubts about Zwingli's orthodoxy and defended him as a prophet and a martyr. During Bullinger's ascendancy, the confessional divisions of the Swiss Confederation stabilised. Bullinger rallied the reformed cities and cantons and helped them to recover from the defeat at Kappel. Zwingli had instituted fundamental reforms; Bullinger consolidated and refined them.

Scholars have found it difficult to assess Zwingli's impact on history, for several reasons. There is no consensus on the definition of "Zwinglianism"; by any definition, Zwinglianism evolved under his successor, Heinrich Bullinger; and research into Zwingli's influence on Bullinger and John Calvin remains rudimentary. Bullinger adopted most of Zwingli's points of doctrine. Like Zwingli, he summarised his theology several times, the best-known example being the Second Helvetic Confession of 1566. Meanwhile, Calvin had taken over the Reformation in Geneva. Calvin differed with Zwingli on the eucharist and criticised him for regarding it as simply a metaphorical event. In 1549, however, Bullinger and Calvin succeeded in overcoming the differences in doctrine and produced the "Consensus Tigurinus" (Zürich Consensus). They declared that the eucharist was not just symbolic of the meal, but they also rejected the Lutheran position that the body and blood of Christ is in union with the elements. With this rapprochement, Calvin established his role in the Swiss Reformed Churches and eventually in the wider world.

Outside of Switzerland, no church counts Zwingli as its founder. Scholars speculate as to why Zwinglianism has not diffused more widely, even though Zwingli's theology is considered the first expression of Reformed theology. Although his name is not widely recognised, Zwingli's legacy lives on in the basic confessions of the Reformed churches of today. He is often called, after Martin Luther and John Calvin, the "Third Man of the Reformation".

In 2019 the Swiss director released a Swiss-German film on the career of the reformer: "Zwingli".

Zwingli's collected works are expected to fill 21 volumes. A collection of selected works was published in 1995 by the "Zwingliverein" in collaboration with the "Theologischer Verlag Zürich" This four-volume collection contains the following works:

The complete 21-volume edition is being undertaken by the "Zwingliverein" in collaboration with the "Institut für schweizerische Reformationsgeschichte", and is projected to be organised as follows:

Vols. XIII and XIV have been published, vols. XV and XVI are under preparation. Vols. XVII to XXI are planned to cover the New Testament.

Older German / Latin editions available online include:

See also the following English translations of selected works by Zwingli:






</doc>
<doc id="13603" url="https://en.wikipedia.org/wiki?curid=13603" title="Homeschooling">
Homeschooling

Homeschooling, also known as home education, is the education of children at home or at a variety of places other than school. Home education is usually conducted by a parent, tutor, or an online teacher. Many families use less formal ways of educating. "Homeschooling" is the term commonly used in North America, whereas "home education" is commonly used in the United Kingdom, Europe, and in many Commonwealth countries.

Before the introduction of compulsory school attendance laws, most childhood education was done by families and local communities. In many developed countries, homeschooling is a legal alternative to public and private schools. In other nations, homeschooling remains illegal or restricted to specific conditions, as recorded by homeschooling international status and statistics.

For most of history and in different cultures, the education of children at home by family members was a common practice. Enlisting professional tutors was an option available only to the wealthy. Homeschooling declined in the 19th and 20th centuries with the enactment of compulsory attendance laws. However, it continued to be practised in isolated communities. Homeschooling began a resurgence in the 1960s and 1970s with educational reformists dissatisfied with industrialized education.

The earliest public schools in modern Western culture were established during the reformation with the encouragement of Martin Luther in the German states of Gotha and Thuringia in 1524 and 1527. From the 1500s to 1800s the literacy rate increased until a majority of adults were literate, but development of the literacy rate occurred before the implementation of compulsory attendance and universal education.

Home education and apprenticeship continued to remain the main form of education until the 1830s. However, in the 18th century, the majority of people in Europe lacked formal education. Since the early 19th century, formal classroom schooling became the most common means of schooling throughout the developed countries.

In 1647, New England provided compulsory elementary education. Regional differences in schooling existed in colonial America. In the south, farms and plantations were so widely dispersed that community schools such as those in the more compact settlements of the north were impossible. In the middle colonies, the educational situation varied when comparing New York with New England.

Most Native American tribal cultures traditionally used home education and apprenticeship to pass knowledge to children. Parents were supported by extended relatives and tribal leaders in the education of their children. The Native Americans vigorously resisted compulsory education in the United States.

In the 1960s, Rousas John Rushdoony began to advocate homeschooling, which he saw as a way to combat the secular nature of the public school system in the United States. He vigorously attacked progressive school reformers such as Horace Mann and John Dewey, and argued for the dismantling of the state's influence in education in three works: "Intellectual Schizophrenia", "The Messianic Character of American Education", and "The Philosophy of the Christian Curriculum". Rushdoony was frequently called as an expert witness by the Home School Legal Defense Association (HSLDA) in court cases. He frequently advocated the use of private schools.

During this time, American educational professionals Raymond and Dorothy Moore began to research the academic validity of the rapidly growing Early Childhood Education movement. This research included independent studies by other researchers and a review of over 8,000 studies bearing on early childhood education and the physical and mental development of children.

They asserted that formal schooling before ages 8–12 not only lacked the anticipated effectiveness but also harmed children. The Moores published their view that formal schooling was damaging young children academically, socially, mentally, and even physiologically. The Moores presented evidence that childhood problems such as juvenile delinquency, nearsightedness, increased enrollment of students in special education classes and behavioural problems were the results of increasingly earlier enrollment of students. The Moores cited studies demonstrating that orphans who were given surrogate mothers were measurably more intelligent, with superior long-term effects – even though the mothers were "mentally retarded teenagers" – and that illiterate tribal mothers in Africa produced children who were socially and emotionally more advanced than typical western children, "by western standards of measurement".

Their primary assertion was that the bonds and emotional development made at home with parents during these years produced critical long-term results that were cut short by enrollment in schools, and could neither be replaced nor corrected in an institutional setting afterwards. Recognizing a necessity for early out-of-home care for some children, particularly special needs and impoverished children and children from exceptionally inferior homes, they maintained that the vast majority of children were far better situated at home, even with mediocre parents, than with the most gifted and motivated teachers in a school setting. They described the difference as follows: "This is like saying, if you can help a child by taking him off the cold street and housing him in a warm tent, then warm tents should be provided for "all" children – when obviously most children already have even more secure housing."

The Moores embraced homeschooling after the publication of their first work, "Better Late Than Early", in 1975, and became important homeschool advocates and consultants with the publication of books such as "Home Grown Kids" (1981), and "Homeschool Burnout".

Simultaneously, other authors published books questioning the premises and efficacy of compulsory schooling, including "Deschooling Society" by Ivan Illich in 1970 and "No More Public School" by Harold Bennet in 1972.

In 1976, educator John Holt published "Instead of Education; Ways to Help People Do Things Better". In its conclusion, he called for a "Children's Underground Railroad" to help children escape compulsory schooling. In response, Holt was contacted by families from around the U.S. to tell him that they were educating their children at home. In 1977, after corresponding with a number of these families, Holt began producing "Growing Without Schooling", a newsletter dedicated to home education. Holt was nicknamed the "father of homeschooling." Holt later wrote a book about homeschooling, "Teach Your Own", in 1981.

In 1980, Holt said, "I want to make it clear that I don't see homeschooling as some kind of answer to badness of schools. I think that the home is the proper base for the exploration of the world which we call learning or education. The home would be the best base no matter how good the schools were." One common theme in the homeschool philosophies of both Holt and that of the Moores is that home education should not attempt to bring the school to construct into the home, or a view of education as an academic preliminary to life. They viewed home education as a natural, experiential aspect of life that occurs as the members of the family are involved with one another in daily living.

Homeschooling can be used as a form of supplemental education and as a way of helping children learn under specific circumstances. The term may also refer to instruction in the home under the supervision of correspondence schools or umbrella schools. Some jurisdictions require adherence to an approved curriculum. A curriculum-free philosophy of homeschooling is sometimes called "unschooling", a term coined in 1977 by American educator and author John Holt in his magazine, "Growing Without Schooling". The term emphasizes the more spontaneous, less structured learning environment in which a child's interests drive his pursuit of knowledge. Some parents provide a liberal arts education using the trivium and quadrivium as the main models.

Parents commonly cite two main motivations for homeschooling their children: dissatisfaction with the local schools and the interest in increased involvement with their children's learning and development. Parental dissatisfaction with available schools typically includes concerns about the school environment, the quality of academic instruction, the curriculum, bullying, racism and lack of faith in the school's ability to cater to their children's special needs. Some parents homeschool in order to have greater control over what and how their children are taught, to cater more adequately to an individual child's aptitudes and abilities, to provide instruction from a specific religious or moral position, and to take advantage of the efficiency of one-to-one instruction and thus allow the child to spend more time on childhood activities, socializing, and non-academic learning.

Some African-American families choose to homeschool as a way of increasing their children's understanding of African-American history – such as the Jim Crow laws that resulted in their ancestors being beaten or killed for learning to read – and to limit the harm caused by the unintentional and sometimes subtle systemic racism that affects most American schools.

Some parents have objections to the secular nature of public schools and homeschool in order to give their children a religious education. Use of a religious curriculum is common among these families. Recent sociological work suggests that an increasing number of parents are choosing homeschooling because of low academic quality at the local schools, or because of bullying or health problems.

Some parents are of the opinion that certain temperaments are promoted in school, while others are inhibited which may also be a reason to homeschool their children.

Another argument for homeschooling his children may be the protection against physical and emotional violence, bullying, drugs, stress, sexualization, excessive performance thoughts, socialization groups or role models with negative impact and degrading treatment in school.

Homeschooling may also be a factor in the choice of parenting style. Homeschooling can be a matter of consistency for families living in isolated rural locations, for those temporarily abroad, and for those who travel frequently. Many young athletes, actors, and musicians are taught at home to accommodate their training and practice schedules more conveniently. Homeschooling can be about mentorship and apprenticeship, in which a tutor or teacher is with the child for many years and becomes more intimately acquainted with the child.

According to Elizabeth Bartholet, surveys of homeschoolers show that a majority of homeschoolers in the USA are motivated by "conservative Christian beliefs, and seek to remove their children from mainstream culture".

Homeschools use a wide variety of methods and materials. Families choose different educational methods, which represent a variety of educational philosophies and paradigms. Some of the methods or learning environments used include Classical education (including Trivium, Quadrivium), Charlotte Mason education, Montessori method, Theory of multiple intelligences, Unschooling, Radical Unschooling, Waldorf education, School-at-home (curriculum choices from both secular and religious publishers), A Thomas Jefferson Education, unit studies, curriculum made up from private or small publishers, apprenticeship, hands-on-learning, distance learning (both online and correspondence), dual enrollment in local schools or colleges, and curriculum provided by local schools and many others. Some of these approaches are used in private and public schools. Educational research and studies support the use of some of these methods. Unschooling, natural learning, Charlotte Mason Education, Montessori, Waldorf, apprenticeship, hands-on-learning, unit studies are supported to varying degrees by research by constructivist learning theories and situated cognition theories. Elements of these theories may be found in the other methods as well.

A student's education may be customized to support his or her learning level, style, and interests. It is not uncommon for a student to experience more than one approach as the family discovers what works best for their student. Many families use an eclectic approach, picking and choosing from various suppliers. For sources of curricula and books, a study found that 78 per cent utilized "a public library"; 77 per cent used "a homeschooling catalogue, publisher, or individual specialist"; 68 per cent used "retail bookstore or another store"; 60 per cent used "an education publisher that was not affiliated with homeschooling." "Approximately half" used curriculum from "a homeschooling organization", 37 per cent from a "church, synagogue or other religious institution" and 23 per cent from "their local public school or district." In 2003, 41 per cent utilized some sort of distance learning, approximately 20 per cent by "television, video or radio"; 19 per cent via "The Internet, e-mail, or the World Wide Web"; and 15 per cent taking a "correspondence course by mail designed specifically for homeschoolers."

Individual governmental units, e.g. states and local districts, vary in official curriculum and attendance requirements.

As a subset of homeschooling, informal learning happens outside of the classroom but has no traditional boundaries of education. Informal learning is an everyday form of learning through participation and creation, in contrast with the traditional view of teacher-centred learning. The term is often combined with non-formal learning and self-directed learning. Informal learning differs from traditional learning since there are no expected objectives or outcomes. From the learner's standpoint, the knowledge that they receive is not intentional. Anything from planting a garden to baking a cake or even talking to a technician at work about the installation of new software can be considered informal learning. The individual is completing a task with different intentions but ends up learning skills in the process. Children watching their tomato plants grow will not generate questions about photosynthesis but they will learn that their plants are growing with water and sunlight. This leads them to have a base understanding of complex scientific concepts without any background studying. The recent trend of homeschooling becoming less stigmatized has been in connection with the traditional waning of the idea that the state needs to be in primary and ultimate control over the education and upbringing of all children to create future adult citizens. This breeds an ever-growing importance on the ideas and concepts that children learn outside of the traditional classroom setting, including Informal learning.

Depending on the part of the world, informal learning can take on many different identities and has differing cultural importances. Many ways of organizing homeschooling draw on apprenticeship qualities and on non-western cultures. In some South American indigenous cultures, such as the Chillihuani community in Peru, children learn irrigation and farming technique through play, advancing them not only in their own village and society but also in their knowledge of realistic techniques that they will need to survive. In Western culture, children use informal learning in two main ways. The first as talked about is through hands-on experience with new material. The second is asking questions to someone who has more experience than they have (i.e. parents, elders). Children's inquisitive nature is their way of cementing the ideas they have learned through exposure to informal learning. It is a more casual way of learning than traditional learning and serves the purpose of taking in information any which way they can.

All other approaches to homeschooling are subsumed under two basic categories: structured and unstructured homeschooling. Structured homeschooling includes any method or style of home education that follows a basic curriculum with articulated goals and outcomes. This style attempts to imitate the structure of the traditional school setting while personalizing the curriculum. Unstructured homeschooling is any form of home education where parents do not construct a curriculum at all. Unschooling, as it is known, attempts to teach through the child's daily experiences and focuses more on self-directed learning by the child, free of textbooks, teachers, and any formal assessment of success or failure.

In a unit study approach, multiple subjects such as math, science, history, art, and geography, are studied in relation to a single topic. Unit studies are useful for teaching multiple grades simultaneously as the difficulty level can be adjusted for each student. An extended form of unit studies, Integrated Thematic Instruction utilizes one central theme integrated throughout the curriculum so that students finish a school year with a deep understanding of a certain broad subject or idea.

All-in-one homeschooling curricula (variously known as "school-at-home", "the traditional approach", "school-in-a-box" or "The Structured Approach"), are instructional methods of teaching in which the curriculum and homework of the student are similar or identical to those used in a public or private school. Purchased as a grade-level package or separately by subject, the package may contain all of the needed books, materials, tests, answer keys, and extensive teacher guides. These materials cover the same subject areas as public schools, allowing for an easy transition into the school system. These are among the most expensive options for homeschooling, but they require minimal preparation and are easy to use. There is, however, complete curriculum available for free, such as that available at allinonehomeschool.com. Some localities provide the same materials used at local schools to homeschoolers. The purchase of a complete curriculum and their teaching/grading service from an accredited distance learning curriculum provider may allow students to obtain an accredited high school diploma.

"Natural learning" refers to a type of learning-on-demand where children pursue knowledge based on their interests and parents take an active part in facilitating activities and experiences conducive to learning but do not rely heavily on textbooks or spend much time "teaching", looking instead for "learning moments" throughout their daily activities. Parents see their role as that of affirming through positive feedback and modeling the necessary skills, and the child's role as being responsible for asking and learning.

The term "unschooling" as coined by John Holt describes an approach in which parents do not authoritatively direct the child's education, but interact with the child following the child's own interests, leaving them free to explore and learn as their interests lead. "Unschooling" does not indicate that the child is not being educated, but that the child is not being "schooled", or educated in a rigid school-type manner. Holt asserted that children learn through the experiences of life, and he encouraged parents to live their lives with their child. Also known as interest-led or child-led learning, unschooling attempts to follow opportunities as they arise in real life, through which a child will learn without coercion. Children at school learn from 1 teacher and 2 auxiliary teachers in a classroom of approximately 30. Kids have the opportunity of dedicated education at home with a ratio of 1 to 1. An unschooled child may utilize texts or classroom instruction, but these are not considered central to education. Holt asserted that there is no specific body of knowledge that is, or should be, required of a child.

Both unschooling and natural learning advocates believe that children learn best by doing; a child may learn reading to further an interest about history or other cultures, or math skills by operating a small business or sharing in family finances. They may learn animal husbandry keeping dairy goats or meat rabbits, botany tending a kitchen garden, chemistry to understand the operation of firearms or the internal combustion engine, or politics and local history by following a zoning or historical-status dispute. While any type of homeschoolers may also use these methods, the unschooled child initiates these learning activities. The natural learner participates with parents and others in learning together.

Another prominent proponent of unschooling is John Taylor Gatto, author of Dumbing Us Down, The Exhausted School, A Different Kind of Teacher, and Weapons of Mass Instruction. Gatto argues that public education is the primary tool of "state-controlled consciousness" and serves as a prime illustration of the total institution — a social system which impels obedience to the state and quells free-thinking or dissent.

Autonomous learning is a school of education which sees learners as individuals who can and should be i.e. be responsible for their own learning climate.

Autonomous education helps students develop their self-consciousness, vision, practicality, and freedom of discussion. These attributes serve to aid the student in his/her independent learning. However, a student must not start their autonomous learning completely on their own. It is said, that by first having interaction with someone who has more knowledge in a subject, will speed up the student's learning, and hence allow them to learn more independently.

Some degree of autonomous learning is popular with those who home educate their children. In true autonomous learning, the child usually gets to decide what projects they wish to tackle or what interests to pursue. In-home education, this can be instead of or in addition to regular subjects like doing math or English.

According to Home Education UK, the autonomous education philosophy emerged from the epistemology of Karl Popper in "The Myth of the Framework: In Defence of Science and Rationality", which is developed in the debates, which seek to rebut the neo-Marxist social philosophy of convergence proposed by the Frankfurt School (e.g. Theodor W. Adorno, Jürgen Habermas, Max Horkheimer).

A homeschool cooperative is a cooperative of families who homeschool their children. It provides an opportunity for children to learn from other parents who are more specialized in certain areas or subjects. Co-ops also provide social interaction. They may take lessons together or go on field trips. Some co-ops also offer events such as prom and graduation for homeschoolers.

Homeschoolers are beginning to utilize Web 2.0 as a way to simulate homeschool cooperatives online. With social networks, homeschoolers can chat, discuss threads in forums, share information and tips, and even participate in online classes via blackboard systems similar to those used by colleges.

According to the Home School Legal Defense Association (HSLDA) in 2004, "Many studies over the last few years have established the academic excellence of homeschooled children." "Home Schooling Achievement", a compilation of studies published by the HSLDA, supported the academic integrity of homeschooling. This booklet summarized a 1997 study by Ray and the 1999 Rudner study. The Rudner study noted two limitations of its own research: it is not necessarily representative of all homeschoolers and it is not a comparison with other schooling methods. Among the homeschooled students who took the tests, the average homeschooled student outperformed his public school peers by 30 to 37 percentile points across all subjects. The study also indicates that public school performance gaps between minorities and genders were virtually non-existent among the homeschooled students who took the tests.

A survey of 11,739 homeschooled students conducted in 2008 found that, on average, the homeschooled students scored 37 percentile points above public school students on standardized achievement tests. This is consistent with the 1999 Rudner study. However, Rudner said that these same students in public school may have scored just as well because of the dedicated parents they had. The Ray study also found that homeschooled students who had a certified teacher as a parent scored one percentile lower than homeschooled students who did not have a certified teacher as a parent. Another nationwide descriptive study conducted by Ray contained students ranging from ages 5–18 and he found that homeschoolers scored in at least the 80th percentile on their tests.

In 2011, a quasi-experimental study was conducted that included homeschooled and traditional public students between the ages of 5 and 10. It was discovered that the majority of the homeschooled children achieved higher standardized scores compared to their counterparts. However, Martin-Chang also found that unschooling children ages 5–10 scored significantly below traditionally educated children, while academically-oriented homeschooled children scored from one half grade level above to 4.5 grade levels above traditionally schooled children on standardized tests (n=37 homeschooled children matched with children from the same socioeconomic and educational background).

Studies have also examined the impact of homeschooling on students' GPAs. Cogan (2010) found that homeschooled students had higher high school GPAs (3.74) and transfer GPAs (3.65) than conventional students. Snyder (2013) provided corroborating evidence that homeschoolers were outperforming their peers in the areas of standardized tests and overall GPAs. Looking beyond high school, a study by the 1990 National Home Education Research Institute (as cited by Wichers, 2001) found that at least 33% of homeschooled students attended a four-year college, and 17% attended a two-year college. This same study examined the students after one year, finding that 17% pursued higher education. Thus, the data indicate that homeschooling can also prepare students for success in higher education.

On average, studies suggest homeschoolers score at or above the national average on standardized tests. Homeschool students have been accepted into many Ivy League universities. However, The Coalition for Responsible Homeschooling notes that "Our knowledge of homeschooling’s effect on academic achievement is limited by the fact that many of the studies that have been conducted on homeschoolers suffer from methodological problems which make their findings inconclusive."

Homeschooled children may receive more individualized attention than students enrolled in traditional public schools. A 2011 study suggests that a structured environment could play a key role in homeschooler academic achievement. This means that parents were highly involved in their child's education and they were creating clear educational goals. In addition, these students were being offered organized lesson plans which are either self-made or purchased.

A study conducted by Ray (2010), indicates that the higher the level of parents' income, the more likely the homeschooled child is able to achieve academic success.

In the 1970s, Raymond and Dorothy Moore conducted four federally funded analyses of more than 8,000 early childhood studies, from which they published their original findings in "Better Late Than Early", 1975. This was followed by "School Can Wait", a repackaging of these same findings designed specifically for educational professionals. They concluded that "where possible, children should be withheld from formal schooling until at least ages eight to ten." Their reason was that children "are not mature enough for formal school programs until their senses, coordination, neurological development and cognition are ready". They concluded that the outcome of forcing children into formal schooling is a sequence of "1) uncertainty as the child leaves the family nest early for a less secure environment, 2) puzzlement at the new pressures and restrictions of the classroom, 3) frustration because unready learning tools – senses, cognition, brain hemispheres, coordination – cannot handle the regimentation of formal lessons and the pressures they bring, 4) hyperactivity growing out of nerves and jitter, from frustration, 5) failure which quite naturally flows from the four experiences above, and 6) delinquency which is failure's twin and apparently for the same reason." According to the Moores, "early formal schooling is burning out our children. Teachers who attempt to cope with these youngsters also are burning out." Aside from academic performance, they think early formal schooling also destroys "positive sociability", encourages peer dependence, and discourages self-worth, optimism, respect for parents, and trust in peers. They believe this situation is particularly acute for boys because of their delay in maturity. The Moores cited a Smithsonian Report on the development of genius, indicating a requirement for "1) much time spent with warm, responsive parents and other adults, 2) very little time spent with peers, and 3) a great deal of free exploration under parental guidance." Their analysis suggested that children need "more of home and less of formal school", "more free exploration with... parents, and fewer limits of classroom and books", and "more old fashioned chores – children working with parents – and less attention to rivalry sports and amusements."

Along with positive school outcomes, homeschooled youth are also less likely to use and abuse illicit substances and are more likely to disapprove of using alcohol and marijuana.

There are claims that studies showing that homeschooled students do better on standardized tests do not compare with mandatory public-school testing.

By contrast, SAT and ACT tests are self-selected by homeschooled and formally schooled students alike. Some homeschoolers averaged higher scores on these college entrance tests in South Carolina. Other scores (1999 data) showed mixed results, for example showing higher levels for homeschoolers in English (homeschooled 23.4 vs national average 20.5) and reading (homeschooled 24.4 vs national average 21.4) on the ACT, but mixed scores in math (homeschooled 20.4 vs national average 20.7 on the ACT as opposed homeschooled 535 vs national average 511 on the 1999 SAT math).

Some advocates of homeschooling and educational choice counter with an input-output theory, pointing out that home educators expend only an average of $500–$600 a year on each student (not counting the cost of the parents' time), in comparison to $9,000–$10,000 (including the cost of staff time) for each public school student in the United States, which suggests home-educated students would be especially dominant on tests if afforded access to an equal commitment of tax-funded educational resources.

Many teachers and school districts oppose the idea of homeschooling. However, research has shown that homeschooled children often excel in many areas of academic endeavour. According to a study done on the homeschool movement, homeschoolers often achieve academic success and admission into elite universities. There is also evidence that most are remarkably well socialized. According to the National Home Education Research Institute president, Brian Ray, socialization is not a problem for homeschooling children, many of whom are involved in community sports, volunteer activities, book groups, or homeschool co-ops.

Using the Piers-Harris Children's Self-Concept Scale, John Taylor later found that, "while half of the conventionally schooled children scored at or below the 50th percentile (in self-concept), only 10.3% of the home-schooling children did so." He further stated that "the self-concept of home-schooling children is significantly higher statistically than that of children attending conventional school. This has implications in the areas of academic achievement and socialization which have been found to parallel self-concept. Regarding socialization, Taylor's results would mean that very few home-schooling children are socially deprived. He states that critics who speak out against homeschooling on the basis of social deprivation are actually addressing an area which favours homeschoolers.

In 2003, the National Home Education Research Institute conducted a survey of 7,300 U.S. adults who had been homeschooled (5,000 for more than seven years). Their findings included:

Richard G. Medlin, Ph.D.'s research found that homeschooled children have better social skills than children attending traditional schools. 

Opposition to homeschooling comes from some organizations of teachers and school districts. The National Education Association, a United States teachers' union and professional association, opposes homeschooling.

UC Berkeley political scientist Professor Robert Reich wrote in "The Civic Perils of Homeschooling" (2002) that homeschooling can probably result in biased students, as many homeschooling parents view the education of their children as a matter properly under their control and no one else's. A 2014 study showed that greater exposure to homeschooling was associated with more political tolerance.

Gallup polls of American voters have shown a significant change in attitude in the last 20 years, from 73% opposed to home education in 1985 to 54% opposed in 2001. In 1988, when asked whether parents should have a right to choose homeschooling, 53 percent thought that they should, as revealed by another poll.

Homeschooling is legal in many countries. Countries with the most prevalent home education movements include Australia, Canada, New Zealand, the United Kingdom, Mexico, Chile and the United States. Some countries have highly regulated home education programs as an extension of the compulsory school system; few others, such as Germany, have outlawed it entirely. In other countries, while not restricted by law, homeschooling is not socially acceptable or considered desirable and is virtually non-existent.



</doc>
<doc id="13605" url="https://en.wikipedia.org/wiki?curid=13605" title="Heteroatom">
Heteroatom

In chemistry, a heteroatom (from Ancient Greek "heteros", "different", + "atomos", "uncut") is, strictly, any atom that is not carbon or hydrogen. 

In practice, the term is usually used more specifically to indicate that non-carbon atoms have replaced carbon in the backbone of the molecular structure. Typical heteroatoms are nitrogen (N), oxygen (O), sulfur (S), phosphorus (P), chlorine (Cl), bromine (Br), and iodine (I), as well as the metals lithium (Li) and magnesium (Mg).

It can also be used with highly specific meanings in specialised contexts. In the description of protein structure, in particular in the Protein Data Bank file format, a heteroatom record (HETATM) describes an atom as belonging to a small molecule cofactor rather than being part of a biopolymer chain.

In the context of zeolites, the term "heteroatom" refers to partial isomorphous substitution of the typical framework atoms (silicon, aluminium, and phosphorus) by other elements such as beryllium, vanadium, and chromium. The goal is usually to adjust properties of the material (e.g., Lewis acidity) to optimize the material for a certain application (e.g., catalysis).



</doc>
<doc id="13606" url="https://en.wikipedia.org/wiki?curid=13606" title="Half-life">
Half-life

Half-life (symbol "t") is the time required for a quantity to reduce to half of its initial value. The term is commonly used in nuclear physics to describe how quickly unstable atoms undergo, or how long stable atoms survive, radioactive decay. The term is also used more generally to characterize any type of exponential or non-exponential decay. For example, the medical sciences refer to the biological half-life of drugs and other chemicals in the human body. The converse of half-life is doubling time.

The original term, "half-life period", dating to Ernest Rutherford's discovery of the principle in 1907, was shortened to "half-life" in the early 1950s. Rutherford applied the principle of a radioactive element's half-life to studies of age determination of rocks by measuring the decay period of radium to lead-206.

Half-life is constant over the lifetime of an exponentially decaying quantity, and it is a characteristic unit for the exponential decay equation. The accompanying table shows the reduction of a quantity as a function of the number of half-lives elapsed.

A half-life usually describes the decay of discrete entities, such as radioactive atoms. In that case, it does not work to use the definition that states "half-life is the time required for exactly half of the entities to decay". For example, if there is just one radioactive atom, and its half-life is one second, there will "not" be "half of an atom" left after one second.

Instead, the half-life is defined in terms of probability: "Half-life is the time required for exactly half of the entities to decay "on average"". In other words, the "probability" of a radioactive atom decaying within its half-life is 50%.

For example, the image on the right is a simulation of many identical atoms undergoing radioactive decay. Note that after one half-life there are not "exactly" one-half of the atoms remaining, only "approximately", because of the random variation in the process. Nevertheless, when there are many identical atoms decaying (right boxes), the law of large numbers suggests that it is a "very good approximation" to say that half of the atoms remain after one half-life.

Various simple exercises can demonstrate probabilistic decay, for example involving flipping coins or running a statistical computer program.

An exponential decay can be described by any of the following three equivalent formulas:
where

The three parameters , , and are all directly related in the following way:

where ln(2) is the natural logarithm of 2 (approximately 0.693).

Some quantities decay by two exponential-decay processes simultaneously. In this case, the actual half-life can be related to the half-lives "t" and "t" that the quantity would have if each of the decay processes acted in isolation:

For three or more processes, the analogous formula is:
For a proof of these formulas, see Exponential decay § Decay by two or more processes.

There is a half-life describing any exponential-decay process. For example:

The term "half-life" is almost exclusively used for decay processes that are exponential (such as radioactive decay or the other examples above), or approximately exponential (such as biological half-life discussed below). In a decay process that is not even close to exponential, the half-life will change dramatically while the decay is happening. In this situation it is generally uncommon to talk about half-life in the first place, but sometimes people will describe the decay in terms of its "first half-life", "second half-life", etc., where the first half-life is defined as the time required for decay from the initial value to 50%, the second half-life is from 50% to 25%, and so on.

A biological half-life or elimination half-life is the time it takes for a substance (drug, radioactive nuclide, or other) to lose one-half of its pharmacologic, physiologic, or radiological activity. In a medical context, the half-life may also describe the time that it takes for the concentration of a substance in blood plasma to reach one-half of its steady-state value (the "plasma half-life").

The relationship between the biological and plasma half-lives of a substance can be complex, due to factors including accumulation in tissues, active metabolites, and receptor interactions.

While a radioactive isotope decays almost perfectly according to so-called "first order kinetics" where the rate constant is a fixed number, the elimination of a substance from a living organism usually follows more complex chemical kinetics.

For example, the biological half-life of water in a human being is about 9 to 10 days, though this can be altered by behavior and other conditions. The biological half-life of caesium in human beings is between one and four months.

The concept of a half-life has also been utilized for pesticides in plants, and certain authors maintain that pesticide risk and impact assessment models rely on and are sensitive to information describing dissipation from plants.

In epidemiology, the concept of half-life can refer to the length of time for the number of incident cases in a disease outbreak to drop by half, particularly if the dynamics of the outbreak can be modeled exponentially.




</doc>
<doc id="13607" url="https://en.wikipedia.org/wiki?curid=13607" title="Humus">
Humus

In soil science, humus (derived in 1790–1800 from the Latin for 'earth, ground') denominates the fraction of soil organic matter that is amorphous and without the "cellular cake structure characteristic of plants, micro-organisms or animals". Humus significantly affects the bulk density of soil and contributes to its retention of moisture and nutrients. Although the terms "humus" and "compost" are informally used interchangeably, they are distinct soil components with different origins; humus is created through anaerobic fermentation, while compost is the result of aerobic decomposition.

In agriculture, "humus" sometimes also is used to describe mature or natural compost extracted from a woodland or other spontaneous source for use as a soil conditioner. It is also used to describe a topsoil horizon that contains organic matter (humus type, humus form, humus profile).

More precisely, humus is the dark organic matter that forms in soil when dead plant and animal matter (including aerobic compost) breaks down further, specifically through the action of anaerobic organisms. Humus has many nutrients that improve the health of soil, nitrogen being the most important. The ratio of carbon to nitrogen (C:N) of humus is 10:1.

It is difficult to define humus precisely because it is a very complex substance which is not fully understood. Humus is different from decomposing soil organic matter. The latter looks rough and has visible remains of the original plant or animal matter. Fully humified humus, on the contrary, has a uniformly dark, spongy, and jelly-like appearance, and is amorphous; it may gradually decay over several years or persist for millennia. It has no determinate shape, structure, or quality. However, when examined under a microscope, humus may reveal tiny plant, animal, or microbial remains that have been mechanically, but not chemically, degraded. This suggests an ambiguous boundary between humus and soil organic matter. While distinct, humus is an integral part of soil organic matter.

Microorganisms decompose a large portion of the soil organic matter into inorganic minerals that the roots of plants can absorb as nutrients. This process is termed "mineralization". In this process, nitrogen (nitrogen cycle) and the other nutrients (nutrient cycle) in the decomposed organic matter are recycled. Depending on the conditions in which the decomposition occurs, a fraction of the organic matter does not mineralize, and instead is transformed by a process called "humification" into concatenations of organic polymers. Because these organic polymers are resistant to the action of microorganisms, they are stable, and constitute "humus". This stability implies that humus integrates into the permanent structure of the soil, thereby improving it.

Humification can occur naturally in soil or artificially in the production of compost. Organic matter is humified by a combination of saprotrophic fungi, bacteria, microbes and animals such as earthworms, nematodes, protozoa, and arthropods. Plant remains, including those that animals digested and excreted, contain organic compounds: sugars, starches, proteins, carbohydrates, lignins, waxes, resins, and organic acids. Decay in the soil begins with the decomposition of sugars and starches from carbohydrates, which decompose easily as detritivores initially invade the dead plant organs, while the remaining cellulose and lignin decompose more slowly. Simple proteins, organic acids, starches, and sugars decompose rapidly, while crude proteins, fats, waxes, and resins remain relatively unchanged for longer periods of time. Lignin, which is quickly transformed by white-rot fungi, is one of the primary precursors of humus, together with by-products of microbial and animal activity. The humus produced by humification is thus a mixture of compounds and complex biological chemicals of plant, animal, or microbial origin that has many functions and benefits in soil. Some judge earthworm humus (vermicompost) to be the optimal organic manure.

Much of the humus in most soils has persisted for more than 100 years, rather than having been decomposed into CO, and can be regarded as stable; this organic matter has been protected from decomposition by microbial or enzyme action because it is hidden (occluded) inside small aggregates of soil particles, or tightly sorbed or complexed to clays. Most humus that is not protected in this way is decomposed within 10 years and can be regarded as less stable or more labile. Stable humus contributes few plant-available nutrients in soil, but it helps maintain its physical structure. A very stable form of humus is formed from the slow oxidation of soil carbon after the incorporation of finely powdered charcoal into the topsoil. This process is speculated to have been important in the formation of the unusually fertile Amazonian .

Humus has a characteristic black or dark brown color and is organic due to an accumulation of organic carbon. Soil scientists use the capital letters O, A, B, C, and E to identify the master horizons, and lowercase letters for distinctions of these horizons. Most soils have three major horizons: the surface horizon (A), the subsoil (B), and the substratum (C). Some soils have an organic horizon (O) on the surface, but this horizon can also be buried. The master horizon (E) is used for subsurface horizons that have significantly lost minerals (eluviation). Bedrock, which is not soil, uses the letter R.

The importance of chemically stable humus is thought by some to be the fertility it provides to soils in both a physical and chemical sense, though some agricultural experts put a greater focus on other features of it, such as its ability to suppress disease. It helps the soil retain moisture by increasing microporosity, and encourages the formation of good soil structure. The incorporation of oxygen into large organic molecular assemblages generates many active, negatively charged sites that bind to positively charged ions (cations) of plant nutrients, making them more available to the plant by way of ion exchange. Humus allows soil organisms to feed and reproduce, and is often described as the "life-force" of the soil.




</doc>
<doc id="13609" url="https://en.wikipedia.org/wiki?curid=13609" title="Hydrogen bond">
Hydrogen bond

A hydrogen bond (often informally abbreviated H-bond) is a partial intermolecular bonding interaction between a lone pair on an electron rich donor atom, particularly the second-row elements nitrogen (N), oxygen (O), or fluorine (F), and the antibonding molecular orbital of a bond between hydrogen (H) and a more electronegative atom or group. Such an interacting system is generally denoted Dn–H···Ac, where the solid line denotes a polar covalent bond, and the dotted or dashed line indicates the hydrogen bond. The use of three centered dots for the hydrogen bond is specifically recommended by the IUPAC. While hydrogen bonding has both covalent and electrostatic contributions, and the degrees to which they contribute are currently debated, the present evidence strongly implies that the primary contribution is covalent.

Hydrogen bonds can be intermolecular (occurring between separate molecules) or intramolecular (occurring among parts of the same molecule). Depending on the nature of the donor and acceptor atoms which constitute the bond, their geometry, and environment, the energy of a hydrogen bond can vary between 1 and 40 kcal/mol. This makes them somewhat stronger than a van der Waals interaction, and weaker than fully covalent or ionic bonds. This type of bond can occur in inorganic molecules such as water and in organic molecules like DNA and proteins.

The hydrogen bond is responsible for many of the anomalous physical and chemical properties of compounds of N, O, and F. In particular, intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides that have much weaker hydrogen bonds. Intramolecular hydrogen bonding is partly responsible for the secondary and tertiary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.

Weaker hydrogen bonds are known for hydrogen atoms bound to elements such as sulfur (S) or chlorine (Cl); even carbon (C) can serve as a donor, particularly when the carbon or one of its neighbors is electronegative (e.g., in chloroform, aldehydes and terminal acetylenes). Gradually, it was recognized that there are many examples of weaker hydrogen bonding involving donor other than N, O, or F and/or acceptor Ac with electronegativity approaching that of hydrogen (rather than being much more electronegative). Though these "non-traditional" hydrogen bonding interactions are often quite weak (~1 kcal/mol), they are also ubiquitous and are increasingly recognized as important control elements in receptor-ligand interactions in medicinal chemistry or intra-/intermolecular interactions in materials sciences. The definition of hydrogen bonding has gradually broadened over time to include these weaker attractive interactions. In 2011, an IUPAC Task Group recommended a modern evidence-based definition of hydrogen bonding, which was published in the IUPAC journal "Pure and Applied Chemistry". This definition specifies:
As part of a more detailed list of criteria, the IUPAC publication acknowledges that the attractive interaction can arise from some combination of electrostatics (multipole-multipole and multipole-induced multipole interactions), covalency (charge transfer by orbital overlap), and dispersion (London forces), and states that the relative importance of each will vary depending on the system. However, a footnote to the criterion recommends the exclusion of interactions in which dispersion is the primary contributor, specifically giving Ar---CH and CH---CH as examples of such interactions to be excluded from the definition.

Nevertheless, most introductory textbooks still restrict the definition of hydrogen bond to the "classical" type of hydrogen bond characterized in the opening paragraph.

A hydrogen atom attached to a relatively electronegative atom is the hydrogen bond "donor". C-H bonds only participate in hydrogen bonding when the carbon atom is bound to electronegative substituents, as is the case in chloroform, CHCl. In a hydrogen bond, the electronegative atom not covalently attached to the hydrogen is named proton acceptor, whereas the one covalently bound to the hydrogen is named the proton donor. While this nomenclature is recommended by the IUPAC, it can be misleading, since in other donor-acceptor bonds, the donor/acceptor assignment is based on the source of the electron pair (such nomenclature is also used for hydrogen bonds by some authors). In the hydrogen bond donor, the H center is protic. The donor is a Lewis acid. Hydrogen bonds are represented as H···Y system, where the dots represent the hydrogen bond. Liquids that display hydrogen bonding (such as water) are called associated liquids.

The hydrogen bond is often described as an electrostatic dipole-dipole interaction. However, it also has some features of covalent bonding: it is directional and strong, produces interatomic distances shorter than the sum of the van der Waals radii, and usually involves a limited number of interaction partners, which can be interpreted as a type of valence. These covalent features are more substantial when acceptors bind hydrogens from more electronegative donors.

Hydrogen bonds can vary in strength from weak (1–2 kJ mol) to strong (161.5 kJ mol in the ion ). Typical enthalpies in vapor include:
The strength of intermolecular hydrogen bonds is most often evaluated by measurements of equilibria between molecules containing donor and/or acceptor units, most often in solution. The strength of intramolecular hydrogen bonds can be studied with equilibria between conformers with and without hydrogen bonds. The most important method for the identification of hydrogen bonds also in complicated molecules is crystallography, sometimes also NMR-spectroscopy. Structural details, in particular distances between donor and acceptor which are smaller than the sum of the van der Waals radii can be taken as indication of the hydrogen bond strength.

One scheme gives the following somewhat arbitrary classification: those that are 15 to 40 kcal/mol, 5 to 15 kcal/mol, and >0 to 5 kcal/mol are considered strong, moderate, and weak, respectively.

The X−H distance is typically ≈110 pm, whereas the H···Y distance is ≈160 to 200 pm. The typical length of a hydrogen bond in water is 197 pm. The ideal bond angle depends on the nature of the hydrogen bond donor. The following hydrogen bond angles between a hydrofluoric acid donor and various acceptors have been determined experimentally:
Strong hydrogen bonds are revealed by downfield shifts in the H NMR spectrum. For example, the acidic proton in the enol tautomer of acetylacetone appears at δ 15.5, which is about 10 ppm downfield of a conventional alcohol.

In the IR spectrum, hydrogen bonding shifts the X-H stretching frequency to lower energy (i.e. the vibration frequency decreases). This shift reflects a weakening of the X-H bond. Certain hydrogen bonds - improper hydrogen bonds - show a blue shift of the X-H stretching frequency and a decrease in the bond length. H-bonds can also be measured by IR vibrational mode shifts of the acceptor. The amide I mode of backbone carbonyls in α-helices shifts to lower frequencies when they form H-bonds with side-chain hydroxyl groups.

Hydrogen bonding is of continuing theoretical interest. According to a modern description O:H-O integrates both the intermolecular O:H lone pair ":" nonbond and the intramolecular H-O polar-covalent bond associated with O-O repulsive coupling.

Quantum chemical calculations of the relevant interresidue potential constants (compliance constants) revealed large differences between individual H bonds of the same type. For example, the central interresidue N−H···N hydrogen bond between guanine and cytosine is much stronger in comparison to the N−H···N bond between the adenine-thymine pair.

Theoretically, the bond strength of the hydrogen bonds can be assessed using NCI index, non-covalent interactions index, which allows a visualization of these non-covalent interactions, as its name indicates, using the electron density of the system.

From interpretations of the anisotropies in the Compton profile of ordinary ice that the hydrogen bond is partly covalent. However, this interpretation was challenged.

Most generally, the hydrogen bond can be viewed as a metric-dependent electrostatic scalar field between two or more intermolecular bonds. This is slightly different from the intramolecular bound states of, for example, covalent or ionic bonds; however, hydrogen bonding is generally still a bound state phenomenon, since the interaction energy has a net negative sum. The initial theory of hydrogen bonding proposed by Linus Pauling suggested that the hydrogen bonds had a partial covalent nature. This interpretation remained controversial until NMR techniques demonstrated information transfer between hydrogen-bonded nuclei, a feat that would only be possible if the hydrogen bond contained some covalent character.

The concept of hydrogen bonding once was challenging. Linus Pauling credits T. S. Moore and T. F. Winmill with the first mention of the hydrogen bond, in 1912. Moore and Winmill used the hydrogen bond to account for the fact that trimethylammonium hydroxide is a weaker base than tetramethylammonium hydroxide. The description of hydrogen bonding in its better-known setting, water, came some years later, in 1920, from Latimer and Rodebush. In that paper, Latimer and Rodebush cite work by a fellow scientist at their laboratory, Maurice Loyal Huggins, saying, "Mr. Huggins of this laboratory in some work as yet unpublished, has used the idea of a hydrogen kernel held between two atoms as a theory in regard to certain organic compounds."

A ubiquitous example of a hydrogen bond is found between water molecules. In a discrete water molecule, there are two hydrogen atoms and one oxygen atom. Two molecules of water can form a hydrogen bond between them that is to say oxygen-hydrogen bonding; the simplest case, when only two molecules are present, is called the water dimer and is often used as a model system. When more molecules are present, as is the case with liquid water, more bonds are possible because the oxygen of one water molecule has two lone pairs of electrons, each of which can form a hydrogen bond with a hydrogen on another water molecule. This can repeat such that every water molecule is H-bonded with up to four other molecules, as shown in the figure (two through its two lone pairs, and two through its two hydrogen atoms). Hydrogen bonding strongly affects the crystal structure of ice, helping to create an open hexagonal lattice. The density of ice is less than the density of water at the same temperature; thus, the solid phase of water floats on the liquid, unlike most other substances.

Liquid water's high boiling point is due to the high number of hydrogen bonds each molecule can form, relative to its low molecular mass. Owing to the difficulty of breaking these bonds, water has a very high boiling point, melting point, and viscosity compared to otherwise similar liquids not conjoined by hydrogen bonds. Water is unique because its oxygen atom has two lone pairs and two hydrogen atoms, meaning that the total number of bonds of a water molecule is up to four.

The number of hydrogen bonds formed by a molecule of liquid water fluctuates with time and temperature. From TIP4P liquid water simulations at 25 °C, it was estimated that each water molecule participates in an average of 3.59 hydrogen bonds. At 100 °C, this number decreases to 3.24 due to the increased molecular motion and decreased density, while at 0 °C, the average number of hydrogen bonds increases to 3.69. A more recent study found a much smaller number of hydrogen bonds: 2.357 at 25 °C. The differences may be due to the use of a different method for defining and counting the hydrogen bonds.

Where the bond strengths are more equivalent, one might instead find the atoms of two interacting water molecules partitioned into two polyatomic ions of opposite charge, specifically hydroxide (OH) and hydronium (HO). (Hydronium ions are also known as "hydroxonium" ions.)

Indeed, in pure water under conditions of standard temperature and pressure, this latter formulation is applicable only rarely; on average about one in every 5.5 × 10 molecules gives up a proton to another water molecule, in accordance with the value of the dissociation constant for water under such conditions. It is a crucial part of the uniqueness of water.

Because water may form hydrogen bonds with solute proton donors and acceptors, it may competitively inhibit the formation of solute intermolecular or intramolecular hydrogen bonds. Consequently, hydrogen bonds between or within solute molecules dissolved in water are almost always unfavorable relative to hydrogen bonds between water and the donors and acceptors for hydrogen bonds on those solutes. Hydrogen bonds between water molecules have an average lifetime of 10 seconds, or 10 picoseconds.

A single hydrogen atom can participate in two hydrogen bonds, rather than one. This type of bonding is called "bifurcated" (split in two or "two-forked"). It can exist, for instance, in complex natural or synthetic organic molecules. It has been suggested that a bifurcated hydrogen atom is an essential step in water reorientation.
Acceptor-type hydrogen bonds (terminating on an oxygen's lone pairs) are more likely to form bifurcation (it is called overcoordinated oxygen, OCO) than are donor-type hydrogen bonds, beginning on the same oxygen's hydrogens.

For example, hydrogen fluoride—which has three lone pairs on the F atom but only one H atom—can form only two bonds; (ammonia has the opposite problem: three hydrogen atoms but only one lone pair).


Hydrogen bonding plays an important role in determining the three-dimensional structures and the properties adopted by many synthetic and natural proteins. Compared to the C-C, C-O, and C-N bonds that comprise most polymers, hydrogen bonds are far weaker, perhaps 5%. Thus, hydrogen bonds can be broken by chemical or mechanical means while retaining the basic structure of the polymer backbone. This hierarchy of bond strengths (covalent bonds being stronger than hydrogen-bonds being stronger than van der Waals forces) is key to understanding the properties of many materials. 

In these macromolecules, bonding between parts of the same macromolecule cause it to fold into a specific shape, which helps determine the molecule's physiological or biochemical role. For example, the double helical structure of DNA is due largely to hydrogen bonding between its base pairs (as well as pi stacking interactions), which link one complementary strand to the other and enable replication.

In the secondary structure of proteins, hydrogen bonds form between the backbone oxygens and amide hydrogens. When the spacing of the amino acid residues participating in a hydrogen bond occurs regularly between positions "i" and "i" + 4, an alpha helix is formed. When the spacing is less, between positions "i" and "i" + 3, then a 3 helix is formed. When two strands are joined by hydrogen bonds involving alternating residues on each participating strand, a beta sheet is formed. Hydrogen bonds also play a part in forming the tertiary structure of protein through interaction of R-groups. (See also protein folding).

Bifurcated H-bond systems are common in alpha-helical transmembrane proteins between the backbone amide C=O of residue "i" as the H-bond acceptor and two H-bond donors from residue "i+4": the backbone amide N-H and a side-chain hydroxyl or thiol H. The energy preference of the bifurcated H-bond hydroxyl or thiol system is -3.4 kcal/mol or -2.6 kcal/mol, respectively. This type of bifurcated H-bond provides an intrahelical H-bonding partner for polar side-chains, such as serine, threonine, and cysteine within the hydrophobic membrane environments.

The role of hydrogen bonds in protein folding has also been linked to osmolyte-induced protein stabilization. Protective osmolytes, such as trehalose and sorbitol, shift the protein folding equilibrium toward the folded state, in a concentration dependent manner. While the prevalent explanation for osmolyte action relies on excluded volume effects that are entropic in nature, recent circular dichroism (CD) experiments have shown osmolyte to act through an enthalpic effect. The molecular mechanism for their role in protein stabilization is still not well established, though several mechanisms have been proposed. Recently, computer molecular dynamics simulations suggested that osmolytes stabilize proteins by modifying the hydrogen bonds in the protein hydration layer.

Several studies have shown that hydrogen bonds play an important role for the stability between subunits in multimeric proteins. For example, a study of sorbitol dehydrogenase displayed an important hydrogen bonding network which stabilizes the tetrameric quaternary structure within the mammalian sorbitol dehydrogenase protein family.

A protein backbone hydrogen bond incompletely shielded from water attack is a dehydron. Dehydrons promote the removal of water through proteins or ligand binding. The exogenous dehydration enhances the electrostatic interaction between the amide and carbonyl groups by de-shielding their partial charges. Furthermore, the dehydration stabilizes the hydrogen bond by destabilizing the nonbonded state consisting of dehydrated isolated charges.

Wool, being a protein fibre, is held together by hydrogen bonds, causing wool to recoil when stretched. However, washing at high temperatures can permanently break the hydrogen bonds and a garment may permanently lose its shape.

Hydrogen bonds are important in the structure of cellulose and derived polymers in its many different forms in nature, such as cotton and flax.

Many polymers are strengthened by hydrogen bonds within and between the chains. Among the synthetic polymers, a well characterized example is nylon, where hydrogen bonds occur in the repeat unit and play a major role in crystallization of the material. The bonds occur between carbonyl and amine groups in the amide repeat unit. They effectively link adjacent chains, which help reinforce the material. The effect is great in aramid fibre, where hydrogen bonds stabilize the linear chains laterally. The chain axes are aligned along the fibre axis, making the fibres extremely stiff and strong.

The hydrogen-bond networks make both natural and synthetic polymers sensitive to humidity levels in the atmosphere because water molecules can diffuse into the surface and disrupt the network. Some polymers are more sensitive than others. Thus nylons are more sensitive than aramids, and nylon 6 more sensitive than nylon-11.

A symmetric hydrogen bond is a special type of hydrogen bond in which the proton is spaced exactly halfway between two identical atoms. The strength of the bond to each of those atoms is equal. It is an example of a three-center four-electron bond. This type of bond is much stronger than a "normal" hydrogen bond. The effective bond order is 0.5, so its strength is comparable to a covalent bond. It is seen in ice at high pressure, and also in the solid phase of many anhydrous acids such as hydrofluoric acid and formic acid at high pressure. It is also seen in the bifluoride ion [F--H--F]. Due to severe steric constraint, the protonated form of Proton Sponge (1,8-bis(dimethylamino)naphthalene) and its derivatives also have symmetric hydrogen bonds ([N--H--N]), although in the case of protonated Proton Sponge, the assembly is bent.

Symmetric hydrogen bonds have been observed recently spectroscopically in formic acid at high pressure (>GPa). Each hydrogen atom forms a partial covalent bond with two atoms rather than one. Symmetric hydrogen bonds have been postulated in ice at high pressure (Ice X). Low-barrier hydrogen bonds form when the distance between two heteroatoms is very small.

The hydrogen bond can be compared with the closely related dihydrogen bond, which is also an intermolecular bonding interaction involving hydrogen atoms. These structures have been known for some time, and well characterized by crystallography; however, an understanding of their relationship to the conventional hydrogen bond, ionic bond, and covalent bond remains unclear. Generally, the hydrogen bond is characterized by a proton acceptor that is a lone pair of electrons in nonmetallic atoms (most notably in the nitrogen, and chalcogen groups). In some cases, these proton acceptors may be pi-bonds or metal complexes. In the dihydrogen bond, however, a metal hydride serves as a proton acceptor, thus forming a hydrogen-hydrogen interaction. Neutron diffraction has shown that the molecular geometry of these complexes is similar to hydrogen bonds, in that the bond length is very adaptable to the metal complex/hydrogen donor system.

The dynamics of hydrogen bond structures in water can be probed by the IR spectrum of OH stretching vibration. In the hydrogen bonding network in protic organic ionic plastic crystals (POIPCs), which are a type of phase change material exhibiting solid-solid phase transitions prior to melting, variable-temperature infrared spectroscopy can reveal the temperature dependence of hydrogen bonds and the dynamics of both the anions and the cations. The sudden weakening of hydrogen bonds during the solid-solid phase transition seems to be coupled with the onset of orientational or rotational disorder of the ions.

Hydrogen bonding is a key to the design of drugs. According to Lipinski's rule of five the majority of orally active drugs tend to have between five and ten hydrogen bonds. These interactions exist between nitrogen–hydrogen and oxygen–hydrogen centers. As with many other rules of thumb, many exceptions exist.





</doc>
<doc id="13610" url="https://en.wikipedia.org/wiki?curid=13610" title="Heraldry">
Heraldry

Heraldry () is a broad term, encompassing the design, display, and study of armorial bearings (known as "armory"), as well as related disciplines, such as vexillology, together with the study of ceremony, rank, and pedigree. Armory, the best-known branch of heraldry, concerns the design and transmission of the heraldic achievement. The achievement, or armorial bearings usually includes a coat of arms on a shield, helmet, and crest, together with any accompanying devices, such as supporters, badges, heraldic banners, and mottoes.

Although the use of various devices to signify individuals and groups goes back to antiquity, both the form and use of such devices varied widely, and the concept of regular, hereditary designs, constituting the distinguishing feature of heraldry, did not develop until the High Middle Ages. It is very often claimed that the use of helmets with face guards during this period made it difficult to recognize one's commanders in the field when large armies gathered together for extended periods, necessitating the development of heraldry as a symbolic language, but there is very little actual support for this view.

The beauty and pageantry of heraldic designs allowed them to survive the gradual abandonment of armour on the battlefield during the seventeenth century. Heraldry has been described poetically as "the handmaid of history", "the shorthand of history", and "the floral border in the garden of history". In modern times, individuals, public and private organizations, corporations, cities, towns, and regions use heraldry and its conventions to symbolize their heritage, achievements, and aspirations.

Various symbols have been used to represent individuals or groups for thousands of years. The earliest representations of distinct persons and regions in Egyptian art show the use of standards topped with the images or symbols of various gods, and the names of kings appear upon emblems known as serekhs, representing the king's palace, and usually topped with a falcon representing the god Horus, of whom the king was regarded as the earthly incarnation. Similar emblems and devices are found in ancient Mesopotamian art of the same period, and the precursors of heraldic beasts such as the griffin can also be found. In the Bible, the Book of Numbers refers to the standards and ensigns of the children of Israel, who were commanded to gather beneath these emblems and declare their pedigrees. The Greek and Latin writers frequently describe the shields and symbols of various heroes, and units of the Roman army were sometimes identified by distinctive markings on their shields.

Until the nineteenth century, it was common for heraldic writers to cite examples such as these, and metaphorical symbols such as the "Lion of Judah" or "Eagle of the Caesars" as evidence of the antiquity of heraldry itself; and to infer therefrom that the great figures of ancient history bore arms representing their noble status and descent. The Book of Saint Albans, compiled in 1486, declares that Christ himself was a gentleman of coat armour. But these fabulous claims have long since been dismissed as the fantasy of medieval heralds, for there is no evidence of a distinctive symbolic language akin to that of heraldry during this early period; nor do many of the shields described in antiquity bear a close resemblance to those of medieval heraldry; nor is there any evidence that specific symbols or designs were passed down from one generation to the next, representing a particular person or line of descent.

The medieval heralds also devised arms for various knights and lords from history and literature. Notable examples include the toads attributed to Pharamond, the cross and martlets of Edward the Confessor, and the various arms attributed to the Nine Worthies and the Knights of the Round Table. These too are now regarded as a fanciful invention, rather than evidence of the antiquity of heraldry.

The development of the modern heraldic language cannot be attributed to a single individual, time, or place. Although certain designs that are now considered heraldic were evidently in use during the eleventh century, most accounts and depictions of shields up to the beginning of the twelfth century contain little or no evidence of their heraldic character. For example, the Bayeux Tapestry, illustrating the Norman invasion of England in 1066, and probably commissioned about 1077, when the cathedral of Bayeux was rebuilt, depicts a number of shields of various shapes and designs, many of which are plain, while others are decorated with dragons, crosses, or other typically heraldic figures. Yet no individual is depicted twice bearing the same arms, nor are any of the descendants of the various persons depicted known to have borne devices resembling those in the tapestry.

Similarly, an account of the French knights at the court of the Byzantine emperor Alexius I at the beginning of the twelfth century describes their shields of polished metal, utterly devoid of heraldic design. A Spanish manuscript from 1109 describes both plain and decorated shields, none of which appears to have been heraldic. The Abbey of St. Denis contained a window commemorating the knights who embarked on the Second Crusade in 1147, and was probably made soon after the event; but Montfaucon's illustration of the window before it was destroyed shows no heraldic design on any of the shields.

In England, from the time of the Norman conquest, official documents had to be sealed. Beginning in the twelfth century, seals assumed a distinctly heraldic character; a number of seals dating from between 1135 and 1155 appear to show the adoption of heraldic devices in England, France, Germany, Spain, and Italy. A notable example of an early armorial seal is attached to a charter granted by Philip I, Count of Flanders, in 1164. Seals from the latter part of the eleventh and early twelfth centuries show no evidence of heraldic symbolism, but by the end of the twelfth century, seals are uniformly heraldic in nature.

One of the earliest known examples of armory as it subsequently came to be practiced can be seen on the tomb of Geoffrey Plantagenet, Count of Anjou, who died in 1151. An enamel, probably commissioned by Geoffrey's widow between 1155 and 1160, depicts him carrying a blue shield decorated with six golden lions rampant. He wears a blue helmet adorned with another lion, and his cloak is lined in vair. A medieval chronicle states that Geoffrey was given a shield of this description when he was knighted by his father-in-law, Henry I, in 1128; but this account probably dates to about 1175.

The earlier heraldic writers attributed the lions of England to William the Conqueror, but the earliest evidence of the association of lions with the English crown is a seal bearing two lions passant, used by the future King John during the lifetime of his father, Henry II, who died in 1189. Since Henry was the son of Geoffrey Plantagenet, it seems reasonable to suppose that the adoption of lions as an heraldic emblem by Henry or his sons might have been inspired by Geoffrey's shield. John's elder brother, Richard the Lionheart, who succeeded his father on the throne, is believed to have been the first to have borne the arms of three lions passant-guardant, still the arms of England, having earlier used two lions rampant combatant, which arms may also have belonged to his father. Richard is also credited with having originated the English crest of a lion statant (now statant-guardant).

The origins of heraldry are sometimes associated with the Crusades, a series of military campaigns undertaken by Christian armies from 1096 to 1487, with the goal of reconquering Jerusalem and other former Byzantine territories captured by Muslim forces during the seventh century. While there is no evidence that heraldic art originated in the course of the Crusades, there is no reason to doubt that the gathering of large armies, drawn from across Europe for a united cause, would have encouraged the adoption of armorial bearings as a means of identifying one's commanders in the field, or that it helped disseminate the principles of armory across Europe. At least two distinctive features of heraldry are generally accepted as products of the crusaders: the surcoat, an outer garment worn over the armor to protect the wearer from the heat of the sun, was often decorated with the same devices that appeared on a knight's shield. It is from this garment that the phrase "coat of arms" is derived. Also the lambrequin, or mantling, that depends from the helmet and frames the shield in modern heraldry, began as a practical covering for the helmet and the back of the neck during the Crusades, serving much the same function as the surcoat. Its slashed or scalloped edge, today rendered as billowing flourishes, is thought to have originated from hard wearing in the field, or as a means of deadening a sword blow and perhaps entangling the attacker's weapon.

The spread of armorial bearings across Europe soon gave rise to a new occupation: the herald, originally a type of messenger employed by noblemen, assumed the responsibility of learning and knowing the rank, pedigree, and heraldic devices of various knights and lords, as well as the rules and protocols governing the design and description, or "blazoning" of arms, and the precedence of their bearers. As early as the late thirteenth century, certain heralds in the employ of monarchs were given the title "King of Heralds", which eventually became "King of Arms."
In the earliest period, arms were assumed by their bearers without any need for heraldic authority. However, by the middle of the fourteenth century, the principle that only a single individual was entitled to bear a particular coat of arms was generally accepted, and disputes over the ownership of arms seems to have led to gradual establishment of heraldic authorities to regulate their use. The earliest known work of heraldic jurisprudence, "De Insigniis et Armis", was written about 1350 by Bartolus de Saxoferrato, a professor of law at the University of Padua. The most celebrated armorial dispute in English heraldry is that of "Scrope v Grosvenor" (1390), in which two different men claimed the right to bear "azure, a bend or". The continued proliferation of arms, and the number of disputes arising from different men assuming the same arms, led Henry V to issue a proclamation in 1419, forbidding all those who had not borne arms at the Battle of Agincourt from assuming arms, except by inheritance or a grant from the crown.

Beginning in the reign of Henry VIII of England, the English Kings of Arms were commanded to make "visitations", in which they traveled about the country, recording arms borne under proper authority, and requiring those who bore arms without authority either to obtain authority for them, or cease their use. Arms borne improperly were to be taken down and defaced. The first such visitation began in 1530, and the last was carried out in 1700, although no new commissions to carry out visitations were made after the accession of William III in 1689. There is very little evidence that Scots herald ever went on visitations.

In 1484, during the reign of Richard III, the various heralds employed by the crown were incorporated into England's College of Arms, through which all new grants of arms would eventually be issued. The college currently consists of three Kings of Arms, assisted by six Heralds, and four Pursuivants, or junior officers of arms, all under the authority of the Earl Marshal; but all of the arms granted by the college are granted by the authority of the crown. In Scotland Court of the Lord Lyon King of Arms oversees the heraldry, and holds court sessions which are an official part of Scotland's court system.
Similar bodies regulate the granting of arms in other monarchies and several members of the Commonwealth of Nations, but in most other countries there is no heraldic authority, and no law preventing anyone from assuming whatever arms they please, provided that they do not infringe upon the arms of another.

Although heraldry originated from military necessity, it soon found itself at home in the pageantry of the medieval tournament. The opportunity for knights and lords to display their heraldic bearings in a competitive medium led to further refinements, such as the development of elaborate tournament helms, and further popularized the art of heraldry throughout Europe. Prominent burghers and corporations, including many cities and towns, assumed or obtained grants of arms, with only nominal military associations. Heraldic devices were depicted in various contexts, such as religious and funerary art, and in using a wide variety of media, including stonework, carved wood, enamel, stained glass, and embroidery.

As the rise of firearms rendered the mounted knight increasingly irrelevant on the battlefield during the sixteenth and seventeenth centuries, and the tournament faded into history, the military character of heraldry gave way to its use as a decorative art. Freed from the limitations of actual shields and the need for arms to be easily distinguished in combat, heraldic artists designed increasingly elaborate achievements, culminating in the development of "landscape heraldry", incorporating realistic depictions of landscapes, during the latter part of the eighteenth and early part of the nineteenth century. These fell out of fashion during the mid-nineteenth century, when a renewed interest in the history of armory led to the re-evaluation of earlier designs, and a new appreciation for the medieval origins of the art. Since the late nineteenth century, heraldry has focused on the use of varied lines of partition and little-used ordinaries to produce new and unique designs.

A heraldic achievement consists of a shield of arms the coat of arms, or simply coat, together with all of its accompanying elements, such as a crest, supporters, and other heraldic embellishments. The term "coat of arms" technically refers to the shield of arms itself, but the phrase is commonly used to refer to the entire achievement. The one indispensable element of a coat of arms is the shield; many ancient coats of arms consist of nothing else, but no achievement or armorial bearings exists without a coat of arms.

From a very early date, illustrations of arms were frequently embellished with helmets placed above the shields. These in turn came to be decorated with fan-shaped or sculptural crests, often incorporating elements from the shield of arms; as well as a wreath or torse, or sometimes a coronet, from which depended the lambrequin or mantling. To these elements, modern heraldry often adds a motto displayed on a ribbon, typically below the shield. The helmet is borne of right, and forms no part of a grant of arms; it may be assumed without authority by anyone entitled to bear arms, together with mantling and whatever motto the armiger may desire. The crest, however, together with the torse or coronet from which it arises, must be granted or confirmed by the relevant heraldic authority.

If the bearer is entitled to the ribbon, collar, or badge of a knightly order, it may encircle or depend from the shield. Some arms, particularly those of the nobility, are further embellished with supporters, heraldic figures standing alongside or behind the shield; often these stand on a compartment, typically a mound of earth and grass, on which other badges, symbols, or heraldic banners may be displayed. The most elaborate achievements sometimes display the entire coat of arms beneath a pavilion, an embellished tent or canopy of the type associated with the medieval tournament., though this is only very rarely found in English or Scots achievements.

The primary element of an heraldic achievement is the shield, or escutcheon, upon which the coat of arms is depicted. All of the other elements of an achievement are designed to decorate and complement these arms, but only the shield of arms is required. The shape of the shield, like many other details, is normally left to the discretion of the heraldic artist, and many different shapes have prevailed during different periods of heraldic design, and in different parts of Europe.

One shape alone is normally reserved for a specific purpose: the lozenge, a diamond-shaped escutcheon, was traditionally used to display the arms of women, on the grounds that shields, as implements of war, were inappropriate for this purpose. This distinction was not always strictly adhered to, and a general exception was usually made for sovereigns, whose arms represented an entire nation. Sometimes an oval shield, or cartouche, was substituted for the lozenge; this shape was also widely used for the arms of clerics in French, Spanish, and Italian heraldry, although it was never reserved for their use. In recent years, the use of the cartouche for women's arms has become general in Scottish heraldry, while both Scottish and Irish authorities have permitted a traditional shield under certain circumstances, and in Canadian heraldry the shield is now regularly granted.

The whole surface of the escutcheon is termed the field, which may be plain, consisting of a single tincture, or divided into multiple sections of differing tinctures by various lines of partition; and any part of the field may be "semé", or powdered with small charges. The edges and adjacent parts of the escutcheon are used to identify the placement of various heraldic charges; the upper edge, and the corresponding upper third of the shield, are referred to as the chief; the lower part is the base. The sides of the shield are known as the dexter and sinister flanks, although it is important to note that these terms are based on the point of view of the bearer of the shield, who would be standing behind it; accordingly the side which is to the bearer's right is the dexter, and the side to the bearer's left is the sinister, although to the observer, and in all heraldic illustration, the dexter is on the left side, and the sinister on the right.

The placement of various charges may also refer to a number of specific points, nine in number according to some authorities, but eleven according to others. The three most important are "fess point", located in the visual center of the shield; the "honour point", located midway between fess point and the chief; and the "nombril point", located midway between fess point and the base. The other points include "dexter chief", "center chief", and "sinister chief", running along the upper part of the shield from left to right, above the honour point; "dexter flank" and "sinister flank", on the sides approximately level with fess point; and "dexter base", "middle base", and "sinister base" along the lower part of the shield, below the nombril point.

One of the most distinctive qualities of heraldry is the use of a limited palette of colours and patterns, usually referred to as tinctures. These are divided into three categories, known as "metals", "colours", and "furs".

The metals are "or" and "argent", representing gold and silver, respectively, although in practice they are usually depicted as yellow and white. Five colours are universally recognized: "gules", or red; "sable", or black; "azure", or blue; "vert", or green; and "purpure", or purple; and most heraldic authorities also admit two additional colours, known as "sanguine" or "murrey", a dark red or mulberry colour between gules and purpure, and "tenné", an orange or dark yellow to brown colour. These last two are quite rare, and are often referred to as "stains", from the belief that they were used to represent some dishonourable act, although in fact there is no evidence that this use existed outside the imagination of the more fanciful heraldic writers. Perhaps owing to the realization that there is really no such thing as a "stain" in genuine heraldry, as well as the desire to create new and unique designs, the use of these colours for general purposes has become accepted in the twentieth and twenty-first centuries. Occasionally one meets with other colours, particularly in continental heraldry, although they are not generally regarded among the standard heraldic colours. Among these are "cendrée", or ash-colour; "brunâtre", or brown; "bleu-céleste" or "bleu de ciel", sky blue; "amaranth" or "columbine", a bright violet-red or pink colour; and "carnation", commonly used to represent flesh in French heraldry. A more recent addition is the use of "copper" as a metal in one or two Canadian coats of arms.

There are two basic types of heraldic fur, known as ermine and vair, but over the course of centuries each has developed a number of variations. Ermine represents the fur of the stoat, a type of weasel, in its white winter coat, when it is called an ermine. It consists of a white, or occasionally silver field, powdered with black figures known as "ermine spots", representing the black tip of the animal's tail. Ermine was traditionally used to line the cloaks and caps of the nobility. The shape of the heraldic ermine spot has varied considerably over time, and nowadays is typically drawn as an arrowhead surmounted by three small dots, but older forms may be employed at the artist's discretion. When the field is sable and the ermine spots argent, the same pattern is termed "ermines"; when the field is "or" rather than argent, the fur is termed "erminois"; and when the field is sable and the ermine spots "or", it is termed "pean".

Vair represents the winter coat of the red squirrel, which is blue-grey on top and white underneath. To form the linings of cloaks, the pelts were sewn together, forming an undulating, bell-shaped pattern, with interlocking light and dark rows. The heraldic fur is depicted with interlocking rows of argent and azure, although the shape of the pelts, usually referred to as "vair bells", is usually left to the artist's discretion. In the modern form, the bells are depicted with straight lines and sharp angles, and meet only at points; in the older, undulating pattern, now known as "vair ondé" or "vair ancien", the bells of each tincture are curved and joined at the base. There is no fixed rule as to whether the argent bells should be at the top or the bottom of each row. At one time vair commonly came in three sizes, and this distinction is sometimes encountered in continental heraldry; if the field contains fewer than four rows, the fur is termed "gros vair" or "beffroi"; if of six or more, it is "menu-vair", or miniver.

A common variation is "counter-vair", in which alternating rows are reversed, so that the bases of the vair bells of each tincture are joined to those of the same tincture in the row above or below. When the rows are arranged so that the bells of each tincture form vertical columns, it is termed "vair in pale"; in continental heraldry one may encounter "vair in bend", which is similar to vair in pale, but diagonal. When alternating rows are reversed as in counter-vair, and then displaced by half the width of one bell, it is termed "vair in point", or wave-vair. A form peculiar to German heraldry is "alternate vair", in which each vair bell is divided in half vertically, with half argent and half azure. All of these variations can also be depicted in the form known as "potent", in which the shape of the vair bell is replaced by a "T"-shaped figure, known as a potent from its resemblance to a crutch. Although it is really just a variation of vair, it is frequently treated as a separate fur.

When the same patterns are composed of tinctures other than argent and azure, they are termed "vairé" or "vairy" of those tinctures, rather than "vair"; "potenté" of other colours may also be found. Usually vairé will consist of one metal and one colour, but ermine or one of its variations may also be used, and vairé of four tinctures, usually two metals and two colours, is sometimes found.

Three additional furs are sometimes encountered in continental heraldry; in French and Italian heraldry one meets with "plumeté" or "plumetty", in which the field appears to be covered with feathers, and "papelonné", in which it is decorated with scales. In German heraldry one may encounter "kursch", or vair bellies, depicted as brown and furry; all of these probably originated as variations of vair.

Considerable latitude is given to the heraldic artist in depicting the heraldic tinctures; there is no fixed shade or hue to any of them.

Whenever an object is depicted as it appears in nature, rather than in one or more of the heraldic tinctures, it is termed "proper", or the colour of nature. This does not seem to have been done in the earliest heraldry, but examples are known from at least the seventeenth century. While there can be no objection to the occasional depiction of objects in this manner, the overuse of charges in their natural colours is often cited as indicative of bad heraldic practice. The much-maligned practice of landscape heraldry, which flourished in the latter part of the eighteenth and early part of the nineteenth century, made extensive use of such non-heraldic colours.

One of the most important conventions of heraldry is the so-called "rule of tincture". To provide for contrast and visibility, metals should never be placed on metals, and colours should never be placed on colours. This rule does not apply to charges which cross a division of the field, which is partly metal and partly colour; nor, strictly speaking, does it prevent a field from consisting of two metals or two colours, although this is unusual. Furs are considered amphibious, and neither metal nor colour; but in practice ermine and erminois are usually treated as metals, while ermines and pean are treated as colours. This rule is strictly adhered to in British armory, with only rare exceptions; although generally observed in continental heraldry, it is not adhered to quite as strictly. Arms which violate this rule are sometimes known as "puzzle arms", of which the most famous example is the arms of the Kingdom of Jerusalem, consisting of gold crosses on a silver field.

The field of a shield, or less often a charge or crest, is sometimes made up of a pattern of colours, or "variation". A pattern of horizontal (barwise) stripes, for example, is called "barry", while a pattern of vertical (palewise) stripes is called "paly". A pattern of diagonal stripes may be called "bendy" or "bendy sinister", depending on the direction of the stripes. Other variations include "chevrony", "gyronny" and "chequy". Wave shaped stripes are termed "undy". For further variations, these are sometimes combined to produce patterns of "barry-bendy", "paly-bendy", "lozengy" and "fusilly". Semés, or patterns of repeated charges, are also considered variations of the field. The Rule of tincture applies to all semés and variations of the field.

The field of a shield in heraldry can be divided into more than one tincture, as can the various heraldic charges. Many coats of arms consist simply of a division of the field into two contrasting tinctures. These are considered divisions of a shield, so the rule of tincture can be ignored. For example, a shield divided azure and gules would be perfectly acceptable. A line of partition may be straight or it may be varied. The variations of partition lines can be wavy, indented, embattled, engrailed, nebuly, or made into myriad other forms; see Line (heraldry).

In the early days of heraldry, very simple bold rectilinear shapes were painted on shields. These could be easily recognized at a long distance and could be easily remembered. They therefore served the main purpose of heraldry: identification. As more complicated shields came into use, these bold shapes were set apart in a separate class as the "honorable ordinaries". They act as charges and are always written first in blazon. Unless otherwise specified they extend to the edges of the field. Though ordinaries are not easily defined, they are generally described as including the cross, the fess, the pale, the bend, the chevron, the saltire, and the pall.

There is a separate class of charges called sub-ordinaries which are of a geometrical shape subordinate to the ordinary. According to Friar, they are distinguished by their order in blazon. The sub-ordinaries include the inescutcheon, the orle, the tressure, the double tressure, the bordure, the chief, the canton, the label, and flaunches.

Ordinaries may appear in parallel series, in which case blazons in English give them different names such as pallets, bars, bendlets, and chevronels. French blazon makes no such distinction between these diminutives and the ordinaries when borne singly. Unless otherwise specified an ordinary is drawn with straight lines, but each may be indented, embattled, wavy, engrailed, or otherwise have their lines varied.

A charge is any object or figure placed on a heraldic shield or on any other object of an armorial composition. Any object found in nature or technology may appear as a heraldic charge in armory. Charges can be animals, objects, or geometric shapes. Apart from the ordinaries, the most frequent charges are the cross – with its hundreds of variations – and the lion and eagle. Other common animals are stags, wild boars, martlets, and fish. Dragons, bats, unicorns, griffins, and more exotic monsters appear as charges and as supporters.

Animals are found in various stereotyped positions or "attitudes". Quadrupeds can often be found rampant (standing on the left hind foot). Another frequent position is passant, or walking, like the lions of the coat of arms of England. Eagles are almost always shown with their wings spread, or displayed. A pair of wings conjoined is called a vol.

In English heraldry the crescent, mullet, martlet, annulet, fleur-de-lis, and rose may be added to a shield to distinguish cadet branches of a family from the senior line. These cadency marks are usually shown smaller than normal charges, but it still does not follow that a shield containing such a charge belongs to a cadet branch. All of these charges occur frequently in basic undifferenced coats of arms.

To "marshal" two or more coats of arms is to combine them in one shield, to express inheritance, claims to property, or the occupation of an office. This can be done in a number of ways, of which the simplest is impalement: dividing the field "per pale" and putting one whole coat in each half. Impalement replaced the earlier dimidiation – combining the dexter half of one coat with the sinister half of another – because dimidiation can create ambiguity between, for example, a bend and a chevron. "Dexter" (from Latin "dextra", right) means to the right from the viewpoint of the bearer of the arms and "sinister" (from Latin "sinistra", left) means to the left. The dexter side is considered the side of greatest honour (see also Dexter and sinister).

A more versatile method is quartering, division of the field by both vertical and horizontal lines. This practice originated in Spain (Castile and León) after the 13th century. As the name implies, the usual number of divisions is four, but the principle has been extended to very large numbers of "quarters".

Quarters are numbered from the dexter chief (the corner nearest to the right shoulder of a man standing behind the shield), proceeding across the top row, and then across the next row and so on. When three coats are quartered, the first is repeated as the fourth; when only two coats are quartered, the second is also repeated as the third. The quarters of a personal coat of arms correspond to the ancestors from whom the bearer has inherited arms, normally in the same sequence as if the pedigree were laid out with the father's father's ... father (to as many generations as necessary) on the extreme left and the mother's mother's...mother on the extreme right. A few lineages have accumulated hundreds of quarters, though such a number is usually displayed only in documentary contexts. The Scottish and Spanish traditions resist allowing more than four quarters, preferring to subdivide one or more "grand quarters" into sub-quarters as needed.

The third common mode of marshalling is with an inescutcheon, a small shield placed in front of the main shield. In Britain this is most often an "escutcheon of pretence" indicating, in the arms of a married couple, that the wife is an heraldic heiress (i.e., she inherits a coat of arms because she has no brothers). In continental Europe an inescutcheon (sometimes called a "heart shield") usually carries the ancestral arms of a monarch or noble whose domains are represented by the quarters of the main shield.

In German heraldry, animate charges in combined coats usually turn to face the centre of the composition.

In English the word "crest" is commonly (but erroneously) used to refer to an entire heraldic achievement of armorial bearings. The technical use of the heraldic term crest refers to just one component of a complete achievement. The crest rests on top of a helmet which itself rests on the most important part of the achievement: the shield.

The modern crest has grown out of the three-dimensional figure placed on the top of the mounted knights' helms as a further means of identification. In most heraldic traditions, a woman does not display a crest, though this tradition is being relaxed in some heraldic jurisdictions, and the stall plate of Lady Marion Fraser in the Thistle Chapel in St Giles, Edinburgh, shows her coat on a lozenge but with helmet, crest, and motto.

The crest is usually found on a wreath of twisted cloth and sometimes within a coronet. Crest-coronets are generally simpler than coronets of rank, but several specialized forms exist; for example, in Canada, descendants of the United Empire Loyalists are entitled to use a Loyalist military coronet (for descendants of members of Loyalist regiments) or Loyalist civil coronet (for others).

When the helm and crest are shown, they are usually accompanied by a mantling. This was originally a cloth worn over the back of the helmet as partial protection against heating by sunlight. Today it takes the form of a stylized cloak hanging from the helmet. Typically in British heraldry, the outer surface of the mantling is of the principal colour in the shield and the inner surface is of the principal metal, though peers in the United Kingdom use standard colourings (Gules doubled Argent - Red/White) regardless of rank or the colourings of their arms. The mantling is sometimes conventionally depicted with a ragged edge, as if damaged in combat, though the edges of most are simply decorated at the emblazoner's discretion.

Clergy often refrain from displaying a helm or crest in their heraldic achievements. Members of the clergy may display appropriate headwear. This often takes the form of a small crowned, wide brimmed hat called a galero with the colours and tassels denoting rank; or, in the case of Papal coats of arms until the inauguration of Pope Benedict XVI in 2005, an elaborate triple crown known as a tiara. Benedict broke with tradition to substitute a mitre in his arms. Orthodox and Presbyterian clergy do sometimes adopt other forms of head gear to ensign their shields. In the Anglican tradition, clergy members may pass crests on to their offspring, but rarely display them on their own shields.

An armorial motto is a phrase or collection of words intended to describe the motivation or intention of the armigerous person or corporation. This can form a pun on the family name as in Thomas Nevile's motto "Ne vile velis". Mottoes are generally changed at will and do not make up an integral part of the armorial achievement. Mottoes can typically be found on a scroll under the shield. In Scottish heraldry, where the motto is granted as part of the blazon, it is usually shown on a scroll above the crest, and may not be changed at will. A motto may be in any language.

Supporters are human or animal figures or, very rarely, inanimate objects, usually placed on either side of a coat of arms as though supporting it. In many traditions, these have acquired strict guidelines for use by certain social classes. On the European continent, there are often fewer restrictions on the use of supporters. In the United Kingdom, only peers of the realm, a few baronets, senior members of orders of knighthood, and some corporate bodies are granted supporters. Often, these can have local significance or a historical link to the armiger.

If the armiger has the title of baron, hereditary knight, or higher, he may display a coronet of rank above the shield. In the United Kingdom, this is shown between the shield and helmet, though it is often above the crest in Continental heraldry.

Another addition that can be made to a coat of arms is the insignia of a baronet or of an order of knighthood. This is usually represented by a collar or similar band surrounding the shield. When the arms of a knight and his wife are shown in one achievement, the insignia of knighthood surround the husband's arms only, and the wife's arms are customarily surrounded by an ornamental garland of leaves for visual balance.

Since arms pass from parents to offspring, and there is frequently more than one child per couple, it is necessary to distinguish the arms of siblings and extended family members from the original arms as passed on from eldest son to eldest son. Over time several schemes have been used.

To "blazon" arms means to describe them using the formal language of heraldry. This language has its own vocabulary and syntax, or rules governing word order, which becomes essential for comprehension when blazoning a complex coat of arms. The verb comes from the Middle English "blasoun", itself a derivative of the French "blason" meaning "shield". The system of blazoning arms used in English-speaking countries today was developed by heraldic officers in the Middle Ages. The blazon includes a description of the arms contained within the escutcheon or shield, the crest, supporters where present, motto and other insignia. Complex rules, such as the rule of tincture, apply to the physical and artistic form of newly created arms, and a thorough understanding of these rules is essential to the art of heraldry. Though heraldic forms initially were broadly similar across Europe, several national styles had developed by the end of the Middle Ages, and artistic and blazoning styles today range from the very simple to extraordinarily complex.

The emergence of heraldry occurred across western Europe almost simultaneously in the various countries. Originally, heraldic style was very similar from country to country. Over time, heraldic tradition diverged into four broad styles: German-Nordic, Gallo-British, Latin, and Eastern. In addition it can be argued that newer national heraldic traditions, such as South African and Canadian, have emerged in the 20th century.

Coats of arms in Germany, the Nordic countries, Estonia, Latvia, Czech lands and northern Switzerland generally change very little over time. Marks of difference are very rare in this tradition as are heraldic furs. One of the most striking characteristics of German-Nordic heraldry is the treatment of the crest. Often, the same design is repeated in the shield and the crest. The use of multiple crests is also common. The crest is rarely used separately as in British heraldry, but can sometimes serve as a mark of difference between different branches of a family. Torse is optional. Heraldic courtoisie is observed: that is, charges in a composite shield (or two shields displayed together) usually turn to face the centre.

Coats consisting only of a divided field are somewhat more frequent in Germany than elsewhere.

The Low Countries were great centres of heraldry in medieval times. One of the famous armorials is the Gelre Armorial or "Wapenboek", written between 1370 and 1414.
Coats of arms in the Netherlands were not controlled by an official heraldic system like the two in the United Kingdom, nor were they used solely by noble families. Any person could develop and use a coat of arms if they wished to do so, provided they did not usurp someone else's arms, and historically, this right was enshrined in Roman Dutch law. As a result, many merchant families had coats of arms even though they were not members of the nobility. These are sometimes referred to as "burgher arms," and it is thought that most arms of this type were adopted while the Netherlands was a republic (1581–1806). This heraldic tradition was also exported to the erstwhile Dutch colonies.

Dutch heraldry is characterised by its simple and rather sober style, and in this sense, is closer to its medieval origins than the elaborate styles which developed in other heraldic traditions.

The use of cadency marks to difference arms within the same family and the use of semy fields are distinctive features of Gallo-British heraldry (in Scotland the most significant mark of cadency being the bordure, the small brisures playing a very minor role). It is common to see heraldic furs used. In the United Kingdom, the style is notably still controlled by royal officers of arms. French heraldry experienced a period of strict rules of construction under Napoleon. English and Scots heraldries make greater use of supporters than other European countries.

Furs, chevrons and five-pointed stars are more frequent in France and Britain than elsewhere.

The heraldry of southern France, Andorra, Spain, and Italy is characterized by a lack of crests, and uniquely shaped shields. Portuguese heraldry, however, does use crests. Portuguese and Spanish heraldry, which together form a larger Iberian tradition of heraldry, occasionally introduce words to the shield of arms, a practice usually avoided in British heraldry. Latin heraldry is known for extensive use of quartering, because of armorial inheritance via the male and the female lines. Moreover, Italian heraldry is dominated by the Roman Catholic Church, featuring many shields and achievements, most bearing some reference to the Church.

Trees are frequent charges in Latin arms. Charged bordures, including bordures inscribed with words, are seen often in Spain.

Eastern European heraldry is in the traditions developed in Belarus, Bulgaria, Serbia, Croatia, Hungary, Romania, Lithuania, Poland, Slovakia, Ukraine, and Russia. Eastern coats of arms are characterized by a pronounced, territorial, clan system – often, entire villages or military groups were granted the same coat of arms irrespective of family relationships. In Poland, nearly six hundred unrelated families are known to bear the same Jastrzębiec coat of arms. Marks of cadency are almost unknown, and shields are generally very simple, with only one charge. Many heraldic shields derive from ancient house marks. At the least, fifteen per cent of all Hungarian personal arms bear a severed Turk's head, referring to their wars against the Ottoman Empire.

True heraldry, as now generally understood, has its roots in medieval Europe. However, there have been other historical cultures which have used symbols and emblems to represent families or individuals, and in some cases these symbols have been adopted into Western heraldry. For example, the coat of arms of the Ottoman Empire incorporated the royal tughra as part of its crest, along with such traditional Western heraldic elements as the escutcheon and the compartment.

Ancient Greeks were among the first civilizations to use symbols consistently in order to identify a warrior, clan or a state. The first record of a shield blazon is illustrated in Aeschylus' tragedy "Seven Against Thebes".

, also , , and , are Japanese emblems used to decorate and identify an individual or family. While "mon" is an encompassing term that may refer to any such device, "kamon" and "mondokoro" refer specifically to emblems used to identify a family. An authoritative "mon" reference compiles Japan's 241 general categories of "mon" based on structural resemblance (a single "mon" may belong to multiple categories), with 5116 distinct individual "mon" (it is however well acknowledged that there exist lost or obscure "mon" that are not in this compilation).

The devices are similar to the badges and coats of arms in European heraldic tradition, which likewise are used to identify individuals and families. "Mon" are often referred to as crests in Western literature, another European heraldic device similar to the "mon" in function.

Socialist heraldry, also called communist heraldry, consists of emblems in a style typically adopted by communist states and characterized by communist symbolism. Although commonly called "coats of arms", most such devices are not actually coats of arms in the traditional heraldic sense and should therefore, in a strict sense, not be called arms at all. Many communist governments purposely diverged from the traditional forms of European heraldry in order to distance themselves from the monarchies that they usually replaced, with actual coats of arms being seen as symbols of the monarchs.

The Soviet Union was the first state to use socialist heraldry, beginning at its creation in 1922. The style became more widespread after World War II, when many other communist states were established. Even a few non-socialist states have adopted the style, for various reasons—usually because communists had helped them to gain independence—but also when no apparent connection to a Communist nation exists, such as the emblem of Italy. After the fall of the Soviet Union and the other communist states in Eastern Europe in 1989–1991, this style of heraldry was often abandoned for the old heraldic practices, with many (but not all) of the new governments reinstating the traditional heraldry that was previously cast aside.

A tamga or tamgha "stamp, seal" (, Turkic: tamga) is an abstract seal or stamp used by Eurasian nomadic peoples and by cultures influenced by them. The tamga was normally the emblem of a particular tribe, clan or family. They were common among the Eurasian nomads throughout Classical Antiquity and the Middle Ages (including Alans, Mongols, Sarmatians, Scythians and Turkic peoples). Similar "tamga-like" symbols were sometimes also adopted by sedentary peoples adjacent to the Pontic-Caspian steppe both in Eastern Europe and Central Asia, such as the East Slavs, whose ancient royal symbols are sometimes referred to as "tamgas" and have similar appearance.

Unlike European coats of arms, tamgas were not always inherited, and could stand for families or clans (for example, when denoting territory, livestock, or religious items) as well as for specific individuals (such as when used for weapons, or for royal seals). One could also adopt the tamga of one's master or ruler, therefore signifying said master's patronage. Outside of denoting ownership, tamgas also possessed religious significance, and were used as talismans to protect one from curses (it was believed that, as symbols of family, tamgas embodied the power of one's heritage). Tamgas depicted geometric shapes, images of animals, items, or glyphs. As they were usually inscribed using heavy and unwieldy instruments, such as knives or brands, and on different surfaces (meaning that their appearance could vary somewhat), tamgas were always simple and stylised, and needed to be laconic and easily recognisable.

Every sultan of the Ottoman Empire had his own monogram, called the tughra, which served as a royal symbol. A coat of arms in the European heraldic sense was created in the late 19th century. Hampton Court requested from Ottoman Empire the coat of arms to be included in their collection. As the coat of arms had not been previously used in Ottoman Empire, it was designed after this request and the final design was adopted by Sultan Abdul Hamid II on April 17, 1882. It included two flags: the flag of the Ottoman Dynasty, which had a crescent and a star on red base, and the flag of the Islamic Caliph, which had three crescents on a green base.

Heraldry flourishes in the modern world; institutions, companies, and private persons continue using coats of arms as their pictorial identification. In the United Kingdom and Ireland, the English Kings of Arms, Scotland's Lord Lyon King of Arms, and the Chief Herald of Ireland continue making grants of arms. There are heraldic authorities in Canada, South Africa, Spain, and Sweden that grant or register coats of arms. In South Africa, the right to armorial bearings is also determined by Roman Dutch law, due to its origins as a 17th-century colony of the Netherlands.

Heraldic societies abound in Africa, Asia, Australasia, the Americas and Europe. Heraldry aficionados participate in the Society for Creative Anachronism, medieval revivals, micronations and other related projects. Modern armigers use heraldry to express ancestral and personal heritage as well as professional, academic, civic, and national pride. Little is left of class identification in modern heraldry, where the emphasis is more than ever on expression of identity.

Heraldry continues to build on its rich tradition in academia, government, guilds and professional associations, religious institutions, and the military. Nations and their subdivisions – provinces, states, counties, cities, etc. – continue to build on the traditions of civic heraldry. The Roman Catholic Church, Anglican churches, and other religious institutions maintain the traditions of ecclesiastical heraldry for clergy, religious orders, and schools.

Many of these institutions have begun to employ blazons representing modern objects unknown in the medieval world. For example, some heraldic symbols issued by the United States Army Institute of Heraldry incorporate symbols such as guns, airplanes, or locomotives. Some scientific institutions incorporate symbols of modern science such as the atom or particular scientific instruments. The arms of the United Kingdom Atomic Energy Authority uses traditional heraldic symbols to depict the harnessing of atomic power. Locations with strong associations to particular industries may incorporate associated symbols. The coat of arms of Stenungsund Municipality in Sweden, pictured right, incorporates a hydrocarbon molecule, alluding to the historical significance of the petrochemical industry in the region.

Heraldry in countries with heraldic authorities continues to be regulated generally by laws granting rights to arms and recognizing possession of arms as well as protecting against their misuse. Countries without heraldic authorities usually treat coats of arms as creative property in the manner of logos, offering protection under copyright laws. This is the case in Nigeria, where most of the components of its heraldic system are otherwise unregulated.




</doc>
<doc id="13611" url="https://en.wikipedia.org/wiki?curid=13611" title="Heretic (video game)">
Heretic (video game)

Heretic is a dark fantasy first-person shooter video game released in 1994. It was developed by Raven Software and published by id Software through GT Interactive. The game was released on Steam on August 3, 2007.

Using a modified version of the "Doom" engine, "Heretic" was one of the first first-person games to feature inventory manipulation and the ability to look up and down. It also introduced multiple gib objects that spawned when a character suffered a death by extreme force or heat. Previously, the character would simply crumple into a heap. The game used randomized ambient sounds and noises, such as evil laughter, chains rattling, distantly ringing bells, and water dripping in addition to the background music to further enhance the atmosphere. The music in the game was composed by Kevin Schilder. An indirect sequel, "", was released the following year. "Heretic II" was released in 1998, which served as a direct sequel continuing the story.

Three brothers (D'Sparil, Korax, and Eidolon), known as the Serpent Riders, have used their powerful magic to possess seven kings of Parthoris, turning them into mindless puppets and corrupting their armies. The Sidhe elves resist the Serpent Riders' magic. The Serpent Riders thus declared the Sidhe as heretics and waged war against them. The Sidhe are forced to take a drastic measure to sever the natural power of the kings destroying them and their armies, but at the cost of weakening the elves' power, giving the Serpent Riders an advantage to slay the elders. While the Sidhe retreat, one elf (revealed to be named Corvus in "Heretic II") sets off on a quest of vengeance against the weakest of the three Serpent Riders, D'Sparil. He travels through the "City of the Damned", the ruined capital of the Sidhe (its real name is revealed to be Silverspring in "Heretic II"), then past the demonic breeding grounds of Hell's Maw and finally the secret Dome of D'Sparil.

The player must first fight through the undead hordes infesting the location where the elders performed their ritual. At its end is the gateway to Hell's Maw, guarded by the Iron Liches. After defeating them, the player must seal the portal and so prevent further infestation, but after he enters the portal guarded by the Maulotaurs, he finds himself inside D'Sparil's dome. After killing D'Sparil, Corvus ends up on a perilous journey with little hope of returning home.

The gameplay of "Heretic" is heavily derived from "Doom", with a level-based structure and an emphasis on finding the proper keys to progress. Many weapons are similar to those from "Doom"; the early weapons in particular are near-exact copies in functionality to those seen in "Doom". Raven added a number of features to "Heretic" that differentiated it from "Doom", however, notably interactive environments, such as rushing water that pushes the player along, and inventory items. In "Heretic", the player can pick up many different items to use at their discretion. These items range from health potions to the "morph ovum", which transforms enemies into chickens. One of the most notable pickups that can be found is the "Tome of Power" which acts as a secondary firing mode for certain weapons, resulting in a much more powerful projectile from each weapon, some of which change the look of the projectile entirely. "Heretic" also features an improved version of the "Doom" engine, sporting the ability to look up and down within constraints, as well as fly. However, the rendering method for looking up and down merely uses a proportional pixel-shearing effect rather than any new rendering algorithm, which distorts the view considerably when looking at high-elevation angles.

As with "Doom", "Heretic" contains various cheat codes that allow the player to be invulnerable, obtain every weapon, be able to instantly kill every monster in a particular level, and several other abilities. However, if the player uses the "all weapons and keys" cheat ("codice_1") from "Doom", a message appears warning the player against cheating and takes away all of his weapons, leaving him with only a quarterstaff. If the player uses the "god mode" cheat ("codice_2") from "Doom", the game will display a message saying "Trying to cheat, eh? Now you die!" and kills the player character.

The original shareware release of "Heretic" came bundled with support for online multiplayer through the new DWANGO service.

Like "Doom", "Heretic" was developed on NeXTSTEP. John Romero helped Raven employees set up the development computers, and taught them how to use id's tools and "Doom" engine.

The original version of "Heretic" was only available through shareware registration (i.e. mail order) and contained three episodes. The retail version, "Heretic: Shadow of the Serpent Riders", was distributed by GT Interactive in 1996, and featured the original three episodes and two additional episodes: "The Ossuary", which takes the player to the shattered remains of a world conquered by the Serpent Riders several centuries ago, and "The Stagnant Demesne", where the player enters D'Sparil's birthplace. This version was the first official release of "Heretic" in Europe. A free patch was also downloadable from Raven's website to update the original "Heretic" with the content found in "Shadow of the Serpent Riders".

Along with the two full additional episodes, "Shadow of the Serpent Riders" contains 3 additional levels in a third additional episode (unofficially known as "Fate's Path") which is inaccessible without the use of cheat codes. The first of these three levels can be accessed by typing the cheat ("codice_3"). The first two levels are fully playable, but the third level does not have an exit so the player is unable to progress further.

On January 11, 1999, the source code of the game engine used in "Heretic" was published by Raven Software under a license that granted rights to non-commercial use, and was re-released under the GNU General Public License on September 4, 2008. This resulted in ports to Linux, Amiga, Atari, and other operating systems, and updates to the game engine to utilize 3D acceleration. The shareware version of a console port for the Dreamcast was also released.

"Heretic" received mixed reviews, garnering an aggregated score of 62% on GameRankings and 78% on PC Zone. "Heretic" and "Hexen" shipped a combined total of roughly 1 million units by August 1997.

While remarking that "Heretic" is a thinly-veiled clone of "Doom", and that its being released in Europe after and with "Quake" due out shortly makes it somewhat outdated, "Maximum" nonetheless regarded it as an extremely polished and worthwhile purchase. They particularly highlighted the two additional episodes of the retail version, saying they offer a satisfying challenge even to first person shooter veterans and are largely what make the game worth buying.

In 1996, "Computer Gaming World" listed being turned into a chicken as #3 on its list of "the 15 best ways to die in computer gaming".

"Next Generation" reviewed the PC version of the game, and stated that "If you're only going to get one action game in the next couple of months, this is the one."

"Heretic" has received three sequels: "", "Hexen II", and "Heretic II". Following ZeniMax Media's acquisition of id Software, the rights to the series have been disputed between both id and Raven Software; Raven's parent company Activision holds the developing rights, while id holds the publishing rights to the first three games. Until both companies come to an agreement, neither will be able to make another installment in the series.

Further homages to the series have been made in other id Software titles; In 2009's "Wolfenstein", which Raven Software developed, "Heretic"'s Tomes of Power are collectible power-ups found throughout the game. The character Galena from "Quake Champions" wears armor bearing the icon of the Serpent Riders.



</doc>
<doc id="13612" url="https://en.wikipedia.org/wiki?curid=13612" title="Hexen: Beyond Heretic">
Hexen: Beyond Heretic

Hexen: Beyond Heretic is a dark fantasy first-person shooter video game developed by Raven Software and published by id Software through GT Interactive Software on October 30, 1995. It is the sequel to 1994's "Heretic", and the second game in Raven Software's "Serpent Riders" trilogy, which culminated with "Hexen II". The title comes from the German noun , which means "witches", and/or the verb , which means "to cast a spell". Game producer John Romero stated that a third, unreleased game in this series was to be called "Hecatomb".

"Hexen: Beyond Heretic" met with highly positive reviews upon release, though the various 1997 console ports were negatively received due to issues with frame rate and controls and the aging of the game itself. Critical plaudits for the game centered on the non-linear level design and the selection of three playable characters, each offering a distinct gameplay experience.

Following the tale of D'Sparil's defeat in "Heretic", "Hexen" takes place in another realm, Cronos, which is besieged by the second of the three Serpent Riders, Korax. Three heroes set out to destroy Korax. The player assumes the role of one such hero. Throughout the course of his quest, he travels through elemental dungeons, a wilderness region, a mountainside seminary, a large castle, and finally a necropolis, before the final showdown with the Serpent Rider.

A new series feature introduced in "Hexen" is the choice of three character classes. Players may choose to play as a fighter (Baratus), a cleric (Parias), or a mage (Daedolon). Each character has unique weapons and physical characteristics, lending an additional degree of variety and replay value to the game. The Fighter relies mainly on close quarter physical attacks with weapons both mundane and magical in nature, and is tougher and faster than the other characters. The Mage uses an assortment of long-range spells, whose reach is counterbalanced by the fact that he is the most fragile and slowest moving of the classes. The Cleric arms himself with a combination of both melee and ranged capabilities, being a middle ground of sorts between the other two classes. Additionally, certain items, such as the flechette (poison gas bomb), behave differently when collected and used by each of the classes, functioning in a manner better suiting their varying approach to combat.

"Hexen" introduces "hub" levels to the series, wherein the player can travel back and forth between central hub levels and connected side levels. This is done in order to solve larger-scale puzzles that require a series of items or switches to be used. The player must traverse through a hub in order to reach a boss and advance to the next hub.

The inventory system returns from "Heretic" with several new items such as the "disc of repulsion" which pushes enemies away from the player and the "icon of the defender" which provides invincibility to each class in a different manner.

Like "Heretic", "Hexen" was developed on NeXTSTEP. "Hexen" uses a modified version of the "Doom" engine, which allows looking up and down, network play with up to eight players, and the choice of three character classes. It also popularized the "hub system" of level progression in the genre of first-person shooter games. Unlike previous games, which had relied purely on General MIDI for music, "Hexen" is also able to play tracks from a CD. The game's own CD contained a soundtrack in an audio format that was exactly the same as the MIDI soundtrack, but played through a high-quality sound module. However, the most significant improvement was the addition of wall translation, rotation, and level scripting.

"Polyobjects" are the walls that move within the game. Because the "Doom" engine uses the binary space partitioning system for rendering, it does not enable moving walls. "Hexen"s moving walls are actually one-sided lines built somewhere else on the map and rendered at the desired start spot when the level is loaded. This enables a pseudo-moving wall, but does not allow moving sectors (such as seeing the tops of moving doors). This often creates problems in sectors containing more than one node, however, explaining the relatively limited use of polyobjects.

Whereas "Doom", "Doom II", and "Heretic" rely on lines within the maps to perform simple actions, "Hexen" also allows these actions to be activated by Action Code Script (ACS). These scripts use a syntactic variant of C, thus allowing special sequencing of game actions. Programming features such as randomization, variables, and intermap script activation enable smooth hub gameplay and are responsible for most of the special effects within the game: on-screen messages, random sound effects, monster spawning, sidedef texture changes, versatile control of polyobjects, level initialization for deathmatch, and even complex environment changes such as earthquakes manipulating floor textures and heights.

On January 11, 1999, the source code for "Hexen" was released by Raven Software under a license that granted rights to non-commercial use, and was re-released under the GNU General Public License on September 4, 2008. This allowed the game to be ported to different platforms such as Linux, AmigaOS, and OS/2 (EComStation).

"Hexen" is compatible with many "Doom" source ports; "Hexen"s features are also compatible with "Doom" WADs made for source ports regardless of what game they are being played on.

The score was composed by Kevin Schilder. In contrast to "Heretic", some songs in "Hexen", in addition to MIDI versions, had higher-quality versions on CD. When playing in CD-audio mode, songs absent from CD would be replaced by some existing CD tracks.

"Hexen" was released for the Sega Saturn, PlayStation, and Nintendo 64, all released by GT Interactive during the first half of 1997. While presenting several specific differences in their respective translations of the original PC game, all of them constitute essentially the same game with no major changes to level design, plot, or overall delivery.

The PlayStation version, developed by Probe Entertainment, has the FMV scenes and Redbook audio music from the PC CD-ROM version, but no multiplayer mode. The scripting and animation is slower, enemies have only their front sprites and lack gory deaths when attacked by strong hits or weapons, and the frame rate is slower. Although all levels are present in this version and feature their correct layouts, their architecture details are somewhat simplified and there is some loss in overall lighting quality. This port is based on a beta version of the original PC version of "Hexen" as many gameplay tweaks are shared, such as the simpler level design and the Fighter's fists being weaker compared to other versions.

The Sega Saturn version, also developed by Probe, inherits most of the restrictions of the PlayStation version, such as the simplified scenery architecture and the downgraded lighting, although it does feature improvements in certain aspects. The scripting is faster, and the frame rate, while not fluid or consistent, is slightly better. The enemies still have all but their front sprites missing, but they retain their gory deaths when killed by a strong hit or weapon. This version also has hidden two-player link-up cooperative and deathmatch modes, accessible only through the unlockable cheat menu. While this port shares the FMV scenes and most of the Redbook audio music from the other CD-ROM versions, it also includes some new music tracks.

The Nintendo 64 version, developed by Software Creations, retains all of the graphical quality and scenery architecture, has a consistent frame rate, and includes high detail and smooth filtering. This version also has four-player split-screen cooperative and deathmatch modes, although they must be played in low detail mode. Due to cartridge storage limitations, the Nintendo 64 version is based on the original PC floppy version and lacks the FMV scenes and Redbook audio music introduced in the CD-ROM version, although it has new narrative introductions to the levels.

"Deathkings of the Dark Citadel" is an official expansion pack that was released for "Hexen" in 1996. It features three more hubs with a total of 20 new single player levels and six new deathmatch levels. Unlike the "Shadow of the Serpent Riders" expansion pack for "Heretic", it had to be purchased in retail stores or by mail order. This was unusual at the time, as most non-free expansion packs also included other new or revised gameplay elements. "Deathkings of the Dark Citadel", unlike "Shadow of the Serpent Riders", was not packaged with the original game, meaning that both had to be purchased separately, and the expansion would not work without already having "Hexen". This expansion pack also did not initially include nor enable any music. Music could be fully enabled by applying a patch specially released to address this issue (usually found online under the name "dkpatch").

Each of the hubs (The Blight, The Constable's Gate, and The Nave) features one secret level, and new puzzles based on the quest items from the original game (no new quest artifacts were added). Any type of enemy may spawn on the map.

The final level of the expansion, the Dark Citadel itself, is an arena-like level, which features teleporting waves of monsters and three bosses (Fighter, Cleric, and Mage clones).

"Heretic" and "Hexen" shipped a combined total of roughly 1 million units to retailers by August 1997.

Reviewing the PC version, "Maximum" remarked that "Hexen" sets itself apart from other "3D slashers" with its selection of characters and novel approach to level design, which "leads to your character choosing their path rather than being guided around a rather linear series of rooms, proving that 3D games have matured". They also commented that the gameplay is consistently intense due to the difficulty of the enemies, the variety of weapons and power-ups, and the sheer size and breadth of the levels. They gave the game 5 out of 5 stars and their "Maximum Game of the Month" award. A reviewer for "Next Generation" opined that ""Hexen" takes everything that was good about "Heretic", and makes it even better." He commented that the ability to choose between three different character classes gives the game replay value, something that had been missing from first-person shooters up until then, and though the graphics are blocky and pixelated, the "eerily lifelike" sound effects make up for it to a large extent. Like "Maximum", he praised the non-linear level design and concluded the game to be a must-have for any first-person shooter fan. Chris Hudak, citing the differing abilities of the three playable characters, called "Hexen" "Slicker, smarter and more stylish than "Doom"---with all the killing and three times the replay value."

"Computer Games Strategy Plus" named "Hexen" the best "First-Person Action" title of 1995. It was also a runner-up for "Computer Gaming World"s 1995 "Action Game of the Year" award, which ultimately went to "". The editors called it "another "Doom" bloodfest distinguished by its fantasy setting and the fact that it let you play as either a fighter, priest or mage, each with unique attributes and weapons".

The Saturn version was far less well-received. A review in "Next Generation" of the Saturn version reasoned that, "Like oil and water, "Doom"-style games and console conversions don't mix well. Unless the programmers are willing to rewrite the graphics engine from scratch, PC ports suffer from getting cramped into too little memory and neglecting the console's native 3D hardware." The reviewer recommended Saturn owners instead try "PowerSlave" or "Ghen War", first-person shooters specifically designed for the console. Shawn Smith and Sushi-X of "Electronic Gaming Monthly" similarly said the game had not been converted well from PC. Others described the Saturn port as an exact conversion, and argued the problem was simply that "Hexen" was too old a game to be released for console in 1997 without any improvements. Though they disagreed on exact reasons, most critics agreed that the Saturn version suffers from pixelated graphics, dramatic drops in frame rate, and cumbersome controls. Scary Larry of "GamePro" gave it a mixed review, summarizing that "although it doesn't live up to "PowerSlave"s standards, it's still decent fun." John Broady of "GameSpot" gave a slightly more dismal assessment: "Despite these glaring deficiencies, "Hexen" nonetheless offers enough enhancements over the standard shooter to warrant a rental, especially for fans of role-playing games who thirst for real-time action. ... But for the rest, the Saturn version of "Hexen" is a classic game of too little and too late." Rich Leadbetter of "Sega Saturn Magazine" and James Price of "Saturn Power" defended the Saturn version, commenting that, although not outstanding, it is far superior to the Saturn version of "Doom", which was released at roughly the same time. Price was particularly enthusiastic about the link cable-enabled multiplayer mode.

The Nintendo 64 version also left most critics unimpressed. The four-player mode was praised as an unprecedented feature in console first person shooters, but the graphics were considered unacceptably poor, particularly the frame rate and the usage of the Nintendo 64's mip-mapping and anti-aliasing in a way which actually worsened the visuals of the game. As with the Saturn version, some critics opined that "Hexen" was too dated by this time to be receiving a straightforward port. Joe Fielder of "GameSpot" additionally complained of a severe bug in the save feature. In a dissenting opinion, Scary Larry concluded that "Although not as polished as "" or as fun and creepy as "Doom 64", "Hexen" gives you three characters to choose from, and the action's addicting once you get into it." He gave it higher scores than the Saturn version in every category except sound. In contrast, Matt Casamassina of "IGN" called it "A shoddy port of a PC game that wasn't so great to begin with."

The PlayStation version was even more negatively received; critics universally panned the port for its poor frame rate, pixelated graphics, and sloppy platform-jumping controls.

"Electronic Gaming Monthly"s 1998 Video Game Buyer's Guide named "Hexen" the 1997 "Game that Should've Stayed on the PC", commenting that while the Nintendo 64 version was the best of the console ports, all three were poor conversions, and "Hexen" was too old by the time they were released.



</doc>
<doc id="13613" url="https://en.wikipedia.org/wiki?curid=13613" title="Hexen II">
Hexen II

Hexen II is a dark fantasy first-person shooter (FPS) video game developed by Raven Software from 1996 to 1997, then published by id Software and distributed by Activision. It is the third game in the ""/"Heretic" series, and the last in the Serpent Riders trilogy. It was later made available on Steam on August 3, 2007. Using a modified "Quake" engine, it features single-player and multiplayer game modes, as well as four character classes to choose from, each with different abilities. These include the "offensive" Paladin, the "defensive" Crusader, the spell-casting Necromancer, and the stealthy Assassin.

Improvements from "" and "Quake" include destructible environments, mounted weapons, and unique level up abilities. Like its predecessor, "Hexen II" also uses a "hub" system. These hubs are a number of interconnected levels; changes made in one level have effects in another. Furthermore, the Tome of Power artifact makes a return from "Heretic".

The gameplay of "Hexen II" is very similar to that of the original "Hexen". Instead of three classes, "Hexen II" features four: Paladin, Crusader, Assassin, and Necromancer, each with their own unique weapons and play style.

"Hexen II" also adds certain role-playing video game elements to the mix. Each character has a series of statistics which increase as they gain experience. This then causes the player character to grow in power as his or her HP and Mana increases.

Thyrion is a world that was enslaved by the Serpent Riders. The two previous games in the series documented the liberation of two other worlds, along with the death of their Serpent Rider overlords. Now, the oldest and most powerful of the three Serpent Rider brothers, Eidolon, must be defeated to free Thyrion. Eidolon is supported by his four generals, themselves a reference to the Four Horsemen of the Apocalypse. To confront each general, the player has to travel to four different continents, each possessing a distinct theme (Medieval European for Blackmarsh, Mesoamerican for Mazaera, Ancient Egyptian for Thysis, and Greco-Roman for Septimus). Then, finally, the player returns to Blackmarsh in order to confront Eidolon himself inside of his own dominion Cathedral.

What was originally supposed to be the final game in a trilogy, the sequel to Hexen was originally titled "Hecatomb" but was abandoned after John Romero left id Software in 1996. Activision, the distributor at the time, pressured Raven Software to split development of Hecatomb into two different games, Hexen II and Heretic II. Activision felt that the previous entries in the series, Heretic and Hexen, were different enough from one another that they should treat them as separate entities going forward, instead of just one final game to complete a trilogy. Only a select few ideas of Romero's from "Hecatomb" would ultimately make their way into what became Hexen II and Heretic II.

"Hexen II" was based on an enhanced version of the "Quake" engine. "Hexen II", by way of the "Quake" engine, uses OpenGL for 3D acceleration. However, due to the prevalence of 3dfx hardware at the time of release, the Windows version of the game installs an OpenGL ICD (opengl32.dll) designed specifically for 3dfx's hardware. This driver acts as a wrapper for the proprietary Glide API, and thus is only compatible with 3dfx hardware. Custom OpenGL drivers were also released by PowerVR and Rendition for running "Hexen II" with their respective (and also now defunct) products. Removal of the ICD allows the game to use the default OpenGL system library. Much of the music in this game is remixed versions of the soundtracks of "" and "Heretic" to match the hub themes.

Activision acquired the rights to publish versions of the game for the PlayStation and Sega Saturn. However, neither port was released.

A modification titled "Siege" was created and released by Raven Software in 1998 using updated QuakeWorld architecture, aptly dubbed "HexenWorld". The production concept was to eliminate a normal deathmatch environment in favor of a teamplay castle siege. The basic premise was to divide the players into two teams—attackers and defenders—with each side either assaulting or protecting the castle respectively. At the end of the time limit, whichever team controlled the crown was declared victorious. The mod featured appropriate objects used in the single-player portion of the game, namely catapults and ballistae. The classes, however, were drastically altered with new weapons and abilities, reflecting the departure from the normal deathmatch experience presented in "HexenWorld".

Following the tradition from "Heretic" and "Hexen", Raven released the source code of the "Hexen II" engine on November 10, 2000. This time the source was released under the GNU General Public License, allowing source ports to be made to different platforms like Linux and the Dreamcast.

An expansion pack called "Hexen II Mission Pack: Portal of Praevus" was released on April 1, 1998. It features new levels, new enemies and a new playable character class, The Demoness. It focuses on the attempted resurrection of the three Serpent Riders by the evil wizard Praevus, and takes place in a fifth continent, Tulku, featuring a Sino-Tibetan setting. Unlike the original game, the expansion was not published by id Software, and as such is not currently available via digital re-releases.

The expansion features new quest items, new enemies, and new weapons for the Demoness. She is the only player class to have a ranged starting weapon (similar to the Mage class in the original "Hexen"), whereas all other characters start with melee weapons. It also introduced minor enhancements to the game engine, mostly related to user interface, level scripts, particle effects (rain or snow), and 3D objects. "Portal of Praevus" also features a secret (easter egg) skill level, with respawning monsters. The only released patch for the expansion added respawning of certain items (such as health and ammo) in Nightmare mode, so that it would be slightly easier for playing.

Because of the popularity of the original "Hexen", the game was heavily anticipated. Upon its release, "Hexen II" received "mixed to positive" reviews. "Edge" praised the game for being different from other "Quake" engine-based games, highlighting its inventive and interactive levels, enemy variety, and artificial intelligence. The magazine also credited the game's diversity of weapons and spells for offering different combat strategies.

According to Erik Bethke, "Hexen II" was commercially unsuccessful, with sales slightly above 30,000 units.



</doc>
<doc id="13614" url="https://en.wikipedia.org/wiki?curid=13614" title="Heretic II">
Heretic II

Heretic II is a dark fantasy action-adventure game developed by Raven Software and published by Activision in 1998 continuing the story of Corvus, the main character from its predecessor, "Heretic". It is the fourth game in the "" series and comes after the "Serpent Rider" trilogy.

Using a modified Quake II engine, the game features a mix of a third-person camera with a first-person shooter's action, making for a new gaming experience at the time. While progressive, this was a controversial design decision among fans of the original game, a well-known first-person shooter built on the Doom engine. The music was composed by Kevin Schilder. Gerald Brom contributed conceptual work to characters and creatures for the game. This is the only "Heretic"/"Hexen" video game that is unrelated to id Software, apart from its role as engine licenser.

"Heretic II" was later ported to Linux by Loki Software, to the Amiga by Hyperion Entertainment, and Macintosh by MacPlay.

After Corvus returns from his banishment, he finds that a mysterious plague has swept the land of Parthoris, taking the sanity of those it does not kill. Corvus, the protagonist of the first game, is forced to flee his hometown of Silverspring after the infected attack him, but not before he is infected himself. The effects of the disease are held at bay in Corvus’ case because he holds one of the Tomes of Power, but he still must find a cure before he succumbs.

His quest leads him through the city and swamps to a jungle palace, then through a desert canyon and insect hive, followed by a dark network of mines and finally to a castle on a high mountain where he finds an ancient Seraph named Morcalavin. Morcalavin is trying to reach immortality using the seven Tomes of Power, but he uses a false tome, as Corvus has one of them. This has caused Morcalavin to go insane and create the plague. During a battle between Corvus and Morcalavin, Corvus switches the false tome for his real one, curing Morcalavin's insanity and ending the plague.

Unlike previous games in the "Heretic/Hexen" series, which were first-person shooters, players control Corvus from a camera fixed behind him in the third-person perspective. Players are able to use a combination of both melee and ranged attacks, similar to its predecessor. While there are still three weapons the player can collect that each use their own ammo, they also have the ability to use several offensive and defensive spells that draw from pools of green and blue mana, respectively. The Tome of Power is no longer an item scattered around the levels, but a defensive spell that still works in the same manner as the other games in the series by improving damage and granting weapons and offensive spells new abilities for a limited time. Melee combat is also more varied, with the ability to perform several attacks using Corvus' bladestaff and cut off the limbs of enemies, rendering them harmless. Players are also able to utilize magical shrines throughout the game that grant a variety of effects upon use, such as silver or gold armor, a temporary boost in health, a permanent enhancement to the bladestaff, etc.

The game consists of a wide variety of high fantasy medieval backdrops to Corvus's adventure. The third-person perspective and three-dimensional game environment allowed developers to introduce a wide variety of gymnastic moves, like climbing up ledges, back-flipping off walls, and pole vaulting, in a much more dynamic environment than the original game's engine could produce. Both games invite comparison with their respective game-engine namesake: the original "Heretic" was built on the "Doom" engine, and "Heretic II" was built using the "Quake II" engine, later known as id Tech 2. "Heretic II" was favorably received at release because it took a different approach to its design.

Inspired by the Tomb Raider series, Raven Software decided to make use of the Quake II engine to create a third person action game. A major step in the early development was Gerald Brom's concept art. In a month, the company had programmed the game's camera system. After Activision's approval of the game's demo, Raven Software aimed to get the full game finished by Christmas. (it would release just prior to that Thanksgiving) To add to complications, they needed a software renderer to make the game playable to 16-bit users (especially in Europe).

For the animation, the main character Corvus was provided with a backbone for realism and had a total of 1600 frames. Most of the animations were done using Softimage. The static world objects and simplified animations were done with 3D Studio Max. The engine was capable of showing up to 4,000 polygons on screen.

Following ZeniMax Media's acquisition of id Software in 2009, the rights to the series have been disputed between both id and Raven Software; Raven holds the development rights, while id holds the publishing rights to "Heretic II"'s predecessors. Until both companies come to an agreement, neither will be able to release another installment in the series.

"Heretic II" was a commercial flop. According to PC Data, its sales in the United States totaled 28,994 units by April 1999. Activision's Steve Felsen blamed this performance on the game's design: he noted that "fans of first-person shooters—the target audience for this game—stayed away due to the third-person perspective".

"Next Generation" reviewed the PC version of the game, rating it three stars out of five, and stated that ""Heretic II" has a lot going for it. It easily earns it space on the shelf with the heavy hitters this season, but it also serves as a reminder to all that every aspect of game design needs to be pushed if you want your project to truly stand out."

"Edge" praised the game for its mixture of platform and shoot 'em up action, stating that "Heretic II" is different enough to stand out from both first-person and third-person games like id Software's first-person shooters or Core Design's "Tomb Raider" games. "Heretic II" was a finalist for "Computer Gaming World"s 1998 "Best Action" award, which ultimately went to "Battlezone". The editors wrote that "Heretic II" "proved that the "Quake II" engine could work in a third-person game "and" that a spell-casting, shirtless elf could actually kick ass."



</doc>
<doc id="13615" url="https://en.wikipedia.org/wiki?curid=13615" title="Household hardware">
Household hardware

Household hardware (or simply, hardware) is equipment that can be touched or held by hand such as keys, locks, nuts, screws, washers, hinges, latches, handles, wire, chains, belts, plumbing supplies, electrical supplies, tools, utensils, cutlery and machine parts. Household hardware is typically sold in hardware stores.



</doc>
<doc id="13616" url="https://en.wikipedia.org/wiki?curid=13616" title="Howard Carter">
Howard Carter

Howard Carter (9 May 18742 March 1939) was a British archaeologist and Egyptologist who became world-famous after discovering the intact tomb (designated KV62) of the 18th Dynasty Pharaoh, Tutankhamun in November 1922.

Howard Carter was born in Kensington on 9 May 1874, the youngest child (of eleven) of artist and illustrator Samuel John Carter and Martha Joyce Sands. His father helped train and develop his artistic talents.

Carter spent much of his childhood with relatives in the Norfolk market town of Swaffham, the birthplace of both his parents. Receiving only limited formal education at Swaffham, he showed talent as an artist. The nearby mansion of the Amherst family, Didlington Hall, contained a sizable collection of Egyptian antiques, which sparked Carter's interest in that subject. Lady Amherst was impressed by his artistic skills, and in 1891 she prompted the Egypt Exploration Fund (EEF) to send Carter to assist an Amherst family friend, Percy Newberry, in the excavation and recording of Middle Kingdom tombs at Beni Hasan.

Although only 17, Carter was innovative in improving the methods of copying tomb decoration. In 1892, he worked under the tutelage of Flinders Petrie for one season at Amarna, the capital founded by the pharaoh Akhenaten. From 1894 to 1899, he worked with Édouard Naville at Deir el-Bahari, where he recorded the wall reliefs in the temple of Hatshepsut.

In 1899, Carter was appointed Inspector of Monuments for Upper Egypt in the Egyptian Antiquities Service (EAS). Based at Luxor, he oversaw a number of excavations and restorations at nearby Thebes, while in the Valley of the Kings he supervised the systematic exploration of the valley by the American archaeologist Theodore Davis. In 1904, after a dispute with local people over tomb thefts, he was transferred to the Inspectorate of Lower Egypt. Carter was praised for his improvements in the protection of, and accessibility to, existing excavation sites, and his development of a grid-block system for searching for tombs. The Antiquities Service also provided funding for Carter to head his own excavation projects.

Carter resigned from the Antiquities Service in 1905 after a formal inquiry into what became known as the Saqqara Affair, a violent confrontation between Egyptian site guards and a group of French tourists. Carter sided with the Egyptian personnel, refusing to apologise when the French authorities made an official complaint. Moving back to Luxor, Carter was without formal employment for nearly three years. He made a living by painting and selling watercolours to tourists and, in 1906, acting as a freelance draughtsman for Theodore Davis.

In 1907 Carter began to work for Lord Carnarvon, who employed him to supervise excavations of nobles' tombs in Deir el-Bahri, near Thebes. Gaston Maspero, head of the Egyptian Antiquities Service, had recommended Carter to Carnarvon as he knew he would apply modern archaeological methods and systems of recording.

In 1914, Lord Carnarvon received the concession to dig in the Valley of the Kings, Carter was again employed to lead the work. However excavations and study were soon interrupted by the First World War, Carter spending the war years working for the British Government as a diplomatic courier and translator. He enthusiastically resumed his excavation work towards the end of 1917.

By 1922, Lord Carnarvon had become dissatisfied with the lack of results after many years of finding little. After considering withdrawing his funding, Carnarvon agreed, after a discussion with Carter, that he would fund one more season of work in the Valley of the Kings.

Carter returned to the Valley of Kings, and investigated a line of huts that he had abandoned a few seasons earlier. The crew cleared the huts and rock debris beneath. On 4 November 1922, their young water boy accidentally stumbled on a stone that turned out to be the top of a flight of steps cut into the bedrock. Carter had the steps partially dug out until the top of a mud-plastered doorway was found. The doorway was stamped with indistinct cartouches (oval seals with hieroglyphic writing). Carter ordered the staircase to be refilled, and sent a telegram to Carnarvon, who arrived two-and-a-half weeks later on 23 November.

On 26 November 1922 Carter, with Carnarvon, his daughter Lady Evelyn Herbert and assistant Arthur Callender in attendance, made a "tiny breach in the top left-hand corner" of the doorway, using a chisel that his grandmother had given him for his 17th birthday. He was able to peer in by the light of a candle and see that many of the gold and ebony treasures were still in place. He did not yet know whether it was "a tomb or merely an old cache", but he did see a promising sealed doorway between two sentinel statues. Carnarvon asked, "Can you see anything?" Carter replied with the famous words: "Yes, wonderful things!" Carter had, in fact, discovered Tutankhamun's tomb (subsequently designated KV62).
Carter's notes and photographic evidence indicate that he, Lord Carnarvon, and Lady Evelyn Herbert entered the burial chamber in November 1922, before the official opening.

Realising the size and scope of the task ahead, Carter sought help from the Metropolitan Museum's excavation team, working nearby, who readily agreed to loan a number of their staff, including archaeological photographer Harry Burton. The next several months were spent cataloguing the contents of the antechamber under the "often stressful" supervision of Pierre Lacau, director general of the Department of Antiquities of Egypt. On 16 February 1923, Carter opened the sealed doorway and found that it did indeed lead to a burial chamber, and he got his first glimpse of the sarcophagus of Tutankhamun. The tomb was considered the best preserved and most intact pharaonic tomb ever found in the Valley of the Kings, and the discovery was eagerly covered by the world's press. However, much to the annoyance of other newspapers, Lord Carnarvon gave exclusive reporting rights to "The Times". Only H. V. Morton of that paper was allowed on the scene, and his vivid descriptions helped to cement Carter's reputation with the British public. 

Towards the end of February 1923, a rift between Lord Carnarvon and Carter, probably caused by a disagreement on how to manage the supervising Egyptian authorities, temporarily halted the excavation. Work recommenced in early March after Lord Carnarvon apologised to Carter. Later that month Lord Carnarvon contracted blood poisoning while staying in Luxor near the tomb site. He died in Cairo on 5 April 1923. Lady Carnarvon retained her late husband's concession in the Valley of the Kings, allowing Carter to continue his work.

Carter's meticulous assessing and cataloguing of the thousands of objects in the tomb took nearly ten years, most being moved to the Egyptian Museum in Cairo. There were several breaks in the work, including one lasting nearly a year in 1924–25, caused by a dispute over what Carter saw as excessive control of the excavation by the Egyptian Antiquities Service. The Egyptian authorities eventually agreed that Carter should complete the tomb's clearance. This continued until 1929, with some final work lasting until February 1932. 

Despite the significance of his archaeological find, Carter received no honour from the British government. However, in 1926, he received the Order of the Nile, third class, from King Fuad I of Egypt. He was also awarded an honorary degree of Doctor of Science by Yale University and honorary membership in the Real Academia de la Historia of Madrid, Spain.

Carter wrote a number of books on Egyptology during his career, including a three volume popular account of the discovery and excavation of Tutankhamun’s tomb. He also delivered a series of illustrated lectures on the excavation, including a 1924 tour of Britain, France, Spain and the United States. Those in New York City and other US cities were attended by large and enthusiastic audiences, sparking American Egyptomania.

The suggestion that Carter had an affair with Lady Evelyn Herbert, the daughter of the 5th Earl of Carnarvon, was later rejected by Lady Evelyn herself, who told her daughter Patricia that "at first I was in awe of him, later I was rather frightened of him", she resenting Carter's "determination" to come between her and her father. More recently, the 8th Earl dismissed the idea, describing Carter as a "stoical loner". Harold Plenderleith, a former associate of Carter's at the British Museum, was quoted as saying that he knew "something about Carter that was not fit to disclose", suggesting that Plenderleith believed that Carter was homosexual. There is, however, no sign that Carter enjoyed any close relationships throughout his life, and he never married nor had children.

After the clearance of the tomb had been completed in 1932 Carter retired from archaeology. He continued to live in his house near Luxor in winter and retained a flat in London but, as interest in Tutankhamun declined, he lived a fairly isolated existence with few close friends. He acted as a part-time agent for collectors and museums, including the Cleveland Museum of Art and the Detroit Institute of Arts.

Carter died at his London flat at 49 Albert Court, next to the Royal Albert Hall, on 2 March 1939, aged 64 from Hodgkin's Disease. He was buried in Putney Vale Cemetery in London on 6 March, nine people attending his funeral.

The epitaph on his gravestone reads: ""May your spirit live, may you spend millions of years, you who love Thebes, sitting with your face to the north wind, your eyes beholding happiness"", a quotation taken from the Wishing Cup of Tutankhamun, and ""O night, spread thy wings over me as the imperishable stars"."

Probate was granted on 5 July 1939 to Egyptologist Henry Burton and to publisher Bruce Sterling Ingram. Carter is described as Howard Carter of Luxor, Upper Egypt, Africa, and of 49 Albert Court, Kensington Grove, Kensington, London. His estate was valued at £2,002. The second grant of Probate was issued in Cairo on 1 September 1939. In his role as executor, Burton identified at least eighteen items in Carter’s antiquities collection that had been taken from Tutankhamun’s tomb without authorisation. As this was a sensitive matter that could affect Anglo-Egyptian relations, Burton sought wider advice, finally recommending that the items be discreetly presented or sold to the Metropolitan Museum of Art, with most eventually going either there or to the Egyptian Museum in Cairo. The Metropolitan Museum items were later returned to Egypt.
Carter's discovery of Tutankhamun's tomb revived popular interest in Ancient Egypt – 'Egyptomania' – and created “Tutmania”, which influenced popular song and fashion. Carter used this heightened interest to promote his books on the discovery and his lecture tours in Britain, America and Europe. While interest had waned by the mid 1930s, from the early 1970s touring exhibitions of the tomb's artefacts led to a sustained rise in popularity. This has been reflected in TV dramas, films and books, with Carter's quest and discovery of the tomb portrayed with varying levels of accuracy.
Carter has been portrayed or referenced to in many film, television and radio productions:





</doc>
<doc id="13617" url="https://en.wikipedia.org/wiki?curid=13617" title="History of Scotland">
History of Scotland

The recorded begins with the arrival of the Roman Empire in the 1st century, when the province of Britannia reached as far north as the Antonine Wall. North of this was Caledonia, inhabited by the "Picti", whose uprisings forced Rome's legions back to Hadrian's Wall. As Rome finally withdrew from Britain, Gaelic raiders called the "Scoti" began colonising Western Scotland and Wales. Prior to Roman times, prehistoric Scotland entered the Neolithic Era about 4000 BC, the Bronze Age about 2000 BC, and the Iron Age around 700 BC.

The Gaelic kingdom of Dál Riata was founded on the west coast of Scotland in the 6th century. In the following century, Irish missionaries introduced the previously pagan Picts to Celtic Christianity. Following England's Gregorian mission, the Pictish king Nechtan chose to abolish most Celtic practices in favour of the Roman rite, restricting Gaelic influence on his kingdom and avoiding war with Anglian Northumbria. Towards the end of the 8th century, the Viking invasions began, forcing the Picts and Gaels to cease their historic hostility to each other and to unite in the 9th century, forming the Kingdom of Scotland.

The Kingdom of Scotland was united under the House of Alpin, whose members fought among each other during frequent disputed successions. The last Alpin king, Malcolm II, died without a male issue in the early 11th century and the kingdom passed through his daughter's son to the House of Dunkeld or Canmore. The last Dunkeld king, Alexander III, died in 1286. He left only his infant granddaughter Margaret, Maid of Norway as heir, who died herself four years later. England, under Edward I, would take advantage of this questioned succession to launch a series of conquests, resulting in the Wars of Scottish Independence, as Scotland passed back and forth between the House of Balliol and the House of Bruce. Scotland's ultimate victory confirmed Scotland as a fully independent and sovereign kingdom.

When King David II died without issue, his nephew Robert II established the House of Stuart, which would rule Scotland uncontested for the next three centuries. James VI, Stuart king of Scotland, also inherited the throne of England in 1603, and the Stuart kings and queens ruled both independent kingdoms until the Acts of Union in 1707 merged the two kingdoms into a new state, the Kingdom of Great Britain. Ruling until 1714, Queen Anne was the last Stuart monarch. Since 1714, the succession of the British monarchs of the houses of Hanover and Saxe-Coburg and Gotha (Windsor) has been due to their descent from James VI and I of the House of Stuart.

During the Scottish Enlightenment and Industrial Revolution, Scotland became one of the commercial, intellectual and industrial powerhouses of Europe. Later, its industrial decline following the Second World War was particularly acute. In recent decades Scotland has enjoyed something of a cultural and economic renaissance, fuelled in part by a resurgent financial services sector and the proceeds of North Sea oil and gas. Since the 1950s, nationalism has become a strong political topic, with serious debates on Scottish independence, and a referendum in 2014 about leaving the British Union.

People lived in Scotland for at least 8,500 years before Britain's recorded history. At times during the last interglacial period (130,000–70,000 BC) Europe had a climate warmer than today's, and early humans may have made their way to Scotland, with the possible discovery of pre-Ice Age axes on Orkney and mainland Scotland. Glaciers then scoured their way across most of Britain, and only after the ice retreated did Scotland again become habitable, around 9600 BC. Upper Paleolithic hunter-gatherer encampments formed the first known settlements, and archaeologists have dated an encampment near Biggar to around 12000 BC. Numerous other sites found around Scotland build up a picture of highly mobile boat-using people making tools from bone, stone and antlers. The oldest house for which there is evidence in Britain is the oval structure of wooden posts found at South Queensferry near the Firth of Forth, dating from the Mesolithic period, about 8240 BC. The earliest stone structures are probably the three hearths found at Jura, dated to about 6000 BC.

Neolithic farming brought permanent settlements. Evidence of these includes the well-preserved stone house at Knap of Howar on Papa Westray, dating from around 3500 BC and the village of similar houses at Skara Brae on West Mainland, Orkney from about 500 years later. The settlers introduced chambered cairn tombs from around 3500 BC, as at Maeshowe, and from about 3000 BC the many standing stones and circles such as those at Stenness on the mainland of Orkney, which date from about 3100 BC, of four stones, the tallest of which is in height. These were part of a pattern that developed in many regions across Europe at about the same time.

The creation of cairns and Megalithic monuments continued into the Bronze Age, which began in Scotland about 2000 BC. As elsewhere in Europe, hill forts were first introduced in this period, including the occupation of Eildon Hill near Melrose in the Scottish Borders, from around 1000 BC, which accommodated several hundred houses on a fortified hilltop. From the Early and Middle Bronze Age there is evidence of cellular round houses of stone, as at Jarlshof and Sumburgh in Shetland. There is also evidence of the occupation of crannogs, roundhouses partially or entirely built on artificial islands, usually in lakes, rivers and estuarine waters.

In the early Iron Age, from the seventh century BC, cellular houses began to be replaced on the northern isles by simple Atlantic roundhouses, substantial circular buildings with a dry stone construction. From about 400 BC, more complex Atlantic roundhouses began to be built, as at Howe, Orkney and Crosskirk, Caithness. The most massive constructions that date from this era are the circular broch towers, probably dating from about 200 BC. This period also saw the first wheelhouses, a roundhouse with a characteristic outer wall, within which was a circle of stone piers (bearing a resemblance to the spokes of a wheel), but these would flourish most in the era of Roman occupation. There is evidence for about 1,000 Iron Age hill forts in Scotland, most located below the Clyde-Forth line, which have suggested to some archaeologists the emergence of a society of petty rulers and warrior elites recognisable from Roman accounts.

The surviving pre-Roman accounts of Scotland originated with the Greek Pytheas of Massalia, who may have circumnavigated the British Isles of Albion (Britain) and Ierne (Ireland) sometime around 325 BC. The most northerly point of Britain was called "Orcas" (Orkney). By the time of Pliny the Elder, who died in AD 79, Roman knowledge of the geography of Scotland had extended to the "Hebudes" (The Hebrides), "Dumna" (probably the Outer Hebrides), the Caledonian Forest and the people of the Caledonii, from whom the Romans named the region north of their control Caledonia. Ptolemy, possibly drawing on earlier sources of information as well as more contemporary accounts from the Agricolan invasion, identified 18 tribes in Scotland in his "Geography", but many of the names are obscure and the geography becomes less reliable in the north and west, suggesting early Roman knowledge of these areas was confined to observations from the sea.

The Roman invasion of Britain began in earnest in AD 43, leading to the establishment of the Roman province of Britannia in the south. By the year 71, the Roman governor Quintus Petillius Cerialis had launched an invasion of what is now Scotland. In the year 78, Gnaeus Julius Agricola arrived in Britain to take up his appointment as the new governor and began a series of major incursions. He is said to have pushed his armies to the estuary of the "River Taus" (usually assumed to be the River Tay) and established forts there, including a legionary fortress at Inchtuthil. After his victory over the northern tribes at Mons Graupius in 84, a series of forts and towers were established along the Gask Ridge, which marked the boundary between the Lowland and Highland zones, probably forming the first Roman "limes" or frontier in Scotland. Agricola's successors were unable or unwilling to further subdue the far north. By the year 87, the occupation was limited to the Southern Uplands and by the end of the first century the northern limit of Roman expansion was a line drawn between the Tyne and Solway Firth. The Romans eventually withdrew to a line in what is now northern England, building the fortification known as Hadrian's Wall from coast to coast.

Around 141, the Romans undertook a reoccupation of southern Scotland, moving up to construct a new "limes" between the Firth of Forth and the Firth of Clyde, which became the Antonine Wall. The largest Roman construction inside Scotland, it is a sward-covered wall made of turf around high, with nineteen forts. It extended for . Having taken twelve years to build, the wall was overrun and abandoned soon after 160. The Romans retreated to the line of Hadrian's Wall. Roman troops penetrated far into the north of modern Scotland several more times, with at least four major campaigns. The most notable invasion was in 209 when the emperor Septimius Severus led a major force north. After the death of Severus in 210 they withdrew south to Hadrian's Wall, which would be Roman frontier until it collapsed in the 5th century. By the close of the Roman occupation of southern and central Britain in the 5th century, the Picts had emerged as the dominant force in northern Scotland, with the various Brythonic tribes the Romans had first encountered there occupying the southern half of the country. Roman influence on Scottish culture and history was not enduring.

In the centuries after the departure of the Romans from Britain, there were four groups within the borders of what is now Scotland. In the east were the Picts, with kingdoms between the river Forth and Shetland. In the late 6th century the dominant force was the Kingdom of Fortriu, whose lands were centred on Strathearn and Menteith and who raided along the eastern coast into modern England. In the west were the Gaelic (Goidelic)-speaking people of Dál Riata with their royal fortress at Dunadd in Argyll, with close links with the island of Ireland, from whom comes the name Scots. In the south was the British (Brythonic) Kingdom of Strathclyde, descendants of the peoples of the Roman influenced kingdoms of "Hen Ogledd" (Old north), often named Alt Clut, the Brythonic name for their capital at Dumbarton Rock. Finally, there were the English or "Angles", Germanic invaders who had overrun much of southern Britain and held the Kingdom of Bernicia, in the south-east. The first English king in the historical record is Ida, who is said to have obtained the throne and the kingdom about 547. Ida's grandson, Æthelfrith, united his kingdom with Deira to the south to form Northumbria around the year 604. There were changes of dynasty, and the kingdom was divided, but it was re-united under Æthelfrith's son Oswald (r. 634-42).

Scotland was largely converted to Christianity by Irish-Scots missions associated with figures such as St Columba, from the fifth to the seventh centuries. These missions tended to found monastic institutions and collegiate churches that served large areas. Partly as a result of these factors, some scholars have identified a distinctive form of Celtic Christianity, in which abbots were more significant than bishops, attitudes to clerical celibacy were more relaxed and there was some significant differences in practice with Roman Christianity, particularly the form of tonsure and the method of calculating Easter, although most of these issues had been resolved by the mid-7th century.

Conversion to Christianity may have sped a long-term process of gaelicisation of the Pictish kingdoms, which adopted Gaelic language and customs. There was also a merger of the Gaelic and Pictish crowns, although historians debate whether it was a Pictish takeover of Dál Riata, or the other way around. This culminated in the rise of Cínaed mac Ailpín (Kenneth MacAlpin) in the 840s, which brought to power the House of Alpin. In 867 AD the Vikings seized the southern half of Northumbria, forming the Kingdom of York; three years later they stormed the Britons' fortress of Dumbarton and subsequently conquered much of England except for a reduced Kingdom of Wessex, leaving the new combined Pictish and Gaelic kingdom almost encircled. When he died as king of the combined kingdom in 900, Domnall II (Donald II) was the first man to be called "rí Alban" (i.e. "King of Alba"). The term Scotia was increasingly used to describe the kingdom between North of the Forth and Clyde and eventually the entire area controlled by its kings was referred to as Scotland.
The long reign (900–942/3) of Causantín (Constantine II) is often regarded as the key to formation of the Kingdom of Alba. He was later credited with bringing Scottish Christianity into conformity with the Catholic Church. After fighting many battles, his defeat at Brunanburh was followed by his retirement as a Culdee monk at St. Andrews. The period between the accession of his successor Máel Coluim I (Malcolm I) and Máel Coluim mac Cináeda (Malcolm II) was marked by good relations with the Wessex rulers of England, intense internal dynastic disunity and relatively successful expansionary policies. In 945, Máel Coluim I annexed Strathclyde as part of a deal with King Edmund of England, where the kings of Alba had probably exercised some authority since the later 9th century, an event offset somewhat by loss of control in Moray. The reign of King Donnchad I (Duncan I) from 1034 was marred by failed military adventures, and he was defeated and killed by MacBeth, the Mormaer of Moray, who became king in 1040. MacBeth ruled for seventeen years before he was overthrown by Máel Coluim, the son of Donnchad, who some months later defeated MacBeth's step-son and successor Lulach to become King Máel Coluim III (Malcolm III).

It was Máel Coluim III, who acquired the nickname "Canmore" ("Cenn Mór", "Great Chief"), which he passed to his successors and who did most to create the Dunkeld dynasty that ruled Scotland for the following two centuries. Particularly important was his second marriage to the Anglo-Hungarian princess Margaret. This marriage, and raids on northern England, prompted William the Conqueror to invade and Máel Coluim submitted to his authority, opening up Scotland to later claims of sovereignty by English kings. When Malcolm died in 1093, his brother Domnall III (Donald III) succeeded him. However, William II of England backed Máel Coluim's son by his first marriage, Donnchad, as a pretender to the throne and he seized power. His murder within a few months saw Domnall restored with one of Máel Coluim sons by his second marriage, Edmund, as his heir. The two ruled Scotland until two of Edmund's younger brothers returned from exile in England, again with English military backing. Victorious, Edgar, the oldest of the three, became king in 1097. Shortly afterwards Edgar and the King of Norway, Magnus Barefoot concluded a treaty recognising Norwegian authority over the Western Isles. In practice Norse control of the Isles was loose, with local chiefs enjoying a high degree of independence. He was succeeded by his brother Alexander, who reigned 1107–24.
When Alexander died in 1124, the crown passed to Margaret's fourth son David I, who had spent most of his life as a Norman French baron in England. His reign saw what has been characterised as a "Davidian Revolution", by which native institutions and personnel were replaced by English and French ones, underpinning the development of later Medieval Scotland. Members of the Anglo-Norman nobility took up places in the Scottish aristocracy and he introduced a system of feudal land tenure, which produced knight service, castles and an available body of heavily armed cavalry. He created an Anglo-Norman style of court, introduced the office of justicar to oversee justice, and local offices of sheriffs to administer localities. He established the first royal burghs in Scotland, granting rights to particular settlements, which led to the development of the first true Scottish towns and helped facilitate economic development as did the introduction of the first recorded Scottish coinage. He continued a process begun by his mother and brothers helping to establish foundations that brought reform to Scottish monasticism based on those at Cluny and he played a part in organising diocese on lines closer to those in the rest of Western Europe.

These reforms were pursued under his successors and grandchildren Malcolm IV of Scotland and William I, with the crown now passing down the main line of descent through primogeniture, leading to the first of a series of minorities. The benefits of greater authority were reaped by William's son Alexander II and his son Alexander III, who pursued a policy of peace with England to expand their authority in the Highlands and Islands. By the reign of Alexander III, the Scots were in a position to annexe the remainder of the western seaboard, which they did following Haakon Haakonarson's ill-fated invasion and the stalemate of the Battle of Largs with the Treaty of Perth in 1266.

The death of King Alexander III in 1286, and the death of his granddaughter and heir Margaret, Maid of Norway in 1290, left 14 rivals for succession. To prevent civil war the Scottish magnates asked Edward I of England to arbitrate, for which he extracted legal recognition that the realm of Scotland was held as a feudal dependency to the throne of England before choosing John Balliol, the man with the strongest claim, who became king in 1292. Robert Bruce, 5th Lord of Annandale, the next strongest claimant, accepted this outcome with reluctance. Over the next few years Edward I used the concessions he had gained to systematically undermine both the authority of King John and the independence of Scotland. In 1295, John, on the urgings of his chief councillors, entered into an alliance with France, known as the Auld Alliance.
In 1296, Edward invaded Scotland, deposing King John. The following year William Wallace and Andrew de Moray raised forces to resist the occupation and under their joint leadership an English army was defeated at the Battle of Stirling Bridge. For a short time Wallace ruled Scotland in the name of John Balliol as Guardian of the realm. Edward came north in person and defeated Wallace at the Battle of Falkirk in 1298. Wallace escaped but probably resigned as Guardian of Scotland. In 1305, he fell into the hands of the English, who executed him for treason despite the fact that he owed no allegiance to England.

Rivals John Comyn and Robert the Bruce, grandson of the claimant, were appointed as joint guardians in his place. On 10 February 1306, Bruce participated in the murder of Comyn, at Greyfriars Kirk in Dumfries. Less than seven weeks later, on 25 March, Bruce was crowned as King. However, Edward's forces overran the country after defeating Bruce's small army at the Battle of Methven. Despite the excommunication of Bruce and his followers by Pope Clement V, his support slowly strengthened; and by 1314 with the help of leading nobles such as Sir James Douglas and Thomas Randolph only the castles at Bothwell and Stirling remained under English control. Edward I had died in 1307. His heir Edward II moved an army north to break the siege of Stirling Castle and reassert control. Robert defeated that army at the Battle of Bannockburn in 1314, securing "de facto" independence. In 1320, the Declaration of Arbroath, a remonstrance to the Pope from the nobles of Scotland, helped convince Pope John XXII to overturn the earlier excommunication and nullify the various acts of submission by Scottish kings to English ones so that Scotland's sovereignty could be recognised by the major European dynasties. The Declaration has also been seen as one of the most important documents in the development of a Scottish national identity.

In 1326, what may have been the first full Parliament of Scotland met. The parliament had evolved from an earlier council of nobility and clergy, the "colloquium", constituted around 1235, but perhaps in 1326 representatives of the burghs – the burgh commissioners – joined them to form the Three Estates. In 1328, Edward III signed the Treaty of Edinburgh–Northampton acknowledging Scottish independence under the rule of Robert the Bruce. However, four years after Robert's death in 1329, England once more invaded on the pretext of restoring Edward Balliol, son of John Balliol, to the Scottish throne, thus starting the Second War of Independence. Despite victories at Dupplin Moor and Halidon Hill, in the face of tough Scottish resistance led by Sir Andrew Murray, the son of Wallace's comrade in arms, successive attempts to secure Balliol on the throne failed. Edward III lost interest in the fate of his protégé after the outbreak of the Hundred Years' War with France. In 1341, David II, King Robert's son and heir, was able to return from temporary exile in France. Balliol finally resigned his claim to the throne to Edward in 1356, before retiring to Yorkshire, where he died in 1364.

After David II's death, Robert II, the first of the Stewart kings, came to the throne in 1371. He was followed in 1390 by his ailing son John, who took the regnal name Robert III. During Robert III's reign (1390–1406), actual power rested largely in the hands of his brother, Robert Stewart, Duke of Albany. After the suspicious death (possibly on the orders of the Duke of Albany) of his elder son, David, Duke of Rothesay in 1402, Robert, fearful for the safety of his younger son, the future James I, sent him to France in 1406. However, the English captured him en route and he spent the next 18 years as a prisoner held for ransom. As a result, after the death of Robert III, regents ruled Scotland: first, the Duke of Albany; and later his son Murdoch. When Scotland finally paid the ransom in 1424, James, aged 32, returned with his English bride determined to assert this authority. Several of the Albany family were executed; but he succeeded in centralising control in the hands of the crown, at the cost of increasing unpopularity, and was assassinated in 1437. His son James II (reigned 1437–1460), when he came of age in 1449, continued his father's policy of weakening the great noble families, most notably taking on the powerful Black Douglas family that had come to prominence at the time of the Bruce.

In 1468, the last significant acquisition of Scottish territory occurred when James III was engaged to Margaret of Denmark, receiving the Orkney Islands and the Shetland Islands in payment of her dowry. Berwick upon Tweed was captured by England in 1482. With the death of James III in 1488 at the Battle of Sauchieburn, his successor James IV successfully ended the quasi-independent rule of the Lord of the Isles, bringing the Western Isles under effective Royal control for the first time. In 1503, he married Margaret Tudor, daughter of Henry VII of England, thus laying the foundation for the 17th-century Union of the Crowns.

Scotland advanced markedly in educational terms during the 15th century with the founding of the University of St Andrews in 1413, the University of Glasgow in 1450 and the University of Aberdeen in 1495, and with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools. James IV's reign is often considered to have seen a flowering of Scottish culture under the influence of the European Renaissance.
In 1512, the Auld Alliance was renewed and under its terms, when the French were attacked by the English under Henry VIII, James IV invaded England in support. The invasion was stopped decisively at the Battle of Flodden Field during which the King, many of his nobles, and a large number of ordinary troops were killed, commemorated by the song "Flowers of the Forest". Once again Scotland's government lay in the hands of regents in the name of the infant James V.

James V finally managed to escape from the custody of the regents in 1528. He continued his father's policy of subduing the rebellious Highlands, Western and Northern isles and the troublesome borders. He also continued the French alliance, marrying first the French noblewoman Madeleine of Valois and then after her death Marie of Guise. James V's domestic and foreign policy successes were overshadowed by another disastrous campaign against England that led to defeat at the Battle of Solway Moss (1542). James died a short time later, a demise blamed by contemporaries on "a broken heart". The day before his death, he was brought news of the birth of an heir: a daughter, who would become Mary, Queen of Scots.

Once again, Scotland was in the hands of a regent. Within two years, the Rough Wooing began, Henry VIII's military attempt to force a marriage between Mary and his son, Edward. This took the form of border skirmishing and several English campaigns into Scotland. In 1547, after the death of Henry VIII, forces under the English regent Edward Seymour, 1st Duke of Somerset were victorious at the Battle of Pinkie Cleugh, the climax of the Rough Wooing, and followed up by the occupation of Haddington. Mary was then sent to France at the age of five, as the intended bride of the heir to the French throne. Her mother, Marie de Guise, stayed in Scotland to look after the interests of Mary – and of France – although the Earl of Arran acted officially as regent. Guise responded by calling on French troops, who helped stiffen resistance to the English occupation. By 1550, after a change of regent in England, the English withdrew from Scotland completely.

From 1554, Marie de Guise, took over the regency, and continued to advance French interests in Scotland. French cultural influence resulted in a large influx of French vocabulary into Scots. But anti-French sentiment also grew, particularly among Protestants, who saw the English as their natural allies. In 1560, Marie de Guise died, and soon after the Auld Alliance also ended, with the signing of the Treaty of Edinburgh, which provided for the removal of French and English troops from Scotland. The Scottish Reformation took place only days later when the Scottish Parliament abolished the Roman Catholic religion and outlawed the Mass.
Meanwhile, Queen Mary had been raised as a Catholic in France, and married to the Dauphin, who became king as Francis II in 1559, making her queen consort of France. When Francis died in 1560, Mary, now 19, returned to Scotland to take up the government. Despite her private religion, she did not attempt to re-impose Catholicism on her largely Protestant subjects, thus angering the chief Catholic nobles. Her six-year personal reign was marred by a series of crises, largely caused by the intrigues and rivalries of the leading nobles. The murder of her secretary, David Riccio, was followed by that of her unpopular second husband Lord Darnley, and her abduction by and marriage to the Earl of Bothwell, who was implicated in Darnley's murder. Mary and Bothwell confronted the lords at Carberry Hill and after their forces melted away, he fled and she was captured by Bothwell's rivals. Mary was imprisoned in Loch Leven Castle, and in July 1567, was forced to abdicate in favour of her infant son James VI. Mary eventually escaped and attempted to regain the throne by force. After her defeat at the Battle of Langside in 1568, she took refuge in England, leaving her young son in the hands of regents. In Scotland the regents fought a civil war on behalf of James VI against his mother's supporters. In England, Mary became a focal point for Catholic conspirators and was eventually tried for treason and executed on the orders of her kinswoman Elizabeth I.

During the 16th century, Scotland underwent a Protestant Reformation that created a predominantly Calvinist national Kirk, which became Presbyterian in outlook and severely reduced the powers of bishops. In the earlier part of the century, the teachings of first Martin Luther and then John Calvin began to influence Scotland, particularly through Scottish scholars, often training for the priesthood, who had visited Continental universities. The Lutheran preacher Patrick Hamilton was executed for heresy in St. Andrews in 1528. The execution of others, especially the Zwingli-influenced George Wishart, who was burnt at the stake on the orders of Cardinal Beaton in 1546, angered Protestants. Wishart's supporters assassinated Beaton soon after and seized St. Andrews Castle, which they held for a year before they were defeated with the help of French forces. The survivors, including chaplain John Knox, were condemned to be galley slaves in France, stoking resentment of the French and creating martyrs for the Protestant cause.

Limited toleration and the influence of exiled Scots and Protestants in other countries, led to the expansion of Protestantism, with a group of lairds declaring themselves Lords of the Congregation in 1557 and representing their interests politically. The collapse of the French alliance and English intervention in 1560 meant that a relatively small, but highly influential, group of Protestants were in a position to impose reform on the Scottish church. A confession of faith, rejecting papal jurisdiction and the mass, was adopted by Parliament in 1560, while the young Mary, Queen of Scots, was still in France.

Knox, having escaped the galleys and spent time in Geneva as a follower of Calvin, emerged as the most significant figure of the period. The Calvinism of the reformers led by Knox resulted in a settlement that adopted a Presbyterian system and rejected most of the elaborate trappings of the medieval church. The reformed Kirk gave considerable power to local lairds, who often had control over the appointment of the clergy. There were widespread, but generally orderly outbreaks of iconoclasm. At this point the majority of the population was probably still Catholic in persuasion and the Kirk found it difficult to penetrate the Highlands and Islands, but began a gradual process of conversion and consolidation that, compared with reformations elsewhere, was conducted with relatively little persecution.

Women shared in the religiosity of the day. The egalitarian and emotional aspects of Calvinism appealed to men and women alike. Historian Alasdair Raffe finds that, "Men and women were thought equally likely to be among the elect...Godly men valued the prayers and conversation of their female co-religionists, and this reciprocity made for loving marriages and close friendships between men and women." Furthermore, there was an increasingly intense relationship in the pious bonds between minister and his women parishioners. For the first time, laywomen gained numerous new religious roles and took a prominent place in prayer societies.

In 1603, James VI King of Scots inherited the throne of the Kingdom of England, and became King James I of England, leaving Edinburgh for London, uniting England and Scotland under one monarch. The Union was a personal or dynastic union, with the Crowns remaining both distinct and separate—despite James's best efforts to create a new "imperial" throne of "Great Britain". The acquisition of the Irish crown along with the English, facilitated a process of settlement by Scots in what was historically the most troublesome area of the kingdom in Ulster, with perhaps 50,000 Scots settling in the province by the mid-17th century. James adopted a different approach to impose his authority in the western Highlands and Islands. The additional military resource that was now available, particularly the English navy, resulted in the enactment of the Statutes of Iona which compelled integration of Hebridean clan leaders with the rest of Scottish society. Attempts to found a Scottish colony in North America in Nova Scotia were largely unsuccessful, with insufficient funds and willing colonists.

Although James had tried to get the Scottish Church to accept some of the High Church Anglicanism of his southern kingdom, he met with limited success. His son and successor, Charles I, took matters further, introducing an English-style Prayer Book into the Scottish church in 1637. This resulted in anger and widespread rioting. (The story goes that it was initiated by a certain Jenny Geddes who threw a stool in St Giles Cathedral.) Representatives of various sections of Scottish society drew up the National Covenant in 1638, objecting to the King's liturgical innovations. In November of the same year matters were taken even further, when at a meeting of the General Assembly in Glasgow the Scottish bishops were formally expelled from the Church, which was then established on a full Presbyterian basis. Charles gathered a military force; but as neither side wished to push the matter to a full military conflict, a temporary settlement was concluded at Pacification of Berwick. Matters remained unresolved until 1640 when, in a renewal of hostilities, Charles's northern forces were defeated by the Scots at the Battle of Newburn to the west of Newcastle. During the course of these Bishops' Wars Charles tried to raise an army of Irish Catholics, but was forced to back down after a storm of protest in Scotland and England. The backlash from this venture provoked a rebellion in Ireland and Charles was forced to appeal to the English Parliament for funds. Parliament's demands for reform in England eventually resulted in the English Civil War. This series of civil wars that engulfed England, Ireland and Scotland in the 1640s and 1650s is known to modern historians as the Wars of the Three Kingdoms. The Covenanters meanwhile, were left governing Scotland, where they raised a large army of their own and tried to impose their religious settlement on Episcopalians and Roman Catholics in the north of the country. In England his religious policies caused similar resentment and he ruled without recourse to parliament from 1629.

As the civil wars developed, the English Parliamentarians appealed to the Scots Covenanters for military aid against the King. A Solemn League and Covenant was entered into, guaranteeing the Scottish Church settlement and promising further reform in England. Scottish troops played a major part in the defeat of Charles I, notably at the battle of Marston Moor. An army under the Earl of Leven occupied the North of England for some time.

However, not all Scots supported the Covenanter's taking arms against their King. In 1644, James Graham, 1st Marquess of Montrose attempted to raise the Highlands for the King. Few Scots would follow him, but, aided by 1,000 Irish, Highland and Islesmen troops sent by the Irish Confederates under Alasdair MacDonald (MacColla), and an instinctive genius for mobile warfare, he was stunningly successful. A Scottish Civil War began in September 1644 with his victory at battle of Tippermuir. After a series of victories over poorly trained Covenanter militias, the lowlands were at his mercy. However, at this high point, his army was reduced in size, as MacColla and the Highlanders preferred to continue the war in the north against the Campbells. Shortly after, what was left of his force was defeated at the Battle of Philiphaugh. Escaping to the north, Montrose attempted to continue the struggle with fresh troops; but in July 1646 his army was disbanded after the King surrendered to the Scots army at Newark, and the civil war came to an end.

The following year Charles, while he was being held captive in Carisbrooke Castle, entered into an agreement with moderate Scots Presbyterians. In this secret 'Engagement', the Scots promised military aid in return for the King's agreement to implement Presbyterianism in England on a three-year trial basis. The Duke of Hamilton led an invasion of England to free the King, but he was defeated by Oliver Cromwell in August 1648 at the Battle of Preston.

The execution of Charles I in 1649 was carried out in the face of objections by the Covenanter government and his son was immediately proclaimed as King Charles II in Edinburgh. Oliver Cromwell led an invasion of Scotland in 1650, and defeated the Scottish army at Dunbar and then defeated a Scottish invasion of England at Worcester on 3 September 1651 (the anniversary of his victory at Dunbar). Cromwell emerged as the leading figure in the English government and Scotland was occupied by an English force under George Monck. The country was incorporated into the Puritan-governed Commonwealth and lost its independent church government, parliament and legal system, but gained access to English markets. Various attempts were made to legitimise the union, calling representatives from the Scottish burghs and shires to negotiations and to various English parliaments, where they were always under-represented and had little opportunity for dissent. However, final ratification was delayed by Cromwell's problems with his various parliaments and the union did not become the subject of an act until 1657 (see Tender of Union).

After the death of Cromwell and the regime's collapse, Charles II was restored in 1660 and Scotland again became an independent kingdom. Scotland regained its system of law, parliament and kirk, but also the Lords of the Articles (by which the crown managed parliament), bishops and a king who did not visit the country. He ruled largely without reference to Parliament, through a series of commissioners. These began with John, Earl of Middleton and ended with the king's brother and heir, James, Duke of York (known in Scotland as the Duke of Albany). The English Navigation Acts prevented the Scots engaging in what would have been lucrative trading with England's colonies. The restoration of episcopacy was a source of trouble, particularly in the south-west of the country, an area with strong Presbyterian sympathies. Abandoning the official church, many of the inhabitants began to attend illegal field assemblies, known as conventicles. Official attempts to suppress these led to a rising in 1679, defeated by James, Duke of Monmouth, the King's illegitimate son, at the Battle of Bothwell Bridge. In the early 1680s a more intense phase of persecution began, later to be called "the Killing Time". When Charles died in 1685 and his brother, a Roman Catholic, succeeded him as James VII of Scotland (and II of England), matters came to a head.

James put Catholics in key positions in the government and attendance at conventicles was made punishable by death. He disregarded parliament, purged the Council and forced through religious toleration to Roman Catholics, alienating his Protestant subjects. It was believed that the king would be succeeded by his daughter Mary, a Protestant and the wife of William of Orange, Stadtholder of the Netherlands, but when in 1688, James produced a male heir, James Francis Edward Stuart, it was clear that his policies would outlive him. An invitation by seven leading Englishmen led William to land in England with 40,000 men, and James fled, leading to the almost bloodless "Glorious Revolution". The Estates issued a "Claim of Right" that suggested that James had forfeited the crown by his actions (in contrast to England, which relied on the legal fiction of an abdication) and offered it to William and Mary, which William accepted, along with limitations on royal power. The final settlement restored Presbyterianism and abolished the bishops who had generally supported James. However, William, who was more tolerant than the Kirk tended to be, passed acts restoring the Episcopalian clergy excluded after the Revolution.

Although William's supporters dominated the government, there remained a significant following for James, particularly in the Highlands. His cause, which became known as Jacobitism, from the Latin "(Jacobus)" for James, led to a series of risings. An initial Jacobite military attempt was led by John Graham, Viscount Dundee. His forces, almost all Highlanders, defeated William's forces at the Battle of Killiecrankie in 1689, but they took heavy losses and Dundee was slain in the fighting. Without his leadership the Jacobite army was soon defeated at the Battle of Dunkeld. In the aftermath of the Jacobite defeat on 13 February 1692, in an incident since known as the Massacre of Glencoe, 38 members of the Clan MacDonald of Glencoe were killed by members of the Earl of Argyll's Regiment of Foot, on the grounds that they had not been prompt in pledging allegiance to the new monarchs.

The closing decade of the 17th century saw the generally favourable economic conditions that had dominated since the Restoration come to an end. There was a slump in trade with the Baltic and France from 1689 to 1691, caused by French protectionism and changes in the Scottish cattle trade, followed by four years of failed harvests (1695, 1696 and 1698–9), an era known as the "seven ill years". The result was severe famine and depopulation, particularly in the north. The Parliament of Scotland of 1695 enacted proposals to help the desperate economic situation, including setting up the Bank of Scotland. The "Company of Scotland Trading to Africa and the Indies" received a charter to raise capital through public subscription.

With the dream of building a lucrative overseas colony for Scotland, the Company of Scotland invested in the Darien scheme, an ambitious plan devised by William Paterson to establish a colony on the Isthmus of Panama in the hope of establishing trade with the Far East. The Darién scheme won widespread support in Scotland as the landed gentry and the merchant class were in agreement in seeing overseas trade and colonialism as routes to upgrade Scotland's economy. Since the capital resources of the Edinburgh merchants and landholder elite were insufficient, the company appealed to middling social ranks, who responded with patriotic fervour to the call for money; the lower classes volunteered as colonists. But the English government opposed the idea: involved in the War of the Grand Alliance from 1689 to 1697 against France, it did not want to offend Spain, which claimed the territory as part of New Granada. The English investors withdrew. Returning to Edinburgh, the Company raised 400,000 pounds in a few weeks. Three small fleets with a total of 3,000 men eventually set out for Panama in 1698. The exercise proved a disaster. Poorly equipped; beset by incessant rain; under attack by the Spanish from nearby Cartagena; and refused aid by the English in the West Indies, the colonists abandoned their project in 1700. Only 1,000 survived and only one ship managed to return to Scotland.

Scotland was a poor rural, agricultural society with a population of 1.3 million in 1755. Although Scotland lost home rule, the Union allowed it to break free of a stultifying system and opened the way for the Scottish enlightenment as well as a great expansion of trade and increase in opportunity and wealth. Edinburgh economist Adam Smith concluded in 1776 that "By the union with England, the middling and inferior ranks of people in Scotland gained a complete deliverance from the power of an aristocracy which had always before oppressed them." Historian Jonathan Israel holds that the Union "proved a decisive catalyst politically and economically," by allowing ambitious Scots entry on an equal basis to a rich expanding empire and its increasing trade.

Scotland's transformation into a rich leader of modern industry came suddenly and unexpectedly in the next 150 years, following its union with England in 1707 and its integration with the advanced English and imperial economies. The transformation was led by two cities that grew rapidly after 1770. Glasgow, on the river Clyde, was the base for the tobacco and sugar trade with an emerging textile industry. Edinburgh was the administrative and intellectual centre where the Scottish Enlightenment was chiefly based.

By the start of the 18th century, a political union between Scotland and England became politically and economically attractive, promising to open up the much larger markets of England, as well as those of the growing English Empire. With economic stagnation since the late 17th century, which was particularly acute in 1704, the country depended more and more heavily on sales of cattle and linen to England, who used this to create pressure for a union. The Scottish parliament voted on 6 January 1707, by 110 to 69, to adopt the Treaty of Union. It was also a full economic union; indeed, most of its 25 articles dealt with economic arrangements for the new state known as "Great Britain". It added 45 Scots to the 513 members of the House of Commons and 16 Scots to the 190 members of the House of Lords, and ended the Scottish parliament. It also replaced the Scottish systems of currency, taxation and laws regulating trade with laws made in London. Scottish law remained separate from English law, and the religious system was not changed. England had about five times the population of Scotland at the time, and about 36 times as much wealth.

Jacobitism was revived by the unpopularity of the union. In 1708, James Francis Edward Stuart, the son of James VII, who became known as "The Old Pretender", attempted an invasion with a French fleet carrying 6,000 men, but the Royal Navy prevented it from landing troops. A more serious attempt occurred in 1715, soon after the death of Anne and the accession of the first Hanoverian king, the eldest son of Sophie, as George I of Great Britain. This rising (known as "The 'Fifteen") envisaged simultaneous uprisings in Wales, Devon, and Scotland. However, government arrests forestalled the southern ventures. In Scotland, John Erskine, Earl of Mar, nicknamed "Bobbin' John", raised the Jacobite clans but proved to be an indecisive leader and an incompetent soldier. Mar captured Perth, but let a smaller government force under the Duke of Argyll hold the Stirling plain. Part of Mar's army joined up with risings in northern England and southern Scotland, and the Jacobites fought their way into England before being defeated at the Battle of Preston, surrendering on 14 November 1715. The day before, Mar had failed to defeat Argyll at the Battle of Sheriffmuir. At this point, James belatedly landed in Scotland, but was advised that the cause was hopeless. He fled back to France. An attempted Jacobite invasion with Spanish assistance in 1719 met with little support from the clans and ended in defeat at the Battle of Glen Shiel.

In 1745, the Jacobite rising known as "The 'Forty-Five" began. Charles Edward Stuart, son of the "Old Pretender", often referred to as "Bonnie Prince Charlie" or the "Young Pretender", landed on the island of Eriskay in the Outer Hebrides. Several clans unenthusiastically joined him. At the outset he was successful, taking Edinburgh and then defeating the only government army in Scotland at the Battle of Prestonpans. The Jacobite army marched into England, took Carlisle and advanced as far as south as Derby. However, it became increasingly evident that England would not support a Roman Catholic Stuart monarch. The Jacobite leadership had a crisis of confidence and they retreated to Scotland as two English armies closed in and Hanoverian troops began to return from the continent. Charles' position in Scotland began to deteriorate as the Whig supporters rallied and regained control of Edinburgh. After an unsuccessful attempt on Stirling, he retreated north towards Inverness. He was pursued by the Duke of Cumberland and gave battle with an exhausted army at Culloden on 16 April 1746, where the Jacobite cause was crushed. Charles hid in Scotland with the aid of Highlanders until September 1746, when he escaped back to France. There were bloody reprisals against his supporters and foreign powers abandoned the Jacobite cause, with the court in exile forced to leave France. The Old Pretender died in 1760 and the Young Pretender, without legitimate issue, in 1788. When his brother, Henry, Cardinal of York, died in 1807, the Jacobite cause was at an end.

With the advent of the Union and the demise of Jacobitism, access to London and the Empire opened up very attractive career opportunities for ambitious middle-class and upper-class Scots, who seized the chance to become entrepreneurs, intellectuals, and soldiers. Thousands of Scots, mainly Lowlanders, took up positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes that "after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland". Davidson also states that "far from being ‘peripheral’ to the British economy, Scotland – or more precisely, the Lowlands – lay at its core". British officials especially appreciated Scottish soldiers. As the Secretary of War told Parliament in 1751, "I am for having always in our army as many Scottish soldiers as possible...because they are generally more hardy and less mutinous". The national policy of aggressively recruiting Scots for senior civilian positions stirred up resentment among Englishmen, ranging from violent diatribes by John Wilkes, to vulgar jokes and obscene cartoons in the popular press, and the haughty ridicule by intellectuals such as Samuel Johnson that was much resented by Scots. In his great "Dictionary" Johnson defined oats as, "a grain, which in England is generally given to horses, but in Scotland supports the people." To which Lord Elibank retorted, "Very true, and where will you find such men and such horses?"

Scottish politics in the late 18th century was dominated by the Whigs, with the benign management of Archibald Campbell, 3rd Duke of Argyll (1682–1761), who was in effect the "viceroy of Scotland" from the 1720s until his death in 1761. Scotland generally supported the king with enthusiasm during the American Revolution. Henry Dundas (1742–1811) dominated political affairs in the latter part of the century. Dundas put a brake on intellectual and social change through his ruthless manipulation of patronage in alliance with Prime Minister William Pitt the Younger, until he lost power in 1806.

The main unit of local government was the parish, and since it was also part of the church, the elders imposed public humiliation for what the locals considered immoral behaviour, including fornication, drunkenness, wife beating, cursing and Sabbath breaking. The main focus was on the poor and the landlords ("lairds") and gentry, and their servants, were not subject to the parish's control. The policing system weakened after 1800 and disappeared in most places by the 1850s.

The clan system of the Highlands and Islands had been seen as a challenge to the rulers of Scotland from before the 17th century. James VI's various measures to exert control included the Statutes of Iona, an attempt to force clan leaders to become integrated into the rest of Scottish society. This started a slow process of change which, by the second half of the 18th century, saw clan chiefs start to think of themselves as commercial landlords, rather than as patriarchs of their people. To their tenants, initially this meant that monetary rents replaced those paid in kind. Later, rent increases became common. In the 1710s the Dukes of Argyll started putting leases of some of their land up for auction; by 1737 this was done across the Argyll property. This commercial attitude replaced the principle of "", which included the obligation on clan chiefs to provide land for clan members. The shift of this attitude slowly spread through the Highland elite (but not among their tenants). As clan chiefs became more integrated into Scottish and British society, many of them built up large debts. It became easier to borrow against the security of a Highland estate from the 1770s onwards. As the lenders became predominantly people and organisations outside the Highlands, there was a greater willingness to foreclose if the borrower defaulted. Combined with an astounding level of financial incompetence among the Highland elite, this ultimately forced the sale of the estates of many Highland landed families over the period 1770–1850. (The greatest number of sales of whole estates was toward the end of this period.)

The Jacobite rebellion of 1745 gave a final period of importance to the ability of Highland clans to raise bodies of fighting men at short notice. With the defeat at Culloden, any enthusiasm for continued warfare disappeared and clan leaders returned to their transition to being commercial landlords. This was arguably accelerated by some of the punitive laws enacted after the rebellion. These included the Heritable Jurisdictions Act of 1746, which removed judicial roles from clan chiefs and gave them to the Scottish law courts. T. M. Devine warns against seeing a clear cause and effect relationship between the post-Culloden legislation and the collapse of clanship. He questions the basic effectiveness of the measures, quoting W. A. Speck who ascribes the pacification of the area more to "a disinclination to rebel than to the government's repressive measures." Devine points out that social change in Gaeldom did not pick up until the 1760s and 1770s, as this coincided with the increased market pressures from the industrialising and urbanising Lowlands.

41 properties belonging to rebels were forfeited to the Crown in the aftermath of the '45. The vast majority of these were sold by auction to pay creditors. 13 were retained and managed on behalf of the government between 1752 and 1784.

The changes by the Dukes of Argyll in the 1730s displaced many of the tacksmen in the area. From the 1770s onwards, this became a matter of policy throughout the Highlands. The restriction on subletting by tacksmen meant that landlords received all the rent paid by the actual farming tenants – thereby increasing their income. By the early part of the 19th century, the tacksman had become a rare component of Highland society. T. M. Devine describes "the displacement of this class as one of the clearest demonstrations of the death of the old Gaelic society." Many emigrated, leading parties of their tenants to North America. These tenants were from the better off part of Highland peasant society, and, together with the tacksmen, they took their capital and entrepreneurial energy to the New World, unwilling to participate in economic changes imposed by their landlords which often involved a loss of status for the tenant.

Agricultural improvement was introduced across the Highlands over the relatively short period of 1760–1850. The evictions involved in this became known as the Highland clearances. There was regional variation. In the east and south of the Highlands, the old townships or "", which were farmed under the run rig system were replaced by larger enclosed farms, with fewer people holding leases and proportionately more of the population working as employees on these larger farms. (This was broadly similar to the situation in the Lowlands.) In the north and west, including the Hebrides, as land was taken out of run rig, Crofting communities were established. Much of this change involved establishing large pastoral sheep farms, with the old displaced tenants moving to new crofts in coastal areas or on poor quality land. Sheep farming was increasingly profitable at the end of the 18th century, so could pay substantially higher rents than the previous tenants. Particularly in the Hebrides, some crofting communities were established to work in the kelp industry. Others were engaged in fishing. Croft sizes were kept small, so that the occupiers were forced to seek employment to supplement what they could grow. This increased the number of seasonal migrant workers travelling to the Lowlands. The resulting connection with the Lowlands was highly influential on all aspects of Highland life, touching on income levels, social attitudes and language. Migrant working gave an advantage in speaking English, which came to be considered "the language of work".

In 1846 the Highland potato famine struck the crofting communities of the North and West Highlands. By 1850 the charitable relief effort was wound up, despite the continuing crop failure, and landlords, charities and the government resorted to encouraging emigration. The overall result was that almost 11,000 people were provided with "assisted passages" by their landlords between 1846 and 1856, with the greatest number travelling in 1851. A further 5,000 emigrated to Australia, through the Highland and Island Emigration Society. To this should be added an unknown, but significant number, who paid their own fares to emigrate, and a further unknown number assisted by the Colonial Land and Emigration commission. This was out of a famine-affected population of about 200,000 people. Many of those who remained became even more involved in temporary migration for work in the Lowlands, both out of necessity during the famine and having become accustomed to working away by the time the famine ceased. Much longer periods were spent out of the Highlands – often for much of the year or more. One illustration of this migrant working was the estimated 30,000 men and women from the far west of the Gaelic speaking area who travelled to the east coast fishing ports for the herring fishing season – providing labour in an industry that grew by 60% between 1854 and 1884.

The clearances were followed by a period of even greater emigration from the Highlands, which continued (with a brief lull for the First World War) up to the start of the Great Depression.

Historian Jonathan Israel argues that by 1750 Scotland's major cities had created an intellectual infrastructure of mutually supporting institutions, such as universities, reading societies, libraries, periodicals, museums and masonic lodges. The Scottish network was "predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment ." In France Voltaire said "we look to Scotland for all our ideas of civilization," and the Scots in turn paid close attention to French ideas. Historian Bruce Lenman says their "central achievement was a new capacity to recognize and interpret social patterns." The first major philosopher of the Scottish Enlightenment was Francis Hutcheson, who held the Chair of Philosophy at the University of Glasgow from 1729 to 1746. A moral philosopher who produced alternatives to the ideas of Thomas Hobbes, one of his major contributions to world thought was the utilitarian and consequentialist principle that virtue is that which provides, in his words, "the greatest happiness for the greatest numbers". Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. He and other Scottish Enlightenment thinkers developed what he called a 'science of man', which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behave in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement and Hume's philosophical concepts that directly influenced James Madison (and thus the US Constitution) and when popularised by Dugald Stewart, would be the basis of classical liberalism. Adam Smith published "The Wealth of Nations", often considered the first work on modern economics. It had an immediate impact on British economic policy and in the 21st century still framed discussions on globalisation and tariffs. The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of the physician and chemist William Cullen, the agriculturalist and economist James Anderson, chemist and physician Joseph Black, natural historian John Walker and James Hutton, the first modern geologist.

With tariffs with England now abolished, the potential for trade for Scottish merchants was considerable. However, Scotland in 1750 was still a poor rural, agricultural society with a population of 1.3 million. Some progress was visible: agriculture in the Lowlands was steadily upgraded after 1700 and standards remained high. There were the sales of linen and cattle to England, the cash flows from military service, and the tobacco trade that was dominated by Glasgow Tobacco Lords after 1740. Merchants who profited from the American trade began investing in leather, textiles, iron, coal, sugar, rope, sailcloth, glassworks, breweries, and soapworks, setting the foundations for the city's emergence as a leading industrial centre after 1815. The tobacco trade collapsed during the American Revolution (1776–83), when its sources were cut off by the British blockade of American ports. However, trade with the West Indies began to make up for the loss of the tobacco business, reflecting the British demand for sugar and the demand in the West Indies for herring and linen goods.

Linen was Scotland's premier industry in the 18th century and formed the basis for the later cotton, jute, and woollen industries. Scottish industrial policy was made by the Board of Trustees for Fisheries and Manufactures in Scotland, which sought to build an economy complementary, not competitive, with England. Since England had woollens, this meant linen. Encouraged and subsidised by the Board of Trustees so it could compete with German products, merchant entrepreneurs became dominant in all stages of linen manufacturing and built up the market share of Scottish linens, especially in the American colonial market. The British Linen Company, established in 1746, was the largest firm in the Scottish linen industry in the 18th century, exporting linen to England and America. As a joint-stock company, it had the right to raise funds through the issue of promissory notes or bonds. With its bonds functioning as bank notes, the company gradually moved into the business of lending and discounting to other linen manufacturers, and in the early 1770s banking became its main activity. It joined the established Scottish banks such as the Bank of Scotland (Edinburgh, 1695) and the Royal Bank of Scotland (Edinburgh, 1727). Glasgow would soon follow and Scotland had a flourishing financial system by the end of the century. There were over 400 branches, amounting to one office per 7,000 people, double the level in England, where banks were also more heavily regulated. Historians have emphasised that the flexibility and dynamism of the Scottish banking system contributed significantly to the rapid development of the economy in the 19th century.

German sociologist Max Weber mentioned Scottish Presbyterianism in The Protestant Ethic and the Spirit of Capitalism (1905), and many scholars argued that "this worldly asceticism" of Calvinism was integral to Scotland's rapid economic modernisation. More recent scholarship however emphasises other factors. These include technology transfers from England and the appeal of a highly mobile, low-cost labour-force for English investors like Richard Arkwright. Scotland's natural resources in water power, black-band ironstone and coal were also important foundations for mechanised industry. 

In the 1690s the Presbyterian establishment purged the land of Episcopalians and heretics, and made blasphemy a capital crime. Thomas Aitkenhead, the son of an Edinburgh surgeon, aged 18, was indicted for blasphemy by order of the Privy Council for calling the New Testament "The History of the Imposter Christ"; he was hanged in 1696. Their extremism led to a reaction known as the "Moderate" cause that ultimately prevailed and opened the way for liberal thinking in the cities.

The early 18th century saw the beginnings of a fragmentation of the Church of Scotland. These fractures were prompted by issues of government and patronage, but reflected a wider division between the hard-line Evangelicals and the theologically more tolerant Moderate Party. The battle was over fears of fanaticism by the former and the promotion of Enlightenment ideas by the latter. The Patronage Act of 1712 was a major blow to the evangelicals, for it meant that local landlords could choose the minister, not the members of the congregation. Schisms erupted as the evangelicals left the main body, starting in 1733 with the First Secession headed by figures including Ebenezer Erskine. The second schism in 1761 lead to the foundation of the independent Relief Church. These churches gained strength in the Evangelical Revival of the later 18th century. A key result was the main Presbyterian church was in the hands of the Moderate faction, which provided critical support for the Enlightenment in the cities.

Long after the triumph of the Church of Scotland in the Lowlands, Highlanders and Islanders clung to an old-fashioned Christianity infused with animistic folk beliefs and practices. The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw some success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society. Catholicism had been reduced to the fringes of the country, particularly the Gaelic-speaking areas of the Highlands and Islands. Conditions also grew worse for Catholics after the Jacobite rebellions and Catholicism was reduced to little more than a poorly run mission. Also important was Episcopalianism, which had retained supporters through the civil wars and changes of regime in the 17th century. Since most Episcopalians had given their support to the Jacobite rebellions in the early 18th century, they also suffered a decline in fortunes.

Although Scotland increasingly adopted the English language and wider cultural norms, its literature developed a distinct national identity and began to enjoy an international reputation. Allan Ramsay (1686–1758) laid the foundations of a reawakening of interest in older Scottish literature, as well as leading the trend for pastoral poetry, helping to develop the Habbie stanza as a poetic form. James Macpherson was the first Scottish poet to gain an international reputation, claiming to have found poetry written by Ossian, he published translations that acquired international popularity, being proclaimed as a Celtic equivalent of the Classical epics. "Fingal" written in 1762 was speedily translated into many European languages, and its deep appreciation of natural beauty and the melancholy tenderness of its treatment of the ancient legend did more than any single work to bring about the Romantic movement in European, and especially in German, literature, influencing Herder and Goethe. Eventually it became clear that the poems were not direct translations from the Gaelic, but flowery adaptations made to suit the aesthetic expectations of his audience. Both the major literary figures of the following century, Robert Burns and Walter Scott, would be highly influenced by the Ossian cycle. Burns, an Ayrshire poet and lyricist, is widely regarded as the national poet of Scotland and a major figure in the Romantic movement. As well as making original compositions, Burns also collected folk songs from across Scotland, often revising or adapting them. His poem (and song) "Auld Lang Syne" is often sung at Hogmanay (the last day of the year), and "Scots Wha Hae" served for a long time as an unofficial national anthem of the country.

A legacy of the Reformation in Scotland was the aim of having a school in every parish, which was underlined by an act of the Scottish parliament in 1696 (reinforced in 1801). In rural communities this obliged local landowners (heritors) to provide a schoolhouse and pay a schoolmaster, while ministers and local presbyteries oversaw the quality of the education. The headmaster or "dominie" was often university educated and enjoyed high local prestige. The kirk schools were active in the rural lowlands but played a minor role in the Highlands, the islands, and in the fast-growing industrial towns and cities. The schools taught in English, not in Gaelic, because that language was seen as a leftover of Catholicism and was not an expression of Scottish nationalism. In cities such as Glasgow the Catholics operated their own schools, which directed their youth into clerical and middle class occupations, as well as religious vocations.

A "democratic myth" emerged in the 19th century to the effect that many a "lad of pairts" had been able to rise up through the system to take high office and that literacy was much more widespread in Scotland than in neighbouring states, particularly England. Historical research has largely undermined the myth. Kirk schools were not free, attendance was not compulsory and they generally imparted only basic literacy such as the ability to read the Bible. Poor children, starting at age 7, were done by age 8 or 9; the majority were finished by age 11 or 12. The result was widespread basic reading ability; since there was an extra fee for writing, half the people never learned to write. Scots were not significantly better educated than the English and other contemporary nations. A few talented poor boys did go to university, but usually they were helped by aristocratic or gentry sponsors. Most of them became poorly paid teachers or ministers, and none became important figures in the Scottish Enlightenment or the Industrial Revolution.

By the 18th century there were five universities in Scotland, at Edinburgh, Glasgow, St. Andrews and King's and Marischial Colleges in Aberdeen, compared with only two in England. Originally oriented to clerical and legal training, after the religious and political upheavals of the 17th century they recovered with a lecture-based curriculum that was able to embrace economics and science, offering a high quality liberal education to the sons of the nobility and gentry. It helped the universities to become major centres of medical education and to put Scotland at the forefront of Enlightenment thinking.

Scotland's transformation into a rich leader of modern industry came suddenly and unexpectedly. The population grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. The economy, long based on agriculture, began to industrialise after 1790. At first the leading industry, based in the west, was the spinning and weaving of cotton. In 1861, the American Civil War suddenly cut off the supplies of raw cotton and the industry never recovered. Thanks to its many entrepreneurs and engineers, and its large stock of easily mined coal, Scotland became a world centre for engineering, shipbuilding, and locomotive construction, with steel replacing iron after 1870.

The Scottish Reform Act 1832 increased the number of Scottish MPs and significantly widened the franchise to include more of the middle classes. From this point until the end of the century, the Whigs and (after 1859) their successors the Liberal Party, managed to gain a majority of the Westminster Parliamentary seats for Scotland, although these were often outnumbered by the much larger number of English and Welsh Conservatives. The English-educated Scottish peer Lord Aberdeen (1784–1860) led a coalition government from 1852–5, but in general very few Scots held office in the government. From the mid-century there were increasing calls for Home Rule for Scotland and when the Conservative Lord Salisbury became prime minister in 1885 he responded to pressure by reviving the post of Secretary of State for Scotland, which had been in abeyance since 1746. He appointed the Duke of Richmond, a wealthy landowner who was both Chancellor of Aberdeen University and Lord Lieutenant of Banff. Towards the end of the century Prime Ministers of Scottish descent included the Tory, Peelite and Liberal William Gladstone, who held the office four times between 1868 and 1894. The first Scottish Liberal to become prime minister was the Earl of Rosebery, from 1894 to 1895, like Aberdeen before him a product of the English education system. In the later 19th century the issue of Irish Home Rule led to a split among the Liberals, with a minority breaking away to form the Liberal Unionists in 1886. The growing importance of the working classes was marked by Keir Hardie's success in the 1888 Mid Lanarkshire by-election, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.

From about 1790 textiles became the most important industry in the west of Scotland, especially the spinning and weaving of cotton, which flourished until in 1861 the American Civil War cut off the supplies of raw cotton. The industry never recovered, but by that time Scotland had developed heavy industries based on its coal and iron resources. The invention of the hot blast for smelting iron (1828) revolutionised the Scottish iron industry. As a result, Scotland became a centre for engineering, shipbuilding and the production of locomotives. Toward the end of the 19th century, steel production largely replaced iron production. Coal mining continued to grow into the 20th century, producing the fuel to heat homes, factories and drive steam engines locomotives and steamships. By 1914, there were 1,000,000 coal miners in Scotland. The stereotype emerged early on of Scottish colliers as brutish, non-religious and socially isolated serfs; that was an exaggeration, for their life style resembled the miners everywhere, with a strong emphasis on masculinity, equalitarianism, group solidarity, and support for radical labour movements.

Britain was the world leader in the construction of railways, and their use to expand trade and coal supplies. The first successful locomotive-powered line in Scotland, between Monkland and Kirkintilloch, opened in 1831. Not only was good passenger service established by the late 1840s, but an excellent network of freight lines reduce the cost of shipping coal, and made products manufactured in Scotland competitive throughout Britain. For example, railways opened the London market to Scottish beef and milk. They enabled the Aberdeen Angus to become a cattle breed of worldwide reputation. By 1900, Scotland had 3500 miles of railway; their main economic contribution was moving supplies in and product out for heavy industry, especially coal-mining.
Scotland was already one of the most urbanised societies in Europe by 1800. The industrial belt ran across the country from southwest to northeast; by 1900 the four industrialised counties of Lanarkshire, Renfrewshire, Dunbartonshire, and Ayrshire contained 44 per cent of the population. Glasgow became one of the largest cities in the world, and known as "the Second City of the Empire" after London. Shipbuilding on Clydeside (the river Clyde through Glasgow and other points) began when the first small yards were opened in 1712 at the Scott family's shipyard at Greenock. After 1860, the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre. "Clydebuilt" became an industry benchmark of quality, and the river's shipyards were given contracts for warships.

The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis. The companies attracted rural workers, as well as immigrants from Catholic Ireland, by inexpensive company housing that was a dramatic move upward from the inner-city slums. This paternalistic policy led many owners to endorse government sponsored housing programs as well as self-help projects among the respectable working class.

While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the mathematicians and physicists James Clerk Maxwell, Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain.

In literature the most successful figure of the mid-nineteenth century was Walter Scott, who began as a poet and also collected and published Scottish ballads. His first prose work, Waverley in 1814, is often called the first historical novel. It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity. In the late 19th century, a number of Scottish-born authors achieved international reputations. Robert Louis Stevenson's work included the urban Gothic novella "Strange Case of Dr Jekyll and Mr Hyde" (1886), and played a major part in developing the historical adventure in books like "Kidnapped" and "Treasure Island". Arthur Conan Doyle's "Sherlock Holmes" stories helped found the tradition of detective fiction. The "kailyard tradition" at the end of the century, brought elements of fantasy and folklore back into fashion as can be seen in the work of figures like J. M. Barrie, most famous for his creation of Peter Pan, and George MacDonald, whose works, including "Phantasies", played a major part in the creation of the fantasy genre.

Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts Movement, and Japonisme, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Among the most prominent members were the loose collective of The Four: acclaimed architect Charles Rennie Mackintosh, his wife the painter and glass artist Margaret MacDonald, her sister the artist Frances, and her husband, the artist and teacher Herbert MacNair.

This period saw a process of rehabilitation for highland culture. Tartan had already been adopted for highland regiments in the British army, which poor highlanders joined in large numbers until the end of the Napoleonic Wars in 1815, but by the 19th century it had largely been abandoned by the ordinary people. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe, prompted by the popularity of Macpherson's Ossian cycle and then Walter Scott's Waverley novels. The world paid attention to their literary redefinition of Scottishness, as they forged an image largely based on characteristics in polar opposition to those associated with England and modernity. This new identity made it possible for Scottish culture to become integrated into a wider European and North American context, not to mention tourist sites, but it also locked in a sense of "otherness" which Scotland began to shed only in the late 20th century. Scott's "staging" of the royal Visit of King George IV to Scotland in 1822 and the king's wearing of tartan, resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish linen industry. The designation of individual clan tartans was largely defined in this period and became a major symbol of Scottish identity. The fashion for all things Scottish was maintained by Queen Victoria, who helped secure the identity of Scotland as a tourist resort, with Balmoral Castle in Aberdeenshire becoming a major royal residence from 1852.

Despite these changes the highlands remained very poor and traditional, with few connections to the uplift of the Scottish Enlightenment and little role in the Industrial Revolution. A handful of powerful families, typified by the dukes of Argyll, Atholl, Buccleuch, and Sutherland, owned large amounts of land and controlled local political, legal and economic affairs. Particularly after the end of the boom created by the Revolutionary and Napoleonic Wars (1790–1815), these landlords needed cash to maintain their position in London society, and had less need of soldiers. They turned to money rents, displaced farmers to raise sheep, and downplayed the traditional patriarchal relationship that had historically sustained the clans. Potato blight reached the Highlands in 1846, where 150,000 people faced disaster because their food supply was largely potatoes (with a little herring, oatmeal and milk). They were rescued by an effective emergency relief system that stands in dramatic contrast to the failures of relief in Ireland. As the famine continued, landlords, charities and government agencies provided "assisted passages" for destitute tenants to emigrate to Canada and Australia; in excess of 16,000 people emigrated, with most travelling in 1851.

Caused by the advent of refrigeration and imports of lamb, mutton and wool from overseas, the 1870s brought with them a collapse of sheep prices and an abrupt halt in the previous sheep farming boom. Land prices subsequently plummeted, too, and accelerated the process of the so-called "Balmoralisation" of Scotland, an era in the second half of the 19th century that saw an increase in tourism and the establishment of large estates dedicated to field sports like deer stalking and grouse shooting, especially in the Scottish Highlands. The process was named after Balmoral estate, purchased by Queen Victoria in 1848, that fueled the romanticisation of upland Scotland and initiated an influx of newly the wealthy acquiring similar estates in the following decades. By the late 19th century just 118 people owned half of Scotland, with nearly 60 per cent of the whole country being part of shooting estates. While their relative importance has somewhat declined due to changing recreational interests throughout the 20th century, deer stalking and grouse shooting remain of prime importance on many private estates in Scotland.

The unequal concentration of land ownership remained an emotional subject and eventually became a cornerstone of liberal radicalism. The politically powerless poor crofters embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800, and the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. This energised the crofters and separated them from the landlords, preparing them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League. Violence began on the Isle of Skye when Highland landlords cleared their lands for sheep and deer parks. It was quieted when the government stepped in passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. In 1885, three Independent Crofter candidates were elected to Parliament, leading to explicit security for the Scottish smallholders; the legal right to bequeath tenancies to descendants; and creating a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained most of their votes.

The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. Even with the development of industry there were insufficient good jobs; as a result, during the period 1841–1931, about 2 million Scots emigrated to North America and Australia, and another 750,000 Scots relocated to England. Scotland lost a much higher proportion of its population than England and Wales, reaching perhaps as much as 30.2 per cent of its natural increase from the 1850s onwards. This not only limited Scotland's population increase, but meant that almost every family lost members due to emigration and, because more of them were young males, it skewed the sex and age ratios of the country.

Scots-born emigrants that played a leading role in the foundation and development of the United States included cleric and revolutionary John Witherspoon, sailor John Paul Jones, industrialist and philanthropist Andrew Carnegie, and scientist and inventor Alexander Graham Bell. In Canada they included soldier and governor of Quebec James Murray, Prime Minister John A. Macdonald and politician and social reformer Tommy Douglas. For Australia they included soldier and governor Lachlan Macquarie, governor and scientist Thomas Brisbane and Prime Minister Andrew Fisher. For New Zealand they included politician Peter Fraser and outlaw James Mckenzie. By the 21st century, there would be about as many people who were Scottish Canadians and Scottish Americans as the 5 million remaining in Scotland.

After prolonged years of struggle, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland. The evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly in the Highlands and Islands, appealing much more strongly than did the established church. Chalmers's ideas shaped the breakaway group. He stressed a social vision that revived and preserved Scotland's communal traditions at a time of strain on the social fabric of the country. Chalmers's idealised small equalitarian, kirk-based, self-contained communities that recognised the individuality of their members and the need for co-operation. That vision also affected the mainstream Presbyterian churches, and by the 1870s it had been assimilated by the established Church of Scotland. Chalmers's ideals demonstrated that the church was concerned with the problems of urban society, and they represented a real attempt to overcome the social fragmentation that took place in industrial towns and cities.

In the late 19th century the major debates were between fundamentalist Calvinists and theological liberals, who rejected a literal interpretation of the Bible. This resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893. There were, however, also moves towards reunion, beginning with the unification of some secessionist churches into the United Secession Church in 1820, which united with the Relief Church in 1847 to form the United Presbyterian Church, which in turn joined with the Free Church in 1900 to form the United Free Church of Scotland. The removal of legislation on lay patronage would allow the majority of the Free Church to rejoin Church of Scotland in 1929. The schisms left small denominations including the Free Presbyterians and a remnant that had not merged in 1900 as the Free Church.

Catholic Emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, principally to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland. Episcopalianism also revived in the 19th century as the issue of succession receded, becoming established as the Episcopal Church in Scotland in 1804, as an autonomous organisation in communion with the Church of England. Baptist, Congregationalist and Methodist churches had appeared in Scotland in the 18th century, but did not begin significant growth until the 19th century, partly because more radical and evangelical traditions already existed within the Church of Scotland and the free churches. From 1879 they were joined by the evangelical revivalism of the Salvation Army, which attempted to make major inroads in the growing urban centres.

Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants, then from 1846 it was funding schools by direct sponsorship, and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards. Overall administration was in the hands of the Scotch (later Scottish) Education Department in London. Education was now compulsory from five to thirteen and many new board schools were built. Larger urban school boards established "higher grade" (secondary) schools as a cheaper alternative to the burgh schools. The Scottish Education Department introduced a Leaving Certificate Examination in 1888 to set national standards for secondary education and in 1890 school fees were abolished, creating a state-funded national system of free basic education and common examinations.

At the beginning of the 19th century, Scottish universities had no entrance exam, students typically entered at ages of 15 or 16, attended for as little as two years, chose which lectures to attend and could leave without qualifications. After two commissions of enquiry in 1826 and 1876 and reforming acts of parliament in 1858 and 1889, the curriculum and system of graduation were reformed to meet the needs of the emerging middle classes and the professions. Entrance examinations equivalent to the School Leaving Certificate were introduced and average ages of entry rose to 17 or 18. Standard patterns of graduation in the arts curriculum offered 3-year ordinary and 4-year honours degrees and separate science faculties were able to move away from the compulsory Latin, Greek and philosophy of the old MA curriculum. The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as well as the upper class. It prepared students for non-commercial careers in government, the law, medicine, education, and the ministry and a smaller group for careers in science and engineering. St Andrews pioneered the admission of women to Scottish universities, creating the Lady Licentiate in Arts (LLA), which proved highly popular. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.

The years before the First World War were the golden age of the inshore fisheries. Landings reached new heights, and Scottish catches dominated Europe's herring trade, accounting for a third of the British catch. High productivity came about thanks to the transition to more productive steam-powered boats, while the rest of Europe's fishing fleets were slower because they were still powered by sails.

In the Khaki Election of 1900, nationalist concern with the Boer War meant that the Conservatives and their Liberal Unionist allies gained a majority of Scottish seats for the first time, although the Liberals regained their ascendancy in the next election. The Unionists and Conservatives merged in 1912, usually known as the Conservatives in England and Wales, they adopted the name Unionist Party in Scotland. Scots played a major part in the leadership of UK political parties producing a Conservative Prime Minister in Arthur Balfour (1902–05) and a Liberal one in Henry Campbell-Bannerman (1905–08). Various organisations, including the Independent Labour Party, joined to make the British Labour Party in 1906, with Keir Hardie as its first chairman.

Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, food (particularly fish) and money, engaging with the conflict with some enthusiasm. Scotland's industries were directed at the war effort. For example, the Singer Clydebank sewing machine factory received over 5000 government contracts, and made 303 million artillery shells, shell components, fuses, and aeroplane parts, as well as grenades, rifle parts, and 361,000 horseshoes. Its labour force of 14,000 was about 70 percent female at war's end.

With a population of 4.8 million in 1911, Scotland sent 690,000 men to the war, of whom 74,000 died in combat or from disease, and 150,000 were seriously wounded. Scottish urban centres, with their poverty and unemployment, were favourite recruiting grounds of the regular British army, and Dundee, where the female-dominated jute industry limited male employment, had one of the highest proportion of reservists and serving soldiers than almost any other British city. Concern for their families' standard of living made men hesitate to enlist; voluntary enlistment rates went up after the government guaranteed a weekly stipend for life to the survivors of men who were killed or disabled. After the introduction of conscription from January 1916 every part of the country was affected. Occasionally Scottish troops made up large proportions of the active combatants, and suffered corresponding loses, as at the Battle of Loos, where there were three full Scots divisions and other Scottish units. Thus, although Scots were only 10 per cent of the British population, they made up 15 per cent of the national armed forces and eventually accounted for 20 per cent of the dead. Some areas, like the thinly populated island of Lewis and Harris, suffered some of the highest proportional losses of any part of Britain. Clydeside shipyards and the nearby engineering shops were the major centres of war industry in Scotland. In Glasgow, radical agitation led to industrial and political unrest that continued after the war ended. After the end of the war in June 1919 the German fleet interned at Scapa Flow was scuttled by its German crews, to avoid its ships being taken over by the victorious allies.

A boom was created by the First World War, with the shipbuilding industry expanding by a third, but a serious depression hit the economy by 1922. The most skilled craftsmen were especially hard hit, because there were few alternative uses for their specialised skills. The main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.

A few industries did grow, such as chemicals and whisky, which developed a global market for premium "Scotch". However, in general the Scottish economy stagnated leading to growing unemployment and political agitation among industrial workers.

After World War I the Liberal Party began to disintegrate and Labour emerged as the party of progressive politics in Scotland, gaining a solid following among working classes of the urban lowlands. As a result, the Unionists were able to gain most of the votes of the middle classes, who now feared Bolshevik revolution, setting the social and geographical electoral pattern in Scotland that would last until the late 20th century. The fear of the left had been fuelled by the emergence of a radical movement led by militant trades unionists. John MacLean emerged as a key political figure in what became known as Red Clydeside, and in January 1919, the British Government, fearful of a revolutionary uprising, deployed tanks and soldiers in central Glasgow. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base in the Irish Catholic working class districts. Women were especially active in building neighbourhood solidarity on housing and rent issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament; in the face of heavy unemployment the workers' mood changed to passive despair by the late 1920s. Scottish educated Bonar Law led a Conservative government from 1922 to 1923 and another Scot, Ramsay MacDonald, would be the Labour Party's first Prime Minister in 1924 and again from 1929 to 1935.

With all the main parties committed to the Union, new nationalist and independent political groupings began to emerge, including the National Party of Scotland in 1928 and Scottish Party in 1930. They joined to form the Scottish National Party (SNP) in 1934, with the goal of creating an independent Scotland, but it enjoyed little electoral success in the Westminster system.

As in World War I, Scapa Flow in Orkney served as an important Royal Navy base. Attacks on Scapa Flow and Rosyth gave RAF fighters their first successes downing bombers in the Firth of Forth and East Lothian. The shipyards and heavy engineering factories in Glasgow and Clydeside played a key part in the war effort, and suffered attacks from the Luftwaffe, enduring great destruction and loss of life. As transatlantic voyages involved negotiating north-west Britain, Scotland played a key part in the battle of the North Atlantic. Shetland's relative proximity to occupied Norway resulted in the Shetland Bus by which fishing boats helped Norwegians flee the Nazis, and expeditions across the North Sea to assist resistance. Significant individual contributions to the war effort by Scots included the invention of radar by Robert Watson-Watt, which was invaluable in the Battle of Britain, as was the leadership at RAF Fighter Command of Air Chief Marshal Hugh Dowding.

In World War II, Prime Minister Winston Churchill appointed Labour politician Tom Johnston as Secretary of State for Scotland in February 1941; he controlled Scottish affairs until the war ended. He launched numerous initiatives to promote Scotland, attracting businesses and new jobs through his new Scottish Council of Industry. He set up 32 committees to deal with social and economic problems, ranging from juvenile delinquency to sheep farming. He regulated rents, and set up a prototype national health service, using new hospitals set up in the expectation of large numbers of casualties from German bombing. His most successful venture was setting up a system of hydro electricity using water power in the Highlands. A long-standing supporter of the Home Rule movement, Johnston persuaded Churchill of the need to counter the nationalist threat north of the border and created a Scottish Council of State and a Council of Industry as institutions to devolve some power away from Whitehall.

In World War II, despite extensive bombing by the Luftwaffe, Scottish industry came out of the depression slump by a dramatic expansion of its industrial activity, absorbing unemployed men and many women as well. The shipyards were the centre of more activity, but many smaller industries produced the machinery needed by the British bombers, tanks and warships. Agriculture prospered, as did all sectors except for coal mining, which was operating mines near exhaustion. Real wages, adjusted for inflation, rose 25 per cent, and unemployment temporarily vanished. Increased income, and the more equal distribution of food, obtained through a tight rationing system, dramatically improved the health and nutrition; the average height of 13-year-olds in Glasgow increased by .

While emigration began to tail off in England and Wales after the First World War, it continued apace in Scotland, with 400,000 Scots, ten per cent of the population, estimated to have left the country between 1921 and 1931. The economic stagnation was only one factor; other push factors included a zest for travel and adventure, and the pull factors of better job opportunities abroad, personal networks to link into, and the basic cultural similarity of the United States, Canada, and Australia. Government subsidies for travel and relocation facilitated the decision to emigrate. Personal networks of family and friends who had gone ahead and wrote back, or sent money, prompted emigrants to retrace their paths. When the Great Depression hit in the 1930s there were no easily available jobs in the US and Canada and the numbers leaving fell to less than 50,000 a year, bringing to an end the period of mass emigrations that had opened in the mid-18th century.

In the early 20th century there was a new surge of activity in Scottish literature, influenced by modernism and resurgent nationalism, known as the Scottish Renaissance. The leading figure in the movement was Hugh MacDiarmid (the pseudonym of Christopher Murray Grieve). MacDiarmid attempted to revive the Scots language as a medium for serious literature in poetic works including "A Drunk Man Looks at the Thistle" (1936), developing a form of Synthetic Scots that combined different regional dialects and archaic terms. Other writers that emerged in this period, and are often treated as part of the movement, include the poets Edwin Muir and William Soutar, the novelists Neil Gunn, George Blake, Nan Shepherd, A. J. Cronin, Naomi Mitchison, Eric Linklater and Lewis Grassic Gibbon, and the playwright James Bridie. All were born within a fifteen-year period (1887 and 1901) and, although they cannot be described as members of a single school, they all pursued an exploration of identity, rejecting nostalgia and parochialism and engaging with social and political issues.

In the 20th century, the centre of the education system became more focused on Scotland, with the ministry of education partly moving north in 1918 and then finally having its headquarters relocated to Edinburgh in 1939. The school leaving age was raised to 14 in 1901, but despite attempts to raise it to 15 this was only made law in 1939 and then postponed because of the outbreak of war. In 1918, Roman Catholic schools were brought into the state system, but retained their distinct religious character, access to schools by priests and the requirement that school staff be acceptable to the Church.

The first half of the 20th century saw Scottish universities fall behind those in England and Europe in terms of participation and investment. The decline of traditional industries between the wars undermined recruitment. English universities increased the numbers of students registered between 1924 and 1927 by 19 per cent, but in Scotland the numbers fell, particularly among women. In the same period, while expenditure in English universities rose by 90 per cent, in Scotland the increase was less than a third of that figure.

Scotland's Scapa Flow was the main base for the Royal Navy in the 20th century. As the Cold War intensified in 1961, the United States deployed Polaris ballistic missiles, and submarines, in the Firth of Clyde's Holy Loch. Public protests from CND campaigners proved futile. The Royal Navy successfully convinced the government to allow the base because it wanted its own Polaris submarines, and it obtained them in 1963. The RN's nuclear submarine base opened with four Polaris submarines at the expanded Faslane Naval Base on the Gare Loch. The first patrol of a Trident-armed submarine occurred in 1994, although the US base was closed at the end of the Cold War.

After World War II, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. This only began to change in the 1970s, partly due to the discovery and development of North Sea oil and gas and partly as Scotland moved towards a more service-based economy. This period saw the emergence of the Scottish National Party and movements for both Scottish independence and more popularly devolution. However, a referendum on devolution in 1979 was unsuccessful as it did not achieve the support of 40 per cent of the electorate (despite a small majority of those who voted supporting the proposal.)

A national referendum to decide on Scottish independence was held on 18 September 2014. Voters were asked to answer either "Yes" or "No" to the question: "Should Scotland be an independent country?" 55.3% of voters answered "No" and 44.7% answered "Yes", with a voter turnout of 84.5%.

In the second half of the 20th century the Labour Party usually won most Scottish seats in the Westminster parliament, losing this dominance briefly to the Unionists in the 1950s. Support in Scotland was critical to Labour's overall electoral fortunes as without Scottish MPs it would have gained only two UK electoral victories in the 20th century (1945 and 1966). The number of Scottish seats represented by Unionists (known as Conservatives from 1965 onwards) went into steady decline from 1959 onwards, until it fell to zero in 1997. Politicians with Scottish connections continued to play a prominent part in UK political life, with Prime Ministers including the Conservatives Harold Macmillan (whose father was Scottish) from 1957 to 1963 and Alec Douglas-Home from 1963 to 1964.

The Scottish National Party gained its first seat at Westminster in 1945 and became a party of national prominence during the 1970s, achieving 11 MPs in 1974. However, a referendum on devolution in 1979 was unsuccessful as it did not achieve the necessary support of 40 per cent of the electorate (despite a small majority of those who voted supporting the proposal) and the SNP went into electoral decline during the 1980s. The introduction in 1989 by the Thatcher-led Conservative government of the Community Charge (widely known as the Poll Tax), one year before the rest of the United Kingdom, contributed to a growing movement for a return to direct Scottish control over domestic affairs. The electoral success of New Labour in 1997 was led by two Prime Ministers with Scottish connections: Tony Blair (who was brought up in Scotland) from 1997 to 2007 and Gordon Brown from 2007 to 2010, opened the way for constitutional change. On 11 September 1997, the 700th anniversary of Battle of Stirling Bridge, the Blair led Labour government again held a referendum on the issue of devolution. A positive outcome led to the establishment of a devolved Scottish Parliament in 1999. A coalition government, which would last until 2007, was formed between Labour and the Liberal Democrats, with Donald Dewar as First Minister. The new Scottish Parliament Building, adjacent to Holyrood House in Edinburgh, opened in 2004. Although not initially reaching its 1970s peak in Westminster elections, the SNP had more success in the Scottish Parliamentary elections with their system of mixed member proportional representation. It became the official opposition in 1999, a minority government in 2007 and a majority government from 2011. In 2014, the independence referendum saw voters reject independence, choosing instead to remain in the United Kingdom. In the 2015 Westminster election, the SNP won 56 out of 59 Scottish seats, making them the third largest party in Westminster.

After World War II, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. This only began to change in the 1970s, partly due to the discovery and development of North Sea oil and gas and partly as Scotland moved towards a more service-based economy. The discovery of the giant Forties oilfield in October 1970 signalled that Scotland was about to become a major oil producing nation, a view confirmed when Shell Expro discovered the giant Brent oilfield in the northern North Sea east of Shetland in 1971. Oil production started from the Argyll field (now Ardmore) in June 1975, followed by Forties in November of that year. Deindustrialisation took place rapidly in the 1970s and 1980s, as most of the traditional industries drastically shrank or were completely closed down. A new service-oriented economy emerged to replace traditional heavy industries. This included a resurgent financial services industry and the electronics manufacturing of Silicon Glen.

In the 20th century existing Christian denominations were joined by other organisations, including the Brethren and Pentecostal churches. Although some denominations thrived, after World War II there was a steady overall decline in church attendance and resulting church closures for most denominations. Talks began in the 1950s aiming at a grand merger of the main Presbyterian, Episcopal and Methodist bodies in Scotland. The talks were ended in 2003, when the General Assembly of the Church of Scotland rejected the proposals. In the 2011 census, 53.8% of the Scottish population identified as Christian (declining from 65.1% in 2001). The Church of Scotland is the largest religious grouping in Scotland, with 32.4% of the population. The Roman Catholic Church accounted for 15.9% of the population and is especially important in West Central Scotland and the Highlands. In recent years other religions have established a presence in Scotland, mainly through immigration and higher birth rates among ethnic minorities, with a small number of converts. Those with the most adherents in the 2011 census are Islam (1.4%, mainly among immigrants from South Asia), Hinduism (0.3%), Buddhism (0.2%) and Sikhism (0.2%). Other minority faiths include the Bahá'í Faith and small Neopagan groups. There are also various organisations which actively promote humanism and secularism, included within the 43.6% who either indicated no religion or did not state a religion in the 2011 census.

Although plans to raise the school leaving age to 15 in the 1940s were never ratified, increasing numbers stayed on beyond elementary education and it was eventually raised to 16 in 1973. As a result, secondary education was the major area of growth in the second half of the 20th century. New qualifications were developed to cope with changing aspirations and economics, with the Leaving Certificate being replaced by the Scottish Certificate of Education Ordinary Grade ('O-Grade') and Higher Grade ('Higher') qualifications in 1962, which became the basic entry qualification for university study. The higher education sector expanded in the second half of the 20th century, with four institutions being given university status in the 1960s (Dundee, Heriot-Watt, Stirling and Strathclyde) and five in the 1990s (Abertay, Glasgow Caledonian, Napier, Paisley and Robert Gordon). After devolution, in 1999 the new Scottish Executive set up an Education Department and an Enterprise, Transport and Lifelong Learning Department. One of the major diversions from practice in England, possible because of devolution, was the abolition of student tuition fees in 1999, instead retaining a system of means-tested student grants.

Some writers that emerged after the Second World War followed Hugh MacDiarmid by writing in Scots, including Robert Garioch and Sydney Goodsir Smith. Others demonstrated a greater interest in English language poetry, among them Norman MacCaig, George Bruce and Maurice Lindsay. George Mackay Brown from Orkney, and Iain Crichton Smith from Lewis, wrote both poetry and prose fiction shaped by their distinctive island backgrounds. The Glaswegian poet Edwin Morgan became known for translations of works from a wide range of European languages. He was also the first Scots Makar (the official national poet), appointed by the inaugural Scottish government in 2004. Many major Scottish post-war novelists, such as Muriel Spark, with "The Prime of Miss Jean Brodie" (1961) spent much or most of their lives outside Scotland, but often dealt with Scottish themes. Successful mass-market works included the action novels of Alistair MacLean, and the historical fiction of Dorothy Dunnett. A younger generation of novelists that emerged in the 1960s and 1970s included Shena Mackay, Alan Spence, Allan Massie and the work of William McIlvanney. From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of Glasgow writers focused around critic, poet and teacher Philip Hobsbaum and editor Peter Kravitz. In the 1990s major, prize winning, Scottish novels, often overtly political, that emerged from this movement included Irvine Welsh's "Trainspotting" (1993), Warner's "Morvern Callar" (1995), Gray's "Poor Things" (1992) and Kelman's "How Late It Was, How Late" (1994). Scottish crime fiction has been a major area of growth, particularly the success of Edinburgh's Ian Rankin and his Inspector Rebus novels. This period also saw the emergence of a new generation of Scottish poets that became leading figures on the UK stage, including Carol Ann Duffy, who was named as Poet Laureate in May 2009, the first woman, the first Scot and the first openly gay poet to take the post.















</doc>
<doc id="13621" url="https://en.wikipedia.org/wiki?curid=13621" title="Hadrian">
Hadrian

Hadrian (; ; 24 January 76 – 10 July 138) was Roman emperor from 117 to 138. He was born into a Roman Italo-Hispanic family that settled in Spain from the Italian city of Atri in Picenum. His father was of senatorial rank and was a first cousin of Emperor Trajan. He married Trajan's grand-niece Vibia Sabina early in his career, before Trajan became emperor and possibly at the behest of Trajan's wife Pompeia Plotina. Plotina and Trajan's close friend and adviser Lucius Licinius Sura were well disposed towards Hadrian. When Trajan died, his widow claimed that he had nominated Hadrian as emperor immediately before his death.

Rome's military and Senate approved Hadrian's succession, but four leading senators were unlawfully put to death soon after. They had opposed Hadrian or seemed to threaten his succession, and the senate held him responsible for it and never forgave him. He earned further disapproval among the elite by abandoning Trajan's expansionist policies and territorial gains in Mesopotamia, Assyria, Armenia, and parts of Dacia. Hadrian preferred to invest in the development of stable, defensible borders and the unification of the empire's disparate peoples. He is known for building Hadrian's Wall, which marked the northern limit of Britannia.

Hadrian energetically pursued his own Imperial ideals and personal interests. He visited almost every province of the Empire, accompanied by an Imperial retinue of specialists and administrators. He encouraged military preparedness and discipline, and he fostered, designed, or personally subsidised various civil and religious institutions and building projects. In Rome itself, he rebuilt the Pantheon and constructed the vast Temple of Venus and Roma. In Egypt, he may have rebuilt the Serapeum of Alexandria. He was an ardent admirer of Greece and sought to make Athens the cultural capital of the Empire, so he ordered the construction of many opulent temples there. His intense relationship with Greek youth Antinous and the latter's untimely death led Hadrian to establish a widespread cult late in his reign. He suppressed the Bar Kokhba revolt in Judaea, but his reign was otherwise peaceful.

Hadrian's last years were marred by chronic illness. He saw the Bar Kokhba revolt as the failure of his panhellenic ideal. He executed two more senators for their alleged plots against him, and this provoked further resentment. His marriage to Vibia Sabina had been unhappy and childless; he adopted Antoninus Pius in 138 and nominated him as a successor, on the condition that Antoninus adopt Marcus Aurelius and Lucius Verus as his own heirs. Hadrian died the same year at Baiae, and Antoninus had him deified, despite opposition from the Senate. Edward Gibbon includes him among the Empire's "Five Good Emperors", a "benevolent dictator"; Hadrian's own senate found him remote and authoritarian. He has been described as enigmatic and contradictory, with a capacity for both great personal generosity and extreme cruelty and driven by insatiable curiosity, self-conceit, and ambition.

Hadrian was born on 24 January 76, probably in Italica (near modern Seville) in the Roman province of Hispania Baetica; one Roman biographer claims he was born at Rome. He was named Publius Aelius Hadrianus. His father was Publius Aelius Hadrianus Afer, a senator of praetorian rank, born and raised in Italica but paternally linked, through many generations over several centuries, to a family from Hadria (modern Atri), an ancient town in Picenum. The family had settled in Italica soon after its founding by Scipio Africanus. Hadrian's mother was Domitia Paulina, daughter of a distinguished Hispano-Roman senatorial family from Gades (Cádiz). His only sibling was an elder sister, Aelia Domitia Paulina. Hadrian's great-nephew, Gnaeus Pedanius Fuscus Salinator, from Barcino (Barcelona) would become Hadrian's colleague as co-consul in 118. As a senator, Hadrian's father would have spent much of his time in Rome. In terms of his later career, Hadrian's most significant family connection was to Trajan, his father's first cousin, who was also of senatorial stock, and had been born and raised in Italica. Hadrian and Trajan were both considered to bein the words of Aurelius Victor"aliens", people "from the outside" ("advenae").

Hadrian's parents died in 86, when he was ten years old. He and his sister became wards of Trajan and Publius Acilius Attianus (who later became Trajan's Praetorian prefect). Hadrian was physically active, and enjoyed hunting; when he was 14, Trajan called him to Rome and arranged his further education in subjects appropriate to a young Roman aristocrat. Hadrian's enthusiasm for Greek literature and culture earned him the nickname "Graeculus" ("Greekling").

Hadrian's first official post in Rome was as a member of the "decemviri stlitibus judicandis", one among many vigintivirate offices at the lowest level of the cursus honorum ("course of honours") that could lead to higher office and a senatorial career. He then served as a military tribune, first with the LegioII "Adiutrix" in 95, then with the Legio V Macedonica. During Hadrian's second stint as tribune, the frail and aged reigning emperor Nerva adopted Trajan as his heir; Hadrian was dispatched to give Trajan the news— or most probably was one of many emissaries charged with this same commission. Then Hadrian was transferred to Legio XXII Primigenia and a third tribunate. Hadrian's three tribunates gave him some career advantage. Most scions of the older senatorial families might serve one, or at most two military tribunates as a prerequisite to higher office. When Nerva died in 98, Hadrian is said to have hastened to Trajan, to inform him ahead of the official envoy sent by the governor, Hadrian's brother-in-law and rival Lucius Julius Ursus Servianus.

In 101, Hadrian was back in Rome; he was elected quaestor, then "quaestor imperatoris Traiani", liaison officer between Emperor and the assembled Senate, to whom he read the Emperor's communiqués and speeches – which he possibly composed on the emperor's behalf. In his role as imperial ghostwriter, Hadrian took the place of the recently deceased Licinius Sura, Trajan's all-powerful friend and kingmaker. His next post was as "ab actis senatus", keeping the Senate's records. During the First Dacian War, Hadrian took the field as a member of Trajan's personal entourage, but was excused from his military post to take office in Rome as Tribune of the Plebs, in 105. After the war, he was probably elected praetor. During the Second Dacian War, Hadrian was in Trajan's personal service again, but was released to serve as legate of Legio I Minervia, then as governor of Lower Pannonia in 107, tasked with "holding back the Sarmatians".

Now in his mid-thirties, Hadrian travelled to Greece; he was granted Athenian citizenship and was appointed eponymous archon of Athens for a brief time (in 112). The Athenians awarded him a statue with an inscription in the Theater of Dionysus (IG II2 3286) offering a detailed account of his "cursus honorum" thus far. Thereafter no more is heard of him until Trajan's Parthian War. It is possible that he remained in Greece until his recall to the imperial retinue, when he joined Trajan's expedition against Parthia as a legate. When the governor of Syria was sent to deal with renewed troubles in Dacia, Hadrian was appointed his replacement, with independent command. Trajan became seriously ill, and took ship for Rome, while Hadrian remained in Syria, "de facto" general commander of the Eastern Roman army. Trajan got as far as the coastal city of Selinus, in Cilicia, and died there, on 8 August; he would be regarded as one of Rome's most admired, popular and best emperors.

Around the time of his quaestorship, in 100 or 101, Hadrian had married Trajan's seventeen or eighteen-year-old grandniece, Vibia Sabina. Trajan himself seems to have been less than enthusiastic about the marriage, and with good reason, as the couple's relationship would prove to be scandalously poor. The marriage might have been arranged by Trajan's empress, Plotina. This highly cultured, influential woman shared many of Hadrian's values and interests, including the idea of the Roman Empire as a commonwealth with an underlying Hellenic culture. If Hadrian were to be appointed Trajan's successor, Plotina and her extended family could retain their social profile and political influence after Trajan's death. Hadrian could also count on the support of his mother-in-law, Salonina Matidia, who was daughter of Trajan's beloved sister Ulpia Marciana. When Ulpia Marciana died, in 112, Trajan had her deified, and made Salonina Matidia an Augusta.

Hadrian's personal relationship with Trajan was complex, and may have been difficult. Hadrian seems to have sought influence over Trajan, or Trajan's decisions, through cultivation of the latter's boy favourites; this gave rise to some unexplained quarrel, around the time of Hadrian's marriage to Sabina. Late in Trajan's reign, Hadrian failed to achieve a senior consulship, being only suffect consul for 108; this gave him parity of status with other members of the senatorial nobility, but no particular distinction befitting an heir designate. Had Trajan wished it, he could have promoted his protege to patrician rank and its privileges, which included opportunities for a fast track to consulship without prior experience as tribune; he chose not to. While Hadrian seems to have been granted the office of Tribune of the Plebs a year or so younger than was customary, he had to leave Dacia, and Trajan, to take up the appointment; Trajan might simply have wanted him out of the way. The "Historia Augusta" describes Trajan's gift to Hadrian of a diamond ring that Trajan himself had received from Nerva, which "encouraged [Hadrian's] hopes of succeeding to the throne". While Trajan actively promoted Hadrian's advancement, he did so with caution.
Failure to nominate an heir could invite chaotic, destructive wresting of power by a succession of competing claimants – a civil war. Too early a nomination could be seen as an abdication, and reduce the chance for an orderly transmission of power. As Trajan lay dying, nursed by his wife, Plotina, and closely watched by Prefect Attianus, he could have lawfully adopted Hadrian as heir, by means of a simple deathbed wish, expressed before witnesses; but when an adoption document was eventually presented, it was signed not by Trajan but by Plotina, and was dated the day after Trajan's death. That Hadrian was still in Syria was a further irregularity, as Roman adoption law required the presence of both parties at the adoption ceremony. Rumours, doubts, and speculation attended Hadrian's adoption and succession. It has been suggested that Trajan's young manservant Phaedimus, who died very soon after Trajan, was killed (or killed himself) rather than face awkward questions. Ancient sources are divided on the legitimacy of Hadrian's adoption: Dio Cassius saw it as bogus and the "Historia Augusta" writer as genuine. An aureus minted early in Hadrian's reign represents the official position; it presents Hadrian as Trajan's "Caesar" (Trajan's heir designate).

According to the "Historia Augusta", Hadrian informed the Senate of his accession in a letter as a "fait accompli", explaining that "the unseemly haste of the troops in acclaiming him emperor was due to the belief that the state could not be without an emperor". The new emperor rewarded the legions' loyalty with the customary bonus, and the Senate endorsed the acclamation. Various public ceremonies were organised on Hadrian's behalf, celebrating his "divine election" by all the gods, whose community now included Trajan, deified at Hadrian's request.

Hadrian remained in the east for a while, suppressing the Jewish revolt that had broken out under Trajan. He relieved Judea's governor, the outstanding Moorish general Lusius Quietus, of his personal guard of Moorish auxiliaries; 
then he moved on to quell disturbances along the Danube frontier. In Rome, Hadrian's former guardian and current Praetorian Prefect, Attianus, claimed to have uncovered a conspiracy involving Lusius Quietus and three others leading senators, Lucius Publilius Celsus, Aulus Cornelius Palma Frontonianus and Gaius Avidius Nigrinus. There was no public trial for the four – they were tried "in absentia", hunted down and killed. Hadrian claimed that Attianus had acted on his own initiative, and rewarded him with senatorial status and consular rank; then pensioned him off, no later than 120. Hadrian assured the senate that henceforth their ancient right to prosecute and judge their own would be respected.

The reasons for these four executions remain obscure. Official recognition of Hadrian as legitimate heir may have come too late to dissuade other potential claimants. Hadrian's greatest rivals were Trajan's closest friends, the most experienced and senior members of the imperial council; any of them might have been a legitimate competitor for the imperial office ("capaces imperii"); and any of them might have supported Trajan's expansionist policies, which Hadrian intended to change. One of their number was Aulus Cornelius Palma who as a former conqueror of Arabia Nabatea would have retained a stake in the East. The "Historia Augusta" describes Palma and a third executed senator, Lucius Publilius Celsus (consul for the second time in 113), as Hadrian's personal enemies, who had spoken in public against him. The fourth was Gaius Avidius Nigrinus, an ex-consul, intellectual, friend of Pliny the Younger and (briefly) Governor of Dacia at the start of Hadrian's reign. He was probably Hadrian's chief rival for the throne; a senator of highest rank, breeding, and connections; according to the "Historia Augusta", Hadrian had considered making Nigrinus his heir apparent, before deciding to get rid of him.

Soon after, in 125, Hadrian appointed Marcius Turbo as his Praetorian Prefect. Turbo was his close friend, a leading figure of the equestrian order, a senior court judge and a procurator. As Hadrian also forbade equestrians to try cases against senators, the Senate retained full legal authority over its members; it also remained the highest court of appeal, and formal appeals to the emperor regarding its decisions were forbidden. If this was an attempt to repair the damage done by Attianus, with or without Hadrian's full knowledge, it was not enough; Hadrian's reputation and relationship with his Senate were iredeemably soured, for the rest of his reign. Some sources describe Hadrian's occasional recourse to a network of informers, the "frumentarii" to discreetly investigate persons of high social standing, including senators and his close friends.

Hadrian was to spend more than half his reign outside Italy. Whereas previous emperors had, for the most part, relied on the reports of their imperial representatives around the Empire, Hadrian wished to see things for himself. Previous emperors had often left Rome for long periods, but mostly to go to war, returning once the conflict was settled. Hadrian's near-incessant travels may represent a calculated break with traditions and attitudes in which the empire was a purely Roman hegemony. Hadrian sought to include provincials in a commonwealth of civilised peoples and a common Hellenic culture under Roman supervision. He supported the creation of provincial towns (municipia), semi-autonomous urban communities with their own customs and laws, rather than the imposition of new Roman colonies with Roman constitutions.

A cosmopolitan, ecumenical intent is evident in coin issues of Hadrian's later reign, showing the emperor "raising up" the personifications of various provinces. Aelius Aristides would later write that Hadrian "extended over his subjects a protecting hand, raising them as one helps fallen men on their feet". All this did not go well with Roman traditionalists. The self-indulgent emperor Nero had enjoyed a prolonged and peaceful tour of Greece, and had been criticised by the Roman elite for abandoning his fundamental responsibilities as emperor. In the eastern provinces, and to some extent in the west, Nero had enjoyed popular support; claims of his imminent return or rebirth emerged almost immediately after his death. Hadrian may have consciously exploited these positive, popular connections during his own travels. In the "Historia Augusta", Hadrian is described as "a little too much Greek", too cosmopolitan for a Roman emperor.

Prior to Hadrian's arrival in Britannia, the province had suffered a major rebellion, from 119 to 121. Inscriptions tell of an "expeditio Britannica" that involved major troop movements, including the dispatch of a detachment (vexillatio), comprising some 3,000 soldiers. Fronto writes about military losses in Britannia at the time. Coin legends of 119–120 attest that Quintus Pompeius Falco was sent to restore order. In 122 Hadrian initiated the construction of a wall, "to separate Romans from barbarians". The idea that the wall was built in order to deal with an actual threat or its resurgence, however, is probable but nevertheless conjectural. A general desire to cease the Empire's extension may have been the determining motive. Reduction of defence costs may also have played a role, as the Wall deterred attacks on Roman territory at a lower cost than a massed border army, and controlled cross-border trade and immigration. A shrine was erected in York to Brittania as the divine personification of Britain; coins were struck, bearing her image, identified as . By the end of 122, Hadrian had concluded his visit to Britannia. He never saw the finished wall that bears his name.

Hadrian appears to have continued through southern Gaul. At Nemausus, he may have overseen the building of a basilica dedicated to his patroness Plotina, who had recently died in Rome and had been deified at Hadrian's request. At around this time, Hadrian dismissed his secretary "ab epistulis", the biographer Suetonius, for "excessive familiarity" towards the empress. Marcius Turbo's colleague as Praetorian Prefect, Gaius Septicius Clarus, was dismissed for the same alleged reason, perhaps a pretext to remove him from office. Hadrian spent the winter of 122/123 at Tarraco, in Spain, where he restored the Temple of Augustus.

In 123, Hadrian crossed the Mediterranean to Mauretania, where he personally led a minor campaign against local rebels. The visit was cut short by reports of war preparations by Parthia; Hadrian quickly headed eastwards. At some point, he visited Cyrene, where he personally funded the training of young men from well-bred families for the Roman military. Cyrene had benefited earlier (in 119) from his restoration of public buildings destroyed during the earlier Jewish revolt.

When Hadrian arrived on the Euphrates, he personally negotiated a settlement with the Parthian King Osroes I, inspected the Roman defences, then set off westwards, along the Black Sea coast. He probably wintered in Nicomedia, the main city of Bithynia. Nicomedia had been hit by an earthquake only shortly before his stay; Hadrian provided funds for its rebuilding, and was acclaimed as restorer of the province.

It is possible that Hadrian visited Claudiopolis and saw the beautiful Antinous, a young man of humble birth who became Hadrian's beloved. Literary and epigraphic sources say nothing of when or where they met; depictions of Antinous show him aged 20 or so, shortly before his death in 130. In 123 he would most likely have been a youth of 13 or 14. It is also possible that Antinous was sent to Rome to be trained as a page to serve the emperor and only gradually rose to the status of imperial favourite. The actual history of their relationship is mostly unknown.

With or without Antinous, Hadrian travelled through Anatolia. Various traditions suggest his presence at particular locations, and allege his foundation of a city within Mysia, Hadrianutherae, after a successful boar hunt. At about this time, plans to complete the Temple of Zeus in Cyzicus, begun by the kings of Pergamon, were put into practice. The temple received a colossal statue of Hadrian. Cyzicus, Pergamon, Smyrna, Ephesus and Sardes were promoted as regional centres for the Imperial cult ("neocoros").

Hadrian arrived in Greece during the autumn of 124, and participated in the Eleusinian Mysteries. He had a particular commitment to Athens, which had previously granted him citizenship and an archonate; at the Athenians' request, he revised their constitution – among other things, he added a new phyle (tribe), which was named after him. Hadrian combined active, hands-on interventions with cautious restraint. He refused to intervene in a local dispute between producers of olive oil and the Athenian Assembly and Council, who had imposed production quotas on oil producers; yet he granted an imperial subsidy for the Athenian grain supply. Hadrian created two foundations, to fund Athens' public games, festivals and competitions if no citizen proved wealthy or willing enough to sponsor them as a Gymnasiarch or Agonothetes. Generally Hadrian preferred that Greek notables, including priests of the Imperial cult, focus on more durable provisions, such as aqueducts and public fountains ("nymphaea"). Athens was given two such fountains; another was given to Argos.

During the winter he toured the Peloponnese. His exact route is uncertain, but it took in Epidaurus; Pausanias describes temples built there by Hadrian, and his statue – in heroic nudity – erected by its citizens in thanks to their "restorer". Antinous and Hadrian may have already been lovers at this time; Hadrian showed particular generosity to Mantinea, which shared ancient, mythic, politically useful links with Antinous' home at Bithynia. He restored Mantinea's Temple of Poseidon Hippios, and according to Pausanias, restored the city's original, classical name. It had been renamed Antigoneia since Hellenistic times, after the Macedonian King Antigonus III Doson. Hadrian also rebuilt the ancient shrines of Abae and Megara, and the Heraion of Argos.

During his tour of the Peloponnese, Hadrian persuaded the Spartan grandee Eurycles Herculanus – leader of the Euryclid family that had ruled Sparta since Augustus' day – to enter the Senate, alongside the Athenian grandee Herodes Atticus the Elder. The two aristocrats would be the first from "Old Greece" to enter the Roman Senate, as representatives of the two "great powers" of the Classical Age. This was an important step in overcoming Greek notables' reluctance to take part in Roman political life. In March 125, Hadrian presided at the Athenian festival of Dionysia, wearing Athenian dress. The Temple of Olympian Zeus had been under construction for more than five centuries; Hadrian committed the vast resources at his command to ensure that the job would be finished. He also organised the planning and construction of a particularly challenging and ambitious aqueduct to bring water to the Athenian Agora.

On his return to Italy, Hadrian made a detour to Sicily. Coins celebrate him as the restorer of the island. Back in Rome, he saw the rebuilt Pantheon, and his completed villa at nearby Tibur, among the Sabine Hills. In early March 127 Hadrian set off on a tour of Italy; his route has been reconstructed through the evidence of his gifts and donations. He restored the shrine of Cupra in Cupra Maritima, and improved the drainage of the Fucine lake. Less welcome than such largesse was his decision in 127 to divide Italy into four regions under imperial legates with consular rank, acting as governors. They were given jurisdiction over all of Italy, excluding Rome itself, therefore shifting Italian cases from the courts of Rome. Having Italy effectively reduced to the status of a group of mere provinces did not go down well with the Roman Senate, and the innovation did not long outlive Hadrian's reign.

Hadrian fell ill around this time; whatever the nature of his illness, it did not stop him from setting off in the spring of 128 to visit Africa. His arrival coincided with the good omen of rain, which ended a drought. Along with his usual role as benefactor and restorer, he found time to inspect the troops; his speech to them survives. Hadrian returned to Italy in the summer of 128 but his stay was brief, as he set off on another tour that would last three years.

In September 128, Hadrian attended the Eleusinian mysteries again. This time his visit to Greece seems to have concentrated on Athens and Sparta – the two ancient rivals for dominance of Greece. Hadrian had played with the idea of focusing his Greek revival around the Amphictyonic League based in Delphi, but by now he had decided on something far grander. His new Panhellenion was going to be a council that would bring Greek cities together. Having set in motion the preparations – deciding whose claim to be a Greek city was genuine would take time – Hadrian set off for Ephesus. From Greece, Hadrian proceeded by way of Asia to Egypt, probably conveyed across the Aegean with his entourage by an Ephesian merchant, Lucius Erastus. Hadrian later sent a letter to the Council of Ephesus, supporting Erastus as a worthy candidate for town councillor and offering to pay the requisite fee.

Hadrian arrived in Egypt before the Egyptian New Year on 29 August 130. He opened his stay in Egypt by restoring Pompey the Great's tomb at Pelusium, offering sacrifice to him as a hero and composing an epigraph for the tomb. As Pompey was universally acknowledged as responsible for establishing Rome's power in the east, this restoration was probably linked to a need to reaffirm Roman Eastern hegemony, following social unrest there during Trajan's late reign. Hadrian and Antinous held a lion hunt in the Libyan desert; a poem on the subject by the Greek Pankrates is the earliest evidence that they travelled together.

While Hadrian and his entourage were sailing on the Nile, Antinous drowned. The exact circumstances surrounding his death are unknown, and accident, suicide, murder and religious sacrifice have all been postulated. "Historia Augusta" offers the following account:
Hadrian founded the city of Antinoöpolis in Antinous' honour on 30 October 130. He then continued down the Nile to Thebes, where his visit to the Colossi of Memnon on 20 and 21 November was commemorated by four epigrams inscribed by Julia Balbilla, which still survive. After that, he headed north, reaching the Fayyum at the beginning of December.

Hadrian's movements after his journey down the Nile are uncertain. Whether or not he returned to Rome, he travelled in the East during 130/131, to organise and inaugurate his new Panhellenion, which was to be focused on the Athenian Temple to Olympian Zeus. As local conflicts had led to the failure of the previous scheme for an Hellenic association centered on Delphi, Hadrian decided instead for a grand league of all Greek cities. Successful applications for membership involved mythologised or fabricated claims to Greek origins, and affirmations of loyalty to Imperial Rome, to satisfy Hadrian's personal, idealised notions of Hellenism. Hadrian saw himself as protector of Greek culture and the "liberties" of Greece – in this case, urban self-government. It allowed Hadrian to appear as the fictive heir to Pericles, who supposedly had convened a previous Panhellenic Congress – such a Congress is mentioned only in Pericles' biography by Plutarch, who respected Rome's Imperial order.

Epigraphical evidence suggests that the prospect of applying to the Panhellenion held little attraction to the wealthier, Hellenised cities of Asia Minor, which were jealous of Athenian and European Greek preeminence within Hadrian's scheme. Hadrian's notion of Hellenism was narrow and deliberately archaising; he defined "Greekness" in terms of classical roots, rather than a broader, Hellenistic culture. Some cities with a dubious claim to Greekness, however – such as Side – were acknowledged as fully Hellenic. The German sociologist Georg Simmel remarked that the Panhellenion was based on "games, commemorations, preservation of an ideal, an entirely non-political Hellenism".

Hadrian bestowed honorific titles on many regional centres. Palmyra received a state visit and was given the civic name Hadriana Palmyra. Hadrian also bestowed honours on various Palmyrene magnates, among them one Soados, who had done much to protect Palmyrene trade between the Roman Empire and Parthia.

Hadrian had spent the winter of 131–32 in Athens, where he dedicated the now-completed Temple of Olympian Zeus, At some time in 132, he headed East, to Judaea.

In Roman Judaea Hadrian visited Jerusalem, which was still in ruins after the First Roman–Jewish War of 66–73. He may have planned to rebuild Jerusalem as a Roman colony – as Vespasian had done with Caesarea Maritima – with various honorific and fiscal privileges. The non-Roman population would have no obligation to participate in Roman religious rituals, but were expected to support the Roman imperial order; this is attested in Caesarea, where some Jews served in the Roman army during both the 66 and 132 rebellions. It has been speculated that Hadrian intended to assimilate the Jewish Temple to the traditional Roman civic-religious Imperial cult; such assimilations had long been commonplace practise in Greece and in other provinces, and on the whole, had been successful. The neighbouring Samaritans had already integrated their religious rites with Hellenistic ones. Strict Jewish monotheism proved more resistant to Imperial cajoling, and then to Imperial demands. A massive anti-Hellenistic and anti-Roman Jewish uprising broke out, led by Simon bar Kokhba. The Roman governor Tineius (Tynius) Rufus asked for an army to crush the resistance; bar Kokhba punished any Jew who refused to join his ranks. According to Justin Martyr and Eusebius, that had to do mostly with Christian converts, who opposed bar Kokhba's messianic claims.

A tradition based on the "Historia Augusta" suggests that the revolt was spurred by Hadrian's abolition of circumcision ("brit milah"); which as a Hellenist he viewed as mutilation. The scholar Peter Schäfer maintains that there is no evidence for this claim, given the notoriously problematical nature of the "Historia Augusta" as a source, the "tomfoolery" shown by the writer in the relevant passage, and the fact that contemporary Roman legislation on "genital mutilation" seems to address the general issue of castration of slaves by their masters. Other issues could have contributed to the outbreak; a heavy-handed, culturally insensitive Roman administration; tensions between the landless poor and incoming Roman colonists privileged with land-grants; and a strong undercurrent of messianism, predicated on Jeremiah's prophecy that the Temple would be rebuilt seventy years after its destruction, as the First Temple had been after the Babylonian exile.
Given the fragmentary nature of the existing evidence, it is impossible to ascertain an exact date for the beginning of the uprising, but it is probable that it began in-between summer and fall 132. The Romans were overwhelmed by the organised ferocity of the uprising. Hadrian called his general Sextus Julius Severus from Britain, and brought troops in from as far as the Danube. Roman losses were heavy; an entire legion or its numeric equivalent of around 4,000. Hadrian's report on the war to the Roman Senate omitted the customary salutation, "If you and your children are in health, it is well; I and the legions are in health." The rebellion was quashed by 135. According to Cassius Dio, Roman war operations in Judea left some 580,000 Jews dead, and 50 fortified towns and 985 villages razed. An unknown proportion of the population was enslaved. Beitar, a fortified city southwest of Jerusalem, fell after a three and a half year siege. The extent of punitive measures against the Jewish population remains a matter of debate.

Hadrian erased the province's name from the Roman map, renaming it Syria Palaestina. He renamed Jerusalem Aelia Capitolina after himself and Jupiter Capitolinus, and had it rebuilt in Greek style. According to Epiphanius, Hadrian appointed Aquila from Sinope in Pontus as "overseer of the work of building the city", since he was related to him by marriage. Hadrian is said to have placed the city's main Forum at the junction of the main Cardo and Decumanus Maximus, now the location for the (smaller) Muristan. After the suppression of the Jewish revolt, Hadrian provided the Samaritans with a temple, dedicated to Zeus Hypsistos ("Highest Zeus") on Mount Gerizim. The bloody repression of the revolt ended Jewish political independence from the Roman Imperial order.

Inscriptions make it clear that in 133 Hadrian took to the field with his armies against the rebels. He then returned to Rome, probably in that year and almost certainly – judging from inscriptions – via Illyricum.

Hadrian spent the final years of his life at Rome. In 134, he took an Imperial salutation for the end of the Second Jewish War (which was not actually concluded until the following year). Commemorations and achievement awards were kept to a minimum, as Hadrian came to see the war "as a cruel and sudden disappointment to his aspirations" towards a cosmopolitan empire.

The Empress Sabina died, probably in 136, after an unhappy marriage with which Hadrian had coped as a political necessity. The "Historia Augusta" biography states that Hadrian himself declared that his wife's "ill-temper and irritability" would be reason enough for a divorce, were he a private citizen. That gave credence, after Sabina's death, to the common belief that Hadrian had her poisoned. In keeping with well-established Imperial propriety, Sabina – who had been made an "Augusta" sometime around 128 – was deified not long after her death.

Hadrian's marriage to Sabina had been childless. Suffering from poor health, Hadrian turned to the problem of the succession. In 136 he adopted one of the ordinary consuls of that year, Lucius Ceionius Commodus, who as an emperor-in waiting took the name Lucius Aelius Caesar. He was the son-in-law of Gaius Avidius Nigrinus, one of the "four consulars" executed in 118, but was himself in delicate health, apparently with a reputation more "of a voluptuous, well educated great lord than that of a leader". Various modern attempts have been made to explain Hadrian's choice: Jerome Carcopino proposes that Aelius was Hadrian's natural son. It has also been speculated that his adoption was Hadrian's belated attempt to reconcile with one of the most important of the four senatorial families whose leading members had been executed soon after Hadrian's succession. Aelius acquitted himself honourably as joint governor of Pannonia Superior and Pannonia Inferior; he held a further consulship in 137, but died on 1 January 138.

Hadrian next adopted Titus Aurelius Fulvus Boionius Arrius Antoninus (the future emperor Antoninus Pius), who had served Hadrian as one of the five imperial legates of Italy, and as proconsul of Asia. In the interests of dynastic stability, Hadrian required that Antoninus adopt both Lucius Ceionius Commodus (son of the deceased Aelius Caesar) and Marcus Annius Verus (grandson of an influential senator of the same name who had been Hadrian's close friend); Annius was already betrothed to Aelius Caesar's daughter Ceionia Fabia. It may not have been Hadrian, but rather Antoninus Pius – Annius Verus's uncle – who supported Annius Verus' advancement; the latter's divorce of Ceionia Fabia and subsequent marriage to Antoninus' daughter Annia Faustina points in the same direction. When he eventually became Emperor, Marcus Aurelius would co-opt Ceionius Commodus as his co-Emperor, under the name of Lucius Verus, on his own initiative.

Hadrian's last few years were marked by conflict and unhappiness. His adoption of Aelius Caesar proved unpopular, not least with Hadrian's brother-in-law Lucius Julius Ursus Servianus and Servianus's grandson Gnaeus Pedanius Fuscus Salinator. Servianus, though now far too old, had stood in the line of succession at the beginning of Hadrian's reign; Fuscus is said to have had designs on the imperial power for himself. In 137 he may have attempted a coup in which his grandfather was implicated; Hadrian ordered that both be put to death. Servianus is reported to have prayed before his execution that Hadrian would "long for death but be unable to die". During his final, protracted illness, Hadrian was prevented from suicide on several occasions.

Hadrian died in the year 138 on 10 July, in his villa at Baiae at the age of 62. Dio Cassius and the "Historia Augusta" record details of his failing health. He had reigned for 21 years, the longest since Tiberius, and the fourth longest in the Principate, after Augustus, Hadrian's successor Antoninus Pius, and Tiberius.

He was buried first at Puteoli, near Baiae, on an estate that had once belonged to Cicero. Soon after, his remains were transferred to Rome and buried in the Gardens of Domitia, close by the almost-complete mausoleum. Upon completion of the Tomb of Hadrian in Rome in 139 by his successor Antoninus Pius, his body was cremated, and his ashes were placed there together with those of his wife Vibia Sabina and his first adopted son, Lucius Aelius, who also died in 138. The Senate had been reluctant to grant Hadrian divine honours; but Antoninus persuaded them by threatening to refuse the position of Emperor. Hadrian was given a temple on the Campus Martius, ornamented with reliefs representing the provinces. The Senate awarded Antoninus the title of "Pius", in recognition of his filial piety in pressing for the deification of his adoptive father. At the same time, perhaps in reflection of the senate's ill will towards Hadrian, commemorative coinage honouring his consecration was kept to a minimum.

Most of Hadrian's military activities were consistent with his ideology of Empire as a community of mutual interest and support. He focused on protection from external and internal threats; on "raising up" existing provinces, rather than the aggressive acquisition of wealth and territory through subjugation of "foreign" peoples that had characterised the early Empire. Hadrian's policy shift was part of a trend towards the slowing down of the empire's expansion, such expansion being not closed after him (the Empire greatest extent being achieved only during the Severan dynasty), but a significant step in this direction, given the empire's overstretching. While the empire as a whole benefited from this, military careerists resented the loss of opportunities.
The 4th-century historian Aurelius Victor saw Hadrian's withdrawal from Trajan's territorial gains in Mesopotamia as a jealous belittlement of Trajan's achievements ("Traiani gloriae invidens"). More likely, an expansionist policy was no longer sustainable; the Empire had lost two legions, the Legio XXII Deiotariana and the "lost legion" IX Hispania, possibly destroyed in a late Trajanic uprising by the Brigantes in Britain. Trajan himself may have thought his gains in Mesopotamia indefensible, and abandoned them shortly before his death. Hadrian granted parts of Dacia to the Roxolani Sarmatians; their king Rasparaganus received Roman citizenship, client king status, and possibly an increased subsidy. Hadrian's presence on the Dacian front at this time is mere conjecture, but Dacia was included in his coin series with allegories of the provinces. A controlled, partial withdrawal of troops from the Dacian plains would have been less costly than maintaining several Roman cavalry units and a supporting network of fortifications.

Hadrian retained control over Osroene through the client king Parthamaspates, who had once served as Trajan's client king of Parthia; and around 121, Hadrian negotiated a peace treaty with the now-independent Parthia. Late in his reign (135), the Alani attacked Roman Cappadocia with the covert support of Pharasmanes, king of Caucasian Iberia. The attack was repulsed by Hadrian's governor, the historian Arrian, who subsequently installed a Roman "adviser" in Iberia. Arrian kept Hadrian well-informed on matters related to the Black Sea and the Caucasus. Between 131 and 132 he sent Hadrian a lengthy letter ("Periplus of the Euxine") on a maritime trip around the Black Sea, intended to offer relevant information in case a Roman intervention was needed.

Hadrian also developed permanent fortifications and military posts along the empire's borders ("limites", sl. "limes") to support his policy of stability, peace and preparedness. This helped keep the military usefully occupied in times of peace; his Wall across Britania was built by ordinary troops. A series of mostly wooden fortifications, forts, outposts and watchtowers strengthened the Danube and Rhine borders. Troops practised intensive, regular drill routines. Although his coins showed military images almost as often as peaceful ones, Hadrian's policy was peace through strength, even threat, with an emphasis on "disciplina" (discipline), which was the subject of two monetary series. Cassius Dio praised Hadrian's emphasis on "spit and polish" as cause for the generally peaceful character of his reign. Fronto expressed other opinions on the subject. In his view, Hadrian preferred war games to actual war, and enjoyed "giving eloquent speeches to the armies" – like the inscribed series of addresses he made while on an inspection tour, during 128, at the new headquarters of Legio III Augusta in Lambaesis

Faced with a shortage of legionary recruits from Italy and other Romanised provinces, Hadrian systematised the use of less costly "numeri" – ethnic non-citizen troops with special weapons, such as Eastern mounted archers – in low-intensity, mobile defensive tasks such as dealing with border infiltrators and skirmishers. Hadrian is also credited with introducing units of heavy cavalry (cataphracts) into the Roman army. Fronto later blamed Hadrian for declining standards in the Roman army of his own time.

Hadrian enacted, through the jurist Salvius Julianus, the first attempt to codify Roman law. This was the Perpetual Edict, according to which the legal actions of praetors became fixed statutes, and as such could no longer be subjected to personal interpretation or change by any magistrate other than the Emperor. At the same time, following a procedure initiated by Domitian, Hadrian made the Emperor's legal advisory board, the "consilia principis" ("council of the princeps") into a permanent body, staffed by salaried legal aides. Its members were mostly drawn from the equestrian class, replacing the earlier freedmen of the Imperial household. This innovation marked the superseding of surviving Republican institutions by an openly autocratic political system. The reformed bureaucracy was supposed to exercise administrative functions independently of traditional magistracies; objectively it did not detract from the Senate's position. The new civil servants were free men and as such supposed to act on behalf of the interests of the "Crown", not of the Emperor as an individual. However, the Senate never accepted the loss of its prestige caused by the emergence of a new aristocracy alongside it, placing more strain on the already troubled relationship between the Senate and the Emperor.

Hadrian codified the customary legal privileges of the wealthiest, most influential or highest status citizens (described as "splendidiores personae" or "honestiores"), who held a traditional right to pay fines when found guilty of relatively minor, non-treasonous offences. Low ranking persons – "alii" ("the others"), including low-ranking citizens – were "humiliores" who for the same offences could be subject to extreme physical punishments, including forced labour in the mines or in public works, as a form of fixed-term servitude. While Republican citizenship had carried at least notional equality under law, and the right to justice, offences in Imperial courts were judged and punished according to the relative prestige, rank, reputation and moral worth of both parties; senatorial courts were apt to be lenient when trying one of their peers, and to deal very harshly with offences committed against one of their number by low ranking citizens or non-citizens. For treason (maiestas) beheading was the worst punishment that the law could inflict on "honestiores"; the "humiliores" might suffer crucifixion, burning, or condemnation to the beasts in the arena.

A great number of Roman citizens maintained a precarious social and economic advantage at the lower end of the hierarchy. Hadrian found it necessary to clarify that decurions, the usually middle-class, elected local officials responsible for running the ordinary, everyday official business of the provinces, counted as "honestiores"; so did soldiers, veterans and their families, as far as civil law was concerned; by implication, all others, including freedmen and slaves, counted as "humiliores". Like most Romans, Hadrian seems to have accepted slavery as morally correct, an expression of the same natural order that rewarded "the best men" with wealth, power and respect. When confronted by a crowd demanding the freeing of a popular slave charioteer, Hadrian replied that he could not free a slave belonging to another person. However, he limited the punishments that slaves could suffer; they could be lawfully tortured to provide evidence, but they could not be lawfully killed unless guilty of a capital offence. Masters were also forbidden to sell slaves to a gladiator trainer (lanista) or to a procurer, except as legally justified punishment. Hadrian also forbade torture of free defendants and witnesses. He abolished ergastula, private prisons for slaves in which kidnapped free men had sometimes been illegally detained.

Hadrian issued a general rescript, imposing a ban on castration, performed on freedman or slave, voluntarily or not, on pain of death for both the performer and the patient. Under the "Lex Cornelia de Sicaris et Veneficis", castration was placed on a par with conspiracy to murder, and punished accordingly. Notwithstanding his philhellenism, Hadrian was also a traditionalist. He enforced dress-standards among the "honestiores"; senators and knights were expected to wear the toga when in public. He imposed strict separation between the sexes in theatres and public baths; to discourage idleness, the latter were not allowed to open until 2.00 in the afternoon, "except for medical reasons".

One of Hadrian's immediate duties on accession was to seek senatorial consent for the apotheosis of his predecessor, Trajan, and any members of Trajan's family to whom he owed a debt of gratitude. Matidia Augusta, Hadrian's mother-in-law, died in December 119, and was duly deified. Hadrian may have stopped at Nemausus during his return from Britannia, to oversee the completion or foundation of a basilica dedicated to his patroness Plotina. She had recently died in Rome and had been deified at Hadrian's request.

As Emperor, Hadrian was also Rome's pontifex maximus, responsible for all religious affairs and the proper functioning of official religious institutions throughout the empire. His Hispano-Roman origins and marked pro-Hellenism shifted the focus of the official imperial cult, from Rome to the Provinces. While his standard coin issues still identified him with the traditional "genius populi Romani", other issues stressed his personal identification with "Hercules Gaditanus" (Hercules of Gades), and Rome's imperial protection of Greek civilisation. He promoted Sagalassos in Greek Pisidia as the Empire's leading Imperial cult centre; his exclusively Greek "Panhellenion" extolled Athens as the spiritual centre of Greek culture.

Hadrian added several Imperial cult centres to the existing roster, particularly in Greece, where traditional intercity rivalries were commonplace. Cities promoted as Imperial cult centres drew Imperial sponsorship of festivals and sacred games, attracted tourism, trade and private investment. Local worthies and sponsors were encouraged to seek self-publicity as cult officials under the aegis of Roman rule, and to foster reverence for Imperial authority. Hadrian's rebuilding of long-established religious centres would have further underlined his respect for the glories of classical Greece – something well in line with contemporary antiquarian tastes. During Hadrian's third and last trip to the Greek East, there seems to have been an upwelling of religious fervour, focused on Hadrian himself. He was given personal cult as a deity, monuments and civic homage, according to the religious syncretism at the time. He may have had the great Serapeum of Alexandria rebuilt, following damage sustained in 116, during the Kitos War.

In 136, just two years before his death, Hadrian dedicated his Temple of Venus and Roma. It was built on land he had set aside for the purpose in 121, formerly the site of Nero's Golden House. The temple was the largest in Rome, and was built in an Hellenising style, more Greek than Roman. The temple's dedication and statuary associated the worship of the traditional Roman goddess Venus, divine ancestress and protector of the Roman people, with the worship of the goddess Roma – herself a Greek invention, hitherto worshiped only in the provinces – to emphasise the universal nature of the empire.

Hadrian had Antinous deified as Osiris-Antinous by an Egyptian priest at the ancient Temple of Ramesses II, very near the place of his death. Hadrian dedicated a new temple-city complex there, built in a Graeco-Roman style, and named it Antinoöpolis. It was a proper Greek polis; it was granted an Imperially subsidised alimentary scheme similar to Trajan's alimenta, and its citizens were allowed intermarriage with members of the native population, without loss of citizen-status. Hadrian thus identified an existing native cult (to Osiris) with Roman rule. The cult of Antinous was to become very popular in the Greek-speaking world, and also found support in the West. In Hadrian's villa, statues of the Tyrannicides, with a bearded Aristogeiton and a clean-shaven Harmodios, linked his favourite to the classical tradition of Greek love. In the west, Antinous was identified with the Celtic sun-god Belenos.

Hadrian was criticised for the open intensity of his grief at Antinous's death, particularly as he had delayed the apotheosis of his own sister Paulina after her death. Nevertheless, his recreation of the deceased youth as a cult-figure found little opposition. Though not a subject of the state-sponsored, official Roman imperial cult, Antinous offered a common focus for the emperor and his subjects, emphasising their sense of community. Medals were struck with his effigy, and statues erected to him in all parts of the empire, in all kinds of garb, including Egyptian dress. Temples were built for his worship in Bithynia and Mantineia in Arcadia. In Athens, festivals were celebrated in his honour and oracles delivered in his name. As an "international" cult figure, Antinous had an enduring fame, far outlasting Hadrian's reign. Local coins with his effigy were still being struck during Caracalla's reign, and he was invoked in a poem to celebrate the accession of Diocletian.

Hadrian continued Trajan's policy on Christians; they should not be sought out, and should only be prosecuted for specific offences, such as refusal to swear oaths. In a rescript addressed to the proconsul of Asia, Gaius Minicius Fundanus, and preserved by Justin Martyr, Hadrian laid down that accusers of Christians had to bear the burden of proof for their denunciations or be punished for "calumnia" (defamation).

Hadrian had an abiding and enthusiastic interest in art, architecture and public works. Rome's Pantheon (temple "to all the gods"), originally built by Agrippa and destroyed by fire in 80, was partly restored under Trajan and completed under Hadrian in the domed form it retains to this day. Hadrian's Villa at Tibur (Tivoli) provides the greatest Roman equivalent of an Alexandrian garden, complete with domed Serapeum, recreating a sacred landscape. An anecdote from Cassius Dio's history suggests Hadrian had a high opinion of his own architectural tastes and talents, and took their rejection as a personal offence: at some time before his reign, his predecessor Trajan was discussing an architectural problem with Apollodorus of Damascus – architect and designer of Trajan's Forum, the Column commemorating his Dacian conquest, and his bridge across the Danube – when Hadrian interrupted to offer his advice. Apollodorus gave him a scathing response: "Be off, and draw your gourds [a sarcastic reference to the domes which Hadrian apparently liked to draw]. You don't understand any of these matters." Dio claims that once Hadrian became emperor, he showed Apollodorus drawings of the gigantic Temple of Venus and Roma, implying that great buildings could be created without his help. When Apollodorus pointed out the building's various insoluble problems and faults, Hadrian was enraged, sent him into exile and later put him to death on trumped up charges.

Hadrian wrote poetry in both Latin and Greek; one of the few surviving examples is a Latin poem he reportedly composed on his deathbed (see below). Some of his Greek productions found their way into the Palatine Anthology. He also wrote an autobiography, which "Historia Augusta" says was published under the name of Hadrian's freedman Phlegon of Tralles. It was not, apparently, a work of great length or revelation, but designed to scotch various rumours or explain Hadrian's most controversial actions. It is possible that this autobiography had the form of a series of open letters to Antoninus Pius.

Hadrian was a passionate hunter from a young age. In northwest Asia, he founded and dedicated a city to commemorate a she-bear he killed. It is documented that in Egypt he and his beloved Antinous killed a lion. In Rome, eight reliefs featuring Hadrian in different stages of hunting decorate a building that began as a monument celebrating a kill.

Hadrian's philhellenism may have been one reason for his adoption, like Nero before him, of the beard as suited to Roman imperial dignity; Dio of Prusa had equated the growth of the beard with the Hellenic ethos. Hadrian's beard may also have served to conceal his natural facial blemishes. All emperors before him (except Nero) had been clean-shaven; emperors who came after him until Constantine the Great were bearded and this imperial fashion was revived again by Phocas at the beginning of the 7th century.

Hadrian was familiar with the rival philosophers Epictetus and Favorinus, and with their works, and held an interest in Roman philosophy. During his first stay in Greece, before he became emperor, he attended lectures by Epictetus at Nicopolis. Shortly before the death of Plotina, Hadrian had granted her wish that the leadership of the Epicurean School in Athens be open to a non-Roman candidate.

During Hadrian's time as Tribune of the Plebs, omens and portents supposedly announced his future imperial condition. According to the "Historia Augusta", Hadrian had a great interest in astrology and divination and had been told of his future accession to the Empire by a grand-uncle who was himself a skilled astrologer.

According to the "Historia Augusta", Hadrian composed the following poem shortly before his death:

The poem has enjoyed remarkable popularity, but uneven critical acclaim. According to Aelius Spartianus, the alleged author of Hadrian's biography in the "Historia Augusta", Hadrian "wrote also similar poems in Greek, not much better than this one". T. S. Eliot's poem "Animula" may have been inspired by Hadrian's, though the relationship is not unambiguous.

Hadrian has been described as the most versatile of all Roman emperors, who "adroitly concealed a mind envious, melancholy, hedonistic, and excessive with respect to his own ostentation; he simulated restraint, affability, clemency, and conversely disguised the ardor for fame with which he burned." His successor Marcus Aurelius, in his "Meditations", lists those to whom he owes a debt of gratitude; Hadrian is conspicuously absent. Hadrian's tense, authoritarian relationship with his senate was acknowledged a generation after his death by Fronto, himself a senator, who wrote in one of his letters to Marcus Aurelius that "I praised the deified Hadrian, your grandfather, in the senate on a number of occasions with great enthusiasm, and I did this willingly, too [...] But, if it can be said – respectfully acknowledging your devotion towards your grandfather – I wanted to appease and assuage Hadrian as I would Mars Gradivus or Dis Pater, rather than to love him." Fronto adds, in another letter, that he kept some friendships, during Hadrian's reign, "under the risk of my life" ("cum periculo capitis"). Hadrian underscored the autocratic character of his reign by counting his "dies imperii" from the day of his acclamation by the armies, rather than the senate, and legislating by frequent use of imperial decrees to bypass the Senate's approval. The veiled antagonism between Hadrian and the Senate never grew to overt confrontation as had happened during the reigns of overtly "bad" emperors, because Hadrian knew how to remain aloof and avoid an open clash. That Hadrian spent half of his reign away from Rome in constant travel probably helped to mitigate the worst of this permanently strained relationship.

In 1503, Niccolò Machiavelli, though an avowed republican, esteemed Hadrian as an ideal "princeps", one of Rome's Five Good Emperors. Friedrich Schiller called Hadrian "the Empire's first servant". Edward Gibbon admired his "vast and active genius" and his "equity and moderation", and considered Hadrian's era as part of the "happiest era of human history". In Ronald Syme's view, Hadrian "was a Führer, a Duce, a Caudillo". According to Syme, Tacitus' description of the rise and accession of Tiberius is a disguised account of Hadrian's authoritarian Principate. According, again, to Syme, Tacitus' Annals would be a work of contemporary history, written "during Hadrian's reign and hating it".

While the balance of ancient literary opinion almost invariably compares Hadrian unfavourably to his predecessor, modern historians have sought to examine his motives, purposes and the consequences of his actions and policies. For M.A. Levi, a summing-up of Hadrian's policies should stress the ecumenical character of the Empire, his development of an alternate bureaucracy disconnected from the Senate and adapted to the needs of an "enlightened" autocracy, and his overall defensive strategy; this would qualify him as a grand Roman political reformer, creator of an openly absolute monarchy to replace a sham senatorial republic. Robin Lane Fox credits Hadrian as creator of a unified Greco-Roman cultural tradition, and as the end of this same tradition; Hadrian's attempted "restoration" of Classical culture within a non-democratic Empire drained it of substantive meaning, or, in Fox's words, "kill[ed] it with kindness".

In Hadrian's time, there was already a well established convention that one could not write a contemporary Roman imperial history for fear of contradicting what the emperors wanted to say, read or hear about themselves. As an earlier Latin source, Fronto's correspondence and works attest to Hadrian's character and the internal politics of his rule. Greek authors such as Philostratus and Pausanias wrote shortly after Hadrian's reign, but confined their scope to the general historical framework that shaped Hadrian's decisions, especially those relating the Greek-speaking world, Greek cities and notables. Pausanias especially wrote a lot in praise of Hadrian's benefactions to Greece in general and Athens in particular. Political histories of Hadrian's reign come mostly from later sources, some of them written centuries after the reign itself. The early 3rd-century "Roman History" by Cassius Dio, written in Greek, gave a general account of Hadrian's reign, but the original is lost, and what survives, aside from some fragments, is a brief, Byzantine-era abridgment by the 11th-century monk Xiphilinius, who focused on Hadrian's religious interests, the Bar Kokhba war, and little else—mostly on Hadrian's moral qualities and his fraught relationship with the Senate. The principal source for Hadrian's life and reign is therefore in Latin: one of several late 4th-century imperial biographies, collectively known as the "Historia Augusta". The collection as a whole is notorious for its unreliability ("a mish mash of actual fact, cloak and dagger, sword and sandal, with a sprinkling of "Ubu Roi""), but most modern historians consider its account of Hadrian to be relatively free of outright fictions, and probably based on sound historical sources, principally one of a lost series of imperial biographies by the prominent 3rd-century senator Marius Maximus, who covered the reigns of Nerva through to Elagabalus.

The first modern historian to produce a chronological account of Hadrian's life, supplementing the written sources with other epigraphical, numismatic, and archaeological evidence, was the German 19th-century medievalist Ferdinand Gregorovius. A 1907 biography by Weber, a German nationalist and later Nazi Party supporter, incorporates the same archaeological evidence to produce an account of Hadrian, and especially his Bar Kokhba war, that has been described as ideologically loaded. Epigraphical studies in the post-war period help support alternate views of Hadrian. Anthony Birley's 1997 biography of Hadrian sums up and reflects these developments in Hadrian historiography.

Inscriptions:





</doc>
<doc id="13623" url="https://en.wikipedia.org/wiki?curid=13623" title="Herman Melville">
Herman Melville

Herman Melville (born Melvill; August 1, 1819 – September 28, 1891) was an American novelist, short story writer, and poet of the American Renaissance period. Among his best-known works are "Moby-Dick" (1851), "Typee" (1846), a romanticized account of his experiences in Polynesia, and "Billy Budd, Sailor", a posthumously published novella. Although his reputation was not high at the time of his death, the centennial of his birth in 1919 was the starting point of a Melville revival and "Moby-Dick" grew to be considered one of the great American novels.

Melville was born in New York City, the third child of a prosperous merchant whose death in 1832 left the family in financial straits. He took to sea in 1839 as a common sailor on a merchant ship and then on the whaler "Acushnet" but he jumped ship in the Marquesas Islands. "Typee", his first book, and its sequel, "Omoo" (1847), were travel-adventures based on his encounters with the peoples of the island. Their success gave him the financial security to marry Elizabeth Shaw, the daughter of a prominent Boston family. "Mardi" (1849), a romance-adventure and his first book not based on his own experience, was not well received. "Redburn" (1849) and "White Jacket" (1850), both tales based on his experience as a well-born young man at sea, were given respectable reviews but did not sell well enough to support his expanding family.

Melville's growing literary ambition showed in "Moby-Dick" (1851), which took nearly a year and a half to write, but it did not find an audience and critics scorned his psychological novel "" (1852). From 1853 to 1856, Melville published short fiction in magazines, including "Benito Cereno" and "Bartleby, the Scrivener". In 1857, he traveled to England, toured the Near East, and published his last work of prose, "The Confidence-Man" (1857). He moved to New York in 1863 to take a position as Customs Inspector. 

From that point, Melville focused his creative powers on poetry. "Battle-Pieces and Aspects of the War" (1866) was his poetic reflection on the moral questions of the American Civil War. In 1867, his eldest child Malcolm died at home from a self-inflicted gunshot. Melville's metaphysical epic "Clarel: A Poem and Pilgrimage in the Holy Land" was published in 1876. In 1886, his other son Stanwix died of apparent tuberculosis, and Melville retired. During his last years, he privately published two volumes of poetry, and left one volume unpublished. The novella "Billy Budd" was left unfinished at his death but was published posthumously in 1924. Melville died from cardiovascular disease in 1891.

Herman Melville was born in New York City on August 1, 1819, to Allan Melvill (1782–1832) and Maria (Gansevoort) Melvill (1791–1872). Herman was the third of eight children in a family of Dutch heredity and background. His siblings, who played important roles in his career as well as in his emotional life, were Gansevoort (1815–1846); Helen Maria (1817–1888); Augusta (1821–1876); Allan (1823–1872); Catherine (1825–1905); Frances Priscilla (1827–1885); and Thomas (1830–1884), who eventually became a governor of Sailors Snug Harbor. Part of a well-established and colorful Boston family, Allan Melvill spent much time out of New York and in Europe as a commission merchant and an importer of French dry goods.
Both of Melville's grandfathers were heroes of the Revolutionary War, and Melville found satisfaction in his "double revolutionary descent". Major Thomas Melvill (1751–1832) had taken part in the Boston Tea Party, and his maternal grandfather, General Peter Gansevoort (1749–1812), was famous for having commanded the defense of Fort Stanwix in New York in 1777. Major Melvill sent his son Allan (Herman's father) to France instead of college at the turn of the 19th century, where he spent two years in Paris and learned to speak and write French fluently. In 1814, Allan, who subscribed to his father's Unitarianism, married Maria Gansevoort, who was committed to the more strict and biblically oriented Dutch Reformed version of the Calvinist creed of her family. This more severe Protestantism of the Gansevoorts' tradition ensured she was well versed in the Bible, both in English as well as in Dutch, the language she had grown up speaking with her parents.

On August 19, almost three weeks after his birth, Herman Melville was baptized at home by a minister of the South Reformed Dutch Church. During the 1820s, Melville lived a privileged, opulent life in a household with three or more servants at a time. At four-year intervals, the family would move to more spacious and elegant quarters, finally settling on Broadway in 1828. Allan Melvill lived beyond his means and on large sums he borrowed from both his father and his wife's widowed mother. Although his wife's opinion of his financial conduct is unknown, biographer Hershel Parker suggests Maria "thought her mother's money was infinite and that she was entitled to much of her portion" while her children were young. How well the parents managed to hide the truth from their children is "impossible to know", according to biographer Andrew Delbanco.

In 1830, Maria's family finally lost patience and their support came to a halt, at which point Allan's total debt to both families exceeded $20,000 (), showing his lack of financial responsibility . The relative happiness and comfort of Melville's early childhood, biographer Newton Arvin writes, depended not so much on Allan's wealth, or his lack of fiscal prudence, as on the "exceptionally tender and affectionate spirit in all the family relationships, especially in the immediate circle". Arvin describes Allan as "a man of real sensibility and a particularly warm and loving father," while Maria was "warmly maternal, simple, robust, and affectionately devoted to her husband and her brood".

Herman's education began in 1824 when he was five, around the time the Melvills moved to a newly built house at 33 Bleecker Street in Manhattan. Herman and his older brother Gansevoort were sent to the New York Male High School. In 1826, the same year that Herman contracted scarlet fever, Allan Melvill described him as "very backwards in speech & somewhat slow in comprehension" at first, but his development increased its pace and Allan was surprised "that Herman proved the best Speaker in the introductory Department". In 1829, both Gansevoort and Herman were transferred to Columbia Grammar and Preparatory School, and Herman enrolled in the English Department on September 28. "Herman I think is making more progress than formerly," Allan wrote in May 1830 to Major Melvill, "and without being a bright Scholar, he maintains a respectable standing, and would proceed further, if he could only be induced to study more—being a most amiable and innocent child, I cannot find it in my heart to coerce him".

Emotionally unstable and behind on paying the rent for the house on Broadway, Herman's father tried to recover from his setbacks by moving his family to Albany, New York, in 1830 and going into the fur business. Herman attended the Albany Academy from October 1830 to October 1831, where he took the standard preparatory course, studying reading and spelling; penmanship; arithmetic; English grammar; geography; natural history; universal, Greek, Roman and English history; classical biography; and Jewish antiquities. "The ubiquitous classical references in Melville's published writings," as Melville scholar Merton Sealts observed "suggest that his study of ancient history, biography, and literature during his school days left a lasting impression on both his thought and his art, as did his almost encyclopedic knowledge of both the Old and the New Testaments". Parker speculates that he left the Academy in October 1831 because "even the tiny tuition fee seemed too much to pay". His brothers Gansevoort and Allan continued their attendance a few months longer, Gansevoort until March the next year.

In December, Herman's father returned from New York City by steamboat, but ice forced him to travel the last seventy miles over two days and two nights in an open horse carriage at , causing him to become ill. In early January, he began to show "signs of delirium," and his situation grew worse until his wife felt his suffering deprived him of his intellect. Two months before reaching fifty, Allan Melvill died on January 28, 1832. As Herman was no longer attending school, he likely witnessed these scenes. Twenty years later he described a similar death in "Pierre".

The death of Allan caused many major shifts in the family's material and spiritual circumstances. One result was the greater influence of his mother's religious beliefs. Maria sought consolation in her faith and in April was admitted as a member of the First Reformed Dutch Church. Herman's saturation in orthodox Calvinism was surely the most decisive intellectual and spiritual influence of his early life. 
Two months after his father's death, Gansevoort entered the cap and fur business. Uncle Peter Gansevoort, a director of the New York State Bank, got Herman a job as clerk for $150 a year (). Biographers cite a passage from "Redburn" when trying to answer what Herman must have felt then: "I had learned to think much and bitterly before my time," the narrator remarks, adding, "I must not think of those delightful days, before my father became a bankrupt ... and we removed from the city; for when I think of those days, something rises up in my throat and almost strangles me". With Melville, Arvin argues, one has to reckon with "psychology, the tormented psychology, of the decayed patrician".

When Melville's paternal grandfather died on September 16, 1832, Maria and her children discovered Allan, somewhat unscrupulously, had borrowed more than his share of his inheritance, meaning Maria received only $20 (). His paternal grandmother died almost exactly seven months later. Melville did his job well at the bank; although he was only 14 in 1834, the bank considered him competent enough to be sent to Schenectady, New York on an errand. Not much else is known from this period except that he was very fond of drawing. The visual arts became a lifelong interest. Around May 1834, the Melvilles moved to another house in Albany, a three-story brick house. That same month a fire destroyed Gansevoort's skin-preparing factory, which left him with personnel he could neither employ nor afford. Instead he pulled Melville out of the bank to man the cap and fur store.

In 1835, while still working in the store, Melville enrolled in Albany Classical School, perhaps using Maria's part of the proceeds from the sale of the estate of his maternal grandmother in March 1835. In September of the following year, Herman was back in Albany Academy in the Latin course. He also participated in debating societies, in an apparent effort to make up as much as he could for his missed years of schooling. In this period he read Shakespeare—at least "Macbeth", whose witch scenes gave him the chance to teasingly scare his sisters. In March 1837, he was again withdrawn from Albany Academy.

Gansevoort served as a role model and support for Melville in many ways throughout his life, at this time particularly in forming a self-directed educational plan. In early 1834 Gansevoort had become a member of Albany's Young Men's Association for Mutual Improvement, and in January 1835 Melville joined him there. Gansevoort also had copies of John Todd's "Index Rerum", a blank register for indexing remarkable passages from books one had read for easy retrieval. Among the sample entries which Gansevoort made showing his academic scrupulousness was "Pequot, beautiful description of the war with," with a short title reference to the place in Benjamin Trumbull's "A Complete History of Connecticut" (Volume I in 1797, and Volume II in 1898) where the description could be found. The two surviving volumes of Gansevoort's are the best evidence for Melville's reading in this period. Gansevoort's entries include books Melville used for "Moby-Dick" and "Clarel", such as "Parsees—of India—an excellent description of their character, and religion and an account of their descent—East India Sketch Book p. 21". Other entries are on Panther, the pirate's cabin, and storm at sea from James Fenimore Cooper's "The Red Rover", Saint-Saba.

The Panic of 1837 forced Gansevoort to file for bankruptcy in April. In June, Maria told the younger children they must leave Albany for somewhere cheaper. Gansevoort began studying law in New York City while Herman managed the farm before getting a teaching position at Sikes District School near Lenox, Massachusetts. He taught about 30 students of various ages, including some his own age.

The semester over, he returned to his mother in 1838. In February he was elected president of the Philo Logos Society, which Peter Gansevoort invited to move into Stanwix Hall for no rent. In the "Albany Microscope" in March, Melville published two polemical letters about issues in vogue in the debating societies. Historians Leon Howard and Hershel Parker suggest the motive behind the letters was a youthful desire to have his rhetorical skills publicly recognized. In May, the Melvilles moved to a rented house in Lansingburgh, almost 12 miles north of Albany. Nothing is known about what Melville did or where he went for several months after he finished teaching at Sikes. On November 12, five days after arriving in Lansingburgh, Melville paid for a term at Lansingburgh Academy to study surveying and engineering. In an April 1839 letter recommending Herman for a job in the Engineer Department of the Erie Canal, Peter Gansevoort says his nephew "possesses the ambition to make himself useful in a business which he desires to make his profession," but no job resulted.

Just weeks after this failure, Melville's first known published essay appeared. Using the initials "L.A.V"., Herman contributed "Fragments from a Writing Desk" to the weekly newspaper "Democratic Press and Lansingburgh Advertiser", which printed it in two installments, the first on May 4. According to Merton Sealts, his use of heavy-handed allusions reveals familiarity with the work of William Shakespeare, John Milton, Walter Scott, Richard Brinsley Sheridan, Edmund Burke, Samuel Taylor Coleridge, Lord Byron, and Thomas Moore. Parker calls the piece "characteristic Melvillean mood-stuff" and considers its style "excessive enough [...] to indulge his extravagances and just enough overdone to allow him to deny that he was taking his style seriously". For Delbanco, the style is "overheated in the manner of Poe, with sexually charged echoes of Byron and "The Arabian Nights"".

On May 31, 1839, Gansevoort, then living in New York City, wrote that he was sure Herman could get a job on a whaler or merchant vessel. The next day, he signed aboard the merchant ship "St. Lawrence" as a "boy" (a green hand), which cruised from New York to Liverpool. "Redburn: His First Voyage" (1849) draws on his experiences in this journey; at least two of the nine guide-books listed in chapter 30 of the book had been part of Allan Melvill's library. He arrived back in New York October 1, 1839 and resumed teaching, now at Greenbush, New York, but left after one term because he had not been paid. In the summer of 1840 he and his friend James Murdock Fly went to Galena, Illinois to see if his Uncle Thomas could help them find work. Unsuccessful, he and his friend returned home in autumn, likely by way of St. Louis and up the Ohio River.

As part of the response to contemporaneous popular cultural reading, including Richard Henry Dana Jr.'s new book "Two Years Before the Mast", and by Jeremiah N. Reynolds's account in the May 1839 issue of "The Knickerbocker" magazine of the hunt for a great white sperm whale named Mocha Dick, Melville and Gansevoort traveled to New Bedford, where Melville signed up for a whaling voyage aboard a new ship, the "Acushnet". Built in 1840, the ship measured some 104 feet in length, almost 28 feet in breadth, and almost 14 feet in depth. She measured slightly less than 360 tons, had two decks] and three masts, but no quarter galleries. Melville signed a contract on Christmas Day with the ship's agent as a "green hand" for 1/175th of whatever profits the voyage would yield. On Sunday the 27th the brothers heard the Reverend Enoch Mudge preach at the Seamen's Bethel on Johnny-Cake Hill, where white marble cenotaphs on the walls memorialized local sailors who had died at sea, often in battle with whales. When he signed the crew list the next day he was advanced $84.

On January 3, 1841, the "Acushnet" set sail. Melville slept with some twenty others in the forecastle; Captain Valentine Pease, the mates, and the skilled men slept aft. Whales were found near The Bahamas, and in March 150 barrels of oil were sent home from Rio de Janeiro. Cutting in and trying-out (boiling) a single whale took about three days, and a whale yielded approximately one barrel of oil per foot of length and per ton of weight (the average whale weighed 40 to 60 tons). The oil was kept on deck for a day to cool off, and was then stowed down; scrubbing the deck completed the labor. An average voyage meant that some forty whales were killed to yield some 1600 barrels of oil.

On April 15, the "Acushnet" sailed around Cape Horn, and traveled to the South Pacific, where the crew sighted whales without catching any. Then up the coast of Chile to the region of Selkirk Island and on 7 May, near Juan Fernández Islands, she had 160 barrels. On June 23 the ship anchored for the first time since Rio, in Santa Harbor. The cruising grounds the "Acushnet" was sailing attracted much traffic, and Captain Pease not only paused to visit other whalers, but at times hunted in company with them. From July 23 into August the "Acushnet" regularly gammed with the "Lima" from Nantucket, and Melville met William Henry Chase, the son of Owen Chase, who gave him a copy of his father's account of his adventures aboard the "Essex". Ten years later Melville wrote in his other copy of the book: "The reading of this wondrous story upon the landless sea, & close to the very latitude of the shipwreck had a surprising effect upon me".

On September 25 the ship reported 600 barrels of oil to another whaler, and in October 700 barrels. On October 24 the "Acushnet" crossed the equator to the north, and six or seven days later arrived at the Galápagos Islands. This short visit would be the basis for "The Encantadas". On November 2, the "Acushnet" and three other American whalers were hunting together near the Galápagos Islands; Melville later exaggerated that number in Sketch Fourth of "The Encantadas". From November 19 to 25 the ship anchored at Chatham's Isle, and on December 2 reached the coast of Peru and anchored at Tombez near Paita, with 570 barrels of oil on board. On December 27 the "Acushnet" sighted Cape Blanco, off Ecuador, Point St. Elena was sighted the next day, and on January 6, 1842, the ship approached the Galápagos Islands from the southeast. From February 13 to 7 May, seven sightings of sperm whales were recorded but none killed. From early May to early June, the "Acushnet" cooperatively set about its whaling endeavors several times with the "Columbus" of New Bedford, which also took letters from Melville's ship; the two ships were in the same area just south of the Equator. On June 16 she carried 750 barrels, and sent home 200 on the "Herald the Second". On June 23, the "Acushnet" reached the Marquesas Islands, and anchored at Nuku Hiva.

A time of some emotional turbulence for Melville ensued over the next summer months. On July 9, 1842, Melville and his shipmate Richard Tobias Greene jumped ship at Nuku Hiva Bay and ventured into the mountains to avoid capture. While Melville's first book, "Typee" (1845), is loosely based on his stay in or near the Taipi Valley, scholarly research has increasingly shown that much if not all of this account was either taken from Melville's readings or exaggerated to dramatize a contrast between idyllic native culture and Western civilization. On August 9, Melville boarded the Australian whaler "Lucy Ann", bound for Tahiti, where on arrival he took part in a mutiny and was briefly jailed in the native "Calabooza Beretanee". In October, he and crew mate John B. Troy escaped Tahiti for Eimeo. He then spent a month as beachcomber and island rover ("omoo" in Tahitian), eventually crossing over to Moorea. He drew on these experiences for "Omoo", the sequel to "Typee". In November, he contracted to be a seaman on the Nantucket whaler "Charles & Henry" for a six-month cruise (November 1842 − April 1843), and was discharged at Lahaina, Maui in the Hawaiian Islands in May 1843.

After four months of working several jobs, including as a clerk, he joined the US Navy initially as one of the crew of the frigate as an ordinary seaman on August 20. During the next year, the homeward bound ship visited the Marquesas Islands, Tahiti, and Valparaiso, and then, from summer to fall 1844, Mazatlan, Lima, and Rio de Janeiro, before reaching Boston on October 3. Melville was discharged on October 14. This Navy experience is used in "White-Jacket" (1850), Melville's fifth book. Melville's wander-years created what biographer Arvin calls "a settled hatred of external authority, a lust for personal freedom" and a "growing and intensifying sense of his own exceptionalness as a person," along with "the resentful sense that circumstance and mankind together had already imposed their will upon him in a series of injurious ways". Scholar Robert Milder believes the encounter with the wide ocean, where he was seemingly abandoned by God, led Melville to experience a "metaphysical estrangement" and influenced his social views in two ways: first, that he belonged to the genteel classes but sympathized with the "disinherited commons" he had been placed among; and second that experiencing the cultures of Polynesia let him view the West from an outsider's perspective.

Upon his return, Melville regaled his family and friends with his adventurous tales and romantic experiences, and they urged him to put them into writing. Melville completed "Typee", his first book, in the summer of 1845 while living in Troy, New York. His brother Gansevoort found a publisher for it in London, where it was published in February 1846 by John Murray in his travel adventure series. It became an overnight bestseller in England, then in New York, when it was published on March 17 by Wiley & Putnam.

Melville extended the period his narrator spent on the island by three months, made it appear he understood the native language, and incorporated material from source books he had assembled. Milder calls "Typee" "an appealing mixture of adventure, anecdote, ethnography, and social criticism presented with a genial latitudinarianism that gave novelty to a South Sea idyll at once erotically suggestive and romantically chaste".

An unsigned review in the "Salem Advertiser" written by Nathaniel Hawthorne called the book a "skilfully managed" narrative by an author with "that freedom of view ... which renders him tolerant of codes of morals that may be little in accordance with our own". Hawthorne continued: This book is lightly but vigorously written; and we are acquainted with no work that gives a freer and more effective picture of barbarian life, in that unadulterated state of which there are now so few specimens remaining. The gentleness of disposition that seems akin to the delicious climate, is shown in contrast with the traits of savage fierceness...He has that freedom of view—it would be too harsh to call it laxity of principle—which renders him tolerant of codes of morals that may be little in accordance with our own, a spirit proper enough to a young and adventurous sailor, and which makes his book the more wholesome to our staid landsmen. 

Pleased but not overwhelmed by the adulation of his new public, Melville later expressed concern that he would "go down to posterity ... as a 'man who lived among the cannibals'!" The writing of "Typee" brought Melville back into contact with his friend Greene—Toby in the book—who wrote confirming Melville's account in newspapers. The two corresponded until 1863, and in his final years Melville "traced and successfully located his old friend" for a further meeting of the two friends. In March 1847, "Omoo", a sequel to "Typee" was published by Murray in London, and in May by Harper in New York. "Omoo" is "a slighter but more professional book," according to Milder. "Typee" and "Omoo" gave Melville overnight renown as a writer and adventurer, and he often entertained by telling stories to his admirers. As the writer and editor Nathaniel Parker Willis wrote, "With his cigar and his Spanish eyes, he "talks" Typee and Omoo, just as you find the flow of his delightful mind on paper". In 1847 Melville tried unsuccessfully to find a "government job" in Washington.
In June 1847, Melville and Elizabeth "Lizzie" Knapp Shaw were engaged, after knowing each other for approximately three months. Melville had first asked her father, Lemuel Shaw, for her hand in March, but was turned down at the time. Shaw, Chief Justice of Massachusetts, had been a close friend of Melville's father, and his marriage with Melville's aunt Nancy was prevented only by her death. His warmth and financial support for the family continued after Allan's death. Melville dedicated his first book, "Typee", to him. Lizzie was raised by her grandmother and an Irish nurse. Arvin suggests that Melville's interest in Lizzie may have been stimulated by "his need of Judge Shaw's paternal presence". They were married on August 4, 1847. Lizzie described their marriage as "very unexpected, and scarcely thought of until about two months before it actually took place". She wanted to be married in church, but they had a private wedding ceremony at home to avoid possible crowds hoping to see the celebrity. The couple honeymooned in the then-British Province of Canada, and traveled to Montreal. They settled in a house on Fourth Avenue in New York City (now called Park Avenue).

According to scholars Joyce Deveau Kennedy and Frederick James Kennedy, Lizzie brought to their marriage a sense of religious obligation, an intent to make a home with Melville regardless of place, a willingness to please her husband by performing such "tasks of drudgery" as mending stockings, an ability to hide her agitation, and a desire "to shield Melville from unpleasantness". The Kennedys conclude their assessment with:

Biographer Robertson-Lorant cites "Lizzie's adventurous spirit and abundant energy," and she suggests that "her pluck and good humor might have been what attracted Melville to her, and vice versa". An example of such good humor appears in a letter about her not yet used to being married: "It seems sometimes exactly as if I were here for a "visit". The illusion is quite dispelled however when Herman stalks into my room without even the ceremony of knocking, bringing me perhaps a button to sew on, or some equally romantic occupation". On February 16, 1849, the Melvilles' first child, Malcolm, was born.

In March 1848, "Mardi" was published by Richard Bentley in London, and in April by Harper in New York. Nathaniel Hawthorne thought it a rich book "with depths here and there that compel a man to swim for his life". According to Milder, the book began as another South Sea story but, as he wrote, Melville left that genre behind, first in favor of "a romance of the narrator Taji and the lost maiden Yillah," and then "to an allegorical voyage of the philosopher Babbalanja and his companions through the imaginary archipelago of Mardi".

In October 1849, "Redburn" was published by Bentley in London, and in November by Harper in New York. The bankruptcy and death of Allan Melvill, and Melville's own youthful humiliations surface in this "story of outward adaptation and inner impairment". Biographer Robertson-Lorant regards the work as a deliberate attempt for popular appeal: "Melville modeled each episode almost systematically on every genre that was popular with some group of antebellum readers," combining elements of "the picaresque novel, the travelogue, the nautical adventure, the sentimental novel, the sensational French romance, the gothic thriller, temperance tracts, urban reform literature, and the English pastoral". His next novel, "White-Jacket", was published by Bentley in London in January 1850, and in March by Harper in New York.

The earliest surviving mention of "Moby-Dick" is from early May 1850, when Melville told fellow sea author Richard Henry Dana Jr. it was "half way" written. In June, he described the book to his English publisher as "a romance of adventure, founded upon certain wild legends in the Southern Sperm Whale Fisheries," and promised it would be done by the fall. The original manuscript has not survived, but over the next several months Melville radically transformed his initial plan, conceiving what Delbanco described in 2005 as "the most ambitious book ever conceived by an American writer".

From August 4 to 12, 1850, the Melvilles, Sarah Morewood, Evert Duyckinck, Oliver Wendell Holmes, and other literary figures from New York and Boston came to Pittsfield to enjoy a period of parties, picnics, dinners, and the like. Nathaniel Hawthorne and his publisher James T. Fields joined the group while Hawthorne's wife stayed at home to look after the children. Melville wrote that these stories revealed a dark side to Hawthorne, "shrouded in blackness, ten times black". Later that summer, Duyckinck sent Hawthorne copies of Melville's three latest books. Hawthorne read them, as he wrote to Duyckinck on August 29, "with a progressive appreciation of their author".He thought Melville in "Redburn" and "White-Jacket" put the reality "more unflinchingly" before his reader than any writer, and he thought "Mardi" was "a rich book, with depths here and there that compel a man to swim for his life. It is so good that one scarcely pardons the writer for not having brooded long over it, so as to make it a great deal better".

In September 1850, Melville borrowed three thousand dollars from his father-in-law Lemuel Shaw to buy a 160-acre farm in Pittsfield, Massachusetts. Melville called his new home Arrowhead because of the arrowheads that were dug up around the property during planting season. That winter, Melville paid Hawthorne an unexpected visit, only to discover he was working and "not in the mood for company". Hawthorne's wife Sophia gave him copies of "Twice-Told Tales" and, for Malcolm, "The Grandfather's Chair". Melville invited them to visit Arrowhead soon, hoping to "[discuss] the Universe with a bottle of brandy & cigars" with Hawthorne, but Hawthorne would not stop working on his new book for more than one day and they did not come. After a second visit from Melville, Hawthorne surprised him by arriving at Arrowhead with his daughter Una. According to Robertson-Lorant, "The handsome Hawthorne made quite an impression on the Melville women, especially Augusta, who was a great fan of his books". They spent the day mostly "smoking and talking metaphysics".

In Robertson-Lorant's assessment of the friendship, Melville was "infatuated with Hawthorne's intellect, captivated by his artistry, and charmed by his elusive personality," and though the two writers were "drawn together in an undeniable sympathy of soul and intellect, the friendship meant something different to each of them," with Hawthorne offering Melville "the kind of intellectual stimulation he needed". They may have been "natural allies and friends," yet they were also "fifteen years apart in age and temperamentally quite different" and Hawthorne "found Melville's manic intensity exhausting at times". Melville wrote 10 letters to Hawthorne; one scholar identifies "sexual excitement...in all the letters". Melville was inspired and with Hawthorne during the period that he was writing "Moby-Dick." Melville dedicated the work to Hawthorne: "In token of my admiration for his genius, this book is inscribed to Nathaniel Hawthorne".

On October 18, 1851, "The Whale" was published in Britain in three volumes, and on November 14 "Moby-Dick" appeared in the United States as a single volume. In between these dates, on October 22, 1851, the Melvilles' second child, Stanwix, was born. In December, Hawthorne told Duyckinck, "What a book Melville has written! It gives me an idea of much greater power than his preceding ones." Unlike other contemporaneous reviewers of Melville, Hawthorne had seen the uniqueness of Melville's new novel and acknowledged it. In early December 1852, Melville visited the Hawthornes in Concord and discussed the idea of the "Agatha" story he had pitched to Hawthorne. This was the last known contact between the two writers before Melville visited Hawthorne in Liverpool four years later when Hawthorne had relocated to England.

After having borrowed three thousand dollars from his father-in-law in September 1850 to buy a 160-acre farm in Pittsfield, Massachusetts, Melville had high hopes that his next book would please the public and restore his finances. In April 1851 he told his British publisher, Richard Bentley, that his new book had "unquestionable novelty" and was calculated to have wide appeal with elements of romance and mystery. In fact, "" was heavily psychological, though drawing on the conventions of the romance, and difficult in style. It was not well received. The New York "Day Book" published a venomous attack on September 8, 1852, headlined "HERMAN MELVILLE CRAZY". The item, offered as a news story, reported, 

On May 22, 1853, Melville's third child and first daughter Elizabeth (Bessie) was born, and on or about that day Herman finished work on the Agatha story, "Isle of the Cross". Melville traveled to New York to discuss a book, presumably "Isle of the Cross", with his publisher, but later wrote that Harper & Brothers was "prevented" from publishing his manuscript because it was lost.

After the commercial and critical failure of "Pierre", Melville had difficulty finding a publisher for his follow-up novel "Israel Potter". Instead, this narrative of a Revolutionary War veteran was serialized in "Putnam's Monthly Magazine" in 1853. From November 1853 to 1856, Melville published fourteen tales and sketches in "Putnam's" and "Harper's" magazines. In December 1855 he proposed to Dix & Edwards, the new owners of "Putnam's", that they publish a selective collection of the short fiction. The collection, titled "The Piazza Tales", was named after a new introductory story Melville wrote for it, "The Piazza". It also contained five previously published stories, including "Bartleby, the Scrivener" and "Benito Cereno". On March 2, 1855, the Melvilles' fourth child, Frances (Fanny), was born. In this period, his book "Israel Potter" was published.

The writing of "The Confidence-Man" put great strain on Melville, leading Sam Shaw, a nephew of Lizzie, to write to his uncle Lemuel Shaw: "Herman I hope has had no more of those ugly attacks"—a reference to what Robertson-Lorant calls "the bouts of rheumatism and sciatica that plagued Melville". Melville's father-in-law apparently shared his daughter's "great anxiety about him" when he wrote a letter to a cousin, in which he described Melville's working habits: "When he is deeply engaged in one of his literary works, he confines him[self] to hard study many hours in the day, with little or no exercise, and this specially in winter for a great many days together. He probably thus overworks himself and brings on severe nervous affections". Shaw advanced Melville $1,500 from Lizzie's inheritance to travel four or five months in Europe and the Holy Land.

From October 11, 1856, to May 20, 1857, Melville made a six-month Grand Tour of Europe and the Mediterranean. While in England, in November 1856, he briefly reunited for three days with Hawthorne, who had taken the position of United States Consul at Liverpool, at that time the hub of Britain's Atlantic trade. At the nearby coast resort of Southport, amid the sand dunes where they had stopped to smoke cigars, they had a conversation which Hawthorne later described in his journal: "Melville, as he always does, began to reason of Providence and futurity, and of everything that lies beyond human ken, and informed me that he 'pretty much made up his mind to be annihilated' [...] If he were a religious man, he would be one of the most truly religious and reverential; he has a very high and noble nature, and better worth immortality than most of us."

The Mediterranean part of the tour took in the Holy Land, which inspired his epic poem "Clarel." On April 1, 1857, Melville published his last full-length novel "The Confidence-Man". This novel, subtitled "His Masquerade", has won general acclaim in modern times as a complex and mysterious exploration of issues of fraud and honesty, identity and masquerade. However, when it was published, it received reviews ranging from the bewildered to the denunciatory.

To repair his faltering finances, Melville took up public lecturing from late 1857 to 1860. He embarked upon three lecture tours and spoke at lyceums, chiefly on Roman statuary and sightseeing in Rome. Melville's lectures, which mocked the pseudo-intellectualism of lyceum culture, were panned by contemporary audiences. On May 30, 1860, Melville boarded the clipper "Meteor" for California, with his brother Thomas at the helm. After a shaky trip around Cape Horn, Melville returned to New York alone via Panama in November. Later that year, he submitted a poetry collection to a publisher but it was not accepted, and is now lost. In 1863, he bought his brother's house at 104 East 26th Street in New York City and moved there.

In 1864, Melville visited the Virginia battlefields of the American Civil War. After the war, he published "Battle Pieces and Aspects of the War" (1866), a collection of 72 poems that has been described as "a polyphonic verse journal of the conflict". The work did not do well commercially—of the print run of 1,260 copies, 300 were sent as review copies, and 551 copies were sold—and reviewers did not realize that Melville had purposely avoided the ostentatious diction and fine writing that were in fashion, choosing to be concise and spare.

In 1866, Melville became a customs inspector for New York City. He held the post for 19 years and had a reputation for honesty in a notoriously corrupt institution. Unbeknownst to him, his position was sometimes protected by Chester A. Arthur, at that time a customs official who admired Melville's writing but never spoke to him. During this time, Melville was short-tempered because of nervous exhaustion, physical pain, and drinking. He would sometimes mistreat his family and servants in his unpredictable mood swings. Robertson-Lorant compared Melville's behavior to the "tyrannical captains he had portrayed in his novels".

In 1867, his oldest son died at home at the age of 18 from a self-inflicted gun shot. Historians and psychologists disagree on if it was intentional or accidental. In May 1867, Lizzie's brother plotted to help her leave Melville without suffering the consequences divorce carried at the time, particularly the loss of all claim to her children. His plan was for Lizzie to visit Boston, and friends would inform Melville she would not come back. To get a divorce, she would then have to bring charges against Melville, asserting her husband to be insane, but she ultimately decided against pursuing a divorce.

Though Melville's professional writing career had ended, he remained dedicated to his writing. He spent years on what Milder called "his autumnal masterpiece" "Clarel: A Poem and a Pilgrimage", an 18,000-line epic poem inspired by his 1856 trip to the Holy Land. It is among the longest single poems in American literature. The title character is a young American student of divinity who travels to Jerusalem to renew his faith. One of the central characters, Rolfe, is similar to Melville in his younger days, a seeker and adventurer, while the reclusive Vine is loosely based on Hawthorne, who had died twelve years before. Publication of 350 copies was funded with a bequest from his uncle in 1876, but sales failed miserably and the unsold copies were burned when Melville was unable to buy them at cost. Critic Lewis Mumford found an unread copy in the New York Public Library in 1925 "with its pages uncut".

Although Melville's own finances remained limited, in 1884, Lizzie received a legacy that enabled him to buy a steady stream of books and prints each month. Melville retired on December 31, 1885, after several of his wife's relatives further supported the couple with supplementary legacies and inheritances. On February 22, 1886, Stanwix Melville died in San Francisco at age 36, apparently from tuberculosis. In 1889, Melville became a member of the New York Society Library.

Melville had a modest revival of popularity in England when readers rediscovered his novels in the late 19th century. A series of poems inspired by his early experiences at sea, with prose head notes, were published in two collections for his relatives and friends, each with a print run of 25 copies. The first, "John Marr and Other Sailors", was published in 1888, followed by "Timoleon" in 1891.
He died the morning of September 28, 1891. His death certificate shows "cardiac dilation" as the cause. He was interred in the Woodlawn Cemetery in the Bronx, New York City. "The New York Times" obituary mistakenly called his masterpiece ""Mobie Dick"", erroneously implying that he and his books were unappreciated at his time of death. A later article was published on October 6 in the same paper, referring to him as "the late Hiram Melville", but this appears to have been a typesetting error.

Melville left a volume of poetry, "Weeds and Wildings", and a sketch, "Daniel Orme", unpublished at the time of his death. His wife also found pages for an unfinished novella, titled "Billy Budd". Melville had revised and rearranged the manuscript in several stages, leaving the pages in disarray. Lizzie could not decide her husband's intentions (or even read his handwriting in some places) and abandoned attempts to edit the manuscript for publication. The pages were stored in a family breadbox until 1919 when Melville's granddaughter gave them to Raymond Weaver. Weaver, who initially dismissed the work's importance, published a quick transcription in 1924. This version, however, contained many misreadings, some of which affected interpretation. It was an immediate critical success in England, then in the United States. In 1962, the Melville scholars Harrison Hayford and Merton M. Sealts published a critical reading text that was widely accepted. It was adapted as a stage play on Broadway in 1951, then an opera, and in 1961 as a film.

Melville's writing style shows both consistencies and enormous changes throughout the years. His development "had been abnormally postponed, and when it came, it came with a rush and a force that had the menace of quick exhaustion in it". As early as "Fragments from a Writing Desk", written when Melville was 20, scholar Sealts sees "a number of elements that anticipate Melville's later writing, especially his characteristic habit of abundant literary allusion". "Typee" and "Omoo" were documentary adventures that called for a division of the narrative in short chapters. Such compact organization bears the risk of fragmentation when applied to a lengthy work such as "Mardi", but with "Redburn" and "White Jacket," Melville turned the short chapter into a concentrated narrative.

Some chapters of "Moby-Dick" are no more than two pages in standard editions, and an extreme example is Chapter 122, consisting of a single paragraph of 36 words. The skillful handling of chapters in "Moby-Dick" is one of the most fully developed Melvillean signatures, and is a measure of his masterly writing style. Individual chapters have become "a touchstone for appreciation of Melville's art and for explanation" of his themes. In contrast, the chapters in "Pierre", called Books, are divided into short-numbered sections, seemingly an "odd formal compromise" between Melville's natural length and his purpose to write a regular romance that called for longer chapters. As satirical elements were introduced, the chapter arrangement restores "some degree of organization and pace from the chaos". The usual chapter unit then reappears for "Israel Potter", "The Confidence-Man" and even "Clarel", but only becomes "a vital part in the whole creative achievement" again in the juxtaposition of accents and of topics in "Billy Budd".

Newton Arvin points out that only superficially the books after "Mardi" seem as if Melville's writing went back to the vein of his first two books. In reality, his movement "was not a retrograde but a spiral one", and while "Redburn" and "White-Jacket" may lack the spontaneous, youthful charm of his first two books, they are "denser in substance, richer in feeling, tauter, more complex, more connotative in texture and imagery". The rhythm of the prose in "Omoo" "achieves little more than easiness; the language is almost neutral and without idiosyncrasy", while "Redburn" shows an improved ability in narrative which fuses imagery and emotion.

Melville's early works were "increasingly baroque" in style, and with "Moby-Dick" Melville's vocabulary had grown superabundant. Bezanson calls it an "immensely varied style". According to critic Warner Berthoff, three characteristic uses of language can be recognized. First, the exaggerated repetition of words, as in the series "pitiable," "pity," "pitied," and "piteous" (Ch. 81, "The Pequod Meets the Virgin"). A second typical device is the use of unusual adjective-noun combinations, as in "concentrating brow" and "immaculate manliness" (Ch. 26, "Knights and Squires"). A third characteristic is the presence of a participial modifier to emphasize and to reinforce the already established expectations of the reader, as the words "preluding" and "foreshadowing" ("so still and subdued and yet somehow preluding was all the scene ..." "In this foreshadowing interval ...").
After his use of hyphenated compounds in "Pierre", Melville's writing gives Berthoff the impression of becoming less exploratory and less provocative in his choices of words and phrases. Instead of providing a lead "into possible meanings and openings-out of the material in hand," the vocabulary now served "to crystallize governing impressions," the diction no longer attracted attention to itself, except as an effort at exact definition. The language, Berthoff continues, reflects a "controlling intelligence, of right judgment and completed understanding". The sense of free inquiry and exploration which infused his earlier writing and accounted for its "rare force and expansiveness," tended to give way to "static enumeration". By comparison to the verbal music and kinetic energy of "Moby-Dick", Melville's subsequent writings seem "relatively muted, even withheld" in his later works.

Melville's paragraphing in his best work Berthoff considers to be the virtuous result of "compactness of form and free assembling of unanticipated further data", such as when the mysterious sperm whale is compared with Exodus's invisibility of God's face in the final paragraph of Chapter 86 ("The Tail"). Over time Melville's paragraphs became shorter as his sentences grew longer, until he arrived at the "one-sentence paragraphing characteristic of his later prose". Berthoff points to the opening chapter of "The Confidence-Man" for an example, as it counts fifteen paragraphs, seven of which consist of only one elaborate sentence, and four that have only two sentences. The use of similar technique in "Billy Budd" contributes in large part, Berthoff says, to its "remarkable narrative economy".

In Nathalia Wright's view, Melville's sentences generally have a looseness of structure, easy to use for devices as catalogue and allusion, parallel and refrain, proverb and allegory. The length of his clauses may vary greatly, but the narrative style of writing in "Pierre" and "The Confidence-Man" is there to convey feeling, not thought. Unlike Henry James, who was an innovator of sentence ordering to render the subtlest nuances in thought, Melville made few such innovations. His domain is the mainstream of English prose, with its rhythm and simplicity influenced by the King James Bible. Another important characteristic of Melville's writing style is in its echoes and overtones. Melville's imitation of certain distinct styles is responsible for this. His three most important sources, in order, are the Bible, Shakespeare, and Milton. Direct quotation from any of the sources is slight; only one sixth of his Biblical allusions can be qualified as such because Melville adapts Biblical usage to his own narrated textual requirements of clarifying his plot.

In terms of Biblical influence, Melville's style can be divided into three categories. First, Melville's use of Biblical allusion is more at the narrative level of including the allusions within his own writing style rather that formally identifying Biblical quotation. Several uses of his preferred Biblical allusions appear appear repeated several times throughout his body of work, taking on the nature of refrains. Examples of this idiom are the injunctions to be 'as wise as serpents and as harmless as doves,' 'death on a pale horse,' 'the man of sorrows', the 'many mansions of heaven;' proverbs 'as the hairs on our heads are numbered,' 'pride goes before a fall,' 'the wages of sin is death;' adverbs and pronouns as 'verily, whoso, forasmuch as; phrases as come to pass, children's children, the fat of the land, vanity of vanities, outer darkness, the apple of his eye, Ancient of Days, the rose of Sharon.' Second, there are paraphrases of individual and combined verses. Redburn's "Thou shalt not lay stripes upon these Roman citizens" makes use of language of the Ten Commandments in Ex.20, and Pierre's inquiry of Lucy: "Loveth she me with the love past all understanding?" combines John 21:15–17 and Philippians 4:7. Third, certain Hebraisms are used, such as a succession of genitives ("all the waves of the billows of the seas of the boisterous mob"), the cognate accusative ("I dreamed a dream," "Liverpool was created with the Creation"), and the parallel ("Closer home does it go than a rammer; and fighting with steel is a play without ever an interlude"). A passage from "Redburn" shows how all these different ways of alluding interlock and result in a fabric texture of Biblical language, though there is very little direct quotation:

In addition to this, Melville successfully imitates three Biblical strains: the apocalyptic, the prophetic and the sermonic narrative tone of writing. Melville sustains the apocalyptic tone of anxiety and foreboding for a whole chapter of "Mardi." The prophetic strain is expressed by Melville in "Moby-Dick", most notably in Father Mapple's sermon. The tradition of the Psalms is imitated at length by Melville in "The Confidence-Man".

In 1849, Melville acquired an edition of Shakespeare's works printed in a font large enough for his tired eyes, which led to a deeper study of Shakespeare that greatly influenced the style of his next book, "Moby-Dick" (1851). The critic F. O. Matthiessen found that the language of Shakespeare far surpasses other influences upon the book, in that it inspired Melville to discover his own full strength. On almost every page, debts to Shakespeare can be discovered. The "mere sounds, full of Leviathanism, but signifying nothing" at the end of "Cetology" (Ch. 32) echo the famous phrase in "Macbeth:" "Told by an idiot, full of sound and fury/ Signifying nothing". Ahab's first extended speech to the crew, in the "Quarter-Deck" (Ch. 36) is practically blank verse and so is Ahab's soliloquy at the beginning of "Sunset" (Ch. 37):'I leave a white and turbid wake;/ Pale waters, paler cheeks, where'er I sail./ The envious billows sidelong swell to whelm/ My track; let them; but first I pass.' Through Shakespeare, Melville infused "Moby-Dick" with a power of expression he had not previously expressed. Reading Shakespeare had been "a catalytic agent" for Melville, one that transformed his writing from merely reporting to "the expression of profound natural forces". The extent to which Melville assimilated Shakespeare is evident in the description of Ahab, Matthiessen continues, which ends in language that seems Shakespearean yet is no imitation: 'Oh, Ahab! what shall be grand in thee, it must needs be plucked from the skies and dived for in the deep, and featured in the unbodied air!' The imaginative richness of the final phrase seems particularly Shakespearean, "but its two key words appear only once each in the plays...and to neither of these usages is Melville indebted for his fresh combination". Melville's diction depended upon no source, and his prose is not based on anybody else's verse but on an awareness of "speech rhythm".

Melville's mastering of Shakespeare, Matthiessen finds, supplied him with verbal resources that enabled him to create dramatic language through three essential techniques. First, the use of verbs of action creates a sense of movement and meaning. The effective tension caused by the contrast of "thou launchest navies of full-freighted worlds" and "there's that in here that still remains indifferent" in "The Candles" (Ch. 119) makes the last clause lead to a "compulsion to strike the breast," which suggests "how thoroughly the drama has come to inhere in the words;" Second, Melville took advantage of the Shakespearean energy of verbal compounds, as in "full-freighted". Third, Melville employed the device of making one part of speech act as another, for example, 'earthquake' as an adjective, or turning an adjective into a noun, as in "placeless".

Melville's style, in Nathalia Wright's analysis, seamlessly flows over into theme, because all these borrowings have an artistic purpose, which is to suggest an appearance "larger and more significant than life" for characters and themes that are in fact unremarkable. The allusions suggest that beyond the world of appearances another world exists, one that influences this world, and where ultimate truth can be found. Moreover, the ancient background thus suggested for Melville's narratives –ancient allusions being next in number to the Biblical ones –invests them with a sense of timelessness.

Melville was not financially successful as a writer; over his entire lifetime Melville's writings earned him just over $10,000 (). Melville's travelogues based on voyages to the South Seas and stories based on his time in the merchant marine and navy led to some initial success, but his popularity declined dramatically afterwards. By 1876, all of his books were out of print. He was viewed as a minor figure in American literature in the later years of his life and during the years immediately after his death.

Melville did not publish poetry until his late thirties, with "Battle-Pieces" (1866) and did not receive recognition as a poet until well into the 20th century. But he wrote predominantly poetry for about 25 years, twice as long as his prose career. The three novels of the 1850s that Melville worked on most seriously to present his philosophical explorations, "Moby-Dick", "Pierre", and "The Confidence Man", seem to make the step to philosophical poetry a natural one rather than simply a consequence of commercial failure. Since he turned to poetry as a meditative practice, his poetic style, even more than most Victorian poets, was not marked by linguistic play or melodic considerations.

Early critics were not sympathetic. Henry Chapin, in his introduction to "John Marr and Other Poems" (1922), one of the earlier selections of Melville's poetry, said Melville's verse is "of an amateurish and uneven quality" but in it "that loveable freshness of personality, which his philosophical dejection never quenched, is everywhere in evidence," in "the voice of a true poet". The poet and novelist Robert Penn Warren became a champion of Melville as a great American poet and issued a selection of Melville's poetry in 1971 prefaced by an admiring critical essay. In the 1990s critic Lawrence Buell argued that Melville "is justly said to be nineteenth-century America's leading poet after Whitman and Dickinson." and Helen Vendler remarked of "Clarel": "What it cost Melville to write this poem makes us pause, reading it. Alone, it is enough to win him, as a poet, what he called 'the belated funeral flower of fame'." Some critics now place him as the first modernist poet in the United States while others assert that his work more strongly suggests what today would be a postmodern view.

The centennial of Melville's birth in 1919 coincided with a renewed interest in his writings known as the Melville revival where his work experienced a significant critical reassessment. The renewed appreciation began in 1917 with Carl Van Doren's article on Melville in a standard history of American literature. Van Doren also encouraged Raymond Weaver, who wrote the author's first full-length biography, "Herman Melville: Mariner and Mystic" (1921). Discovering the unfinished manuscript of "Billy Budd", among papers shown to him by Melville's granddaughter, Weaver edited it and published it in a new collected edition of Melville's works. Other works that helped fan the flames for Melville were Carl Van Doren's "The American Novel" (1921), D.H. Lawrence's "Studies in Classic American Literature" (1923), Carl Van Vechten's essay in "The Double Dealer" (1922), and Lewis Mumford's biography "Herman Melville" (1929).
Starting in the mid-1930s, the Yale University scholar Stanley Thomas Williams supervised more than a dozen dissertations on Melville that were eventually published as books. Where the first wave of Melville scholars focused on psychology, Williams' students were prominent in establishing Melville Studies as an academic field concerned with texts and manuscripts, tracing Melville's influences and borrowings (even plagiarism), and exploring archives and local publications. To provide historical evidence, the independent scholar Jay Leyda searched libraries, family papers, local archives and newspapers across New England and New York to document Melville's life day by day for his two-volume "The Melville Log" (1951). Sparked by Leyda and post-war scholars, the second phase of the Melville Revival emphasized research into the biography of Melville rather than accepting Melville's early books as reliable accounts.
In 1945, The Melville Society was founded, a non-profit organisation dedicated to the study of Melville's life and works. Between 1969 and 2003 it published 125 issues of "Melville Society Extracts", which are now freely available on the society's website. Since 1999 it has published "Leviathan: A Journal of Melville Studies", currently three issues a year, published by Johns Hopkins University Press.

The postwar scholars tended to think that Weaver, Harvard psychologist Henry Murray, and Mumford favored Freudian interpretations which read Melville's fiction too literally as autobiography; exaggerated his suffering in the family; and inferred a homosexual attachment to Hawthorne. They saw a different arc to Melville's writing career. The first biographers saw a tragic withdrawal after the cold critical reception for his prose works and largely dismissed his poetry. A new view emerged of Melville's turn to poetry as a conscious choice that placed him among the most important American poets. Other post-war studies, however, continued the broad imaginative and interpretive style; Charles Olson's "Call Me Ishmael" (1947) presented Ahab as a Shakespearean tragic hero, and Newton Arvin's critical biography, "Herman Melville" (1950), won the National Book Award for non-fiction in 1951.

In the 1960s, Harrison Hayford organized an alliance between Northwestern University Press and the Newberry Library, with backing from the Modern Language Association and funding from the National Endowment for the Humanities, to edit and publish reliable critical texts of Melville's complete works, including unpublished poems, journals, and correspondence. The first volume of the Northwestern-Newberry Edition of the Writings of Herman Melville was published in 1968 and the last in the fall of 2017. The aim of the editors was to present a text "as close as possible to the author's intention as surviving evidence permits". The volumes have extensive appendices, including textual variants from each of the editions published in Melville's lifetime, an historical note on the publishing history and critical reception, and related documents. Because the texts were prepared with financial support from the United States Department of Education, no royalties are charged, and they have been widely reprinted. Hershel Parker published his two-volume "Herman Melville: A Biography", in 1996 and 2002, based on extensive original research and his involvement as editor of the Northwestern-Newberry Melville edition.

Melville's writings did not attract the attention of women's studies scholars of the 1970s and 1980s, though his preference for sea-going tales that involved almost only males has been of interest to scholars in men's studies and especially gay and queer studies. Melville was remarkably open in his exploration of sexuality of all sorts. For example, Alvin Sandberg claimed that the short story "The Paradise of Bachelors and the Tartarus of Maids" offers "an exploration of impotency, a portrayal of a man retreating to an all-male childhood to avoid confrontation with sexual manhood," from which the narrator engages in "congenial" digressions in heterogeneity. In line with this view, Warren Rosenberg argues the homosocial "Paradise of Bachelors" is shown to be "superficial and sterile".

David Harley Serlin observes in the second half of Melville's diptych, "The Tartarus of Maids", the narrator gives voice to the oppressed women he observes:

In the end Serlin says that the narrator is never fully able to come to terms with the contrasting masculine and feminine modalities.

Issues of sexuality have been observed in other works as well. Rosenberg notes Taji, in "Mardi", and the protagonist in "Pierre" "think they are saving young 'maidens in distress' (Yillah and Isabel) out of the purest of reasons but both are also conscious of a lurking sexual motive". When Taji kills the old priest holding Yillah captive, he says,

In "Pierre," the motive of the protagonist's sacrifice for Isabel is admitted: "womanly beauty and not womanly ugliness invited him to champion the right". Rosenberg argues,

Rosenberg says that Melville fully explores the theme of sexuality in his major epic poem, "Clarel". When the narrator is separated from Ruth, with whom he has fallen in love, he is free to explore other sexual (and religious) possibilities before deciding at the end of the poem to participate in the ritualistic order represented by marriage. In the course of the poem, "he considers every form of sexual orientation – celibacy, homosexuality, hedonism, and heterosexuality – raising the same kinds of questions as when he considers Islam or Democracy".

Some passages and sections of Melville's works demonstrate his willingness to address all forms of sexuality, including the homoerotic, in his works. Commonly noted examples from "Moby-Dick" are the "marriage bed" episode involving Ishmael and Queequeg, which is interpreted as male bonding; and the "Squeeze of the Hand" chapter, describing the camaraderie of sailors' extracting spermaceti from a dead whale presented in Chapter Ten of the novel titled "A Bosom Friend". Rosenberg notes that critics say that "Ahab's pursuit of the whale, which they suggest can be associated with the feminine in its shape, mystery, and in its naturalness, represents the ultimate fusion of the epistemological and sexual quest". In addition, he notes that Billy Budd's physical attractiveness is described in quasi-feminine terms: "As the Handsome Sailor, Billy Budd's position aboard the seventy-four was something analogous to that of a rustic beauty transplanted from the provinces and brought into competition with the highborn dames of the court".

Since the late 20th century, "Billy Budd" has become a central text in the field of legal scholarship known as law and literature. In the novel, Billy, a handsome and popular young sailor, is impressed from the merchant vessel "Rights of Man" to serve aboard H.M.S. "Bellipotent" in the late 1790s, during the war between Revolutionary France and Great Britain. He excites the enmity and hatred of the ship's master-at-arms, John Claggart. Claggart brings phony charges against Billy, accusing him of mutiny and other crimes, and the Captain, the Honorable Edward Fairfax Vere, brings them together for an informal inquiry. At this encounter, Billy is frustrated by his stammer, which prevents him from speaking, and strikes Claggart. The blow catches Claggart squarely on the forehead and, after a gasp or two, the master-at-arms dies. This death sets up the narrative climax of the novel; Vere immediately convenes a court-martial at which he urges the court to convict and sentence Billy to death.

The climactic trial has been the focus of scholarly inquiry regarding the motives of Vere and the legal necessity of Billy's condemnation. Vere states, given the circumstances of Claggart's slaying, condemning Billy to death would be unjust. While critics have viewed Vere as a character caught between the pressures between unbending legalism and malleable moral principles, other critics have differed in opinion. Such other critics have argued that Vere represents a ressentient protagonist whose disdain for Lord Admiral Nelson he takes out on Billy, in whom Vere sees the traits of Nelson's that he resents. argues that Vere manipulated and misrepresented the applicable laws in order to condemn Billy, showing that the laws of the time did not require a sentence of death and that legally any such sentence required review before being carried out. While this argument has been criticized for drawing on information outside the novel, Weisberg also shows that sufficient liberties existed in the laws Melville describes to avoid a capital sentence.

Melville's work often touched on themes of communicative expression and the pursuit of the absolute among illusions. As early as 1839, in the juvenile sketch "Fragments from a Writing Desk," Melville explores a problem which would reappear in the short stories "Bartleby" (1853) and "Benito Cereno" (1855): the impossibility to find common ground for mutual communication. The sketch centers on the protagonist and a mute lady, leading scholar Sealts to observe: "Melville's deep concern with expression and communication evidently began early in his career".

According to scholar Nathalia Wright, Melville's characters are all preoccupied by the same intense, superhuman and eternal quest for "the absolute amidst its relative manifestations," an enterprise central to the Melville canon: "All Melville's plots describe this pursuit, and all his themes represent the delicate and shifting relationship between its truth and its illusion". It is not clear, however, what the moral and metaphysical implications of this quest are, because Melville did not distinguish between these two aspects. Throughout his life Melville struggled with and gave shape to the same set of epistemological doubts and the metaphysical issues these doubts engendered. An obsession for the limits of knowledge led to the question of God's existence and nature, the indifference of the universe, and the problem of evil.

In 1982, the Library of America (LOA) began publication. In honor of Melville's central place in American culture, the very first volume contained "Typee", "Omoo", and "Mardi". The first volumes published in 1983 and 1985 also contained Melville's work, in 1983 "Redburn", "White-Jacket", and "Moby-Dick" and in 1985 "Pierre", "Israel Potter", "The Confidence Man", "Tales", and "Billy Budd". LOA did not publish his complete poetry until 2019.

On August 1, 1984, as part of the Literary Arts Series of stamps, the United States Postal Service issued a 20-cent commemorative stamp to honor Melville. The setting for the first day of issue was the Whaling Museum in New Bedford, Massachusetts.

In 1985, the New York City Herman Melville Society gathered at 104 East 26th Street to dedicate the intersection of Park Avenue South and 26th Street as Herman Melville Square. This is the street where Melville lived from 1863 to 1891 and where, among other works, he wrote "Billy Budd". Melville's house in Lansingburgh, New York, houses the Lansingburgh Historical Society.

In 2010, a species of extinct giant sperm whale, "Livyatan melvillei", was named in honor of Melville. The paleontologists who discovered the fossil were fans of "Moby-Dick" and dedicated their discovery to the author.






</doc>
<doc id="13624" url="https://en.wikipedia.org/wiki?curid=13624" title="High fidelity">
High fidelity

High fidelity (often shortened to hi-fi or hifi) is a term used by listeners, audiophiles and home audio enthusiasts to refer to high-quality reproduction of sound. This is in contrast to the lower quality sound produced by inexpensive audio equipment, AM radio, or the inferior quality of sound reproduction that can be heard in recordings made until the late 1940s.

Ideally, high-fidelity equipment has inaudible noise and distortion, and a flat (neutral, uncolored) frequency response within the human hearing range.

Bell Laboratories began experimenting with a range of recording techniques in the early 1930s. Performances by Leopold Stokowski and the Philadelphia Orchestra were recorded in 1931 and 1932 using telephone lines between the Academy of Music in Philadelphia and the Bell labs in New Jersey. Some multitrack recordings were made on optical sound film, which led to new advances used primarily by MGM (as early as 1937) and Twentieth Century Fox Film Corporation (as early as 1941). RCA Victor began recording performances by several orchestras using optical sound around 1941, resulting in higher-fidelity masters for 78-rpm discs. During the 1930s, Avery Fisher, an amateur violinist, began experimenting with audio design and acoustics. He wanted to make a radio that would sound like he was listening to a live orchestra—that would achieve high fidelity to the original sound. After World War II, Harry F. Olson conducted an experiment whereby test subjects listened to a live orchestra through a hidden variable acoustic filter. The results proved that listeners preferred high-fidelity reproduction, once the noise and distortion introduced by early sound equipment was removed.

Beginning in 1948, several innovations created the conditions that made major improvements of home-audio quality possible:

In the 1950s, audio manufacturers employed the phrase "high fidelity" as a marketing term to describe records and equipment intended to provide faithful sound reproduction. While some consumers simply interpreted "high fidelity" as fancy and expensive equipment, many found the difference in quality compared to the then-standard AM radios and 78-rpm records readily apparent and bought high-fidelity phonographs and 33⅓ LPs such as RCA's New Orthophonics and London's ffrr (Full Frequency Range Recording, a UK Decca system). Audiophiles paid attention to technical characteristics and bought individual components, such as separate turntables, radio tuners, preamplifiers, power amplifiers and loudspeakers. Some enthusiasts even assembled their own loudspeaker systems. In the 1950s, "hi-fi" became a generic term for home sound equipment, to some extent displacing "phonograph" and "record player".

In the late 1950s and early 1960s, the development of stereophonic equipment and recodings led to the next wave of home-audio improvement, and in common parlance "stereo" displaced "hi-fi". Records were now played on "a stereo". In the world of the audiophile, however, the concept of "high fidelity" continued to refer to the goal of highly accurate sound reproduction and to the technological resources available for approaching that goal. This period is regarded as the "Golden Age of Hi-Fi", when vacuum tube equipment manufacturers of the time produced many models considered endearing by modern audiophiles, and just before solid state (transistorized) equipment was introduced to the market, subsequently replacing tube equipment as the mainstream technology.

The metal-oxide-semiconductor field-effect transistor (MOSFET) was adapted into a power MOSFET for audio by Jun-ichi Nishizawa at Tohoku University in 1974. Power MOSFETs were soon manufactured by Yamaha for their hi-fi audio amplifiers. JVC, Pioneer Corporation, Sony and Toshiba also began manufacturing amplifiers with power MOSFETs in 1974. In 1977, Hitachi introduced the LDMOS (lateral diffused MOS), a type of power MOSFET. Hitachi was the only LDMOS manufacturer between 1977 and 1983, during which time LDMOS was used in audio power amplifiers from manufacturers such as HH Electronics (V-series) and Ashly Audio, and were used for music and public address systems. Class-D amplifiers became successful in the mid-1980s when low-cost, fast-switching MOSFETs were made available. Many transistor amps use MOSFET devices in their power sections, because their distortion curve is more tube-like.

A popular type of system for reproducing music beginning in the 1970s was the integrated music centre—which combined a phonograph turntable, AM-FM radio tuner, tape player, preamplifier, and power amplifier in one package, often sold with its own separate, detachable or integrated speakers. These systems advertised their simplicity. The consumer did not have to select and assemble individual components or be familiar with impedance and power ratings. Purists generally avoid referring to these systems as high fidelity, though some are capable of very good quality sound reproduction. 

Audiophiles in the 1970s and 1980s preferred to buy each component separately. That way, they could choose models of each component with the specifications that they desired. In the 1980s, a number of audiophile magazines became available, offering reviews of components and articles on how to choose and test speakers, amplifiers and other components.

Listening tests are used by hi-fi manufacturers, audiophile magazines and audio engineering researchers and scientists. If a listening test is done in such a way that the listener who is assessing the sound quality of a component or recording can see the components that are being used for the test (e.g., the same musical piece listened to through a tube power amplifier and a solid-state amplifier), then it is possible that the listener's pre-existing biases towards or against certain components or brands could affect their judgment. To respond to this issue, researchers began to use blind tests, in which listeners cannot see the components being tested. A commonly used variant of this test is the ABX test. A subject is presented with two known samples (sample "A", the reference, and sample "B", an alternative), and one unknown sample "X," for three samples total. "X" is randomly selected from "A" and "B", and the subject identifies "X" as being either "A" or "B". Although there is no way to prove that a certain methodology is transparent, a properly conducted double-blind test can prove that a method is "not" transparent.

Blind tests are sometimes used as part of attempts to ascertain whether certain audio components (such as expensive, exotic cables) have any subjectively perceivable effect on sound quality. Data gleaned from these blind tests is not accepted by some audiophile magazines such as "Stereophile" and "The Absolute Sound" in their evaluations of audio equipment. John Atkinson, current editor of "Stereophile", stated that he once purchased a solid-state amplifier, the Quad 405, in 1978 after seeing the results from blind tests, but came to realize months later that "the magic was gone" until he replaced it with a tube amp. Robert Harley of "The Absolute Sound" wrote, in 2008, that: "...blind listening tests fundamentally distort the listening process and are worthless in determining the audibility of a certain phenomenon."

Doug Schneider, editor of the online Soundstage network, refuted this position with two editorials in 2009. He stated: "Blind tests are at the core of the decades' worth of research into loudspeaker design done at Canada's National Research Council (NRC). The NRC researchers knew that for their result to be credible within the scientific community and to have the most meaningful results, they had to eliminate bias, and blind testing was the only way to do so." Many Canadian companies such as Axiom, Energy, Mirage, Paradigm, PSB and Revel use blind testing extensively in designing their loudspeakers. Audio professional Dr. Sean Olive of Harman International shares this view.

Stereophonic sound provided a partial solution to the problem of creating the illusion of live orchestral performers by creating a phantom middle channel when the listener sits exactly in the middle of the two front loudspeakers. When the listener moves to the side, however, this phantom channel disappears or is greatly reduced. An attempt to provide for the reproduction of the reverberation was tried in the 1970s through quadraphonic sound. Consumers did not want to pay the additional costs and space required for the marginal improvements in realism. With the rise in popularity of home theater, however, multi-channel playback systems became popular, and many consumers were willing to tolerate the six to eight channels required in a home theater.

In addition to spatial realism, the playback of music must be subjectively free from noise, such as hiss or hum, to achieve realism. The compact disc (CD) provides about 90 decibels of dynamic range, which exceeds the 80 dB dynamic range of music as normally perceived in a concert hall. Audio equipment must be able to reproduce frequencies high enough and low enough to be realistic. The human hearing range, for healthy young persons, is 20 Hz to 20,000 Hz.

"Integrated", "mini", or "lifestyle" systems (also known by the older terms "music centre" or "midi system") contain one or more sources such as a CD player, a tuner, or a cassette deck together with a preamplifier and a power amplifier in one box. Although some high-end manufacturers do produce integrated systems, such products are generally disparaged by audiophiles, who prefer to build a system from "separates" (or "components"), often with each item from a different manufacturer specialising in a particular component. This provides the most flexibility for piece-by-piece upgrades and repairs.

For slightly less flexibility in upgrades, a preamplifier and a power amplifier in one box is called an "integrated amplifier"; with a tuner, it is a "receiver". A monophonic power amplifier, which is called a "monoblock", is often used for powering a subwoofer. Other modules in the system may include components like cartridges, tonearms, hi-fi turntables, Digital Media Players, digital audio players, DVD players that play a wide variety of discs including CDs, CD recorders, MiniDisc recorders, hi-fi videocassette recorders (VCRs) and reel-to-reel tape recorders. Signal modification equipment can include equalizers and signal processors.

This modularity allows the enthusiast to spend as little or as much as they want on a component that suits their specific needs. In a system built from separates, sometimes a failure on one component still allows partial use of the rest of the system. A repair of an integrated system, though, means complete lack of use of the system. Another advantage of modularity is the ability to spend money on only a few core components at first and then later add additional components to the system. Some of the disadvantages of this approach are increased cost, complexity, and space required for the components.

In the 2000s, modern hi-fi equipment can include signal sources such as digital audio tape (DAT), digital audio broadcasting (DAB) or HD Radio tuners. Some modern hi-fi equipment can be digitally connected using fibre optic TOSLINK cables, universal serial bus (USB) ports (including one to play digital audio files), or Wi-Fi support. Another modern component is the "music server" consisting of one or more computer hard drives that hold music in the form of computer files. When the music is stored in an audio file format that is lossless such as FLAC, Monkey's Audio or WMA Lossless, the computer playback of recorded audio can serve as an audiophile-quality source for a hi-fi system. There is now a push from certain streaming services to offer hi-fi services. Streaming services typically have a modified dynamic range and possibly bit rates lower than audiophiles would be happy with. Tidal (service) has launched a hi-fi tier which includes access to FLAC and Master Quality Authenticated studio masters for many tracks through the desktop version of the player. This integration is also available for high-end audio systems.




</doc>
<doc id="13625" url="https://en.wikipedia.org/wiki?curid=13625" title="Holden">
Holden

Holden, formerly known as General Motors-Holden, is an Australian automobile marque and former automobile manufacturer, which manufactured cars in Australia before switching to importing cars under the Holden brand. It is headquartered in Port Melbourne.

The company was founded in 1856 as a saddlery manufacturer in South Australia. In 1908, it moved into the automotive field before later becoming a subsidiary of the United States–based General Motors (GM) in 1931, when the company was renamed General Motors-Holden's Ltd. It was renamed Holden Ltd in 1998, adopting the name GM Holden Ltd in 2005.

In the past, Holden has offered badge-engineered models due to sharing arrangements with Chevrolet, Isuzu, Nissan, Opel, Suzuki, Toyota, and Vauxhall Motors. In previous years, the vehicle lineup consisted of models from GM Korea, GM Thailand, GM North America, and self-developed models like the Holden Commodore, Holden Caprice, and the Holden Ute. Holden also distributed the European Opel brand in Australia in 2012 until its Australian demise in mid-2013.

Holden briefly owned assembly plants in New Zealand during the early 1990s. The plants had belonged to General Motors from 1926 until 1990 in an earlier and quite separate operation from GM's Holden investment in Australia. From 1994 to 2017, all Australian-built Holden vehicles were manufactured in Elizabeth, South Australia, and engines were produced at the Fishermans Bend plant in Melbourne. Historically, production or assembly plants were operated in all mainland states of Australia. The consolidation of final assembly at Elizabeth was completed in 1988, but some assembly operations continued at Dandenong until 1994.

Although Holden's involvement in exports has fluctuated since the 1950s, the declining sales of large cars in Australia led the company to look to international markets to increase profitability. From 2010, Holden incurred losses due to the strong Australian dollar, and reductions of government grants and subsidies. This led to the announcement, on 11 December 2013, that Holden would cease vehicle and engine production by the end of 2017. On 20 October 2017, the last existing vehicle plant, located in Elizabeth, was closed as the production of the Holden Commodore ended. On 17 February 2020, General Motors announced that the Holden brand would be retired by 2021.

In 1852, James Alexander Holden emigrated to South Australia from Walsall, England, and in 1856 established J.A. Holden & Co., a saddlery business in Adelaide. In 1879 J A Holden's eldest son Henry James (HJ) Holden, became a partner and effectively managed the company. In 1885, German-born H. A. Frost joined the business as a junior partner and J.A. Holden & Co became Holden & Frost Ltd. Edward Holden, James' grandson, joined the firm in 1905 with an interest in automobiles. From there, the firm evolved through various partnerships, and in 1908, Holden & Frost moved into the business of minor repairs to car upholstery. The company began to re-body older chassis using motor bodies produced by F T Hack and Co from 1914. Holden & Frost mounted the body, and painted and trimmed it. The company began to produce complete motorcycle sidecar bodies after 1913. After 1917, wartime trade restrictions led the company to start full-scale production of vehicle body shells. H.J. Holden founded a new company in late 1917, and registered Holden's Motor Body Builders Ltd (HMBB) on 25 February 1919, specialising in car bodies and using the former F T Hack & Co facility at 400 King William Street in Adelaide before erecting a large four-story factory on the site.

By 1923, HMBB were producing 12,000 units per year. During this time, HMBB assembled bodies for Ford Motor Company of Australia until its Geelong plant was completed. From 1924, HMBB became the exclusive supplier of car bodies for GM in Australia, with manufacturing taking place at the new Woodville plant. These bodies were made to suit a number of chassis imported from manufacturers including Austin, Buick, Chevrolet, Cleveland, Dodge, Essex, Fiat, Hudson, Oakland, Oldsmobile, Overland, Reo, Studebaker, and Willys-Knight.

In 1926, General Motors (Australia) Limited was established with assembly plants at Newstead, Queensland; Marrickville, New South Wales; City Road, Melbourne, Victoria; Birkenhead, South Australia; and Cottesloe, Western Australia using bodies produced by HMBB and imported complete knock down chassis. In 1930 alone, the still independent Woodville plant built bodies for Austin, Chrysler, DeSoto, Morris, Hillman, Humber, Hupmobile, and Willys-Overland, as well GM cars. The last of this line of business was the assembly of Hillman Minx sedans in 1948. The Great Depression led to a substantial downturn in production by Holden, from 34,000 units annually in 1930 to just 1,651 units one year later. In 1931, GM purchased HMBB and merged it with General Motors (Australia) Pty Ltd to form General Motors-Holden's Ltd (GM-H). It's acquisition of Holden allowed General Motors to inherit an Australian identity, which it used to cultivate nationalist appeal for the firm, largely through the use of public relations, a then novel form of business communcation which was imported to Australia through the formation of General Motors (Australia) Limited. Throughout the 1920s, Holden also supplied 60 W-class tramcar bodies to the Melbourne & Metropolitan Tramways Board, of which several examples have been preserved in both Australia and New Zealand.

Holden's second full-scale car factory, located in Fishermans Bend (Port Melbourne), was opened on 5 November 1936 by Prime Minister Joseph Lyons, with construction beginning in 1939 on a new plant in Pagewood, New South Wales. However, World War II delayed car production with efforts shifted to the construction of vehicle bodies, field guns, aircraft, and engines. Before the war ended, the Australian government took steps to encourage an Australian automotive industry. Both GM and Ford provided studies to the Australian government outlining the production of the first Australian-designed car. Ford's proposal was the government's first choice, but required substantial financial assistance. GM's study was ultimately chosen because of its low level of government intervention. After the war, Holden returned to producing vehicle bodies, this time for Buick, Chevrolet, Pontiac, and Vauxhall. The Oldsmobile Ace was also produced from 1946 to 1948.

From here, Holden continued to pursue the goal of producing an Australian car. This involved compromise with GM, as Holden's managing director, Laurence Hartnett, favoured development of a local design, while GM preferred to see an American design as the basis for "Australia's Own Car". In the end, the design was based on a previously rejected postwar Chevrolet proposal. The Holden was launched in 1948, creating long waiting lists extending through 1949 and beyond. The name "Holden" was chosen in honour of Sir Edward Holden, the company's first chairman and grandson of J.A. Holden. Other names considered were "GeM", "Austral", "Melba", "Woomerah", "Boomerang", "Emu", and "Canbra", a phonetic spelling of Canberra. Although officially designated "48–215", the car was marketed simply as the "Holden". The unofficial usage of the name "FX" originated within Holden, referring to the updated suspension on the 48–215 of 1953.

During the 1950s, Holden dominated the Australian car market. GM invested heavily in production capacity, which allowed the company to meet increased postwar demand for motor cars. Less expensive, four-cylinder cars did not offer Holdens the ability to deal with rugged rural areas. Holden 48–215 sedans were produced in parallel with the 50-2106 coupé utility from 1951; the latter was known colloquially as the "ute" and became ubiquitous in Australian rural areas as the workhorse of choice. Production of both the utility and sedan continued with minor changes until 1953, when they were replaced by the facelifted FJ model, introducing a third panel van body style. The FJ was the first major change to the Holden since its 1948 introduction. Over time, it gained iconic status and remains one of Australia's most recognisable automotive symbols. A new horizontally slatted grille dominated the front end of the FJ, which received various other trim and minor mechanical revisions. In 1954, Holden began exporting the FJ to New Zealand. Although little changed from the 48–215, marketing campaigns and price cuts kept FJ sales steady until a completely redesigned model was launched. At the 2005 Australian International Motor Show in Sydney, Holden paid homage to the FJ with the Efijy concept car. Commercial success underpinned the rise of Holden as a cultural icon, as the Holden car became synonymous with the 'Australian way of life', coming to symbolise the stability of post-war Australian capitalism.

Holden's next model, the FE, launched in 1956, offered in a new station wagon body style dubbed "Station Sedan" in the company's sales literature. In the same year, Holden commenced exports to Malaya, Thailand, and North Borneo. Strong sales continued in Australia, and Holden achieved a market share of more than 50% in 1958 with the revised FC model. This was the first Holden to be tested on the new "Holden Proving Ground" based in Lang Lang, Victoria. In 1957, Holden's export markets grew to 17 countries, with new additions including Indonesia, Hong Kong, Singapore, Fiji, Sudan, the East Africa region, and South Africa. Indonesian market cars were assembled locally by P.T. Udatin. The opening of the Dandenong, Victoria, production facility in 1956 brought further jobs; by 1959, Holden employed 19,000 workers country-wide. In 1959, complete knock-down assembly began in South Africa and Indonesia.

In 1960, Holden introduced its third major new model, the FB. The car's style was inspired by 1950s Chevrolets, with tailfins and a wrap-around windscreen with "dog leg" A-pillars. By the time it was introduced, many considered the appearance dated. Much of the motoring industry at the time noted that the adopted style did not translate well to the more compact Holden. The FB became the first Holden that was adapted for left-hand drive markets, enhancing its export potential, and as such was exported to New Caledonia, New Hebrides, the Philippines, and Hawaii.
In 1960, Ford unveiled the new Falcon in Australia, only months after its introduction in the United States. To Holden's advantage, the Falcon was not durable, particularly in the front suspension, making it ill-suited for Australian conditions. In response to the Falcon, Holden introduced the facelifted EK series in 1961; the new model featured two-tone paintwork and optional Hydramatic automatic transmission. A restyled EJ series came in 1962, debuting the new luxury oriented Premier model. The EH update came a year later, bringing the new Red motor, providing better performance than the previous Grey motor. The HD series of 1965 had the introduction of the Powerglide automatic transmission. At the same time, an "X2" performance option with a more powerful version of the six-cylinder engine was made available. In 1966, the HR was introduced, including changes in the form of new front and rear styling and higher-capacity engines. More significantly, the HR fitted standard front seat belts; Holden thus became the first Australian automaker to provide the safety device as standard equipment across all models. This coincided with the completion of the production plant in Acacia Ridge, Queensland. By 1963, Holden was exporting cars to Africa, the Middle East, Southeast Asia, the Pacific Islands, and the Caribbean.

Holden began assembling the compact HA series Vauxhall Viva in 1964. This was superseded by the Holden Torana in 1967, a development of the Viva ending Vauxhall production in Australia. Holden offered the LC, a Torana with new styling, in 1969 with the availability of Holden's six-cylinder engine. In the development days, the six-cylinder Torana was reserved for motor racing, but research had shown a business case existed for such a model. The LC Torana was the first application of Holden's new three-speed Tri-Matic automatic transmission. This was the result of Holden's A$16.5 million transformation of the Woodville, South Australia, factory for its production.
Holden's association with the manufacture of Chevrolets and Pontiacs ended in 1968, coinciding with the year of Holden's next major new model, the HK . This included Holden's first V8 engine, a Chevrolet engine imported from Canada. Models based on the HK series included an extended-length prestige model, the Brougham; and a two-door coupé, the Monaro. The mainstream Holden Special was rebranded the Kingswood, and the basic fleet model, the Standard, became the Belmont. On 3 March 1969, Alexander Rhea, managing director of General Motors-Holden's at the time, was joined by press photographers and the Federal Minister of Shipping and Transport, Ian Sinclair as the two men drove the two-millionth Holden, an HK Brougham, off the production line. This came just over half a decade since the one-millionth car, an EJ Premier sedan, rolled off the Dandenong line on 25 October 1962. Following the Chevrolet V8 fitted to the HK, the first Australian-designed and mass-produced V8, the Holden V8 engine debuted in the Hurricane concept of 1969 before fitment to facelifted HT model. This was available in two capacities: and . Late in HT production, use of the new Tri-Matic automatic transmission, first seen in the LC Torana was phased in as Powerglide stock was exhausted, but Holden's official line was that the HG of 1971 was the first full-sized Holden to receive it.
Despite the arrival of serious competitors—namely, the Ford Falcon, Chrysler Valiant, and Japanese cars—in the 1960s, Holden's locally produced large six- and eight-cylinder cars remained Australia's top-selling vehicles. Sales were boosted by exporting the Kingswood sedan, station wagon, and utility body styles to Indonesia, Trinidad and Tobago, Pakistan, the Philippines, and South Africa in complete knock-down form.

Holden launched the new HQ series in 1971. At this time, the company was producing all of its passenger cars in Australia, and every model was of Australian design; however, by the end of the decade, Holden was producing cars based on overseas designs. The HQ was thoroughly re-engineered, featuring a perimeter frame and semimonocoque (unibody) construction. Other firsts included an all-coil suspension and an extended wheelbase for station wagons, while the utilities and panel vans retained the traditional coil/leaf suspension configuration. The series included the new prestige Statesman brand, which also had a longer wheelbase, replacing the Brougham. The Statesman remains noteworthy because it was not marketed as a "Holden", but rather a "Statesman".

The HQ framework led to a new generation of two-door Monaros, and despite the introduction of the similar-sized competitors, the HQ range became the top-selling Holden of all time, with 485,650 units sold in three years; 14,558 units were exported and 72,290  CKD kits were constructed. The HQ series was facelifted in 1974 with the introduction of the HJ, heralding new front-panel styling and a revised rear fascia. This new bodywork was to remain, albeit with minor upgrades, through the HX and HZ series. Detuned engines adhering to government emission standards were brought in with the HX series, whilst the HZ brought considerably improved road handling and comfort with the introduction of radial-tuned suspension. As a result of GM's toying with the Wankel rotary engine, as used by Mazda of Japan, an export agreement was initiated in 1975. This involved Holden exporting with powertrains, HJ, and later, HX series Premiers as the Mazda Roadpacer AP. Mazda then fitted these cars with the 13B rotary engine and three-speed automatic transmission. Production ended in 1977, after just 840 units sold.

Development of the Torana continued in with the larger mid-sized LH series released in 1974, offered only as a four-door sedan. The LH Torana was one of the few cars worldwide engineered to accommodate four-, six-, and eight-cylinder engines. This trend continued until Holden introduced the Sunbird in 1976, essentially the four-cylinder Torana with a new name. Designated LX, both the Sunbird and Torana introduced a three-door hatchback variant. A final UC update appeared in 1978. During its production run, the Torana achieved legendary racing success in Australia, achieving victories at the Mount Panorama Circuit in Bathurst, New South Wales.

In 1975, Holden introduced the compact Gemini, the Australian version of the "T-car", based on the Opel Kadett C. The Gemini was an overseas design developed jointly with Isuzu, GM's Japanese affiliate; and was powered by a 1.6-litre four-cylinder engine. Fast becoming a popular car, the Gemini rapidly attained sales leadership in its class, and the nameplate lived on until 1987.

Holden's most popular car to date, the Commodore, was introduced in 1978 as the VB. The new family car was loosely based on the Opel Rekord E body shell, but with the front from the Opel Senator grafted to accommodate the larger Holden six-cylinder and V8 engines. Initially, the Commodore maintained Holden's sales leadership in Australia. However, some of the compromises resulting from the adoption of a design intended for another market hampered the car's acceptance. In particular, it was narrower than its predecessor and its Falcon rival, making it less comfortable for three rear-seat passengers. With the abandonment of left-hand drive markets, Holden exported almost 100,000 Commodores to markets such as New Zealand, Thailand, Hong Kong, Malaysia, Indonesia, Malta and Singapore.

During the 1970s, Holden ran an advertising jingle "Football, Meat Pies, Kangaroos, and Holden cars", a localised version of the "Baseball, Hot Dogs, Apple Pies, and Chevrolet" jingle used by GM's Chevrolet division in the United States.

Holden discontinued the Torana in 1979 and the Sunbird in 1980. After the 1978 introduction of the Commodore, the Torana became the "in-between" car, surrounded by the smaller and more economical Gemini and the larger, more sophisticated Commodore. The closest successor to the Torana was the Camira, released in 1982 as Australia's version of GM's medium-sized "J-car".
The 1980s were challenging for Holden and the Australian automotive industry. The Australian Government tried to revive the industry with the Button car plan, which encouraged car makers to focus on producing fewer models at higher, more economical volumes, and to export cars. The decade opened with the shut-down of the Pagewood, New South Wales production plant and introduction of the light commercial Rodeo, sourced from Isuzu in Japan. The Rodeo was available in both two- and four-wheel drive chassis cab models with a choice of petrol and diesel powerplants. The range was updated in 1988 with the TF series, based on the Isuzu TF. Other cars sourced from Isuzu during the 1980s were the four-wheel drive Jackaroo (1981), the Shuttle (1982) van and the Piazza (1986) three-door sports hatchback. The second generation Holden Gemini from 1985 was also based on an Isuzu design, although, its manufacture was undertaken in Australia.

While GM Australia's commercial vehicle range had originally been mostly based on Bedford products, these had gradually been replaced by Isuzu products. This process began in the 1970s and by 1982 Holden's commercial vehicle arm no longer offered any Bedford products.

The new Holden WB commercial vehicles and the Statesman WB limousines were introduced in 1980. However, the designs, based on the HQ and updated HJ, HX and HZ models from the 1970s were less competitive than similar models in Ford's lineup. Thus, Holden abandoned those vehicle classes altogether in 1984. Sales of the Commodore also fell, with the effects of the 1979 energy crisis lessening, and for the first time the Commodore lost ground to the Ford Falcon. Sales in other segments also suffered when competition from Ford intensified, and other Australian manufacturers: Mitsubishi, Nissan and Toyota gained market share. When released in 1982, the Camira initially generated good sales, which later declined because buyers considered the 1.6-litre engine underpowered, and the car's build and ride quality below-average. The Camira lasted just seven years, and contributed to Holden's accumulated losses of over A$500 million by the mid-1980s.

In 1984, Holden introduced the VK Commodore, with significant styling changes from the previous VH. The Commodore was next updated in 1986 as the VL, which had new front and rear styling. Controversially, the VL was powered by the 3.0-litre Nissan "RB30" six-cylinder engine and had a Nissan-built, electronically controlled four-speed automatic transmission. Holden even went to court in 1984 to stop local motoring magazine "Wheels" from reporting on the matter. The engine change was necessitated by the legal requirement that all new cars sold in Australia after 1986 had to consume unleaded petrol. Because it was unfeasible to convert the existing six-cylinder engine to run on unleaded fuel, the Nissan engine was chosen as the best engine available. However, changing currency exchange rates doubled the cost of the engine and transmission over the life of the VL. The decision to opt for a Japanese-made transmission led to the closure of the Woodville, South Australia assembly plant. Confident by the apparent sign of turnaround, GM paid off Holden's mounted losses of A$780 million on 19 December 1986. At GM headquarters' request, Holden was then reorganised and recapitalised, separating the engine and car manufacturing divisions in the process. This involved the splitting of Holden into "Holden's Motor Company" (HMC) and "Holden's Engine Company" (HEC). For the most part, car bodies were now manufactured at Elizabeth, South Australia, with engines as before, confined to the Fishermans Bend plant in Port Melbourne, Victoria. The engine manufacturing business was successful, building four-cylinder "Family II" engines for use in cars built overseas. The final phase of the Commodore's recovery strategy involved the 1988 VN, a significantly wider model powered by the American-designed, Australian-assembled 3.8-litre Buick V6 engine.

Holden began to sell the subcompact Suzuki Swift-based Barina in 1985. The Barina was launched concurrently with the Suzuki-sourced Holden Drover, followed by the Scurry later on in 1985. In the previous year, Nissan Pulsar hatchbacks were rebadged as the Holden Astra, as a result of a deal with Nissan. This arrangement ceased in 1989 when Holden entered a new alliance with Toyota, forming a new company: United Australian Automobile Industries (UAAI). UAAI resulted in Holden selling rebadged versions of Toyota's Corolla and Camry, as the Holden Nova and Apollo respectively, with Toyota re-branding the Commodore as the Lexcen.

The company changed throughout the 1990s, increasing its Australian market share from 21 percent in 1991 to 28.2 percent in 1999. Besides manufacturing Australia's best selling car, which was exported in significant numbers, Holden continued to export many locally produced engines to power cars made elsewhere. In this decade, Holden adopted a strategy of importing cars it needed to offer a full range of competitive vehicles. During 1998, General Motors-Holden's Ltd name was shortened to "Holden Ltd".

On 26 April 1990, GM's New Zealand subsidiary Holden New Zealand announced that production at the assembly plant based in Trentham would be phased out and vehicles would be imported duty-free—this came after the 1984 closure of the Petone assembly line due to low output volumes. During the 1990s, Holden, other Australian automakers and trade unions pressured the Australian Government to halt the lowering of car import tariffs. By 1997, the federal government had already cut tariffs to 22.5 percent, from 57.5 percent ten years earlier; by 2000, a plan was formulated to reduce the tariffs to 15 percent. Holden was critical, saying that Australia's population was not large enough, and that the changes could tarnish the local industry.

Holden re-introduced its defunct Statesman title in 1990—this time under the Holden marque, as the Statesman and Caprice. For 1991, Holden updated the Statesman and Caprice with a range of improvements, including the introduction of four-wheel anti-lock brakes (ABS); although, a rear-wheel system had been standard on the Statesman Caprice from March 1976. ABS was added to the short-wheelbase Commodore range in 1992. Another returning variant was the full-size utility, and on this occasion it was based on the Commodore. The VN Commodore received a major facelift in 1993 with the VR—compared to the VN, approximately 80 percent of the car model was new. Exterior changes resulted in a smoother overall body and a "twin-kidney" grille—a Commodore styling trait that remained until the 2002 VY model and, as of 2013, remains a permanent staple on HSV variants.

Holden introduced the all-new VT Commodore in 1997, the outcome of a A$600 million development programme that spanned more than five years. The new model featured a rounded exterior body shell, improved handling and many firsts for an Australian-built car. Also, a stronger body structure increased crash safety. The locally produced Buick-sourced V6 engine powered the Commodore range, as did the 5.0-litre Holden V8 engine, and was replaced in 1999 by the 5.7-litre "LS" unit.

The UAAI badge-engineered cars first introduced in 1989 sold in far fewer numbers than anticipated, but the Holden Commodore, Toyota Camry, and Corolla were all successful when sold under their original nameplates. The first generation Nova and the donor Corolla were produced at Holden's Dandenong, Victoria facility until 1994. UAAI was dissolved in 1996, and Holden returned to selling only GM products. The Holden Astra and Vectra, both designed by Opel in Germany, replaced the Toyota-sourced Holden Nova and Apollo. This came after the 1994 introduction of the Opel Corsa replacing the already available Suzuki Swift as the source for the Holden Barina. Sales of the full-size Holden Suburban SUV sourced from Chevrolet commenced in 1998—lasting until 2001. Also in 1998, local assembly of the Vectra began at Elizabeth, South Australia. These cars were exported to Japan and Southeast Asia with Opel badges. However, the Vectra did not achieve sufficient sales in Australia to justify local assembly, and reverted to being fully imported in 2000.

Holden's market surge from the 1990s reversed in the 2000s decade. In Australia, Holden's market share dropped from 27.5 percent in 2000 to 15.2 percent in 2006. From March 2003, Holden no longer held the number one sales position in Australia, losing ground to Toyota.

This overall downturn affected Holden's profits; the company recorded a combined gain of A$842.9 million from 2002 to 2004, and a combined loss of A$290 million from 2005 to 2006. Factors contributing to the loss included the development of an all-new model, the strong Australian dollar and the cost of reducing the workforce at the Elizabeth plant, including the loss of 1,400 jobs after the closure of the third-shift assembly line in 2005, after two years in operation. Holden fared better in 2007, posting an A$6 million loss. This was followed by an A$70.2 million loss in the 2008, an A$210.6 million loss in 2009, and a profit of A$112 million in 2010. On 18 May 2005, "Holden Ltd" became "GM Holden Ltd", coinciding with the resettling to the new Holden headquarters on 191 Salmon Street, Port Melbourne, Victoria.

Holden caused controversy in 2005 with their Holden Employee Pricing television advertisement, which ran from October to December 2005. The campaign publicised, "for the first time ever, all Australians can enjoy the financial benefit of Holden Employee Pricing". However, this did not include a discounted dealer delivery fee and savings on factory fitted options and accessories that employees received. At the same time, employees were given a further discount of 25 to 29 percent on selected models.

Holden revived the Monaro coupe in 2001. Based on the VT Commodore architecture, the coupe attracted worldwide attention after being shown as a concept car at Australian auto shows. The VT Commodore received its first major update in 2002 with the VY series. A mildly facelifted VZ model launched in 2004, introducing the "High Feature" engine. This was built at the Fishermans Bend facility completed in 2003, with a maximum output of 900 engines per day. This has reportedly added A$5.2 billion to the Australian economy; exports account for about A$450 million alone. After the VZ, the "High Feature" engine powered the all-new Holden Commodore (VE). In contrast to previous models, the VE no longer used an Opel-sourced platform adapted both mechanically and in size, but was based on the GM Zeta platform that was earmarked to become a "Global RWD Architecture", until plans were cancelled due to the 2007/08 global financial crisis.

Throughout the 1990s, Opel had also been the source of many Holden models. To increase profitability, Holden looked to the South Korean Daewoo brand for replacements after acquiring a 44.6 percent stake—worth US$251 million—in the company in 2002 as a representative of GM. This was increased to 50.9 percent in 2005, but when GM further increased its stake to 70.1 percent around the time of its 2009 Chapter 11 reorganisation, Holden's interest was relinquished and transferred to another (undisclosed) part of GM.

The commencement of the Holden-branded Daewoo models began with the 2005 Holden Barina, which based on the Daewoo Kalos, replaced the Opel Corsa as the source of the Barina. In the same year, the Viva, based on the Daewoo Lacetti, replaced the entry-level Holden Astra Classic, although the new-generation Astra introduced in 2004 continued on. The Captiva crossover SUV came next in 2006. After discontinuing the Frontera and Jackaroo models in 2003, Holden was only left with one all-wheel drive model: the Adventra, a Commodore-based station wagon. The fourth model to be replaced with a South Korean alternative was the Vectra by the mid-size Epica in 2007. As a result of the split between GM and Isuzu, Holden lost the rights to use the "Rodeo" nameplate. Consequently, the Holden Rodeo was facelifted and relaunched as the Colorado in 2008. Following Holden's successful application for a A$149 million government grant to build a localised version of the Chevrolet Cruze in Australia from 2011, Holden in 2009 announced that it would initially import the small car unchanged from South Korea as the Holden Cruze.

Following the government grant announcement, Kevin Rudd, Australia's Prime Minister at the time, stated that production would support 600 new jobs at the Elizabeth facility; however, this failed to take into account Holden's previous announcement, whereby 600 jobs would be shed when production of the "Family II" engine ceased in late 2009. In mid-2013, Holden sought a further A$265 million, in addition to the A$275 million that was already committed by the governments of Canberra, South Australia and Victoria, to remain viable as a car manufacturer in Australia. A source close to Holden informed the "Australian" news publication that the car company is losing money on every vehicle that it produces and consequently initiated negotiations to reduce employee wages by up to A$200 per week to cut costs, following the announcement of 400 job cuts and an assembly line reduction of 65 (400 to 335) cars per day. From 2001 to 2012, Holden received over A$150 million a year in subsidy from Australian government. The subsidy from 2007 was more than Holden's capital investment of the same period. From 2004, Holden was only able to make a profit in 2010 and 2011.

In March 2012, Holden was given a $270 million lifeline by the Australian, South Australian and Victorian governments. In return, Holden planned to inject over $1 billion into car manufacturing in Australia. They estimated the new investment package would return around $4 billion to the Australian economy and see GM Holden continue making cars in Australia until at least 2022.

Industry Minister Kim Carr confirmed on 10 July 2013 that talks had been scheduled between the Australian government and Holden. On 13 August 2013, 1,700 employees at the Elizabeth plant in northern Adelaide voted to accept a three-year wage freeze in order to decrease the chances of the production line's closure in 2016. Holden's ultimate survival, though, depended on continued negotiations with the Federal Government—to secure funding for the period from 2016 to 2022—and the final decision of the global headquarters in Detroit, US.

Following an unsuccessful attempt to secure the extra funding required from the new Liberal/National coalition government, on 11 December 2013, General Motors announced that Holden would cease engine and vehicle manufacturing operations in Australia by the end of 2017. As a result, 2,900 jobs would be lost over four years. Beyond 2017 Holden's Australian presence would consist of a national sales company, a parts distribution centre and a global design studio.

In May 2014, GM reversed their decision to abandon the Lang Lang Proving Ground and decided to keep it as part of their engineering capability in Australia.

In 2015, Holden again began selling a range of Opel-derived cars comprising the Astra VXR and Insignia VXR (both based on the OPC models sold by Vauxhall) and Cascada. Later that year, Holden also announced plans to sell the European Astra and the Korean Cruze alongside each other from 2017.

In December 2015, Belgian entrepreneur Guido Dumarey commenced negotiations to buy the Commodore manufacturing plant in South Australia, with a view to continue producing a rebadged Zeta-based premium range of rear and all-wheel drive vehicles for local and export sales. The proposal was met with doubt in South Australia, and it later came to nothing. On 20 October 2017, Holden ceased manufacturing vehicles in Australia. Holden then imported their cars from Opel in Germany and GM plants in Canada, U.S., Thailand, and South Korea.

On 17 February 2020, General Motors announced that the Holden brand would be retired by 2021, after GM stated it would not make all right-hand drive vehicles globally, leaving the Australia and New Zealand market altogether, costing close to AUD$1.6 billion.








On 8 May 2015, Jeff Rolfs, Holden's CFO, became interim chairman and managing director. Holden announced on 6 February 2015 that Mark Bernhard would return to Holden as chairman and managing director, the first Australian to hold the post in 25 years. In 2010, Holden sold vehicles across Australia through the Holden Dealer Network (310 authorised stores and 12 service centres), which employed more than 13,500 people.

In 1987, Holden established Holden Special Vehicles (HSV) in partnership with Tom Walkinshaw, who primarily manufactured modified, high-performance Commodore variants. To further reinforce the brand, HSV introduced the HSV Dealer Team into the V8 Supercar fold in 2005 under the naming rights of Toll HSV Dealer Team.

Holden's logo, of a lion holding a stone, was introduced in 1928. Holden's Motor Body Builders appointed Rayner Hoff to design the emblem, which refers to a fable in which observations of lions rolling stones led to the invention of the wheel. With the 1948 launch of the 48–215, Holden revised its logo. It commissioned another redesign in 1972 to better represent the company. The emblem was reworked once more in 1994.

Holden began to export vehicles in 1954, sending the FJ to New Zealand. Exports to New Zealand continued, but to broaden their export potential, Holden began to cater their Commodore, Monaro and Statesman/Caprice models for both right- and left-hand drive markets. The Middle East was Holden's largest export market, with the Commodore sold as the Chevrolet Lumina from 1998, and the Statesman from 1999 as the Chevrolet Caprice. Commodores were also sold as the Chevrolet Lumina in Brunei, Fiji and South Africa, and as the Chevrolet Omega in Brazil. Pontiac in North America also imported Commodore sedans from 2008 through to 2009 as the G8. The G8's cessation was a consequence of GM's Chapter 11 bankruptcy resulting in the demise of the Pontiac brand.

Sales of the Monaro began in 2003 to the Middle East as the Chevrolet Lumina Coupe. Later that year a modified version of the Monaro began selling in the United States (but not in Canada) as the Pontiac GTO, and under the Monaro name through Vauxhall dealerships in the United Kingdom. This arrangement continued through to 2005 when the car was discontinued. The long-wheelbase Statesman sales in the Chinese market as the Buick Royaum began in 2005, before being replaced in 2007 by the Statesman-based Buick Park Avenue. Statesman/Caprice exports to South Korea also began in 2005. These Korean models were sold as the Daewoo Statesman, and later as the Daewoo Veritas from 2008. Holden's move into international markets proved profitable; export revenue increased from A$973 million in 1999 to just under $1.3 billion in 2006.

From 2011, the WM Caprice was exported to North America as the Chevrolet Caprice PPV, a version of the Caprice built exclusively for law enforcement in North America and sold only to police. From 2007, the HSV-based Commodore was exported to the United Kingdom as the Vauxhall VXR8.

In 2013, Chevrolet announced that exports of the Commodore would resume to North America in the form of the VF Commodore as the Chevrolet SS sedan for the 2014 model year. The Chevrolet SS Sedan was also imported to the United States (but again, not to Canada) for 2015 with only minor changes, notably the addition of Magnetic Ride Control suspension and a Tremec TR-6060 manual transmission. For the 2016 model year the SS sedan received a facelift based on the VF Series II Commodore unveiled in September 2015. In 2017, production of Holden's last two American exports, the SS and the Caprice PPV was discontinued.


Whilst previously holding the number one position in Australian vehicle sales, Holden has sold progressively fewer cars during most of the 21st century, in part due to a large drop in Commodore sales.

Holden has been involved with factory backed teams in Australian touring car racing since 1968. The main factory-backed teams have been the Holden Dealer Team (1969–1987) and the Holden Racing Team (1990–2016). Since 2017, Triple Eight Race Engineering has been Holden's factory team. Holden has won the Bathurst 1000 32 times, more than any other manufacturer, and has won the Australian Touring Car and Supercars Championship title 20 times. Brad Jones Racing, Charlie Schwerkolt Racing, Erebus Motorsport, Matt Stone Racing, Tekno Autosports and Walkinshaw Andretti United also run Holden Commodores in the series.






</doc>
<doc id="13627" url="https://en.wikipedia.org/wiki?curid=13627" title="Hank Greenberg">
Hank Greenberg

Henry Benjamin Greenberg (born Hyman Greenberg; January 1, 1911 – September 4, 1986), nicknamed "Hammerin' Hank", "Hankus Pankus", or "The Hebrew Hammer", was an American professional baseball player and team executive. He played in Major League Baseball (MLB), primarily for the Detroit Tigers as a first baseman in the 1930s and 1940s. A member of the Baseball Hall of Fame and a two-time Most Valuable Player (MVP) Award winner, he was one of the premier power hitters of his generation and is widely considered as one of the greatest sluggers in baseball history. He had 47 months of military service including service in World War II, all of which took place during what would have been prime years in his major league career.

Greenberg played the first twelve of his 13 major league seasons for Detroit. He was an American League (AL) All-Star for four seasons and an AL MVP in 1935 (first baseman) and 1940 (left fielder). He had a batting average over .300 in eight seasons, and won two World Series championships with the Tigers ( and ). He was the AL home run leader four times and his 58 home runs for the Tigers in 1938 equaled Jimmie Foxx's 1932 mark for the most in one season by anyone other than Babe Ruth, and tied Foxx for the most home runs between Ruth's record 60 in 1927 and Roger Maris' record 61 in 1961. Greenberg was the first major league player to hit 25 or more home runs in a season in each league, and remains the AL record-holder for most runs batted in in a single season by a right-handed batter (183 in 1937, a 154-game schedule). 
His career statistics would have certainly been higher had he not served in the armed services during wartime. In 1947, Greenberg signed a contract for a record $85,000 salary before being sold to the Pittsburgh Pirates, where he played his final MLB season that year. After retiring from playing, Greenberg continued to work in baseball as a team executive for the Cleveland Indians and Chicago White Sox.

Greenberg was the first Jewish superstar in American team sports. He attracted national attention in 1934 in the middle of a pennant race when he had to decide whether to play baseball on two major Jewish holidays; after consultation with his rabbi, he agreed to play on Rosh Hashanah, but on Yom Kippur he spent the day at his synagogue, even though he was not particularly observant religiously. Having endured his share of anti-semitic abuse in his career, Greenberg was one of the few opposing players to publicly welcome African-American player Jackie Robinson to the major leagues in 1947.

Hank Greenberg was born Hyman Greenberg on January 1, 1911, in Greenwich Village, New York City, to Romanian Orthodox Jewish parents, David and Sarah Greenberg, who had emigrated from Bucharest. The family owned a successful cloth-shrinking plant in New York. He had two brothers, Ben, four years older, and Joe, five years younger, who also played baseball, and a sister, Lillian, two years older. His family moved to the Bronx when he was about seven.

He attended James Monroe High School in the Bronx, where he was an outstanding all-around athlete and was bestowed with the long-standing nickname of "Bruggy" by his basketball coach. His preferred sport was baseball, and his preferred position was first base. In high school basketball, he was on the Monroe team that won the city championship.

In 1929, the 18-year-old 6-foot-4-inch Greenberg was recruited by the New York Yankees, who already had Lou Gehrig at first base. Greenberg turned them down and instead attended New York University for a year, where he was a member of Sigma Alpha Mu, after which he signed with the Detroit Tigers for $9,000 ($ today).

Greenberg played minor league baseball for three years. Greenberg played 17 games in 1930 for the Hartford Senators, then played at Raleigh, North Carolina, for the Raleigh Capitals, where he hit .314 with 19 home runs. In 1931, he played at Evansville for the Evansville Hubs in the Illinois–Indiana–Iowa League (.318, 15 homers, 85 RBIs). In 1932, at Beaumont for the Beaumont Exporters in the Texas League, he hit 39 homers with 131 RBIs, won the MVP award, and led Beaumont to the Texas League title.

When he broke into the major leagues in 1930, Greenberg was the youngest MLB player (19).

In 1933, he rejoined the Tigers and hit .301 while driving in 87 runs. At the same time, he was third in the league in strikeouts (78). 
In 1934, his second major-league season, he hit .339 and helped the Tigers reach their first World Series in 25 years. He led the league in doubles, with 63 (the fourth-highest all-time in a single season), and extra base hits (96). He was third in the AL in slugging percentage (.600) – behind Jimmie Foxx and Lou Gehrig, but ahead of Babe Ruth, and in RBIs (139), sixth in batting average (.339), seventh in home runs (26), and ninth in on-base percentage (.404).

Late in the 1934 season, he announced that he would not play on September 10, which was Rosh Hashanah, the Jewish New Year, or on September 19, the Day of Atonement, Yom Kippur. Fans grumbled, "Rosh Hashanah comes every year but the Tigers haven't won the pennant since 1909." Greenberg did considerable soul-searching, and discussed the matter with his rabbi; finally he relented and agreed to play on Rosh Hashanah, but stuck with his decision not to play on Yom Kippur. Dramatically, Greenberg hit two home runs in a 2–1 Tigers victory over Boston on Rosh Hashanah. The next day's "Detroit Free Press" ran the Hebrew lettering for "Happy New Year" across its front page. Columnist and poet Edgar A. Guest expressed the general opinion in a poem titled "Speaking of Greenberg", in which he used the Irish (and thus Catholic) names Murphy and Mulroney. The poem ends with the lines ""We shall miss him on the infield and shall miss him at the bat / But he's true to his religion—and I honor him for that."" The complete text of the poem is at the end of Greenberg's biography page at the website of the International Jewish Sports Hall of Fame. The Detroit press was not so kind regarding the Yom Kippur decision, nor were many fans, but Greenberg in his autobiography recalled that he received a standing ovation from congregants at Congregation Shaarey Zedek when he arrived. Absent Greenberg, the Tigers lost to the New York Yankees, 5–2. The Tigers went on to face the St. Louis Cardinals in the 1934 World Series.

In 1935 Greenberg led the league in RBIs (170), total bases (389), and extra base hits (98), tied Foxx for the AL title in home runs (36), was 2nd in the league in doubles (46), slugging percentage (.628), was 3rd in the league in triples (16), and in runs scored (121), 6th in on-base percentage (.411) and walks (87), and was 7th in batting average (.328). He was unanimously voted the American League's Most Valuable Player. At the All-Star break that season, Greenberg hit 25 home runs and set an MLB record (still standing) of 103 RBIs – but was not selected to the AL All-Star roster (both managers put themselves on the rosters but did not play). He helped lead the Tigers to their first World Series title, but sprained his wrist in the second game and did not play in the other 4 games.

In 1936, Greenberg reinjured his wrist in a collision with Jake Powell of the Washington Senators in April and did not play the remainder of the season. He finished the season with 16 hits, 1 home run, and 15 RBIs in 12 games.

In 1937, Greenberg recovered from his injury and was voted to the AL All-Star roster, but did not play. On September 19, 1937, he hit the first home run into the center field bleachers at Yankee Stadium. He led the AL by driving in 183 runs (third all-time, behind Hack Wilson in 1930 and Lou Gehrig in 1931), and in extra base hits (103), while batting .337 with 200 hits. He was second in the league in home runs (40), doubles (49), total bases (397), slugging percentage (.668), and walks (102), third in on-base percentage (.436), and seventh in batting average (.337). Greenberg came in third in the vote for MVP.

A prodigious home run hitter, Greenberg narrowly missed breaking Babe Ruth's single-season home run record in 1938, when he hit 58 home runs, leading the league for the second time. That year, he had 11 games with multiple home runs, a new major league record. Sammy Sosa tied the record in 1998. Greenberg matched what was then the single-season home run record by a right-handed batter, (Jimmie Foxx, 1932); the mark stood for 66 years until it was broken by Sammy Sosa and Mark McGwire. Greenberg also had a 59th home run washed away in a rainout. It has been long speculated that Greenberg was intentionally walked late in the season to prevent him from breaking Ruth's record, but Greenberg dismissed this speculation, calling it "crazy stories." Nonetheless, Howard Megdal has calculated that in September 1938, Greenberg was walked in over 20% of his plate appearances, the highest percentage in his career by far.

Greenberg was again voted to the AL All-Star roster in 1938, but because he was not named to the 1935 AL All-Star roster and was benched in the 1937 game, he declined to accept a starting position on the 1938 AL team and did not play (the NL won 4-1). He led the league in runs scored (144) and at-bats per home run (9.6), tied for the AL lead in walks (119), was second in RBIs (146), slugging percentage (.683), and total bases (380), and third in OBP (.438) and set a still-standing major league record of 39 homers in his home park, the newly reconfigured Briggs Stadium. He also set a major-league record with 11 multiple-home run games. He came in third in the vote for MVP.

In 1939 Greenberg was voted to the AL All-Star roster for the third year in a row and was a starter at first base, and singled and walked in 4 at-bats (AL won 3-1). He finished second in the AL in home runs (33) and strikeouts (95), third in doubles (42) and slugging percentage (.622), fourth in RBIs (112), sixth in walks (91), and ninth in on-base percentage (.420).

After the 1939 season ended, Greenberg was asked by general manager Jack Zeller to take a salary cut of $5,000 ($ today) as a result of his off year in power and run production. He was asked to move from first base to the outfield to accommodate Rudy York, who was one of the best young hitters of his generation; York was tried at catcher, third baseman, and outfielder and proved to be a defensive liability at each position. Greenberg in turn, demanded a $10,000 bonus if he mastered the outfield, insisting "he" was the one taking the risk in learning a new position. Greenberg received his bonus at the end of spring training.

In 1940, Greenberg switched from playing the first base position to the left field position. For the 4th consecutive time, he was voted by the season's AL All-Star team manager to the AL All-Star team. In the bottom of the 6th inning, Greenberg and Lou Finney were sent into the game to replace right fielder Charlie Keller and left fielder Ted Williams with Greenberg playing in left field and Finney in right field. Greenberg batted twice in the game and fouled out to the catcher two-times. The NL won the game 4-0. That season, he led the AL in home runs for the third time in 6 years with 41; in RBIs (150), doubles (50), total bases (384), extra base hits (99), at-bats per home run (14.0), and slugging percentage (.670; 44 points ahead of Joe DiMaggio). He was second in the league behind Williams in runs scored (129) and OBP (.433), all while batting .340 (fifth best in the AL). He also led the Tigers to the AL pennant, and won his second American League MVP award, becoming the first player in major-league history to win an MVP award at two different playing positions.

On October 16, 1940, Greenberg became the first American League player to register for the nation's first peacetime draft. In the spring of 1941, the Detroit draft board initially classified Greenberg as 4F for "flat feet" after his first physical for military service and was recommended for light duty. The rumors that he had bribed the board, and concern that he would be likened to Jack Dempsey who had received negative publicity for failure to serve in World War I, led Greenberg to request to be reexamined. On April 18, he was found fit for regular military service and was reclassified.
On May 7, 1941, he was inducted into the U.S. Army after playing left field in 19 games and reported to Fort Custer at Battle Creek, Michigan. His salary was cut from $55,000 ($ today) a year to $21 ($ today) a month. He was not bitter, and stated, "I made up my mind to go when I was called. My country comes first." In November, while serving as an anti-tank gunner, he was promoted to sergeant, but was honorably discharged on December 5 (the United States Congress released men aged 28 years and older from service), two days before Japan bombed Pearl Harbor.
Greenberg re-enlisted as a sergeant on February 1, 1942, and volunteered for service in the Army Air Forces, becoming the first major league player to do so. He graduated from Officer Candidate School and was commissioned as a first lieutenant in the Air Corps (the new "Air Forces" service retaining the old name for its own logistics and training elements) and was assigned to the Physical Education Program. In February 1944, he was sent to the U.S. Army Special Services school. Promoted to captain, he requested overseas duty later that year and served in the China-Burma-India Theater for over six months, scouting locations for B-29 bomber bases and was a physical training officer with the 58th Bomber Wing. He was a Special Services officer of the 20th Bomber Command, 20th Air Force in China when it began bombing Japan on June 15. He was ordered to New York, and in late 1944, to Richmond, Virginia. Greenberg served 47 months, the longest of any major league player.

Greenberg remained in military uniform until he was placed on the military inactive list and discharged from the U.S. Army on June 14, 1945. He was the first major league player to return to MLB after the war. He returned to the Tigers team, and in his first game back on July 1, he homered. The All-Star Game scheduled for July 10 had been officially cancelled on April 24 and MLB did not name All-Stars that season due to strict travel restrictions during the last days of the war with Germany and Japan and the ending of World War II. In place of the All-Star Game, seven interleague games were played (eight had been scheduled) on July 9 and 10 to benefit the American Red Cross and the War Relief fund. An Associated Press All-Star roster was named (no game was played) for the AL and NL by a group of their sportswriters that included Greenberg as one of the All-Stars.

Greenberg, who played left field in 72 games and batted .311 in 1945, helped lead the Tigers to a come-from-behind American League pennant, clinching it with a grand slam home run in the dark—there were no lights in Sportsman's Park in St. Louis—ninth inning of the final game of the season. The ump—former Yankee pitching star of the 1920s Murderers Row team George Pipgras—supposedly said, "Sorry Hank, but I'm gonna have to call the game. I can't see the ball." Greenberg replied, "Don't worry, George, I can see it just fine", so the game continued. It ended with Greenberg's grand slam on the next pitch, clinching Hal Newhouser's 25th victory of the season. His home run allowed the Tigers to clinch the pennant and avoid a one-game playoff (that would have been necessary without the win) against the now-second-place Washington Senators. The Tigers went on to beat the Cubs in the World Series in seven games. Only three home runs were hit in that World Series. Phil Cavarretta hit a home run for the Cubs in Game One, Greenberg hit a homer in Game Two, where he batted in three runs in a 4–1 Tigers win, and he hit a two-run homer in Game Six in the eighth inning that tied the score 8–8; the Cubs went on to win that game with a run in the bottom of the 12th.

In 1946, he returned to peak form and playing at first base. He led the AL in home runs (44) and RBIs (127), both for the fourth time. He was second in slugging percentage (.604) and total bases (316) behind Ted Williams.

In 1947, Greenberg and the Tigers had a lengthy salary dispute. When Greenberg decided to retire rather than play for less, Detroit sold his contract to the Pittsburgh Pirates. To persuade him not to retire, Pittsburgh made Greenberg the first baseball player to earn over $80,000 ($ today) in a season as pure salary (though the exact amount is a matter of some dispute). Team co-owner Bing Crosby recorded a song, "Goodbye, Mr. Ball, Goodbye" with Groucho Marx and Greenberg to celebrate Greenberg's arrival. The Pirates also reduced the size of Forbes Field's cavernous left field, renaming the section "Greenberg Gardens" to accommodate Greenberg's pull-hitting style. Greenberg played first base for the Pirates in 1947 and was one of the few opposing players to publicly welcome Jackie Robinson to the majors.
That year he also had a chance to mentor a young future Hall-of-Famer, the 24-year-old Ralph Kiner. Said Greenberg, "Ralph had a natural home run swing. All he needed was somebody to teach him the value of hard work and self-discipline. Early in the morning on off-days, every chance we got, we worked on hitting." Kiner would go on to hit 51 home runs that year to lead the National League.

In his final season of 1947, Greenberg tied for the league lead in walks with 104, with a .408 on-base percentage and finished eighth in the league in home runs and tenth in slugging percentage. Greenberg became the first major league player to hit 25 or more home runs in a season in each league. Johnny Mize became the second in 1950.

Nevertheless, Greenberg retired as a player to take a front-office post with the Cleveland Indians. No player had ever retired after a final season in which they hit so many home runs. Since then, only Ted Williams (1960, 29), Dave Kingman (1986; 35), Mark McGwire (2001; 29), Barry Bonds (2007; 28) and David Ortiz (2016; 38) have hit as many or more homers in their final season.

Through 2010, he was first in career home runs and RBIs (ahead of Shawn Green) and batting average (ahead of Ryan Braun), and fourth in hits (behind Lou Boudreau), among all-time Jewish major league baseball players.
As a fielder, the 193-cm (6-foot-4-inch) Greenberg was awkward and unsure of himself early in his career, but mastered first base through countless hours of practice. Over the course of his career he demonstrated a higher-than-average fielding percentage and range at first base. When asked to move to left field in 1940 to make room for Rudy York, he worked tirelessly to conquer that position as well, reducing his errors in the outfield from 15 in 1940 to 0 in 1945.

Greenberg felt that runs batted in were more important than home runs. He would tell his teammates, "just get on base", or "just get the runner to third", and he would do the rest.

Greenberg would likely have approached 500 home runs and 1,800 RBIs had he not served in the military. As it was, he compiled 331 home runs, 1,051 runs and 1,276 RBI in a 1,394-game career. (b and c, see in Footnotes section below). Greenberg also hit for average, earning a lifetime batting average of .313. Starring as a first baseman and outfielder with the Tigers (1930, 1933–46) and doing duty only briefly with the Pirates (1947), Greenberg played only nine full seasons. He missed all but 19 games of the 1941 season, the three full seasons that followed, and most of 1945 to World War II military service and missed most of another season with a broken wrist.

After the 1947 season, Greenberg retired as a player, and Bill Veeck hired him as the Cleveland Indians' farm system director, and two years later, their General Manager; Greenberg did not, however, become a part-owner of the Indians until 1956, well after Veeck had sold his interest in the team. During his tenure, he sponsored more African American players than any other major league executive. Greenberg's contributions to the Cleveland farm system led to the team's successes throughout the 1950s, although Bill James once wrote that the Indians' late 1950s collapse should also be attributed to him. In 1949, Larry Doby also recommended Greenberg scout three players Doby used to play with in the Negro leagues: Hank Aaron, Ernie Banks, and Willie Mays. The next offseason Doby asked what Indians' scouts said about his recommendations. Said Greenberg, "Our guys checked 'em out and their reports were not good. They said that Aaron has a hitch in his swing and will never hit good pitching. Banks is too slow and didn't have enough range [at shortstop], and Mays can't hit a curveball." When Veeck sold his interest, Greenberg remained as general manager and part-owner (for one year) until 1957. In 1953, he was partly responsible for an important change to baseball's waivers rule. In previous seasons, once a player passed through waivers in his team's league (the AL or NL), any team from the other league could acquire him, a detail the Yankees used to often outbid other AL teams for NL players. Greenberg successfully campaigned for a new rule that, after June 15, required players to pass through waivers in both leagues before teams in the other league could attempt to obtain them. Greenberg was the mastermind behind a potential move of the club to Minneapolis that was vetoed by the rest of ownership at the last minute. Greenberg was furious and sold his share soon afterwards.

In 1959, Greenberg and Veeck teamed up for a second time when their syndicate purchased the Chicago White Sox; Veeck served as team president with Greenberg as vice president and general manager. During Veeck and Greenberg's first season, the White Sox won their first AL pennant since 1919. Veeck would sell his shares in the White Sox in 1961, and Greenberg stepped down as general manager on August 26 of that season.

After the 1960 season, the American League announced plans to put a team in Los Angeles. Greenberg immediately became the favorite to become the new team's first owner and persuaded Veeck to join him as his partner. However, when Dodgers owner Walter O'Malley got wind of these developments, he threatened to scuttle the whole deal by invoking his exclusive rights to operate a major league team in southern California. In truth, O'Malley wanted no part of competing against an expansion team owned by a master promoter such as Veeck, even if he was only a minority partner. Greenberg wouldn't budge and pulled out of the running for what became the Los Angeles Angels (now the Los Angeles Angels of Anaheim). Greenberg later became a successful investment banker, briefly returning to baseball as a minority partner with Veeck when the latter repurchased the White Sox in 1975.
On September 20, 1961, Greenberg along with Bob Neal called a baseball game for ABC between the New York Yankees and Baltimore Orioles.

Greenberg married Caral Gimbel (daughter of Bernard Gimbel of the Gimbel's New York department store family) on February 18, 1946, three days after signing a $60,000 ($ today) contract with the Tigers. The couple had three children—sons Glenn H. Greenberg and Stephen and a daughter, Alva—before divorcing in 1958. Their son, Stephen, played five years in the Washington Senators/Texas Rangers organization. In 1995, Stephen Greenberg co-founded Classic Sports Network with Brian Bedol, which was purchased by ESPN and became ESPN Classic. He also was the chairman of CSTV, the first cable network devoted exclusively to college sports.

In 1966, Greenberg married Mary Jo Tarola, a minor actress who appeared on-screen as Linda Douglas, and remained with her until his death. They had no children.

Greenberg died of metastatic kidney cancer in Beverly Hills, California, in 1986, and his remains were entombed at Hillside Memorial Park Cemetery, in Culver City, California.



Incidents of anti-Semitism Greenberg faced included having players stare at him and having racial slurs thrown at him by spectators and sometimes opposing players. Examples of these imprecations were: "Hey Mo!" (referring to the Jewish prophet Moses) and "Throw a pork chop—he can't hit that!" (a reference to Judaic kosher laws). In the 1935 World Series umpire George Moriarty warned some Chicago Cubs players to stop yelling anti-Semitic slurs at Greenberg and eventually cleared the players from the Cubs bench. Moriarty was disciplined for this action by then-commissioner Kenesaw Mountain Landis.

Greenberg befriended Jackie Robinson after he signed with the Dodgers in 1947, and encouraged him; Robinson credited Greenberg with helping him through the difficulties of his rookie year.

In an article in 1976 in "Esquire" magazine, sportswriter Harry Stein published an "All Time All-Star Argument Starter", consisting of five ethnic baseball teams. Greenberg was the first baseman on Stein's Jewish team.

In 2006, Greenberg was featured on a United States postage stamp. The stamp is one of a block of four honoring "baseball sluggers", the others being Mickey Mantle, Mel Ott, and Roy Campanella.






</doc>
<doc id="13628" url="https://en.wikipedia.org/wiki?curid=13628" title="Heinrich Schliemann">
Heinrich Schliemann

Heinrich Schliemann (; 6 January 1822 – 26 December 1890) was a German businessman and a pioneer in the field of archaeology. He was an advocate of the historicity of places mentioned in the works of Homer and an archaeological excavator of Hisarlik, now presumed to be the site of Troy, along with the Mycenaean sites Mycenae and Tiryns. His work lent weight to the idea that Homer's "Iliad" reflects historical events. Schliemann's excavation of nine levels of archaeological remains with dynamite has been criticized as destructive of significant historical artifacts, including the level that is believed to be the historical Troy.

Along with Arthur Evans, Schliemann was a pioneer in the study of Aegean civilization in the Bronze Age. The two men knew of each other, Evans having visited Schliemann's sites. Schliemann had planned to excavate at Knossos but died before fulfilling that dream. Evans bought the site and stepped in to take charge of the project, which was then still in its infancy.

Schliemann was born January 6, 1822 Heinrich Schliemann in Neubukow, Mecklenburg-Schwerin (part of the German Confederation). His father, Ernst Schliemann, was a Lutheran minister. The family moved to Ankershagen in 1823 (today their home houses the Heinrich Schliemann Museum).

Heinrich's father was a poor Pastor. His mother, Luise Therese Sophie Schliemann, died in 1831, when Heinrich was nine years old. After his mother's death, his father sent Heinrich to live with his uncle. When he was eleven years old, his father paid for him to enroll in the Gymnasium (grammar school) at Neustrelitz. Heinrich's later interest in history was initially encouraged by his father, who had schooled him in the tales of the Iliad and the Odyssey and had given him a copy of Ludwig Jerrer's "Illustrated History of the World" for Christmas in 1829. Schliemann later claimed that at the age of 7 he had declared he would one day excavate the city of Troy.

However, Heinrich had to transfer to the Realschule (vocational school) after his father was accused of embezzling church funds and had to leave that institution in 1836 when his father was no longer able to pay for it. His family's poverty made a university education impossible, so it was Schliemann's early academic experiences that influenced the course of his education as an adult. In his archaeological career, however, there was often a division between Schliemann and the educated professionals.

At age 14, after leaving Realschule, Heinrich became an apprentice at Herr Holtz's grocery in Fürstenberg. He later told that his passion for Homer was born when he heard a drunkard reciting it at the grocer's. He laboured for five years, until he was forced to leave because he burst a blood vessel lifting a heavy barrel. In 1841, Schliemann moved to Hamburg and became a cabin boy on the "Dorothea," a steamer bound for Venezuela. After twelve days at sea, the ship foundered in a gale. The survivors washed up on the shores of the Netherlands. Schliemann became a messenger, office attendant, and later, a bookkeeper in Amsterdam.

On March 1, 1844, 22-year-old Schliemann took a position with B. H. Schröder & Co., an import/export firm. In 1846, the firm sent him as a General Agent to St. Petersburg.

In time, Schliemann represented a number of companies. He learned Russian and Greek, employing a system that he used his entire life to learn languages; Schliemann claimed that it took him six weeks to learn a language and wrote his diary in the language of whatever country he happened to be in. By the end of his life, he could converse in English, French, Dutch, Spanish, Portuguese, Italian, Russian, Swedish, Polish, Greek, Latin, and Arabic, besides his native German.

Schliemann's ability with languages was an important part of his career as a businessman in the importing trade. In 1850, he learned of the death of his brother, Ludwig, who had become wealthy as a speculator in the California gold fields.

Schliemann went to California in early 1851 and started a bank in Sacramento buying and reselling over a million dollars' worth of gold dust in just six months. When the local Rothschild agent complained about short-weight consignments, he left California, pretending it was because of illness. While he was there, California became the 31st state in September 1850, and Schliemann acquired United States citizenship. While this story was propounded in Schliemann's autobiography of 1881, Christo Thanos and Wout Arentzen, state clearly that Schliemann was in St Petersburg that day, and "in actual fact, ...obtained his American citizenship only in 1869."

According to his memoirs, before arriving in California he dined in Washington, D.C. with President Millard Fillmore and his family, but W. Calder III says that Schliemann didn't attend but simply read about a similar gathering in the papers.

Schliemann also published what he said was an eyewitness account of the San Francisco Fire of 1851, which he said was in June although it took place in May. At the time he was in Sacramento and used the report of the fire in the "Sacramento Daily Journal" to write his report.

On April 7, 1852, he sold his business and returned to Russia. There he attempted to live the life of a gentleman, which brought him into contact with Ekaterina Petrovna Lyschin (1826–1896), the niece of one of his wealthy friends. Schliemann had previously learned that his childhood sweetheart, Minna, had married.

Heinrich and Ekaterina married on October 12, 1852. The marriage was troubled from the start.

Schliemann next cornered the market in indigo dye and then went into the indigo business itself, turning a good profit. Ekaterina and Heinrich had a son, Sergey (1855–1941), and two daughters, Natalya (1859–1869) and Nadezhda (1861–1935).

Schliemann made yet another quick fortune as a military contractor in the Crimean War, 1854–1856. He cornered the market in saltpeter, sulfur, and lead, constituents of ammunition, which he resold to the Russian government.

By 1858, Schliemann was 36 years old and wealthy enough to retire. In his memoirs, he claimed that he wished to dedicate himself to the pursuit of Troy.

As a consequence of his many travels, Schliemann was often separated from his wife and small children. He spent a month studying at the Sorbonne in 1866, while moving his assets from St. Petersburg to Paris to invest in real estate. He asked his wife to join him, but she refused.

Schliemann threatened to divorce Ekaterina twice before doing so. In 1869, he bought property and settled in Indianapolis for about three months to take advantage of Indiana's liberal divorce laws, although he obtained the divorce by lying about his residency in the U.S. and his intention to remain in the state. He moved to Athens as soon as an Indiana court granted him the divorce and married again two months later.

Heinrich Schliemann was an amateur-archaeologist. He is often used as a good example for archaeology students of how it shouldn't be done.

Schliemann was obsessed with the stories of Homer and ancient Mediterranean civilizations. He dedicated his life's work to unveiling the actual physical remains of the cities of Homer's epic tales. Many refer to him as the "father of pre-Hellenistic archaeology." 

In 1868, Schliemann visited sites in the Greek world, published "Ithaka, der Peloponnesus und Troja" in which he asserted that Hissarlik was the site of Troy, and submitted a dissertation in Ancient Greek proposing the same thesis to the University of Rostock. In 1869, he was awarded a PhD "in absentia" from the University of Rostock, in Germany, for that submission. David Traill wrote that the examiners gave him his PhD on the basis of his topographical analyses of Ithaca, which were in part simply translations of another author's work or drawn from poetic descriptions by the same author.

In 1869, Schliemann divorced his first wife, Ekaterina Petrovna Lyshin, whom he had married in 1852, and bore him three children. A former teacher and Athenian friend, Theokletos Vimpos, the Archbishop of Mantineia and Kynouria, helped Schliemann find someone "enthusiastic about Homer and about a rebirth of my beloved Greece...with a Greek name and a soul impassioned for learning." The archbishop suggested a young schoolgirl, Sophia Engastromenos, daughter of his cousin. They were married by the archbishop on 23 September 1869. They later had two children, Andromache and Agamemnon Schliemann.

Schliemann was elected a member of the American Antiquarian Society in 1880.

Schliemann's first interest of a classical nature seems to have been the location of Troy. At the time he began excavating in Turkey, the site commonly believed to be Troy was at Pınarbaşı, a hilltop at the south end of the Trojan Plain. The site had been previously excavated by archaeologist and local expert, Frank Calvert. Schliemann performed soundings at Pınarbaşı but was disappointed by his findings. It was Calvert who identified Hissarlik as Troy and suggested Schliemann dig there on land owned by Calvert's family.

Schliemann was at first skeptical about the identification of Hissarlik with Troy but was persuaded by Calvert. Schliemann began digging at Hissarlik in 1870, and by 1873 had discovered nine buried cities. The day before digging was to stop on 15 June 1873, was the day he discovered gold, which he took to be Priam's treasure trove.

A cache of gold and several other objects appeared on or around May 27, 1873; Schliemann named it "Priam's Treasure". He later wrote that he had seen the gold glinting in the dirt and dismissed the workmen so that he and Sophia could excavate it themselves; they removed it in her shawl. However, Schliemann's oft-repeated story of the treasure's being carried by Sophia in her shawl was untrue. Schliemann later admitted fabricating it; at the time of the discovery Sophia was in fact with her family in Athens, following the death of her father. Sophia later wore "the Jewels of Helen" for the public. 

Schliemann smuggled the treasure out of Turkey into Greece. The Turkish government sued Schliemann in a Greek court, and Schliemann was forced to pay a 10,000 gold franc indemnity. Schliemann ended up sending 50,000 gold francs to the Constantinople Imperial Museum, and some of the artifacts. Schliemann published "Troy and Its Remains" in 1874. Schliemann at first offered his collections, which included Priam's Gold, to the Greek government, then the French, and finally the Russians. However, in 1881, his collections ended up in Berlin, housed first in the Ethnographic Museum, and then the Museum for Pre- and Early History, until the start of WWII. In 1939, all exhibits were packed and stored in the museum basement, then moved to the Prussian State Bank vault in January 1941. Later in 1941, the treasure was moved to the Flakturm located at the Berlin Zoological Garden, called the Zoo Tower. Dr. Wilhelm Unverzagt protected the three crates containing the Trojan gold when the Battle for Berlin commenced, right up until SMERSH forces took control of the tower on 1 May. On 26 May 1945, Soviet forces, led by Lt. Gen. Nikolai Antipenko, Andre Konstantinov, deputy head of the Arts Committee, Viktor Lazarev, and Serafim Druzhinin, took the three crates away on trucks. The crates were then flown to Moscow on 30 June 1945, and taken to the Pushkin Museum ten days later. In 1994, the museum admitted the collection was in their possession.

In 1876, he began digging at Mycenae. There, he discovered the Shaft Graves, with their skeletons and more regal gold (including the so-called Mask of Agamemnon). These findings were published in "Mycenae" in 1878.

Although he had received permission in 1876 to continue excavation, Schliemann did not reopen the dig site at Troy until 1878–1879, after another excavation in Ithaca designed to locate a site mentioned in the "Odyssey". This was his second excavation at Troy. Emile Burnouf and Rudolf Virchow joined him there in 1879. 

Schliemann began excavation of the Treasury of Minyas at Orchomenus (Boeotia) in 1880. 

Schliemann made a third excavation at Troy in 1882–1883, an excavation of Tiryns with Wilhelm Dörpfeld in 1884, and a fourth excavation at Troy, also with Dörpfeld (who emphasized the importance of strata), in 1888–1890.

On August 1, 1890, Schliemann returned reluctantly to Athens, and in November travelled to Halle, where his chronic ear infection was operated upon, on November 13. The doctors deemed the operation a success, but his inner ear became painfully inflamed. Ignoring his doctors' advice, he left the hospital and travelled to Leipzig, Berlin, and Paris. From the latter, he planned to return to Athens in time for Christmas, but his ear condition became even worse. Too sick to make the boat ride from Naples to Greece, Schliemann remained in Naples but managed to make a journey to the ruins of Pompeii. On Christmas Day 1890, he collapsed into a coma; he died in a Naples hotel room the following day; the cause of death was cholesteatoma.

His corpse was then transported by friends to the First Cemetery in Athens. It was interred in a mausoleum shaped like a temple erected in ancient Greek style, designed by Ernst Ziller in the form of an amphiprostylee temple on top of a tall base. The frieze circling the outside of the mausoleum shows Schliemann conducting the excavations at Mycenae and other sites.

Schliemann's magnificent residence in the city centre of Athens, the "Iliou Melathron" (Ιλίου Μέλαθρον, "Palace of Ilium") houses today the Numismatic Museum of Athens.

Further excavation of the Troy site by others indicated that the level he named the Troy of the "Iliad" was inaccurate, although they retain the names given by Schliemann. In an article for "The Classical World," D.F. Easton wrote that Schliemann "was not very good at separating fact from interpretation" and claimed that, "Even in 1872 Frank Calvert could see from the pottery that Troy II had to be hundreds of years too early to be the Troy of the Trojan War, a point finally proven by the discovery of Mycenaean pottery in Troy VI in 1890."
"King Priam's Treasure" was found in the Troy II level, that of the Early Bronze Age, long before Priam's city of Troy VI or Troy VIIa in the prosperous and elaborate Mycenaean Age. Moreover, the finds were unique. The elaborate gold artifacts do not appear to belong to the Early Bronze Age.

His excavations were condemned by later archaeologists as having destroyed the main layers of the real Troy. Kenneth W. Harl, in the Teaching Company's "Great Ancient Civilizations of Asia Minor" lecture series, sarcastically claimed that Schliemann's excavations were carried out with such rough methods that he did to Troy what the Greeks could not do in their times, destroying and levelling down the entire city walls to the ground.

In 1972, Professor William Calder of the University of Colorado, speaking at a commemoration of Schliemann's birthday, claimed that he had uncovered several possible problems in Schliemann's work. Other investigators followed, such as Professor David Traill of the University of California.

An article published by the National Geographic Society called into question Schliemann's qualifications, his motives, and his methods:
In northwestern Turkey, Heinrich Schliemann excavated the site believed to be Troy in 1870. Schliemann was a German adventurer and con man who took sole credit for the discovery, even though he was digging at the site, called Hisarlik, at the behest of British archaeologist Frank Calvert. [...] Eager to find the legendary treasures of Troy, Schliemann blasted his way down to the second city, where he found what he believed were the jewels that once belonged to Helen. As it turns out, the jewels were a thousand years older than the time described in Homer's epic.

Another article presented similar criticisms when reporting on a speech by University of Pennsylvania scholar C. Brian Rose:
German archaeologist Heinrich Schliemann was the first to explore the Mound of Troy in the 1870s. Unfortunately, he had had no formal education in archaeology, and dug an enormous trench "which we still call the Schliemann Trench," according to Rose, because in the process Schliemann “destroyed a phenomenal amount of material." [...] Only much later in his career would he accept the fact that the treasure had been found at a layer one thousand years removed from the battle between the Greeks and Trojans, and thus that it could not have been the treasure of King Priam. Schliemann may not have discovered the truth, but the publicity stunt worked, making Schliemann and the site famous and igniting the field of Homeric studies in the late 19th century. During this period he was criticized and ridiculed of claims to fathering an offspring with a local Assyrian Girl sparking infidelity and adultery which Schliemann did not confirm or deny. '

Schliemann's methods have been described as "savage and brutal. He plowed through layers of soil and everything in them without proper record keeping—no mapping of finds, few descriptions of discoveries." Carl Blegen forgave his recklessness, saying "Although there were some regrettable blunders, those criticisms are largely colored by a comparison with modern techniques of digging; but it is only fair to remember that before 1876 very few persons, if anyone, yet really knew how excavations should properly be conducted. There was no science of archaeological investigation, and there was probably no other digger who was better than Schliemann in actual field work."

In 1874, Schliemann also initiated and sponsored the removal of medieval edifices from the Acropolis of Athens, including the great Frankish Tower. Despite considerable opposition, including from King George I of Greece, Schliemann saw the project through. The eminent historian of Frankish Greece William Miller later denounced this as "an act of vandalism unworthy of any people imbued with a sense of the continuity of history", and "pedantic barbarism".

Peter Ackroyd's novel "The Fall of Troy" (2006) is based on Schliemann's excavation of Troy. Schliemann is portrayed as "Heinrich Obermann".

Schliemann is also the subject of Chris Kuzneski's novel" The Lost Throne".

Schliemann is the subject of Irving Stone's novel "The Greek Treasure" (1975), which was the basis for the 2007 German television production "" ("Hunt for Troy").

Schliemann is a peripheral character in the historical mystery, "A Terrible Beauty". It is the 11th book in a series of novels featuring Lady Emily Hargreaves by Tasha Alexander. 

Schliemann is also mentioned in the 2005 TV film "The Magic of Ordinary Days" by the character Livy.

The questionable authenticity of Schliemann’s discovery of Priam’s Treasure is a central plot point to Lian Dolan’s novel "Helen of Pasadena".

Schliemann is also mentioned in 2011 book "" by the character Ian Welch.







</doc>
<doc id="13629" url="https://en.wikipedia.org/wiki?curid=13629" title="Hypnos">
Hypnos

In Greek mythology, Hypnos (; , "sleep") is the personification of sleep; the Roman equivalent is known as Somnus. His name is the origin of the word hypnosis.

Hypnos is the son of Nyx ("The Night") and Erebus ("The Darkness"). His brother is Thanatos ("Death"). Both siblings live in the underworld ("Hades") or in Erebus, another valley of the Greek underworld. According to rumors, Hypnos lived in a big cave, which the river Lethe ("Forgetfulness") comes from and where night and day meet. His bed is made of ebony, on the entrance of the cave grow a number of poppies and other hypnotic plants. No light and no sound would ever enter his grotto. According to Homer, he lives on the island Lemnos, which later on has been claimed to be his very own dream-island. He is said to be a calm and gentle god, as he helps humans in need and, due to their sleep, owns half of their lives.

Hypnos lived next to his twin brother, Thanatos (Θάνατος, "death personified") in the underworld.

Hypnos' mother was Nyx (Νύξ, "Night"), the deity of Night, and his father was Erebus, the deity of Darkness. Nyx was a dreadful and powerful goddess, and even Zeus feared to enter her realm.

His wife, Pasithea, was one of the youngest of the Charites and was promised to him by Hera, who is the goddess of marriage and birth. Pasithea is the deity of hallucination or relaxation.

Hypnos used his powers to trick Zeus. Hypnos was able to trick him and help the Danaans win the Trojan war. During the war, Hera loathed her brother and husband, Zeus, so she devised a plot to trick him. She decided that in order to trick him she needed to make him so enamoured with her that he would fall for the trick. So she washed herself with ambrosia and anointed herself with oil, made especially for her to make herself impossible to resist for Zeus. She wove flowers through her hair, put on three brilliant pendants for earrings, and donned a wondrous robe. She then called for Aphrodite, the goddess of love, and asked her for a charm that would ensure that her trick would not fail. In order to procure the charm, however, she lied to Aphrodite because they sided on opposite sides of the war. She told Aphrodite that she wanted the charm to help herself and Zeus stop fighting. Aphrodite willingly agreed. Hera was almost ready to trick Zeus, but she needed the help of Hypnos, who had tricked Zeus once before.

Hera called on Hypnos and asked him to help her by putting Zeus to sleep. Hypnos was reluctant because the last time he had put the god to sleep, he was furious when he awoke. It was Hera who had asked him to trick Zeus the first time as well. She was furious that Heracles, Zeus' son, sacked the city of the Trojans. So she had Hypnos put Zeus to sleep, and set blasts of angry winds upon the sea while Heracles was still sailing home. When Zeus awoke he was furious and went on a rampage looking for Hypnos. Hypnos managed to avoid Zeus by hiding with his mother, Nyx. This made Hypnos reluctant to accept Hera's proposal and help her trick Zeus again. Hera first offered him a beautiful golden seat that can never fall apart and a footstool to go with it. He refused this first offer, remembering the last time he tricked Zeus. Hera finally got him to agree by promising that he would be married to Pasithea, one of the youngest Graces, whom he had always wanted to marry. Hypnos made her swear by the river Styx and call on gods of the underworld to be witnesses so that he would be ensured that he would marry Pasithea.

Hera went to see Zeus on Gargarus, the topmost peak of Mount Ida. Zeus was extremely taken by her and suspected nothing as Hypnos was shrouded in a thick mist and hidden upon a pine tree that was close to where Hera and Zeus were talking. Zeus asked Hera what she was doing there and why she had come from Olympus, and she told him the same lie she told Aphrodite. She told him that she wanted to go help her parent stop quarrelling and she stopped there to consult him because she didn't want to go without his knowledge and have him be angry with her when he found out. Zeus said that she could go any time, and that she should postpone her visit and stay there with him so they could enjoy each other's company. He told her that he was never in love with anyone as much as he loved her at that moment. He took her in his embrace and Hypnos went to work putting him to sleep, with Hera in his arms. While this went on, Hypnos travelled to the ships of the Achaeans to tell Poseidon, God of the Sea, that he could now help the Danaans and give them a victory while Zeus was sleeping. This is where Hypnos leaves the story, leaving Poseidon eager to help the Danaans. Thanks to Hypnos helping to trick Zeus, the war changed its course to Hera's favour, and Zeus never found out that Hypnos had tricked him one more time.

According to a passage in "Deipnosophistae", the sophist and dithyrambic poet Licymnius of Chios tells a different tale about the Endymion myth, in which Hypnos, in awe of his beauty, causes him to sleep with his eyes open, so he can fully admire his face.

Hypnos appears in numerous works of art, most of which are vases. An example of one vase that Hypnos is featured on is called "Ariadne Abandoned by Theseus," which is part of the Museum of Fine Arts in Boston’s collection. In this vase, Hypnos is shown as a winged god dripping Lethean water upon the head of Ariadne as she sleeps. One of the most famous works of art featuring Hypnos is a bronze head of Hypnos himself, now kept in the British Museum in London. This bronze head has wings sprouting from his temples and the hair is elaborately arranged, some tying in knots and some hanging freely from his head.

The English word "hypnosis" is derived from his name, referring to the fact that when hypnotized, a person is put into a sleep-like state (hypnos "sleep" + -osis "condition"). The class of medicines known as "hypnotics" which induce sleep also take their name from Hypnos.

Additionally, the English word "insomnia" comes from the name of his Latin counterpart, Somnus. (in- "not" + somnus "sleep"), as well as a few less-common words such as "somnolent", meaning sleepy or tending to cause sleep and hypersomnia meaning excessive sleep, which can be caused by many conditions (known as secondary hypersomnia) or a rare sleep disorder causing excessive sleep with unknown cause, called Idiopathic Hypersomnia.


3D model of "Bronze head of Hypnos" via laser scan of a cast of British Museum's bronze.


</doc>
<doc id="13631" url="https://en.wikipedia.org/wiki?curid=13631" title="Holy orders">
Holy orders

In certain Christian churches, holy orders are ordained ministries such as bishop, priest, or deacon, and the sacrament or rite by which candidates are ordained to those orders. Churches recognizing these orders include the Catholic Church, the Eastern Orthodox (ιερωσύνη ["hierōsynē"], ιεράτευμα ["hierateuma"], Священство ["Svyashchenstvo"]), Oriental Orthodox, Anglican, Assyrian, Old Catholic, Independent Catholic and some Lutheran churches. Except for Lutherans and some Anglicans, these churches regard ordination as a sacrament (the "sacramentum ordinis"). The Anglo-Catholic tradition within Anglicanism identifies more with the Roman Catholic position about the sacramental nature of ordination.

Denominations have varied conceptions of holy orders. In Anglican and some Lutheran churches the traditional orders of bishop, priest and deacon are bestowed using ordination rites. The extent to which ordination is considered sacramental in these traditions has, however, been a matter of some internal dispute. Baptists are among the denominations that do not consider ministry as being sacramental in nature and would not think of it in terms of "holy orders" as such. Historically, the word "order" (Latin "ordo") designated an established civil body or corporation with a hierarchy, and "ordinatio" meant legal incorporation into an "ordo". The word "holy" refers to the church. In context, therefore, a holy order is set apart for ministry in the church. Other positions, such as pope, patriarch, cardinal, monsignor, archbishop, archimandrite, archpriest, protopresbyter, hieromonk, protodeacon and archdeacon, are not sacramental orders but specialized ministries.

The Eastern Orthodox Church considers ordination (known as "cheirotonia", "laying on of hands") to be a sacred mystery (μυστήριο, what in the West is called a sacrament). Although all other mysteries may be performed by a presbyter, ordination may only be conferred by a bishop, and the ordination of a bishop may only be performed by several bishops together. "Cheirotonia" always takes place during the Divine Liturgy.

It was the mission of the Apostles to go forth into all the world and preach the Gospel, baptizing those who believed in the name of the Holy Trinity (). In the Early Church those who presided over congregations were referred to variously as "episcopos" (bishop) or "presbyteros" (priest). These successors of the Apostles were ordained to their office by the laying on of hands, and according to Orthodox theology formed a living, organic link with the Apostles, and through them with Jesus Christ himself. This link is believed to continue in unbroken succession to this day. Over time, the ministry of bishops (who hold the fullness of the priesthood) and presbyters or priests (who hold a portion of the priesthood as bestowed by their bishop) came to be distinguished. In Orthodox terminology, "priesthood" or "sacerdotal" refers to the ministry of bishops and priests.

The Eastern Orthodox Church also has ordination to minor orders (known as "cheirothesia", "imposition of hands") which is performed outside of the Divine Liturgy, typically by a bishop, although certain archimandrites of stavropegial monasteries may bestow cheirothesia on members of their communities.

A bishop is the collector of the money of the diocese and the living Vessel of Grace through whom the "energeia" (divine grace) of the Holy Spirit flows into the rest of the church. A bishop is consecrated through the laying on of hands by several bishops. (With the consent of several other bishops, a single bishop has performed the ordination of another bishop in emergency situations, such as times of persecution.) The consecration of a bishop takes place near the beginning of the Liturgy, since a bishop can, in addition to performing the Mystery of the Eucharist, also ordain priests and deacons. Before the commencement of the Holy Liturgy, the bishop-elect professes, in the middle of the church before the seated bishops who will consecrate him, in detail the doctrines of the Orthodox Christian Faith and pledges to observe the canons of the Apostles and Councils, the Typikon and customs of the Orthodox Church and to obey ecclesiastical authority. After the Little Entrance, the arch-priest and arch-deacon conduct the bishop-elect before the Royal Gates where he is met by the bishops and kneels before the altar on both knees. The Gospel Book is laid over his head and the consecrating bishops lay their hands upon the Gospel Book, while the prayers of ordination are read by the eldest bishop. After this, the newly consecrated bishop ascends the "synthranon" (bishop's throne in the sanctuary) for the first time. Customarily, the newly consecrated bishop ordains a priest and a deacon at the Liturgy during which he is consecrated.

A priest may serve only at the pleasure of his bishop. A bishop bestows faculties (permission to minister within his diocese) giving a priest chrism and an antimins; he may withdraw faculties and demand the return of these items. The ordination of a priest occurs before the Anaphora (Eucharistic Prayer) in order that he may on the same day take part in the celebration of the Eucharist: During the Great Entrance, the candidate for ordination carries the Aër (chalice veil) over his head (rather than on his shoulder, as a deacon otherwise carries it then) as a symbol of giving up his diaconate, and comes last in the procession and stands at the end of the pair of lines of the priests. After the Aër is taken from the candidate to cover the chalice and diskos, a chair is brought for the bishop to sit on by the northeast corner of the Holy Table (altar). Two deacons go to priest-elect who, at that point, had been standing alone in the middle of the church, and bow him down to the west (to the people) and to the east (to the clergy), asking their consent by saying “Command ye!” and then lead him through the holy doors of the altar where the archdeacon asks the bishop’s consent, saying, “Command, most sacred master!” after which a priest escorts the candidate three times around the Holy Table, during which he kisses each corner of the Holy Table as well as the bishop's epigonation and right hand and prostrates himself before the holy table at each circuit. The candidate is then taken to the southeast corner of the Holy Table and kneels on both knees, resting his forehead on the edge of the Holy Table. The ordaining bishop then places his omophor and right hand over the ordinand's head and recites aloud the first "Prayer of Cheirotonia" and then prays silently the other two prayers of cheirotonia while a deacon quietly recites a litany and the clergy, then the congregation, chant “Lord, have mercy”. Afterwards, the bishop brings the newly ordained priest to stand in the Holy Doors and presents him to the faithful. He then clothes the priest in each of his sacerdotal vestments, at each of which the people sing, "Worthy!". Later, after the Epiklesis of the Liturgy, the bishop hands him a portion of the Lamb (Host) saying:
A deacon may not perform any Sacrament and performs no liturgical services on his own but serves only as an assistant to a priest and may not even vest without the blessing of a priest. The ordination of a deacon occurs after the Anaphora (Eucharistic Prayer) since his role is not in performing the Holy Mystery but consists only in serving; the ceremony is much the same as at the ordination of a priest, but the deacon-elect is presented to the people and escorted to the holy doors by two sub-deacons (his peers, analogous to the two deacons who so present a priest-elect), is escorted three times around the Holy Table by a deacon, and he kneels on only one knee during the "Prayer of Cheirotonia". After being vested as a deacon and given a liturgical fan "(ripidion or hexapterygion)", he is led to the side of the Holy Table where he uses the ripidion to gently fan the Holy Gifts (consecrated Body and Blood of Christ).

The Anglican churches hold their bishops to be in apostolic succession, although there is some difference of opinion with regard to whether ordination is to be regarded as a sacrament. The Anglican Articles of Religion hold that only Baptism and the Lord's Supper are to be counted as sacraments of the gospel, and assert that other rites "commonly called Sacraments", considered to be sacraments by such as the Roman Catholic and Eastern churches, were not ordained by Christ in the Gospel. They do not have the nature of a sacrament of the gospel in the absence of any physical matter such as the water in Baptism and the bread and wine in the Eucharist. The Book of Common Prayer provides rites for ordination of bishops, priests and deacons. Only bishops may ordain. Within Anglicanism, three bishops are normally required for ordination to the episcopate, while one bishop is sufficient for performing ordinations to the priesthood and diaconate.

Lutherans reject the Roman Catholic understanding of holy orders because they do not think sacerdotalism is supported by the Bible. Martin Luther taught that each individual was expected to fulfill his God-appointed task in everyday life. The modern usage of the term vocation as a life-task was first employed by Martin Luther. In Luther's Small Catechism, the holy orders include but are not limited to the following: bishops, pastors, preachers, governmental offices, citizens, husbands, wives, children, employees, employers, young people, and widows. However, also according to the Book of Concord: "But if ordination be understood as applying to the ministry of the Word, we are not unwilling to call ordination a sacrament. For the ministry of the Word has God's command and glorious promises, Rom. 1:16: The Gospel is the power of God unto salvation to every one that believeth. Likewise, Isa. 55:11: So shall My Word be that goeth forth out of My mouth; it shall not return unto Me void, but it shall accomplish that which I please. ...If ordination be understood in this way, neither will we refuse to call the imposition of hands a sacrament. For the Church has the command to appoint ministers, which should be most pleasing to us, because we know that God approves this ministry, and is present in the ministry [that God will preach and work through men and those who have been chosen by men]."

The ministerial orders of the Catholic Church include the orders of bishops, deacons and presbyters, which in Latin is "sacerdos". The ordained priesthood and common priesthood (or priesthood of the all the baptized) are different in function and essence.

A distinction is made between "priest" and "presbyter". In the 1983 Code of Canon Law, "The Latin words "sacerdos" and "sacerdotium" are used to refer in general to the ministerial priesthood shared by bishops and presbyters. The words "presbyter, presbyterium and presbyteratus" refer to priests [in the English use of the word] and presbyters".

While the consecrated life is neither clerical nor lay by definition, clerics can be members of institutes of consecrated or secular (diocesan) life.

The sequence in which holy orders are received are: minor orders, deacon, priest, bishop.

For Catholics, it is typical in the year of seminary training that a man will be ordained to the diaconate, which Catholics since the Second Vatican Council sometimes call the "transitional diaconate" to distinguish men bound for priesthood from permanent deacons. They are licensed to preach sermons (under certain circumstances a permanent deacon may not receive faculties to preach), to perform baptisms, and to witness Catholic marriages, but to perform no other sacraments. They assist at the Eucharist or the Mass, but are not able to consecrate the bread and wine. Normally, after six months or more as a transitional deacon, a man will be ordained to the priesthood. Priests are able to preach, perform baptisms, confirm (with special dispensation from their ordinary), witness marriages, hear confessions and give absolutions, anoint the sick, and celebrate the Eucharist or the Mass.

Orthodox seminarians are typically tonsured as readers before entering the seminary, and may later be made subdeacons or deacons; customs vary between seminaries and between Orthodox jurisdictions. Some deacons remain permanently in the diaconate while most subsequently are ordained as priests. Orthodox clergy are typically either married or monastic. Monastic deacons are called hierodeacons, monastic priests are called hieromonks. Orthodox clergy who marry must do so prior to ordination to the subdiaconate (or diaconate, according to local custom) and typically one is either tonsured a monk or married before ordination. A deacon or priest may not marry, or remarry if widowed, without abandoning his clerical office. Often, widowed priests take monastic vows. Orthodox bishops are always monks; a single or widowed man may be elected a bishop but he must be tonsured a monk before consecration as a bishop.

For Anglicans, a person is usually ordained a deacon once he (or she) has completed training at a theological college. The historic practice of a bishop tutoring a candidate himself ("reading for orders") is still to be found. The candidate then typically serves as an assistant curate and may later be ordained as a priest at the discretion of the bishop. Other deacons may choose to remain in this order. Anglican deacons can preach sermons, perform baptisms and conduct funerals, but, unlike priests, cannot celebrate the Eucharist. In most branches of the Anglican church, women can be ordained as priests, and in some of them, can also be ordained bishops.

Bishops are chosen from among priests in churches that adhere to Catholic usage.
In the Roman Catholic Church, bishops, like priests, are celibate and thus unmarried; further, a bishop is said to possess the fullness of the sacrament of holy orders, empowering him to ordain deacons, priests, and – with papal consent – other bishops. If a bishop, especially one acting as an ordinary – a head of a diocese or archdiocese – is to be ordained, three bishops must usually co-consecrate him with one bishop, usually an archbishop or the bishop of the place, being the chief consecrating prelate.

Among Eastern Rite Catholic and Eastern Orthodox churches, which permit married priests, bishops must either be unmarried or agree to abstain from contact with their wives. It is a common misconception that all such bishops come from religious orders; while this is generally true, it is not an absolute rule. In the case of both Catholics – (Western and) Eastern Catholic, Oriental Orthodox and Eastern Orthodox, they are usually leaders of territorial units called dioceses (or its equivalent in the east, an eparchy). Only bishops can validly administer the sacrament of holy orders.

The Roman Catholic Church unconditionally recognizes the validity of ordinations in the Eastern churches. Some Eastern Orthodox churches reordain Catholic priests who convert while others accept their Roman Catholic ordination using the concept of economia (church economy).

Anglican churches claim to have maintained apostolic succession. The succession of Anglican bishops is not universally recognized, however. The Roman Catholic Church judged Anglican orders invalid when Pope Leo XIII in 1896 wrote in "Apostolicae curae" that Anglican orders lack validity because the rite by which priests were ordained was not correctly worded from 1547 to 1553 and from 1559 to the time of Archbishop William Laud (Archbishop of Canterbury 1633–1645). The papacy claimed the form and matter was inadequate to make a Catholic bishop. The actual "mechanical" succession, prayer and laying on hands, was not disputed. Two of the four consecrators of Matthew Parker in 1559 had been consecrated using the English Ordinal and two using the Roman Pontifical. Nonetheless, they believed that this caused a break of continuity in apostolic succession, making all further ordinations null and void.

Eastern Orthodox bishops have, on occasion, granted "economy" when Anglican priests convert to Orthodoxy. Various Orthodox churches have also declared Anglican orders valid subject to a finding that the bishops in question did indeed maintain the true faith, the Orthodox concept of apostolic succession being one in which the faith must be properly adhered to and transmitted, not simply that the ceremony by which a man is made a bishop is conducted correctly.

Changes in the Anglican Ordinal since King Edward VI, and a fuller appreciation of the pre-Reformation ordinals, suggest that the correctness of the enduring dismissal of Anglican orders is questionable. To reduce doubt concerning Anglican apostolic succession, especially since the 1930 Bonn agreement between the Anglican and Old Catholic churches, some Anglican bishops have included among their consecrators bishops of the Old Catholic Church, whose holy orders are recognised as valid and regular by the Roman Catholic Church.

Neither Roman Catholics nor Anglicans recognize the validity of ordinations of ministers in Protestant churches that do not maintain apostolic succession; but some Anglicans, especially Low Church or Evangelical ones, commonly treat Protestant ministers and their sacraments as valid. Rome also does not recognize the apostolic succession of those Lutheran bodies which retained apostolic succession.

Officially, the Anglican Communion accepts the ordinations of those denominations which are in full communion with their own churches, such as the Lutheran state churches of Scandinavia. Those clergy may preside at services requiring a priest if one is not otherwise available.

Married men may be ordained to the diaconate as permanent deacons, but in the Latin Rite of the Roman Catholic Church generally may not be ordained to the priesthood. In the Eastern Catholic Churches and in the Eastern Orthodox Church, married deacons may be ordained priests but may not become bishops. Bishops in the Eastern Rites and the Eastern Orthodox churches are almost always drawn from among monks, who have taken a vow of celibacy. They may be widowers, though; it is not required of them never to have been married.

In some cases, widowed permanent deacons have been ordained to the priesthood. There have been some situations in which men previously married and ordained to the priesthood in an Anglican church or in a Lutheran church have been ordained to the Catholic priesthood and allowed to function much as an Eastern Rite priest but in a Latin Rite setting. This is never "sub conditione" (conditionally), as there is in Catholic canon law no true priesthood in Protestant denominations. Such ordination may only happen with the approval of the priest's Bishop and a special permission by the Pope.

Anglican clergy may be married or may marry after ordination. In the Old Catholic Church and the Independent Catholic Churches there are no ordination restrictions related to marriage.

Ordination ritual and procedures vary by denomination. Different churches and denominations specify more or less rigorous requirements for entering into office, and the process of ordination is likewise given more or less ceremonial pomp depending on the group. Many Protestants still communicate authority and ordain to office by having the existing overseers physically lay hands on the candidates for office.

The American Methodist model is an episcopal system loosely based on the Anglican model, as the Methodist Church arose from the Anglican Church. It was first devised under the leadership of Bishops Thomas Coke and Francis Asbury of the Methodist Episcopal Church in the late 18th century. In this approach, an elder (or 'presbyter') is ordained to word (preaching and teaching), sacrament (administering Baptism and the Lord's Supper), order (administering the life of the church and, in the case of bishops, ordaining others for mission and ministry), and service. A deacon is a person ordained only to word and service.

In the United Methodist Church, for instance, seminary graduates are examined and approved by the Conference Board of Ordained Ministry and then the Clergy Session. They are accepted as "probationary (provisional) members of the conference." The resident bishop may commission them to full-time ministry as "provisional" ministers. (Before 1996, the graduate was ordained as a transitional deacon at this point, a provisional role since eliminated. The order of deacon is now a separate and distinct clergy order in the United Methodist Church.) After serving the probationary period, of a minimum of two years, the probationer is then examined again and either continued on probation, discontinued altogether, or approved for ordination. Upon final approval by the Clergy Session of the Conference, the probationer becomes a full member of the Conference and is then ordained as an elder or deacon by the resident Bishop. Those ordained as elders are members of the Order of Elders, and those ordained deacons are members of the Order of Deacons.

John Wesley appointed Thomas Coke (above mentioned as bishop) as 'Superintendent', his translation of the Greek "episcopos" ("overseer") – which is normally translated 'bishop' in English. The British Methodist Conference has two distinct orders of presbyter and deacon. It does not have bishops as a separate order of ministry. The British Methodist Church has more than 500 superintendents, who are not a separate order of ministry but a role within the order of presbyters. The roles normally undertaken by bishops are expressed in ordaining presbyters and deacons by the annual Conference through its president (or a past president); in confirmation by all presbyters; in local oversight by superintendents; in regional oversight by chairs of Districts.

Presbyterian churches, following their Scottish forebears, reject the traditions surrounding overseers and instead identify the offices of bishop ("episkopos" in Greek) and elder ("presbuteros" in Greek, from which the term "presbyterian" comes). The two terms seem to be used interchangeably in the Bible (compare Titus 1.5–9 and I Tim. 3.2–7). Their form of church governance is known as presbyterian polity. While there is increasing authority with each level of gathering of elders ('Session' over a congregation or parish, then presbytery, then possibly a synod, then the General Assembly), there is no hierarchy of elders. Each elder has an equal vote at the court on which they stand.

Elders are usually chosen at their local level, either elected by the congregation and approved by the Session, or appointed directly by the Session. Some churches place limits on the term that the elders serve, while others ordain elders for life.

Presbyterians also ordain (by laying on of hands) ministers of Word and Sacrament (sometimes known as 'teaching elders'). These ministers are regarded simply as Presbyters ordained to a different function, but in practice they provide the leadership for the local Session.

Some Presbyterians identify those appointed (by the laying on of hands) to serve in practical ways (Acts 6.1–7) as deacons ("diakonos" in Greek, meaning 'servant'). In many congregations, a group of men or women is thus set aside to deal with matters such as congregational fabric and finance, releasing elders for more 'spiritual' work. These persons may be known as 'deacons', 'board members' or 'managers', depending on the local tradition. Unlike elders and ministers, they are not usually 'ordained', and are often elected by the congregation for a set period of time.

Other Presbyterians have used an 'order of deacons' as full-time servants of the wider Church. Unlike ministers, they do not administer sacraments or routinely preach. The Church of Scotland has recently begun ordaining deacons to this role.

Unlike the Episcopalian system, but similar to the United Methodist system described above, the two Presbyterian offices are different in "kind" rather than in "degree", since one need not be a deacon before becoming an elder. Since there is no hierarchy, the two offices do not make up an 'order' in the technical sense, but the terminology of holy orders is sometimes still developed.

Congregationalist churches implement different schemes, but the officers usually have less authority than in the presbyterian or episcopalian forms. Some ordain only ministers and rotate members on an advisory board (sometimes called a board of elders or a board of deacons). Because the positions are by comparison less powerful, there is usually less rigor or fanfare in how officers are ordained.

The Church of Jesus Christ of Latter-day Saints (LDS Church) accepts the legal authority of clergy to perform marriages but does not recognize any other sacraments performed by ministers not ordained to the Latter-day Saint priesthood. Although the Latter-day Saints do claim a doctrine of a certain spiritual "apostolic succession," it is significantly different from that claimed by Catholics and Protestants since there is no succession or continuity between the first century and the lifetime of Joseph Smith, the founder of the LDS church. Mormons teach that the priesthood was lost in ancient times not to be restored by Christ until the nineteenth century when it was given to Joseph Smith directly.

The Church of Jesus Christ of Latter-day Saints has a relatively open priesthood, ordaining nearly all worthy adult males and boys of the age of twelve and older. Latter-day Saint priesthood consists of two divisions: the Melchizedek Priesthood and Aaronic Priesthood. The Melchizedek Priesthood because Melchizedek was such a great high priest. Before his day it was called the Holy Priesthood, after the Order of the Son of God. But out of respect or reverence to the name of the Supreme Being, to avoid the too frequent repetition of his name, the church, in ancient days, called that priesthood after Melchizedek. The lesser priesthood is an appendage to the Melchizedek Priesthood. It is called the Aaronic Priesthood because it was conferred on Aaron and his sons throughout all their generations.
The offices, or ranks, of the Melchizedek order (in roughly descending order) include apostle, seventy, patriarch, high priest, and elder. The offices of the Aaronic order are bishop, priest, teacher, and deacon. The manner of ordination consists of the laying on of hands by two or more men holding at least the office being conferred while one acts as voice in conferring the priesthood or office and usually pronounces a blessing upon the recipient. Teachers and deacons do not have the authority to ordain others to the priesthood. All church members are authorized to teach and preach regardless of priesthood ordination so long as they maintain good standing within the church. The church does not use the term "holy orders."

Community of Christ has a largely volunteer priesthood, and all members of the priesthood are free to marry (as traditionally defined by the Christian community). The priesthood is divided into two orders, the Aaronic priesthood and the Melchisedec priesthood. The Aaronic order consists of the offices of deacon, teacher and priest. The Melchisedec Order consists of the offices of elder (including the specialized office of seventy) and high priest (including the specialized offices of evangelist, bishop, apostle, and prophet). Paid ministers include “appointees” and the general officers of the church, which include some specialized priesthood offices (such as the office of president, reserved for the three top members of the church leadership team). As of 1984, women have been eligible for priesthood, which is conferred through the sacrament of ordination by the laying-on-of-hands. While there is technically no age requirement for any office of priesthood, there is no automatic ordination or progression as in the LDS Church. Young people are occasionally ordained as deacon, and sometimes teacher or priest, but generally most priesthood members are called following completion of post secondary school education. In March 2007 a woman was ordained for the first time to the office of president.

The Roman Catholic Church, in accordance with its understanding of the theological tradition on the issue, and the definitive clarification found in the encyclical letter "Ordinatio sacerdotalis" (1994) written by Pope John Paul II, officially teaches that it has no authority to ordain female as priests and thus there is no possibility of women becoming priests at any time in the future. "Ordaining" women as deaconesses is not a possibility in any sacramental sense of the diaconate, for a deaconess is not simply a female who is a deacon but instead holds a position of lay service. As such, she does not receive the sacrament of holy orders. Many Anglican and Protestant churches ordain women, but in many cases, only to the office of deacon.

Various branches of the Orthodox churches, including the Greek Orthodox, currently set aside vows of deaconesses. Some churches are internally divided on whether the Scriptures permit the ordination of women. When one considers the relative size of the churches (1.1 billion Roman Catholics, 300 million Orthodox, 590 million Anglicans and Protestants), it is a minority of Christian churches that ordain women. Protestants constitute about 27 percent of Christians worldwide, and most of their churches that do ordain women have only done so within the past century.

In some traditions women may be ordained to the same orders as men. In others women are restricted from certain offices. Females may be ordained bishop in the Old Catholic churches and in the Anglican/Episcopal churches in Scotland, Ireland, Wales, Cuba, Brazil, South Africa, Canada, US, Australia, Aotearoa New Zealand and Polynesia. The Church of Ireland had installed Pat Storey in 2013. On 19 September 2013, Storey was chosen by the House of Bishops to succeed Richard Clarke as Bishop of Meath and Kildare. She was consecrated to the episcopate at Christ Church Cathedral, Dublin, on 30 November 2013. She is the first woman to be elected as a bishop in the Church of Ireland and the first female to be an Anglican Communion bishop in Ireland and Great Britain. The Church of England's General Synod voted in 2014 to allow females to be ordained to the episcopate, with Libby Lane being the first woman to be ordained bishop. Continuing Anglican churches of the world do not permit women to be ordained. In some Protestant denominations, females may serve as assistant pastors but not as pastors in charge of congregations. In some denominations, females can be ordained to be an elder or deacon. Some denominations allow for the ordination of females for certain religious orders. Within certain traditions, such as the Anglican and Lutheran, there is a diversity of theology and practice regarding ordination of women and females

The ordination of lesbian, gay, bisexual or transgender clergy who are sexually active, and open about it, represents a fiercely contested subject within many mainline Protestant communities. The majority of churches are opposed to such ordinations because they view homosexuality as a sin and incompatible with Biblical teaching and traditional Christian practice. Yet there are an increasing number of Christian congregations and communities that are open to ordaining people who are gay or lesbian. These are liberal Protestant denominations, such as the Episcopal Church, the United Church of Christ, and the Evangelical Lutheran Church in America, plus the small Metropolitan Community Church, founded as a church intending to minister primarily to LGBT people, and the Church of Sweden where such clergy may serve in senior clerical positions. The Church of Norway has for many years had both gay and lesbian priests, even bishops, and in 2006 the first woman who was appointed a bishop in Norway came out as an active homosexual herself, and that she had been a homosexual since before she joined the church.

The issue of ordination has caused particular controversy in the worldwide Anglican Communion, following the approval of Gene Robinson to be Bishop of New Hampshire in the US Episcopal Church.





</doc>
<doc id="13633" url="https://en.wikipedia.org/wiki?curid=13633" title="Homer">
Homer

Homer (; , "Hómēros") is the presumed author of the "Iliad" and the "Odyssey", two epic poems that are the central works of ancient Greek literature. The "Iliad" is set during the Trojan War, the ten-year siege of the city of Troy by a coalition of Greek kingdoms. It focuses on a quarrel between King Agamemnon and the warrior Achilles lasting a few weeks during the last year of the war. The "Odyssey" focuses on the ten-year journey home of Odysseus, king of Ithaca, after the fall of Troy. Many accounts of Homer's life circulated in classical antiquity, the most widespread being that he was a blind bard from Ionia, a region of central coastal Anatolia in present-day Turkey. Modern scholars consider these accounts legendary.

The Homeric Question – concerning by whom, when, where and under what circumstances the "Iliad" and "Odyssey" were composed – continues to be debated. Broadly speaking, modern scholarly opinion falls into two groups. One holds that most of the "Iliad" and (according to some) the "Odyssey" are the works of a single poet of genius. The other considers the Homeric poems to be the result of a process of working and reworking by many contributors, and that "Homer" is best seen as a label for an entire tradition. It is generally accepted that the poems were composed at some point around the late eighth or early seventh century BC.

The poems are in Homeric Greek, also known as Epic Greek, a literary language which shows a mixture of features of the Ionic and Aeolic dialects from different centuries; the predominant influence is Eastern Ionic. Most researchers believe that the poems were originally transmitted orally. From antiquity until the present day, the influence of Homeric epic on Western civilization has been great, inspiring many of its most famous works of literature, music, art and film. The Homeric epics were the greatest influence on ancient Greek culture and education; to Plato, Homer was simply the one who "has taught Greece" – "ten Hellada pepaideuken".

Today only the "Iliad" and "Odyssey" are associated with the name 'Homer'. In antiquity, a very large number of other works were sometimes attributed to him, including the "Homeric Hymns", the "Contest of Homer and Hesiod", the "Little Iliad", the "Nostoi", the "Thebaid", the "Cypria", the "Epigoni", the comic mini-epic "Batrachomyomachia" ("The Frog-Mouse War"), the "Margites", the "Capture of Oechalia", and the "Phocais". These claims are not considered authentic today and were by no means universally accepted in the ancient world. As with the multitude of legends surrounding Homer's life, they indicate little more than the centrality of Homer to ancient Greek culture.

In the early 4th century BC Alcidamas composed a fictional account of a poetry contest at Chalcis with both Homer and Hesiod. Homer was expected to win, and answered all of Hesiod's questions and puzzles with ease. Then, each of the poets was invited to recite the best passage from their work. Hesiod selected the beginning of "Works and Days": "When the Pleiades born of Atlas...all in due season". Homer chose a description of Greek warriors in formation, facing the foe, taken from the "Iliad". Though the crowd acclaimed Homer victor, the judge awarded Hesiod the prize; the poet who praised husbandry, he said, was greater than the one who told tales of battles and slaughter.

Many traditions circulated in the ancient world concerning Homer, most of which are lost. Modern scholarly consensus is that they have no value as history. Some claims were established early and repeated often. They include that Homer was blind (taking as self-referential a passage describing the blind bard Demodocus), that he was born in Chios, that he was the son of the river Meles and the nymph Critheïs, that he was a wandering bard, that he composed a varying list of other works (the "Homerica"), that he died either in Ios or after failing to solve a riddle set by fishermen, and various explanations for the name "Homer". The two best known ancient biographies of Homer are the "Life of Homer" by the Pseudo-Herodotus and the "Contest of Homer and Hesiod".

 The study of Homer is one of the oldest topics in scholarship, dating back to antiquity. Nonetheless, the aims of Homeric studies have changed over the course of the millennia. The earliest preserved comments on Homer concern his treatment of the gods, which hostile critics such as the poet Xenophanes of Colophon denounced as immoral. The allegorist Theagenes of Rhegium is said to have defended Homer by arguing that the Homeric poems are allegories. The "Iliad" and the "Odyssey" were widely used as school texts in ancient Greek and Hellenistic cultures. They were the first literary works taught to all students. The "Iliad", particularly its first few books, was far more intently studied than the "Odyssey" during the Hellenistic and Roman periods.

As a result of the poems' prominence in classical Greek education, extensive commentaries on them developed to explain parts of the poems that were culturally or linguistically difficult. During the Hellenistic and Roman periods, many interpreters, especially the Stoics, who believed that Homeric poems conveyed Stoic doctrines, regarded them as allegories, containing hidden wisdom. Perhaps partially because of the Homeric poems' extensive use in education, many authors believed that Homer's original purpose had been to educate. Homer's wisdom became so widely praised that he began to acquire the image of almost a prototypical philosopher. Byzantine scholars such as Eustathius of Thessalonica and John Tzetzes produced commentaries, extensions and scholia to Homer, especially in the twelfth century. Eustathius's commentary on the "Iliad" alone is massive, sprawling over nearly 4,000 oversized pages in a twenty-first century printed version and his commentary on the "Odyssey" an additional nearly 2,000.

In 1488, the Greek scholar Demetrios Chalkokondyles published the "editio princeps" of the Homeric poems. The earliest modern Homeric scholars started with the same basic approaches towards the Homeric poems as scholars in antiquity. The allegorical interpretation of the Homeric poems that had been so prevalent in antiquity returned to become the prevailing view of the Renaissance. Renaissance humanists praised Homer as the archetypically wise poet, whose writings contain hidden wisdom, disguised through allegory. In western Europe during the Renaissance, Virgil was more widely read than Homer and Homer was often seen through a Virgilian lens.

In 1664, contradicting the widespread praise of Homer as the epitome of wisdom, François Hédelin, abbé d'Aubignac wrote a scathing attack on the Homeric poems, declaring that they were incoherent, immoral, tasteless, and without style, that Homer never existed, and that the poems were hastily cobbled together by incompetent editors from unrelated oral songs. Fifty years later, the English scholar Richard Bentley concluded that Homer did exist, but that he was an obscure, prehistoric oral poet whose compositions bear little relation to the "Iliad" and the "Odyssey" as they have been passed down. According to Bentley, Homer "wrote a Sequel of Songs and Rhapsodies, to be sung by himself for small Earnings and good Cheer at Festivals and other Days of Merriment; the "Ilias" he wrote for men, and the "Odysseis" for the other Sex. These loose songs were not collected together in the Form of an epic Poem till Pisistratus' time, about 500 Years after."

Friedrich August Wolf's "Prolegomena ad Homerum", published in 1795, argued that much of the material later incorporated into the "Iliad" and the "Odyssey" was originally composed in the tenth century BC in the form of short, separate oral songs, which passed through oral tradition for roughly four hundred years before being assembled into prototypical versions of the "Iliad" and the "Odyssey" in the sixth century BC by literate authors. After being written down, Wolf maintained that the two poems were extensively edited, modernized, and eventually shaped into their present state as artistic unities. Wolf and the "Analyst" school, which led the field in the nineteenth century, sought to recover the original, authentic poems which were thought to be concealed by later excrescences.

Within the Analyst school were two camps: proponents of the "lay theory," which held that the "Iliad" and the "Odyssey" were put together from a large number of short, independent songs, and proponents of the "nucleus theory", which held that Homer had originally composed shorter versions of the "Iliad" and the "Odyssey", which later poets expanded and revised. A small group of scholars opposed to the Analysts, dubbed "Unitarians", saw the later additions as superior, the work of a single inspired poet. By around 1830, the central preoccupations of Homeric scholars, dealing with whether or not "Homer" actually existed, when and how the Homeric poems originated, how they were transmitted, when and how they were finally written down, and their overall unity, had been dubbed "the Homeric Question".

Following World War I, the Analyst school began to fall out of favor among Homeric scholars. It did not die out entirely, but it came to be increasingly seen as a discredited dead end. Starting in around 1928, Milman Parry and Albert Lord, after their studies of folk bards in the Balkans, developed the "Oral-Formulaic Theory" that the Homeric poems were originally composed through improvised oral performances, which relied on traditional epithets and poetic formulas. This theory found very wide scholarly acceptance and explained many previously puzzling features of the Homeric poems, including their unusually archaic language, their extensive use of stock epithets, and their other "repetitive" features. Many scholars concluded that the "Homeric question" had finally been answered. Meanwhile, the 'Neoanalysts' sought to bridge the gap between the 'Analysts' and 'Unitarians'. The Neoanalysts sought to trace the relationships between the Homeric poems and other epic poems, which have now been lost, but of which modern scholars do possess some patchy knowledge. Knowledge of earlier versions of the epics can be derived from anomalies of structure and detail in our surviving version of the Iliad and Odyssey. These anomalies point to earlier versions of the Iliad in which Ajax played a more prominent role, in which the Achaean embassy to Achilles comprised different characters, and in which Patroclus was actually mistaken for Achilles by the Trojans. They point to earlier versions of the Odyssey in which Telemachus went in search of news of his father not to Menelaus in Sparta but to Idomeneus in Crete, in which Telemachus met up with his father in Crete and conspired with him to return to Ithaca disguised as the soothsayer Theoclymenus, and in which Penelope recognized Odysseus much earlier in the narrative and conspired with him in the destruction of the suitors. Neoanalysis can be viewed as a form of Analysis informed by the principles of Oral Theory, recognizing as it does the existence and influence of previously existing tales and yet appreciating the technique of a single poet in adapting them to his Iliad and Odyssey.

Most contemporary scholars, although they disagree on other questions about the genesis of the poems, agree that the "Iliad" and the "Odyssey" were not produced by the same author, based on "the many differences of narrative manner, theology, ethics, vocabulary, and geographical perspective, and by the apparently imitative character of certain passages of the "Odyssey" in relation to the "Iliad"." Nearly all scholars agree that the "Iliad" and the "Odyssey" are unified poems, in that each poem shows a clear overall design, and that they are not merely strung together from unrelated songs. It is also generally agreed that each poem was composed mostly by a single author, who probably relied heavily on older oral traditions. Nearly all scholars agree that the "Doloneia" in Book X of the "Iliad" is not part of the original poem, but rather a later insertion by a different poet.

Some ancient scholars believed Homer to have been an eyewitness to the Trojan War; others thought he had lived up to 500 years afterwards. Contemporary scholars continue to debate the date of the poems. A long history of oral transmission lies behind the composition of the poems, complicating the search for a precise date. At one extreme, Richard Janko has proposed a date for both poems to the eighth century BC based on linguistic analysis and statistics. Barry B. Powell dates the composition of the "Iliad" and the "Odyssey" to sometime between 800 and 750 BC, based on the statement from Herodotus, who lived in the late fifth century BC, that Homer lived four hundred years before his own time "and not more" (καὶ οὐ πλέοσι), and on the fact that the poems do not mention hoplite battle tactics, inhumation, or literacy. Martin Litchfield West has argued that the "Iliad" echoes the poetry of Hesiod, and that it must have been composed around 660–650 BC at the earliest, with the "Odyssey" up to a generation later. He also interprets passages in the "Iliad" as showing knowledge of historical events that occurred in the ancient Near East during the middle of the seventh century BC, including the destruction of Babylon by Sennacherib in 689 BC and the Sack of Thebes by Ashurbanipal in 663/4 BC. At the other extreme, a few American scholars such as Gregory Nagy see "Homer" as a continually evolving tradition, which grew much more stable as the tradition progressed, but which did not fully cease to continue changing and evolving until as late as the middle of the second century BC. 

"'Homer" is a name of unknown etymological origin, around which many theories were erected in antiquity. One such linkage was to the Greek ("hómēros"), "hostage" (or "surety"). The explanations suggested by modern scholars tend to mirror their position on the overall Homeric question. Nagy interprets it as "he who fits (the song) together". West has advanced both possible Greek and Phoenician etymologies.

Scholars continue to debate questions such as whether the Trojan War actually took place – and if so when and where – and to what extent the society depicted by Homer is based on his own or one which was, even at the time of the poems' composition, known only as legend. The Homeric epics are largely set in the east and center of the Mediterranean, with some scattered references to Egypt, Ethiopia and other distant lands, in a warlike society that resembles that of the Greek world slightly before the hypothesized date of the poems' composition.

In ancient Greek chronology, the sack of Troy was dated to 1184 BC. By the nineteenth century, there was widespread scholarly skepticism that the Trojan War had ever happened and that Troy had even existed, but in 1873 Heinrich Schliemann announced to the world that he had discovered the ruins of Homer's Troy at Hissarlik in modern Turkey. Some contemporary scholars think the destruction of Troy VIIa "circa" 1220 BC was the origin of the myth of the Trojan War, others that the poem was inspired by multiple similar sieges that took place over the centuries.

Most scholars now agree that the Homeric poems depict customs and elements of the material world that are derived from different periods of Greek history. For instance, the heroes in the poems use bronze weapons, characteristic of the Bronze Age in which the poems are set, rather than the later Iron Age during which they were composed; yet the same heroes are cremated (an Iron Age practice) rather than buried (as they were in the Bronze Age). In some parts of the Homeric poems, heroes are accurately described as carrying large shields like those used by warriors during the Mycenaean period, but, in other places, they are instead described carrying the smaller shields that were commonly used during the time when the poems were written in the early Iron Age.

In the "Iliad" 10.260–265, Odysseus is described as wearing a helmet made of boar's tusks. Such helmets were not worn in Homer's time, but were commonly worn by aristocratic warriors between 1600 and 1150 BC. The decipherment of Linear B in the 1950s by Michael Ventris and continued archaeological investigation has increased modern scholars' understanding of Aegean civilisation, which in many ways resembles the ancient Near East more than the society described by Homer. Some aspects of the Homeric world are simply made up; for instance, the "Iliad" 22.145–56 describes there being two springs that run near the city of Troy, one that runs steaming hot and the other that runs icy cold. It is here that Hector takes his final stand against Achilles. Archaeologists, however, have uncovered no evidence that springs of this description ever actually existed.

The Homeric epics are written in an artificial literary language or 'Kunstsprache' only used in epic hexameter poetry. Homeric Greek shows features of multiple regional Greek dialects and periods, but is fundamentally based on Ionic Greek, in keeping with the tradition that Homer was from Ionia. Linguistic analysis suggests that the "Iliad" was composed slightly before the "Odyssey", and that Homeric formulae preserve older features than other parts of the poems.

The Homeric poems were composed in unrhymed dactylic hexameter; ancient Greek metre was quantity-based rather than stress-based. Homer frequently uses set phrases such as epithets ('crafty Odysseus', 'rosy-fingered Dawn', 'owl-eyed Athena', etc.), Homeric formulae ('and then answered [him/her], Agamemnon, king of men', 'when the early-born rose-fingered Dawn came to light', 'thus he/she spoke'), simile, type scenes, ring composition and repetition. These habits aid the extemporizing bard, and are characteristic of oral poetry. For instance, the main words of a Homeric sentence are generally placed towards the beginning, whereas literate poets like Virgil or Milton use longer and more complicated syntactical structures. Homer then expands on these ideas in subsequent clauses; this technique is called parataxis.

The so-called 'type scenes' ("typischen Scenen"), were named by Walter Arend in 1933. He noted that Homer often, when describing frequently recurring activities such as eating, praying, fighting and dressing, used blocks of set phrases in sequence that were then elaborated by the poet. The 'Analyst' school had considered these repetitions as un-Homeric, whereas Arend interpreted them philosophically. Parry and Lord noted that these conventions are found in many other cultures.

'Ring composition' or chiastic structure (when a phrase or idea is repeated at both the beginning and end of a story, or a series of such ideas first appears in the order A, B, C... before being reversed as ...C, B, A) has been observed in the Homeric epics. Opinion differs as to whether these occurrences are a conscious artistic device, a mnemonic aid or a spontaneous feature of human storytelling.

Both of the Homeric poems begin with an invocation to the Muse. In the "Iliad", the poet invokes her to sing of "the anger of Achilles", and, in the "Odyssey", he asks her to sing of "the man of many ways". A similar opening was later employed by Virgil in his "Aeneid".

The orally transmitted Homeric poems were put into written form at some point between the eighth and sixth centuries BC. Some scholars believe that they were dictated to a scribe by the poet and that our inherited versions of the "Iliad" and "Odyssey" were in origin orally-dictated texts. Albert Lord noted that the Balkan bards that he was studying revised and expanded their songs in their process of dictating. Some scholars hypothesize that a similar process of revision and expansion occurred when the Homeric poems were first written down. Other scholars hold that, after the poems were created in the eighth century, they continued to be orally transmitted with considerable revision until they were written down in the sixth century. After textualisation, the poems were each divided into 24 rhapsodes, today referred to as books, and labelled by the letters of the Greek alphabet. Most scholars attribute the book divisions to the Hellenistic scholars of Alexandria, in Egypt. Some trace the divisions back further to the Classical period. Very few credit Homer himself with the divisions.

In antiquity, it was widely held that the Homeric poems were collected and organised in Athens in the late sixth century BC by the tyrant Peisistratos (died 528/7 BC), in what subsequent scholars have dubbed the "Peisistratean recension". The idea that the Homeric poems were originally transmitted orally and first written down during the reign of Peisistratos is referenced by the first-century BC Roman orator Cicero and is also referenced in a number of other surviving sources, including two ancient "Lives of Homer". From around 150 BC, the texts of the Homeric poems seem to have become relatively established. After the establishment of the Library of Alexandria, Homeric scholars such as Zenodotus of Ephesus, Aristophanes of Byzantium and in particular Aristarchus of Samothrace helped establish a canonical text.

The first printed edition of Homer was produced in 1488 in Milan, Italy. Today scholars use medieval manuscripts, papyri and other sources; some argue for a "multi-text" view, rather than seeking a single definitive text. The nineteenth-century edition of Arthur Ludwich mainly follows Aristarchus's work, whereas van Thiel's (1991, 1996) follows the medieval vulgate. Others, such as Martin West (1998–2000) or T.W. Allen, fall somewhere between these two extremes.



This is a partial list of translations into English of Homer's "Iliad" and "Odyssey".






</doc>
<doc id="13635" url="https://en.wikipedia.org/wiki?curid=13635" title="Hugo Gernsback">
Hugo Gernsback

Hugo Gernsback (; born Hugo Gernsbacher, August 16, 1884 – August 19, 1967) was a Luxembourgish-American inventor, writer, editor, and magazine publisher, best known for publications including the first science fiction magazine. His contributions to the genre as publisher—although not as a writer—were so significant that, along with the novelists H. G. Wells and Jules Verne, he is sometimes called "The Father of Science Fiction". In his honour, annual awards presented at the World Science Fiction Convention are named the "Hugos".

Gernsback was born in 1884 in Luxembourg City, to Berta (Dürlacher), a housewife, and Moritz Gernsbacher, a winemaker. His family was Jewish. Gernsback emigrated to the United States in 1904 and later became a naturalized citizen. He married three times: to Rose Harvey in 1906, Dorothy Kantrowitz in 1921, and Mary Hancher in 1951. In 1925, he founded radio station WRNY, which was broadcast from the 18th floor of the Roosevelt Hotel in New York City. In 1928, WRNY aired some of the first television broadcasts. During the show, audio stopped and each artist waved or bowed onscreen. When audio resumed, they performed. Gernsback is also considered a pioneer in amateur radio.

Before helping to create science fiction, Gernsback was an entrepreneur in the electronics industry, importing radio parts from Europe to the United States and helping to popularize amateur "wireless". In April 1908 he founded "Modern Electrics", the world's first magazine about both electronics and radio, called "wireless" at the time. While the cover of the magazine itself states it was a catalog, most historians note that it contained articles, features, and plotlines, qualifying it as a magazine.

Under its auspices, in January 1909, he founded the Wireless Association of America, which had 10,000 members within a year. In 1912, Gernsback said that he estimated 400,000 people in the U.S. were involved in amateur radio. In 1913, he founded a similar magazine, "The Electrical Experimenter", which became "Science and Invention" in 1920. It was in these magazines that he began including scientific fiction stories alongside science journalism—including his novel "Ralph 124C 41+" which he ran for 12 months from April 1911 in "Modern Electrics".

Hugo Gernsback started the Radio News magazine for amateur radio enthusiasts in 1919.

He died at Roosevelt Hospital (Mount Sinai West as of 2020) in New York City on August 19, 1967.

Gernsback provided a forum for the modern genre of science fiction in 1926 by founding the first magazine dedicated to it, "Amazing Stories". The inaugural April issue comprised a one-page editorial and reissues of six stories, three less than ten years old and three by Poe, Verne, and Wells. He said he became interested in the concept after reading a translation of the work of Percival Lowell as a child. His idea of a perfect science fiction story was "75 percent literature interwoven with 25 percent science". He also played an important role in starting science fiction fandom, by organizing the Science Fiction League and by publishing the addresses of people who wrote letters to his magazines. Fans began to organize, and became aware of themselves as a movement, a social force; this was probably decisive for the subsequent history of the genre. He also created the term "science fiction", though he preferred the term "scientifiction".

In 1929, he lost ownership of his first magazines after a bankruptcy lawsuit. There is some debate about whether this process was genuine, manipulated by publisher Bernarr Macfadden, or was a Gernsback scheme to begin another company. After losing control of "Amazing Stories", Gernsback founded two new science fiction magazines, "Science Wonder Stories" and "Air Wonder Stories". A year later, due to Depression-era financial troubles, the two were merged into "Wonder Stories", which Gernsback continued to publish until 1936, when it was sold to Thrilling Publications and renamed "Thrilling Wonder Stories". Gernsback returned in 1952–53 with "Science-Fiction Plus".

Gernsback was noted for sharp (and sometimes shady) business practices, and for paying his writers extremely low fees or not paying them at all. H. P. Lovecraft and Clark Ashton Smith referred to him as "Hugo the Rat".

As Barry Malzberg has said:

Gernsback's venality and corruption, his sleaziness and his utter disregard for the financial rights of authors, have been well documented and discussed in critical and fan literature. That the founder of genre science fiction who gave his name to the field's most prestigious award and who was the Guest of Honor at the 1952 Worldcon was pretty much a crook (and a contemptuous crook who stiffed his writers but paid himself $100K a year as President of Gernsback Publications) has been clearly established. 

Jack Williamson, who had to hire an attorney associated with the American Fiction Guild to force Gernsback to pay him, summed up his importance for the genre:

At any rate, his main influence in the field was simply to start Amazing and Wonder Stories and get SF out to the public newsstands—and to name the genre he had earlier called "scientifiction."

Frederik Pohl said in 1965 that Gernsback's "Amazing Stories" published "the kind of stories Gernsback himself used to write: a sort of animated catalogue of gadgets". Gernsback's fiction includes the novel "Ralph 124C 41+"; the title is a pun on the phrase "one to foresee for many" ("one plus"). Even though "Ralph 124C 41+" has been described as pioneering many ideas and themes found in later SF work, it has often been neglected due to what most critics deem poor artistic quality. Author Brian Aldiss called the story a "tawdry illiterate tale" and a "sorry concoction", while author and editor Lester del Rey called it "simply dreadful." While most other modern critics have little positive to say about the story's writing, "Ralph 124C 41+" is considered by science fiction critic Gary Westfahl as "essential text for all studies of science fiction."

Gernsback's second novel, "Baron Münchausen's Scientific Adventures", was serialized in "Amazing Stories" in 1928.

Gernsback's third (and final) novel, "Ultimate World", written c. 1958, was not published until 1971. Lester del Rey described it simply as "a bad book", marked more by routine social commentary than by scientific insight or extrapolation. James Blish, in a caustic review, described the novel as "incompetent, pedantic, graceless, incredible, unpopulated and boring" and concluded that its publication "accomplishes nothing but the placing of a blot on the memory of a justly honored man."

Gernsback combined his fiction and science into "Everyday Science and Mechanics" magazine, serving as the editor in the 1930s.

In 1954, Gernsback was awarded an Officer of Luxembourg's Order of the Oak Crown, an honor equivalent to being knighted.
The Hugo Awards or "Hugos" are the annual achievement awards presented at the World Science Fiction Convention, selected in a process that ends with vote by current Convention members. They originated and acquired the "Hugo" nickname during the 1950s and were formally defined as a convention responsibility under the name "Science Fiction Achievement Awards" early in the 1960s. The nickname soon became almost universal and its use legally protected; "Hugo Award(s)" replaced the longer name in all official uses after the 1991 cycle.

In 1960 Gernsback received a special Hugo Award as "The Father of Magazine Science Fiction".

The Science Fiction and Fantasy Hall of Fame inducted him in 1996, its inaugural class of two deceased and two living persons.

Science fiction author Brian W. Aldiss held a contrary view about Gernsback's contributions: "It is easy to argue that Hugo Gernsback ... was one of the worst disasters to hit the science fiction field ... Gernsback himself was utterly without any literary understanding. He created dangerous precedents which many later editors in the field followed."

Gernsback made significant contributions to the growth of early broadcasting, mostly through his efforts as a publisher. He originated the industry of specialized publications for radio with "Modern Electrics" and "Electrical Experimenter". Later on, and more influentially, he published "Radio News", which would have the largest readership among radio magazines in radio broadcasting's formative years. He edited "Radio News" until 1929. For a short time he hired John F. Rider to be editor. Rider was a former engineer working with the US Army Signal Corps and a radio engineer for Alfred H. Grebe, a radio manufacturer. However, Rider would soon leave Gernsback and form his own publishing company, John F. Rider Publisher, New York around 1931.

Gernsback made use of the magazine to promote his interests, including having his radio station's call letters on the cover starting in 1925. WRNY and "Radio News" were used to cross-promote each other, with programs on his station often used to discuss articles he had published, and articles in the magazine often covering program activities at WRNY. He also advocated for future directions in innovation and regulation of radio. The magazine contained many drawings and diagrams, encouraging radio listeners of the 1920s to experiment themselves to improve the technology. WRNY was often used as a laboratory to see if various radio inventions were worthwhile.

Articles that were published about television were also tested in this manner when the radio station was used to send pictures to experimental television receivers in August 1928. The technology, however, required sending sight and sound one after the other rather than sending both at the same time, as WRNY only broadcast on one channel. Such experiments were expensive, eventually contributing to Gernsback's Experimenter Publishing Company going into bankruptcy in 1929. WRNY was sold to Aviation Radio, who maintained the channel part-time to broadcast aviation weather reports and related feature programs. Along with other stations sharing the same frequency, it was acquired by Metro-Goldwyn-Mayer and consolidated into that company's WHN in 1934.

Gernsback held 80 patents by the time of his death in New York City on August 19, 1967. 

His first patent was a new method for manufacturing dry-cell batteries, a patent applied for on June 28, 1906 and granted February 5, 1907.

Among his inventions are a combined electric hair brush and comb (US Patent 1,016,138), 1912; an ear cushion (US Patent 1,514,152), 1927; and a hydraulic fishery (US Patent 2,718,083), 1955.

Other patents held by Gernsback are related to: Incandescent Lamp, Electrorheostat Regulator, Electro Adjustable Condenser, Detectorium, Relay, Potentiometer, Electrolytic Interrupter, Rotary Variable Condenser, Luminous Electric Mirror, Transmitter, Postal Card, Telephone Headband, Electromagnetic Sounding Device, Submersible Amusement Device, Apparatus for Landing Flying Machines, Tuned Telephone Receiver, Electric Valve, Detector, Acoustic Apparatus, Electrically Operated Fountain, Cord Terminal, Coil Mounting, Radio Horn, Variable Condenser, Switch, Telephone Receiver, Crystal Detector, Process for Mounting Inductances, Depilator, Switch, Code Learner's Instrument.

Novels:


Short stories:








</doc>
<doc id="13636" url="https://en.wikipedia.org/wiki?curid=13636" title="History of computing hardware">
History of computing hardware

The history of computing hardware covers the developments from early simple devices to aid calculation to modern day computers. Before the 20th century, most calculations were done by humans. Early mechanical tools to help humans with digital calculations, like the abacus, were referred to as "calculating machines" or "calculators" (and other proprietary names). The machine operator was called the "computer".

The first aids to computation were purely mechanical devices which required the operator to set up the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form (e.g. distance along a scale, rotation of a shaft, or a voltage). Numbers could also be represented in the form of digits, automatically manipulated by a mechanism. Although this approach generally required more complex mechanisms, it greatly increased the precision of results. The development of transistor technology and then the integrated circuit chip led to a series of breakthroughs, starting with transistor computers and then integrated circuit computers, causing digital computers to largely replace analog computers. Metal-oxide-semiconductor (MOS) large-scale integration (LSI) then enabled semiconductor memory and the microprocessor, leading to another key breakthrough, the miniaturized personal computer (PC), in the 1970s. The cost of computers gradually became so low that personal computers by the 1990s, and then mobile computers (smartphones and tablets) in the 2000s, became ubiquitous.

Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick.The Lebombo bone from the mountains between Swaziland and South Africa may be the oldest known mathematical artifact. It dates from 35,000 BCE and consists of 29 distinct notches that were deliberately cut into a baboon's fibula. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example. The abacus was early used for arithmetic tasks. What we now call the Roman abacus was used in Babylonia as early as c. 2700–2300 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.

Several analog computers were constructed in ancient and medieval times to perform astronomical calculations. These included the astrolabe and Antikythera mechanism from the Hellenistic world (c. 150–100 BC). In Roman Egypt, Hero of Alexandria (c. 10–70 AD) made mechanical devices including automata and a programmable cart. Other early mechanical devices used to perform one or another type of calculations include the planisphere and other mechanical computing devices invented by Abu Rayhan al-Biruni (c. AD 1000); the equatorium and universal latitude-independent astrolabe by Abū Ishāq Ibrāhīm al-Zarqālī (c. AD 1015); the astronomical analog computers of other medieval Muslim astronomers and engineers; and the astronomical clock tower of Su Song (1094) during the Song dynasty. The castle clock, a hydropowered mechanical astronomical clock invented by Ismail al-Jazari in 1206, was the first programmable analog computer. Ramon Llull invented the Lullian Circle: a notional machine for calculating answers to philosophical questions (in this case, to do with Christianity) via logical combinatorics. This idea was taken up by Leibniz centuries later, and is thus one of the founding elements in computing and information science.

Scottish mathematician and physicist John Napier discovered that the multiplication and division of numbers could be performed by the addition and subtraction, respectively, of the logarithms of those numbers. While producing the first logarithmic tables, Napier needed to perform many tedious multiplications. It was at this point that he designed his 'Napier's bones', an abacus-like device that greatly simplified calculations that involved multiplication and division.
Since real numbers can be represented as distances or intervals on a line, the slide rule was invented in the 1620s, shortly after Napier's work, to allow multiplication and division operations to be carried out significantly faster than was previously possible. Edmund Gunter built a calculating device with a single logarithmic scale at the University of Oxford. His device greatly simplified arithmetic calculations, including multiplication and division. William Oughtred greatly improved this in 1630 with his circular slide rule. He followed this up with the modern slide rule in 1632, essentially a combination of two Gunter rules, held together with the hands. Slide rules were used by generations of engineers and other mathematically involved professional workers, until the invention of the pocket calculator.

Wilhelm Schickard, a German polymath, designed a calculating machine in 1623 which combined a mechanised form of Napier's rods with the world's first mechanical adding machine built into the base. Because it made use of a single-tooth gear there were circumstances in which its carry mechanism would jam. A fire destroyed at least one of the machines in 1624 and it is believed Schickard was too disheartened to build another.

In 1642, while still a teenager, Blaise Pascal started some pioneering work on calculating machines and after three years of effort and 50 prototypes he invented a mechanical calculator. He built twenty of these machines (called Pascal's calculator or Pascaline) in the following ten years. Nine Pascalines have survived, most of which are on display in European museums. A continuing debate exists over whether Schickard or Pascal should be regarded as the "inventor of the mechanical calculator" and the range of issues to be considered is discussed elsewhere.

Gottfried Wilhelm von Leibniz invented the stepped reckoner and his famous stepped drum mechanism around 1672. He attempted to create a machine that could be used not only for addition and subtraction but would utilise a moveable carriage to enable long multiplication and division. Leibniz once said "It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used." However, Leibniz did not incorporate a fully successful carry mechanism. Leibniz also described the binary numeral system, a central ingredient of all modern computers. However, up to the 1940s, many subsequent designs (including Charles Babbage's machines of the 1822 and even ENIAC of 1945) were based on the decimal system.

Around 1820, Charles Xavier Thomas de Colmar created what would over the rest of the century become the first successful, mass-produced mechanical calculator, the Thomas Arithmometer. It could be used to add and subtract, and with a moveable carriage the operator could also multiply, and divide by a process of long multiplication and long division. It utilised a stepped drum similar in conception to that invented by Leibniz. Mechanical calculators remained in use until the 1970s.

In 1804, French weaver Joseph Marie Jacquard developed a loom in which the pattern being woven was controlled by a paper tape constructed from punched cards. The paper tape could be changed without changing the mechanical design of the loom. This was a landmark achievement in programmability. His machine was an improvement over similar weaving looms. Punched cards were preceded by punch bands, as in the machine proposed by Basile Bouchon. These bands would inspire information recording for automatic pianos and more recently numerical control machine tools.
In the late 1880s, the American Herman Hollerith invented data storage on punched cards that could then be read by a machine. To process these punched cards, he invented the tabulator and the keypunch machine. His machines used electromechanical relays and counters. Hollerith's method was used in the 1890 United States Census. That census was processed two years faster than the prior census had been. Hollerith's company eventually became the core of IBM.

By 1920, electromechanical tabulating machines could add, subtract, and print accumulated totals. Machine functions were directed by inserting dozens of wire jumpers into removable control panels. When the United States instituted Social Security in 1935, IBM punched-card systems were used to process records of 26 million workers. Punched cards became ubiquitous in industry and government for accounting and administration.

Leslie Comrie's articles on punched-card methods and W. J. Eckert's publication of "Punched Card Methods in Scientific Computation" in 1940, described punched-card techniques sufficiently advanced to solve some differential equations or perform multiplication and division using floating point representations, all on punched cards and unit record machines. Such machines were used during World War II for cryptographic statistical processing, as well as a vast number of administrative uses. The Astronomical Computing Bureau, Columbia University, performed astronomical calculations representing the state of the art in computing.

By the 20th century, earlier mechanical calculators, cash registers, accounting machines, and so on were redesigned to use electric motors, with gear position as the representation for the state of a variable. The word "computer" was a job title assigned to primarily women who used these calculators to perform mathematical calculations. By the 1920s, British scientist Lewis Fry Richardson's interest in weather prediction led him to propose human computers and numerical analysis to model the weather; to this day, the most powerful computers on Earth are needed to adequately model its weather using the Navier–Stokes equations.

Companies like Friden, Marchant Calculator and Monroe made desktop mechanical calculators from the 1930s that could add, subtract, multiply and divide. In 1948, the Curta was introduced by Austrian inventor Curt Herzstark. It was a small, hand-cranked mechanical calculator and as such, a descendant of Gottfried Leibniz's Stepped Reckoner and Thomas's Arithmometer.

The world's first "all-electronic desktop" calculator was the British Bell Punch ANITA, released in 1961. It used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick. The tube technology was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a CRT, and introduced reverse Polish notation (RPN).

Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.

The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.

There was to be a store, or memory, capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.7 kB). An arithmetical unit, called the "mill", would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side. (Later drawings depict a regularized grid layout.) Like the central processing unit (CPU) in a modern computer, the mill would rely on its own internal procedures, roughly equivalent to microcode in modern CPUs, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.
The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards.

The machine was about a century ahead of its time. However, the project was slowed by various problems including disputes with the chief machinist building parts for it. All the parts for his machine had to be made by hand—this was a major problem for a machine with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Ada Lovelace translated and added notes to the ""Sketch of the Analytical Engine"" by Luigi Federico Menabrea. This appears to be the first published description of programming, so Ada Lovelace is widely regarded as the first computer programmer.

Following Babbage, although unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.

In the first half of the 20th century, analog computers were considered by many to be the future of computing. These devices used the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved, in contrast to digital computers that represented varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines.

The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson, later Lord Kelvin, in 1872. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location and was of great utility to navigation in shallow waters. His device was the foundation for further developments in analog computing.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. He explored the possible construction of such calculators, but was stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output.
An important advance in analog computing was the development of the first fire-control systems for long range ship gunlaying. When gunnery ranges increased dramatically in the late 19th century it was no longer a simple matter of calculating the proper aim point, given the flight times of the shells. Various spotters on board the ship would relay distance measures and observations to a central plotting station. There the fire direction teams fed in the location, speed and direction of the ship and its target, as well as various adjustments for Coriolis effect, weather effects on the air, and other adjustments; the computer would then output a firing solution, which would be fed to the turrets for laying. In 1912, British engineer Arthur Pollen developed the first electrically powered mechanical analogue computer (called at the time the Argo Clock). It was used by the Imperial Russian Navy in World War I. The alternative Dreyer Table fire control system was fitted to British capital ships by mid-1916.

Mechanical devices were also used to aid the accuracy of aerial bombing. Drift Sight was the first such aid, developed by Harry Wimperis in 1916 for the Royal Naval Air Service; it measured the wind speed from the air, and used that measurement to calculate the wind's effects on the trajectory of the bombs. The system was later improved with the Course Setting Bomb Sight, and reached a climax with World War II bomb sights, Mark XIV bomb sight (RAF Bomber Command) and the Norden (United States Army Air Forces).

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927, which built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious; the most powerful was constructed at the University of Pennsylvania's Moore School of Electrical Engineering, where the ENIAC was built.

A fully electronic analog computer was built by Helmut Hölzer in 1942 at Peenemünde Army Research Center

By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but hybrid analog computers, controlled by digital electronics, remained in substantial use into the 1950s and 1960s, and later in some specialized applications.

The principle of the modern computer was first described by computer scientist Alan Turing, who set out the idea in his seminal 1936 paper, "On Computable Numbers". Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the "Entscheidungsproblem" by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.

He also introduced the notion of a "universal machine" (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

The era of modern computing began with a flurry of development before and during World War II. Most digital computers built in this period were electromechanical – electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes.

The Z2 was one of the earliest examples of an electromechanical relay computer, and was created by German engineer Konrad Zuse in 1940. It was an improvement on his earlier Z1; although it used the same mechanical memory, it replaced the arithmetic and control logic with electrical relay circuits.
In the same year, electro-mechanical devices called bombes were built by British cryptologists to help decipher German Enigma-machine-encrypted secret messages during World War II. The bombe's initial design was created in 1939 at the UK Government Code and Cypher School (GC&CS) at Bletchley Park by Alan Turing, with an important refinement devised in 1940 by Gordon Welchman. The engineering design and construction was the work of Harold Keen of the British Tabulating Machine Company. It was a substantial development from a device that had been designed in 1938 by Polish Cipher Bureau cryptologist Marian Rejewski, and known as the "cryptologic bomb" (Polish: ""bomba kryptologiczna"").

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22-bit word length that operated at a clock frequency of about 5–10 Hz. Program code and data were stored on punched film. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was probably a Turing-complete machine. In two 1936 patent applications, Zuse also anticipated that machine instructions could be stored in the same storage used for data—the key insight of what became known as the von Neumann architecture, first implemented in 1948 in America in the electromechanical IBM SSEC and in Britain in the fully electronic Manchester Baby.

Zuse suffered setbacks during World War II when some of his machines were destroyed in the course of Allied bombing campaigns. Apparently his work remained largely unknown to engineers in the UK and US until much later, although at least IBM was aware of it as it financed his post-war startup company in 1946 in return for an option on Zuse's patents.

In 1944, the Harvard Mark I was constructed at IBM's Endicott laboratories; it was a similar general purpose electro-mechanical computer to the Z3, but was not quite Turing-complete.

The term digital was first suggested by George Robert Stibitz and refers to where a signal, such as a voltage, is not used to directly represent a value (as it would be in an analog computer), but to encode it. In November 1937, George Stibitz, then working at Bell Labs (1930–1941), completed a relay-based calculator he later dubbed the "Model K" (for "kitchen table", on which he had assembled it), which became the first binary adder. Typically signals have two states – low (usually representing 0) and high (usually representing 1), but sometimes three-valued logic is used, especially in high-density memory. Modern computers generally use binary logic, but many early machines were decimal computers. In these machines, the basic unit of data was the decimal digit, encoded in one of several schemes, including binary-coded decimal or BCD, bi-quinary, excess-3, and two-out-of-five code.

The mathematical basis of digital computing is Boolean algebra, developed by the British mathematician George Boole in his work "The Laws of Thought", published in 1854. His Boolean algebra was further refined in the 1860s by William Jevons and Charles Sanders Peirce, and was first presented systematically by Ernst Schröder and A. N. Whitehead. In 1879 Gottlob Frege develops the formal approach to logic and proposes the first logic language for logical equations.

In the 1930s and working independently, American electronic engineer Claude Shannon and Soviet logician Victor Shestakov both showed a one-to-one correspondence between the concepts of Boolean logic and certain electrical circuits, now called logic gates, which are now ubiquitous in digital computers. They showed that electronic relays and switches can realize the expressions of Boolean algebra. This thesis essentially founded practical digital circuit design.

Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. Machines such as the Z3, the Atanasoff–Berry Computer, the Colossus computers, and the ENIAC were built by hand, using circuits containing relays or valves (vacuum tubes), and often used punched cards or punched paper tape for input and as the main (non-volatile) storage medium.

The engineer Tommy Flowers joined the telecommunications branch of the General Post Office in 1926. While working at the research station in Dollis Hill in the 1930s, he began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.

In the US, in 1940 Arthur Dickinson (IBM) invented the first digital electronic computer. This calculating device was fully electronic – control, calculations and output (the first electronic display). John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed the Atanasoff–Berry Computer (ABC) in 1942, the first binary electronic digital calculating device. This design was semi-electronic (electro-mechanical control and electronic calculations), and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory. However, its paper card writer/reader was unreliable and the regenerative drum contact system was mechanical. The machine's special-purpose nature and lack of changeable, stored program distinguish it from modern computers.

Computers whose logic was primarily built using vacuum tubes are now known as first generation computers.

During World War II, British codebreakers at Bletchley Park, north of London, achieved a number of successes at breaking encrypted enemy military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. Women often operated these bombe machines. They ruled out possible Enigma settings by performing chains of logical deductions implemented electrically. Most possibilities led to a contradiction, and the few remaining could be tested by hand.

The Germans also developed a series of teleprinter encryption systems, quite different from Enigma. The Lorenz SZ 40/42 machine was used for high-level Army communications, code-named "Tunny" by the British. The first intercepts of Lorenz messages began in 1941. As part of an attack on Tunny, Max Newman and his colleagues developed the Heath Robinson, a fixed-function machine to aid in code breaking. Tommy Flowers, a senior engineer at the Post Office Research Station was recommended to Max Newman by Alan Turing and spent eleven months from early February 1943 designing and building the more flexible Colossus computer (which superseded the Heath Robinson). After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.
Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Data input to Colossus was by photoelectric reading of a paper tape transcription of the enciphered intercepted message. This was arranged in a continuous loop so that it could be read and re-read multiple times – there being no internal store for the data. The reading mechanism ran at 5,000 characters per second with the paper tape moving at . Colossus Mark 1 contained 1500 thermionic valves (tubes), but Mark 2 with 2400 valves and five processors in parallel, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process. Mark 2 was designed while Mark 1 was being constructed. Allen Coombs took over leadership of the Colossus Mark 2 project when Tommy Flowers moved on to other projects. The first Mark 2 Colossus became operational on 1 June 1944, just in time for the Allied Invasion of Normandy on D-Day.

Most of the use of Colossus was in determining the start positions of the Tunny rotors for a message, which was called "wheel setting". Colossus included the first-ever use of shift registers and systolic arrays, enabling five simultaneous tests, each involving up to 100 Boolean calculations. This enabled five different possible start positions to be examined for one transit of the paper tape. As well as wheel setting some later Colossi included mechanisms intended to help determine pin patterns known as "wheel breaking". Both models were programmable using switches and plug panels in a way their predecessors had not been. Ten Mk 2 Colossi were operational by the end of the war.
Without the use of these machines, the Allies would have been deprived of the very valuable intelligence that was obtained from reading the vast quantity of enciphered high-level telegraphic messages between the German High Command (OKW) and their army commands throughout occupied Europe. Details of their existence, design, and use were kept secret well into the 1970s. Winston Churchill personally issued an order for their destruction into pieces no larger than a man's hand, to keep secret that the British were capable of cracking Lorenz SZ cyphers (from German rotor stream cipher machines) during the oncoming Cold War. Two of the machines were transferred to the newly formed GCHQ and the others were destroyed. As a result, the machines were not included in many histories of computing. A reconstructed working copy of one of the Colossus machines is now on display at Bletchley Park.

The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were women who had been trained as mathematicians.

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High-speed memory was limited to 20 words (equivalent to about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors. One of its major engineering feats was to minimize the effects of tube burnout, which was a common problem in machine reliability at that time. The machine was in almost constant use for the next ten years.

Early computing machines were programmable in the sense that they could follow the sequence of steps they had been set up to execute, but the "program", or steps that the machine was to execute, were set up usually by changing how the wires were plugged into a patch panel or plugboard. "Reprogramming", when it was possible at all, was a laborious process, starting with engineers working out flowcharts, designing the new set up, and then the often-exacting process of physically re-wiring patch panels. Stored-program computers, by contrast, were designed to store a set of instructions (a program), in memory – typically the same memory as stored data.

The theoretical basis for the stored-program computer had been proposed by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began his work on developing an electronic stored-program digital computer. His 1945 report ‘Proposed Electronic Calculator’ was the first specification for such a device.

Meanwhile, John von Neumann at the Moore School of Electrical Engineering, University of Pennsylvania, circulated his "First Draft of a Report on the EDVAC" in 1945. Although substantially similar to Turing's design and containing comparatively little engineering detail, the computer architecture it outlined became known as the "von Neumann architecture". Turing presented a more detailed paper to the National Physical Laboratory (NPL) Executive Committee in 1946, giving the first reasonably complete design of a stored-program computer, a device he called the Automatic Computing Engine (ACE). However, the better-known EDVAC design of John von Neumann, who knew of Turing's theoretical work, received more publicity, despite its incomplete nature and questionable lack of attribution of the sources of some of the ideas.

Turing thought that the speed and the size of computer memory were crucial elements, so he proposed a high-speed memory of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used "Abbreviated Computer Instructions," an early form of programming language.

The Manchester Baby was the world's first electronic stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.

The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, the first random-access digital storage device. Invented by Freddie Williams and Tom Kilburn at the University of Manchester in 1946 and 1947, it was a cathode ray tube that used an effect called secondary emission to temporarily store electronic binary data, and was used successfully in several early computers.

Although the computer was small and primitive by the standards of the 1990s, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.

The Baby had a 32-bit word length and a memory of 32 words. As it was designed to be the simplest possible stored-program computer, the only arithmetic operations implemented in hardware were subtraction and negation; other arithmetic operations were implemented in software. The first of three programs written for the machine found the highest proper divisor of 2 (262,144), a calculation that was known would take a long time to run—and so prove the computer's reliability—by testing every integer from 2 − 1 downwards, as division was implemented by repeated subtraction of the divisor. The program consisted of 17 instructions and ran for 52 minutes before reaching the correct answer of 131,072, after the Baby had performed 3.5 million operations (for an effective CPU speed of 1.1 kIPS).

The Experimental machine led on to the development of the Manchester Mark 1 at the University of Manchester. Work began in August 1948, and the first version was operational by April 1949; a program written to search for Mersenne primes ran error-free for nine hours on the night of 16/17 June 1949.
The machine's successful operation was widely reported in the British press, which used the phrase "electronic brain" in describing it to their readers.

The computer is especially historically significant because of its pioneering inclusion of index registers, an innovation which made it easier for a program to read sequentially through an array of words in memory. Thirty-four patents resulted from the machine's development, and many of the ideas behind its design were incorporated in subsequent commercial products such as the and 702 as well as the Ferranti Mark 1. The chief designers, Frederic C. Williams and Tom Kilburn, concluded from their experiences with the Mark 1 that computers would be used more in scientific roles than in pure mathematics. In 1951 they started development work on Meg, the Mark 1's successor, which would include a floating point unit.

The other contender for being the first recognizably modern digital stored-program computer was the EDSAC, designed and constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England at the University of Cambridge in 1949. The machine was inspired by John von Neumann's seminal "First Draft of a Report on the EDVAC" and was one of the first usefully operational electronic digital stored-program computer.

EDSAC ran its first programs on 6 May 1949, when it calculated a table of squares and a list of prime numbers.The EDSAC also served as the basis for the first commercially applied computer, the LEO I, used by food manufacturing company J. Lyons & Co. Ltd. EDSAC 1 and was finally shut down on 11 July 1958, having been superseded by EDSAC 2 which stayed in use until 1965.

ENIAC inventors John Mauchly and J. Presper Eckert proposed the EDVAC's construction in August 1944, and design work for the EDVAC commenced at the University of Pennsylvania's Moore School of Electrical Engineering, before the ENIAC was fully operational. The design implemented a number of important architectural and logical improvements conceived during the ENIAC's construction, and a high-speed serial-access memory. However, Eckert and Mauchly left the project and its construction floundered.

It was finally delivered to the U.S. Army's Ballistics Research Laboratory at the Aberdeen Proving Ground in August 1949, but due to a number of problems, the computer only began operation in 1951, and then only on a limited basis.

The first commercial computer was the Ferranti Mark 1, built by Ferranti and delivered to the University of Manchester in February 1951. It was based on the Manchester Mark 1. The main improvements over the Manchester Mark 1 were in the size of the primary storage (using random access Williams tubes), secondary storage (using a magnetic drum), a faster multiplier, and additional instructions. The basic cycle time was 1.2 milliseconds, and a multiplication could be completed in about 2.16 milliseconds. The multiplier used almost a quarter of the machine's 4,050 vacuum tubes (valves). A second machine was purchased by the University of Toronto, before the design was revised into the Mark 1 Star. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.

In October 1947, the directors of J. Lyons & Company, a British catering company famous for its teashops but with strong interests in new office management techniques, decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job. On 17 November 1951, the J. Lyons company began weekly operation of a bakery valuations job on the LEO (Lyons Electronic Office). This was the first business to go live on a stored program computer.
In June 1951, the UNIVAC I (Universal Automatic Computer) was delivered to the U.S. Census Bureau. Remington Rand eventually sold 46 machines at more than US$1 million each ($ as of 2020). UNIVAC was the first "mass produced" computer. It used 5,200 vacuum tubes and consumed 125 kW of power. Its primary storage was serial-access mercury delay lines capable of storing 1,000 words of 11 decimal digits plus sign (72-bit words).

IBM introduced a smaller, more affordable computer in 1954 that proved very popular. The IBM 650 weighed over 900 kg, the attached power supply weighed around 1350 kg and both were held in separate cabinets of roughly 1.5 meters by 0.9 meters by 1.8 meters. It cost US$500,000 ($ as of 2020) or could be leased for US$3,500 a month ($ as of 2020). Its drum memory was originally 2,000 ten-digit words, later expanded to 4,000 words. Memory limitations such as this were to dominate programming for decades afterward. The program instructions were fetched from the spinning drum as the code ran. Efficient execution using drum memory was provided by a combination of hardware architecture: the instruction format included the address of the next instruction; and software: the Symbolic Optimal Assembly Program, SOAP, assigned instructions to the optimal addresses (to the extent possible by static analysis of the source program). Thus many instructions were, when needed, located in the next row of the drum to be read and additional wait time for drum rotation was not required.

In 1951, British scientist Maurice Wilkes developed the concept of microprogramming from the realisation that the central processing unit of a computer could be controlled by a miniature, highly specialised computer program in high-speed ROM. Microprogramming allows the base instruction set to be defined or extended by built-in programs (now called firmware or microcode). This concept greatly simplified CPU development. He first described this at the University of Manchester Computer Inaugural Conference in 1951, then published in expanded form in "IEEE Spectrum" in 1955.

It was widely used in the CPUs and floating-point units of mainframe and other computers; it was implemented for the first time in EDSAC 2, which also used multiple identical "bit slices" to simplify design. Interchangeable, replaceable tube assemblies were used for each bit of the processor.

Magnetic drum memories were developed for the US Navy during WW II with the work continuing at Engineering Research Associates (ERA) in 1946 and 1947. ERA, then a part of Univac included a drum memory in its 1103, announced in February 1953. The first mass-produced computer, the IBM 650, also announced in 1953 had about 8.5 kilobytes of drum memory.

Magnetic core memory patented in 1949 with its first usage demonstrated for the Whirlwind computer in August 1953. Commercialization followed quickly. Magnetic core was used in peripherals of the IBM 702 delivered in July 1955, and later in the 702 itself. The IBM 704 (1955) and the Ferranti Mercury (1957) used magnetic-core memory. It went on to dominate the field into the 1970s, when it was replaced with semiconductor memory. Magnetic core peaked in volume about 1975 and declined in usage and market share thereafter.

As late as 1980, PDP-11/45 machines using magnetic-core main memory and drums for swapping were still in use at many of the original UNIX sites.

The bipolar transistor was invented in 1947. From 1955 onward transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. Transistors greatly reduced computers' size, initial cost, and operating cost. Typically, second-generation computers were composed of large numbers of printed circuit boards such as the IBM Standard Modular System, each carrying one to four logic gates or flip-flops.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Initially the only devices available were germanium point-contact transistors, less reliable than the valves they replaced but which consumed far less power. Their first transistorised computer, and the first in the world, was operational by 1953, and a second version was completed there in April 1955. The 1955 version used 200 transistors, 1,300 solid-state diodes, and had a power consumption of 150 watts. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer.

That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell. The design featured a 64-kilobyte magnetic drum memory store with multiple moving heads that had been designed at the National Physical Laboratory, UK. By 1953 this team had transistor circuits operating to read and write on a smaller magnetic drum from the Royal Radar Establishment. The machine used a low clock speed of only 58 kHz to avoid having to use any valves to generate the clock waveforms.

CADET used 324 point-contact transistors provided by the UK company Standard Telephones and Cables; 76 junction transistors were used for the first stage amplifiers for data read from the drum, since point-contact transistors were too noisy. From August 1956 CADET was offering a regular computing service, during which it often executed continuous computing runs of 80 hours or more. Problems with the reliability of early batches of point contact and alloyed junction transistors meant that the machine's mean time between failures was about 90 minutes, but this improved once the more reliable bipolar junction transistors became available.

The Manchester University Transistor Computer's design was adopted by the local engineering firm of Metropolitan-Vickers in their Metrovick 950, the first commercial transistor computer anywhere. Six Metrovick 950s were built, the first completed in 1956. They were successfully deployed within various departments of the company and were in use for about five years. A second generation computer, the IBM 1401, captured about one third of the world market. IBM installed more than ten thousand 1401s between 1960 and 1964.

Transistorized electronics improved not only the CPU (Central Processing Unit), but also the peripheral devices. The second generation disk data storage units were able to store tens of millions of letters and digits. Next to the fixed disk storage units, connected to the CPU via high-speed data transmission, were removable disk data storage units. A removable disk pack can be easily exchanged with another pack in a few seconds. Even if the removable disks' capacity is smaller than fixed disks, their interchangeability guarantees a nearly unlimited quantity of data close at hand. Magnetic tape provided archival capability for this data, at a lower cost than disk.

Many second-generation CPUs delegated peripheral device communications to a secondary processor. For example, while the communication processor controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took at least two memory cycles; one for the instruction, one for the operand data fetch.

During the second generation remote terminal units (often in the form of Teleprinters like a Friden Flexowriter) saw greatly increased use. Telephone connections provided sufficient speed for early remote terminals and allowed hundreds of kilometers separation between remote-terminals and the computing center. Eventually these stand-alone computer networks would be generalized into an interconnected "network of networks"—the Internet.

The early 1960s saw the advent of supercomputing. The Atlas was a joint development between the University of Manchester, Ferranti, and Plessey, and was first installed at Manchester University and officially commissioned in 1962 as one of the world's first supercomputers – considered to be the most powerful computer in the world at that time. It was said that whenever Atlas went offline half of the United Kingdom's computer capacity was lost. It was a second-generation machine, using discrete germanium transistors. Atlas also pioneered the Atlas Supervisor, "considered by many to be the first recognisable modern operating system".

In the US, a series of computers at Control Data Corporation (CDC) were designed by Seymour Cray to use innovative designs and parallelism to achieve superior computational peak performance. The CDC 6600, released in 1964, is generally considered the first supercomputer. The CDC 6600 outperformed its predecessor, the IBM 7030 Stretch, by about a factor of 3. With performance of about 1 megaFLOPS, the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.

The "third-generation" of digital electronic computers used integrated circuit (IC) chips as the basis of their logic.

The idea of an integrated circuit was conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer.

The first working integrated circuits were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. Kilby's invention was a hybrid integrated circuit (hybrid IC). It had external wire connections, which made it difficult to mass-produce.

Noyce came up with his own idea of an integrated circuit half a year after Kilby. Noyce's invention was a monolithic integrated circuit (IC) chip. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. The basis for Noyce's monolithic IC was Fairchild's planar process, which allowed integrated circuits to be laid out using the same principles as those of printed circuits. The planar process was developed by Noyce's colleague Jean Hoerni in early 1959, based on the silicon surface passivation and thermal oxidation processes developed by Mohamed M. Atalla at Bell Labs in the late 1950s.

Third generation (integrated circuit) computers first appeared in the early 1960s in computers developed for government purposes, and then in commercial computers beginning in the mid-1960s.

The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959. In addition to data processing, the MOSFET enabled the practical use of MOS transistors as memory cell storage elements, a function previously served by magnetic cores. Semiconductor memory, also known as MOS memory, was cheaper and consumed less power than magnetic-core memory. MOS random-access memory (RAM), in the form of static RAM (SRAM), was developed by John Schmidt at Fairchild Semiconductor in 1964. In 1966, Robert Dennard at the IBM Thomas J. Watson Research Center developed MOS dynamic RAM (DRAM). In 1967, Dawon Kahng and Simon Sze at Bell Labs developed the floating-gate MOSFET, the basis for MOS non-volatile memory such as EPROM, EEPROM and flash memory.

The "fourth-generation" of digital electronic computers used microprocessors as the basis of their logic. The microprocessor has origins in the MOS integrated circuit (MOS IC) chip. The MOS IC was first proposed by Mohamed M. Atalla at Bell Labs in 1960, and then fabricated by Fred Heiman and Steven Hofstein at RCA in 1962. Due to rapid MOSFET scaling, MOS IC chips rapidly increased in complexity at a rate predicted by Moore's law, leading to large-scale integration (LSI) with hundreds of transistors on a single MOS chip by the late 1960s. The application of MOS LSI chips to computing was the basis for the first microprocessors, as engineers began recognizing that a complete computer processor could be contained on a single MOS LSI chip.

The subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor". The earliest multi-chip microprocessors were the Four-Phase Systems AL-1 in 1969 and Garrett AiResearch MP944 in 1970, developed with multiple MOS LSI chips. The first single-chip microprocessor was the Intel 4004, developed on a single PMOS LSI chip. It was designed and realized by Ted Hoff, Federico Faggin, Masatoshi Shima and Stanley Mazor at Intel, and released in 1971. Tadashi Sasaki and Masatoshi Shima at Busicom, a calculator manufacturer, had the initial insight that the CPU could be a single MOS LSI chip, supplied by Intel.
While the earliest microprocessor ICs literally contained only the processor, i.e. the central processing unit, of a computer, their progressive development naturally led to chips containing most or all of the internal electronic parts of a computer. The integrated circuit in the image on the right, for example, an Intel 8742, is an 8-bit microcontroller that includes a CPU running at 12 MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip.

During the 1960s there was considerable overlap between second and third generation technologies. IBM implemented its IBM Solid Logic Technology modules in hybrid circuits for the IBM System/360 in 1964. As late as 1975, Sperry Univac continued the manufacture of second-generation machines such as the UNIVAC 494. The Burroughs large systems such as the B5000 were stack machines, which allowed for simpler programming. These pushdown automatons were also implemented in minicomputers and microprocessors later, which influenced programming language design. Minicomputers served as low-cost computer centers for industry, business and universities. It became possible to simulate analog circuits with the "simulation program with integrated circuit emphasis", or SPICE (1971) on minicomputers, one of the programs for electronic design automation ().
The microprocessor led to the development of the microcomputer, small, low-cost computers that could be owned by individuals and small businesses. Microcomputers, the first of which appeared in the 1970s, became ubiquitous in the 1980s and beyond.
While which specific system is considered the first microcomputer is a matter of debate, as there were several unique hobbyist systems developed based on the Intel 4004 and its successor, the Intel 8008, the first commercially available microcomputer kit was the Intel 8080-based Altair 8800, which was announced in the January 1975 cover article of "Popular Electronics". However, this was an extremely limited system in its initial stages, having only 256 bytes of DRAM in its initial package and no input-output except its toggle switches and LED register display. Despite this, it was initially surprisingly popular, with several hundred sales in the first year, and demand rapidly outstripped supply. Several early third-party vendors such as Cromemco and Processor Technology soon began supplying additional S-100 bus hardware for the Altair 8800.

In April 1975 at the Hannover Fair, Olivetti presented the P6060, the world's first complete, pre-assembled personal computer system. The central processing unit consisted of two cards, code named PUCE1 and PUCE2, and unlike most other personal computers was built with TTL components rather than a microprocessor. It had one or two 8" floppy disk drives, a 32-character plasma display, 80-column graphical thermal printer, 48 Kbytes of RAM, and BASIC language. It weighed . As a complete system, this was a significant step from the Altair, though it never achieved the same success. It was in competition with a similar product by IBM that had an external floppy disk drive.

From 1975 to 1977, most microcomputers, such as the MOS Technology KIM-1, the Altair 8800, and some versions of the Apple I, were sold as kits for do-it-yourselfers. Pre-assembled systems did not gain much ground until 1977, with the introduction of the Apple II, the Tandy TRS-80, the first SWTPC computers, and the Commodore PET. Computing has evolved with microcomputer architectures, with features added from their larger brethren, now dominant in most market segments.

A NeXT Computer and its object-oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN httpd, and also used to write the first web browser, WorldWideWeb.

Systems as complicated as computers require very high reliability. ENIAC remained on, in continuous operation from 1947 to 1955, for eight years before being shut down. Although a vacuum tube might fail, it would be replaced without bringing down the system. By the simple strategy of never shutting down ENIAC, the failures were dramatically reduced. The vacuum-tube SAGE air-defense computers became remarkably reliable – installed in pairs, one off-line, tubes likely to fail did so when the computer was intentionally run at reduced power to find them. Hot-pluggable hard disks, like the hot-pluggable vacuum tubes of yesteryear, continue the tradition of repair during continuous operation. Semiconductor memories routinely have no errors when they operate, although operating systems like Unix have employed memory tests on start-up to detect failing hardware. Today, the requirement of reliable performance is made even more stringent when server farms are the delivery platform. Google has managed this by using fault-tolerant software to recover from hardware failures, and is even working on the concept of replacing entire server farms on-the-fly, during a service event.

In the 21st century, multi-core CPUs became commercially available. Content-addressable memory (CAM) has become inexpensive enough to be used in networking, and is frequently used for on-chip cache memory in modern microprocessors, although no computer system has yet implemented hardware CAMs for use in programming languages. Currently, CAMs (or associative arrays) in software are programming-language-specific. Semiconductor memory cell arrays are very regular structures, and manufacturers prove their processes on them; this allows price reductions on memory products. During the 1980s, CMOS logic gates developed into devices that could be made as fast as other circuit types; computer power consumption could therefore be decreased dramatically. Unlike the continuous current draw of a gate based on other logic types, a CMOS gate only draws significant current during the 'transition' between logic states, except for leakage.

This has allowed computing to become a commodity which is now ubiquitous, embedded in many forms, from greeting cards and telephones to satellites. The thermal design power which is dissipated during operation has become as essential as computing speed of operation. In 2006 servers consumed 1.5% of the total energy budget of the U.S. The energy consumption of computer data centers was expected to double to 3% of world consumption by 2011. The SoC (system on a chip) has compressed even more of the integrated circuitry into a single chip; SoCs are enabling phones and PCs to converge into single hand-held wireless mobile devices.

MIT Technology Review reported 10 November 2017 that IBM has created a 50-qubit computer; currently its quantum state lasts 50 microseconds. Physical Review X reported a technique for 'single-gate sensing as a viable readout method for spin qubits' (a singlet-triplet spin state in silicon) on 26 November 2018. A Google team has succeeded in operating their RF pulse modulator chip at 3 Kelvin, simplifying the cryogenics of their 72-qubit computer, which is setup to operate at 0.3 Kelvin; but the readout circuitry and another driver remain to be brought into the cryogenics. "See: Quantum supremacy" Silicon qubit systems have demonstrated entanglement at non-local distances.

Computing hardware and its software have even become a metaphor for the operation of the universe.

An indication of the rapidity of development of this field can be inferred from the history of the seminal 1947 article by Burks, Goldstine and von Neumann. By the time that anyone had time to write anything down, it was obsolete. After 1945, others read John von Neumann's "First Draft of a Report on the EDVAC", and immediately started implementing their own systems. To this day, the rapid pace of development has continued, worldwide.

A 1966 article in "Time" predicted that: "By 2000, the machines will be producing so much that everyone in the U.S. will, in effect, be independently wealthy. How to use leisure time will be a major problem."





</doc>
<doc id="13637" url="https://en.wikipedia.org/wiki?curid=13637" title="Hausdorff space">
Hausdorff space

In topology and related branches of mathematics, a Hausdorff space, separated space or T space is a topological space where for any two distinct points there exist neighbourhoods of each which are disjoint from each other. Of the many separation axioms that can be imposed on a topological space, the "Hausdorff condition" (T) is the most frequently used and discussed. It implies the uniqueness of limits of sequences, nets, and filters.

Hausdorff spaces are named after Felix Hausdorff, one of the founders of topology. Hausdorff's original definition of a topological space (in 1914) included the Hausdorff condition as an axiom.

Points formula_1 and formula_2 in a topological space formula_3 can be "separated by neighbourhoods" if there exists a neighbourhood formula_4 of formula_1 and a neighbourhood formula_6 of formula_2 such that formula_4 and formula_6 are disjoint (formula_10).
formula_3 is a Hausdorff space if all distinct points in formula_3 are pairwise neighbourhood-separable. This condition is the third separation axiom (after formula_13), which is why Hausdorff spaces are also called formula_14 spaces. The name "separated space" is also used.

A related, but weaker, notion is that of a preregular space. formula_3 is a preregular space if any two topologically distinguishable points can be separated by disjoint neighbourhoods. Preregular spaces are also called "formula_16 spaces".

The relationship between these two conditions is as follows. A topological space is Hausdorff if and only if it is both preregular (i.e. topologically distinguishable points are separated by neighbourhoods) and Kolmogorov (i.e. distinct points are topologically distinguishable). A topological space is preregular if and only if its Kolmogorov quotient is Hausdorff.

For a topological space "X", the following are equivalent:


Almost all spaces encountered in analysis are Hausdorff; most importantly, the real numbers (under the standard metric topology on real numbers) are a Hausdorff space. More generally, all metric spaces are Hausdorff. In fact, many spaces of use in analysis, such as topological groups and topological manifolds, have the Hausdorff condition explicitly stated in their definitions.

A simple example of a topology that is T but is not Hausdorff is the cofinite topology defined on an infinite set.

Pseudometric spaces typically are not Hausdorff, but they are preregular, and their use in analysis is usually only in the construction of Hausdorff gauge spaces. Indeed, when analysts run across a non-Hausdorff space, it is still probably at least preregular, and then they simply replace it with its Kolmogorov quotient, which is Hausdorff.

In contrast, non-preregular spaces are encountered much more frequently in abstract algebra and algebraic geometry, in particular as the Zariski topology on an algebraic variety or the spectrum of a ring. They also arise in the model theory of intuitionistic logic: every complete Heyting algebra is the algebra of open sets of some topological space, but this space need not be preregular, much less Hausdorff, and in fact usually is neither. The related concept of Scott domain also consists of non-preregular spaces.

While the existence of unique limits for convergent nets and filters implies that a space is Hausdorff, there are non-Hausdorff T spaces in which every convergent sequence has a unique limit.

Subspaces and products of Hausdorff spaces are Hausdorff, but quotient spaces of Hausdorff spaces need not be Hausdorff. In fact, "every" topological space can be realized as the quotient of some Hausdorff space.

Hausdorff spaces are T, meaning that all singletons are closed. Similarly, preregular spaces are R.

Another nice property of Hausdorff spaces is that compact sets are always closed. This may fail in non-Hausdorff spaces such as the Sierpiński space. 

The definition of a Hausdorff space says that points can be separated by neighborhoods. It turns out that this implies something which is seemingly stronger: in a Hausdorff space every pair of disjoint compact sets can also be separated by neighborhoods, in other words there is a neighborhood of one set and a neighborhood of the other, such that the two neighborhoods are disjoint. This is an example of the general rule that compact sets often behave like points.

Compactness conditions together with preregularity often imply stronger separation axioms. For example, any locally compact preregular space is completely regular. Compact preregular spaces are normal, meaning that they satisfy Urysohn's lemma and the Tietze extension theorem and have partitions of unity subordinate to locally finite open covers. The Hausdorff versions of these statements are: every locally compact Hausdorff space is Tychonoff, and every compact Hausdorff space is normal Hausdorff.

The following results are some technical properties regarding maps (continuous and otherwise) to and from Hausdorff spaces.

Let "f" : "X" → "Y" be a continuous function and suppose "Y" is Hausdorff. Then the graph of "f", formula_17, is a closed subset of "X" × "Y".

Let "f" : "X" → "Y" be a function and let formula_18 be its kernel regarded as a subspace of "X" × "X".

If "f,g" : "X" → "Y" are continuous maps and "Y" is Hausdorff then the equalizer formula_19 is closed in "X". It follows that if "Y" is Hausdorff and "f" and "g" agree on a dense subset of "X" then "f" = "g". In other words, continuous functions into Hausdorff spaces are determined by their values on dense subsets.

Let "f" : "X" → "Y" be a closed surjection such that "f"("y") is compact for all "y" ∈ "Y". Then if "X" is Hausdorff so is "Y".

Let "f" : "X" → "Y" be a quotient map with "X" a compact Hausdorff space. Then the following are equivalent:

All regular spaces are preregular, as are all Hausdorff spaces. There are many results for topological spaces that hold for both regular and Hausdorff spaces.
Most of the time, these results hold for all preregular spaces; they were listed for regular and Hausdorff spaces separately because the idea of preregular spaces came later.
On the other hand, those results that are truly about regularity generally do not also apply to nonregular Hausdorff spaces.

There are many situations where another condition of topological spaces (such as paracompactness or local compactness) will imply regularity if preregularity is satisfied.
Such conditions often come in two versions: a regular version and a Hausdorff version.
Although Hausdorff spaces are not, in general, regular, a Hausdorff space that is also (say) locally compact will be regular, because any Hausdorff space is preregular.
Thus from a certain point of view, it is really preregularity, rather than regularity, that matters in these situations.
However, definitions are usually still phrased in terms of regularity, since this condition is better known than preregularity.

See History of the separation axioms for more on this issue.

The terms "Hausdorff", "separated", and "preregular" can also be applied to such variants on topological spaces as uniform spaces, Cauchy spaces, and convergence spaces.
The characteristic that unites the concept in all of these examples is that limits of nets and filters (when they exist) are unique (for separated spaces) or unique up to topological indistinguishability (for preregular spaces).

As it turns out, uniform spaces, and more generally Cauchy spaces, are always preregular, so the Hausdorff condition in these cases reduces to the T condition.
These are also the spaces in which completeness makes sense, and Hausdorffness is a natural companion to completeness in these cases.
Specifically, a space is complete if and only if every Cauchy net has at "least" one limit, while a space is Hausdorff if and only if every Cauchy net has at "most" one limit (since only Cauchy nets can have limits in the first place).

The algebra of continuous (real or complex) functions on a compact Hausdorff space is a commutative C*-algebra, and conversely by the Banach–Stone theorem one can recover the topology of the space from the algebraic properties of its algebra of continuous functions. This leads to noncommutative geometry, where one considers noncommutative C*-algebras as representing algebras of functions on a noncommutative space.





</doc>
<doc id="13644" url="https://en.wikipedia.org/wiki?curid=13644" title="Hawkwind">
Hawkwind

Hawkwind are an English rock band known as one of the earliest space rock groups. Since their formation in November 1969, Hawkwind have gone through many incarnations and have incorporated many different styles into their music, including hard rock, progressive rock and psychedelic rock. They are also regarded as an influential proto-punk band. Their lyrics favour urban and science fiction themes.

Many musicians, dancers and writers have worked with the band since their inception. Notable musicians who have performed in Hawkwind include Lemmy, Ginger Baker, Robert Calvert, Nik Turner and Huw Lloyd-Langton. However, the band are most closely associated with their founder, singer, songwriter and guitarist Dave Brock, who is the only remaining original member. 

Hawkwind are best known for the song "Silver Machine", which became a number three UK hit single in 1972, but they scored further hit singles with "Urban Guerrilla" (another Top 40 hit) and "Shot Down in the Night". The band had a run of twenty-two of their albums charting in the UK from 1971 to 1993. 

Dave Brock and Mick Slattery had been in the London-based psychedelic band Famous Cure, and a meeting with bassist John Harrison revealed a mutual interest in electronic music which led the trio to embark upon a new musical venture together. Seventeen-year-old drummer Terry Ollis replied to an advert in a music weekly, while Nik Turner and Michael "Dik Mik" Davies, old acquaintances of Brock, offered help with transport and gear, but were soon pulled into the band.

Gatecrashing a local talent night at the All Saints Hall, Notting Hill, they were so disorganised as to not even have a name, opting for "Group X" at the last minute, nor any songs, choosing to play an extended 20-minute jam on the Byrds' "Eight Miles High". BBC Radio 1 DJ John Peel was in the audience and was impressed enough to tell event organiser, Douglas Smith, to keep an eye on them. Smith signed them up and got them a deal with Liberty Records on the back of a deal he was setting up for Cochise.

The band settled on the name "Hawkwind" after briefly being billed as "Group X" and "Hawkwind Zoo".

An Abbey Road session took place recording demos of "Hurry on Sundown" and others (included on the remasters version of "Hawkwind"), after which Slattery left to be replaced by Huw Lloyd-Langton, who had known Brock from his days working in a music shop selling guitar strings to Brock, then a busker.

Pretty Things guitarist Dick Taylor was brought in to produce the 1970 debut album "Hawkwind". Although it was not a commercial success, it did bring them to the attention of the UK underground scene, which found them playing free concerts, benefit gigs, and festivals. Playing free outside the Bath Festival, they encountered another Ladbroke Grove based band, the Pink Fairies, who shared similar interests in music and recreational activities; a friendship developed which led to the two bands becoming running partners and performing as "Pinkwind". Their use of drugs, however, led to the departure of Harrison, who did not partake, to be replaced briefly by Thomas Crimble (about July 1970 – March 1971). Crimble played on a few BBC sessions before leaving to help organise the Glastonbury Free Festival 1971; he sat in during the band's performance there. Lloyd-Langton also quit, after a bad LSD trip at the Isle of Wight Festival led to a nervous breakdown.

Their follow-up album, 1971's "In Search of Space", brought greater commercial success, reaching number 18 on the UK album charts. This album offered a refinement of the band's image and philosophy courtesy of graphic artist Barney Bubbles and underground press writer Robert Calvert, as depicted in the accompanying "Hawklog" booklet, which would be further developed into the "Space Ritual" stage show. Science fiction author Michael Moorcock and dancer Stacia also started contributing to the band. Dik Mik had left the band, replaced by sound engineer Del Dettmar, but chose to return for this album giving the band two electronics players. Bass player Dave Anderson, who had been in the German band Amon Düül II, had also joined and played on the album but departed before its release because of personal tensions with some other members of the band. Anderson and Lloyd-Langton then formed the short-lived band Amon Din. Meanwhile, Ollis quit, unhappy with the commercial direction the band were heading in.
The addition of bassist Ian "Lemmy" Kilmister and drummer Simon King propelled the band to greater heights. One of the early gigs the band played was a benefit for the Greasy Truckers at The Roundhouse on 13 February 1972. A live album of the concert, "Greasy Truckers Party", was released, and after re-recording the vocal, a single, "Silver Machine", was also released, reaching number three in the UK charts. This generated sufficient funds for the subsequent album "Doremi Fasol Latido" Space Ritual tour. The show featured dancers Stacia and Miss Renee typically performing either topless or wearing only body paint, mime artist Tony Carrera and a light show by Liquid Len and was recorded on the elaborate package "Space Ritual". At the height of their success, in 1973, the band released the single "Urban Guerrilla", which coincided with an IRA bombing campaign in London, so the BBC refused to play it and the band's management reluctantly decided to withdraw it fearing accusations of opportunism, despite the disc having already climbed to number 39 in the UK chart.

Dik Mik departed during 1973 and Calvert ended his association with the band to concentrate on solo projects. Dettmar also indicated that he was to leave the band, so Simon House was recruited as keyboardist and violinist playing live shows, a North America tour and recording the 1974 album "Hall of the Mountain Grill". Dettmar left after a European tour and emigrated to Canada, whilst Alan Powell deputised for an incapacitated King on that European tour, but remained giving the band two drummers.

At the beginning of 1975, the band recorded the album "Warrior on the Edge of Time" in collaboration with Michael Moorcock, loosely based on his Eternal Champion figure. However, during a North American tour in May, Lemmy was caught in possession of amphetamine crossing the border from the US into Canada. The border police mistook the powder for cocaine and he was jailed, forcing the band to cancel some shows. Fed up with his erratic behaviour, the band dismissed the bass player replacing him with their long-standing friend and former Pink Fairies guitarist Paul Rudolph. Lemmy then teamed up with another Pink Fairies guitarist, Larry Wallis, to form Motörhead, named after the last song he had written for Hawkwind.

Calvert made a guest appearance with the band for their headline set at the Reading Festival in August 1975, after which he chose to rejoin the band as a full-time lead vocalist. Stacia chose to relinquish her dancing duties and settle down to family life. The band changed record company to Tony Stratton-Smith's Charisma Records and, on Stratton-Smith's suggestion, band management from Douglas Smith to Tony Howard.

"Astounding Sounds, Amazing Music" is the first album of this era. On the eve of recording the follow-up "Back on the Streets" single, Turner was dismissed for his erratic live playing and Powell was deemed surplus to requirements. After a tour to promote the single and during the recording of the next album, Rudolph was also dismissed, for allegedly trying to steer the band into a musical direction at odds with Calvert and Brock's vision.

Adrian "Ade" Shaw, who, as bass player for Magic Muscle, had supported Hawkwind on the "Space Ritual" tour, came in for the 1977 album "Quark, Strangeness and Charm". The band continued to enjoy moderate commercial success, but Calvert's mental illness often caused problems. A manic phase saw the band abandon a European tour in France, while a depression phase during a 1978 North American tour convinced Brock to disband the group. In between these two tours, the band had recorded the album "PXR5" in January 1978, but its release was delayed until 1979.

On 23 December 1977 in Barnstaple, Brock and Calvert had performed a one-off gig with Devon band Ark as the Sonic Assassins, and looking for a new project in 1978, bassist Harvey Bainbridge and drummer Martin Griffin were recruited from this event. Steve Swindells was recruited as keyboard player. The band was named Hawklords, (probably for legal reasons, the band having recently split from their management), and recording took place on a farm in Devon using a mobile studio, resulting in the album "25 Years On". King had originally been the drummer for the project but quit during recording sessions to return to London, while House, who had temporarily left the band to join a David Bowie tour, elected to remain with Bowie full-time, but nevertheless contributed violin to these sessions. At the end of the band's UK tour, Calvert, wanting King back in the band, dismissed Griffin, then promptly resigned himself, choosing to pursue a career in literature. Swindells left to record a solo album after an offer had been made to him by the record company ATCO.

In late 1979, Hawkwind reformed with Brock, Bainbridge and King being joined by Huw Lloyd-Langton (who had played on the debut album) and Tim Blake (formerly of Gong), embarking upon a UK tour despite not having a record deal or any product to promote. Some shows were recorded and a deal was made with Bronze Records, resulting in the "Live Seventy Nine" album, quickly followed by the studio album "Levitation". However, during the recording of "Levitation" King quit and Ginger Baker was drafted in for the sessions, but he chose to stay with the band for the tour, during which Blake left to be replaced by Keith Hale.

In 1981 Baker and Hale left after their insistence that Bainbridge should be dismissed was ignored, and Brock and Bainbridge elected to handle synthesisers and sequencers themselves, with drummer Griffin from the Hawklords rejoining. Three albums, which again saw Moorcock contributing lyrics and vocals, were recorded for RCA/Active: "Sonic Attack", the electronic "Church of Hawkwind" and "Choose Your Masques". This band headlined the 1981 Glastonbury Festival and made an appearance at the 1982 Donington Monsters of Rock Festival, as well as continuing to play the summer solstice at Stonehenge Free Festival.

In the early 1980s, Brock had started using drum machines for his home demos and became increasingly frustrated at the inability of drummers to keep perfect time, leading to a succession of drummers coming and going. First, Griffin was ousted and the band tried King again, but, unhappy with his playing at that time, he was rejected. Andy Anderson briefly joined while he was also playing for The Cure, and Robert Heaton also filled the spot briefly prior to the rise of New Model Army. Lloyd Langton Group drummer John Clark did some recording sessions, and in late 1983 Rick Martinez joined the band to play drums on the "Earth Ritual" tour in February and March 1984, later replaced by Clive Deamer.

Turner had returned as a guest for the 1982 "Choose Your Masques" tour and was invited back permanently. Further tours ensued with Phil "Dead Fred" Reeves augmenting the line-up on keyboards and violin, but neither Turner nor Reeves would appear on the only recording of 1983–84, "The Earth Ritual Preview", however there was a guest spot for Lemmy. The "Earth Ritual" tour was filmed for Hawkwind's first video release, "Night of the Hawk".

Alan Davey was a young fan of the band who had sent a tape of his playing to Brock, and Brock chose to oust Reeves moving Bainbridge from bass to keyboards to accommodate Davey. This experimental line-up played at the Stonehenge Free Festival in 1984, which was filmed and release as "Stonehenge 84". Subsequent personal and professional tensions between Brock and Turner led to the latter's expulsion at the beginning of 1985. Clive Deamer, who was deemed "too professional" for the band, was eventually replaced in 1985 by Danny Thompson Jr (son of folk-rock bassist Danny Thompson), a friend of Alan Davey, and remained almost to the end of the decade.

Hawkwind's association with Moorcock climaxed in their most ambitious project, "The Chronicle of the Black Sword", based loosely around the Elric series of books and theatrically staged with Tony Crerar as the central character. Moorcock contributed lyrics, but only performed some spoken pieces on some live dates. The tour was recorded and issued as an album "Live Chronicles" and video "The Chronicle of the Black Sword". The band also performed at the Worldcon (World Science Fiction Convention) in Brighton.

In August 1985, The band performed at Crystal Palace Bowl, with several other rock bands, for a benefit concert for Pete Townshend's Double-O anti-heroin charity. Lemmy and Stacey were reunited with the band for this event. Vera Lynn closed the show.

A headline appearance at the 1986 Reading Festival was followed by a UK tour to promote the "Live Chronicles" album which was filmed and released as "Chaos". In 1988 the band recorded the album "The Xenon Codex" with Guy Bidmead, but all was not well in the band and soon after, both Lloyd-Langton and Thompson departed.

Drummer Richard Chadwick, who joined in the summer of 1988, had been playing in small alternative free festival bands, most notably Bath's Smart Pils, for a decade and had frequently crossed paths with Hawkwind and Brock. He was initially invited simply to play with the band, but eventually replaced stand in drummer Mick Kirton to become the band's drummer to the present day.

To fill in the gap of lead sound, lost when Lloyd-Langton left, violinist House was re-instated into the line-up in 1989 (having previously been a member from 1974 until 1978), and, notably, Hawkwind embarked on their first North American visit in eleven years (since the somewhat disastrous 1978 tour), in which House did not partake. The successfully received tour was the first of several over the coming years, in an effort by the band to re-introduce themselves to the American market.

Bridget Wishart, an associate of Chadwick's from the festival circuit, also joined to become the band's one and only singing front-woman, the band had been fronted in earlier days by Stacia but only as a dancer. This band produced two albums, 1990's "Space Bandits" and 1991's "Palace Springs" and also filmed a 1-hour appearance for the "Bedrock TV" series with dancer Julie Murray-Anderson, who performed with Hawkwind between 1988 and 1991.

1990 saw Hawkwind tour North America again, the second instalment in a series of American visits made at around this time in an effort to re-establish the Hawkwind brand in America. The original business plan was to hold three consecutive US tours, annually, from 1989–1991, with the first losing money, the second breaking even, and the third turning a profit, ultimately bringing Hawkwind back into recognition across the Atlantic. Progress, however, was somewhat stunted, due to ex-member Nik Turner touring the United States with his own band at the time, in which the shows were often marketed as Hawkwind.

Still supporting Space Bandits, 1991 commenced with perhaps the most surprising Hawkwind tour in the band's history, without Dave Brock. Brock's temporary replacement was former Smart Pils guitarist Steve Bemand (who had played with Chadwick and Wishart in the Demented Stoats). The tour began in Amsterdam on 12 March and took in Germany, Greece, Italy and France before wrapping up in Belgium on 10 April after 24 dates.

In 1991 Bainbridge, House and Wishart departed and the band continued as a three piece relying heavily on synthesisers and sequencers to create a wall-of-sound. The 1992 album "Electric Tepee" combined hard rock and light ambient pieces, while "It is the Business of the Future to be Dangerous" is almost devoid of the rock leanings. "The Business Trip" is a record of the previous album's tour, but rockier as would be expected from a live outing. The "White Zone" album was released under the alias Psychedelic Warriors to distance itself entirely from the rock expectancy of Hawkwind.

A general criticism of techno music at that time was its facelessness and lack of personality, which the band were coming to feel also plagued them. Ron Tree had known the band on the festival circuit and offered his services as a front-man, and the band duly employed him for the album "Alien 4" and its accompanying tour which resulted in the album "Love in Space" and "video".

In 1996, unhappy with the musical direction of the band, bassist Davey left, forming his own Middle-Eastern flavoured hard-rock group Bedouin and a Motörhead tribute act named Ace of Spades. His bass playing role was reluctantly picked up by singer Tree and the band were joined full-time by lead guitarist Jerry Richards (another stalwart of the festival scene, playing for Tubilah Dog who had merged with Brock's Agents of Chaos during 1988) for the albums "Distant Horizons" and "In Your Area". Rasta chanter Captain Rizz also joined the band for guest spots during live shows.

Hawkestra — a re-union event featuring appearances from past and present members — had originally been intended to coincide with the band's 30th anniversary and the release of the career spanning "Epocheclipse – 30 Year Anthology" set, but logistical problems delayed it until 21 October 2000. It took place at the Brixton Academy with about 20 members taking part in a more than 3-hour set, which was filmed and recorded. Guests included Samantha Fox who sang "Master of the Universe". However, arguments and disputes over financial recompense and musical input resulted in the prospect of the event being re-staged unlikely, and any album or DVD release being indefinitely shelved.

The Hawkestra had set a template for Brock to assemble a core band of Tree, Brock, Richards, Davey, Chadwick and for the use of former members as guests on live shows and studio recordings. The 2000 Christmas Astoria show was recorded with contributions from House, Blake, Rizz, Moorcock, Jez Huggett and Keith Kniveton and released as "Yule Ritual" the following year. In 2001, Davey agreed to rejoin the band permanently, but only after the departure of Tree and Richards.

Meanwhile, having rekindled relationships with old friends at the Hawkestra, Turner organised further Hawkestra gigs resulting in the formation of xhawkwind.com, a band consisting mainly of ex-Hawkwind members and playing old Hawkwind songs. An appearance at Guilfest in 2002 led to confusion as to whether this actually was Hawkwind, sufficiently irking Brock into taking legal action to prohibit Turner from trading under the name Hawkwind. Turner lost the case and the band began performing as Space Ritual.

An appearance at the Canterbury Sound Festival in August 2001, resulting in another live album "Canterbury Fayre 2001", saw guest appearances from Lloyd-Langton, House, Kniveton with Arthur Brown on "Silver Machine". The band organised the first of their own weekend festivals, named Hawkfest, in Devon in the summer of 2002. Brown joined the band in 2002 for a Winter tour which featured some Kingdom Come songs and saw appearances from Blake and Lloyd-Langton, the Newcastle show being released on DVD as "Out of the Shadows" and the London show on CD as "Spaced Out in London".

In 2005 a new album "Take Me to Your Leader" was released. Recorded by the core band of Brock/Davey/Chadwick, contributors included new keyboardist Jason Stuart, Arthur Brown, tabloid writer and TV personality Matthew Wright, 1970s New Wave singer Lene Lovich, Simon House and Jez Huggett. This was followed in 2006 by the CD/DVD "Take Me to Your Future".

The band were the subject of an hour-long television documentary entitled "Hawkwind: Do Not Panic" that aired on BBC Four as part of the "Originals" series. It was broadcast on 30 March 2007 and repeated on 10 August 2007. Although Brock participated in its making he did not appear in the programme, it is alleged that he requested all footage of himself be removed after he was denied any artistic control over the documentary. In one of the documentary's opening narratives regarding Brock, it is stated that he declined to be interviewed for the programme because of Nik Turner's involvement, indicating that the two men have still not reconciled over the xhawkwind.com incident.

December 2006 saw the official departure of Alan Davey, who left to perform and record with two new bands: Gunslinger and Thunor. He was replaced by Mr Dibs, a long-standing member of the road crew. The band performed at their annual Hawkfest festival and headlined the US festival Nearfest and played gigs in PA and NY. At the end of 2007, Tim Blake once again joined the band filling the lead role playing keyboards and theremin. The band played 5 Christmas dates, the London show being released as an audio CD and video DVD under the title "Knights of Space".

In January 2008 the band reversed its anti-taping policy, long a sore-point with many fans, announcing that it would allow audio recording and non-commercial distribution of such recordings, provided there was no competing official release. At the end of 2008, Atomhenge Records (a subsidiary of Cherry Red Records) commenced the re-issuing of Hawkwind's back catalogue from the years 1976 through to 1997 with the release of two triple CD anthologies "Spirit of the Age (anthology 1976–84)" and "The Dream Goes On (anthology 1985–97)".

On 8 September 2008 keyboard player Jason Stuart died due to a brain haemorrhage. In October 2008, Niall Hone (former Tribe of Cro) joined Hawkwind for their Winter 2008 tour playing guitar, along with returning synth/theremin player Tim Blake. In this period, Hone also occasionally played bass guitar alongside Mr Dibs and used laptops for live electronic improvisation.

In 2009, the band began occasionally featuring Jon Sevink from The Levellers as guest violinist at some shows. Later that year, Hawkwind embarked on a winter tour to celebrate the band's 40th anniversary, including two gigs on 28 and 29 August marking the anniversary of their first live performances. In 2010, Hawkwind held their annual Hawkfest at the site of the original Isle of Wight Festival, marking the 40th anniversary of their appearance there.

On 21 June 2010, Hawkwind released a studio album entitled "Blood of the Earth" on Eastworld Records. During and since the "Blood of the Earth" support tours, Hone's primary on-stage responsibility shifted to bass, while Mr. Dibs moved to a more traditional lead singer/front man role.

In 2011, Hawkwind toured Australia for the second time.

April 2012 saw the release of a new album, "Onward", again on Eastworld. Keyboardist Dead Fred rejoined Hawkwind for the 2012 tour in support of "Onward" and has since remained with the band. In November 2012, Brock, Chadwick and Hone — credited as "Hawkwind Light Orchestra" — released "Stellar Variations" on Esoteric Recordings.

2013 marked the first Hawkeaster, a two-day festival held in Seaton, Devon during the Easter weekend. A US tour was booked for October 2013, but due to health issues, was postponed and later cancelled.

In February 2014, as part of a one-off Space Ritual performance, Hawkwind performed at the O2 Shepherd's Bush Empire featuring an appearance by Brian Blessed for the spoken word element of Sonic Attack; a studio recording of this performance was released as a single in September 2014. Later in the year, former Soft Machine guitarist John Etheridge joined the live line-up of the band, though he had departed again prior to early 2015 dates.

Following Hawkeaster 2015, Hawkwind made their debut visit to Japan, playing two sold-out shows in Tokyo. Hawkwind performed two Solstice Ritual shows in December 2015, with Steve Hillage guesting, and Haz Wheaton joining Hawkwind on bass guitar. Wheaton is a former member of the band's road crew who had previously appeared with Technicians of Spaceship Hawkwind, a "skeleton crew" spin off live band. Additionally, he had guested on bass for Dave Brock's solo album "Brockworld" released earlier in the year.

The band released "The Machine Stops" on 15 April 2016. The album marked Wheaton's first appearance on a Hawkwind studio album, and the first album without Tim Blake's involvement since he had rejoined the band in 2010 and appeared on "Blood of the Earth". His departure was offset by increased synthesiser work by Hone and Brock.

Dead Fred's last live appearance with Hawkwind was at The Eastbourne Winter Gardens April 1, 2016. Hone took over keyboards and synth duties live until though Blake returned for shows in summer 2016.

It was announced in November 2016 that Hawkwind were recording a new studio album, entitled "Into The Woods". Keyboardist-guitarist Magnus Martin replaced both Hone and Blake in the lineup for the new album, leaving the 2017 core band composed of Brock, Chadwick, Mr Dibs, Wheaton and Martin.

In 2018, Hawkwind recorded an acoustic album "The Road to Utopia" consisting primarily of cover versions of their 1970s songs with production, arrangement and additional orchestrations by Mike Batt and a guest appearance from Eric Clapton. Batt conducted a series concerts of Hawkwind songs featuring the band and orchestra in October and November.

In May 2018 Haz Wheaton left and later joined Electric Wizard. Niall Hone returned on bass. Mr Dibs left on August 22 stating ‘irreconcilable differences’ in a statement on the Hawkwind fans Facebook page.

In October 2019, Hawkwind released "All Aboard the Skylark," marketed as a return to their space rock roots. This was the first album with the line-up of Brock, Chadwick, Hone, and Martin. Accompanying the CD version, and sold as a separate vinyl LP, was "Acoustic Daze." This recording included tracks from the 2018 album "The Road to Utopia", minus the additions by Batt and Clapton.

Hawkwind have been cited as an influence by artists such as Al Jourgensen of Ministry, Monster Magnet, the Sex Pistols (who covered "Silver Machine"), Henry Rollins and Dez Cadena of Black Flag, Siobhan Fahey, Ty Segall, The Mekano Set, and Ozric Tentacles.

Hard rock musician Lemmy of the band Motörhead gained a lot from his tenure in Hawkwind. He has remarked, "I really found myself as an instrumentalist in Hawkwind. Before that I was just a guitar player who was pretending to be good, when actually I was no good at all. In Hawkwind I became a good bass player. It was where I learned I was good at something."

Current members




</doc>
<doc id="13645" url="https://en.wikipedia.org/wiki?curid=13645" title="Horse">
Horse

The horse ("Equus ferus caballus") is one of two extant subspecies of "Equus ferus". It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, "Eohippus", into the large, single-toed animal of today. Humans began domesticating horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC. Horses in the subspecies "caballus" are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.

Horses are adapted to run, allowing them to quickly escape predators, possessing an excellent sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horses tending to sleep significantly more than adults. Female horses, called mares, carry their young for approximately 11 months, and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under a saddle or in a harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.

Horse breeds are loosely divided into three categories based on general temperament: spirited "hot bloods" with speed and endurance; "cold bloods", such as draft horses and some ponies, suitable for slow, heavy work; and "warmbloods", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.

Horses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits, as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water, and shelter, as well as attention from specialists such as veterinarians and farriers.

Specific terms and specialized language are used to describe equine anatomy, different life stages, and colors and breeds.

Depending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years. Uncommonly, a few animals live into their 40s and, occasionally, beyond. The oldest verifiable record was "Old Billy", a 19th-century horse that lived to the age of 62. In modern times, Sugar Puff, who had been listed in "Guinness World Records" as the world's oldest living pony, died in 2007 at age 56.

Regardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere and each August 1 in the Southern Hemisphere. The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.

The following terminology is used to describe horses of various ages:

In horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old. However, Australian Thoroughbred racing defines colts and fillies as less than four years old.

The height of horses is measured at the highest point of the withers, where the neck meets the back. This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.

In English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to . The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation "h" or "hh" (for "hands high"). Thus, a horse described as "15.2 h" is 15 hands plus 2 inches, for a total of in height.
The size of horses varies by breed, but also is influenced by nutrition. Light riding horses usually range in height from and can weigh from . Larger riding horses usually start at about and often are as tall as , weighing from . Heavy or draft horses are usually at least high and can be as tall as high. They can weigh from about .

The largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood high and his peak weight was estimated at . The current record holder for the world's smallest horse is Thumbelina, a fully mature miniature horse affected by dwarfism. She is tall and weighs .

Ponies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.

The traditional standard for height of a horse or a pony at maturity is . An animal 14.2 h or over is usually considered to be a horse and one less than 14.2 h a pony, but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under . For competition in the Western division of the United States Equestrian Federation, the cutoff is . The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than at the withers without shoes, which is just over 14.2 h, and , or just over 14.2 h, with shoes.

Height is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 h consider all animals of that breed to be horses regardless of their height. Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 h, but are still considered to be ponies.

Ponies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers. Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages , is considered a pony. Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than , are classified by their registries as very small horses, not ponies.

Horses have 64 chromosomes. The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs, which is larger than the dog genome, but smaller than the human genome or the bovine genome. The map is available to researchers.

Horses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex. Horses of the same color may be distinguished from one another by white markings, which, along with various spotting patterns, are inherited separately from coat color.

Many genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color, and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor, also known as the "extension gene" or "red factor," as its recessive form is "red" (chestnut) and its dominant form is black. Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as graying, and all the other factors that create the many possible coat colors found in horses.

Horses that have a white coat color are often mislabeled; a horse that looks "white" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence. Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene. However, there are no "albino" horses, defined as having both pink skin and red eyes.

Gestation lasts approximately 340 days, with an average range 320–370 days, and usually results in one foal; twins are rare. Horses are a precocial species, and foals are capable of standing and running within a short time following birth. Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an "anestrus" period during the winter and thus do not cycle in this period. Foals are generally weaned from their mothers between four and six months of age.

Horses, particularly colts, sometimes are physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females. Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.

Depending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four. Although Thoroughbred race horses are put on the track as young as the age of two in some countries, horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed. For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.

The horse skeleton averages 205 bones. A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's four legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's "knee" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the "ankle") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the "knuckles" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.

The critical importance of the feet and legs is summed up by the traditional adage, "no foot, no horse". The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail. The end result is that a horse, weighing on average , travels on the same bones as would a human on tiptoe. For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks, though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.

Horses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called "tushes". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as "wolf" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or "bars" of the horse's mouth when the horse is bridled.

An estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.

Horses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A horse will eat of food per day and, under normal use, drink of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can utilize cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or "water gut", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.

The horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times. They have the largest eyes of any land mammal, and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads. This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision. Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.

Their sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the Vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.

A horse's hearing is good, and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head. Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: A 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels. An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.

Horses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times. A horse's sense of touch is well-developed. The most sensitive areas are around the eyes, ears, and nose. Horses are able to sense contact as subtle as an insect landing anywhere on the body.

Horses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat, and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.

All horses move naturally with four basic gaits: the four-beat walk, which averages ; the two-beat trot or jog at (faster for harness racing horses); the canter or lope, a three-beat gait that is ; and the gallop. The gallop averages , but the world record for a horse galloping over a short, sprint distance is . Besides these basic gaits, some horses perform a two-beat pace, instead of the trot. There also are several four-beat "ambling" gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot. Ambling gaits are often genetic in some breeds, known collectively as gaited horses. Often, gaited horses replace the trot with one of the ambling gaits.

Horses are prey animals with a strong fight-or-flight response. Their first reaction to a threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened. They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.

Horses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses. However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, "weaving" (rocking back and forth), and other problems.

Studies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities. They are naturally curious and apt to investigate things they have not seen before. Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement. One study has indicated that horses can differentiate between "more or less" if the quantity involved is less than four.

Domesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural. Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that "intelligent" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.

Horses are mammals, and as such are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the "hot-bloods", such as many race horses, exhibit more sensitivity and energy, while the "cold-bloods", such as most draft breeds, are quieter and calmer. Sometimes "hot-bloods" are classified as "light horses" or "riding horses", with the "cold-bloods" classified as "draft horses" or "work horses".
"Hot blooded" breeds include "oriental horses" such as the Akhal-Teke, Arabian horse, Barb and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds. Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed. They tend to be physically refined—thin-skinned, slim, and long-legged. The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.

Muscular, heavy draft horses are known as "cold bloods", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people. They are sometimes nicknamed "gentle giants". Well-known draft breeds include the Belgian and the Clydesdale. Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates. Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils. The cold-blooded group also includes some pony breeds.

"Warmblood" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed. Certain pony breeds with warmblood characteristics have been developed for smaller riders. Warmbloods are considered a "light horse" or "riding horse".

Today, the term "Warmblood" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping. Strictly speaking, the term "warm blood" refers to any cross between cold-blooded and hot-blooded breeds. Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.

Horses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a "stay apparatus" in their legs, allowing them to doze without collapsing. Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.

Unlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours, mostly in short intervals of about 15 minutes each. The average sleep time of a domestic horse is said to be 2.9 hours per day.

Horses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements. However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing. This condition differs from narcolepsy, although horses may also suffer from that disorder.

The horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not. Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals that was dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.

The earliest known member of the family Equidae was the "Hyracotherium", which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot. The extra toe on the front feet soon disappeared with the "Mesohippus", which lived 32 to 37 million years ago. Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee, known informally as splint bones. Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed. By about 5 million years ago, the modern "Equus" had evolved. Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.

By about 15,000 years ago, "Equus ferus" was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America. Yet between 10,000 and 7,600 years ago, the horse became extinct in North America and rare elsewhere. The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival. Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.

A truly wild horse is a species or subspecies with no ancestors that were ever domesticated. Therefore, most "wild" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals. Only two never-domesticated subspecies, the tarpan and the Przewalski's horse, survived into recorded history and only the latter survives today.

The Przewalski's horse ("Equus ferus przewalskii"), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the "taki", and the Kyrgyz people call it a "kirtag". The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild due to the conservation efforts of numerous zoos. Today, a small wild breeding population exists in Mongolia. There are additional animals still maintained at zoos throughout the world.

The tarpan or European wild horse ("Equus ferus ferus") was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo. Thus, the genetic line was lost. Attempts have been made to recreate the tarpan, which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.

Periodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such, but testing did not reveal genetic differences from domesticated horses. Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan based on shared characteristics, but genetic studies have shown that the Sorraia is more closely related to other horse breeds and that the outward similarity is an unreliable measure of relatedness.

Besides the horse, there are six other species of genus "Equus" in the Equidae family. These are the ass or donkey, "Equus asinus"; the mountain zebra, "Equus zebra"; plains zebra, "Equus quagga"; Grévy's zebra, "Equus grevyi"; the kiang, "Equus kiang"; and the onager, "Equus hemionus".

Horses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a "jack" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a jenny (female donkey). Other hybrids include the zorse, a cross between a zebra and a horse. With rare exceptions, most hybrids are sterile and cannot reproduce.

Domestication of the horse most likely took place in central Asia prior to 3500 BC. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.

The earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 3500–4000 BC. By 3000 BC, the horse was completely domesticated and by 2000 BC there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent. The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BC.

Domestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse, while many mares were part of early domesticated herds. This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability, but a great deal of genetic variation in mitochondrial DNA. There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds. Another characteristic of domestication is an increase in coat color variation. In horses, this increased dramatically between 5000 and 3000 BC.

Before the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication. Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication. However, the lack of a detectable substructure in the horse has resulted in a rejection of both hypotheses.

Feral horses are born and live in the wild, but are descended from domesticated animals. Many populations of feral horses exist throughout the world. Studies of feral herds have provided useful insights into the behavior of prehistoric horses, as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.

There are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in "wild" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.

The concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called "thoroughbreds". Thoroughbred is a specific breed of horse, while a "purebred" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. These pedigrees were originally transmitted via an oral tradition. In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.

Breeds developed due to a need for "form to function", the necessity to develop certain characteristics in order to perform a particular type of work. Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage. Heavy draft horses were developed out of a need to perform demanding farm work and pull heavy wagons. Other horse breeds had been developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets. Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed. There are more than 300 horse breeds in the world today.

Worldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone. The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion. In a 2004 "poll" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.

Communication between human and horse is paramount in any equestrian activity; to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control. Sometimes horses are ridden without a saddle, and occasionally, horses are trained to perform without a bridle or other headgear. Many horses are also driven, which requires a harness, bridle, and some type of vehicle.

Historically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques. Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.

Horses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting. Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as "In-hand" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.
Sports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task. Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other, and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.

Horse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: "flat" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky. A major part of horse racing's economic importance lies in the gambling associated with it.

There are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control. Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain. Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance. Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.
Although machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone. Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses. Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.

Horses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BC, and the use of horses in warfare was widespread by the end of the Bronze Age. Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.

Modern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles. Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events. Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.

Horses are frequently used in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories. Both live horses and iconic images of horses are used in advertising to promote a variety of products. The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment. The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Norse, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun. The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.

People of all ages with physical and mental disabilities obtain beneficial results from an association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence. The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI). Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that utilize equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.

Horses also provide psychological benefits to people whether they actually ride or not. "Equine-assisted" or "equine-facilitated" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes. There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.

Horses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.

Products collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis. Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat. The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy. The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.

Horse meat has been used as food for humans and carnivorous animals throughout the ages. Approximately 5 million horses are slaughtered each year for meat worldwide. It is eaten in many parts of the world, though consumption is taboo in some cultures, and a subject of political controversy in others. Horsehide leather has been used for boots, gloves, jackets, baseballs, and baseball gloves. Horse hooves can also be used to produce animal glue. Horse bones can be used to make implements. Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a "spinto", which is used to test the readiness of a (pig) ham as it cures. In Asia, the saba is a horsehide vessel used in the production of kumis.

Horses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture. They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a adult horse could eat up to of food. Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active. When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.

Horses require a plentiful supply of clean water, a minimum of to per day. Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.

Horses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist. If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being. When turned outside, they require well-maintained, sturdy fences to be safely contained. Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.




</doc>
