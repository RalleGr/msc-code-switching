<doc id="15032" url="https://en.wikipedia.org/wiki?curid=15032" title="IBM Personal Computer">
IBM Personal Computer

The IBM Personal Computer, commonly known as the IBM PC, is the original version of the IBM PC compatible computer design. It is IBM model number 5150 and was introduced on August 12, 1981. It was created by a team of engineers and designers directed by Don Estridge in Boca Raton, Florida.

The generic term "personal computer" ("PC") was in use years before 1981, applied as early as 1972 to the Xerox PARC's Alto, but the term "PC" came to mean more specifically a desktop microcomputer compatible with IBM's "Personal Computer" branded products. The machine was based on open architecture, and third-party suppliers soon developed to provide peripheral devices, expansion cards, and software. IBM had a substantial influence on the personal computer market in standardizing a design for personal computers, and "IBM compatible" became an important criterion for sales growth. Only the Apple Macintosh types of computer kept a significant share of the microcomputer market after the 1980s without compatibility with the IBM personal computer.

International Business Machines (IBM) had a 62% share of the mainframe computer market during the early 1980s. Slow development of a minicomputer product of its own during the 1960s, however, let new rivals like Digital Equipment Corporation (DEC) and others earn billions of dollars of revenue.

IBM did not want to repeat the mistake with personal computers. During the late 1970s, the new industry was dominated by the Commodore PET, Atari 8-bit family, Apple II series, Tandy Corporation's TRS-80, and various CP/M machines. The microcomputer market was large enough for IBM's attention, with $150 million in sales by 1979 and projected annual growth of more than 40% during the early 1980s. Other large technology companies had entered it, such as Hewlett-Packard (HP), Texas Instruments (TI), and Data General, and some large IBM customers were buying Apples. IBM did not want a personal computer with another company's logo on mainframe customers' desks, so introducing its own was both an experiment with a new market and a defense against rivals.

In 1980 and 1981, rumors spread of an IBM personal computer, perhaps a miniaturized version of the IBM System/370, while Matsushita acknowledged that it had discussed with IBM the possibility of manufacturing a personal computer for the American company. The Japanese project was a Zilog Z80-based computer codenamed "Go" but ended before the 1981 release of the American-designed IBM PC codenamed "Chess", and two simultaneous projects confused rumors about the forthcoming product.

Whether IBM had waited too long to enter an industry in which Tandy, Atari and others were already successful was uncertain. Data General and TI's small computers were not very successful, but observers expected AT&T Corporation to begin vending computers, and other large companies such as Exxon, Montgomery Ward, Pentel, and Sony were designing their own microcomputers. Xerox quickly produced the 820 to introduce a personal computer before IBM, becoming the second Fortune 500 company after Tandy to do so, and had its Xerox PARC laboratory's sophisticated technology.

An observer stated that "IBM bringing out a personal computer would be like teaching an elephant to tap dance". Successful microcomputer company Vector Graphic's fiscal 1980 revenue was $12 million. A single IBM computer during the early 1960s cost as much as $9 million, occupied of air-conditioned space, and had a staff of 60 people; in 1980 its least-expensive computer, the 5120, still cost about $13,500. The "Colossus of Armonk" only sold using its own sales force and did not have experience with resellers or retail stores.

Another observer claimed that IBM made decisions so slowly that, when tested, "what they found is that it would take at least nine months to ship an empty box", and an employee complained that "IBM has more committees than the U.S. Government". As with other large computer companies, its new products typically required about four to five years for development. While IBM traditionally let others pioneer a new market—- it released a commercial computer a year after Remington Rand's UNIVAC in 1951, but within five years had 85% of the market—- the personal-computer development and pricing cycles were much faster than for mainframes, with products designed in a few months and obsolete quickly.

Many in the microcomputer industry expected that the personal computer would be, Bill Gates of Microsoft recalled, "the overthrow of IBM". They resented the company's power and wealth, and disliked the perception that an industry founded by startup companies needed a latecomer so said that it had a strict dress code and employee songbook, and prohibited salesmen with client visits in the afternoon from drinking alcohol at lunch. The potential importance to microcomputers of a company so prestigious, that a popular saying in American companies stated "No one ever got fired for buying IBM", was nonetheless evident. "InfoWorld", which described itself as "The Newsweekly for Microcomputer Users", stated that "for my grandmother, and for millions of people like her, "IBM " and "computer" are synonymous". "Byte" ("The Small Systems Journal") stated in an editorial just before the announcement of the IBM PC:

The editorial acknowledged that "some factions in our industry have looked upon IBM as the 'enemy, but concluded with optimism: "I want to see personal computing take a giant step."

Desktop sized programmable calculators by HP had evolved into the HP 9830 BASIC language computer by 1972. During 1972–1973 a team managed by Dr. Paul Friedl at the IBM Los Gatos Scientific Center developed a portable computer prototype known as SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small cathode ray tube, and full-function keyboard. SCAMP emulates the IBM 1130r to execute APL\1130. In 1973 the APL programming language was generally available only for mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because it was the first to emulate APL\1130 performance on a portable, single-user computer, "PC Magazine" in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer". The prototype is in the Smithsonian Institution. A non-working industrial design model was also created in 1973 by industrial designer Tom Hardy illustrating how the SCAMP engineering prototype could be transformed into a usable product design for the marketplace. This design model was requested by IBM executive William C. Lowe to complement the engineering prototype in his early efforts to demonstrate the viability of creating a single-user computer.

Successful demonstrations of the 1973 SCAMP prototype resulted in the IBM 5100 portable microcomputer in 1975. During the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton. The 5100 is a complete computer system programmable with BASIC or APL, with a small built-in CRT monitor, keyboard, and tape drive for data storage. It was also very expensive, as much as US$20,000; the computer was designed for professional and scientific customers, not business users or hobbyists. "BYTE" in 1975 announced the 5100 with the headline "Welcome, IBM, to personal computing", but "PC Magazine" in 1984 described 5100s as "little mainframes" and stated that "as personal computers, these machines were dismal failures ... the antithesis of user-friendly", with no IBM support for third-party software. Despite news reports that the PC was the first IBM product without a model number, it was designated as the IBM 5150, putting it in the "5100" series though its architecture was not developed from the IBM 5100. The same naming system was applied to later models: For example, the IBM Portable Personal Computer, PC/XT, and PC AT were IBM machine types 5155, 5160, and 5170, respectively.

After SCAMP, the IBM Boca Raton, Florida Laboratory created several single-user computer design ideas to assist Lowe's ongoing effort to convince IBM there was a strategic opportunity in the personal computer business. A selection of these early design ideas created by Hardy is featured in the book "DELETE: A Design History of Computer Vapourware". One such concept in 1977, code-named Aquarius, is a working prototype utilizing advanced bubble memory cartridges. While this design is more powerful and smaller than the Apple II introduced the same year, the advanced bubble technology was deemed unstable and not ready for mass production.

Some IBM employees opposed IBM entering the market. One said, "Why on earth would you care about the personal computer? It has nothing at all to do with office automation". The company considered personal computer designs but had determined that IBM was unable to build a personal computer profitably. Walden C. Rhines of TI met with a Boca Raton group during the late 1970s considering the TMS9900 16-bit microprocessor for a secret project. He wrote, "We wouldn’t know until 1981 just what we had lost" by not being chosen.

IBM President John Opel was not among those skeptical of personal computers. He and CEO Frank Cary had created more than a dozen semi-autonomous "Independent Business Units" (IBU) to encourage innovation; "Fortune" termed them "how to start your own company without leaving IBM". Lowe became the first manager of the Entry Level Systems IBU in Boca Raton, and his team researched the market. Computer dealers were very interested in selling an IBM product, but they told Lowe that the company could not design, sell, or service it as IBM had previously done. An IBM microcomputer, they said, must be composed of standard parts that store employees could repair. Dealers disliked Apple's autocratic business practices, including a shortage of the Apple II while the company emphasized the more sophisticated Apple III. They saw no alternative, however; IBM only sold directly so any IBM personal computer would only hurt them. Dealers doubted that IBM's traditional sales methods and bureaucracy would change.

Schools in Broward County—- near Boca Raton—- purchased Apples, a consequence of IBM lacking a personal computer. Atari proposed in 1980 that it act as original equipment manufacturer for an IBM microcomputer. Lowe was aware that the company needed to begin market operations quickly, so he met with Opel, Cary, and others on the Corporate Management Committee in 1980. He demonstrated the proposal with an industrial design model by Hardy based on the Atari 800 design, and suggested acquiring Atari "because we can't do this within the culture of IBM".

Cary agreed about the culture, observing that IBM would need "four years and three hundred people" to develop its own personal computer; Lowe promised one in a year if done without traditional IBM methods. Instead of acquiring Atari, the committee allowed him to form an independent group of employees called "the Dirty Dozen", managed by engineer Bill Sydnes, and Lowe promised that they could design a prototype in 30 days. The crude prototype barely worked when Lowe demonstrated it in August, but he presented a detailed business plan which proposed that the new computer have an open architecture, use non-proprietary components and software, and be sold through retail stores, all contrary to IBM practice. Lowe estimated sales of 220,000 computers over three years, more than IBM's entire installed base.

The committee agreed that Lowe's method was the most likely to succeed, and it approved converting the group into another IBU code named "Project Chess" to develop "Acorn", with unusually large funding to help achieve the goal of introducing the product within one year of the August demonstration. Don Estridge became the manager of Chess. Cary told the team to do whatever was necessary to develop an IBM personal computer quickly. Major members included Sydnes, Lewis Eggebrecht, David Bradley, Mark Dean, and David O'Connor. Many were already hobbyists who owned their own computers, including Estridge, who had an Apple II. Industrial designer Hardy was also assigned to the project. The team received permission to expand to 150 people by the end of 1980, and one day more than 500 IBM employees called in asking to join.

IBM normally was integrated vertically, developing all important hardware and software internally with only what was available in internal catalogs of IBM components, and only purchasing parts like transformers and semiconductors. The company's purchase of Rolm during 1984 was its first acquisition in 18 years. IBM discouraged customers from purchasing compatible third-party products.

Before the PC, IBM began using some components from other companies to save money and time, and when designing it the company avoided vertical integration as much as possible; choosing, for example, to license Microsoft BASIC despite having a version of BASIC of its own for mainframes. (Estridge said that unlike IBM's own version "Microsoft BASIC had hundreds of thousands of users around the world. How are you going to argue with that?") Although the company denied doing so, many observers concluded that IBM intentionally emulated Apple when designing the PC. The many Apple II owners on the team influenced its decision to design the computer with an open architecture and publish technical information so others could create software and expansion slot peripherals.

Eggebrecht wanted to use the Motorola 68000, Gates recalled. Rhines later said that it "was undoubtedly the hands-on winner" among 16-bit CPUs for an IBM microcomputer; big endian like other IBM computers, and more powerful than TMS9900 or Intel 8088. The 68000 was not production ready like the others, however; thus "Motorola, with its superior technology, lost the single most important design contest of the last 50 years", he said. Project Go, from a rival division, planned to use an 8-bit CPU. Gates said that Project Chess also planned to do so until he convinced IBM to choose the 8088. Although the company knew that it could not avoid competition from third-party software on proprietary hardware—- Digital Research released CP/M-86 for the IBM Displaywriter, for example—- it considered using the IBM 801 RISC processor and its operating system, developed at the Thomas J. Watson Research Center in Yorktown Heights, New York. The 801 processor was more than an order of magnitude more powerful than the Intel 8088, and the operating system more advanced than the PC DOS 1.0 operating system from Microsoft. Ruling out an in-house solution made the team's job much easier and may have avoided a delay in the schedule, but the ultimate consequences of this decision for IBM were profound.

IBM had recently developed the IBM System/23 Datamaster business microcomputer, which uses a processor and other microchips from Intel; familiarity with them and the immediate availability of the 8088 was a reason for choosing it for the PC. The 62-pin expansion bus slots were designed to be similar to the Datamaster slots. Differences from the Datamaster include avoiding an all-in-one design while limiting the computer's size so that it fits on a standard desktop with the keyboard (also similar to the Datamaster's), and " disk drives instead of 8". Delays due to in-house development of the Datamaster software was a reason why IBM chose Microsoft BASIC—- already available for the 8088—- and published available technical information to encourage third-party developers. IBM chose the 8088 over the similar but superior 8086 because Intel offered a better price for the former and could provide more units, and the 8088's 8-bit bus reduced the cost of the rest of the computer.

Gates praised Eggebrecht for designing the 8088-based motherboard in 40 days, describing it as "one of the most phenomenal projects". The IBU built a working prototype in four months and made the first internal demonstration by January 1981. The design for the computer was essentially complete by April 1981, when the manufacturing team assumed control of the project. IBM would not be able to make a profit if it were to use only its own hardware with Acorn; to save time and money, the IBU built the machine with commercial off-the-shelf parts from original equipment manufacturers whenever possible, with only final assembly occurring in Boca Raton at a plant Estridge designed. The IBU would decide whether it would be more economical to "Make or Buy" each manufacturing phase.

Various IBM divisions for the first time competed with outsiders to build parts of the new computer; a North Carolina IBM factory built the keyboard, the Endicott, New York factory had to lower its bid for printed circuit boards, and a Taiwanese company built the monitor. The IBU chose an existing monitor from IBM Japan and an Epson printer. Because of the off-the-shelf parts only the system unit and keyboard has unique IBM industrial design elements. There is IBM copyright only for the read-only memory (ROM), Basic input/output system (BIOS) and the company logo, and the company reportedly did not receive any patents for the PC, with outsiders manufacturing 90% of it. Because the product would feature the IBM logo, the only corporate division the IBU could not bypass was the Quality Assurance Unit, part of why IBM did not use the 68000. A component manufacturer described the process of being selected as a supplier as rigorous and "absolutely amazing", with IBM inspectors even testing solder flux. They visited after selection, monitoring and helping to improve the manufacturing process. IBM's size overwhelmed other companies; "a hundred IBM engineers" reportedly visited Mitel to meet with two of the latter's employees about a problem, according to "The New York Times".

Another aspect of IBM that did not change was secrecy; employees at Yorktown knew nothing of Boca Raton's activities. Those working on the project, within and outside of IBM, were subject to strict confidentiality agreements. When someone mentioned in public on a Saturday that his company was working on software for a new IBM computer, IBM security appeared at the company on Monday to investigate. After an IBM official discovered printouts in a supplier's garbage, the former's company persuaded the latter to purchase a paper shredder. Management Science America did not know until after agreeing to buy Peachtree Software in 1981 that the latter was working on software for the PC. Developers such as Software Arts received breadboard prototype computers in soldered boxes lined with lead to block X-rays, and had to keep them in locked, windowless rooms; to develop software, Microsoft emulated the PC on a DEC minicomputer and used the prototype for debugging. After the PC's debut, IBM Boca Raton employees continued to avoid discussing their jobs in public. One writer compared the "silence" after asking one about his role at the company to "hit[ting] the wall at the Boston Marathon: the conversation is over".

After developing it in 12 months — faster than any other hardware product in company history — IBM announced the Personal Computer on August 12, 1981. Pricing started at for a configuration with 16K RAM, Color Graphics Adapter, and no disk drives. The company intentionally set prices for it and other configurations that were comparable to those of Apple and other rivals; the Datamaster, announced two weeks earlier as the previous least-expensive IBM computer, cost $10,000. What Dan Bricklin described as "pretty competitive" pricing surprised him and other Software Arts employees. One analyst stated that IBM "has taken the gloves off", while the company said "we suggest [the PC's price] invites comparison".

"BYTE" described IBM as having "the strongest marketing organization in the world", but the PC's marketing also differed from that of previous products. The company was aware of its strong corporate reputation among potential customers; an early advertisement began "Presenting the IBM of Personal Computers". Estridge recalled that "The most important thing we learned was that how people reacted to a personal computer emotionally was almost more important than what they did with it". Advertisements emphasized the novelty of an individual owning an IBM computer, describing "a product "you" may have a personal interest in" and asking readers to think of My own IBM computer. Imagine that' ... it's yours. For your business, your project, your department, your class, your family and, indeed, for yourself".

In addition to the existing corporate sales force IBM established its own Product Center retail stores. After studying Apple's successful distribution network, the company surprised the industry by selling through others for the first time, including ComputerLand and Sears Roebuck. Because retail stores receive revenue from repairing computers and providing warranty service, IBM ended a 70-year tradition by permitting and training non-IBM service personnel to fix the PC.

IBM considered Alan Alda, Beverly Sills, Kermit the Frog, and Billy Martin to be celebrity endorsers of the PC, but chose Charlie Chaplin's The Little Tramp character for a series of advertisements based on Chaplin's movies, played by Billy Scudder. Chaplin's movie "Modern Times" expressed his opposition to big business, mechanization, and technological efficiency, but the $36-million marketing campaign made Chaplin the (according to "Creative Computing") "warm cuddly" mascot of one of the world's largest companies.

Chaplin and his character became so widely associated with IBM that others used his bowler hat and cane to represent or satirize the company. Chaplin's estate sued those who used the trademark without permission, yet "PC Magazine"s April 1983 issue had 12 advertisements which referred to the Little Tramp.

Perhaps Chess's most unusual decision for IBM was to publish the PC's technical specifications, allowing outsiders to create products for it. "We encourage third-party suppliers ... we are delighted to have them", the company stated. Although the team began "dogfooding" before the PC's debut by managing business operations on prototypes, and despite IBM's $5.3 billion R&D budget in 1982—- larger than the total revenue of many competitors—- the company did not sell internally developed PC software until April 1984, instead relying on already established software companies. Microsoft, Personal Software, and Peachtree Software were among the developers of nine launch titles including EasyWriter and VisiCalc, all already available for other computers. The company contacted Microsoft even before the official approval of Chess, and it and others received cooperation that was, "PC Magazine" said, "unheard of" for IBM. Such openness surprised observers; "BYTE" termed it "striking" and "startling", and one developer reported that "it's a very different IBM". Another said "They were very open and helpful about giving us all the technical information we needed. The feeling was so radically different—- it's like stepping out into a warm breeze." He concluded, "After years of hassling—- fighting the Not-Invented-Here attitude—- we're the gods."

Most other personal-computer companies did not disclose technical details. Tandy hoped to monopolize sales of TRS-80 software and peripherals. Its RadioShack stores only sold Tandy products; third-party developers found selling their offerings difficult. TI intentionally made developing third-party Texas Instruments TI-99/4A software difficult, even requiring a lockout chip in cartridges. IBM itself kept its mainframe technology so secret that rivals were indicted for industrial espionage. For the PC, however, IBM immediately released detailed information. The US$36 "IBM PC Technical Reference Manual" included complete circuit schematics, commented ROM BIOS source code, and other engineering and programming information for all of IBM's PC-related hardware, plus instructions on designing third-party peripherals. It was so comprehensive that one reviewer suggested that the manual could serve as a university textbook, and so easy to understand that a developer claimed that he could design an expansion card without seeing the physical computer.

IBM marketed the technical manual in full-page color print advertisements, stating that "our software story is still being written. Maybe by you". Sydnes stated that "The definition of a personal computer "is" third-party hardware and software". Estridge said that IBM did not keep software development proprietary because it would have to "out-VisiCalc VisiCorp and out-Peachtree Peachtree—- and you just can't do that".

Another advertisement told developers that the company would consider publishing software for "Education. Entertainment. Personal finance. Data management. Self-improvement. Games. Communications. And yes, business." Estridge explicitly invited small, "cottage" amateur and professional developers to create products "with", he said, "our logo and our support". IBM sold the PC at a large discount to employees, encouraged them to write software, and distributed a catalog of inexpensive software written by individuals that might not otherwise appear in public.

The announcement by "a company whose name is synonymous with computers", the "Times" said, gave credibility to the new industry. The press reported on most details of the PC before the official announcement; only IBM not providing internally developed software, including DOS (86-DOS) as the operating system, surprised observers. "BYTE" was correct in predicting that an IBM personal computer would nonetheless receive much public attention. Its rapid development amazed observers, as did the Colossus of Armonk selling as a launch title Microsoft "Adventure" (a video game that, its press release stated, brought "players into a fantasy world of caves and treasures"); the company even offered an optional joystick port. Future Computing estimated that "IBM's Billion Dollar Baby" would have $2.3 billion in hardware sales by 1986. David Bunnell, an editor at Osborne/McGraw-Hill, recalled that

Within seven weeks Bunnell helped found "PC Magazine", the first periodical for the new computer.

The industry awaited and feared IBM's announcement for months. "InfoWorld" reported that "On the morning of the announcement, phone calls to IBM's competitors revealed that almost everyone was having an 'executive meeting' involving the high-level officials who might be in a position to publicly react to the IBM announcement". Claiming that the new IBM computer competed against rivals' products, they were publicly skeptical about the PC. Adam Osborne said that unlike his Osborne I, "when you buy a computer from IBM, you buy a la carte. By the time you have a computer that does anything, it will cost more than an Apple. I don't think Apple has anything to worry about". Apple's Mike Markkula agreed that IBM's product was more expensive than the Apple II, and claimed that the Apple III "offers better performance". He denied that the IBM PC offered more memory, stating that his company could offer more than 128K "but frankly we don't know what anyone would do with that memory". At Tandy, John Roach said "I don't think it's that significant"; Jon Shirley admitted that IBM had a "legendary service reputation" but claimed that the thousands of Radio Shack stores "can provide better service", while predicting that the IBM PC's "major market will be IBM addicts"; and Garland P. Asher said that he was "relieved that whatever they were going to do, they finally did it. I'm certainly relieved at the pricing". Tandy could undersell a $3,000 IBM computer by $1,000, he stated.

Many criticized the PC's design as outdated and not innovative, and believed that alleged weaknesses, such as the use of single-sided, single-density disks with less storage than the computer's RAM, and limited graphics capability (customers who wanted both color and high-quality text needed two graphics cards and two monitors), existed because the company was uncertain about the market and was experimenting before releasing a better computer. (Estridge boasted, "Many ... said that there was nothing technologically new in this machine. That was the best news we could have had; we actually had done what we had set out to do".)

Although the "Times" said that IBM would "pose the stiffest challenge yet to Apple and to Tandy", they and Commodore—- together with more than 50% of the personal-computer market—- had many advantages. While IBM began with one microcomputer, little available hardware or software, and a couple of hundred dealers, Radio Shack had sold more than 350,000 computers. It had 14 million customers and 8,000 stores—- more than McDonald's—- that only sold its broad range of computers and accessories. Apple had sold more than 250,000 computers and had five times as many dealers in the US as IBM and an established international distribution network. Hundreds of independent developers produced software and peripherals for Tandy and Apple computers; at least ten Apple databases and ten word processors were available, while the PC had no databases and one word processor. Altos, Vector Graphic, Cromemco, and Zenith were among those making CP/M, "InfoWorld" said, ""the" small-computer operating system".

Radio Shack and Apple hoped that an IBM personal computer would help increase the market. Steve Jobs at Apple ordered a team to examine an IBM PC. After finding it unimpressive—Chris Espinosa described the computer as "a half-assed, hackneyed attempt"—the company confidently purchased a full-page advertisement in "The Wall Street Journal" with the headline "Welcome, IBM. Seriously". Gates was at Apple headquarters the day of IBM's announcement and later said "They didn't seem to care. It took them a full year to realize what had happened".

The PC was immediately successful. "PC Magazine" later wrote that "IBM's biggest error was in underestimating the demand for the PC". "BYTE" reported a rumor that more than 40,000 were ordered on the day of the announcement; one dealer received 22 $1,000 deposits from customers although he could not promise a delivery date. John Dvorak recalled that a dealer that day praised the computer as an "incredible winner, and IBM knows how to treat us — none of the Apple arrogance". The company could have sold its entire projected first-year production to employees, and IBM customers that were reluctant to purchase Apples were glad to buy microcomputers from their traditional supplier. The computer began shipping during October, ahead of schedule; by then some referred to it simply as "the PC".

"BYTE" estimated that 90% of the 40,000 first-day orders were from software developers. By COMDEX in November Tecmar developed 20 products including memory expansion and expansion chassis, surprising even IBM. Jerry Pournelle reported after attending the West Coast Computer Faire during early 1982 that because IBM "encourages amateurs" with "documents that tell all", "an explosion of [third-party] hardware and software" was visible at the convention. Many manufacturers of professional business application software, who had been planning/developing versions for the Apple II, promptly switched their efforts to the IBM PC when it was announced. Often, these products needed the capacity and speed of a hard-disk. Although IBM did not offer a hard-disk option for almost two years after introduction of its PC, business sales were nonetheless catalyzed by the simultaneous availability of hard-disk subsystems, like those of Tallgrass Technologies which sold in ComputerLand stores alongside the IBM 5150 at the introduction in 1981.

One year after the PC's release, although IBM had sold fewer than 100,000 computers, "PC World" counted 753 software packages for the PC—more than four times the number available for the Apple Macintosh one year after its 1984 release—including 422 applications and almost 200 utilities and languages. "InfoWorld" reported that "most of the major software houses have been frantically adapting their programs to run on the PC", with new PC-specific developers composing "an entire subindustry that has formed around the PC's open system", which Dvorak described as a "de facto standard microcomputer". The magazine estimated that "hundreds of tiny garage-shop operations" were in "bloodthirsty" competition to sell peripherals, with 30 to 40 companies in a price war for memory-expansion cards, for example. "PC Magazine" renamed its planned "1001 Products to Use with Your IBM PC" special issue after the number of product listings it received exceeded the figure. Tecmar and other companies that benefited from IBM's openness rapidly grew in size and importance, as did "PC Magazine"; within two years it expanded from 96 bimonthly to 800 monthly pages, including almost 500 pages of advertisements.

Gates correctly predicted that IBM would sell "not far from 200,000" PCs during 1982; by the end of the year it was selling one every minute of the business day. The company estimated that 50 to 70% of PCs sold in retail stores went to the home, and the publicity from selling a popular product to consumers caused IBM to, a spokesman said, "enter the world" by familiarizing them with IBM. Although the PC only provided two to three percent of sales the company found that demand exceeded its estimate by as much as 800%. Because its prices were based on forecasts of much lower volume—250,000 over five years, which would have made the PC a very successful IBM product—the PC became very profitable; at times the company sold almost that many computers per month. Estridge said in 1983 that from October 1982 to March 1983 customer demand quadrupled, with production increasing three times in one year, and warned of a component shortage if demand continued to increase. Many small suppliers' sales to IBM grew rapidly, both pleasing their executives and causing them to worry about being overdependent on it. Miniscribe, for example, during 1983 received 61% of its hard drive orders from IBM; the company's stock price fell by more than one third in one day after IBM reduced orders in January 1984. Suppliers often found, however, that the prestige of having IBM as a customer resulted in additional sales elsewhere.

By early 1983 the "Times" said "I.B.M.'s role in the personal computer world is beginning to resemble its central role in the mainframe computer business, in which I.B.M. is the sun around which everything else revolves". As "a de facto standard", the newspaper wrote, "Virtually every software company is giving first priority to writing programs for the I.B.M. machine". Yankee Group estimated that year that ten new IBM PC-related products appeared every day. In August the Chess IBU, with 4,000 employees, became the Entry Systems Division, which observers believed indicated that the PC was significantly important to IBM overall, and no longer an experiment. In November the Associated Press stated that the PC "in two years [had] effectively set a new standard in desktop computers", and "The Economist" said that IBM "set a standard that those who hope to compete will usually have to follow".

The PC surpassed the Apple II during late 1983 as the best-selling personal computer with more than 750,000 sold by the end of the year, while DEC only sold 69,000 microcomputers during the first nine months despite offering three models for different markets. IBM recruited the best Apple dealers while avoiding the grey market; by March 1983 770 separate resellers sold the PC in the US and Canada. It was 65% of BusinessLand's revenue. Demand still so exceeded supply two years after its debut that, despite IBM shipping 40,000 PCs a month, dealers reportedly received 60% or less of their desired quantity; some promoted Apples to reduce dependence. Pournelle received the PC he paid for in early July 1983 on 1 November, and IBM Boca Raton employees and neighbors had to wait five weeks to buy the computers assembled there.

Yankee Group also stated that the PC had by 1983 "destroyed the market for some older machines" from companies like Vector Graphic, North Star, and Cromemco. "inCider" wrote "This may be an Apple magazine, but let's not kid ourselves, IBM has devoured competitors like a cloud of locusts". By February 1984 "BYTE" reported on "the phenomenal market acceptance of the IBM PC", and by autumn concluded that the company "has given the field its third major standard, after the Apple II and CP/M". Rivals speculated that the government might again prosecute IBM for antitrust, and Ben Rosen claimed that the company's dominance "is having a chilling effect on new ventures, a fear factor".

By that time, Apple was less welcoming of the rival that "inCider" stated had a "godlike" reputation. The PC almost completely ended sales of the Apple III, the company's most comparable product, but its emphasis on the III had delayed improvements to the II, and the sophisticated Lisa was unsuccessful in part because, unlike the II and the PC, Apple discouraged third-party developers. The head of a retail chain said "It appears that IBM had a better understanding of why the Apple II was successful than had Apple". Jobs, after trying to recruit Estridge to become Apple's president, admitted that in two years IBM had joined Apple as "the industry's two strongest competitors". He warned in a speech before previewing the forthcoming "1984" Super Bowl commercial: "It appears IBM wants it "all" ... Will Big Blue dominate the entire computer industry? The entire information age? Was George Orwell right about 1984?"

IBM had $4 billion in annual PC revenue by 1984, more than twice that of Apple and as much as the sales of Apple, Commodore, HP, and Sperry combined, and 6% of total revenue. Most companies with mainframes used PCs with the larger computers. Customers having what some IBM executives called the "logo machine" on desks likely benefited mainframe sales and discouraged their purchasing non-IBM hardware; they "prefer a single standard", "The Economist" said. A 1983 study of corporate customers found that two thirds of large customers standardizing on one computer chose the PC, compared to 9% for Apple. A 1985 "Fortune" survey found that 56% of American companies with personal computers used PCs, compared to Apple's 16%. 

Yankee Group wrote that the PC's success showed that "technological elegance and a leading price/performance position is almost irrelevant to market success". IBM had defeated UNIVAC with an inferior mainframe computer, and IBM's own documentation agreed with observers that described the PC as inferior to competitors' less-expensive products. The company generally did not compete on price; rather, the 1983 study found that customers preferred "IBM's hegemony" because of its technical assistance. They wanted "vendor recognition, applications software availability (vendor and third-party), a reputation for product reliability and support, moderately competitive pricing, and an assurance that the vendor won't disappear in the impending personal computer market shakeout", Yankee Group wrote.

In 1984, IBM introduced the PC/AT, unlike its predecessor the most sophisticated personal computer from any major company. By 1985, the PC family had more than doubled Future Computing's 1986 revenue estimate, with more than 12,000 applications and 4,500 dealers and distributors worldwide. The PC was similarly dominant in Europe, two years after release there. In his 1985 obituary, "The New York Times" wrote that Estridge had led the "extraordinarily successful entry of the International Business Machines Corporation into the personal computer field". The Entry Systems Division had 10,000 employees and by itself would have been the world's third-largest computer company after IBM and DEC, with more revenue than IBM's minicomputer business despite its much later start. IBM was the only major company with significant minicomputer and microcomputer businesses; rivals like DEC and Wang also released personal computers but did not adjust to retail sales.

Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. Other manufacturers soon reverse engineered the BIOS to produce their own non-infringing functional copies. Columbia Data Products introduced the first IBM-PC compatible computer in June 1982. In November 1982, Compaq Computer Corporation announced the "Compaq Portable", the first portable IBM PC compatible. The first models were shipped during January 1983.

The success of the IBM computer caused other companies to develop "IBM Compatibles", which in turn resulted in IBM-like diskettes being advertised as "IBM format". An IBM PC clone could be built with off-the-shelf parts, but the BIOS required some reverse engineering. Companies like Compaq, Phoenix Software Associates, American Megatrends, Award, and others achieved fully functional versions of the BIOS, allowing companies like Dell, Gateway and HP to manufacture PCs that worked like IBM's product. The IBM PC became the industry standard.

Because IBM did not have retail experience, the retail chains ComputerLand and Sears Roebuck provided important knowledge of the marketplace. They became the main outlets for the new product. More than 190 ComputerLand stores already existed, while Sears was in the process of creating a handful of in-store computer centers for sale of the new product. This guaranteed IBM widespread distribution across the U.S.

Targeting the new PC at the home market, Sears Roebuck sales failed to live up to expectations. This unfavorable outcome revealed that the office market was the most important for greater sales.

All IBM personal computers are software backwards-compatible with each other in general, but not every program will work in every machine. Some programs are time sensitive to a particular speed class. Older programs will not take advantage of newer higher-resolution and higher-color display standards, while some newer programs require newer display adapters. (Note that as the display adapter was an adapter card in all of these IBM models, newer display hardware could easily be, and often was, retrofitted to older models.) A few programs, typically very early ones, are written for and require a specific version of the IBM PC BIOS ROM. Most notably, BASICA which was dependent on the BIOS ROM had a sibling program named GW-BASIC which supported more functions, was 100% backwards compatible and could execute independently from the BIOS ROM.

The CGA video card, with a suitable modulator, could use an NTSC television set or an RGBi monitor for display; IBM's RGBi monitor was their display model 5153. The other option that was offered by IBM was an MDA and their monochrome display model 5151. It was possible to install both an MDA and a CGA card and use both monitors concurrently if supported by the application program. For example, AutoCAD, Lotus 1-2-3 and others allowed use of a CGA Monitor for graphics and a separate monochrome monitor for text menus. Some model 5150 PCs with CGA monitors and a printer port also included the MDA adapter by default, because IBM provided the MDA port and printer port on the same adapter card; it was in fact an MDA/printer port combo card.

Although cassette tape was originally envisioned by IBM as a low-budget storage alternative, the most commonly used medium was the floppy disk. The 5150 was available with one or two " floppy drives – with two drives the program disc(s) would be in drive A, while drive B would hold the disc(s) for working files; with one drive the user had to swap program and file discs into the single drive. For models without any drives or storage medium, IBM intended users to connect their own cassette recorder via the 5150's cassette socket. The cassette tape socket was physically the same DIN plug as the keyboard socket and next to it, but completely different electrically.

A hard disk drive could not be installed into the 5150's system unit without changing to a higher-rated power supply (although later drives with lower power consumption have been known to work with the standard 63.5 Watt unit). The "IBM 5161 Expansion Chassis" came with its own power supply and one 10 MB hard disk and allowed the installation of a second hard disk. Without the expansion chassis perhaps one free slot remains after installing necessary cards, as a working configuration requires that some of the slots be occupied by display, disk, and I/O adapters, as none of these were built into the 5150's motherboard; the only motherboard external connectors are the keyboard and cassette ports.The system unit has five expansion slots, and the expansion unit has eight; however, one of the system unit's slots and one of the expansion unit's slots have to be occupied by the Extender Card and Receiver Card, respectively, which are needed to connect the expansion unit to the system unit and make the expansion unit's other slots available, for a total of 11 slots.

The simple PC speaker sound hardware was also on board.

The original PC's maximum memory using IBM parts was 256 kB, achievable through the installation of 64 kB on the motherboard and three 64 kB expansion cards. The processor was an Intel 8088 running at 4.77 MHz, 4/3 the standard NTSC color burst frequency of 315/88 = 3.579 MHz. (In early units, the Intel 8088 used was a 1978 version, later were 1978/81/2 versions of the Intel chip; second-sourced AMDs were used after 1983). Some owners replaced the 8088 with an NEC V20 for a slight increase in processing speed and support for real mode 80186 instructions. The V20 gained its speed increase through the use of a hardware multiplier which the 8088 lacked. An Intel 8087 coprocessor could also be added for hardware floating-point arithmetic.

IBM sold the first IBM PCs in configurations with 16 or 64 kB of RAM preinstalled using either nine or thirty-six 16-kilobit DRAM chips. (The ninth bit was used for parity checking of memory.) In November 1982, the hardware was changed to allow the use of 64-Kbit chips (as opposed to the original 16-Kbit chips) - the same RAM configuration as the soon-to-be-released IBM XT. (64 kB in one bank, expandable to 256kB by populating the other three banks.)

Although the television-compatible video board, cassette port and Federal Communications Commission Class B certification were all intended to make it a home computer, the original PC proved too expensive for the home market. At introduction, a PC with 64 kB of RAM and a single 5.25-inch floppy drive and monitor sold for (), while the cheapest configuration () that had no floppy drives, only 16 kB RAM, and no monitor (again, under the expectation that users would connect their existing TV sets and cassette recorders) proved too unattractive and low-spec, even for its time (cf. footnotes to the above IBM PC range table). While the 5150 did not become a top selling home computer, its floppy-based configuration became an unexpectedly large success with businesses.

The "IBM Personal Computer XT", IBM model 5160, was introduced two years after the PC and featured a 10 megabyte hard drive. It had eight expansion slots but the same processor and clock speed as the PC. The XT had no cassette jack, but still had the Cassette Basic interpreter in ROMs.

The XT could take 256 kB of memory on the main board (using 64 kbit DRAM); later models were expandable to 640 kB. The remaining 384 kilobytes of the 8088 address space (between 640 KB and 1 MB) were used for the BIOS ROM, adapter ROM and RAM space, including video RAM space. It was usually sold with a Monochrome Display Adapter (MDA) video card or a CGA video card.

The eight expansion slots were the same as the model 5150 but were spaced closer together. Although rare, a card designed for the 5150 could be wide enough to obstruct the adjacent slot in an XT. Because of the spacing, an XT motherboard would not fit into a case designed for the PC motherboard, but the slots and peripheral cards were compatible. The XT expansion bus (later called "8-bit Industry Standard Architecture" (ISA) by competitors) was retained in the IBM AT, which added connectors for some slots to allow 16-bit transfers; 8-bit cards could be used in an AT.

The "IBM Personal Computer XT/370" was an XT with three custom 8-bit cards: the processor card (370PC-P) contained a modified Motorola 68000 chip, microcoded to execute System/370 instructions, a second 68000 to handle bus arbitration and memory transfers, and a modified 8087 to emulate the S/370 floating point instructions. The second card (370PC-M) connected to the first and contained 512 kB of memory. The third card (PC3277-EM), was a 3270 terminal emulator necessary to install the system software for the VM/PC software to run the processors.

The computer booted into DOS, then executed the VM/PC Control Program.

The "IBM PCjr" was IBM's first attempt to join the market for relatively inexpensive educational and home-use personal computers. The PCjr, IBM model number 4860, retained the IBM PC's 8088 CPU and BIOS interface for compatibility, but its cost and differences in the PCjr's architecture, as well as other design and implementation decisions (chief among these was the use of a so-called "chiclet" keyboard, which was difficult to type with), eventually resulted in the PCjr, and the related IBM JX, being commercial failures.

The "IBM Portable Personal Computer" 5155 model 68 was an early portable computer developed by IBM after the success of Compaq's suitcase-size portable machine (the Compaq Portable). It was released during February 1984, and was eventually replaced by the IBM Convertible.

The Portable was an XT motherboard, transplanted into a Compaq-style luggable case. The system featured 256 kilobytes of memory (expandable to 512 KB), an added CGA card connected to an internal monochrome (amber) composite monitor, and one or two half-height 5.25" 360 KB floppy disk drives. Unlike the Compaq Portable, which used a dual-mode monitor and special display card, IBM used a stock CGA board and a composite monitor, which had lower resolution. It could however, display color if connected to an external monitor or television.

The "IBM Personal Computer/AT" (model 5170), announced August 15, 1984, used an Intel 80286 processor, originally running at 6 MHz. It had a 16-bit ISA bus and 20 MB hard drive. A faster model, running at 8 MHz and sporting a 30-megabyte hard disk was introduced during 1986.

The AT was designed to support multitasking; the new SysRq (system request) key, little noted and often overlooked, is part of this design, as is the 80286 itself, the first Intel 16-bit processor with multitasking features (i.e. the 80286 protected mode). IBM made some attempt at marketing the AT as a multi-user machine, but it sold mainly as a faster PC for power users. For the most part, IBM PC/ATs were used as more powerful DOS (single-tasking) personal computers, in the literal sense of the PC name.

Early PC/ATs were plagued with reliability problems, in part because of some software and hardware incompatibilities, but mostly related to the internal 20 MB hard disk, and High Density Floppy Disk Drive.

While some people blamed IBM's hard disk controller card and others blamed the hard disk manufacturer Computer Memories Inc. (CMI), the IBM controller card worked fine with other drives, including CMI's 33-MB model. The problems introduced doubt about the computer and, for a while, even about the 286 architecture in general, but after IBM replaced the 20 MB CMI drives, the PC/AT proved reliable and became a lasting industry standard.

IBM AT's Drive parameter table listed the CMI-33 as having 615 cylinders instead of the 640 the drive was designed with, as to make the size an even 30 MB. Those who re-used the drives mostly found that the 616th cylinder was bad due to it being used as a landing area.

The "IBM Personal Computer AT/370" was an AT with two custom 16-bit cards, running almost exactly the same setup as the XT/370.

The IBM PC Convertible, released April 3, 1986, was IBM's first laptop computer and was also the first IBM computer to utilize the 3.5" floppy disk which later became the standard. Like modern laptops, it featured power management and the ability to operate from batteries. It was the follow-up to the IBM Portable and was model number 5140. The concept and the design of the body was made by the German industrial designer Richard Sapper.

It utilized an Intel 80c88 CPU (a CMOS version of the Intel 8088) running at 4.77 MHz, 256 kB of RAM (expandable to 640 kB), dual 720 kB 3.5" floppy drives, and a monochrome CGA-compatible LCD screen at a price of $2,000. It weighed and featured a built-in carrying handle.

The PC Convertible had expansion capabilities through a proprietary ISA bus-based port on the rear of the machine. Extension modules, including a small printer and a video output module, could be snapped into place. The machine could also take an internal modem, but there was no room for an internal hard disk. Discontinued in August of 1989, the IBM PC Convertible was the only IBM PC model to last beyond the April 2, 1987 discontinuation of all other models.

The IBM PS/2 line was introduced during 1987. The Model 30, the cheapest of the group, was very similar to earlier models; it used an 8086 processor and an ISA bus. The Model 30 was not "IBM compatible" in that it did not have standard 5.25-inch drive bays; it came with a 3.5-inch floppy drive and optionally a 3.5-inch-sized hard disk. Most models of the PS/2 line further departed from "IBM compatible" by replacing the ISA bus completely with Micro Channel Architecture. The MCA bus was not received well by the customer base for PC's, since it was proprietary to IBM. It was rarely implemented by any of the other PC-compatible makers. Eventually IBM would abandon this architecture entirely and resume using the standard ISA bus.

The main circuit board in a PC is called the motherboard (IBM terminology calls it a "planar"). This mainly carries the CPU and RAM, and has a bus with slots for expansion cards. Also on the motherboard are the ROM subsystem, DMA and IRQ controllers, coprocessor socket, sound (PC speaker, tone generation) circuitry, and keyboard interface. The original PC also has a cassette interface.

The bus used in the original PC became very popular, and it was later named Industry Standard Architecture (ISA). It was known originally as the PC-bus or XT-bus; the term "ISA" was used later when industry leaders chose to continue manufacturing machines based on the IBM PC AT architecture rather than license the PS/2 architecture and its Micro Channel bus from IBM. The XT-bus was then retroactively named "8-bit ISA" or "XT ISA", while the unqualified term "ISA" usually refers to the 16-bit AT-bus (as better defined in the ISA specifications). The AT-bus is an extension of the PC-/XT-bus and is in use to this day in computers for industrial use, where its relatively low speed, 5-volt signals, and relatively simple, straightforward design (all by year 2011 standards) give it technical advantages (e.g. noise immunity for reliability).
A monitor and any floppy or hard disk drives are connected to the motherboard through cables connected to graphics adapter and disk controller cards, respectively, installed in expansion slots. Each expansion slot on the motherboard has a corresponding opening in the back of the computer case through which the card can expose connectors; a blank metal cover plate covers this case opening (to prevent dust and debris intrusion and control airflow) when no expansion card is installed. Memory expansion beyond the amount installable on the motherboard was also done with boards installed in expansion slots, and I/O devices such as parallel, serial, or network ports were likewise installed as individual expansion boards. For this reason, it was easy to fill the five expansion slots of the PC, or even the eight slots of the XT, even without installing any special hardware. Companies like Quadram and AST addressed this with their popular multi-I/O cards which combine several peripherals on one adapter card that uses only one slot; Quadram offered the QuadBoard and AST the SixPak.

Intel 8086 and 8088-based PCs require expanded memory (EMS) boards to work with more than 640 kB of memory. (Though the 8088 can address one megabyte of memory, the last 384 kB of that is used or reserved for the BIOS ROM, BASIC ROM, extension ROMs installed on adapter cards, and memory address space used by devices including display adapter RAM and even the 64 kB EMS page frame itself.) The original IBM PC AT used an Intel 80286 processor which can access up to 16 MB of memory (though standard DOS applications cannot use more than one megabyte without using additional APIs). Intel 80286-based computers running under OS/2 can work with the maximum memory.

The set of peripheral chips selected for the original IBM PC defined the functionality of an IBM compatible. These became the de facto base for later application-specific integrated circuits (ASICs) used in compatible products.

The original system chips were one Intel 8259 programmable interrupt controller (PIC) (at I/O address ), one Intel 8237 direct memory access (DMA) controller (at I/O address ), and an Intel 8253 programmable interval timer (PIT) (at I/O address ). The PIT provides the clock ticks, dynamic memory refresh timing, and can be used for speaker output; one DMA channel is used to perform the memory refresh.

The mathematics coprocessor was the Intel 8087 using I/O address 0xF0. This was an option for users who needed extensive floating-point arithmetic, such as users of computer-aided drafting.

The IBM PC AT added a second, slave 8259 PIC (at I/O address ), a second 8237 DMA controller for 16-bit DMA (at I/O address ), a DMA address register (implemented with a 74LS612 IC) (at I/O address ), and a Motorola MC146818 real-time clock (RTC) with nonvolatile memory (NVRAM) used for system configuration (replacing the DIP switches and jumpers used for this purpose in PC and PC/XT models (at I/O address ). On expansion cards, the Intel 8255 programmable peripheral interface (PPI) (at I/O addresses is used for parallel I/O controls the printer, and the 8250 universal asynchronous receiver/transmitter (UART) (at I/O address or ) controls the serial communication at the (pseudo-) RS-232 port.

IBM offered a Game Control Adapter for the PC, which permitted analog joysticks similar to those on the Apple II. Although analog controls proved inferior for arcade-style games, they were an asset in certain other genres such as flight simulators. The joystick port of the IBM PC supported two controllers, but required a Y-splitter cable to connect both at once. It remained the standard joystick interface of IBM compatibles until being replaced by USB during the 2000s.

The keyboard that came with the IBM 5150 was an extremely reliable and high-quality electronic keyboard originally developed in North Carolina for the Datamaster. Each key was rated to be reliable to over 100 million keystrokes. For the IBM PC, a separate keyboard housing was designed with a novel usability feature that allowed users to adjust the keyboard angle for personal comfort. Compared with the keyboards of other small computers at the time, the IBM PC keyboard was far superior and played a significant role in establishing a high-quality impression. For example, the industrial design of the adjustable keyboard, together with the system unit, was recognized with a major design award. "Byte" magazine in the fall of 1981 went so far as to state that the keyboard was 50% of the reason to buy an IBM PC. The importance of the keyboard was definitely established when the 1983 IBM PCjr failed commercially, in large part for having a much different and mediocre "Chiclet keyboard" that made a poor impression on customers. Oddly enough, the same thing almost happened to the original IBM PC when during early 1981 management seriously considered substituting a cheaper and lower quality keyboard. This mistake was avoided on the advice of one of the original development engineers.

However, the original 1981 IBM PC 83-key keyboard was criticized by typists for its non-standard placement of the and left keys, and because it did not have separate cursor and numeric pads that were popular on the pre-PC DEC VT100 series video terminals. In 1982, Key Tronic introduced a 101-key PC keyboard, albeit not with the now-familiar layout. During 1984, IBM corrected the and left keys on its AT keyboard, but shortened the key, making it harder to reach. In 1986, IBM introduced the 101 key Enhanced Keyboard, which added the separate cursor and numeric key pads, relocated all the function keys and the keys, and the key was also relocated to the opposite side of the keyboard. The Enhanced Keyboard was an option for the PC XT/AT in 1986, both of which were also available with their original keyboards, and introduced the key layout that's still the industry standard.

Another feature of the original keyboard is the relatively loud "click" sound each key made when pressed. Since typewriter users were accustomed to keeping their eyes on the hardcopy they were typing from and had come to rely on the mechanical sound that was made as each character was typed onto the paper to ensure that they had pressed the key hard enough (and only once), the PC keyboard used a keyswitch that produced a click and tactile bump intended to provide that same reassurance.

The IBM PC keyboard is very robust and flexible. The low-level interface for each key is the same: each key sends a signal when it is pressed and another signal when it is released. An integrated microcontroller in the keyboard scans the keyboard and encodes a "scan code" and "release code" for each key as it is pressed and released separately. Any key can be used as a shift key, and a large number of keys can be held down simultaneously and separately sensed. The controller in the keyboard handles typematic operation, issuing periodic repeat scan codes for a depressed key and then a single release code when the key is finally released.

An "IBM PC compatible" may have a keyboard that does not recognize every key combination a true IBM PC does, such as shifted cursor keys. In addition, the "compatible" vendors sometimes used proprietary keyboard interfaces, preventing the keyboard from being replaced.

Although the PC/XT and AT used the same style of keyboard connector, the low-level protocol for reading the keyboard was different between these two series. The AT keyboard uses a bidirectional interface which allows the computer to send commands to the keyboard. An AT keyboard could not be used in an XT, nor the reverse. Third-party keyboard manufacturers provided a switch on some of their keyboards to select either the AT-style or XT-style protocol for the keyboard.

The original IBM PC used the 7-bit ASCII alphabet as its basis, but extended it to 8 bits with nonstandard character codes. This character set was not suitable for some international applications, and soon a veritable cottage industry developed providing variants of the original character set in various national variants. In IBM tradition, these variants were termed code pages. These codings are now largely obsolete, having been replaced by more systematic and standardized forms of character coding, such as ISO 8859-1, Windows-1251 and Unicode. The original character set is known as code page 437.

IBM equipped the model 5150 with a cassette port for connecting a cassette drive and assumed that home users would purchase the cheapest model and save files to cassette tapes as was typical of home computers of the time. However, adoption of the floppy- and monitor-less configuration was minor; few (if any) IBM PCs left the factory without a floppy disk drive installed. Also, DOS was not available on cassette tape, only on floppy disks (hence the name "Disk Operating System"). 5150s with just external cassette recorders for storage could only use the built-in ROM BASIC as their operating system. As DOS was increasingly adopted, the incompatibility of DOS programs with PCs that used only cassettes for storage made this configuration even less useful. The ROM BIOS permitted cassette operations.

The IBM PC cassette interface encodes data using frequency modulation with a variable data rate. Either a one or a zero is represented by a single cycle of a square wave, but the square wave frequencies differ by a factor of two, with ones having the lower frequency. Therefore, the bit periods for zeros and ones also differ by a factor of two, with the unusual effect that a data stream with more zeros than ones will use less tape (and time) than an equal-length (in bits) data stream containing more ones than zeros, or equal numbers of each.

IBM also had an exclusive license agreement with Microsoft to include BASIC in the ROM of the PC; clone manufacturers could not have ROM BASIC on their machines, but it also became a problem as the XT, AT, and PS/2 eliminated the cassette port and IBM was still required to install the (now useless) BASIC with them. The agreement finally expired during 1991 when Microsoft replaced BASICA/GW-BASIC with QBASIC. The main core BASIC resided in ROM and "linked" up with the RAM-resident BASIC.COM/BASICA.COM included with PC DOS (they provided disk support and other extended features not present in ROM BASIC). Because BASIC was more than 50 kB in size, this served a useful function during the first three years of the PC when machines only had 64–128 kB of memory, but became less important by 1985. For comparison, clone makers such as Compaq were forced to include a version of BASIC that resided entirely in RAM.

The first IBM 5150 PCs had two 5.25-inch 160 KiB single sided double density (SSDD) floppy disk drives. As two heads drives became available in the spring of 1982, later IBM PC and compatible computers could read 320 KiB double sided double density (DSDD) disks with software support of MS-DOS 1.25 and higher. The same type of physical diskette media could be used for both drives but a disk formatted for double-sided use could not be read on a single-sided drive. PC DOS 2.0 added support for 180 KiB and 360 KiB SSDD and DSDD floppy disks, using the same physical media again.

The disks were Modified Frequency Modulation (MFM) coded in 512-byte sectors, and were soft-sectored. They contained 40 tracks per side at the 48 track per inch (TPI) density, and initially were formatted to contain eight sectors per track. This meant that SSDD disks initially had a formatted capacity of 160 kB, while DSDD disks had a capacity of 320 kB. However, the PC DOS 2.0 and later operating systems allowed formatting the disks with nine sectors per track. This yielded a formatted capacity of 180 kB with SSDD disks/drives, and 360 kB with DSDD disks/drives. The "unformatted" capacity of the floppy disks was advertised as "250KB" for SSDD and "500KB" for DSDD ("KB" ambiguously referring to either 1000 or 1024 bytes; essentially the same for rounded-off values), however these "raw" 250/500 kB were not the same thing as the usable formatted capacity; under DOS, the maximum capacity for SSDD and DSDD disks was 180 kB and 360 kB, respectively. Regardless of type, the file system of all floppy disks (under DOS) was FAT12.

After the upgraded 64k-256k motherboard PCs arrived during early 1983, single-sided drives and the cassette model were discontinued.

IBM's original floppy disk controller card also included an external 37-pin D-shell connector. This allowed users to connect additional external floppy drives by third party vendors, but IBM did not offer their own external floppies until 1986.

The industry-standard way of setting floppy drive numbers was via setting jumper switches on the drive unit, however IBM chose to use instead a method known as the "cable twist" which had a floppy data cable with a bend in the middle of it that served as a switch for the drive motor control. This eliminated the need for users to adjust jumpers while installing a floppy drive.

The 5150 could not itself power hard drives without retrofitting a stronger power supply, but IBM later offered the 5161 Expansion Unit, which not only provided more expansion slots, but also included a 10 MB (later 20 MB) hard drive powered by the 5161's own separate 130-watt power supply. The IBM 5161 Expansion Unit was released during early 1983.

During the first year of the IBM PC, it was commonplace for users to install third-party Winchester hard disks which generally connected to the floppy controller and required a patched version of PC DOS which treated them as a giant floppy disk (there was no subdirectory support).

IBM began offering hard disks with the XT, however the original PC was never sold with them. Nonetheless, many users installed hard disks and upgraded power supplies in them.

After floppy disks became obsolete during the early 2000s, the letters A and B became unused. But for 25 years, virtually all DOS-based PC software assumed the program installation drive was C, so the primary HDD continues to be "the C drive" even today.
Other operating system families (e.g. Unix) are not bound to these designations.

Which operating system IBM customers would choose was at first uncertain. Although the company expected that most would use PC DOS IBM supported using CP/M-86—which became available six months after DOS—or UCSD p-System as operating systems. IBM promised that it would not favor one operating system over the others; the CP/M-86 support surprised Gates, who claimed that IBM was "blackmailed into it". IBM was correct, nonetheless, in its expectation; one survey found that 96.3% of PCs were ordered with the $40 DOS compared to 3.4% for the $240 CP/M-86.

The IBM PC's ROM BASIC and BIOS supported cassette tape storage. PC DOS itself did not support cassette tape storage. PC DOS version 1.00 supported only 160 kB SSDD floppies, but version 1.1, which was released nine months after the PC's introduction, supported 160 kB SSDD and 320 kB DSDD floppies. Support for the slightly larger nine sector per track 180 kB and 360 kB formats began 10 months later during March 1983.

The BIOS (Basic Input/Output System) provided the core ROM code for the PC. It contained a library of functions that software could call for basic tasks such as video output, keyboard input, and disk access in addition to interrupt handling, loading the operating system on boot-up, and testing memory and other system components.

The original IBM PC BIOS was 8k in size and occupied four 2k ROM chips on the motherboard, with a fifth and sixth empty slot left for any extra ROMs the user wished to install. IBM offered three different BIOS revisions during the PC's lifespan. The initial BIOS was dated April 1981 and came on the earliest models with single-sided floppy drives and PC DOS 1.00. The second version was dated October 1981 and arrived on the "Revision B" models sold with double-sided drives and PC DOS 1.10. It corrected some bugs, but was otherwise unchanged. Finally, the third BIOS version was dated October 1982 and found on all IBM PCs with the newer 64k-256k motherboard. This revision was more-or-less identical to the XT's BIOS. It added support for detecting ROMs on expansion cards as well as the ability to use 640k of memory (the earlier BIOS revisions had a limit of 544k). Unlike the XT, the original PC remained functionally unchanged from 1983 until its discontinuation in early 1987 and did not get support for 101-key keyboards or 3.5" floppy drives, nor was it ever offered with half-height floppies.

IBM initially offered two video adapters for the PC, the Color/Graphics Adapter and the Monochrome Display and Printer Adapter. CGA was intended to be a typical home computer display; it had NTSC output and could be connected to a composite monitor or a TV set with an RF modulator in addition to RGB for digital RGBI-type monitors, although IBM did not offer their own RGB monitor until 1983. Supported graphics modes were 40 or 80×25 color text with 8×8 character resolution, 320×200 bitmap graphics with two fixed 4-color palettes, or 640×200 monochrome graphics.

The MDA card and its companion 5151 monitor supported only 80×25 text with a 9×14 character resolution (total pixel resolution was 720×350). It was mainly intended for the business market and so also included a printer port.

During 1982, the first third-party video card for the PC was developed Hercules Computer Technologies released a clone of the MDA that could use bitmap graphics. Although not supported by the BIOS, the Hercules Graphics Adapter became extremely popular for business use due to allowing high resolution graphics plus text and itself was widely cloned by other manufacturers.

During 1985, after the introduction of the IBM AT, the new Enhanced Graphics Adapter became available which could support 320×200 or 640×200 in 16 colors in addition to high-resolution 640×350 16 color graphics.

IBM also offered a video board for the PC, XT, and AT known as the Professional Graphics Adapter during 1984–86, mainly intended for CAD design. It was extremely expensive, required a special monitor, and was rarely ordered by customers.

VGA graphics cards could also be installed in IBM PCs and XTs, although they were introduced after the computer's discontinuation.

The serial port is an 8250 or a derivative (such as the 16450 or 16550), mapped to eight consecutive IO addresses and one interrupt request line.

Only COM1: and COM2: addresses were defined by the original PC. Attempts to share IRQ3 and IRQ4 to use additional ports require special measures in hardware and software, since shared IRQs were not defined in the original PC design. The most typical devices plugged into the serial port were modems and mice. Plotters and serial printers were also among the more commonly used serial peripherals, and there were numerous other more unusual uses such as operating cash registers, factory equipment, and connecting terminals.

IBM made a deal with the Japan-based Epson company to produce printers for the PC and all IBM-branded printers were manufactured by that company (Epson of course also sold printers with their own name). There was a considerable amount of controversy when IBM included a printer port on the PC that did not use the industry-standard Centronics design, and it was rumored that this had been done to prevent customers from using non-Epson/IBM printers with their machines (plugging a Centronics printer into an IBM PC could damage the printer, the parallel port, or both). Although third-party cards were available with Centronics ports on them, PC clones quickly copied the IBM printer port and by the late 1980s, it had largely displaced the Centronics standard.

"BYTE" published in October 1981 that the IBM PC's "hardware is impressive, but even more striking are two decisions made by IBM: to use outside suppliers already established in the microcomputer industry, and to provide information and assistance to independent, small-scale software writers and manufacturers of peripheral devices". It praised the "smart" hardware design and stated that its price was not much higher than the 8-bit machines from Apple and others. The reviewer admitted that the computer "came as a shock. I expected that the giant would stumble by overestimating or underestimating the capabilities the public wants and stubbornly insisting on incompatibility with the rest of the microcomputer world. But IBM didn't stumble at all; instead, the giant jumped leagues in front of the competition". He concluded that "the only disappointment about the IBM Personal Computer is its dull name".

In a more detailed review in January 1982, "BYTE" called the IBM PC "a synthesis of the best the microcomputer industry has offered to date ... as well designed on the inside as it is on the outside". The magazine praised the keyboard as "bar none, the best ... on any microcomputer", describing the unusual Shift key locations as "minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard". The review also complimented IBM's manuals, which it predicted "will set the standard for all microcomputer documentation in the future. Not only are they well packaged, well organized, and easy to understand, but they are also "complete"". Observing that detailed technical information was available "much earlier ... than it has been for other machines", the magazine predicted that "given a reasonable period of time, plenty of hardware and software will probably be developed for" the computer. The review stated that although the IBM PC cost more than comparably configured Apple II and TRS-80 computers, and the insufficient number of slots for all desirable expansion cards was its most serious weakness, "you get a "lot" more for your money". He concluded, "In two years or so, I think [it] will be one of the most popular and best-supported ... IBM should be proud of the people who designed it".

In a special 1984 issue dedicated to the IBM PC, "BYTE" concluded that the PC had succeeded both because of its features like an 80-column screen, open architecture, and high-quality keyboard, and "the failure of other major companies to provide these same fundamental features earlier. In retrospect, it seems IBM stepped into a void that remained, paradoxically, at the center of a crowded market". "Creative Computing" that year named the PC the best desktop computer between $2000 and $4000, praising its vast hardware and software selection, manufacturer support, and resale value.

Many IBM PCs have remained in service long after their technology became largely obsolete. During June 2006, IBM PC and XT models were still in use at the majority of U.S. National Weather Service upper-air observing sites, used to process data as it is returned from the ascending radiosonde, attached to a weather balloon, although they have been slowly phased out. Factors that have contributed to the 5150 PC's longevity are its flexible modular design, its open technical standard (making information needed to adapt, modify, and repair it readily available), use of few special nonstandard parts, and rugged high-standard IBM manufacturing, which provided for exceptional long-term reliability and durability.

Some of the mechanical aspects of the slot specifications are still used in current PCs. A few systems still come with PS/2 style keyboard and mouse connectors.

The IBM model 5150 Personal Computer has become a collectable among vintage computer collectors, due to the system being the first true “PC” as we know them now. , the system had a market value of $50–$500. The IBM model 5150 has proven to be reliable; despite their age of 30 years or more, some still function as they did when new.







</doc>
<doc id="15033" url="https://en.wikipedia.org/wiki?curid=15033" title="Counties of Ireland">
Counties of Ireland

The counties of Ireland (; Ulster-Scots: "coonties o Airlann") are sub-national divisions that have been, and in some cases continue to be, used to geographically demarcate areas of local government. These land divisions were formed following the Norman invasion of Ireland in imitation of the counties then in use as units of local government in the Kingdom of England. The older term ‘shire’ was historically equivalent to ‘county’. The principal function of the county was to impose royal control in the areas of taxation, security and the administration of justice at the local level. Cambro-Norman control was initially limited to the southeastern parts of Ireland; a further four centuries elapsed before the entire island was shired. At the same time, the now obsolete concept of county corporate elevated a small number of towns and cities to a status which was deemed to be no less important than the existing counties in which they lay. This double control mechanism of 32 counties plus 10 counties corporate remained unchanged for a little over two centuries until the early 19th century. Since then, counties have been adapted and in some cases divided by legislation to meet new administrative and political requirements.

The powers exercised by the Cambro-Norman barons and the Old English nobility waned over time. New offices of political control came to be established at a county level. In the Republic of Ireland, some counties have been split resulting in the creation of new counties. Along with certain defined cities, counties still form the basis for the demarcation of areas of local government in the Republic of Ireland. Currently, there are 26 county level, 3 city level and 2 city and county entities – the modern equivalent of counties corporate – that are used to demarcate areas of local government in the Republic.

In Northern Ireland, counties are no longer used for local government; districts are instead used. Upon the partition of Ireland in 1921, the county became one of the basic land divisions employed, along with county boroughs.

The word "county" has come to be used in different senses for different purposes. In common usage, many people have in mind the 32 counties that existed prior to 1838 – the so-called traditional counties. However, in official usage in the Republic of Ireland, the term often refers to the 28 modern counties. The term is also conflated with the 31 areas currently used to demarcate areas of local government in the Republic of Ireland at the level of LAU 1.

In Ireland, usage of the word "county" nearly always comes before rather than after the county name; thus ""County" Roscommon" in Ireland as opposed to "Roscommon "County"" in Michigan, United States. The former "King's County" and "Queen's County" were exceptions; these are now County Offaly and County Laois, respectively. The abbreviation Co. is used, as in "Co. Roscommon". A further exception occurs in the case of those counties created after 1994 which often drop the word "county" entirely, or use it after the name; thus for example internet search engines show many more uses (on Irish sites) of "Fingal" than of either "County Fingal" or "Fingal County". There appears to be no official guidance in the matter, as even the local council uses all three forms. In informal use, the word "county" is often dropped except where necessary to distinguish between county and town or city; thus "Offaly" rather than "County Offaly", but "County Antrim" to distinguish it from Antrim town. The synonym "shire" is not used for Irish counties, although the Marquessate of Downshire was named in 1789 after County Down.

Parts of some towns and cities were exempt from the jurisdiction of the counties that surrounded them. These towns and cities had the status of a County corporate, many granted by Royal Charter, which had all the judicial, administrative and revenue raising powers of the regular counties.

The political geography of Ireland can be traced with some accuracy from the 6th century. At that time Ireland was divided into a patchwork of petty kingdoms with a fluid political hierarchy which, in general, had three traditional grades of king. The lowest level of political control existed at the level of the "túath" (pl. "túatha"). A "túath" was an autonomous group of people of independent political jurisdiction under a rí túaithe, that is, a local petty king. About 150 such units of government existed. Each "rí túaithe" was in turn subject to a regional or "over-king" (). There may have been as many as 20 genuine ruiri in Ireland at any time.

A "king of over-kings" () was often a provincial () or semi-provincial king to whom several ruiri were subordinate. No more than six genuine rí ruirech were ever contemporary. Usually, only five such "king of over-kings" existed contemporaneously and so are described in the Irish annals as "fifths" (). The areas under the control of these kings were: Ulster (), Leinster (), Connacht (), Munster () and Mide (). Later record-makers dubbed them "provinces", in imitation of Roman provinces. In the Norman period, the historic fifths of Leinster and Meath gradually merged, mainly due to the impact of the Pale, which straddled both, thereby forming the present-day province of Leinster.

The use of provinces as divisions of political power was supplanted by the system of counties after the Norman invasion. In modern times clusters of counties have been attributed to certain provinces but these clusters have no legal status. They are today seen mainly in a sporting context, as Ireland's four professional rugby teams play under the names of the provinces, and the Gaelic Athletic Association has separate Provincial councils and Provincial championships.

With the arrival of Cambro-Norman knights in 1169, the Norman invasion of Ireland commenced. This was followed in 1172 by the invasion of King Henry II of England, commencing English royal involvement.

After his intervention in Ireland, Henry II effectively divided the English colony into liberties also known as lordships. These were effectively palatine counties and differed from ordinary counties in that they were disjoined from the crown and that whoever they were granted to essentially had the same authority as the king and that the king's writ had no effect except a writ of error. This covered all land within the county that was not church land. The reasons for the creating of such powerful entities in Ireland was due to the lack of authority the English crown had there. The same process occurred after the Norman conquest of England where despite there being a strong central government, county palatines were needed in border areas with Wales and Scotland. In Ireland this meant that the land was divided and granted to Richard de Clare and his followers who became lords (and sometimes called earls), with the only land which the English crown had any direct control over being the sea-coast towns and territories immediately adjacent.

Of Henry II's grants, at least three of them—Leinster to Richard de Clare; Meath to Walter de Lacy; Ulster to John de Courcy—were equivalent to palatine counties in their bestowing of royal jurisdiction to the grantees. Other grants include the liberties of Connaught and Tipperary.

These initial lordships were later subdivided into smaller "liberties", which appear to have enjoyed the same privileges as their predecessors. The division of Leinster and Munster into smaller counties is commonly attributed to King John, mostly due to lack of prior documentary evidence, which has been destroyed. However, they may have had an earlier origin. These counties were: in Leinster: Carlow (also known as Catherlogh), Dublin, Kildare, Kilkenny, Louth (also known as Uriel), Meath, Wexford, Waterford; in Munster: Cork, Limerick, Kerry and Tipperary. It is thought that these counties did not have the administrative purpose later attached to them until late in the reign of King John, and that no new counties were created until the Tudor dynasty.

The most important office in those that were palatine was that of seneschal. In those liberties that came under Crown control this office was held by a sheriff. The sovereign could and did appoint sheriffs in palatines; however, their power was confined to the church lands, and they became known as sheriffs of a County of the Cross, of which there seem to have been as many in Ireland as there were counties palatine.

The exact boundaries of the liberties and shrievalties appears to have been in constant flux throughout the Plantagenet period, seemingly in line with the extent of English control. For example, in 1297 it is recorded that Kildare had extended to include the lands that now comprise the modern-day counties of Offaly, Laois (Leix) and Wicklow (Arklow). Some attempts had also been made to extend the county system to Ulster.

However the Bruce Invasion of Ireland in 1315 resulted in the collapse of effective English rule in Ireland, with the land controlled by the crown continually shrinking to encompass Dublin, and parts of Meath, Louth and Kildare. Throughout the rest of Ireland, English rule was upheld by the earls of Desmond, Ormond, and Kildare (all created in the 14th-century), with the extension of the county system all but impossible. During the reign of Edward III (1327–77) all franchises, grants and liberties had been temporarily revoked with power passed to the king's sheriffs over the seneschals. This may have been due to the disorganisation caused by the Bruce invasion as well as the renouncing of the Connaught Burkes of their allegiance to the crown.

The Earls of Ulster divided their territory up into counties; however, these are not considered part of the Crown's shiring of Ireland. In 1333, the Earldom of Ulster is recorded as consisting of seven counties: Antrim, Blathewyc, Cragferus, Coulrath, del Art, Dun (also known as Ladcathel), and Twescard.

Of the original lordships or palatine counties:

With the passing of liberties to the Crown, the number of Counties of the Cross declined, and only one, Tipperary, survived into the Stuart era; the others had ceased to exist by the reign of Henry VIII.

It was not until the Tudors, specifically the reign of Henry VIII (1509–47), that crown control started to once again extend throughout Ireland. Having declared himself King of Ireland in 1541, Henry VIII went about converting Irish chiefs into feudal subjects of the crown with land divided into districts, which were eventually amalgamated into the modern counties. County boundaries were still ill-defined; however, in 1543 Meath was split into Meath and Westmeath. Around 1545, the Byrnes and O'Tooles, both native septs who had constantly been a pain for the English administration of the Pale, petitioned the Lord Deputy of Ireland to turn their district into its own county, Wicklow, however this was ignored.

During the reigns of the last two Tudor monarchs, Mary I (1553–58) and Elizabeth I (1568–1603), the majority of the work for the foundation of the modern counties was carried out under the auspices of three Lord Deputies: Thomas Radclyffe, 3rd Earl of Sussex, Sir Henry Sydney, and Sir John Perrot.

Mary's reign saw the first addition of actual new counties since the reign of King John. Radclyffe had conquered the districts of Glenmaliry, Irry, Leix, Offaly, and Slewmargy from the O'Moores and O'Connors, and in 1556 a statute decreed that Offaly and part of Glenmaliry would be made into the county of King's County, whilst the rest of Glenmarliry along with Irry, Leix and Slewmargy was formed into Queen's County. Radclyffe brought forth legislation to shire all land as yet unshired throughout Ireland and sought to divide the island into six parts—Connaught, Leinster, Meath, Nether Munster, Ulster, and Upper Munster. However, his administrative reign in Ireland was cut short, and it was not until the reign of Mary's successor, Elizabeth, that this legislation was re-adopted. Under Elizabeth, Radclyffe was brought back to implement it.

Sydney during his three tenures as Lord Deputy created two presidencies to administer Connaught and Munster. He shired Connaught into the counties of Galway, Mayo, Roscommon, and Sligo. In 1565 the territory of the O'Rourkes within Roscommon was made into the county of Leitrim. In an attempt to reduce the importance of the province of Munster, Sydney, using the River Shannon as a natural boundary took the former kingdom of Thomond (North Munster) and made it into the county of Clare as part of the presidency of Connaught in 1569. A commission headed by Perrot and others in 1571 declared that the territory of Desmond in Munster was to be made a county of itself, and it had its own sheriff appointed, however in 1606 it was merged with the county of Kerry. In 1575 Sydney made an expedition to Ulster to plan its shiring. However, nothing came to bear.

In 1578 the go-ahead was given for turning the districts of the Byrnes and O'Tooles into the county of Wicklow. However, with the outbreak of war in Munster and then Ulster, they resumed their independence. Sydney also sought to split Wexford into two smaller counties, the northern half of which was to be called Ferns, but the matter was dropped as it was considered impossible to properly administer. The territory of the O'Farrells of Annaly, however, which was in Westmeath, in 1583 was formed into the county of Longford and transferred to Connaught. The Desmond rebellion (1579–83) that was taking place in Munster stopped Sydney's work and by the time it had been defeated Sir John Perrot was now Lord Deputy, being appointed in 1584.

Perrot would be most remembered for shiring the only province of Ireland that remained effectively outside of English control, that of Ulster. Prior to his tenancy the only proper county in Ulster was Louth, which had been part of the Pale. There were two other long recognised entities north of Louth—Antrim and Down—that had at one time been "counties" of the Earldom of Ulster and were regarded as apart from the unreformed parts of the province. The date Antrim and Down became constituted is unknown. Perrot was recalled in 1588 and the shiring of Ulster would for two decades basically exist on paper as the territory affected remained firmly outside of English control until the defeat of Hugh O'Neill, Earl of Tyrone in the Nine Years' War. These counties were: Armagh, Cavan, Coleraine, Donegal, Fermanagh, Monaghan, and Tyrone. Cavan was formed from the territory of the O'Reilly's of East Breifne in 1584 and had been transferred from Connaught to Ulster. After O'Neill and his allies fled Ireland in 1607 in the Flight of the Earls, their lands became escheated to the Crown and the county divisions designed by Perrot were used as the basis for the grants of the subsequent Plantation of Ulster effected by King James I, which officially started in 1609.

Around 1600 near the end of Elizabeth's reign, Clare was made an entirely distinct presidency of its own under the Earls of Thomond and would not return to being part of Munster until after the Restoration in 1660.

It was not until the subjugation of the Byrnes and O'Tooles by Lord Deputy Sir Arthur Chichester that in 1606 Wicklow was finally shired. This county was one of the last to be created, yet was the closest to the center of English power in Ireland.

County Londonderry was incorporated in 1613 by the merger of County Coleraine with the barony of Loughinsholin (in County Tyrone), the North West Liberties of Londonderry (in County Donegal), and the North East Liberties of Coleraine (in County Antrim).

Throughout the Elizabethan era and the reign of her successor James I, the exact boundaries of the provinces and the counties they consisted of remained uncertain. In 1598 Meath is considered a province in Hayne's "Description of Ireland", and included the counties of Cavan, East Meath, Longford, and Westmeath. This contrasts to George Carew's 1602 survey where there were only four provinces with Longford part of Connaught and Cavan not mentioned at all with only three counties mentioned for Ulster. During Perrot's tenure as Lord President of Munster before he became Lord Deputy, Munster contained as many as eight counties rather than the six it later consisted of. These eight counties were: the five English counties of Cork, Limerick, Kerry, Tipperary, and Waterford; and the three Irish counties of Desmond, Ormond, and Thomond.

Perrot's divisions in Ulster were for the main confirmed by a series of inquisitions between 1606 and 1610 that settled the demarcation of the counties of Connaught and Ulster. John Speed's "Description of the Kingdom of Ireland" in 1610 showed that there was still a vagueness over what counties constituted the provinces, however Meath was no longer reckoned a province. By 1616 when the Attorney General for Ireland Sir John Davies departed Ireland, almost all counties had been delimited. The only exception was the county of Tipperary, which still belonged to the palatinate of Ormond.

Tipperary would remain an anomaly being in effect two counties, one palatine, the other of the Cross until 1715 during the reign of King George I where an act abolished the "royalties and liberties of the County of Tipperary" and "that whatsoever hath been denominated or called Tipperary or Cross Tipperary, shall henceforth be and remain one county for ever, under the name of the County of Tipperary."

To correspond with the subdivisions of the English shires into honours or baronies, Irish counties were granted out to the Anglo-Norman noblemen in cantreds, later known as baronies, which in turn were subdivided, as in England, into parishes. Parishes were composed of townlands. However, in many cases, these divisions correspond to earlier, pre-Norman, divisions. While there are 331 baronies in Ireland, and more than a thousand civil parishes, there are around sixty thousand townlands that range in size from one to several thousand hectares. Townlands were often traditionally divided into smaller units called "quarters", but these subdivisions are not legally defined.

The following towns/cities had charters specifically granting them the status of a county corporate:
The only entirely new counties created in 1898 were the county boroughs of Londonderry and Belfast. Carrickfergus, Drogheda and Kilkenny were abolished; Galway was also abolished, but recreated in 1986.

Regional presidencies of Connacht and Munster remained in existence until 1672, with special powers over their subsidiary counties. Tipperary remained a county palatine until the passing of the County Palatine of Tipperary Act 1715, with different officials and procedures from other counties. At the same time, Dublin, until the 19th century, had ecclesiastical liberties with rules outside those applying to the rest of Dublin city and county. Exclaves of the county of Dublin existed in counties Kildare and Wicklow. At least eight other enclaves of one county inside another, or between two others, existed. The various enclaves and exclaves were merged into neighbouring and surrounding counties, primarily in the mid-19th century under a series of Orders in Council.

The Church of Ireland exercised functions at the level of civil parish that would later be exercised by county authorities. Vestigial feudal power structures of major old estates remained well into the 18th century. Urban corporations operated individual royal charters. Management of counties came to be exercised by grand juries. Members of grand juries were the local payers of rates who historically held judicial functions, taking maintenance roles in regard to roads and bridges, and the collection of "county cess" taxes. They were usually composed of wealthy "country gentlemen" (i.e. landowners, farmers and merchants):A country gentleman as a member of a Grand Jury...levied the local taxes, appointed the nephews of his old friends to collect them, and spent them when they were gathered in. He controlled the boards of guardians and appointed the dispensary doctors, regulated the diet of paupers, inflicted fines and administered the law at petty sessions. The counties were initially used for judicial purposes, but began to take on some governmental functions in the 17th century, notably with grand juries.

In 1836, the use of counties as local government units was further developed, with grand-jury powers extended under the Grand Jury (Ireland) Act 1836. The traditional county of Tipperary was split into two judicial counties (or ridings) following the establishment of assize courts in 1838. Also in that year, local poor law boards, with a mix of magistrates and elected "guardians" took over the health and social welfare functions of the grand juries.

Sixty years later, a more radical reorganisation of local government took place with the passage of the Local Government (Ireland) Act 1898. This Act established a county council for each of the thirty-three Irish administrative counties. Elected county councils took over the powers of the grand juries. The boundaries of the traditional counties changed on a number of occasions. The 1898 Act changed the boundaries of Counties Galway, Clare, Mayo, Roscommon, Sligo, Waterford, Kilkenny, Meath and Louth, and others. County Tipperary was divided into two regions: North Riding and South Riding. Areas of the cities of Belfast, Cork, Dublin, Limerick, Derry and Waterford were carved from their surrounding counties to become county boroughs in their own right and given powers equivalent to those of administrative counties.

Under the Government of Ireland Act 1920, the island was partitioned between Southern Ireland and Northern Ireland. For the purposes of the Act, ... Northern Ireland shall consist of the parliamentary counties of Antrim, Armagh, Down, Fermanagh, Londonderry and Tyrone, and the parliamentary boroughs of Belfast and Londonderry, and Southern Ireland shall consist of so much of Ireland as is not comprised within the said parliamentary counties and boroughs.

The county and county borough borders were thus used to determine the line of partition. Southern Ireland shortly afterwards became the Irish Free State. This partition was entrenched in the Anglo-Irish Treaty, which was ratified in 1922, by which the Irish Free State left the United Kingdom with Northern Ireland making the decision to not separate two days later.

Under the Local Government Provisional Order Confirmation Act 1976, part of the urban area of Drogheda, which lay in County Meath, was transferred to County Louth on 1 January 1977. This resulted in the land area of County Louth increasing slightly at the expense of County Meath. The possibility of a similar action with regard to Waterford City has been raised in recent years, though opposition from Kilkenny has been strong.

Areas that were shired by 1607 and continued as counties until the local government reforms of 1836, 1898 and 2001 are sometimes referred to as "traditional" or "historic" counties. These were distinct from the counties corporate that existed in some of the larger towns and cities, although linked to the county at large for other purposes. From 1898 to 2001, areas with county councils were known as administrative counties, while the counties corporate were designated as county boroughs. In other cases, the "traditional" county was divided to form two administrative counties. From 2001, certain administrative counties, which were originally "traditional" counties, underwent further splitting.

In the Republic of Ireland the traditional counties are, in general, the basis for local government, planning and community development purposes, are governed by county councils and are still generally respected for other purposes. Administrative borders have been altered to allocate various towns exclusively into one county having been originally split between two counties.

There are now 26 county councils, three city councils, and two city and county councils – a total of 31 local government areas.

County Tipperary was split into North and South Ridings in 1838. These Ridings were established as separate administrative counties under the Local Government (Ireland) Act 1898. The Local Government Reform Act 2014 abolished North Tipperary and South Tipperary, and re-established County Tipperary.

County Dublin was abolished as an administrative county in 1994, while also remaining a point of reference for purposes other than local government. Its territory was divided into three administrative counties: Dún Laoghaire–Rathdown, Fingal, and South Dublin. The county borough of Dublin, together with the county boroughs of Cork, Galway, Limerick and Waterford, were re-styled as city councils under the Local Government Act 2001, with the same status in law as county councils.

The city councils of Limerick and Waterford were merged with their respective county councils by the Local Government Reform Act 2014, to form new city and county councils. The city of Kilkenny does not have a "city council" as it was a borough but not a county borough. It is now administered by its eponymous county council but is, exceptionally, permitted to retain the style of "city" for ornament only.

Of the administrative structures established under the 1898 Act, the only type to have been completely abolished was the Rural District, which was rendered void in the early years of the Irish Free State amidst widespread allegations of corruption. At a level above that of LAU is the Region which clusters counties together for NUTS purposes. The Regions are administered by Regional Authorities which were established by the Local Government Act 1991 and came into existence in 1994.

In 2013 Education and Training Boards (ETBs) were formed throughout the Republic of Ireland, replacing the system of Vocational Education Committees (VECs) created in 1930. Originally, VECs were formed for each administrative county and county borough, and also in a number of larger towns. In 1997 the majority of town VECs were absorbed by the surrounding county. The 33 VEC areas were reduced to 16 ETB areas, with each consisting of one or more local government county or city.

The Institute of technology system was organised on the committee areas or "functional areas", these still remain legal but are not as important as originally envisioned as the institutes are now more national in character and are only really applied today when selecting governing councils, similarly Dublin Institute of Technology was originally a group of several colleges of the City of Dublin committee.

Where possible, Dáil constituencies follow county boundaries. Under the Electoral Act 1997, a Constituency Commission is established following the publication of census figures every five years. The Commission is charged with defining constituency boundaries, and the 1997 Act provides that "the breaching of county boundaries shall be avoided as far as practicable". This provision does not apply to the boundaries between cities and counties, or between the three counties in the Dublin area.

This system usually results in more populated counties having several constituencies: Dublin, including Dublin city, is subdivided into twelve constituencies, Cork into five. On the other hand, smaller counties such as Carlow and Kilkenny or Laois and Offaly may be paired to form constituencies. An extreme case is the splitting of Ireland's least populated county of Leitrim between the constituencies of Sligo–North Leitrim and Roscommon–South Leitrim.

Each county or city is divided into local electoral areas for the election of councillors. The boundaries of the areas and the number of councillors assigned are fixed from time to time by order of the Minister for Housing, Planning and Local Government, following a report by the Local Government Commission, and based on population changes recorded in the census.

In Northern Ireland, a major reorganisation of local government in 1973 replaced the six traditional counties and two county boroughs (Belfast and Derry) with 26 single-tier districts for local government purposes. In 2015, as a result of a reform process that started in 2005, these districts were merged to form 11 new single-tier "super districts".

The six traditional counties remain in use for some purposes, including the three-letter coding of vehicle number plates, the Royal Mail Postcode Address File (which records counties in all addresses although they are no longer required for postcoded mail) and Lord Lieutenancies (for which the former county boroughs are also used). There are no longer official 'county towns'. However the counties are still very widely acknowledged, for example as administrative divisions for sporting and cultural organisations.

The administrative division of the island along the lines of the traditional 32 counties was also adopted by non-governmental and cultural organisations. In particular the Gaelic Athletic Association continues to organise its activities on the basis of GAA counties that, throughout the island, correspond almost exactly to the 32 traditional counties in use at the time of the foundation of that organisation in 1884. The GAA also uses the term "county" for some of its organisational units in Britain and further afield.

The 35 divisions listed below include the ‘traditional’ counties of Ireland as well as those created or re-created after the 19th century. Twenty four counties still delimit the remit of local government divisions in the Republic of Ireland (in some cases with slightly redrawn boundaries). In Northern Ireland, the counties listed no longer serve this purpose. The Irish-language names of counties in the Republic of Ireland are prescribed by ministerial order, which in the case of three newer counties, omits the word "contae" (county). Irish names form the basis for all English-language county names except Waterford, Wexford, and Wicklow, which are of Norse origin.

In the ’Region’ column of the table below, except for the six Northern Ireland counties the reference is to NUTS 3 statistical regions of the Republic of Ireland. ’County town’ is the current or former administrative capital of the county.

Cities which, in the Republic, are currently administered outside the county system, but with the same legal status as administrative counties, are not shown separately: these are Cork, Dublin and, Galway. Also omitted are the former county boroughs of Londonderry and Belfast which in Northern Ireland had the same legal status as the six counties until the reorganisation of local government in 1973. County Dublin, which was officially abolished in 1994, is included, as are the three new administrative counties which took over the functions of the defunct Dublin County Council.
‡ Also administrative




</doc>
<doc id="15034" url="https://en.wikipedia.org/wiki?curid=15034" title="Information Sciences Institute">
Information Sciences Institute

The USC Information Sciences Institute (ISI) is a component of the University of Southern California (USC) Viterbi School of Engineering, and specializes in research and development in information processing, computing, and communications technologies. It is located in Marina del Rey, California.

ISI actively participated in the information revolution, and it played a leading role in developing and managing the early Internet and its predecessor ARPAnet. The Institute conducts basic and applied research supported by more than 20 U.S. government agencies involved in defense, science, health, homeland security, energy and other areas. Annual funding is about $100 million.

ISI employs about 350 research scientists, research programmers, graduate students and administrative staff at its Marina del Rey, California headquarters and in Arlington, Virginia. About half of the research staff hold PhD degrees, and about 40 are research faculty who teach at USC and advise graduate students. Several senior researchers are tenured USC faculty in the Viterbi School.

ISI research spans artificial intelligence (AI), cybersecurity, grid computing, cloud computing, quantum computing, microelectronics, supercomputing, nano-satellites and many other areas. AI expertise includes natural language processing, in which ISI has an international reputation, reconfigurable robotics, information integration, motion analysis and social media analysis. Hardware/software expertise includes cyber-physical system security, data mining, reconfigurable computing and cloud computing. In networking, ISI explores Internet resilience, Internet traffic analysis and photonics, among other areas. Researchers also work in scientific data management, wireless technologies, biomimetics and electrical smart grid, in which ISI is advising the Los Angeles Department of Water and Power on a major demonstration project. Another current initiative involves big data brain imaging jointly with the Keck School of Medicine of USC.

Federal agency sponsors include the Air Force Office of Scientific Research, Department of Defense Advanced Research Projects Agency, Department of Education, Department of Energy, Department of Homeland Security, National Institutes of Health, National Science Foundation, and other scientific, technical, and defense-related agencies.

Corporate partners include Chevron Corp. in the Center for Interactive Smart Oilfield Technologies (CiSoft), Lockheed Martin Company in the USC-Lockheed Martin Quantum Computing Center, and Parsons Corp. subsidiary Sparta Inc. in the DETER Project, a cybersecurity research initiative and international testbed. ISI also has partnered with businesses including IBM Corporation, Samsung Electronics Company, the Raytheon Company, GlobalFoundries Inc., Northrop Grumman Corporation and Carl Zeiss AG, and currently is working with Micron Technology, Inc., Altera Corporation and Fujitsu Ltd.

ISI also operates the Metal Oxide Semiconductor Implementation Service (MOSIS), a multi-project electronic circuit wafer service that has prototyped more than 60,000 chips since 1981. MOSIS provides design tools and pools circuit designs to produce specialty and low-volume chips for corporations, universities and other research entities worldwide. The Institute also has given rise to several startup and spinoff companies in grid software, geospatial information fusion, machine translation, data integration and other technologies.

ISI was founded by Keith Uncapher, who headed the computer research group at RAND Corporation in the 1960s and early 1970s. Uncapher decided to leave RAND after his group's funding was cut in 1971. He approached the University of California at Los Angeles about creating an off-campus technology institute, but was told that a decision would take 15 months. He then presented the concept to USC, which approved the proposal in five days. ISI was launched with three employees in 1972. Its first proposal was funded by the Defense Advanced Research Projects Agency (DARPA) in 30 days for $6 million.

ISI became one of the earliest nodes on ARPANET, the predecessor to the Internet, and in 1977 figured prominently in a demonstration of its international viability. ISI also helped refine the TCP/IP communications protocols fundamental to Net operations, and researcher Paul Mockapetris developed the now-familiar Domain Name System characterized by .com, .org, .net, .gov, and .edu on which the Net still operates. (The names .com, .org et al. were invented at SRI International, an ongoing collaborator.) Steve Crocker originated the Request for Comments (RFC) series, the written record of the network's technical structure and operation that both documented and shaped the emerging Internet. Another ISI researcher, Danny Cohen, became first to implement packet voice and packet video over ARPANET, demonstrating the viability of packet switching for real-time applications.

Jonathan Postel collaborated in development of TCP/IP, DNS and the SMTP protocol that supports email. He also edited the RFC for nearly three decades until his sudden death in 1998, when ISI colleagues assumed responsibility. The Institute retained that role until 2009. Postel simultaneously directed the Internet Assigned Numbers Authority (IANA) and its predecessor, which assign Internet addresses. IANA was administered from ISI until a nonprofit organization, ICANN, was created for that purpose in 1998.

Some of the first Net security applications, and one of the world's first portable computers, also originated at ISI.

ISI researchers also created or co-created the:

In 2011, several ISI natural language experts advised the IBM team that created Watson, the computer that became the first machine to win against human competitors on the "Jeopardy!" TV show. In 2012, ISI's Kevin Knight spearheaded a successful drive to crack the Copiale cipher, a lengthy encrypted manuscript that had remained unreadable for 250 years. Also in 2012, the USC-Lockheed Martin Quantum Computing Center (QCC) became the first organization to operate a quantum annealing system outside of its manufacturer, D-Wave Systems, Inc. USC, ISI and Lockheed Martin now are performing basic and applied research into quantum computing. A second quantum annealing system is located at NASA Ames Research Center, and is operated jointly by NASA and Google.

The USC Andrew and Erna Viterbi School of Engineering was ranked among the nation's top 10 engineering graduate schools by "US News & World Report" in 2015. Including ISI, USC is ranked first nationally in federal computer science research and development expenditures.

ISI is organized into six divisions focused on differing areas of research expertise:

Smaller, specialized research groups operate within almost all divisions.

ISI is led by Executive Director Prem Natarajan, previously an executive vice president and principal scientist at Raytheon BBN Technologies. He is a natural language specialist with research interests that focus on optical character recognition, speech processing, and multimedia analysis. Natarajan joined ISI in 2013, succeeding USC Viterbi School vice dean John O'Brien, who served as interim executive director in 2012 and 2013. From 1988 to 2012, ISI was led by former IBM executive Herbert Schorr.



</doc>
<doc id="15036" url="https://en.wikipedia.org/wiki?curid=15036" title="Information security">
Information security

Information security, sometimes shortened to infosec, is the practice of protecting information by mitigating information risks. It is part of information risk management. It typically involves preventing or at least reducing the probability of unauthorized/inappropriate access to data, or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g. electronic or physical, tangible (e.g. paperwork) or intangible (e.g. knowledge). Information security's primary focus is the balanced protection of the confidentiality, integrity and availability of data (also known as the CIA triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a structured risk management process that involves: 

To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on password, antivirus software, firewall, encryption software, legal liability, security awareness and training, and so forth. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred and destroyed. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement isn't adopted.

Various definitions of information security are suggested below, summarized from different sources:


At the core of information security is information assurance, the act of maintaining the confidentiality, integrity and availability (CIA) of information, ensuring that information is not compromised in any way when critical issues arise. These issues include but are not limited to natural disasters, computer/server malfunction, and physical theft. While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized, with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). It is worthwhile to note that a computer does not necessarily mean a home desktop. A computer is any device with a processor and some memory. Such devices can range from non-networked standalone devices as simple as calculators, to networked mobile computing devices such as smartphones and tablet computers. IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses. They are responsible for keeping all of the technology within the company secure from malicious cyber attacks that often attempt to acquire critical private information or gain control of the internal systems.

The field of information security has grown and evolved significantly in recent years. It offers many areas for specialization, including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics. Information security professionals are very stable in their employment. more than 80 percent of professionals had no change in employer or employment over a period of a year, and the number of professionals is projected to continuously grow more than 11 percent annually from 2014 to 2019.

Information security threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, identity theft, theft of equipment or information, sabotage, and information extortion. Most people have experienced software attacks of some sort. Viruses, worms, phishing attacks and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses in the information technology (IT) field. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information through social engineering. Theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile, are prone to theft and have also become far more desirable as the amount of data capacity increases. Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers. Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware. There are many ways to help protect yourself from some of these attacks but one of the most functional precautions is conduct periodical user awareness. The number one threat to any organisation are users or internal employees, they are also called insider threats.

Governments, military, corporations, financial institutions, hospitals, non-profit organisations and private businesses amass a great deal of confidential information about their employees, customers, products, research and financial status. Should confidential information about a business' customers or finances or new product line fall into the hands of a competitor or a black hat hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.

For the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.

Possible responses to a security threat or risk are:

Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands. However, for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653).

In the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. Section 1 of the law concerned espionage and unlawful disclosures of information, while Section 2 dealt with breaches of official trust. A public interest defense was soon added to defend disclosures in the interest of the state. A similar law was passed in India in 1889, The Indian Official Secrets Act, which was associated with the British colonial era and used to crackdown on newspapers that opposed the Raj’s policies. A newer version was passed in 1923 that extended to all matters of confidential or secret information for governance.

By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than enlisted troops) and where they should be stored as increasingly complex safes and storage facilities were developed. The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information. Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570).

The end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user. The establishment of Transfer Control Protocol/Internetwork Protocol (TCP/IP) in the early 1980s enabled different types of computers to communicate. These computers quickly became interconnected through the internet.

The rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.

The CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad—confidentiality, integrity and availability—are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) However, debate continues about whether or not this CIA triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy. Other principles such as "accountability" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.

The triad seems to have first been mentioned in a NIST publication in 1977.

In 1992 and revised in 2002, the OECD's "Guidelines for the Security of Information Systems and Networks" proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment. Building upon those, in 2004 the NIST's "Engineering Principles for Information Technology Security" proposed 33 principles. From each of these derived guidelines and practices.

In 1998, Donn Parker proposed an alternative model for the classic CIA triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.

In 2011, The Open Group published the information security management standard O-ISM3. This standard proposed an operational definition of the key concepts of security, with elements called "security objectives", related to access control (9), availability (3), data quality (1), compliance and technical (4). In 2009, DoD Software Protection Initiative released the Three Tenets of Cybersecurity which are System Susceptibility, Access to the Flaw, and Capability to Exploit the Flaw. Neither of these models are widely adopted.

In information security, confidentiality "is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes." While similar to "privacy," the two words aren't interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers. Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.

In information security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically provide message integrity alongside confidentiality.

For any information system to serve its purpose, the information must be available when it is needed. This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.

In the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program. Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect. This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails. Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response and policy/change management. A successful information security team involves many different key roles to mesh and align for the CIA triad to be provided effectively.

In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.

It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity). The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).

Broadly speaking, risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm. The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property). 

The "Certified Information Systems Auditor (CISA) Review Manual 2006" defines risk management as "the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization."

There are two things in this definition that may need some clarification. First, the "process" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected. Furthermore, these processes have limitations as security breaches are generally rare and emerge in a specific context which may not be easily duplicated. Thus, any process and countermeasure should itself be evaluated for vulnerabilities. It is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called "residual risk."

A risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.

Research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:

In broad terms, the risk management process consists of:

For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.

Selecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas. Organizations can implement additional controls according to requirement of the organization. ISO/IEC 27002 offers a guideline for organizational information security standards.

Administrative controls consist of approved written policies, procedures, standards and guidelines. Administrative controls form the framework for running the business and managing people. They inform people on how the business is to be run and how day-to-day operations are to be conducted. Laws and regulations created by government bodies are also a type of administrative control because they inform the business. Some industry sectors have policies, procedures, standards and guidelines that must be followed – the Payment Card Industry Data Security Standard (PCI DSS) required by Visa and MasterCard is such an example. Other examples of administrative controls include the corporate security policy, password policy, hiring policies, and disciplinary policies.

Administrative controls form the basis for the selection and implementation of logical and physical controls. Logical and physical controls are manifestations of administrative controls, which are of paramount importance.

Logical controls (also called technical controls) use software and data to monitor and control access to information and computing systems. Passwords, network and host-based firewalls, network intrusion detection systems, access control lists, and data encryption are examples of logical controls.

An important logical control that is frequently overlooked is the principle of least privilege, which requires that an individual, program or system process not be granted any more access privileges than are necessary to perform the task. A blatant example of the failure to adhere to the principle of least privilege is logging into Windows as user Administrator to read email and surf the web. Violations of this principle can also occur when an individual collects additional access privileges over time. This happens when employees' job duties change, employees are promoted to a new position, or employees are transferred to another department. The access privileges required by their new duties are frequently added onto their already existing access privileges, which may no longer be necessary or appropriate.

Physical controls monitor and control the environment of the work place and computing facilities. They also monitor and control access to and from such facilities and include doors, locks, heating and air conditioning, smoke and fire alarms, fire suppression systems, cameras, barricades, fencing, security guards, cable locks, etc. Separating the network and workplace into functional areas are also physical controls.

An important physical control that is frequently overlooked is separation of duties, which ensures that an individual can not complete a critical task by himself. For example, an employee who submits a request for reimbursement should not also be able to authorize payment or print the check. An applications programmer should not also be the server administrator or the database administrator; these roles and responsibilities must be separated from one another.

Information security must protect information throughout its lifespan, from the initial creation of the information on through to the final disposal of the information. The information must be protected while in motion and while at rest. During its lifetime, information may pass through many different information processing systems and through many different parts of information processing systems. There are many different ways the information and information systems can be threatened. To fully protect the information during its lifetime, each component of the information processing system must have its own protection mechanisms. The building up, layering on and overlapping of security measures is called "defense in depth." In contrast to a metal chain, which is famously only as strong as its weakest link, the defense in depth strategy aims at a structure where, should one defensive measure fail, other measures will continue to provide protection.

Recall the earlier discussion about administrative controls, logical controls, and physical controls. The three types of controls can be used to form the basis upon which to build a defense in depth strategy. With this approach, defense in depth can be conceptualized as three distinct layers or planes laid one on top of the other. Additional insight into defense in depth can be gained by thinking of it as forming the layers of an onion, with data at the core of the onion, people the next outer layer of the onion, and network security, host-based security and application security forming the outermost layers of the onion. Both perspectives are equally valid, and each provides valuable insight into the implementation of a good defense in depth strategy.

An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification. The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.

Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information. The Information Systems Audit and Control Association (ISACA) and its "Business Model for Information Security" also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.

The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:

All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.

Access to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.

Access control is generally considered in three steps: identification, authentication, and authorization.

Identification is an assertion of who someone is or what something is. If a person makes the statement "Hello, my name is John Doe" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming "I am the person the username belongs to".

Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.

There are three different types of information that can be used for authentication:

Strong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose, but they are increasingly inadequate. Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as Time-based One-time Password algorithms.

After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.

The non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.

Examples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems; Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.

To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.

Also, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions. This principle is used in the government when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privilege to prevent employees from accessing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.

Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.

Cryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU‑T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and email.

Cryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.

The terms "reasonable and prudent person," "due care" and "due diligence" have been used in the fields of finance, securities, and law for many years. In recent years these terms have found their way into the fields of computing and information security. U.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.

In the business world, stockholders, customers, business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements. This is often described as the "reasonable and prudent person" rule. A prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in a legal, ethical manner. A prudent person is also diligent (mindful, attentive, ongoing) in their due care of the business.

In the field of information security, Harris
offers the following definitions of due care and due diligence:

""Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees."" And, <nowiki>[Due diligence are the]</nowiki> ""continual activities that make sure the protection mechanisms are continually maintained and operational.""
Attention should be made to two important points in these definitions. First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.

Organizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA) provides principles and practices for evaluating risk. It considers all parties that could be affected by those risks. DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden. With increased data breach litigation, companies must balance security controls, compliance, and its mission.

The Software Engineering Institute at Carnegie Mellon University, in a publication titled "Governing for Enterprise Security (GES) Implementation Guide", defines characteristics of effective security governance. These include:

An incident response plan is a group of policies that dictate an organizations reaction to a cyber attack. Once an security breach has been identified the plan is initiated. It is important to note that there can be legal implications to a data breach. Knowing local and federal laws is critical. Every plan is unique to the needs of the organization, and it can involve skill set that are not part of an IT team. For example, a lawyer may be included in the response plan to help navigate legal implications to a data breach.

As mentioned above every plan is unique but most plans will include the following:

Good preparation includes the development of an Incident Response Team (IRT). Skills need to be used by this team would be, penetration testing, computer forensics, network security, etc. This team should also keep track of trends in cybersecurity and modern attack strategies. A training program for end users is important as well as most modern attack strategies target users on the network.

This part of the incident response plan identifies if there was a security event. When an end user reports information or an admin notices irregularities, an investigation is launched. An incident log is a crucial part of this step. All of the members of the team should be updating this log to ensure that information flows as fast as possible. If it has been identified that a security breach has occurred the next step should be activated.

In this phase, the IRT works to isolate the areas that the breach took place to limit the scope of the security event. During this phase it is important to preserve information forensically so it can be analyzed later in the process. Containment could be as simple as physically containing a server room or as complex as segmenting a network to not allow the spread of a virus.

This is where the threat that was identified is removed from the affected systems. This could include using deleting malicious files, terminating compromised accounts, or deleting other components. Some events do not require this step, however it is important to fully understand the event before moving to this step. This will help to ensure that the threat is completely removed.

This stage is where the systems are restored back to original operation. This stage could include the recovery of data, changing user access information, or updating firewall rules or policies to prevent a breach in the future. Without executing this step, the system could still be vulnerable to future security threats.

In this step information that has been gathered during this process is used to make future decisions on security. This step is crucial to the ensure that future events are prevented. Using this information to further train admins is critical to the process. This step can also be used to process information that is distributed from other entities who have experienced a security event.

Change management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.

Any change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.

Not every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.

Change management is usually overseen by a change review board composed of representatives from key business areas, security, networking, systems administrators, database administration, application developers, desktop support and the help desk. The tasks of the change review board can be facilitated with the use of automated work flow application. The responsibility of the change review board is to ensure the organization's documented change management procedures are followed. The change management process is as follows


Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation and communication.

ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and Information Technology Infrastructure Library all provide valuable guidance on implementing an efficient and effective change management program information security.

Business continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects. BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual. The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.

It encompasses:

Whereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster. A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.

Below is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security.


Describing more than simply how security aware employees are, information security culture is the ideas, customs, and social behaviors of an organization that impact information security in both positive and negative ways. Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. The way employees think and feel about security and the actions they take can have a big impact on information security in organizations. Roer & Petric (2017) identify seven core dimensions of information security culture in organizations:


Andersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security "effort" and often take actions that ignore organizational information security best interests. Research shows information security culture needs to be improved continuously. In "Information Security Culture from Analysis to Change", authors commented, "It's a never ending process, a cycle of evaluation and change or maintenance." To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.


The International Organization for Standardization (ISO) is a consortium of national standards institutes from 157 countries, coordinated through a secretariat in Geneva, Switzerland. ISO is the world's largest developer of standards. ISO 15443: "Information technology – Security techniques – A framework for IT security assurance", ISO/IEC 27002: "Information technology – Security techniques – Code of practice for information security management", ISO-20000: "Information technology – Service management", and ISO/IEC 27001: "Information technology – Security techniques – Information security management systems – Requirements" are of particular interest to information security professionals.

The US National Institute of Standards and Technology (NIST) is a non-regulatory federal agency within the U.S. Department of Commerce. The NIST Computer Security Division
develops standards, metrics, tests and validation programs as well as publishes standards and guidelines to increase secure IT planning, implementation, management and operation. NIST is also the custodian of the U.S. Federal Information Processing Standard publications (FIPS).

The Internet Society is a professional membership society with more than 100 organizations and over 20,000 individual members in over 180 countries. It provides leadership in addressing issues that confront the future of the internet, and it is the organizational home for the groups responsible for internet infrastructure standards, including the Internet Engineering Task Force (IETF) and the Internet Architecture Board (IAB). The ISOC hosts the Requests for Comments (RFCs) which includes the Official Internet Protocol Standards and the RFC-2196 Site Security Handbook.

The Information Security Forum (ISF) is a global nonprofit organization of several hundred leading organizations in financial services, manufacturing, telecommunications, consumer goods, government, and other areas. It undertakes research into information security practices and offers advice in its biannual Standard of Good Practice and more detailed advisories for members.

The Institute of Information Security Professionals (IISP) is an independent, non-profit body governed by its members, with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as a whole. The institute developed the IISP Skills Framework. This framework describes the range of competencies expected of information security and information assurance professionals in the effective performance of their roles. It was developed through collaboration between both private and public sector organizations and world-renowned academics and security leaders.

The German Federal Office for Information Security (in German "Bundesamt für Sicherheit in der Informationstechnik (BSI)") BSI-Standards 100-1 to 100-4 are a set of recommendations including "methods, processes, procedures, approaches and measures relating to information security". The BSI-Standard 100-2 "IT-Grundschutz Methodology" describes how information security management can be implemented and operated. The standard includes a very specific guide, the IT Baseline Protection Catalogs (also known as IT-Grundschutz Catalogs). Before 2005, the catalogs were formerly known as "IT Baseline Protection Manual". The Catalogs are a collection of documents useful for detecting and combating security-relevant weak points in the IT environment (IT cluster). The collection encompasses as of September 2013 over 4,400 pages with the introduction and catalogs. The IT-Grundschutz approach is aligned with to the ISO/IEC 2700x family.

The European Telecommunications Standards Institute standardized a catalog of information security indicators, headed by the Industrial Specification Group (ISG) ISI.





</doc>
<doc id="15037" url="https://en.wikipedia.org/wiki?curid=15037" title="Income">
Income

Income is the consumption and saving opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms. 

For households and individuals, "income is the sum of all the wages, salaries, profits, interest payments, rents, and other forms of earnings received in a given period of time." (also known as gross income). Net income is defined as the gross income minus taxes and other deductions (e.g., mandatory pension contributions), and is usually the basis to calculate how much income tax is owed. 

In the field of public economics, the concept may comprise the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income.

For a firm, gross income can be defined as sum of all revenue minus the cost of goods sold. Net income nets out expenses: net income equals revenue minus cost of goods sold, expenses, depreciation, interest, and taxes.

In economics, "factor income" is the return accruing for a person, or a nation, derived from the "factors of production": rental income, wages generated by labor, the interest created by capital, and profits from entrepreneurial ventures.

In consumer theory 'income' is another name for the "budget constraint," an amount formula_1 to be spent on different goods x and y in quantities formula_2 and formula_3 at prices formula_4 and formula_5. The basic equation for this is
This equation implies two things. First buying one more unit of good x implies buying formula_7 less units of good y. So, formula_7 is the "relative" price of a unit of x as to the number of units given up in y. Second, if the price of x falls for a fixed formula_1 and fixed formula_10 then its relative price falls. The usual hypothesis, the law of demand, is that the quantity demanded of x would increase at the lower price. The analysis can be generalized to more than two goods.

The theoretical generalization to more than one period is a multi-period wealth and income constraint. For example, the same person can gain more productive skills or acquire more productive income-earning assets to earn a higher income. In the multi-period case, something might also happen to the economy beyond the control of the individual to reduce (or increase) the flow of income. Changing measured income and its relation to consumption over time might be modeled accordingly, such as in the permanent income hypothesis.

"Full income" refers to the accumulation of both the monetary and the non-monetary consumption-ability of any given entity, such as a person or a household. According to what the economist Nicholas Barr describes as the "classical definition of income" (the 1938 Haig–Simons definition): "income may be defined as the... sum of (1) the market value of rights exercised in consumption and (2) the change in the value of the store of property rights..." Since the consumption potential of non-monetary goods, such as leisure, cannot be measured, monetary income may be thought of as a proxy for full income. As such, however, it is criticized for being unreliable, "i.e." failing to accurately reflect affluence (and thus the consumption opportunities) of any given agent. It omits the utility a person may derive from non-monetary income and, on a macroeconomic level, fails to accurately chart social welfare. According to Barr, "in practice money income as a proportion of total income varies widely and unsystematically. Non-observability of full-income prevent a complete characterization of the individual opportunity set, forcing us to use the unreliable yardstick of money income.

Income per capita has been increasing steadily in most countries. Many factors contribute to people having a higher income, including education, globalisation and favorable political circumstances such as economic freedom and peace. Increases in income also tend to lead to people choosing to work fewer hours.
Developed countries (defined as countries with a "developed economy") have higher incomes as opposed to developing countries tending to have lower incomes.

Income inequality is the extent to which income is distributed in an uneven manner. It can be measured by various methods, including the Lorenz curve and the Gini coefficient. Many economists argue that certain amounts of inequality are necessary and desirable but that excessive inequality leads to efficiency problems and social injustice.

National income, measured by statistics such as net national income (NNI), measures the total income of individuals, corporations, and government in the economy. For more information see Measures of national income and output.

Throughout history, many have written about the impact of income on morality and society. Saint Paul wrote 'For the love of money is a root of all kinds of evil:' ( (ASV)).

Some scholars have come to the conclusion that material progress and prosperity, as manifested in continuous income growth at both the individual and the national level, provide the indispensable foundation for sustaining any kind of morality. This argument was explicitly given by Adam Smith in his "Theory of Moral Sentiments", and has more recently been developed by Harvard economist Benjamin Friedman in his book "The Moral Consequences of Economic Growth".

The International Accounting Standards Board (IASB) uses the following definition: "Income is increases in economic benefits during the accounting period in the form of inflows or enhancements of assets or decreases of liabilities that result in increases in equity, other than those relating to contributions from equity participants." [F.70] (IFRS Framework).

According to John Hicks' definitions, income "is the maximum amount which can be spent during a period if there is to be an expectation of maintaining intact, the capital value of prospective receipts (in money terms)”.

John Hicks used "I" for income, but Keynes wrote to him in 1937, ""after trying both, I believe it is easier to use Y for income and I for investment."" Some consider Y as an alternative letter for the phonem I in languages like Spanish, although Y as the "Greek I" was actually pronounced like the modern German ü or the phonetic /y/.




</doc>
<doc id="15039" url="https://en.wikipedia.org/wiki?curid=15039" title="Iona">
Iona

Iona (, sometimes simply "Ì") is a small island in the Inner Hebrides off the Ross of Mull on the western coast of Scotland. It is mainly known for Iona Abbey, though there are other buildings on the island. Iona Abbey was a centre of Gaelic monasticism for three centuries and is today known for its relative tranquility and natural environment. It is a tourist destination and a place for spiritual retreats. Its modern Scottish Gaelic name means "Iona of (Saint) Columba" (formerly anglicised "Icolmkill").

The Hebrides have been occupied by the speakers of several languages since the Iron Age, and as a result many of the names of these islands have more than one possible meaning. Nonetheless few, if any, can have accumulated as many different names over the centuries as the island now known in English as "Iona".

The earliest forms of the name enabled place-name scholar William J. Watson to show that the name originally meant something like "yew-place". The element "Ivo-", denoting "yew", occurs in Ogham inscriptions ("Iva-cattos" [genitive], "Iva-geni" [genitive]) and in Gaulish names ("Ivo-rix", "Ivo-magus") and may form the basis of early Gaelic names like "Eógan" (ogham: "Ivo-genos"). It is possible that the name is related to the mythological figure, "Fer hÍ mac Eogabail", foster-son of Manannan, the forename meaning "man of the yew".

Mac an Tàilleir (2003) lists the more recent Gaelic names of "Ì", "Ì Chaluim Chille" and "Eilean Idhe" noting that the first named is "generally lengthened to avoid confusion" to the second, which means "Calum's (i.e. in latinised form "Columba's") Iona" or "island of Calum's monastery". The confusion results from "ì", despite its original etymology as the name of the island, being confused with the Gaelic noun "ì" "island" (now obsolete) of Old Norse origin ("ey" "island", "Eilean Idhe" means "the isle of Iona", also known as "Ì nam ban bòidheach" ("the isle of beautiful women"). The modern English name comes of yet another variant, "Ioua", which was either just Adomnán's attempt to make the Gaelic name fit Latin grammar or else a genuine derivative from "Ivova" ("yew place"). "Ioua"'s change to "Iona", attested from c.1274, results from a transcription mistake resulting from the similarity of "n" and "u" in Insular Minuscule.

Despite the continuity of forms in Gaelic between the pre-Norse and post-Norse eras, Haswell-Smith (2004) speculates that the name may have a Norse connection, "Hiōe" meaning "island of the den of the brown bear". The medieval English language version was "Icolmkill" (and variants thereof).

Murray (1966) claims that the "ancient" Gaelic name was "Innis nan Druinich" ("the isle of Druidic hermits") and repeats a Gaelic story (which he admits is apocryphal) that as Columba's coracle first drew close to the island one of his companions cried out ""Chì mi i"" meaning "I see her" and that Columba's response was "Henceforth we shall call her Ì".

The geology of Iona is quite complex given the island’s size and quite distinct from that of nearby Mull. About half of the island’s bedrock is Scourian gneiss assigned to the Lewisian complex and dating from the Archaean eon making it some of the oldest rock in Britain and indeed Europe. Closely associated with these gneisses are mylonite and meta-anorthosite and melagabbro. Along the eastern coast facing Mull are steeply dipping Neoproterozoic age metaconglomerates, metasandstones, metamudstones and hornfelsed metasiltstones ascribed to the Iona Group, described traditionally as Torridonian. In the southwest and on parts of the west coast are pelites and semipelites of Archaean to Proterozoic age. There are small outcrops of Silurian age pink granite on southeastern beaches, similar to those of the Ross of Mull pluton cross the sound to the east. Numerous geological faults cross the island, many in a E-W or NW-SE alignment. Devonian aged microdiorite dykes are found in places and some of these are themselves cut by Palaeocene age camptonite and monchiquite dykes ascribed to the ‘Iona-Ross of Mull dyke swarm’. More recent sedimentary deposits of Quaternary age include both present day beach deposits and raised marine deposits around Iona as well as some restricted areas of blown sand.

Iona lies about from the coast of Mull. It is about wide and long with a resident population of 125. Like other places swept by ocean breezes, there are few trees; most of them are near the parish church.

Iona's highest point is Dùn Ì, , an Iron Age hill fort dating from 100 BC – AD 200. Iona's geographical features include the Bay at the Back of the Ocean and "Càrn Cùl ri Éirinn" (the Hill/Cairn of [turning the] Back to Ireland), said to be adjacent to the beach where St. Columba first landed.

The main settlement, located at St. Ronan's Bay on the eastern side of the island, is called "Baile Mòr" and is also known locally as "The Village". The primary school, post office, the island's two hotels, the Bishop's House and the ruins of the Nunnery are here. The Abbey and MacLeod Centre are a short walk to the north. Port Bàn (white port) beach on the west side of the island is home to the Iona Beach Party.

There are numerous offshore islets and skerries: Eilean Annraidh (island of storm) and Eilean Chalbha (calf island) to the north, Rèidh Eilean and Stac MhicMhurchaidh to the west and Eilean Mùsimul (mouse holm island) and Soa Island to the south are amongst the largest. The steamer "Cathcart Park" carrying a cargo of salt from Runcorn to Wick ran aground on Soa on 15 April 1912, the crew of 11 escaping in two boats.

On a map of 1874, the following territorial subdivision is indicated (from north to south):

In the early Historic Period Iona lay within the Gaelic kingdom of Dál Riata, in the region controlled by the Cenél Loairn (i.e. Lorn, as it was then). The island was the site of a highly important monastery (see Iona Abbey) during the Early Middle Ages. According to tradition the monastery was founded in 563 by the monk Columba, also known as Colm Cille, who had been exiled from his native Ireland as a result of his involvement in the Battle of Cul Dreimhne. Columba and twelve companions went into exile on Iona and founded a monastery there. The monastery was hugely successful, and played a crucial role in the conversion to Christianity of the Picts of present-day Scotland in the late 6th century and of the Anglo-Saxon kingdom of Northumbria in 635. Many satellite institutions were founded, and Iona became the centre of one of the most important monastic systems in Great Britain and Ireland.

Iona became a renowned centre of learning, and its scriptorium produced highly important documents, probably including the original texts of the Iona Chronicle, thought to be the source for the early Irish annals. The monastery is often associated with the distinctive practices and traditions known as Celtic Christianity. In particular, Iona was a major supporter of the "Celtic" system for calculating the date of Easter at the time of the Easter controversy, which pitted supporters of the Celtic system against those favoring the "Roman" system used elsewhere in Western Christianity. The controversy weakened Iona's ties to Northumbria, which adopted the Roman system at the Synod of Whitby in 664, and to Pictland, which followed suit in the early 8th century. Iona itself did not adopt the Roman system until 715, according to the Anglo-Saxon historian Bede. Iona's prominence was further diminished over the next centuries as a result of Viking raids and the rise of other powerful monasteries in the system, such as the Abbey of Kells.

The Book of Kells may have been produced or begun on Iona towards the end of the 8th century. Around this time the island's exemplary high crosses were sculpted; these may be the first such crosses to contain the ring around the intersection that became characteristic of the "Celtic cross". The series of Viking raids on Iona began in 794 and, after its treasures had been plundered many times, Columba's relics were removed and divided two ways between Scotland and Ireland in 849 as the monastery was abandoned.

As the Norse domination of the west coast of Scotland advanced, Iona became part of the Kingdom of the Isles. The Norse "Rex plurimarum insularum" Amlaíb Cuarán died in 980 or 981 whilst in "religious retirement" on Iona. Nonetheless the island was sacked twice by his successors, on Christmas night 986 and again in 987. Although Iona was never again important to Ireland, it rose to prominence once more in Scotland following the establishment of the Kingdom of Scotland in the later 9th century; the ruling dynasty of Scotland traced its origin to Iona, and the island thus became an important spiritual centre for the new kingdom, with many of its early kings buried there. However, a campaign by Magnus Barelegs led to the formal acknowledgement of Norwegian control of Argyll, in 1098.

Somerled, the brother-in-law of Norway's governor of the region (the "King of the Isles"), launched a revolt, and made the kingdom independent. A convent for Augustinian nuns was established in about 1208, with Bethóc, Somerled's daughter, as first prioress. The present Benedictine abbey, Iona Abbey, was built in about 1203. 

On Somerled's death, nominal Norwegian overlordship of the Kingdom was re-established, but de facto control was split between Somerled's sons, and his brother-in-law.

Following the 1266 Treaty of Perth the Hebrides were transferred from Norwegian to Scottish overlordship. At the end of the century, King John Balliol was challenged for the throne by Robert the Bruce. By this point, Somerled's descendants had split into three groups, the MacRory, MacDougalls, and MacDonalds. The MacDougalls backed Balliol, so when he was defeated by de Bruys, the latter exiled the MacDougalls and transferred their island territories to the MacDonalds; by marrying the heir of the MacRorys, the heir of the MacDonalds re-unified most of Somerled's realm, creating the Lordship of the Isles, under nominal Scottish authority. Iona, which had been a MacDougall territory (together with the rest of Lorn), was given to the Campbells, where it remained for half a century.

In 1354, though in exile and without control of his ancestral lands, John, the MacDougall heir, quitclaimed any rights he had over Mull and Iona to the Lord of the Isles (though this had no meaningful effect at the time). When Robert's son, David II, became king, he spent some time in English captivity; following his release, in 1357, he restored MacDougall authority over Lorn. The 1354 quitclaim, which seems to have been an attempt to ensure peace in just such an eventuality, took automatic effect, splitting Mull and Iona from Lorn, and making it subject to the Lordship of the Isles. Iona remained part of the Lordship of the Isles for the next century and a half.

Following the 1491 Raid on Ross, the Lordship of the Isles was dismantled, and Scotland gained full control of Iona for the second time. The monastery and nunnery continued to be active until the Reformation, when buildings were demolished and all but three of the 360 carved crosses destroyed. The Augustine nunnery now only survives as a number of 13th century ruins, including a church and cloister. By the 1760s little more of the nunnery remained standing than at present, though it is the most complete remnant of a medieval nunnery in Scotland.

After a visit in 1773, the English writer Samuel Johnson remarked:

He estimated the population of the village at 70 families or perhaps 350 inhabitants.

In the 19th century green-streaked marble was commercially mined in the south-east of Iona; the quarry and machinery survive, see 'Marble Quarry remains' below.

Iona Abbey, now an ecumenical church, is of particular historical and religious interest to pilgrims and visitors alike. It is the most elaborate and best-preserved ecclesiastical building surviving from the Middle Ages in the Western Isles of Scotland. Though modest in scale in comparison to medieval abbeys elsewhere in Western Europe, it has a wealth of fine architectural detail, and monuments of many periods. The 8th Duke of Argyll presented the sacred buildings and sites of the island to the Iona Cathedral trust in 1899.

In front of the Abbey stands the 9th century St Martin's Cross, one of the best-preserved Celtic crosses in the British Isles, and a replica of the 8th century St John's Cross (original fragments in the Abbey museum).

The ancient burial ground, called the Rèilig Odhrain (Eng: Oran's "burial place" or "cemetery"), contains the 12th century chapel of St Odhrán (said to be Columba's uncle), restored at the same time as the Abbey itself. It contains a number of medieval grave monuments. The abbey graveyard is said to contain the graves of many early Scottish Kings, as well as Norse kings from Ireland and Norway. Iona became the burial site for the kings of Dál Riata and their successors. Notable burials there include:


In 1549 an inventory of 48 Scottish, 8 Norwegian and 4 Irish kings was recorded. None of these graves are now identifiable (their inscriptions were reported to have worn away at the end of the 17th century). Saint Baithin and Saint Failbhe may also be buried on the island. The Abbey graveyard is also the final resting place of John Smith, the former Labour Party leader, who loved Iona. His grave is marked with an epitaph quoting Alexander Pope: "An honest man's the noblest work of God".

Limited archaeological investigations commissioned by the National Trust for Scotland found some evidence for ancient burials in 2013. The excavations, conducted in the area of Martyrs Bay, revealed burials from the 6th-8th centuries, probably jumbled up and reburied in the 13-15th century. 

Other early Christian and medieval monuments have been removed for preservation to the cloister arcade of the Abbey, and the Abbey museum (in the medieval infirmary). The ancient buildings of Iona Abbey are now cared for by Historic Environment Scotland (entrance charge).

The remains of a marble quarrying enterprise can be seen in a small bay on the south-east shore of Iona. The quarry is the source of ‘Iona Marble’, a beautiful translucent green and white stone, much used in brooches and other jewellery. The stone has been known of for centuries and was credited with healing and other powers. While the quarry had been used in a small way, it was not until around the end of the 18th century when it was opened up on a more industrial scale by the Duke of Argyle. The then difficulties of extracting the hard stone and transporting it meant that the scheme was short lived. Another attempt was started in 1907, this time more successful with considerable quantities of stone extracted and indeed exported, but the First World War put paid to this as well, with little quarrying after 1914 and the operation finally closing in 1919. A painting showing the quarry in operation, "The Marble Quarry, Iona" (1909) by David Young Cameron, is in the collection of Cartwright Hall art gallery in Bradford.. Such is the site’s rarity that it has been designated as a Scheduled Ancient Monument.

The island, other than the land owned by the Iona Cathedral Trust, was purchased from the Duke of Argyll by Hugh Fraser in 1979 and donated to the National Trust for Scotland. In 2001 Iona's population was 125 and by the time of the 2011 census this had grown to 177 usual residents. During the same period Scottish island populations as a whole grew by 4% to 103,702.

Not to be confused with the local island community, Iona (Abbey) Community are based within Iona Abbey. 

In 1938 George MacLeod founded the Iona Community, an ecumenical Christian community of men and women from different walks of life and different traditions in the Christian church committed to seeking new ways of living the Gospel of Jesus in today's world. This community is a leading force in the present Celtic Christian revival.

The Iona Community runs 3 residential centres on the Isle of Iona and on Mull, where one can live together in community with people of every background from all over the world. Weeks at the centres often follow a programme related to the concerns of the Iona Community.

The 8 tonne "Fallen Christ" sculpture by Ronald Rae was permanently situated outside the MacLeod Centre in February 2008.

Visitors can reach Iona by the 10-minute ferry trip across the Sound of Iona from Fionnphort on Mull. The most common route from the mainland is via Oban in Argyll and Bute, where regular ferries connect to Craignure on Mull, from where the scenic road runs to Fionnphort. Tourist coaches and local bus services meet the ferries.

Car ownership is lightly regulated, with no requirement for an MOT Certificate or payment of Road Tax for cars kept permanently on the island, but vehicular access is restricted to permanent residents and there are few cars. Visitors must leave their car in Fionnphort, but upon landing on Iona they will find the village, the shops, the post office, the cafe, the hotels and the abbey are all within walking distance. Bike hire is available at the pier, and on Mull.

In addition to the hotels, there are several bed and breakfasts on Iona and various self-catering properties. The Iona Hostel at Lagandorain and the Iona campsite at Cnoc Oran also offer accommodation.

The island of Iona has played an important role in Scottish landscape painting, especially during the Twentieth Century. As travel to north and west Scotland became easier from the mid C18 on, artists’ visits to the island steadily increased. The Abbey remains in particular became frequently recorded during this early period. Many of the artists are listed and illustrated in the valuable book, "‘Iona Portrayed – The Island through Artists’ Eyes 1760-1960’", which lists over 170 artists known to have painted on the island.

The C20 however saw the greatest period of influence on landscape painting, in particular through the many paintings of the island produced by F C B Cadell and S J Peploe, two of the ‘Scottish Colourists’. As with many artists, both professional and amateur, they were attracted by the unique quality of light, the white sandy beaches, the aquamarine colours of the sea and the landscape of rich greens and rocky outcrops. While Cadell and Peploe are perhaps best known, many major Scottish painters of the C20 worked on Iona and visited many times – for example George Houston, D Y Cameron, James Shearer, John Duncan and John Maclauchlan Milne, among many.

Samuel Johnson wrote "That man is little to be envied whose patriotism would not gain force upon the plains of Marathon, or whose piety would not grow warmer amid the ruins of Iona."

In Jules Verne's novel "The Green Ray", the heroes visit Iona in chapters 13 to 16. The inspiration is romantic, the ruins of the island are conducive to daydreaming. The young heroine, Helena Campbell, argues that Scotland in general and Iona in particular are the scene of the appearance of goblins and other familiar demons.

In Jean Raspail's novel "The Fisherman's Ring" (1995), his cardinal is one of the last to support the antipope Benedict XIII and his successors.

In the novel "The Carved Stone" (by Guillaume Prévost), the young Samuel Faulkner is projected in time as he searches for his father and lands on Iona in the year 800, then threatened by the Vikings.

"Peace of Iona" is a song written by Mike Scott that appears on the studio album "Universal Hall" and on the live recording "Karma to Burn" by The Waterboys. Iona is the setting for the song "Oran" on the 1997 Steve McDonald album "Stone of Destiny".

Kenneth C. Steven published an anthology of poetry entitled "Iona: Poems" in 2000 inspired by his association with the island and the surrounding area.

Iona is featured prominently in the first episode ("By the Skin of Our Teeth") of the celebrated arts series "" (1969).

Iona is the setting of Jeanne M. Dams' Dorothy Martin mystery "Holy Terror of the Hebrides" (1998).

The Academy Award–nominated Irish animated film "The Secret of Kells" is about the creation of the Book of Kells. One of the characters, Brother Aiden, is a master illuminator from Iona Abbey who had helped to illustrate the Book, but had to escape the island with it during a Viking invasion.

After his death in 2011, the cremated remains of songwriter/recording artist Gerry Rafferty were scattered on Iona.

Frances Macdonald the contemporary Scottish artist based in Crinian, Argyll, regularly paints landscapes on Iona.

Iona Abbey is mentioned in Tori Amos's "Twinkle" from her 1996 album "Boys for Pele":
"And last time I knew, she worked at an abbey in Iona. She said 'I killed a man, T, I've gotta stay hidden in this abbey' "

Iona is the name of a progressive Celtic rock band (first album released in 1990; not active at present), many of whose songs are inspired by the island of Iona and Columba's life.

Neil Gaiman's poem "In Relig Odhrain", published in "Trigger Warning: Short Fictions and Disturbances (2015)", retells the story of Oran's death, and the creation of the chapel on Iona. This poem was made into a short stop-motion animated film, released in 2019. 






</doc>
<doc id="15040" url="https://en.wikipedia.org/wiki?curid=15040" title="Ido">
Ido

Ido (, sometimes ) is a constructed language, derived from Reformed Esperanto, created to be a universal second language for speakers of diverse backgrounds. Ido was specifically designed to be grammatically, orthographically, and lexicographically regular, and above all easy to learn and use. In this sense, Ido is classified as a constructed international auxiliary language. It is the most successful of many Esperanto derivatives, called Esperantidos.

Ido was created in 1907 out of a desire to reform perceived flaws in Esperanto, a language that had been created 20 years earlier to facilitate international communication. The name of the language traces its origin to the Esperanto word "", meaning "offspring", since the language is a "descendant" of Esperanto. After its inception, Ido gained support from some in the Esperanto community, but following the sudden death in 1914 of one of its most influential proponents, Louis Couturat, it declined in popularity. There were two reasons for this: first, the emergence of further schisms arising from competing reform projects; and second, a general lack of awareness of Ido as a candidate for an international language. These obstacles weakened the movement and it was not until the rise of the Internet that it began to regain momentum.

Ido uses the same 26 letters as the English (Latin) alphabet, with no diacritics. It draws its vocabulary from English, French, German, Italian, Latin, Russian, Spanish and Portuguese, and is largely intelligible to those who have studied Esperanto.

Several works of literature have been translated into Ido, including "The Little Prince", the Book of Psalms, and the Gospel of Luke. As of the year 2000, there were approximately 100–200 Ido speakers in the world.

The idea of a universal second language is not new, and constructed languages are not a recent phenomenon. The first known constructed language was Lingua Ignota, created in the 12th century. But the idea did not catch on in large numbers until the language Volapük was created in 1879. Volapük was popular for some time and apparently had a few thousand users, but was later eclipsed by the popularity of Esperanto, which arose in 1887. Several other languages such as Latino sine Flexione and Idiom Neutral had also been put forward. It was during this time that French mathematician Louis Couturat formed the "Delegation for the Adoption of an International Auxiliary Language".

This delegation made a formal request to the International Association of Academies in Vienna to select and endorse an international language; the request was rejected in May 1907. The Delegation then met as a Committee in Paris in October 1907 to discuss the adoption of a standard international language. Among the languages considered was a new language anonymously submitted at the last moment (and therefore against the Committee rules) under the pen name "Ido". In the end the Committee, always without plenary sessions and consisting of only 12 members, concluded the last day with 4 votes for and 1 abstention. They concluded that no language was completely acceptable, but that Esperanto could be accepted "on condition of several modifications to be realized by the permanent Commission in the direction defined by the conclusions of the Report of the Secretaries [Louis Couturat and Léopold Leau] and by the Ido project".

Esperanto's inventor, L. L. Zamenhof, having heard a number of complaints, had suggested in 1894 a proposal for a Reformed Esperanto with several changes that Ido adopted and made it closer to French: eliminating the accented letters and the accusative case, changing the plural to an Italianesque "-i", and replacing the table of correlatives with more Latinate words. However, the Esperanto community voted and rejected Reformed Esperanto, and likewise most rejected the recommendations of the 1907 Committee composed by 12 members. Zamenhof deferred to their judgment, although doubtful. Furthermore, controversy ensued when the "Ido project" was found to have been primarily devised by Louis de Beaufront, whom Zamenhof had chosen to represent Esperanto before the Committee, as the Committee's rules dictated that the creator of a submitted language could not defend it. The Committee's language was French and not everyone could speak in French. When the president of the Committee asked who was the author of Ido's project, Couturat, Beaufront and Leau answered that they were not. Beaufront was the person who presented Ido's project and gave a description as a better, richer version of Esperanto. Couturat, Leau, Beaufront and Jespersen were finally the only members who voted, all of them for Ido's project. A month later, Couturat accidentally put Jespersen in a copy of a letter in which he acknowledged that Beaufront was the author of the Ido project. Jespersen was angered by this and asked for a public confession, which was never forthcoming.

It is estimated that some 20% of Esperanto leaders and 3–4% of ordinary Esperantists defected to Ido, which from then on suffered constant modifications seeking to perfect it, but which ultimately had the effect of causing many Ido speakers to give up on trying to learn it. Although it fractured the Esperanto movement, the schism gave the remaining Esperantists the freedom to concentrate on using and promoting their language as it stood. At the same time, it gave the Idists freedom to continue working on their own language for several more years before actively promoting it. The "Uniono di la Amiki di la Linguo Internaciona" ("Union of Friends of the International Language") was established along with an Ido Academy to work out the details of the new language.

Couturat, who was the leading proponent of Ido, was killed in an automobile accident in 1914. This, along with World War I, practically suspended the activities of the Ido Academy from 1914 to 1920. In 1928 Ido's major intellectual supporter, the Danish linguist Otto Jespersen, published his own planned language, Novial. His defection from the Ido movement set it back even further.

The language still has active speakers, having a total of 500 speakers. The Internet has sparked a renewal of interest in the language in recent years. A sample of 24 Idists on the Yahoo! group "Idolisto" during November 2005 showed that 57% had begun their studies of the language during the preceding three years, 32% from the mid-1990s to 2002, and 8% had known the language from before.

Few changes have been made to Ido since 1922.

Camiel de Cock was named secretary of linguistic issues in 1990, succeeding Roger Moureaux. He resigned after the creation of a linguistic committee in 1991. De Cock was succeeded by Robert C. Carnaghan, who held the position from 1992 to 2008. No new words were adopted between 2001 and 2006. Following the 2008–2011 elections of ULI's direction committee, Gonçalo Neves replaced Carnaghan as secretary of linguistic issues in February 2008. Neves resigned in August 2008. A new linguistic committee was formed in 2010. In April 2010, Tiberio Madonna was appointed as secretary of linguistic issues, succeeding Neves. 
In January 2011, ULI approved eight new words. This was the first addition of words in many years. As of April 2012, the secretary of linguistic issues remains Tiberio Madonna.

Ido has five vowel phonemes. The vowels and are interchangeable depending on speaker preference, as are and . The combinations /au/ and /eu/ become diphthongs in word roots but not when adding affixes.

All polysyllabic words are stressed on the second-to-last syllable except for verb infinitives, which are stressed on the last syllableskolo, kafeo and lernas for "school", "coffee" and the present tense of "to learn", but irar, savar and drinkar for "to go", "to know" and "to drink". If an i or u precedes another vowel, the pair is considered part of the same syllable when applying the accent rulethus radio, familio and manuo for "radio", "family" and "hand", unless the two vowels are the only ones in the word, in which case the "i" or "u" is stressed: dio, frua for "day" and "early".

Ido uses the same 26 letters as the English alphabet and ISO Basic Latin alphabet with three digraphs and no ligatures or diacritics. Where the table below lists two pronunciations, either is perfectly acceptable.

The digraphs are:

The definite article is ""la"" and is invariable. The indefinite article (a/an) does not exist in Ido. Each word in the Ido vocabulary is built from a root word. A word consists of a root and a grammatical ending. Other words can be formed from that word by removing the grammatical ending and adding a new one, or by inserting certain affixes between the root and the grammatical ending.

Some of the grammatical endings are defined as follows:

These are the same as in Esperanto except for "-i", "-ir", "-ar", "-or" and "-ez". Esperanto marks noun plurals by an "agglutinative" ending "-j" (so plural nouns end in "-oj"), uses "-i" for verb infinitives (Esperanto infinitives are tenseless), and uses "-u" for the imperative. Verbs in Ido, as in Esperanto, do not conjugate depending on person, number or gender; the -as, -is, and -os endings suffice whether the subject is I, you, he, she, they, or anything else. For the word "to be," Ido allows either ""esas"" or ""es"" in the present tense; however, the full forms must be used for the past tense ""esis"" and future tense ""esos"." Adjectives and adverbs are compared in Ido by means of the words "plu" = more, "maxim" = most, "min" = less, "minim" = least, "kam" = than/as. There exist in Ido three categories of adverbs: the simple, the derived, and the composed. The simple adverbs do not need special endings, for example: "tre" = very, "tro" = too, "olim" =formerly, "nun" = now, "nur" = only. The derived and composed adverbs, not being originally adverbs but derived from nouns, adjectives and verbs, have the ending -e.

Ido word order is generally the same as English (subject–verb–object), so the sentence "Me havas la blua libro" is the same as the English "I have the blue book", both in meaning and word order. There are a few differences, however:

Ido generally does not impose rules of grammatical agreement between grammatical categories within a sentence. For example, the verb in a sentence is invariable regardless of the number and person of the subject. Nor must the adjectives be pluralized as well the nounsin Ido "the large books" would be "la granda libri" as opposed to the French "les grands livres" or the Esperanto "la grandaj libroj".

Negation occurs in Ido by simply adding ne before a verb: Me ne havas libro means "I do not have a book". This as well does not vary, and thus the "I do not", "He does not", "They do not" before a verb are simply Me ne, Il ne, and Li ne. In the same way, past tense and future tense negatives are formed by ne before the conjugated verb. "I will not go" and "I did not go" become Me ne iros and Me ne iris respectively.

Yes/no questions are formed by the particle ka in front of the question. "I have a book" (me havas libro) becomes Ka me havas libro? (do I have a book?). Ka can also be placed in front of a noun without a verb to make a simple question, corresponding to the English "is it?" Ka Mark? can mean, "Are you Mark?", "Is it Mark?", "Do you mean Mark?" depending on the context.

The pronouns of Ido were revised to make them more acoustically distinct than those of Esperanto, which all end in "i". Especially the singular and plural first-person pronouns "mi" and "ni" may be difficult to distinguish in a noisy environment, so Ido has "me" and "ni" instead. Ido also distinguishes between intimate ("tu") and formal ("vu") second-person singular pronouns as well as plural second-person pronouns ("vi") not marked for intimacy. Furthermore, Ido has a pan-gender third-person pronoun "lu" (it can mean "he", "she", or "it", depending on the context) in addition to its masculine ("il"), feminine ("el"), and neuter ("ol") third-person pronouns.

"ol", like English "it" and Esperanto "ĝi", is not limited to inanimate objects, but can be used "for entities whose sex is indeterminate: "babies, children, humans, youths, elders, people, individuals, horses, [cattle], cats," etc."

"Lu" is often mistakenly labeled an epicene pronoun, that is, one that refers to both masculine and feminine beings, but in fact, "lu" is more properly a "pan-gender" pronoun, as it is also used for referring to inanimate objects. From "Kompleta Gramatiko Detaloza di la Linguo Internaciona Ido" by Beaufront:

Ido makes correlatives by combining entire words together and changing the word ending, with some irregularities to show distinction.

Composition in Ido obeys stricter rules than in Esperanto, especially formation of
nouns, adjectives and verbs from a radical of a different
class. The reversibility principle assumes that for each composition rule (affix addition), the corresponding decomposition rule (affix removal) is valid.

Hence, while in Esperanto an adjective (for instance , formed on the noun radical , can mean an attribute ( “paper-made encyclopedia”) and a relation ( “paper-making factory”), Ido will distinguish the attribute (“paper” or “of paper” (not “paper-made” exactly)) from the relation (“paper-making”).

Similarly, means in both Esperanto and Ido the noun “crown”; where Esperanto allows formation of “to crown” by simply changing the ending from noun to verb (“crowning” is ), Ido requires an affix so the composition is reversible: (“the act of crowning” is ).

According to Claude Piron, some modifications brought by Ido are in practice impossible to use and ruin spontaneous expression: Ido displays, on linguistic level, other drawbacks Esperanto succeeded to avoid, but I don’t have at hand documents which would allow me to go further in detail. For instance, if I remember correctly, where Esperanto only has the suffix *, Ido has several: **, **, **, which match subtleties which were meant to make language clearer, but that, in practice, inhibit natural expression.

Vocabulary in Ido is derived from French, Italian, Spanish, English, German, and Russian. Basing the vocabulary on various widespread languages was intended to make Ido as easy as possible for the greatest number of people possible. Early on, the first 5,371 Ido word roots were analyzed compared to the vocabulary of the six source languages, and the following result was found:


Another analysis showed that:


Vocabulary in Ido is often created through a number of official prefixes and suffixes that alter the meaning of the word. This allows a user to take existing words and modify them to create neologisms when necessary, and allows for a wide range of expression without the need to learn new vocabulary each time. Though their number is too large to be included in one article, some examples include:

New vocabulary is generally created through an analysis of the word, its etymology, and reference to the six source languages. If a word can be created through vocabulary already existing in the language then it will usually be adopted without need for a new radical (such as wikipedio for "Wikipedia", which consists of wiki + enciklopedio for "encyclopedia"), and if not an entirely new word will be created. The word alternatoro for example was adopted in 1926, likely because five of the six source languages used largely the same orthography for the word, and because it was long enough to avoid being mistaken for other words in the existing vocabulary. Adoption of a word is done through consensus, after which the word will be made official by the union. Care must also be taken to avoid homonyms if possible, and usually a new word undergoes some discussion before being adopted. Foreign words that have a restricted sense and are not likely to be used in everyday life (such as the word "intifada" to refer to the conflict between Israel and Palestine) are left untouched, and often written in italics.

Ido, unlike Esperanto, does not assume the male sex by default. For example, Ido does not derive the word for “waitress” by adding a feminine suffix to “waiter”, as Esperanto does. Instead, Ido words are defined as sex-neutral, and two different suffixes derive masculine and feminine words from the root: ' for a waiter of either sex, ' for a male waiter, and ' for a waitress. There are only two exceptions to this rule: First, ' for “father”, ' for “mother”, and ' for “parent”, and second, ' for “man”, ' for “woman”, and "" for “adult”.

The Lord's Prayer:

Ido has a number of publications that can be subscribed to or downloaded for free in most cases. "Kuriero Internaciona" is a magazine produced in France every few months with a range of topics. "Adavane!" is a magazine produced by the Spanish Ido Society every two months that has a range of topics, as well as a few dozen pages of work translated from other languages. "Progreso" is the official organ of the Ido movement and has been around since the inception of the movement in 1908. Other sites can be found with various stories, fables or proverbs along with a few books of the Bible translated into Ido on a smaller scale. The site "publikaji" has a few podcasts in Ido along with various songs and other recorded material.

Wikipedia includes an Ido-language edition (known in Ido as "Wikipedio"); in January 2012 it was the 81st most visited Wikipedia.

ULI organises Ido conventions yearly, and the conventions include a mix of tourism and work.


Additional notes



</doc>
<doc id="15041" url="https://en.wikipedia.org/wiki?curid=15041" title="Improvisational theatre">
Improvisational theatre

Improvisational theatre, often called improvisation or improv, is the form of theatre, often comedy, in which most or all of what is performed is unplanned or unscripted: created spontaneously by the performers. In its purest form, the dialogue, action, story, and characters are created collaboratively by the players as the improvisation unfolds in present time, without use of an already prepared, written script.

Improvisational theatre exists in performance as a range of styles of improvisational comedy as well as some non-comedic theatrical performances. It is sometimes used in film and television, both to develop characters and scripts and occasionally as part of the final product.

Improvisational techniques are often used extensively in drama programs to train actors for stage, film, and television and can be an important part of the rehearsal process. However, the skills and processes of improvisation are also used outside the context of performing arts. This practice, known as applied improvisation, is used in classrooms as an educational tool and in businesses as a way to develop communication skills, creative problem solving, and supportive team-work abilities that are used by improvisational, ensemble players. It is sometimes used in psychotherapy as a tool to gain insight into a person's thoughts, feelings, and relationships.

The earliest well-documented use of improvisational theatre in Western history is found in the Atellan Farce of 391 BC. From the 16th to the 18th centuries, "commedia dell'arte" performers improvised based on a broad outline in the streets of Italy. In the 1890s, theatrical theorists and directors such as the Russian Konstantin Stanislavski and the French Jacques Copeau, founders of two major streams of acting theory, both heavily utilized improvisation in acting training and rehearsal.

Modern theatrical improvisation games began as drama exercises for children, which were a staple of drama education in the early 20th century thanks in part to the progressive education movement initiated by John Dewey in 1916. Some people credit American Dudley Riggs as the first vaudevillian to use audience suggestions to create improvised sketches on stage. Improvisation exercises were developed further by Viola Spolin in the 1940s, 50s, and 60s, and codified in her book "Improvisation For The Theater", the first book that gave specific techniques for learning to do and teach improvisational theater. In the 1970s in Canada, British playwright and director Keith Johnstone wrote "", a book outlining his ideas on improvisation, and invented Theatresports, which has become a staple of modern improvisational comedy and is the inspiration for the popular television show "Whose Line Is It Anyway?"

Spolin influenced the first generation of modern American improvisers at The Compass Players in Chicago, which led to The Second City. Her son, Paul Sills, along with David Shepherd, started The Compass Players. Following the demise of the Compass Players, Paul Sills began The Second City. They were the first organized troupes in Chicago, and the modern Chicago improvisational comedy movement grew from their success.

Many of the current "rules" of comedic improv were first formalized in Chicago in the late 1950s and early 1960s, initially among The Compass Players troupe, which was directed by Paul Sills. From most accounts, David Shepherd provided the philosophical vision of the Compass Players, while Elaine May was central to the development of the premises for its improvisations. Mike Nichols, Ted Flicker, and Del Close were her most frequent collaborators in this regard. When The Second City opened its doors on December 16, 1959, directed by Paul Sills, his mother Viola Spolin began training new improvisers through a series of classes and exercises which became the cornerstone of modern improv training. By the mid-1960s, Viola Spolin's classes were handed over to her protégé, Jo Forsberg, who further developed Spolin's methods into a one-year course, which eventually became The Players Workshop, the first official school of improvisation in the USA. During this time, Forsberg trained many of the performers who went on to star on The Second City stage.

Many of the original cast of "Saturday Night Live" came from The Second City, and the franchise has produced such comedy stars as Mike Myers, Tina Fey, Bob Odenkirk, Amy Sedaris, Stephen Colbert, Eugene Levy, Jack McBrayer, Steve Carell, Chris Farley, Dan Aykroyd, and John Belushi.

Simultaneously, Keith Johnstone's group The Theatre Machine, which originated in London, was touring Europe. This work gave birth to Theatresports, at first secretly in Johnstone's workshops, and eventually in public when he moved to Canada. Toronto has been home to a rich improv tradition.

In 1984, Dick Chudnow (Kentucky Fried Theater) founded ComedySportz in Milwaukee, WI. Expansion began with the addition of ComedySportz-Madison (WI), in 1985. The first Comedy League of America National Tournament was held in 1988, with 10 teams participating. The league is now known as CSz Worldwide and boasts a roster of 29 international cities.

In San Francisco, The Committee theater was active in North Beach during the 1960s. It was founded by alumni of Chicago's Second City, Alan Myerson and his wife Jessica. When The Committee disbanded in 1972, three major companies were formed: The Pitchell Players, The Wing, and Improvisation Inc. The only company that continued to perform Close's Harold was the latter one. Its two former members, Michael Bossier and John Elk, formed Spaghetti Jam in San Francisco's Old Spaghetti Factory in 1976, where shortform improv and Harolds were performed through 1983. Stand-up comedians performing down the street at the Intersection for the Arts would drop by and sit in. In 1979, Elk brought shortform to England, teaching workshops at Jacksons Lane Theatre, and he was the first American to perform at The Comedy Store, London, above a Soho strip club.

Modern political improvisation's roots include Jerzy Grotowski's work in Poland during the late 1950s and early 1960s, Peter Brook's "happenings" in England during the late 1960s, Augusto Boal's "Forum Theatre" in South America in the early 1970s, and San Francisco's The Diggers' work in the 1960s. Some of this work led to pure improvisational performance styles, while others simply added to the theatrical vocabulary and were, on the whole, avant-garde experiments.

Joan Littlewood, an English actress and director who was active from the 1950s to 1960s, made extensive use of improv in developing plays for performance. However, she was successfully prosecuted twice for allowing her actors to improvise in performance. Until 1968, British law required scripts to be approved by the Lord Chamberlain's Office. The department also sent inspectors to some performances to check that the approved script was performed exactly as approved.

In 1987, Annoyance Theatre began as a club in Chicago that emphasizes longform improvisation. The Annoyance Theatre has grown into multiple locations in Chicago and New York City. It is the home of the longest running musical improv show in history at 11 years.

In 2012, Lebanese writer and director Lucien Bourjeily used improvisational theater techniques to create a multi-sensory play entitled "66 Minutes in Damascus". This play premiered at the London International Festival of Theater, and is considered one of the most extreme kinds of interactive improvised theater put on stage. The audience play the part of kidnapped tourists in today's Syria in a hyperreal sensory environment.

Rob Wittig and Mark C. Marino have developed a form of improv for online theatrical improvisation called netprov. The form relies on social media to engage audiences in the creation of dynamic fictional scenarios that evolve in real-time.

Modern improvisational comedy, as it is practiced in the West, falls generally into two categories: shortform and longform.

Shortform improv consists of short scenes usually constructed from a predetermined game, structure, or idea and driven by an audience suggestion. Many short form exercises were first created by Viola Spolin, who called them theatre games, influenced by her training from recreational games expert Neva Boyd. The short-form improv comedy television series "Whose Line Is It Anyway?" has familiarized American and British viewers with short-form.

Longform improv performers create shows in which short scenes are often interrelated by story, characters, or themes. Longform shows may take the form of an existing type of theatre, for example a full-length play or Broadway-style musical such as Spontaneous Broadway. One of the better-known longform structures is the Harold, developed by ImprovOlympic co-founder Del Close. Many such longform structures now exist. 

Longform improvisation is especially performed in Chicago, New York City, Los Angeles; has a strong presence in Austin, Boston, Minneapolis, Phoenix, Philadelphia, San Francisco, Seattle, Detroit, Toronto, Vancouver, Washington, D.C.; and is building a growing following in Baltimore, Denver, Kansas City, Montreal, Columbus, New Orleans, Omaha, Rochester, and Hawaii. Outside the United States, longform improv has a growing presence in the United Kingdom, especially in cities such as London, Bristol, and at the Edinburgh Festival Fringe.

Other forms of improvisational theatre training and performance techniques are experimental and avant-garde in nature and not necessarily intended to be comedic. These include Playback Theatre and Theatre of the Oppressed, the Poor Theatre, the Open Theatre, to name only a few.

The Open Theatre was founded in New York City by a group of former students of acting teacher Nola Chilton, and joined shortly thereafter by director Joseph Chaikin, formerly of The Living Theatre, and Peter Feldman. This avante-garde theatre group explored political, artistic, and social issues. The company, developing work through an improvisational process drawn from Chilton and Viola Spolin, created well-known exercises, such as "sound and movement" and "transformations", and originated radical forms and techniques that anticipated or were contemporaneous with Jerzy Grotowski's "poor theater" in Poland.[1] During the sixties Chaikin and the Open Theatre developed full theatrical productions with nothing but the actors, a few chairs and a bare stage, creating character, time and place through a series of transformations the actors physicalized and discovered through improvisations.

Longform, dramatic, and narrative-based improvisation is well-established on the west coast with companies such as San Francisco's BATS Improv. This format allows for full-length plays and musicals to be created improvisationally.

Many people who have studied improv have noted that the guiding principles of improv are useful, not just on stage, but in everyday life. For example, Stephen Colbert in a commencement address said,

Tina Fey in her book "Bossypants" lists several rules of improv that apply in the workplace. There has been much interest in bringing lessons from improv into the corporate world. In a New York Times article titled "Can Executives Learn to Ignore the Script?", Stanford professor and author, Patricia Ryan Madson notes, "executives and engineers and people in transition are looking for support in saying yes to their own voice. Often, the systems we put in place to keep us secure are keeping us from our more creative selves."

Many directors have made use of improvisation in the creation of both mainstream and experimental films. Many silent filmmakers such as Charlie Chaplin and Buster Keaton used improvisation in the making of their films, developing their gags while filming and altering the plot to fit. The Marx Brothers were notorious for deviating from the script they were given, their ad libs often becoming part of the standard routine and making their way into their films. Many people, however, make a distinction between ad-libbing and improvising.

The British director Mike Leigh makes extensive use of improvisation in the creation of his films, including improvising important moments in the characters' lives that will not even appear in the film. "This Is Spinal Tap" and other mockumentary films of director Christopher Guest were created with a mix of scripted and unscripted material. "Blue in the Face" is a 1995 comedy directed by Wayne Wang and Paul Auster created in part by the improvisations during the filming of "Smoke".

Some of the best known American film directors who used improvisation in their work with actors are John Cassavetes, Robert Altman, Christopher Guest, and Rob Reiner.

Improv comedy techniques have also been used in hit television shows such as HBO's "Curb Your Enthusiasm" created by Larry David, the UK Channel 4 and ABC television series "Whose Line Is It Anyway" (and its spinoffs "Drew Carey's Green Screen Show" and "Drew Carey's Improv-A-Ganza"), Nick Cannon's improv comedy show "Wild 'N Out", and "Thank God You're Here". A very early American improv television program was the weekly half-hour "What Happens Now?" which premiered on New York's WOR-TV on October 15, 1949 and ran for 22 episodes. "The Improvisers" were six actors (including Larry Blyden, Ross Martin, and Jean Alexander – Jean Pugsley at the time) who improvised skits based on situations suggested by viewers. In Canada, the series "Train 48" was improvised from scripts which contained a minimal outline of each scene, and the comedy series "This Sitcom Is...Not to Be Repeated" incorporated dialogue drawn from a hat during the course of an episode. The American show "Reno 911!" also contained improvised dialogue based on a plot outline. "Fast and Loose" is an improvisational game show, much like "Whose Line Is It Anyway?". The BBC sitcoms "Outnumbered" and "The Thick of It" also had some improvised elements in them.

In the field of the psychology of consciousness, Eberhard Scheiffele explored the altered state of consciousness experienced by actors and improvisers in his scholarly paper "Acting: an altered state of consciousness". According to G. William Farthing in "The Psychology of Consciousness" comparative study, actors routinely enter into an altered state of consciousness (ASC). Acting is seen as altering most of the 14 dimensions of changed subjective experience which characterize ASCs according to Farthing, namely: attention, perception, imagery and fantasy, inner speech, memory, higher-level thought processes, meaning or significance of experiences, time experience, emotional feeling and expression, level of arousal, self-control, suggestibility, body image, and sense of personal identity.

In the growing field of Drama Therapy, psychodramatic improvisation, along with other techniques developed for Drama Therapy, are used extensively. The ""Yes, and"" rule has been compared to Milton Erickson's "utilization" process and to a variety of acceptance-based psychotherapies. Improv training has been recommended for couples therapy and therapist training, and it has been speculated that improv training may be helpful in some cases of social anxiety disorder.

Improvisational theatre often allows an interactive relationship with the audience. Improv groups frequently solicit suggestions from the audience as a source of inspiration, a way of getting the audience involved, and as a means of proving that the performance is not scripted. That charge is sometimes aimed at the masters of the art, whose performances can seem so detailed that viewers may suspect the scenes are planned.

In order for an improvised scene to be successful, the improvisers involved must work together responsively to define the parameters and action of the scene, in a process of co-creation. With each spoken word or action in the scene, an improviser makes an "offer", meaning that he or she defines some element of the reality of the scene. This might include giving another character a name, identifying a relationship, location, or using mime to define the physical environment. These activities are also known as "endowment". It is the responsibility of the other improvisers to accept the offers that their fellow performers make; to not do so is known as blocking, negation, or denial, which usually prevents the scene from developing. Some performers may deliberately block (or otherwise break out of character) for comedic effect—this is known as "gagging"—but this generally prevents the scene from advancing and is frowned upon by many improvisers. Accepting an offer is usually accompanied by adding a new offer, often building on the earlier one; this is a process improvisers refer to as ""Yes, And..."" and is considered the cornerstone of improvisational technique. Every new piece of information added helps the improvisers to refine their characters and progress the action of the scene. The ""Yes, And..."" rule, however, applies to a scene's early stage since it is in this stage that a "base (or shared) reality" is established in order to be later redefined by applying the ""if (this is true), then (what else can also be true)"" practice progressing the scene into comedy, as explained in the 2013 manual by the "Upright Citizens Brigade" members.

The unscripted nature of improv also implies no predetermined knowledge about the props that might be useful in a scene. Improv companies may have at their disposal some number of readily accessible props that can be called upon at a moment's notice, but many improvisers eschew props in favor of the infinite possibilities available through mime. In improv, this is more commonly known as 'space object work' or 'space work', not 'mime', and the props and locations created by this technique, as 'space objects' created out of 'space substance,' developed as a technique by Viola Spolin. As with all improv "offers", improvisers are encouraged to respect the validity and continuity of the imaginary environment defined by themselves and their fellow performers; this means, for example, taking care not to walk through the table or "miraculously" survive multiple bullet wounds from another improviser's gun.

Because improvisers may be required to play a variety of roles without preparation, they need to be able to construct characters quickly with physicality, gestures, accents, voice changes, or other techniques as demanded by the situation. The improviser may be called upon to play a character of a different age or sex. Character motivations are an important part of successful improv scenes, and improvisers must therefore attempt to act according to the objectives that they believe their character seeks.

In improv formats with multiple scenes, an agreed-upon signal is used to denote scene changes. Most often, this takes the form of a performer running in front of the scene, known as a "wipe." Tapping a character in or out can also be employed. The performers not currently part of the scene often stand at the side or back of the stage, and can enter or exit the scene by stepping into or out of the stage center.

Many theatre troupes are devoted to staging improvisational performances and growing the improv community through their training centers.

In addition to for-profit theatre troupes, there are many college-based improv groups in the United States and around the world.

In Europe the special contribution to the theatre of the abstract, the surreal, the irrational and the subconscious have been part of the stage tradition for centuries. From the 1990s onwards a growing number of European Improv groups have been set up specifically to explore the possibilities offered by the use of the abstract in improvised performance, including dance, movement, sound, music, mask work, lighting, and so on. These groups are not especially interested in comedy, either as a technique or as an effect, but rather in expanding the improv genre so as to incorporate techniques and approaches that have long been a legitimate part of European theatre.

Some key figures in the development of improvisational theatre are Viola Spolin and her son Paul Sills, founder of Chicago's famed Second City troupe and originator of Theater Games, and Del Close, founder of ImprovOlympic (along with Charna Halpern) and creator of a popular longform improv format known as The Harold. Other luminaries include Keith Johnstone, the British teacher and writer–author of "Impro", who founded the Theatre Machine and whose teachings form the foundation of the popular shortform Theatresports format, Dick Chudnow, founder of ComedySportz which evolved its family-friendly show format from Johnstone's Theatersports, and Bill Johnson, creator/director of The Magic Meathands, who pioneered the concept of "Commun-edy Outreach" by tailoring performances to non-traditional audiences, such as the homeless and foster children.

David Shepherd, with Paul Sills, founded The Compass Players in Chicago. Shepherd was intent on developing a true "people's Theatre", and hoped to bring political drama to the stockyards. The Compass went on to play in numerous forms and companies, in a number of cities including NY and Hyannis, after the founding of The Second City. A number of Compass members were also founding members of The Second City. In the 1970s, Shepherd began experimenting with group-created videos. He is the author of "That Movie In Your Head", about these efforts. In the 1970s, David Shepherd and Howard Jerome created the Improvisational Olympics, a format for competition based improv. The Improv Olympics were first demonstrated at Toronto's Homemade Theatre in 1976 and have been continued on as the Canadian Improv Games. In the United States, the Improv Olympics were later produced by Charna Halpern under the name "ImprovOlympic" and now as "IO"; IO operates training centers and theaters in Chicago and Los Angeles. At IO, Halpern combined Shepherd's "Time Dash" game with Del Close's "Harold" game; the revised format for the Harold became the fundamental structure for the development of modern longform improvisation.

In 1975 Jonathan Fox founded Playback Theatre, a form of improvised community theatre which is often not comedic and replays stories as shared by members of the audience.

The Groundlings is a popular and influential improv theatre and training center in Los Angeles, California. The late Gary Austin, founder of The Groundlings, taught improvisation around the country, focusing especially in Los Angeles. He was widely acclaimed as one of the greatest acting teachers in America. His work was grounded in the lessons he learned as an improviser at The Committee with Del Close, as well as in his experiences as founding director of The Groundlings. The Groundlings is often seen as the Los Angeles training ground for the "second generation" of improv luminaries and troupes. Stan Wells developed the "Clap-In" style of longform improvisation here, later using this as the basis for his own theatre, The Empty Stage which in turn bred multiple troupes utilizing this style.

In the late 1990s, Matt Besser, Amy Poehler, Ian Roberts, and Matt Walsh founded the Upright Citizens Brigade Theatre in New York and later they founded one in Los Angeles, each with an accompanying improv/sketch comedy school. In September 2011 the UCB opened a third theatre in New York City's East Village, known as UCBeast.

Hoopla Impro are the founders of the UK and London's 1st improv theatre. They also run an annual UK improv festival and improv marathon.

In 2015, The Free Association opened in London as a counterpart to American improv schools.

Gunter Lösel compared the existing improvisational theater theories (from Moreno, Spolin, Johnstone, Close...), structured them and wrote a general theory of improvisational theater.

Alan Alda's book "If I Understood You, Would I Have This Look on My Face?" investigates the way in which improvisation improves communication in the sciences. The book is based on his work at Alan Alda Center for Communicating Science at Stony Brook University. The book has many examples of how improvisational theater games can increase communication skills and develop empathy.





</doc>
<doc id="15043" url="https://en.wikipedia.org/wiki?curid=15043" title="International Space Station">
International Space Station

The International Space Station (ISS) is a modular space station (habitable artificial satellite) in low Earth orbit. It is a multinational collaborative project between five participating space agencies: NASA (United States), Roscosmos (Russia), JAXA (Japan), ESA (Europe), and CSA (Canada). The ownership and use of the space station is established by intergovernmental treaties and agreements. The ISS program evolved from the Space Station "Freedom", an American proposal in the 1980s to construct a permanently crewed Earth-orbiting station.

The ISS serves as a microgravity and space environment research laboratory in which scientific experiments are conducted in astrobiology, astronomy, meteorology, physics, and other fields. The station is suited for testing the spacecraft systems and equipment required for possible future long-duration missions to the Moon and Mars. It is the largest artificial object in space and the largest satellite in low Earth orbit, regularly visible to the naked eye from Earth's surface. It maintains an orbit with an average altitude of by means of reboost manoeuvres using the engines of the "Zvezda" Service Module or visiting spacecraft. The ISS circles the Earth in roughly 93 minutes, completing  orbits per day.

The station is divided into two sections: the Russian Orbital Segment (ROS), operated by Russia; and the United States Orbital Segment (USOS), which is shared by many nations. Roscosmos has endorsed the continued operation of ISS through 2024, but had previously proposed using elements of the Russian segment to construct a new Russian space station called OPSEK. , the station is expected to operate until 2030.

The first ISS component was launched in 1998, with the first long-term residents arriving on 2 November 2000. The station has since been continuously occupied for . This is the longest continuous human presence in low Earth orbit, having surpassed the previous record of held by the "Mir" space station. The latest major pressurised module was fitted in 2011, with an experimental inflatable space habitat added in 2016. Development and assembly of the station continues, with several major new Russian elements scheduled for launch starting in 2020. The ISS consists of pressurised habitation modules, structural trusses, photovoltaic solar arrays, thermal radiators, docking ports, experiment bays and robotic arms. Major ISS modules have been launched by Russian Proton and Soyuz rockets and US Space Shuttles.

The ISS is the ninth space station to be inhabited by crews, following the Soviet and later Russian "Salyut", "Almaz", and "Mir" stations and the U.S. "Skylab." The station is serviced by a variety of visiting spacecraft: the Russian Soyuz and Progress, the U.S. Dragon and Cygnus, the Japanese H-II Transfer Vehicle, and, formerly, the European Automated Transfer Vehicle. The Dragon spacecraft allows the return of pressurised cargo to Earth (downmass), which is used, for example, to repatriate scientific experiments for further analysis. The Soyuz return capsule has minimal downmass capability next to the astronauts.

, 239 astronauts, cosmonauts, and space tourists from 19 different nations have visited the space station, many of them multiple times. This includes 151 Americans, 47 Russians, nine Japanese, eight Canadians, five Italians, four French, three Germans, and one each from Belgium, Brazil, Denmark, Kazakhstan, Malaysia, the Netherlands, South Africa, South Korea, Spain, Sweden, the United Arab Emirates, and the United Kingdom.

The ISS was originally intended to be a laboratory, observatory, and factory while providing transportation, maintenance, and a low Earth orbit staging base for possible future missions to the Moon, Mars, and asteroids. However, not all of the uses envisioned in the initial memorandum of understanding between NASA and Roscosmos have come to fruition. In the 2010 United States National Space Policy, the ISS was given additional roles of serving commercial, diplomatic, and educational purposes.

The ISS provides a platform to conduct scientific research, with power, data, cooling, and crew available to support experiments. Small uncrewed spacecraft can also provide platforms for experiments, especially those involving zero gravity and exposure to space, but space stations offer a long-term environment where studies can be performed potentially for decades, combined with ready access by human researchers.

The ISS simplifies individual experiments by allowing groups of experiments to share the same launches and crew time. Research is conducted in a wide variety of fields, including astrobiology, astronomy, physical sciences, materials science, space weather, meteorology, and human research including space medicine and the life sciences. Scientists on Earth have timely access to the data and can suggest experimental modifications to the crew. If follow-on experiments are necessary, the routinely scheduled launches of resupply craft allows new hardware to be launched with relative ease. Crews fly expeditions of several months' duration, providing approximately 160 person-hours per week of labour with a crew of six. However, a considerable amount of crew time is taken up by station maintenance.

Perhaps the most notable ISS experiment is the Alpha Magnetic Spectrometer (AMS), which is intended to detect dark matter and answer other fundamental questions about our universe and is as important as the Hubble Space Telescope according to NASA. Currently docked on station, it could not have been easily accommodated on a free flying satellite platform because of its power and bandwidth needs. On 3 April 2013, scientists reported that hints of dark matter may have been detected by the AMS. According to the scientists, "The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays".

The space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind, in addition to cosmic rays), high vacuum, extreme temperatures, and microgravity. Some simple forms of life called extremophiles, as well as small invertebrates called tardigrades can survive in this environment in an extremely dry state through desiccation.

Medical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy, bone loss, and fluid shift. This data will be used to determine whether high duration human spaceflight and space colonisation are feasible. , data on bone loss and muscular atrophy suggest that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars.

Medical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.

In August 2020, scientists reported that bacteria from Earth, particularly "Deinococcus radiodurans" bacteria, which is highly resistant to environmental hazards, were found to survive for three years in outer space, based on studies conducted on the International Space Station. These findings support the notion of panspermia, the hypothesis that life exists throughout the Universe, distributed in various ways, including space dust, meteoroids, asteroids, comets, planetoids or contaminated spacecraft.

Gravity at the altitude of the ISS is approximately 90% as strong as at Earth's surface, but objects in orbit are in a continuous state of freefall, resulting in an apparent state of weightlessness. This perceived weightlessness is disturbed by five separate effects:

Researchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of this data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues, and the unusual protein crystals that can be formed in space.

Investigating the physics of fluids in microgravity will provide better models of the behaviour of fluids. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. In addition, examining reactions that are slowed by low gravity and low temperatures will improve our understanding of superconductivity.

The study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. Other areas of interest include the effect of the low gravity environment on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve current knowledge about energy production, and lead to economic and environmental benefits. Future plans are for the researchers aboard the ISS to examine aerosols, ozone, water vapour, and oxides in Earth's atmosphere, as well as cosmic rays, cosmic dust, antimatter, and dark matter in the Universe.

The ISS provides a location in the relative safety of low Earth orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance as well as repair and replacement activities on-orbit, which will be essential skills in operating spacecraft farther from Earth, mission risks can be reduced and the capabilities of interplanetary spacecraft advanced. Referring to the MARS-500 experiment, ESA states that "Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations". Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a "shorter version" of MARS-500 may be carried out on the ISS.

In 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, "When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars." A crewed mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other four partners that China, India and South Korea be invited to join the ISS partnership. NASA chief Charles Bolden stated in February 2011, "Any mission to Mars is likely to be a global effort". Currently, US federal legislation prevents NASA co-operation with China on space projects.

The ISS crew provides opportunities for students on Earth by running student-developed experiments, making educational demonstrations, allowing for student participation in classroom versions of ISS experiments, and directly engaging students using radio, videolink and email. ESA offers a wide range of free teaching materials that can be downloaded for use in classrooms. In one lesson, students can navigate a 3-D model of the interior and exterior of the ISS, and face spontaneous challenges to solve in real time.

JAXA aims to inspire children to "pursue craftsmanship" and to heighten their "awareness of the importance of life and their responsibilities in society". Through a series of education guides, a deeper understanding of the past and near-term future of crewed space flight, as well as that of Earth and life, will be learned. In the JAXA Seeds in Space experiments, the mutation effects of spaceflight on plant seeds aboard the ISS is explored. Students grow sunflower seeds which flew on the ISS for about nine months. In the first phase of "Kibō" utilisation from 2008 to mid-2010, researchers from more than a dozen Japanese universities conducted experiments in diverse fields.

Cultural activities are another major objective. Tetsuo Tanaka, director of JAXA's Space Environment and Utilization Center, says "There is something about space that touches even people who are not interested in science."

Amateur Radio on the ISS (ARISS) is a volunteer programme which encourages students worldwide to pursue careers in science, technology, engineering and mathematics through amateur radio communications opportunities with the ISS crew. ARISS is an international working group, consisting of delegations from nine countries including several countries in Europe as well as Japan, Russia, Canada, and the United States. In areas where radio equipment cannot be used, speakerphones connect students to ground stations which then connect the calls to the station.

"First Orbit" is a feature-length documentary film about Vostok 1, the first crewed space flight around the Earth. By matching the orbit of the International Space Station to that of Vostok 1 as closely as possible, in terms of ground path and time of day, documentary filmmaker Christopher Riley and ESA astronaut Paolo Nespoli were able to film the view that Yuri Gagarin saw on his pioneering orbital space flight. This new footage was cut together with the original Vostok 1 mission audio recordings sourced from the Russian State Archive. Nespoli, during Expedition 26/27, filmed the majority of the footage for this documentary film, and as a result is credited as its director of photography. The film was streamed through the website firstorbit.org in a global YouTube premiere in 2011, under a free licence.

In May 2013, commander Chris Hadfield shot a music video of David Bowie's "Space Oddity" on board the station; the film was released on YouTube. It was the first music video ever to be filmed in space.

In November 2017, while participating in Expedition 52/53 on the ISS, Paolo Nespoli made two recordings (one in English the other in his native Italian) of his spoken voice, for use on Wikipedia articles. These were the first content made specifically for Wikipedia, in space.

Since the International Space Station is a multi-national collaborative project, the components for in-orbit assembly were manufactured in various countries around the world. Beginning in the mid 1990s, the U.S. components "Destiny", "Unity", the Integrated Truss Structure, and the solar arrays were fabricated at the Marshall Space Flight Center and the Michoud Assembly Facility. These modules were delivered to the Operations and Checkout Building and the Space Station Processing Facility (SSPF) for final assembly and processing for launch.

The Russian modules, including "Zarya" and "Zvezda", were manufactured at the Khrunichev State Research and Production Space Center in Moscow. "Zvezda" was initially manufactured in 1985 as a component for "Mir-2", but was never launched and instead became the ISS Service Module.

The European Space Agency "Columbus" module was manufactured at the EADS Astrium Space Transportation facilities in Bremen, Germany, along with many other contractors throughout Europe. The other ESA-built modules—"Harmony", "Tranquility", the "Leonardo" MPLM, and the "Cupola"—were initially manufactured at the Thales Alenia Space factory in Turin, Italy. The structural steel hulls of the modules were transported by aircraft to the Kennedy Space Center SSPF for launch processing.

The Japanese Experiment Module "Kibō", was fabricated in various technology manufacturing facilities in Japan, at the NASDA (now JAXA) Tsukuba Space Center, and the Institute of Space and Astronautical Science. The "Kibo" module was transported by ship and flown by aircraft to the SSPF.

The Mobile Servicing System, consisting of the Canadarm2 and the "Dextre" grapple fixture, was manufactured at various factories in Canada (such as the David Florida Laboratory) and the United States, under contract by the Canadian Space Agency. The mobile base system, a connecting framework for Canadarm2 mounted on rails, was built by Northrop Grumman.

The assembly of the International Space Station, a major endeavour in space architecture, began in November 1998. Russian modules launched and docked robotically, with the exception of "Rassvet". All other modules were delivered by the Space Shuttle, which required installation by ISS and Shuttle crewmembers using the Canadarm2 (SSRMS) and extra-vehicular activities (EVAs); , they had added 159 components during more than 1,000 hours of EVA (see List of ISS spacewalks). 127 of these spacewalks originated from the station, and the remaining 32 were launched from the airlocks of docked Space Shuttles. The beta angle of the station had to be considered at all times during construction.

The first module of the ISS, "Zarya", was launched on 20 November 1998 on an autonomous Russian Proton rocket. It provided propulsion, attitude control, communications, electrical power, but lacked long-term life support functions. Two weeks later, a passive NASA module "Unity" was launched aboard Space Shuttle flight STS-88 and attached to "Zarya" by astronauts during EVAs. This module has two Pressurised Mating Adapters (PMAs), one connects permanently to "Zarya", the other allowed the Space Shuttle to dock to the space station. At that time, the Russian station "Mir" was still inhabited, and the ISS remained uncrewed for two years. On 12 July 2000, "Zvezda" was launched into orbit. Preprogrammed commands on board deployed its solar arrays and communications antenna. It then became the passive target for a rendezvous with "Zarya" and "Unity": it maintained a station-keeping orbit while the "Zarya"-"Unity" vehicle performed the rendezvous and docking via ground control and the Russian automated rendezvous and docking system. "Zarya" computer transferred control of the station to "Zvezda" computer soon after docking. "Zvezda" added sleeping quarters, a toilet, kitchen, CO scrubbers, dehumidifier, oxygen generators, exercise equipment, plus data, voice and television communications with mission control. This enabled permanent habitation of the station.

The first resident crew, Expedition 1, arrived in November 2000 on Soyuz TM-31. At the end of the first day on the station, astronaut Bill Shepherd requested the use of the radio call sign ""Alpha"", which he and cosmonaut Krikalev preferred to the more cumbersome ""International Space Station"". The name ""Alpha"" had previously been used for the station in the early 1990s, and its use was authorised for the whole of Expedition 1. Shepherd had been advocating the use of a new name to project managers for some time. Referencing a naval tradition in a pre-launch news conference he had said: "For thousands of years, humans have been going to sea in ships. People have designed and built these vessels, launched them with a good feeling that a name will bring good fortune to the crew and success to their voyage." Yuri Semenov, the President of Russian Space Corporation Energia at the time, disapproved of the name ""Alpha"" as he felt that "Mir" was the first modular space station, so the names ""Beta"" or ""Mir" 2" for the ISS would have been more fitting.

Expedition 1 arrived midway between the flights of STS-92 and STS-97. These two Space Shuttle flights each added segments of the station's Integrated Truss Structure, which provided the station with Ku-band communication for US television, additional attitude support needed for the additional mass of the USOS, and substantial solar arrays supplementing the station's four existing solar arrays.

Over the next two years, the station continued to expand. A Soyuz-U rocket delivered the "Pirs" docking compartment. The Space Shuttles "Discovery", "Atlantis", and "Endeavour" delivered the "Destiny" laboratory and "Quest" airlock, in addition to the station's main robot arm, the Canadarm2, and several more segments of the Integrated Truss Structure.

The expansion schedule was interrupted by the disaster in 2003 and a resulting hiatus in flights. The Space Shuttle was grounded until 2005 with STS-114 flown by "Discovery".

Assembly resumed in 2006 with the arrival of STS-115 with "Atlantis", which delivered the station's second set of solar arrays. Several more truss segments and a third set of arrays were delivered on STS-116, STS-117, and STS-118. As a result of the major expansion of the station's power-generating capabilities, more pressurised modules could be accommodated, and the "Harmony" node and "Columbus" European laboratory were added. These were soon followed by the first two components of "Kibō". In March 2009, STS-119 completed the Integrated Truss Structure with the installation of the fourth and final set of solar arrays. The final section of "Kibō" was delivered in July 2009 on STS-127, followed by the Russian "Poisk" module. The third node, "Tranquility", was delivered in February 2010 during STS-130 by the Space Shuttle "Endeavour", alongside the "Cupola", followed in May 2010 by the penultimate Russian module, "Rassvet". "Rassvet" was delivered by Space Shuttle "Atlantis" on STS-132 in exchange for the Russian Proton delivery of the US-funded "Zarya" module in 1998. The last pressurised module of the USOS, "Leonardo", was brought to the station in February 2011 on the final flight of "Discovery", STS-133. The Alpha Magnetic Spectrometer was delivered by "Endeavour" on STS-134 the same year.

, the station consisted of 15 pressurised modules and the Integrated Truss Structure. Five modules are still to be launched, including the "Nauka" with the European Robotic Arm, the "Prichal" module, and two power modules called NEM-1 and NEM-2. , Russia's future primary research module "Nauka" is set to launch in the spring of 2021, along with the European Robotic Arm which will be able to relocate itself to different parts of the Russian modules of the station.

The gross mass of the station changes over time. The total launch mass of the modules on orbit is about (). The mass of experiments, spare parts, personal effects, crew, foodstuff, clothing, propellants, water supplies, gas supplies, docked spacecraft, and other items add to the total mass of the station. Hydrogen gas is constantly vented overboard by the oxygen generators.

The ISS is a third generation modular space station. Modular stations can allow modules to be added to or removed from the existing structure, allowing greater flexibility.

Below is a diagram of major station components. The blue areas are pressurised sections accessible by the crew without using spacesuits. The station's unpressurised superstructure is indicated in red. Other unpressurised components are yellow. The "Unity" node joins directly to the "Destiny" laboratory. For clarity, they are shown apart.

"Zarya" (), also known as the Functional Cargo Block or FGB (from the or "ФГБ"), is the first module of the ISS to be launched. The FGB provided electrical power, storage, propulsion, and guidance to the ISS during the initial stage of assembly. With the launch and assembly in orbit of other modules with more specialised functionality, "Zarya " is currently primarily used for storage, both inside the pressurised section and in the externally mounted fuel tanks. The "Zarya" is a descendant of the TKS spacecraft designed for the Russian "Salyut" programme. The name "Zarya", which means sunrise, was given to the FGB because it signified the dawn of a new era of international cooperation in space. Although it was built by a Russian company, it is owned by the United States.

"Zarya" was built from December 1994 to January 1998 at the Khrunichev State Research and Production Space Center (KhSC) in Moscow.

"Zarya" was launched on 20 November 1998 on a Russian Proton rocket from Baikonur Cosmodrome Site 81 in Kazakhstan to a high orbit with a designed lifetime of at least 15 years. After "Zarya" reached orbit, STS-88 launched on 4 December 1998 to attach the "Unity" module.

The "Unity" connecting module, also known as Node 1, is the first US-built component of the ISS. It connects the Russian and US segments of the station, and is where crew eat meals together.

The module is cylindrical in shape, with six berthing locations (forward, aft, port, starboard, zenith, and nadir) facilitating connections to other modules. "Unity" measures in diameter, is long, made of steel, and was built for NASA by Boeing in a manufacturing facility at the Marshall Space Flight Center in Huntsville, Alabama. "Unity" is the first of the three connecting modules; the other two are "Harmony" and "Tranquility".

"Unity" was carried into orbit as the primary cargo of the on STS-88, the first Space Shuttle mission dedicated to assembly of the station. On 6 December 1998, the STS-88 crew mated the aft berthing port of "Unity" with the forward hatch of the already orbiting "Zarya" module. This was the first connection made between two station modules.

"Zvezda" (, meaning "star"), "Salyut" DOS-8, also known as the "Zvezda" Service Module, is a module of the ISS. It was the third module launched to the station, and provides all of the station's life support systems, some of which are supplemented in the USOS, as well as living quarters for two crew members. It is the structural and functional center of the Russian Orbital Segment, which is the Russian part of the ISS. Crew assemble here to deal with emergencies on the station.

The basic structural frame of "Zvezda", known as "DOS-8", was initially built in the mid-1980s to be the core of the "Mir-2" space station. This means that "Zvezda" is similar in layout to the core module (DOS-7) of the "Mir" space station. It was in fact labeled as "Mir-2" for quite some time in the factory. Its design lineage thus extends back to the original "Salyut" stations. The space frame was completed in February 1985 and major internal equipment was installed by October 1986.

The rocket used for launch to the ISS carried advertising; it was emblazoned with the logo of Pizza Hut restaurants, for which they are reported to have paid more than US$1 million. The money helped support Khrunichev State Research and Production Space Center and the Russian advertising agencies that orchestrated the event.

On 26 July 2000, "Zvezda" became the third component of the ISS when it docked at the aft port of "Zarya". (U.S. "Unity" module had already been attached to the "Zarya".) Later in July, the computers aboard "Zarya" handed over ISS commanding functions to computers on "Zvezda".

The "Destiny" module, also known as the U.S. Lab, is the primary operating facility for U.S. research payloads aboard the International Space Station (ISS). It was berthed to the "Unity" module and activated over a period of five days in February 2001. "Destiny" is NASA's first permanent operating orbital research station since Skylab was vacated in February 1974.

The Boeing Company began construction of the research laboratory in 1995 at the Michoud Assembly Facility and then the Marshall Space Flight Center in Huntsville, Alabama. "Destiny" was shipped to the Kennedy Space Center in Florida in 1998, and was turned over to NASA for pre-launch preparations in August 2000. It launched on 7 February 2001 aboard the on STS-98.

The "Quest" Joint Airlock, previously known as the Joint Airlock Module, is the primary airlock for the ISS. "Quest" was designed to host spacewalks with both Extravehicular Mobility Unit (EMU) spacesuits and Orlan space suits. The airlock was launched on STS-104 on 14 July 2001. Before "Quest" was attached, Russian spacewalks using Orlan suits could only be done from the "Zvezda" service module, and American spacewalks using EMUs were only possible when a Space Shuttle was docked. The arrival of "Pirs" docking compartment on 16 September 2001 provided another airlock from which Orlan spacewalks can be conducted.

"Pirs" () and "Poisk" () are Russian airlock modules, each having two identical hatches. An outward-opening hatch on the "Mir" space station failed after it swung open too fast after unlatching, because of a small amount of air pressure remaining in the airlock. All EVA hatches on the ISS open inwards and are pressure-sealing. "Pirs" was used to store, service, and refurbish Russian Orlan suits and provided contingency entry for crew using the slightly bulkier American suits. The outermost docking ports on both airlocks allow docking of Soyuz and Progress spacecraft, and the automatic transfer of propellants to and from storage on the ROS.

"Pirs" was launched on 14 September 2001, as ISS Assembly Mission 4R, on a Russian Soyuz-U rocket, using a modified Progress spacecraft, Progress M-SO1, as an upper stage. "Poisk" was launched on 10 November 2009 attached to a modified Progress spacecraft, called Progress M-MIM2, on a Soyuz-U rocket from Launch Pad 1 at the Baikonur Cosmodrome in Kazakhstan.

"Harmony", also known as "Node 2", is the "utility hub" of the ISS. It connects the laboratory modules of the United States, Europe and Japan, as well as providing electrical power and electronic data. Sleeping cabins for four of the six crew are housed here.

"Harmony" was successfully launched into space aboard Space Shuttle flight STS-120 on 23 October 2007. After temporarily being attached to the port side of the "Unity", it was moved to its permanent location on the forward end of the Destiny laboratory on 14 November 2007. "Harmony" added to the station's living volume, an increase of almost 20 percent, from to . Its successful installation meant that from NASA's perspective, the station was "U.S. Core Complete".

"Tranquility", also known as Node 3, is a module of the ISS. It contains environmental control systems, life support systems, a toilet, exercise equipment, and an observation cupola.

ESA and the Italian Space Agency had "Tranquility" built by Thales Alenia Space. A ceremony on 20 November 2009 transferred ownership of the module to NASA. On 8 February 2010, NASA launched the module on the Space Shuttle's STS-130 mission.

"Columbus" is a science laboratory that is part of the ISS and is the largest single contribution to the ISS made by the European Space Agency (ESA).

The "Columbus" laboratory was flown to the Kennedy Space Center (KSC) in Florida in an Airbus Beluga. It was launched aboard on 7 February 2008 on flight STS-122. It is designed for ten years of operation. The module is controlled by the Columbus Control Centre, located at the German Space Operations Centre, part of the German Aerospace Center in Oberpfaffenhofen near Munich, Germany.

The European Space Agency has spent €1.4 billion (about US$2 billion) on building "Columbus", including the experiments that will orbit in "Columbus" and the ground control infrastructure necessary to operate the experiments.

The Japanese Experiment Module (JEM), nicknamed , is a Japanese science module for the ISS developed by JAXA. It is the largest single ISS module, and is attached to the "Harmony" module. The first two pieces of "Kibō" were launched on Space Shuttle missions STS-123 and STS-124. The third and final components were launched on STS-127.

The "Cupola" is an ESA-built observatory module of the ISS. Its name derives from the Italian word "", which means "dome". Its seven windows are used to conduct experiments, dockings and observations of Earth. It was launched aboard Space Shuttle mission STS-130 on 8 February 2010 and attached to the "Tranquility" (Node 3) module. With the "Cupola" attached, ISS assembly reached 85 percent completion. The "Cupola" central window has a diameter of .

"Rassvet" (; lit. "dawn"), also known as the Mini-Research Module 1 (MRM-1) (, ) and formerly known as the Docking Cargo Module (DCM), is a component of the ISS. The module's design is similar to the Mir Docking Module launched on STS-74 in 1995. "Rassvet" is primarily used for cargo storage and as a docking port for visiting spacecraft. It was flown to the ISS aboard on the STS-132 mission on 14 May 2010, and was connected to the ISS on 18 May. The hatch connecting "Rassvet" with the ISS was first opened on 20 May. On 28 June 2010, the Soyuz TMA-19 spacecraft performed the first docking with the module.

The "Leonardo" Permanent Multipurpose Module (PMM) is a module of the ISS. It was flown into space aboard the Space Shuttle on STS-133 on 24 February 2011 and installed on 1 March. "Leonardo" is primarily used for storage of spares, supplies and waste on the ISS, which was until then stored in many different places within the space station. The "Leonardo" PMM was a Multi-Purpose Logistics Module (MPLM) before 2011, but was modified into its current configuration. It was formerly one of three MPLM used for bringing cargo to and from the ISS with the Space Shuttle. The module was named for Italian polymath Leonardo da Vinci.

The Bigelow Expandable Activity Module (BEAM) is an experimental expandable space station module developed by Bigelow Aerospace, under contract to NASA, for testing as a temporary module on the ISS from 2016 to at least 2020. It arrived at the ISS on 10 April 2016, was berthed to the station on 16 April, and was expanded and pressurised on 28 May 2016.

The International Docking Adapter (IDA) is a spacecraft docking system adapter developed to convert APAS-95 to the NASA Docking System (NDS)/International Docking System Standard (IDSS). An IDA is placed on each of the ISS' two open Pressurised Mating Adapters (PMAs), both of which are connected to the "Harmony" module.

IDA-1 was lost during the launch failure of SpaceX CRS-7 on 28 June 2015.

IDA-2 was launched on SpaceX CRS-9 on 18 July 2016. It was attached and connected to PMA-2 during a spacewalk on 19 August 2016. First docking was achieved with the arrival of Crew Dragon Demo-1 on 3 March 2019.

IDA-3 was launched on the SpaceX CRS-18 mission in July 2019. IDA-3 is constructed mostly from spare parts to speed construction. It was attached and connected to PMA-3 during a spacewalk on 21 August 2019.

The ISS has a large number of external components that do not require pressurisation. The largest of these is the Integrated Truss Structure (ITS), to which the station's main solar arrays and thermal radiators are mounted. The ITS consists of ten separate segments forming a structure long.

The station was intended to have several smaller external components, such as six robotic arms, three External Stowage Platforms (ESPs) and four ExPRESS Logistics Carriers (ELCs). While these platforms allow experiments (including MISSE, the STP-H3 and the Robotic Refueling Mission) to be deployed and conducted in the vacuum of space by providing electricity and processing experimental data locally, their primary function is to store spare Orbital Replacement Units (ORUs). ORUs are parts that can be replaced when they fail or pass their design life, including pumps, storage tanks, antennas, and battery units. Such units are replaced either by astronauts during EVA or by robotic arms. Several shuttle missions were dedicated to the delivery of ORUs, including STS-129, STS-133 and STS-134. , only one other mode of transportation of ORUs had been utilised—the Japanese cargo vessel HTV-2—which delivered an FHRC and CTC-2 via its Exposed Pallet (EP).

There are also smaller exposure facilities mounted directly to laboratory modules; the "Kibō" Exposed Facility serves as an external "porch" for the "Kibō" complex, and a facility on the European "Columbus" laboratory provides power and data connections for experiments such as the European Technology Exposure Facility and the Atomic Clock Ensemble in Space. A remote sensing instrument, SAGE III-ISS, was delivered to the station in February 2017 aboard CRS-10, and the NICER experiment was delivered aboard CRS-11 in June 2017. The largest scientific payload externally mounted to the ISS is the Alpha Magnetic Spectrometer (AMS), a particle physics experiment launched on STS-134 in May 2011, and mounted externally on the ITS. The AMS measures cosmic rays to look for evidence of dark matter and antimatter.

The commercial "Bartolomeo" External Payload Hosting Platform, manufactured by Airbus, was launched on 6 March 2020 aboard CRS-20 and attached to the European "Columbus" module. It will provide an additional 12 external payload slots, supplementing the eight on the ExPRESS Logistics Carriers, ten on "Kibō", and four on "Columbus". The system is designed to be robotically serviced and will require no astronaut intervention. It is named after Christopher Columbus's younger brother.

The Integrated Truss Structure serves as a base for the station's primary remote manipulator system, called the Mobile Servicing System (MSS). The MSS is composed of three main components:

To gain access to the Russian Segment a grapple fixture was added to "Zarya" on STS-134, so that Canadarm2 can inchworm itself onto the ROS. Also installed during STS-134 was the Orbiter Boom Sensor System (OBSS), which had been used to inspect heat shield tiles on Space Shuttle missions and can be used on station to increase the reach of the MSS. Staff on Earth or the station can operate the MSS components via remote control, performing work outside the station without space walks.

Japan's Remote Manipulator System, which services the "Kibō" Exposed Facility, was launched on STS-124 and is attached to the "Kibō" Pressurised Module. The arm is similar to the Space Shuttle arm as it is permanently attached at one end and has a latching end effector for standard grapple fixtures at the other.

The European Robotic Arm, which will service the Russian Orbital Segment, will be launched alongside the Multipurpose Laboratory Module in 2020. The ROS does not require spacecraft or modules to be manipulated, as all spacecraft and modules dock automatically and may be discarded the same way. Crew use the two "Strela" (Russian: Стрела́; lit. Arrow) cargo cranes during EVAs for moving crew and equipment around the ROS. Each Strela crane has a mass of .

"Nauka" (; lit. "Science"), also known as the Multipurpose Laboratory Module (MLM), (Russian: "Многофункциональный лабораторный модуль", or "МЛМ"), is a component of the ISS that has yet to be launched into space. The MLM is funded by the Roscosmos State Corporation. In the original ISS plans, "Nauka" was to use the location of the Docking and Stowage Module (DSM), but the DSM was later replaced by the "Rassvet" module and moved to "Zarya"s nadir port. Planners now anticipate that "Nauka" will dock at "Zvezda"'s nadir port, replacing the "Pirs" module.

The launch of "Nauka", initially planned for 2007, has been repeatedly delayed for various reasons. , the launch to the ISS is assigned to no earlier than spring 2021. After this date, the warranties of some of "Nauka"s systems will expire.

"Prichal", also known as "Uzlovoy" Module or UM (, "Nodal Module Berth"), is a ball-shaped module that will allow docking of two scientific and power modules during the final stage of the station assembly, and provide the Russian segment additional docking ports to receive Soyuz MS and Progress MS spacecraft. UM is due to be launched in the third quarter of 2021. It will be integrated with a special version of the Progress cargo ship and launched by a standard Soyuz rocket, docking to the nadir port of the "Nauka" module. One port is equipped with an active hybrid docking port, which enables docking with the MLM module. The remaining five ports are passive hybrids, enabling docking of Soyuz and Progress vehicles, as well as heavier modules and future spacecraft with modified docking systems. The node module was intended to serve as the only permanent element of the cancelled OPSEK.

Science Power Module 1 (SPM-1, also known as NEM-1) and Science Power Module 2 (SPM-2, also known as NEM-2) are modules that are planned to arrive at the ISS not earlier than 2024. They will dock to the "Prichal" module, which is planned to be attached to the "Nauka" module. If "Nauka" is cancelled, then "Prichal", SPM-1, and SPM-2 would dock at the zenith port of the "Zvezda" module. SPM-1 and SPM-2 would also be required components for the OPSEK space station.

The NanoRacks Bishop Airlock Module is a commercially-funded airlock module intended to be launched to the ISS on SpaceX CRS-21 in August 2020. The module is being built by NanoRacks, Thales Alenia Space, and Boeing. It will be used to deploy CubeSats, small satellites, and other external payloads for NASA, CASIS, and other commercial and governmental customers.

In January 2020, NASA awarded Axiom Space a contract to build a commercial module for the ISS with a launch date of 2024. The contract is under the NextSTEP2 program. NASA negotiated with Axiom on a firm fixed-price contract basis to build and deliver the module, which will attach to the forward port of the space station's "Harmony (Node 2)" module. Although NASA has only commissioned one module, Axiom plans to build an entire segment consisting of five modules, including a node module, an orbital research and manufacturing facility, a crew habitat, and a "large-windowed Earth observatory". The Axiom segment is expected to greatly increase the capabilities and value of the space station, allowing for larger crews and private spaceflight by other organisations. Axiom plans to convert the segment into a stand-alone space station once the ISS is decommissioned, with the intention that this would act as a successor to the ISS.

Several modules planned for the station were cancelled over the course of the ISS programme. Reasons include budgetary constraints, the modules becoming unnecessary, and station redesigns after the 2003 "Columbia" disaster. The US Centrifuge Accommodations Module would have hosted science experiments in varying levels of artificial gravity. The US Habitation Module would have served as the station's living quarters. Instead, the living quarters are now spread throughout the station. The US Interim Control Module and ISS Propulsion Module would have replaced the functions of "Zvezda" in case of a launch failure. Two Russian Research Modules were planned for scientific research. They would have docked to a Russian Universal Docking Module. The Russian Science Power Platform would have supplied power to the Russian Orbital Segment independent of the ITS solar arrays.

The critical systems are the atmosphere control system, the water supply system, the food supply facilities, the sanitation and hygiene equipment, and fire detection and suppression equipment. The Russian Orbital Segment's life support systems are contained in the "Zvezda" service module. Some of these systems are supplemented by equipment in the USOS. The "Nauka" laboratory has a complete set of life support systems.

The atmosphere on board the ISS is similar to the Earth's. Normal air pressure on the ISS is ; the same as at sea level on Earth. An Earth-like atmosphere offers benefits for crew comfort, and is much safer than a pure oxygen atmosphere, because of the increased risk of a fire such as that responsible for the deaths of the Apollo 1 crew. Earth-like atmospheric conditions have been maintained on all Russian and Soviet spacecraft.

The "Elektron" system aboard "Zvezda" and a similar system in "Destiny" generate oxygen aboard the station. The crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters, a chemical oxygen generator system. Carbon dioxide is removed from the air by the Vozdukh system in "Zvezda". Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters.

Part of the ROS atmosphere control system is the oxygen supply. Triple-redundancy is provided by the Elektron unit, solid fuel generators, and stored oxygen. The primary supply of oxygen is the Elektron unit which produces and by electrolysis of water and vents overboard. The system uses approximately one litre of water per crew member per day. This water is either brought from Earth or recycled from other systems. "Mir" was the first spacecraft to use recycled water for oxygen production. The secondary oxygen supply is provided by burning -producing Vika cartridges (see also ISS ECLSS). Each 'candle' takes 5–20 minutes to decompose at , producing of . This unit is manually operated.

The US Orbital Segment has redundant supplies of oxygen, from a pressurised storage tank on the "Quest" airlock module delivered in 2001, supplemented ten years later by ESA-built Advanced Closed-Loop System (ACLS) in the "Tranquility" module (Node 3), which produces by electrolysis. Hydrogen produced is combined with carbon dioxide from the cabin atmosphere and converted to water and methane.

Double-sided solar arrays provide electrical power to the ISS. These bifacial cells collect direct sunlight on one side and light reflected off from the Earth on the other, and are more efficient and operate at a lower temperature than single-sided cells commonly used on Earth. 

The Russian segment of the station, like most spacecraft, uses 28 V low voltage DC from four rotating solar arrays mounted on "Zarya" and "Zvezda". The USOS uses 130–180 V DC from the USOS PV array, power is stabilised and distributed at 160 V DC and converted to the user-required 124 V DC. The higher distribution voltage allows smaller, lighter conductors, at the expense of crew safety. The two station segments share power with converters.

The USOS solar arrays are arranged as four wing pairs, for a total production of 75 to 90 kilowatts. These arrays normally track the sun to maximise power generation. Each array is about in area and long. In the complete configuration, the solar arrays track the sun by rotating the "alpha gimbal" once per orbit; the "beta gimbal" follows slower changes in the angle of the sun to the orbital plane. The Night Glider mode aligns the solar arrays parallel to the ground at night to reduce the significant aerodynamic drag at the station's relatively low orbital altitude.

The station originally used rechargeable nickel–hydrogen batteries () for continuous power during the 35 minutes of every 90-minute orbit that it is eclipsed by the Earth. The batteries are recharged on the day side of the orbit. They had a 6.5-year lifetime (over 37,000 charge/discharge cycles) and were regularly replaced over the anticipated 20-year life of the station. Starting in 2016, the nickel–hydrogen batteries were replaced by lithium-ion batteries, which are expected to last until the end of the ISS program.

The station's large solar panels generate a high potential voltage difference between the station and the ionosphere. This could cause arcing through insulating surfaces and sputtering of conductive surfaces as ions are accelerated by the spacecraft plasma sheath. To mitigate this, plasma contactor units (PCU)s create current paths between the station and the ambient plasma field.
The station's systems and experiments consume a large amount of electrical power, almost all of which is converted to heat. To keep the internal temperature within workable limits, a passive thermal control system (PTCS) is made of external surface materials, insulation such as MLI, and heat pipes. If the PTCS cannot keep up with the heat load, an External Active Thermal Control System (EATCS) maintains the temperature. The EATCS consists of an internal, non-toxic, water coolant loop used to cool and dehumidify the atmosphere, which transfers collected heat into an external liquid ammonia loop. From the heat exchangers, ammonia is pumped into external radiators that emit heat as infrared radiation, then back to the station. The EATCS provides cooling for all the US pressurised modules, including "Kibō" and "Columbus", as well as the main power distribution electronics of the S0, S1 and P1 trusses. It can reject up to 70 kW. This is much more than the 14 kW of the Early External Active Thermal Control System (EEATCS) via the Early Ammonia Servicer (EAS), which was launched on STS-105 and installed onto the P6 Truss.

Radio communications provide telemetry and scientific data links between the station and mission control centres. Radio links are also used during rendezvous and docking procedures and for audio and video communication between crew members, flight controllers and family members. As a result, the ISS is equipped with internal and external communication systems used for different purposes.

The Russian Orbital Segment communicates directly with the ground via the "Lira" antenna mounted to "Zvezda". The "Lira" antenna also has the capability to use the "Luch" data relay satellite system. This system fell into disrepair during the 1990s, and so was not used during the early years of the ISS, although two new "Luch" satellites—"Luch"-5A and "Luch"-5B—were launched in 2011 and 2012 respectively to restore the operational capability of the system. Another Russian communications system is the Voskhod-M, which enables internal telephone communications between "Zvezda", "Zarya", "Pirs", "Poisk", and the USOS and provides a VHF radio link to ground control centres via antennas on "Zvezda" exterior.

The US Orbital Segment (USOS) makes use of two separate radio links mounted in the Z1 truss structure: the S band (audio) and K band (audio, video and data) systems. These transmissions are routed via the United States Tracking and Data Relay Satellite System (TDRSS) in geostationary orbit, allowing for almost continuous real-time communications with Christopher C. Kraft Jr. Mission Control Center (MCC-H) in Houston. Data channels for the Canadarm2, European "Columbus" laboratory and Japanese "Kibō" modules were originally also routed via the S band and K band systems, with the European Data Relay System and a similar Japanese system intended to eventually complement the TDRSS in this role. Communications between modules are carried on an internal wireless network.
UHF radio is used by astronauts and cosmonauts conducting EVAs and other spacecraft that dock to or undock from the station. Automated spacecraft are fitted with their own communications equipment; the ATV uses a laser attached to the spacecraft and the Proximity Communications Equipment attached to "Zvezda" to accurately dock with the station.

The ISS is equipped with about 100 IBM/Lenovo ThinkPad and HP ZBook 15 laptop computers. The laptops have run Windows 95, Windows 2000, Windows XP, Windows 7, Windows 10 and Linux operating systems. Each computer is a commercial off-the-shelf purchase which is then modified for safety and operation including updates to connectors, cooling and power to accommodate the station's 28V DC power system and weightless environment. Heat generated by the laptops does not rise but stagnates around the laptop, so additional forced ventilation is required. Laptops aboard the ISS are connected to the station's wireless LAN via Wi-Fi and ethernet, which connects to the ground via K band. While originally this provided speeds of 10 Mbit/s download and 3 Mbit/s upload from the station, NASA upgraded the system in late August 2019 and increased the speeds to 600 Mbit/s. Laptop hard drives occasionally fail and must be replaced. Other computer hardware failures include instances in 2001, 2007 and 2017; some of these failures have required EVAs to replace computer modules in externally mounted devices.

The operating system used for key station functions is the Debian Linux distribution. The migration from Microsoft Windows was made in May 2013 for reasons of reliability, stability and flexibility.

In 2017, an SG100 Cloud Computer was launched to the ISS as part of OA-7 mission. It was manufactured by NCSIST of Taiwan and designed in collaboration with Academia Sinica, and National Central University under contract for NASA.

Each permanent crew is given an expedition number. Expeditions run up to six months, from launch until undocking, an 'increment' covers the same time period, but includes cargo ships and all activities. Expeditions 1 to 6 consisted of three-person crews. Expeditions 7 to 12 were reduced to the safe minimum of two following the destruction of the NASA Shuttle Columbia. From Expedition 13 the crew gradually increased to six around 2010. With the planned arrival of crew on US commercial vehicles in the early 2020s, expedition size may be increased to seven crew members, the number ISS is designed for.

Gennady Padalka, member of Expeditions 9, 19/20, 31/32, and 43/44, and Commander of Expedition 11, has spent more time in space than anyone else, a total of 878 days, 11 hours, and 29 minutes. Peggy Whitson has spent the most time in space of any American, totalling 665 days, 22 hours, and 22 minutes during her time on Expeditions 5, 16, and 50/51/52.

Travellers who pay for their own passage into space are termed spaceflight participants by Roscosmos and NASA, and are sometimes referred to as "space tourists", a term they generally dislike. All seven were transported to the ISS on Russian Soyuz spacecraft. When professional crews change over in numbers not divisible by the three seats in a Soyuz, and a short-stay crewmember is not sent, the spare seat is sold by MirCorp through Space Adventures. When the space shuttle retired in 2011, and the station's crew size was reduced to six, space tourism was halted, as the partners relied on Russian transport seats for access to the station. Soyuz flight schedules increase after 2013, allowing five Soyuz flights (15 seats) with only two expeditions (12 seats) required. The remaining seats are sold for around to members of the public who can pass a medical exam. ESA and NASA criticised private spaceflight at the beginning of the ISS, and NASA initially resisted training Dennis Tito, the first person to pay for his own passage to the ISS. 

Anousheh Ansari became the first Iranian in space and the first self-funded woman to fly to the station. Officials reported that her education and experience make her much more than a tourist, and her performance in training had been "excellent." Ansari herself dismisses the idea that she is a tourist. She did Russian and European studies involving medicine and microbiology during her 10-day stay. The documentary "Space Tourists" follows her journey to the station, where she fulfilled "an age-old dream of man: to leave our planet as a "normal person" and travel into outer space."

In 2008, spaceflight participant Richard Garriott placed a geocache aboard the ISS during his flight. This is currently the only non-terrestrial geocache in existence. At the same time, the Immortality Drive, an electronic record of eight digitised human DNA sequences, was placed aboard the ISS.

]

A wide variety of crewed and uncrewed spacecraft have supported the station's activities. Thirty-seven Space Shuttle ISS flights were conducted before retirement. 75 Progress resupply spacecraft (including the modified M-MIM2 and M-SO1 module transports), 59 crewed Soyuz spacecraft, five European ATV, nine Japanese HTV 'Kounotori', 20 SpaceX Dragon, and 13 Northrop Grumman Cygnus have flown to the ISS.




All Russian spacecraft and self-propelled modules are able to rendezvous and dock to the space station without human intervention using the Kurs radar docking system from over 200 kilometres away. The European ATV uses star sensors and GPS to determine its intercept course. When it catches up it uses laser equipment to optically recognise "Zvezda", along with the Kurs system for redundancy. Crew supervise these craft, but do not intervene except to send abort commands in emergencies. Progress and ATV supply craft can remain at the ISS for six months, allowing great flexibility in crew time for loading and unloading of supplies and trash.

From the initial station programs, the Russians pursued an automated docking methodology that used the crew in override or monitoring roles. Although the initial development costs were high, the system has become very reliable with standardisations that provide significant cost benefits in repetitive operations.

Soyuz spacecraft used for crew rotation also serve as lifeboats for emergency evacuation; they are replaced every six months and were used after the "Columbia" disaster to return stranded crew from the ISS. Expeditions require, on average, of supplies, and , crews had consumed a total of around . Soyuz crew rotation flights and Progress resupply flights visit the station on average two and three times respectively each year.

Other vehicles berth instead of docking. The Japanese H-II Transfer Vehicle parks itself in progressively closer orbits to the station, and then awaits 'approach' commands from the crew, until it is close enough for a robotic arm to grapple and berth the vehicle to the USOS. Berthed craft can transfer International Standard Payload Racks. Japanese spacecraft berth for one to two months. The berthing Cygnus and SpaceX Dragon were contracted to fly cargo to the station under the phase 1 of the Commercial Resupply Services program.

From 26 February 2011 to 7 March 2011 four of the governmental partners (United States, ESA, Japan and Russia) had their spacecraft (NASA Shuttle, ATV, HTV, Progress and Soyuz) docked at the ISS, the only time this has happened to date. On 25 May 2012, SpaceX delivered the first commercial cargo with a Dragon spacecraft.

Prior to a ship's docking to the ISS, navigation and attitude control (GNC) is handed over to the ground control of the ship's country of origin. GNC is set to allow the station to drift in space, rather than fire its thrusters or turn using gyroscopes. The solar panels of the station are turned edge-on to the incoming ships, so residue from its thrusters does not damage the cells. Before its retirement, Shuttle launches were often given priority over Soyuz, with occasional priority given to Soyuz arrivals carrying crew and time-critical cargoes, such as biological experiment materials.

The components of the ISS are operated and monitored by their respective space agencies at mission control centres across the globe, including:

Orbital Replacement Units (ORUs) are spare parts that can be readily replaced when a unit either passes its design life or fails. Examples of ORUs are pumps, storage tanks, controller boxes, antennas, and battery units. Some units can be replaced using robotic arms. Most are stored outside the station, either on small pallets called ExPRESS Logistics Carriers (ELCs) or share larger platforms called External Stowage Platforms which also hold science experiments. Both kinds of pallets provide electricity for many parts that could be damaged by the cold of space and require heating. The larger logistics carriers also have local area network (LAN) connections for telemetry to connect experiments. A heavy emphasis on stocking the USOS with ORU's occurred around 2011, before the end of the NASA shuttle programme, as its commercial replacements, Cygnus and Dragon, carry one tenth to one quarter the payload.

Unexpected problems and failures have impacted the station's assembly time-line and work schedules leading to periods of reduced capabilities and, in some cases, could have forced abandonment of the station for safety reasons. Serious problems include an air leak from the USOS in 2004, the venting of fumes from an "Elektron" oxygen generator in 2006, and the failure of the computers in the ROS in 2007 during STS-117 that left the station without thruster, "Elektron", "Vozdukh" and other environmental control system operations. In the latter case, the root cause was found to be condensation inside electrical connectors leading to a short-circuit.

During STS-120 in 2007 and following the relocation of the P6 truss and solar arrays, it was noted during the solar array had torn and was not deploying properly. An EVA was carried out by Scott Parazynski, assisted by Douglas Wheelock. Extra precautions were taken to reduce the risk of electric shock, as the repairs were carried out with the solar array exposed to sunlight. The issues with the array were followed in the same year by problems with the starboard Solar Alpha Rotary Joint (SARJ), which rotates the arrays on the starboard side of the station. Excessive vibration and high-current spikes in the array drive motor were noted, resulting in a decision to substantially curtail motion of the starboard SARJ until the cause was understood. Inspections during EVAs on STS-120 and STS-123 showed extensive contamination from metallic shavings and debris in the large drive gear and confirmed damage to the large metallic bearing surfaces, so the joint was locked to prevent further damage. Repairs to the joints were carried out during STS-126 with lubrication and the replacement of 11 out of 12 trundle bearings on the joint.

In September 2008, damage to the S1 radiator was first noticed in Soyuz imagery. The problem was initially not thought to be serious. The imagery showed that the surface of one sub-panel has peeled back from the underlying central structure, possibly because of micro-meteoroid or debris impact. On 15 May 2009 the damaged radiator panel's ammonia tubing was mechanically shut off from the rest of the cooling system by the computer-controlled closure of a valve. The same valve was then used to vent the ammonia from the damaged panel, eliminating the possibility of an ammonia leak. It is also known that a Service Module thruster cover struck the S1 radiator after being jettisoned during an EVA in 2008, but its effect, if any, has not been determined.

Early on 1 August 2010, a failure in cooling Loop A (starboard side), one of two external cooling loops, left the station with only half of its normal cooling capacity and zero redundancy in some systems. The problem appeared to be in the ammonia pump module that circulates the ammonia cooling fluid. Several subsystems, including two of the four CMGs, were shut down.

Planned operations on the ISS were interrupted through a series of EVAs to address the cooling system issue. A first EVA on 7 August 2010, to replace the failed pump module, was not fully completed because of an ammonia leak in one of four quick-disconnects. A second EVA on 11 August successfully removed the failed pump module. A third EVA was required to restore Loop A to normal functionality.

The USOS's cooling system is largely built by the US company Boeing, which is also the manufacturer of the failed pump.

The four Main Bus Switching Units (MBSUs, located in the S0 truss), control the routing of power from the four solar array wings to the rest of the ISS. Each MBSU has two power channels that feed 160V DC from the arrays to two DC-to-DC power converters (DDCUs) that supply the 124V power used in the station. In late 2011 MBSU-1 ceased responding to commands or sending data confirming its health. While still routing power correctly, it was scheduled to be swapped out at the next available EVA. A spare MBSU was already on board, but a 30 August 2012 EVA failed to be completed when a bolt being tightened to finish installation of the spare unit jammed before the electrical connection was secured. The loss of MBSU-1 limited the station to 75% of its normal power capacity, requiring minor limitations in normal operations until the problem could be addressed.

On 5 September 2012, in a second six-hour EVA, astronauts Sunita Williams and Akihiko Hoshide successfully replaced MBSU-1 and restored the ISS to 100% power.

On 24 December 2013, astronauts installed a new ammonia pump for the station's cooling system. The faulty cooling system had failed earlier in the month, halting many of the station's science experiments. Astronauts had to brave a "mini blizzard" of ammonia while installing the new pump. It was only the second Christmas Eve spacewalk in NASA history.

A typical day for the crew begins with a wake-up at 06:00, followed by post-sleep activities and a morning inspection of the station. The crew then eats breakfast and takes part in a daily planning conference with Mission Control before starting work at around 08:10. The first scheduled exercise of the day follows, after which the crew continues work until 13:05. Following a one-hour lunch break, the afternoon consists of more exercise and work before the crew carries out its pre-sleep activities beginning at 19:30, including dinner and a crew conference. The scheduled sleep period begins at 21:30. In general, the crew works ten hours per day on a weekday, and five hours on Saturdays, with the rest of the time their own for relaxation or work catch-up.

The time zone used aboard the ISS is Coordinated Universal Time (UTC). The windows are covered at night hours to give the impression of darkness because the station experiences 16 sunrises and sunsets per day. During visiting Space Shuttle missions, the ISS crew mostly follows the shuttle's Mission Elapsed Time (MET), which is a flexible time zone based on the launch time of the Space Shuttle mission.

The station provides crew quarters for each member of the expedition's crew, with two 'sleep stations' in the "Zvezda" and four more installed in "Harmony". The USOS quarters are private, approximately person-sized soundproof booths. The ROS crew quarters include a small window, but provide less ventilation and sound proofing. A crew member can sleep in a crew quarter in a tethered sleeping bag, listen to music, use a laptop, and store personal items in a large drawer or in nets attached to the module's walls. The module also provides a reading lamp, a shelf and a desktop. Visiting crews have no allocated sleep module, and attach a sleeping bag to an available space on a wall. It is possible to sleep floating freely through the station, but this is generally avoided because of the possibility of bumping into sensitive equipment. It is important that crew accommodations be well ventilated; otherwise, astronauts can wake up oxygen-deprived and gasping for air, because a bubble of their own exhaled carbon dioxide has formed around their heads. During various station activities and crew rest times, the lights in the ISS can be dimmed, switched off, and colour temperatures adjusted.

On the USOS, most of the food aboard is vacuum sealed in plastic bags; cans are rare because they are heavy and expensive to transport. Preserved food is not highly regarded by the crew and taste is reduced in microgravity, so efforts are taken to make the food more palatable, including using more spices than in regular cooking. The crew looks forward to the arrival of any ships from Earth as they bring fresh fruit and vegetables. Care is taken that foods do not create crumbs, and liquid condiments are preferred over solid to avoid contaminating station equipment. Each crew member has individual food packages and cooks them using the on-board galley. The galley features two food warmers, a refrigerator (added in November 2008), and a water dispenser that provides both heated and unheated water. Drinks are provided as dehydrated powder that is mixed with water before consumption. Drinks and soups are sipped from plastic bags with straws, while solid food is eaten with a knife and fork attached to a tray with magnets to prevent them from floating away. Any food that floats away, including crumbs, must be collected to prevent it from clogging the station's air filters and other equipment.
Showers on space stations were introduced in the early 1970s on "Skylab" and "Salyut" 3. By "Salyut" 6, in the early 1980s, the crew complained of the complexity of showering in space, which was a monthly activity. The ISS does not feature a shower; instead, crewmembers wash using a water jet and wet wipes, with soap dispensed from a toothpaste tube-like container. Crews are also provided with rinseless shampoo and edible toothpaste to save water.

There are two space toilets on the ISS, both of Russian design, located in "Zvezda" and "Tranquility". These Waste and Hygiene Compartments use a fan-driven suction system similar to the Space Shuttle Waste Collection System. Astronauts first fasten themselves to the toilet seat, which is equipped with spring-loaded restraining bars to ensure a good seal. A lever operates a powerful fan and a suction hole slides open: the air stream carries the waste away. Solid waste is collected in individual bags which are stored in an aluminium container. Full containers are transferred to Progress spacecraft for disposal. Liquid waste is evacuated by a hose connected to the front of the toilet, with anatomically correct "urine funnel adapters" attached to the tube so that men and women can use the same toilet. The diverted urine is collected and transferred to the Water Recovery System, where it is recycled into drinking water.

On 12 April 2019, NASA reported medical results from the Astronaut Twin Study. One astronaut twin spent a year in space on the ISS, while the other twin spent the year on Earth. Several long-lasting changes were observed, including those related to alterations in DNA and cognition, when one twin was compared with the other.

In November 2019, researchers reported that astronauts experienced serious blood flow and clot problems while on board the International Space Station, based on a six-month study of 11 healthy astronauts. The results may influence long-term spaceflight, including a mission to the planet Mars, according to the researchers.

The ISS is partially protected from the space environment by Earth's magnetic field. From an average distance of about , depending on Solar activity, the magnetosphere begins to deflect solar wind around Earth and ISS. Solar flares are still a hazard to the crew, who may receive only a few minutes warning. In 2005, during the initial 'proton storm' of an X-3 class solar flare, the crew of Expedition 10 took shelter in a more heavily shielded part of the ROS designed for this purpose.

Subatomic charged particles, primarily protons from cosmic rays and solar wind, are normally absorbed by Earth's atmosphere. When they interact in sufficient quantity, their effect is visible to the naked eye in a phenomenon called an aurora. Outside Earth's atmosphere, crews are exposed to about 1 millisievert each day, which is about a year of natural exposure on Earth, resulting in a higher risk of cancer. Radiation can penetrate living tissue and damage the DNA and chromosomes of lymphocytes. These cells are central to the immune system, and so any damage to them could contribute to the lower immunity experienced by astronauts. Radiation has also been linked to a higher incidence of cataracts in astronauts. Protective shielding and drugs may lower risks to an acceptable level.

Radiation levels on the ISS are about five times greater than those experienced by airline passengers and crew, as Earth's electromagnetic field provides almost the same level of protection against solar and other radiation in low Earth orbit as in the stratosphere. For example, on a 12-hour flight an airline passenger would experience 0.1 millisieverts of radiation, or a rate of 0.2 millisieverts per day; only 1/5 the rate experienced by an astronaut in LEO. Additionally, airline passengers experience this level of radiation for a few hours of flight, while ISS crew are exposed for their whole stay.

There is considerable evidence that psychosocial stressors are among the most important impediments to optimal crew morale and performance. Cosmonaut Valery Ryumin wrote in his journal during a particularly difficult period on board the "Salyut" 6 space station: "All the conditions necessary for murder are met if you shut two men in a cabin measuring 18 feet by 20 and leave them together for two months."

NASA's interest in psychological stress caused by space travel, initially studied when their crewed missions began, was rekindled when astronauts joined cosmonauts on the Russian space station "Mir". Common sources of stress in early US missions included maintaining high performance under public scrutiny and isolation from peers and family. The latter is still often a cause of stress on the ISS, such as when the mother of NASA Astronaut Daniel Tani died in a car accident, and when Michael Fincke was forced to miss the birth of his second child.

A study of the longest spaceflight concluded that the first three weeks are a critical period where attention is adversely affected because of the demand to adjust to the extreme change of environment. ISS crew flights typically last about five to six months.

The ISS working environment includes further stress caused by living and working in cramped conditions with people from very different cultures who speak a different language. First-generation space stations had crews who spoke a single language; second- and third-generation stations have crew from many cultures who speak many languages. Astronauts must speak English and Russian, and knowing additional languages is even better.

Due to the lack of gravity, confusion often occurs. Even though there is no up and down in space, some crew members feel like they are oriented upside down. They may also have difficulty measuring distances. This can cause problems like getting lost inside the space station, pulling switches in the wrong direction or misjudging the speed of an approaching vehicle during docking.

The physiological effects of long-term weightlessness include muscle atrophy, deterioration of the skeleton (osteopenia), fluid redistribution, a slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, and puffiness of the face.

Sleep is regularly disturbed on the ISS because of mission demands, such as incoming or departing ships. Sound levels in the station are unavoidably high. The atmosphere is unable to thermosiphon naturally, so fans are required at all times to process the air which would stagnate in the freefall (zero-G) environment.

To prevent some of the adverse effects on the body, the station is equipped with: two TVIS treadmills (including the COLBERT); the ARED (Advanced Resistive Exercise Device), which enables various weightlifting exercises that add muscle without raising (or compensating for) the astronauts' reduced bone density; and a stationary bicycle. Each astronaut spends at least two hours per day exercising on the equipment. Astronauts use bungee cords to strap themselves to the treadmill.

Hazardous moulds that can foul air and water filters may develop aboard space stations. They can produce acids that degrade metal, glass, and rubber. They can also be harmful to the crew's health. Microbiological hazards have led to a development of the LOCAD-PTS which identifies common bacteria and moulds faster than standard methods of culturing, which may require a sample to be sent back to Earth. Researchers in 2018 reported, after detecting the presence of five "Enterobacter bugandensis" bacterial strains on the ISS (none of which are pathogenic to humans), that microorganisms on the ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.

Contamination on space stations can be prevented by reduced humidity, and by using paint that contains mould-killing chemicals, as well as the use of antiseptic solutions. All materials used in the ISS are tested for resistance against fungi.

In April 2019, NASA reported that a comprehensive study had been conducted into the microorganisms and fungi present on the ISS. The results may be useful in improving the health and safety conditions for astronauts.

Space flight is not inherently quiet, with noise levels exceeding acoustic standards as far back as the Apollo missions. For this reason, NASA and the International Space Station international partners have developed noise control and hearing loss prevention goals as part of the health program for crew members. Specifically, these goals have been the primary focus of the ISS Multilateral Medical Operations Panel (MMOP) Acoustics Subgroup since the first days of ISS assembly and operations. The effort includes contributions from acoustical engineers, audiologists, industrial hygienists, and physicians who comprise the subgroup's membership from NASA, the Russian Space Agency (RSA), the European Space Agency (ESA), the Japanese Aerospace Exploration Agency (JAXA), and the Canadian Space Agency (CSA).

When compared to terrestrial environments, the noise levels incurred by astronauts and cosmonauts on the ISS may seem insignificant and typically occur at levels that would not be of major concern to the Occupational Safety and Health Administration – rarely reaching 85 dBA. But crew members are exposed to these levels 24 hours a day, seven days a week, with current missions averaging six months in duration. These levels of noise also impose risks to crew health and performance in the form of sleep interference and communication, as well as reduced alarm audibility.

Over the 19 plus year history of the ISS, significant efforts have been put forth to limit and reduce noise levels on the ISS. During design and pre-flight activities, members of the Acoustic Subgroup have written acoustic limits and verification requirements, consulted to design and choose quietest available payloads, and then conducted acoustic verification tests prior to launch. During spaceflights, the Acoustics Subgroup has assessed each ISS module's in flight sound levels, produced by a large number of vehicle and science experiment noise sources, to assure compliance with strict acoustic standards. The acoustic environment on ISS changed when additional modules were added during its construction, and as additional spacecraft arrive at the ISS. The Acoustics Subgroup has responded to this dynamic operations schedule by successfully designing and employing acoustic covers, absorptive materials, noise barriers, and vibration isolators to reduce noise levels. Moreover, when pumps, fans, and ventilation systems age and show increased noise levels, this Acoustics Subgroup has guided ISS managers to replace the older, noisier instruments with quiet fan and pump technologies, significantly reducing ambient noise levels.

NASA has adopted most-conservative damage risk criteria (based on recommendations from the National Institute for Occupational Safety and Health and the World Health Organization), in order to protect all crew members. The MMOP Acoustics Subgroup has adjusted its approach to managing noise risks in this unique environment by applying, or modifying, terrestrial approaches for hearing loss prevention to set these conservative limits. One innovative approach has been NASA's Noise Exposure Estimation Tool (NEET), in which noise exposures are calculated in a task-based approach to determine the need for hearing protection devices (HPDs). Guidance for use of HPDs, either mandatory use or recommended, is then documented in the Noise Hazard Inventory, and posted for crew reference during their missions. The Acoustics Subgroup also tracks spacecraft noise exceedances, applies engineering controls, and recommends hearing protective devices to reduce crew noise exposures. Finally, hearing thresholds are monitored on-orbit, during missions .

There have been no persistent mission-related hearing threshold shifts among US Orbital Segment crewmembers (JAXA, CSA, ESA, NASA) during what is approaching 20 years of ISS mission operations, or nearly 175,000 work hours. In 2020, the MMOP Acoustics Subgroup received the Safe-In-Sound Award for Innovation for their combined efforts to mitigate any health effects of noise.

An onboard fire or a toxic gas leak are other potential hazards. Ammonia is used in the external radiators of the station and could potentially leak into the pressurised modules.

The ISS is maintained in a nearly circular orbit with a minimum mean altitude of and a maximum of , in the centre of the thermosphere, at an inclination of 51.6 degrees to Earth's equator. This orbit was selected because it is the lowest inclination that can be directly reached by Russian Soyuz and Progress spacecraft launched from Baikonur Cosmodrome at 46° N latitude without overflying China or dropping spent rocket stages in inhabited areas.
It travels at an average speed of , and completes orbits per day (93 minutes per orbit). The station's altitude was allowed to fall around the time of each NASA shuttle flight to permit heavier loads to be transferred to the station. After the retirement of the shuttle, the nominal orbit of the space station was raised in altitude. Other, more frequent supply ships do not require this adjustment as they are substantially higher performance vehicles.

Orbital boosting can be performed by the station's two main engines on the "Zvezda" service module, or Russian or European spacecraft docked to "Zvezda" aft port. The Automated Transfer Vehicle is constructed with the possibility of adding a second docking port to its aft end, allowing other craft to dock and boost the station. It takes approximately two orbits (three hours) for the boost to a higher altitude to be completed. Maintaining ISS altitude uses about 7.5 tonnes of chemical fuel per annum at an annual cost of about $210 million.

The Russian Orbital Segment contains the Data Management System, which handles Guidance, Navigation and Control (ROS GNC) for the entire station. Initially, "Zarya", the first module of the station, controlled the station until a short time after the Russian service module "Zvezda" docked and was transferred control. "Zvezda" contains the ESA built DMS-R Data Management System. Using two fault-tolerant computers (FTC), "Zvezda" computes the station's position and orbital trajectory using redundant Earth horizon sensors, Solar horizon sensors as well as Sun and star trackers. The FTCs each contain three identical processing units working in parallel and provide advanced fault-masking by majority voting.

"Zvezda" uses gyroscopes (reaction wheels) and thrusters to turn itself around. Gyroscopes do not require propellant; instead they use electricity to 'store' momentum in flywheels by turning in the opposite direction to the station's movement. The USOS has its own computer-controlled gyroscopes to handle its extra mass. When gyroscopes 'saturate', thrusters are used to cancel out the stored momentum. In February 2005, during Expedition 10, an incorrect command was sent to the station's computer, using about 14 kilograms of propellant before the fault was noticed and fixed. When attitude control computers in the ROS and USOS fail to communicate properly, this can result in a rare 'force fight' where the ROS GNC computer must ignore the USOS counterpart, which itself has no thrusters.

Docked spacecraft can also be used to maintain station attitude, such as for troubleshooting or during the installation of the S3/S4 truss, which provides electrical power and data interfaces for the station's electronics.

The low altitudes at which the ISS orbits are also home to a variety of space debris, including spent rocket stages, defunct satellites, explosion fragments (including materials from anti-satellite weapon tests), paint flakes, slag from solid rocket motors, and coolant released by US-A nuclear-powered satellites. These objects, in addition to natural micrometeoroids, are a significant threat. Objects large enough to destroy the station can be tracked, and are not as dangerous as smaller debris. Objects too small to be detected by optical and radar instruments, from approximately 1 cm down to microscopic size, number in the trillions. Despite their small size, some of these objects are a threat because of their kinetic energy and direction in relation to the station. Spacewalking crew in spacesuits are also at risk of suit damage and consequent exposure to vacuum.

Ballistic panels, also called micrometeorite shielding, are incorporated into the station to protect pressurised sections and critical systems. The type and thickness of these panels depend on their predicted exposure to damage. The station's shields and structure have different designs on the ROS and the USOS. On the USOS, Whipple shields are used. The US segment modules consist of an inner layer made from 1.5–5.0 cm thick aluminum, a 10 cm thick intermediate layers of Kevlar and Nextel, and an outer layer of stainless steel, which causes objects to shatter into a cloud before hitting the hull, thereby spreading the energy of impact. On the ROS, a Carbon fiber reinforced polymer honeycomb screen is spaced from the hull, an aluminium honeycomb screen is spaced from that, with a screen-vacuum thermal insulation covering, and glass cloth over the top.

Space debris is tracked remotely from the ground, and the station crew can be notified. If necessary, thrusters on the Russian Orbital Segment can alter the station's orbital altitude, avoiding the debris. These Debris Avoidance Manoeuvres (DAMs) are not uncommon, taking place if computational models show the debris will approach within a certain threat distance. Ten DAMs had been performed by the end of 2009. Usually, an increase in orbital velocity of the order of 1 m/s is used to raise the orbit by one or two kilometres. If necessary, the altitude can also be lowered, although such a manoeuvre wastes propellant. If a threat from orbital debris is identified too late for a DAM to be safely conducted, the station crew close all the hatches aboard the station and retreat into their Soyuz spacecraft in order to be able to evacuate in the event the station was seriously damaged by the debris. This partial station evacuation has occurred on 13 March 2009, 28 June 2011, 24 March 2012 and 16 June 2015.

The ISS is visible to the naked eye as a slow-moving, bright white dot because of reflected sunlight, and can be seen in the hours after sunset and before sunrise, when the station remains sunlit but the ground and sky are dark. The ISS takes about 10 minutes to pass from one horizon to another, and will only be visible part of that time because of moving into or out of the Earth's shadow. Because of the size of its reflective surface area, the ISS is the brightest artificial object in the sky (excluding other satellite flares), with an approximate maximum magnitude of −4 when overhead (similar to Venus). The ISS, like many satellites including the Iridium constellation, can also produce flares of up to 16 times the brightness of Venus as sunlight glints off reflective surfaces. The ISS is also visible in broad daylight, albeit with a great deal more difficulty.

Tools are provided by a number of websites such as Heavens-Above (see "Live viewing" below) as well as smartphone applications that use orbital data and the observer's longitude and latitude to indicate when the ISS will be visible (weather permitting), where the station will appear to rise, the altitude above the horizon it will reach and the duration of the pass before the station disappears either by setting below the horizon or entering into Earth's shadow.

In November 2012 NASA launched its "Spot the Station" service, which sends people text and email alerts when the station is due to fly above their town. The station is visible from 95% of the inhabited land on Earth, but is not visible from extreme northern or southern latitudes.

Using a telescope-mounted camera to photograph the station is a popular hobby for astronomers, while using a mounted camera to photograph the Earth and stars is a popular hobby for crew. The use of a telescope or binoculars allows viewing of the ISS during daylight hours.

Some amateur astronomers also use telescopic lenses to photograph the ISS while it transits the Sun, sometimes doing so during an eclipse (and so the Sun, Moon, and ISS are all positioned approximately in a single line). One example is during the 21 August solar eclipse, where at one location in Wyoming, images of the ISS were captured during the eclipse. Similar images were captured by NASA from a location in Washington.

Parisian engineer and astrophotographer Thierry Legault, known for his photos of spaceships transiting the Sun, travelled to Oman in 2011 to photograph the Sun, Moon and space station all lined up. Legault, who received the Marius Jacquemetton award from the Société astronomique de France in 1999, and other hobbyists, use websites that predict when the ISS will transit the Sun or Moon and from what location those passes will be visible.

Involving five space programs and fifteen countries, the International Space Station is the most politically and legally complex space exploration programme in history. The 1998 Space Station Intergovernmental Agreement sets forth the primary framework for international cooperation among the parties. A series of subsequent agreements govern other aspects of the station, ranging from jurisdictional issues to a code of conduct among visiting astronauts.

According to the Outer Space Treaty, the United States and Russia are legally responsible for all modules they have launched. Natural orbital decay with random reentry (as with "Skylab"), boosting the station to a higher altitude (which would delay reentry), and a controlled targeted de-orbit to a remote ocean area were considered as ISS disposal options. As of late 2010, the preferred plan is to use a slightly modified Progress spacecraft to de-orbit the ISS. This plan was seen as the simplest, cheapest and with the highest margin.

The Orbital Piloted Assembly and Experiment Complex (OPSEK) was previously intended to be constructed of modules from the Russian Orbital Segment after the ISS is decommissioned. The modules under consideration for removal from the current ISS included the Multipurpose Laboratory Module ("Nauka"), planned to be launched in spring 2021 , and the other new Russian modules that are proposed to be attached to "Nauka". These newly launched modules would still be well within their useful lives in 2024.

At the end of 2011, the Exploration Gateway Platform concept also proposed using leftover USOS hardware and "Zvezda 2" as a refuelling depot and service station located at one of the Earth-Moon Lagrange points. However, the entire USOS was not designed for disassembly and will be discarded.

In February 2015, Roscosmos announced that it would remain a part of the ISS programme until 2024. Nine months earlier—in response to US sanctions against Russia over the annexation of Crimea—Russian Deputy Prime Minister Dmitry Rogozin had stated that Russia would reject a US request to prolong the orbiting station's use beyond 2020, and would only supply rocket engines to the US for non-military satellite launches.

On 28 March 2015, Russian sources announced that Roscosmos and NASA had agreed to collaborate on the development of a replacement for the current ISS. Igor Komarov, the head of Russia's Roscosmos, made the announcement with NASA administrator Charles Bolden at his side. In a statement provided to SpaceNews on 28 March, NASA spokesman David Weaver said the agency appreciated the Russian commitment to extending the ISS, but did not confirm any plans for a future space station.

On 30 September 2015, Boeing's contract with NASA as prime contractor for the ISS was extended to 30 September 2020. Part of Boeing's services under the contract will relate to extending the station's primary structural hardware past 2020 to the end of 2028.

Regarding extending the ISS, on 15 November 2016 General Director Vladimir Solntsev of RSC Energia stated "Maybe the ISS will receive continued resources. Today we discussed the possibility of using the station until 2028", with discussion to continue under the new presidential administration. There have also been suggestions that the station could be converted to commercial operations after it is retired by government entities.

In July 2018, the Space Frontier Act of 2018 was intended to extend operations of the ISS to 2030. This bill was unanimously approved in the Senate, but failed to pass in the U.S. House. In September 2018, the Leading Human Spaceflight Act was introduced with the intent to extend operations of the ISS to 2030, and was confirmed in December 2018.

The ISS has been described as the most expensive single item ever constructed. As of 2010 the total cost was US$150 billion. This includes NASA's budget of $58.7 billion (inflation-unadjusted) for the station from 1985 to 2015 ($72.4 billion in 2010 dollars), Russia's $12 billion, Europe's $5 billion, Japan's $5 billion, Canada's $2 billion, and the cost of 36 shuttle flights to build the station, estimated at $1.4 billion each, or $50.4 billion in total. Assuming 20,000 person-days of use from 2000 to 2015 by two- to six-person crews, each person-day would cost $7.5 million, less than half the inflation-adjusted $19.6 million ($5.5 million before inflation) per person-day of "Skylab".









</doc>
<doc id="15044" url="https://en.wikipedia.org/wiki?curid=15044" title="Irish">
Irish

Irish most commonly refers to:

Irish may also refer to:




</doc>
<doc id="15045" url="https://en.wikipedia.org/wiki?curid=15045" title="Cosmicomics">
Cosmicomics

Cosmicomics () is a collection of twelve short stories by Italo Calvino first published in Italian in 1965 and in English in 1968. The stories were originally published between 1964 and 1965 in the Italian periodicals "Il Caffè" and "Il Giorno". Each story takes a scientific "fact" (though sometimes a falsehood by today's understanding), and builds an imaginative story around it. An always-extant being called Qfwfq narrates all of the stories save two, each of which is a memory of an event in the history of the universe. Qfwfq also narrates some stories in Calvino's "t zero".

All of the stories in "Cosmicomics", together with those from "t zero" and other sources, are now available in a single volume collection, "The Complete Cosmicomics" (Penguin UK, 2009).

The first U.S. edition, translated by William Weaver, won the National Book Award in the Translation category.


All of the stories feature non-human characters which have been heavily anthropomorphized.



</doc>
<doc id="15046" url="https://en.wikipedia.org/wiki?curid=15046" title="IA-32">
IA-32

IA-32 (short for "Intel Architecture, 32-bit", sometimes also called i386) is the 32-bit version of the x86 instruction set architecture, designed by Intel and first implemented in the 80386 microprocessor in 1985. IA-32 is the first incarnation of x86 that supports 32-bit computing; as a result, the "IA-32" term may be used as a metonym to refer to all x86 versions that support 32-bit computing.

Within various programming language directives, IA-32 is still sometimes referred to as the "i386" architecture. In some other contexts, certain iterations of the IA-32 ISA are sometimes labelled i486, i586 and i686, referring to the instruction supersets offered by the 80486, the P5 and the P6 microarchitectures respectively. These updates offered numerous additions alongside the base IA-32 set, i.e. floating-point capabilities and the MMX extensions.

Intel was historically the largest manufacturer of IA-32 processors, with the second biggest supplier having been AMD. During the 1990s, VIA, Transmeta and other chip manufacturers also produced IA-32 compatible processors (e.g. WinChip). In the modern era, Intel still produces IA-32 processors under the Intel Quark microcontroller platform, however, since the 2000s, the majority of manufacturers (Intel included) moved almost exclusively to implementing CPUs based on the 64-bit variant of x86, x86-64. x86-64, by specification, offers legacy operating modes that operate on the IA-32 ISA for backwards compatibility. Even given the contemporary prevalence of x86-64, as of 2018, IA-32 protected mode versions of many modern operating systems are still maintained, e.g. Microsoft Windows and the Debian Linux distribution. In spite of IA-32's name (and causing some potential confusion), the 64-bit evolution of x86 that originated out of AMD would not be known as "IA-64", that name instead belonging to Intel's Itanium architecture.

The primary defining characteristic of IA-32 is the availability of 32-bit general-purpose processor registers (for example, EAX and EBX), 32-bit integer arithmetic and logical operations, 32-bit offsets within a segment in protected mode, and the translation of segmented addresses to 32-bit linear addresses. The designers took the opportunity to make other improvements as well. Some of the most significant changes are described below.




</doc>
<doc id="15047" url="https://en.wikipedia.org/wiki?curid=15047" title="Internalism and externalism">
Internalism and externalism

Internalism and externalism are two opposing ways of explaining various subjects in several areas of philosophy. These include human motivation, knowledge, justification, meaning, and truth. The distinction arises in many areas of debate with similar but distinct meanings.

Internalism is the thesis that no fact about the world can provide reasons for action independently of desires and beliefs. Externalism is the thesis that reasons are to be identified with objective features of the world.

In contemporary moral philosophy, motivational internalism (or moral internalism) is the view that moral convictions (which are not necessarily beliefs, e.g. feelings of moral approval or disapproval) are intrinsically motivating. That is, the motivational internalist believes that there is an internal, necessary connection between one's conviction that X ought to be done and one's motivation to do X. Conversely, the motivational externalist (or moral externalist) claims that there is no necessary internal connection between moral convictions and moral motives. That is, there is no necessary connection between the conviction that X is wrong and the motivational drive not to do X. (The use of these terms has roots in W.D. Falk's (1947) paper "'Ought' and Motivation").

These views in moral psychology have various implications. In particular, if motivational internalism is true, then an amoralist is unintelligible (and metaphysically impossible). An amoralist is not simply someone who is immoral, rather it is someone who knows what the moral things to do are, yet is not motivated to do them. Such an agent is unintelligible to the motivational internalist, because moral judgments about the right thing to do have built into them corresponding motivations to do those things that are judged by the agent to be the moral things to do. On the other hand, an amoralist is entirely intelligible to the motivational "externalist", because the motivational externalist thinks that moral judgments about the right thing to do not necessitate some motivation to do those things that are judged to be the right thing to do; rather, an independent desire—such as the desire to do the right thing—is required (Brink, 2003), (Rosati, 2006).

There is also a distinction in ethics and action theory, largely made popular by Bernard Williams (1979, reprinted in 1981), concerning internal and external reasons for action. An "internal reason" is, roughly, something that one has in light of one's own "subjective motivational set"—one's own commitments, desires (or wants), goals, etc. On the other hand, an "external reason" is something that one has independent of one's subjective motivational set. For example, suppose that Sally is going to drink a glass of poison, because she wants to commit suicide and believes that she can do so by drinking the poison. Sally has an internal reason to drink the poison, because she wants to commit suicide. However, one might say that she has an external reason not to drink the poison because, even though she wants to die, one ought not kill oneself no matter what—regardless of whether one wants to die.

Some philosophers embrace the existence of both kinds of reason, while others deny the existence of one or the other. For example, Bernard Williams (1981) argues that there are really only internal reasons for action. Such a view is called "internalism about reasons" (or "reasons internalism"). "Externalism about reasons" (or "reasons externalism") is the denial of reasons internalism. It is the view that there are external reasons for action; that is, there are reasons for action that one can have even if the action is not part of one's subjective motivational set.

Consider the following situation. Suppose that it's against the moral law to steal from the poor, and Sasha knows this. However, Sasha doesn't desire to follow the moral law, and there is currently a poor person next to him. Is it intelligible to say that Sasha has a reason to follow the moral law right now (to not steal from the poor person next to him), even though he doesn't care to do so? The reasons externalist answers in the affirmative ("Yes, Sasha has a reason not to steal from that poor person."), since he believes that one can have reasons for action even if one does not have the relevant desire. Conversely, the reasons internalist answers the question in the negative ("No, Sasha does not have a reason not to steal from that poor person, though others might."). The reasons internalist claims that external reasons are unintelligible; one has a reason for action only if one has the relevant desire (that is, only internal reasons can be reasons for action). The reasons internalist claims the following: the moral facts are a reason "for Sasha's action" not to steal from the poor person next to him only if he currently "wants" to follow the moral law (or if not stealing from the poor person is a way to satisfy his other current goals—that is, part of what Williams calls his "subjective motivational set"). In short, the reasoning behind reasons internalism, according to Williams, is that reasons for action must be able to explain one's action; and only internal reasons can do this.

Generally speaking, internalist conceptions of epistemic justification require that one's justification for a belief be internal to the believer in some way. Two main varieties of epistemic internalism about justification are access internalism and ontological internalism. Access internalists require that a believer must have internal access to the justifier(s) of her belief "p" in order to be justified in believing "p". For the access internalist, justification amounts to something like the believer being aware (or capable of being aware) of certain facts that make her belief in "p" rational, or her being able to give reasons for her belief in "p". At minimum, access internalism requires that the believer have some kind of reflective access or awareness to whatever justifies her belief. Ontological internalism is the view that justification for a belief is established by one's mental states. Ontological internalism can be distinct from access internalism, but the two are often thought to go together since we are generally considered to be capable of having reflective access to mental states.

One popular argument for internalism is known as the 'new evil demon problem'. The new evil demon problem indirectly supports internalism by challenging externalist views of justification, particularly reliabilism. The argument asks us to imagine a subject with beliefs and experiences identical to ours, but the subject is being systematically deceived by a malicious Cartesian demon so that all their beliefs turn out false. In spite of the subject's unfortunate deception, the argument goes, we do not think this subject ceases to be rational in taking things to be as they appear as we do. After all, it is possible that we could be radically deceived in the same way, yet we are still justified in holding most of our beliefs in spite of this possibility. Since reliabilism maintains that one's beliefs are justified via reliable belief-forming processes (where reliable means yielding true beliefs), the subject in the evil demon scenario would not likely have any justified beliefs according to reliabilism because all of their beliefs would be false. Since this result is supposed to clash with our intuitions that the subject is justified in their beliefs in spite of being systematically deceived, some take the new evil demon problem as a reason for rejecting externalist views of justification.

Externalist views of justification emerged in epistemology during the late 20th century. Externalist conceptions of justification assert that facts external to the believer can serve as the justification for a belief. According to the externalist, a believer need not have any internal access or cognitive grasp of any reasons or facts which make their belief justified. The externalist's assessment of justification can be contrasted with access internalism, which demands that the believer have internal reflective access to reasons or facts which corroborate their belief in order to be justified in holding it. Externalism, on the other hand, maintains that the justification for someone's belief can come from facts that are entirely external to the agent's subjective awareness.

Alvin Goldman, one of the most well-known proponents of externalism in epistemology, is known for developing a popular form of externalism called reliabilism. In his paper, “What is Justified Belief?” Goldman characterizes the reliabilist conception of justification as such:

"If S’s believing "p" at "t" results from a reliable cognitive belief-forming process (or set of processes), then S’s belief in "p" at "t" is justified.”

Goldman notes that a reliable belief-forming process is one which generally produces true beliefs.

A unique consequence of reliabilism (and other forms of externalism) is that one can have a justified belief without knowing one is justified (this is not possible under most forms of epistemic internalism). In addition, we do not yet know which cognitive processes are in fact reliable, so anyone who embraces reliabilism must concede that we do not always know whether some of our beliefs are justified (even though there is a fact of the matter).

In responding to skepticism, Hilary Putnam (1982) claims that semantic externalism yields "an argument we can give that shows we are not brains in a vat (BIV). (See also DeRose, 1999.) If semantic externalism is true, then the meaning of a word or sentence is not wholly determined by what individuals think those words mean. For example, semantic externalists maintain that the word "water" referred to the substance whose chemical composition is HO even before scientists had discovered that chemical composition. The fact that the substance out in the world we were calling "water" actually had that composition at least partially determined the meaning of the word. One way to use this in a response to skepticism is to apply the same strategy to the terms used in a skeptical argument in the following way (DeRose, 1999):

To clarify how this argument is supposed to work: Imagine that there is brain in a vat, and a whole world is being simulated for it. Call the individual who is being deceived "Steve." When Steve is given an experience of walking through a park, semantic externalism allows for his thought, "I am walking through a park" to be true so long as the simulated reality is one in which he is walking through a park. Similarly, what it takes for his thought, "I am a brain in a vat," to be true is for the simulated reality to be one where he is a brain in a vat. But in the simulated reality, he is not a brain in a vat.

Apart from disputes over the success of the argument or the plausibility of the specific type of semantic externalism required for it to work, there is question as to what is gained by defeating the skeptical worry with this strategy. Skeptics can give new skeptical cases that wouldn't be subject to the same response (e.g., one where the person was very recently turned into a brain in a vat, so that their words "brain" and "vat" still pick out real brains and vats, rather than simulated ones). Further, if even brains in vats can correctly believe "I am not a brain in a vat," then the skeptic can still press us on how we know we are not in that situation (though the externalist will point out that it may be difficult for the skeptic to describe that situation).

Another attempt to use externalism to refute skepticism is done by Brueckner and Warfield. It involves the claim that our thoughts are "about" things, unlike a BIV's thoughts, which cannot be "about" things (DeRose, 1999).

Semantic externalism comes in two varieties, depending on whether meaning is construed cognitively or linguistically. On a cognitive construal, externalism is the thesis that what concepts (or contents) are available to a thinker is determined by their environment, or their relation to their environment. On a linguistic construal, externalism is the thesis that the meaning of a word is environmentally determined. Likewise, one can construe semantic internalism in two ways, as a denial of either of these two theses.

Externalism and internalism in semantics is closely tied to the distinction in philosophy of mind concerning mental content, since the contents of one's thoughts (specifically, intentional mental states) are usually taken to be semantic objects that are truth-evaluable.

See also:

Within the context of the philosophy of mind, externalism is the theory that the contents of at least some of one's mental states are dependent in part on their relationship to the external world or one's environment.

The traditional discussion on externalism was centered around the semantic aspect of mental content. This is by no means the only meaning of externalism now. Externalism is now a broad collection of philosophical views considering all aspects of mental content and activity. There are various forms of externalism that consider either the content or the vehicles of the mind or both. Furthermore, externalism could be limited to cognition, or it could address broader issues of consciousness.

As to the traditional discussion on semantic externalism (often dubbed "content externalism"), some mental states, such as believing that water is wet, and fearing that the Queen has been insulted, have contents we can capture using 'that' clauses. The content externalist often appeal to observations found as early as Hilary Putnam's seminal essay, "The Meaning of 'Meaning'," (1975). Putnam stated that we can easily imagine pairs of individuals that are microphysical duplicates embedded in different surroundings who use the same words but mean different things when using them.

For example, suppose that Ike and Tina's mothers are identical twins and that Ike and Tina are raised in isolation from one another in indistinguishable environments. When Ike says, "I want my mommy," he expresses a want satisfied only if he is brought to his mommy. If we brought Tina's mommy, Ike might not notice the difference, but he doesn't get what he wants. It seems that what he wants and what he says when he says, "I want my mommy," will be different from what Tina wants and what she says she wants when she says, "I want my mommy."

Externalists say that if we assume competent speakers know what they think, and say what they think, the difference in what these two speakers mean corresponds to a difference in the thoughts of the two speakers that is not (necessarily) reflected by a difference in the internal make up of the speakers or thinkers. They urge us to move from externalism about meaning of the sort Putnam defended to externalism about contentful states of mind. The example pertains to singular terms, but has been extended to cover kind terms as well such as natural kinds (e.g., 'water') and for kinds of artifacts (e.g., 'espresso maker'). There is no general agreement amongst content externalists as to the scope of the thesis.

Philosophers now tend to distinguish between "wide content" (externalist mental content) and "narrow content" (anti-externalist mental content). Some, then, align themselves as endorsing one view of content exclusively, or both. For example, Jerry Fodor (1980) argues for narrow content (although he comes to reject that view in his 1995), while David Chalmers (2002) argues for a two dimensional semantics according to which the contents of mental states can have both wide and narrow content.

Critics of the view have questioned the original thought experiments saying that the lessons that Putnam and later writers such as Tyler Burge (1979, 1982) have urged us to draw can be resisted. Frank Jackson and John Searle, for example, have defended internalist accounts of thought content according to which the contents of our thoughts are fixed by descriptions that pick out the individuals and kinds that our thoughts intuitively pertain to the sorts of things that we take them to. In the Ike/Tina example, one might agree that Ike's thoughts pertain to Ike's mother and that Tina's thoughts pertain to Tina's but insist that this is because Ike thinks of that woman as his mother and we can capture this by saying that he thinks of her as 'the mother of the speaker'. This descriptive phrase will pick out one unique woman. Externalists claim this is implausible, as we would have to ascribe to Ike knowledge he wouldn't need to successfully think about or refer to his mother.

Critics have also claimed that content externalists are committed to epistemological absurdities. Suppose that a speaker can have the concept of water we do only if the speaker lives in a world that contains HO. It seems this speaker could know a priori that they think that water is wet. This is the thesis of privileged access. It also seems that they could know on the basis of simple thought experiments that they can only think that water is wet if they live in a world that contains water. What would prevent her from putting these together and coming to know a priori that the world contains water? If we should say that no one could possibly know whether water exists a priori, it seems either we cannot know content externalism to be true on the basis of thought experiments or we cannot know what we are thinking without first looking into the world to see what it is like.

As mentioned, content externalism (limited to the semantic aspects) is only one among many other options offered by externalism by and large.

See also:

Internalism in the historiography of science claims that science is completely distinct from social influences and pure natural science can exist in any society and at any time given the intellectual capacity. Imre Lakatos is a notable proponent of historiographical internalism.

Externalism in the historiography of science is the view that the history of science is due to its social context – the socio-political climate and the surrounding economy determines scientific progress. Thomas Kuhn is a notable proponent of historiographical externalism.





</doc>
<doc id="15048" url="https://en.wikipedia.org/wiki?curid=15048" title="Isolationism">
Isolationism

Isolationism is a category of foreign policies institutionalized by leaders who assert that nations' best interests are best served by keeping the affairs of other countries at a distance. One possible motivation for limiting international involvement is to avoid being drawn into dangerous and otherwise undesirable conflicts. There may also be a perceived benefit from avoiding international trade agreements or other mutual assistance pacts.

Isolationism has been defined as:

Before 1999, Bhutan had banned television and the Internet in order to preserve its culture, environment, identity etc. Eventually, Jigme Singye Wangchuck lifted the ban on television and the Internet. His son, Jigme Khesar Namgyel Wangchuck, was elected as Druk Gyalpo of Bhutan, which helped forge the Bhutanese democracy. Bhutan has subsequently undergone a transition from an absolute monarchy to a constitutional monarchy multi-party democracy. The development of "Bhutanese democracy" has been marked by the active encouragement and participation of reigning Bhutanese monarchs since the 1950s, beginning with legal reforms such as the abolition of slavery, and culminating in the enactment of Bhutan's Constitution 

After Zheng He's voyages in the 15th century, the foreign policy of the Ming dynasty in China became increasingly isolationist. The Hongwu Emperor was the not first to propose the policy to ban all maritime shipping in 1390. The Qing dynasty that came after the Ming dynasty often continued the Ming dynasty's isolationist policies. Wokou, which literally translates to "Japanese pirates" or "dwarf pirates", were pirates who raided the coastlines of China, Japan, and Korea, and were one of the key primary concerns, although the maritime ban was not without some control.

From 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy which it called "kaikin". The policy prohibited foreign contact with most outside countries. The commonly held idea that Japan was entirely closed, however, is misleading. In fact, Japan maintained limited-scale trade and diplomatic relations with China, Korea and Ryukyu Islands, as well as the Dutch Republic as the only Western trading partner of Japan for much of the period.

The culture of Japan developed with limited influence from the outside world and had one of the longest stretches of peace in history. During this period, Japan developed thriving cities, castle towns, increasing commodification of agriculture and domestic trade, wage labor, increasing literacy and concomitant print culture, laying the groundwork for modernization even as the shogunate itself grew weak.

In 1863, Emperor Gojong took the throne of the Joseon Dynasty when he was a child. His father, Regent Heungseon Daewongun, ruled for him until Gojong reached adulthood. During the mid-1860s he was the main proponent of isolationism and the principal instrument of the persecution of both native and foreign Catholics.

Following the division of the peninsula after independence from Japan in 1945–48, Kim il-Sung inaugurated an isolationist totalitarian regime in the North, which has been continued by his son and grandson to the present day. North Korea is often referred to as "The Hermit Kingdom".

Just after independence was achieved, Paraguay was governed from 1814 by the dictator José Gaspar Rodríguez de Francia, who closed the country's borders and prohibited trade or any relation with the outside world until his death in 1840. The Spanish settlers who had arrived just before independence had to intermarry with either the old colonists or with the native Guarani, in order to create a single Paraguayan people.

Francia had a particular dislike of foreigners and any who came to Paraguay during his rule (which would have been very difficult) were not allowed to leave for the rest of their lives. An independent character, he hated European influences and the Catholic Church, turning church courtyards into artillery parks and confession boxes into border sentry posts, in an attempt to keep foreigners at bay.

While some scholars, such as Robert J. Art, believe that the United States has an isolationist history, other scholars dispute this by describing the United States as following a strategy of unilateralism or non-interventionism instead. Robert Art makes his argument in "A Grand Strategy for America" (2003). Books that have made the argument that the United States followed unilaterism instead of isolationism include Walter A. McDougall's "Promised Land, Crusader State" (1997), John Lewis Gaddis's "Surprise, Security, and the American Experience" (2004), and Bradley F. Podliska's "Acting Alone" (2010). Both sides claim policy prescriptions from George Washington's Farewell Address as evidence for their argument. Bear F. Braumoeller argues that even the best case for isolationism, the United States in the interwar period, has been widely misunderstood and that Americans proved willing to fight as soon as they believed a genuine threat existed.
Events during and after the Revolution related to the treaty of alliance with France, as well as difficulties arising over the neutrality policy pursued during the French revolutionary wars and the Napoleonic wars, encouraged another perspective. A desire for separateness and unilateral freedom of action merged with national pride and a sense of continental safety to foster the policy of isolation. Although the United States maintained diplomatic relations and economic contacts abroad, it sought to restrict these as narrowly as possible in order to retain its independence. The Department of State continually rejected proposals for joint cooperation, a policy made explicit in the Monroe Doctrine's emphasis on unilateral action. Not until 1863 did an American delegate attend an international conference.




</doc>
<doc id="15049" url="https://en.wikipedia.org/wiki?curid=15049" title="Indianapolis Colts">
Indianapolis Colts

The Indianapolis Colts are an American football team based in Indianapolis. The Colts compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) South division. Since the 2008 season, the Colts have played their games in Lucas Oil Stadium. Previously, the team had played for over two decades (1984–2007) at the RCA Dome. Since 1987, the Colts have served as the host team for the NFL Scouting Combine.

The Colts have competed as a member club of the NFL since their founding in Baltimore in 1953. They were one of three NFL teams to join those of the American Football League (AFL) to form the AFC following the 1970 merger. While in Baltimore, the team advanced to the playoffs 10 times and won three NFL Championship games in 1958, 1959, and 1968. The Colts played in two Super Bowl games while they were based in Baltimore, losing to the New York Jets in Super Bowl III and defeating the Dallas Cowboys in Super Bowl V. The Colts relocated to Indianapolis in 1984 and have since appeared in the playoffs 16 times, won two conference championships, and won one Super Bowl, in which they defeated the Chicago Bears in Super Bowl XLI.

Following World War II, a competing professional football league was organized known as the All America Football Conference which began to play in the 1946 season. In its second year the franchise assigned to the Miami Seahawks was relocated to Maryland's major commercial and manufacturing city of Baltimore. After a fan contest the team was renamed the Baltimore Colts and used the team colors of silver and green. The Colts played for the next three seasons in the old AAFC. until they agreed to merge with the old National Football League (of 1920–1922 to 1950) when the NFL was reorganized. The Baltimore Colts were one of the three former AAFC powerhouse teams to merge with the NFL at that time, the others being the San Francisco 49ers and the Cleveland Browns. This Colts team, now in the "big league" of professional American football for the first time, although with shaky financing and ownership, played only in the 1950 season of the NFL, and was later disbanded.

In 1953, a new Baltimore-based group, heavily supported by the City's municipal government and with a large subscription-base of fan-purchased season tickets, led by local owner Carroll Rosenbloom won the rights to a new Baltimore NFL franchise. Rosenbloom was awarded the remains of the former Dallas Texans team, who themselves had a long and winding history starting as the Boston Yanks in 1944, merging later with the Brooklyn Tigers, and who were previously known as the Dayton Triangles, one of the original old NFL teams established even before the League itself, in 1913. The league began with theorganization in 1920 of the original "American Professional Football Conference" [APFC], (soon renamed the "American Professional Football Association", [APF.]), then two years later in 1922, renamed a second time, now permanently as the "National Football League". That team later became the New York Yanks in 1950, and many of the players from the New York Yankees of the former competing All-America Football Conference (1946–49) were added to the team to begin playing in the newly merged League for the 1950 season. The Yanks then moved to Dallas in Texas after the 1951 season having competed for two seasons, but played their final two "home" games of the 1952 season as a so-called "road team" at the Rubber Bowl football stadium in Akron, Ohio. The NFL considers the Texans and Colts to be separate teams, although many of the earlier teams shared the same colors of blue and white. Thus, the Indianapolis Colts are legally considered to be a 1953 expansion team.
The current version of the Colts football team played their first season in Baltimore in 1953, where the team compiled a 3–9 record under first-year head coach Keith Molesworth. The franchise struggled during the first few years in Baltimore, with the team not achieving their first winning record until the 1957 season. However, under head coach Weeb Ewbank and the leadership of quarterback Johnny Unitas, the Colts went on to a 9–3 record during the 1958 season and reached the NFL Championship Game for the first time in their history by winning the NFL Western Conference. The Colts faced the New York Giants in the 1958 NFL Championship Game, which is considered to be among the greatest contests in professional football history. The Colts defeated the Giants 23–17 in the first game ever to utilize the overtime rule, a game seen by 45 million people.

Following the Colts first NFL championship, the team posted a 9–3 record during the 1959 season and once again defeated the Giants in the NFL Championship Game to claim their second title in back to back fashion. Following the two championships in 1958 and 1959, the Colts did not return to the NFL Championship for four seasons and replaced the head coach Ewbank with the young Don Shula in 1963. In Shula's second season the Colts compiled a 12–2 record, but lost to the Cleveland Browns in the NFL Championship. However, in 1968 the Colts returned with the continued leadership of Unitas and Shula and went on to win the Colts' third NFL Championship and made an appearance in Super Bowl III.
Leading up to the Super Bowl and following the 34–0 trouncing of the Cleveland Browns in the NFL Championship, many were calling the 1968 Colts team one of the "greatest pro football teams of all time" and were favored by 18 points against their counterparts from the American Football League, the New York Jets. The Colts, however, were stunned by the Jets, who won the game 16–7 in the first Super Bowl victory for the young AFL. The result of the game surprised many in the sports media as Joe Namath and Matt Snell led the Jets to the Super Bowl victory under head coach Weeb Ewbank, who had previously won two NFL Championships with the Colts.

Rosenbloom of the Colts, Art Modell of the Browns, and Art Rooney of the Pittsburgh Steelers agreed to have their teams join the ten AFL teams in the American Football Conference as part of the AFL–NFL merger in 1970. The Colts immediately went on a rampage in the new league, as new head coach Don McCafferty led the 1970 team to an 11–2–1 regular season record, winning the AFC East title. In the first round of the NFL Playoffs, the Colts beat the Cincinnati Bengals 17–0; one week later in the first ever AFC Championship Game, they beat the Oakland Raiders 27–17. Baltimore went on to win the first post-merger Super Bowl (Super Bowl V), defeating the National Football Conference's Dallas Cowboys 16–13 on a Jim O'Brien field goal with five seconds left to play. The victory gave the Colts their fourth NFL championship and first Super Bowl victory. Following the championship, the Colts returned to the playoffs in 1971 and defeated the Cleveland Browns in the first round, but lost to the Miami Dolphins in the AFC Championship.

Citing friction with the City of Baltimore and the local press, Rosenbloom traded the Colts franchise to Robert Irsay on July 13, 1972 and received the Los Angeles Rams in return. Under the new ownership, the Colts did not reach the postseason for three consecutive seasons after 1971, and after the 1972 season, starting quarterback and legend Johnny Unitas was traded to the San Diego Chargers. Following Unitas' departure, the Colts made the playoffs three consecutive seasons from 1975 to 1977, losing in the divisional round each time. The Colts 1977 playoff loss in double overtime against the Oakland Raiders was famous for the fact that it was the last playoff game for the Colts in Baltimore and is also known for the Ghost to the Post play. These consecutive championship teams featured 1976 NFL Most Valuable Player Bert Jones at quarterback and an outstanding defensive line, nicknamed the "Sack Pack."

Following the 1970s success, the team endured nine consecutive losing seasons beginning in 1978. In 1981, the Colts defense allowed an NFL-record 533 points, set an all-time record for fewest sacks (13), and also set a modern record for fewest punt returns (12). The following year, the offense collapsed, including a game against the Buffalo Bills where the Colts' offense did not cross mid-field the entire game. The Colts finished 0–8–1 in the strike-shortened 1982 season, thereby earning the right to select Stanford quarterback John Elway with the first overall pick. Elway, however, refused to play for Baltimore, and using leverage as a draftee of the New York Yankees baseball club, forced a trade to Denver. Behind an improved defense the team finished 7–9 in 1983, but that would be their last season in Baltimore.

The Baltimore Colts played their final home game in Baltimore on December 18, 1983, against the then Houston Oilers. Irsay continued to request upgrades to Memorial Stadium or construction of a new stadium. As a result of the poor performance on the field and the stadium issues, fan attendance and team revenue continued to dwindle. City officials were precluded from using tax-payer funds for the building of a new stadium, and the modest proposals that were offered by the city were not acceptable to either the Colts or the city's MLB franchise the Orioles. However, all sides continued to negotiate. Relations between Irsay and the city of Baltimore deteriorated. Although Irsay assured fans that his ultimate desire was to stay in Baltimore, he nevertheless began discussions with several other cities willing to build new football stadiums, eventually narrowing the list of cities to two: Phoenix and Indianapolis. Under the administration of mayors Richard Lugar and then William Hudnut, Indianapolis had undertaken an ambitious effort to reinvent itself into a 'Great American City'. The Hoosier Dome, which was later renamed the RCA Dome, had been built specifically for, and was ready to host, an NFL expansion team.

Meanwhile, in Baltimore, the situation worsened. The Maryland General Assembly intervened when a bill was introduced to give the city of Baltimore the right to seize ownership of the team by eminent domain. As a result, Irsay began serious negotiations with Indianapolis Mayor William Hudnut in order to move the team before the Maryland legislature could pass the law. Indianapolis offered loans as well as the Hoosier Dome and a training complex. After the deal was reached, moving vans from Indianapolis-based Mayflower Transit were dispatched overnight to the team's Maryland training complex, arriving on the morning of March 29, 1984. Once in Maryland, workers loaded all of the team's belongings, and by midday the trucks departed for Indianapolis, leaving nothing of the Colts organization that could be seized by Baltimore. The Baltimore Colts' Marching Band had to scramble to retrieve their equipment and uniforms before they were shipped to Indianapolis as well.

The move triggered a flurry of legal activity that ended when representatives of the city of Baltimore and the Colts organization reached a settlement in March 1986. Under the agreement, all lawsuits regarding the relocation were dismissed, and the Colts agreed to endorse a new NFL team for Baltimore.

Upon the Colts' arrival in Indianapolis over 143,000 requests for season tickets were received in just two weeks. The move to Indianapolis, however, did not change the recent fortune of the Colts, with the team appearing in the postseason only once in the first 11 seasons in Indianapolis. During the 1984 season, the first in Indianapolis, the team went 4–12 and accounted for the lowest offensive yardage in the league that season. The 1985 and 1986 teams combined for only eight wins, including an 0–13 start in 1986 which prompted the firing of head coach Rod Dowhower, who was replaced by Ron Meyer. The Colts, however, did receive eventual Hall of Fame running back Eric Dickerson as a result of a trade during the 1987 season, and went on to compile a 9–6 record, thereby winning the AFC East and advancing to the postseason for the first time in Indianapolis; they lost that game to the Cleveland Browns.

Following 1987, the Colts did not see any real success for quite some time, with the team missing the postseason for seven consecutive seasons. The struggles came to a climax in 1991 when the team went 1–15 and was just one point away from the first "imperfect" season in the history of a 16-game schedule. The season resulted in the firing of head coach Ron Meyer and the return of former head coach Ted Marchibroda to the organization in 1992; he had coached the team from 1975 to 1979. The team continued to struggle under Marchibroda and Jim Irsay, son of Robert Irsay and general manager at the time. It was in 1994 that Robert Irsay brought in Bill Tobin to become the general manager of the Indianapolis Colts.

Under Tobin, the Colts drafted running back Marshall Faulk with the second overall pick in the 1994 and acquired quarterback Jim Harbaugh as well. These moves along with others saw the Colts begin to turn their fortunes around with playoff appearances in 1995 and 1996. The Colts won their first postseason game as the Indianapolis Colts in 1995 and advanced to the AFC Championship Game against the Pittsburgh Steelers, coming just a Hail Mary pass reception away from a trip to Super Bowl XXX.

Marchibroda retired following the 1995 season and was replaced by Lindy Infante in 1996. After two consecutive playoff appearances, the Colts regressed and went 3–13 during the 1997 season. Along with the disappointing season, the principal owner and man who moved the team to Indianapolis, Robert Irsay, died in January 1997 after years of declining health. Jim Irsay, Robert Irsay's son, entered the role of principal owner following his father's death and quickly began to change the organization. Irsay replaced general manager Tobin with Bill Polian in 1997 as the team decided to build through their number one overall pick in the 1998 draft.

Jim Irsay began to shape the Colts one year after assuming control from his father by firing head coach Lindy Infante and hiring Bill Polian as the general manager of the organization. Polian in turn hired Jim Mora to become the next head coach of the team and drafted Tennessee Volunteer quarterback Peyton Manning, the son of New Orleans Saints legend Archie Manning, with the first overall pick in the 1998 NFL Draft.

The team and Manning struggled during the 1998 season, winning only three games; Manning threw a league high 28 interceptions. However, Manning did pass for 3,739 yards and threw 26 touchdown passes and was named to the NFL All-Rookie First Team. The Colts began to improve towards the end of the 1998 season and showed continued growth in 1999. Indianapolis drafted Edgerrin James in 1999 and continued to improve their roster heading into the upcoming season. The Colts went 13–3 in 1999 and finished first in the AFC East, their first division title since 1987. Indianapolis lost to the eventual AFC champion Tennessee Titans in the divisional playoffs.

The 2000 and 2001 Colts teams were considerably less successful compared to the 1999 team, and pressure began to mount on team administration and the coaching staff following a 6–10 season in 2001. Head coach Jim Mora was fired at the end of the season and was replaced by former Tampa Bay Buccaneers head coach Tony Dungy. Dungy and the team quickly changed the atmosphere of the organization and returned to the playoffs in 2002 with a 10–6 record. The Colts also returned to the playoffs in 2003 and 2004 with 12–4 records and AFC South championships. The Colts lost to the New England Patriots and Tom Brady in the 2003 AFC Championship Game and in the 2004 divisional playoffs, thereby beginning a rivalry between the two teams, and between Manning and Brady. Following two consecutive playoff losses to the Patriots, the Colts began the 2005 season with a 13–0 record, including a regular season victory over the Patriots, the first in the Manning era. During the season Manning and Marvin Harrison broke the NFL record for touchdowns by a quarterback and receiver tandem. Indianapolis finished the 2005 season with a 14–2 record, the best record in the league that year and the best in a 16 games season for the franchise, but lost to the Pittsburgh Steelers in the divisional round, a disappointing end to the season.

Indianapolis entered the 2006 season with a veteran quarterback, receivers, and defenders, and chose running back Joseph Addai in the 2006 draft. As in the previous season, the Colts began the season undefeated and went 9–0 before losing their first game against the Dallas Cowboys. Indianapolis finished the season with a 12–4 record and entered the playoffs for the fifth consecutive year, this time as the number three seed in the AFC. The Colts won their first two playoff games against the Kansas City Chiefs and the Baltimore Ravens to return to the AFC Championship Game for the first time since the 2003 playoffs, where they faced their rivals, the New England Patriots. In a classic game, the Colts overcame a 21–3 first half deficit to win the game 38–34 and earned a trip to Super Bowl XLI, the franchise's first Super Bowl appearance since 1970 and for the first as Indianapolis. The Colts faced the Chicago Bears in the Super Bowl, winning the game 29–17 and giving Manning, Polian, Irsay, and Dungy, as well as the city of Indianapolis, their first Super Bowl title.

Following their Super Bowl championship, the Colts compiled a 13–3 record during the 2007 season; they lost to the San Diego Chargers in the divisional playoffs, in what was the final game the Colts played at the RCA Dome before moving into Lucas Oil Stadium in 2008. The 2008 season began with Manning being sidelined for most of the pre-season due to surgery. Indianapolis began the season with a 3–4 record, but then won nine consecutive games to end the season at 12–4 and make in into the playoffs as a wild card team, eventually losing to the Chargers in the wild card round. Following the season, Tony Dungy announced his retirement after seven seasons as head coach, having compiled an overall record of 92–33 with the team.<ref name="Dungy Retires/Caldwell Hired"></ref>

Jim Caldwell was hired as head coach of the team following Dungy, and led the team during the 2009 season. The Colts went 14–0 during the season to finish with an overall record of 14–2 after controversially benching their starters during the last two games. The Colts for the second time in the Manning era entered the playoffs with the best record in the AFC. The Colts managed victories over the Baltimore Ravens and New York Jets to advance to Super Bowl XLIV against the New Orleans Saints, but lost to the Saints 31–17 to end the season in disappointment.

At the completion of the 2009 season, the Colts had finished the first decade of the 2000s (2000–2009) with the most regular season wins (115) and highest winning percentage (.719) of any team in the NFL during that span.

The 2010 team compiled a 10–6 record, the first time the Colts did not win 12 games since 2002, and lost to the New York Jets in the wild card round of the playoffs. The loss to the Jets was the last game for Peyton Manning as a Colt.

After missing the preseason, Manning was ruled out for the Colts' opening game in Houston and eventually the entire 2011 season. Taking over as starter was veteran quarterback Kerry Collins, who had been signed to the team after dissatisfaction with backup quarterback Curtis Painter and Dan Orlovsky. However, even with a veteran quarterback, the Colts lost their first 13 games and finished the season with a 2–14 record, enough to receive the first overall pick in the 2012 draft. Immediately following the season, team president Bill Polian was fired, ending his 14-year tenure with the team. The change built the anticipation of the organization's decision regarding Manning's future with the team. The Peyton Manning era came to an end on March 8, 2012 when Jim Irsay announced that Manning was being released from the roster after 13 seasons.

During the 2012 off-season owner Jim Irsay hired Ryan Grigson to be the General Manager. Grigson decided to let Head Coach Jim Caldwell go and Chuck Pagano was hired as the new Head Coach shortly thereafter. The Colts also began to release some higher paid and oft-injured veteran players, including Joseph Addai, Dallas Clark, and Gary Brackett. The Colts used their number one overall draft pick in 2012 to draft Stanford Cardinal quarterback Andrew Luck and also drafted his teammate Coby Fleener in the second round. The team also switched to a 3–4 defensive scheme.

With productive seasons from both Luck and veteran receiver Reggie Wayne, the Colts rebounded from the 2–14 season of 2011 with a 2012 season record of 11–5. The franchise, team, and fan base rallied behind Head Coach Chuck Pagano during his fight with leukemia. Clinching an unexpected playoff spot in the 2012–13 NFL playoffs, the 14th playoff berth for the club since 1995. The season ended in a 24–9 playoff loss to the eventual Super Bowl Champion Baltimore Ravens.

Two weeks into the 2013 season, the Colts traded their first round selection in the 2014 NFL Draft to the Cleveland Browns for running back Trent Richardson. In Week 7, Luck led the Colts to a 39–33 win over his predecessor, Peyton Manning, and the undefeated Broncos. Luck went on to lead the Colts to a 15th division championship later that season. In the first round of the 2013 NFL playoffs, Andrew Luck led the Colts to a 45–44 victory over Kansas City, outscoring the Chiefs 35–13 in the second half in the 2nd biggest comeback in NFL playoff history.

During the 2014 season, Luck led the Colts to the AFC Championship game for the first time in his career after breaking the Colts' single season passing yardage record previously held by Manning.

After the Colts finished 8–8 in both the 2015 and 2016 seasons and missed the playoffs in back-to-back seasons for the first time since 1997–98, Grigson was fired as general manager. Just three of his previous 18 draft picks remained on the team at the time of his firing. On January 30, 2017 the team hired Chris Ballard, who served as the Kansas City Chiefs Director of Football Operations, to replace Grigson.

On December 31, 2017, after winning the final game of the season and a final record of 4-12, the Colts parted ways with Pagano. Luck, who had suffered multiple injuries and missed nine games during the 2015 season, sat out the entire 2017 season recovering from shoulder surgery.

In the weeks following the end of the 2017 season, after two interviews, it was widely reported that the Colts would hire Josh McDaniels, offensive coordinator of the New England Patriots, to replace Pagano, after McDaniels fulfilled his obligations to the Patriots in Super Bowl LII. On February 8, 2018, the Colts announced McDaniels as their new head coach. Hours later, however, McDaniels rescinded his decision to be the head coach, and he returned to the Patriots.

On February 11, 2018, the Colts announced Frank Reich, then offensive coordinator of the Philadelphia Eagles, as their new head coach. In Reich's first season as head coach, Andrew Luck's return to the field got off to a shaky start, as the Colts began the 2018 season 1-5. However, they would surge back to win nine of their last ten games to secure a 10-6 record and a playoff berth. They would win a Wild-Card game against their division rival Houston Texans before falling to the Kansas City Chiefs in the Divisional Round. Luck, benefiting from the Colts' best offensive line of his career, was named the 2018 Comeback Player of the Year.

Colts General Manager Chris Ballard achieved a historic feat in 2018 when two players he had drafted that year, guard Quenton Nelson and linebacker Darius Leonard were both named First-Team All-Pro. This was the first time two rookies from the same team received that honor since Hall-of-Famers Dick Butkus and Gale Sayers achieved the feat in 1965.

On August 24, 2019, Luck informed the Colts that he would be retiring from the NFL after not attending training camp. He cited an unfulfilling cycle of injury and rehab as his primary reason for leaving football. 

On November 17, 2019, the Colts defeated the Jacksonville Jaguars for the team's 300th win in the Indianapolis era, with a record of 300–267. Despite a promising 5-2 start and strong seasons from Leonard, Nelson, and newly acquired defensive end Justin Houston, the Colts struggled in the second half of the 2019 season with new starting quarterback Jacoby Brissett at the helm and finished the year with a 7-9 record.

On March 17, 2020, the Colts signed longtime Los Angeles Chargers quarterback and eight-time Pro Bowler Philip Rivers to a one-year deal worth $25 million.

The Colts' helmets in 1953 were white with a blue stripe. In 1954–55 they were blue with a white stripe and a pair of horseshoes at the rear of the helmet. For 1956, the colors were reversed, white helmet, blue stripe and horseshoes at the rear. In 1957 the horseshoes moved to their current location, one on each side of the helmet. The blue jerseys have white shoulder stripes and the white jerseys have blue stripes. The team also wears white pants with blue stripes down the sides. 

For much of the team's history, the Colts wore blue socks, accenting them with two or three white stripes for much of their history in Baltimore and during the 2004 and 2005 seasons. From 1982 to 1987, the blue socks featured gray stripes. For a period lasting 1955 to 1958 and again from 1988 to 1992, the Colts wore white socks with either two or three blue stripes. 

From 1982 through 1986, the Colts wore gray pants with their blue jerseys. The gray pants featured a horseshoe on the top of the sides with the player's number inside the horseshoe. The Colts continued to wear white pants with their white jerseys throughout this period, and in 1987, the gray pants were retired. 

The Colts wore blue pants with their white jerseys for the first three games of the 1995 season (pairing them with white socks), but then returned to white pants with both the blue and white jerseys. The team made some minor uniform adjustments before the start of the 2004 season, including reverting from blue to the traditional gray face masks, darkening their blue colors from a royal blue to speed blue, as well as adding two white stripes to the socks. In 2006, the stripes were removed from the socks.

In 2002, the Colts made a minor striping pattern change on their jerseys, having the stripes only on top of the shoulders then stop completely. Previously, the stripes used to go around to underneath the jersey sleeves. This was done because the Colts, like many other football teams, were beginning to manufacture the jerseys to be tighter to reduce holding calls and reduce the size of the sleeves. Although the white jerseys of the Minnesota Vikings at the time also had a similar striping pattern and continued as such (as well as the throwbacks the New England Patriots wore in the Thanksgiving game against the Detroit Lions in 2002, though the Patriots later wore the same throwbacks in 2009 with truncated stripes and in 2010 became their official alternate uniform), the Colts and most college teams with this striping pattern did not make this adjustment.

In 2017, the Colts brought back the blue pants but paired them with the blue jerseys as part of the NFL Color Rush program.

The club officially revealed an updated wordmark logo, as well as updated numeral fonts, on April 13, 2020.

After 24 years of playing at the RCA Dome, the Colts moved to their new home Lucas Oil Stadium in the fall of 2008. In December 2004, the City of Indianapolis and Jim Irsay agreed to a new stadium deal at an estimated cost of $1 billion (including the Indiana Convention Center upgrades). In a deal estimated at $122 million, Lucas Oil Products won the naming rights to the stadium for 20 years.

Lucas Oil Stadium is a seven-level stadium which seats 63,000 for football. It can be reconfigured to seat 70,000 or more for NCAA basketball and football and concerts. It covers . The stadium features a retractable roof allowing the Colts to play home games outdoors for the first time since arriving in Indianapolis. Using FieldTurf, the playing surface is roughly below ground level. In addition to being larger than the RCA Dome, the new stadium features: 58 permanent concession stands, 90 portable concession stands, 13 escalators, 11 passenger elevators, 800 restrooms, HD video displays from Daktronics and replay monitors and 142 luxury suites. The stadium also features a retractable roof, with electrification technology developed by VAHLE, Inc. Other than being the home of the Colts, the stadium will host games in both the Men's and Women's NCAA Basketball Tournaments and will serve as the back up host for all NCAA Final Four Tournaments. The stadium hosted the Super Bowl for the 2011 season (Super Bowl XLVI) and has a potential economic impact estimated at $286 million. Lucas Oil Stadium has also hosted the Drum Corps International World Championships since 2009.

As a transplant from the AFC East into the AFC South upon the realignment of the NFL's divisions in , the Colts merely share loose rivalries with the other three teams in its division, namely the Houston Texans, Jacksonville Jaguars, and Tennessee Titans (formerly the Houston Oilers). They have dominated the AFC South for much of the division's history under quarterbacks Peyton Manning and Andrew Luck, but have faced competition for divisional supremacy in recent years from the Texans.

The rivalry between the Indianapolis Colts and New England Patriots is one of the NFL's newest rivalries. The rivalry is fueled by the quarterback comparison between Peyton Manning and Tom Brady. The Patriots owned the beginning of the series, defeating the Colts in six consecutive contests including the 2003 AFC Championship game and a 2004 AFC Divisional game. The Colts won the next three matches, notching two regular season victories and a win in the 2006 AFC Championship game on the way to their win in Super Bowl XLI. On November 4, 2007 the Patriots defeated the Colts 24–20; in the next matchup on November 2, 2008, the Colts won 18–15 in a game that was one of the reasons the Patriots failed to make the playoffs; in the 2009 meeting, the Colts staged a spirited comeback to beat the Patriots 35–34; in 2010 the Colts almost staged another comeback, pulling within 31–28 after trailing 31–14 in the fourth quarter, but fell short due to a Patriots interception of a Manning pass late in the game; it turned out to be Manning's final meeting against the Patriots as a member of the Colts. After a dismal 2011 season that included a 31–24 loss to the Patriots, the Colts drafted Andrew Luck and in November of 2012 the two teams met with identical 6–3 records; the Patriots erased a 14–7 gap to win 59–24. The nature of this rivalry is ironic because the Colts and Patriots were division rivals from 1970 to 2001, but it did not become prominent in league circles until after Indianapolis was relocated to the AFC South. On November 16, 2014, the New England Patriots traveled at 7–2 to play the 6–3 Colts at Lucas Oil Stadium. After a stellar four touchdown performance by New England running back Jonas Gray, the Patriots defeated the Colts 42–20. The Patriots followed up with a 45–7 defeat of the Colts in the 2014 AFC Championship Game.

In the years 1953–66, the Colts played in the NFL Western Conference (also known as division), but did not have significant rivalries with other franchises in that alignment, as they were the eastern-most team and the rest of the division included the Great Lakes franchises Green Bay, Detroit Lions, Chicago Bears, and after 1961, the Minnesota Vikings, along with the league's two West Coast teams in San Francisco and Los Angeles. The closest team to Baltimore was the Washington Redskins, but they were not in the same division and not very competitive during most years at that time.

In 1958, Baltimore played its first NFL Championship Game against the 10–3 New York Giants. The Giants qualified for the championship after a tie-breaking playoff against the Cleveland Browns. Having already been defeated by the Giants in the regular season, Baltimore was not favored to win, yet proceeded to take the title in sudden death overtime. The Colts then repeated the feat by posting an identical record and routing the Giants in the 1959 final. Up until the Colts' back-to-back titles, the Giants had been the premier club in the NFL, and continued to be post-season stalwarts the next decade, losing three straight finals. The situation was reversed by the end of the decade, with Baltimore winning the 1968 NFL title and New York compiling less impressive results. In recent years, the Colts and Giants featured brothers as their starting quarterbacks (Peyton and Eli Manning respectively), leading to their occasional match-up being referred to as the "Manning Bowl".

Super Bowl III became the most famous upset in professional sports history as the American Football League's New York Jets won 16–7 over the overwhelmingly favored Colts. With the merger of the AFL and NFL the Colts and Jets were placed in the new AFC East. The two teams met twice a year (interrupted in 1982 by a player strike) 1970–2001; with the move of the Colts to the AFC South the two teams' rivalry actually escalated, as they met three times in the playoffs in the South's first nine seasons of existence; the Jets crushed the Colts 41–0 in the 2002 Wild Card playoff round; the Colts then defeated the Jets 30–17 in the 2009 AFC Championship Game; but the next year in the Wild Card round the Jets pulled off another playoff upset of the Colts, winning 17–16; it was Peyton Manning's final game with the Colts. The Jets defeated the Colts 35–9 in 2012 in Andrew Luck's debut season; after two straight losses Luck led a 45–10 rout of the Jets in 2016.

Joe Namath and Johnny Unitas were the focal point of the rivalry at its beginning, but they did not meet for a full game until September 24, 1972. Namath erupted with six touchdowns and 496 passing yards despite only 28 throws and 15 completions. Unitas threw for 376 yards and two scores but was sacked six times as the Jets won 44–34; the game was considered one of the top ten passing duels in NFL history.

Baltimore's post NFL-AFL merger passage to the AFC saw them thrust into a new environment with little in common with its fellow divisional teams: the Jets, Miami Dolphins, Buffalo Bills, and Boston Patriots. One angle where the two clubs did have something in common, however came in new Miami coach Don Shula. Shula had coached the Colts the previous seven pre-merger seasons (1963–69) and was signed by Joe Robbie after the merger was consummated; because the signing came after the merger the NFL's rules on tampering came into play, and the Dolphins had to give up their first-round pick to the Colts.

Powered by QB Earl Morrall Baltimore was the first non-AFL franchise to win a division title in the conference, outlasting the Miami Dolphins by one game, and leading the division since Week 3 of 1970. The two franchises were denied a playoff confrontation by Miami's first-round defeat to the Oakland Raiders, whereas Baltimore won its first Super Bowl title that year.

Yet in 1971, the teams were engaged in a heated race that went down to the final week of the season, where Miami won its first division title with a 10–3–1 title compared to the 10–4 Baltimore record after the Colts won the Week 13 matchup between them at home, but proceeded to lose the last game of the season to Boston. In the playoffs Baltimore advanced to the AFC title game after a 20–3 rout of the Cleveland Browns, whereas Miami survived a double-overtime nailbiter against the Kansas City Chiefs. This set up a title game that was favored for the defending league champion Colts. Yet Miami won the AFC championship with a 21–0 shutout and advanced to lose Super Bowl VI to Dallas. In 1975 Baltimore and Miami tied with 10–4 records, yet the Colts advanced to the playoffs based on a head-to-head sweep of their series. In 1977 Baltimore tied for first for the third straight year (in 1976 they tied with the now-New England Patriots) with Miami, and this time advanced to the playoffs on even slimmer pretenses, with a conference record of 9–3 compared to Miami's 8–4, as they had split the season series. The rivalry in the following years was virtually negated by very poor play of the Colts; the Colts won just 117 games in the twenty-one seasons (1978–98) that bracketed their 1977 playoff loss to the Oakland Raiders and the 1999 trade of star running back Marshall Faulk; this included a 0–8–1 record during the NFL's strike shortened 1982 season.

In 1995, now as Indianapolis, the two both posted borderline 9–7 records to tie for second against Buffalo, yet the Colts once again reached the post-season having swept the season series. The following season they edged out Miami by posting a 9–7 record and winning the ordinarily meaningless 3rd-place position, but qualifying for the wild card. The two clubs' 1999 meetings were dramatic affairs between Hall Of Fame-bound Dan Marino and up-and-coming star Peyton Manning. Marino led a 25-point fourth quarter comeback for a 34–31 Dolphins win at the RCA Dome, and then in Miami Marino led another comeback to tie the game 34–34 with 36 seconds remaining; Manning, however, drove the Colts in range for a 53-yard field goal as time expired (37–34 Colts win).

The last truly meaningful matchup between the two franchises was in the 2000 season, when Miami edged out Indianapolis with an 11–5 record for the division championship. The two then met in the wild-card round where the Dolphins won 23–17 before being blown out by Oakland 27–0 (the Colts themselves had suffered a bitter loss to the Raiders in Week 2 of the season when the Raiders erased a 24–7 gap to win 38–31). In 2002 the Colts moved to the newly created AFC South division; the two clubs met at the RCA Dome on September 15 where the Dolphins edged the Colts 21–13 after stopping a late Colts drive. The rivalry was effectively retired after this; the two clubs did meet in a memorable "Monday Night Football" matchup in 2009 where the Colts, despite having the ball for only 15 minutes, defeated the Dolphins 27–23.

The rivalry saw a rekindling after the 2012 NFL Draft brought new quarterbacks to both teams in Ryan Tannehill and Luck. The two met during the 2012 season with Luck breaking the rookie record for passing yards in a game in a 23–20 win over the Dolphins, but Tannehill and the Dolphins beat the Colts 24–20 the next season. The Dolphins win began a slump for Luck and the Colts against AFC East teams (eight straight losses by the Colts) that ended in December 2016 against the Jets, when they defeated them by a score of 41–10.

The Ring of Honor was established on September 23, 1996. There have been 15 inductees.

This is a partial list of the Colts' last five completed seasons. For the full season-by-season franchise results, see List of Indianapolis Colts seasons.

"Note: The Finish, Wins, Losses, and Ties columns list regular season results and exclude any postseason play."

"see also: List of Indianapolis Colts broadcasters"

The Colts' flagship radio stations since 2007 are WFNI (1070 AM, later adding repeater signals at 93.5 FM and 107.5 FM) and WLHK 97.1 FM. The 1070 AM frequency, then known as WIBC, had also been the flagship from 1984 to 1992 and from 1995 to 1997. 

Matt Taylor is the team's play-by-play announcer, succeeding Bob Lamey in 2018. Lamey held the job from 1984 to 1991 and again from 1995 to 2018. Former Colts backup quarterback Jim Sorgi serves as the "color commentator". Mike Jansen serves as the public address announcer at all Colts home games. Jansen has been the public address announcer since the 1998 season.

The team's local TV carriage rights were shaken up in mid-2014 when WTTV's owner Tribune Media came to terms with CBS to become the network's Indianapolis affiliate as of January 1, 2015, replacing WISH-TV. With the deal, both Tribune Media stations, including WXIN (channel 59) carry the bulk of the team's regular season games starting with the 2015 NFL season. Also as of the 2015 season, WTTV and WXIN became the official Colts stations and air the team's preseason games, along with official team programming and coach's shows, and have a signage presence along the fascia of Lucas Oil Stadium. 

WISH's sister station WNDY-TV aired preseason games from 2011–2014, having replaced WTTV at that time.


Before the third regular season game of 2017, against the Cleveland Browns, more than ten Indianapolis Colts players kneeled on one knee as opposed to the tradition of standing during the playing of "The Star-Spangled Banner", while thousands of fans booed and others posted responses to social media. The following day, then Colts head coach Chuck Pagano commented, “I’m proud of our players and their commitment and their compassion toward the game and the [horse] shoe and each community. We are a unified group,” and former head coach, Tony Dungy was quoted saying "A group of our family got attacked, and called names ... and said they should be fired for what we feel is demonstrating our first amendment right".

Before the fourth regular season game of 2017, against the Seattle Seahawks, the Colts stood during "The Star-Spangled Banner", however the entire team, including quarterback Andrew Luck locked arms in protest, instead of the customary holding of the right hand over the heart. Ratings for this "NBC Sunday Night Football" game was down five percent from the prior week's game in the same time slot.

Before the fifth regular season game of 2017, against the San Francisco 49ers, the entire Colts team as in the Week 4 game, stood during "The Star-Spangled Banner", however with locking of arms, instead of the customary holding of the right hand over the heart. In addition to the Colts response, more than 20 members of the opposing team, the San Francisco 49ers, kneeled for "The Star-Spangled Banner". In attendance within the stadium, was then Vice President of the United States and former Governor of Indiana, Mike Pence who responded to these protests by leaving the stadium. This was a heavily attended home game for the halftime retirement of the #18 jersey of former quarterback and 2-time Super Bowl winner, Peyton Manning.

During warmups prior to the sixth regular game of the 2017 season, a "Monday Night Football" game between the Colts and the Tennessee Titans, the Colts wore black T-shirts with the words "We will" on the front and "Stand for equality, justice, unity, respect, dialogue, opportunity" on the back for the third straight week. The Colts players stood with their arms locked during the playing of "The Star-Spangled Banner" instead of the customary holding of the right hand over the heart.



</doc>
<doc id="15051" url="https://en.wikipedia.org/wiki?curid=15051" title="Immigration to the United States">
Immigration to the United States

Immigration to the United States is the international movement of non-U.S. nationals in order to reside permanently in the country. Immigration has been a major source of population growth and cultural change throughout much of the U.S. history. Because the United States is a settler colonial society, all Americans, with the exception of the small percentage of Native Americans, can trace their ancestry to immigrants from other nations around the world.

In absolute numbers, the United States has a larger immigrant population than any other country, with 47 million immigrants as of 2015. This represents 19.1% of the 244 million international migrants worldwide, and 14.4% of the U.S. population. Some other countries have larger proportions of immigrants, such as Switzerland with 24.9% and Canada with 21.9%.

According to the 2016 Yearbook of Immigration Statistics, the United States admitted a total of 1.18 million legal immigrants (618k new arrivals, 565k status adjustments) in 2016. Of these, 48% were the immediate relatives of U.S. citizens, 20% were family-sponsored, 13% were refugees and/or asylum seekers, 12% were employment-based preferences, 4.2% were part of the Diversity Immigrant Visa program, 1.4% who were victims of a crime (U1) or their family members (U2 to U5), and 1.0% who were granted the Special Immigrant Visa (SIV) for Iraqis and Afghans employed by U.S. Government. The remaining 0.4% included small numbers from several other categories, including 0.2% who were granted suspension of deportation as an immediate relative of a citizen (Z13); persons admitted under the Nicaraguan and Central American Relief Act; children born subsequent to the issuance of a parent's visa; and certain parolees from the former Soviet Union, Cambodia, Laos, and Vietnam who were denied refugee status.

The economic, social, and political aspects of immigration have caused controversy regarding such issues as maintaining ethnic homogeneity, workers for employers versus jobs for non-immigrants, settlement patterns, impact on upward social mobility, crime, and voting behavior.

Between 1921 and 1965, policies such as the national origins formula limited immigration and naturalization opportunities for people from areas outside Western Europe. Exclusion laws enacted as early as the 1880s generally prohibited or severely restricted immigration from Asia, and quota laws enacted in the 1920s curtailed Eastern European immigration. The civil rights movement led to the replacement of these ethnic quotas with per-country limits for family-sponsored and employment-based preference visas. Since then, the number of first-generation immigrants living in the United States has quadrupled.

Research suggests that immigration to the United States is beneficial to the U.S. economy. With few exceptions, the evidence suggests that on average, immigration has positive economic effects on the native population, but it is mixed as to whether low-skilled immigration adversely affects low-skilled natives. Studies also show that immigrants have lower crime rates than natives in the United States.

American immigration history can be viewed in four epochs: the colonial period, the mid-19th century, the start of the 20th century, and post-1965. Each period brought distinct national groups, races and ethnicities to the United States.

During the 17th century, approximately 400,000 English people migrated to Colonial America. However, only half stayed permanently. They comprised 85-90% of white immigrants. From 1700 to 1775 between 350-500,000 Europeans immigrated: the estimates vary in the sources. Only 52,000 English supposedly immigrated in the period 1701 to 1775., a figure questioned as too low. The rest, 400-450,000 were Scots, Scots-Irish from Ulster, Germans and Swiss, French Huguenots, and involuntarily 300,000 Africans. Over half of all European immigrants to Colonial America during the 17th and 18th centuries arrived as indentured servants. They numbered 350,000. On the eve of the War for Independence 1770 to 1775 7,000 English, 15,00 Scots, 13,200 Scots-Irish, 5,200 Germans, and 3,900 Irish Catholics arrived Fully half the English immigrants were young single men, well-skilled, trained artisans like the Huguenots The European populations of the Middle Colonies of New York, New Jersey, Pennsylvania and Delaware were ethnically very mixed, the English constituting only 30% in Pennsylvania, 40-45% in New Jersey, to 18% in New York numbered 22,000. The mid-19th century saw an influx mainly from northern Europe from the same major ethnic groups as for the Colonial Period but with large numbers of Catholic Irish and Scandinavians added to the mix; the late 19th and early 20th-century immigrants were mainly from Southern and Eastern Europe, but there were also several million immigrants from Canada; post-1965 most came from Latin America and Asia.

Historians estimate that fewer than 1 million immigrants moved to the United States from Europe between 1600 and 1799. By comparison, in the first federal census, in 1790, the population of the United States was enumerated to be 3,929,214.

The Naturalization Act of 1790 limited naturalization to "free white persons"; it was expanded to include blacks in the 1860s and Asians only in the 1950s. This made the United States an outlier, since laws that made racial distinctions were uncommon in the world in the 18th Century.

In the early years of the United States, immigration was fewer than 8,000 people a year, including French refugees from the slave revolt in Haiti. After 1820, immigration gradually increased. From 1836 to 1914, over 30 million Europeans migrated to the United States. The death rate on these transatlantic voyages was high, during which one in seven travelers died. In 1875, the nation passed its first immigration law, the Page Act of 1875.

After an initial wave of immigration from China following the California Gold Rush, Congress passed a series of laws culminating in the Chinese Exclusion Act of 1882, banning virtually all immigration from China until the law's repeal in 1943. In the late 1800s, immigration from other Asian countries, especially to the West Coast, became more common.

The peak year of European immigration was in 1907, when 1,285,349 persons entered the country. By 1910, 13.5 million immigrants were living in the United States.

While the Chinese Exclusion Act of 1882 had already excluded immigrants from China, the immigration of people from Asian countries in addition to China was banned by the sweeping Immigration Act of 1917, also known as the Asiatic Barred Zone Act, which also banned homosexuals, people with intellectual disability, and people with an anarchist worldview. The Emergency Quota Act was enacted in 1921, followed by the Immigration Act of 1924. The 1924 Act was aimed at further restricting immigrants from Southern and Eastern Europe, particularly Jews, Italians, and Slavs, who had begun to enter the country in large numbers beginning in the 1890s, and consolidated the prohibition of Asian immigration.

Immigration patterns of the 1930s were affected by the Great Depression. In the final prosperous year, 1929, there were 279,678 immigrants recorded, but in 1933, only 23,068 moved to the U.S. In the early 1930s, more people emigrated from the United States than to it. The U.S. government sponsored a Mexican Repatriation program which was intended to encourage people to voluntarily move to Mexico, but thousands were deported against their will. Altogether, approximately 400,000 Mexicans were repatriated; half of them were US citizens. Most of the Jewish refugees fleeing the Nazis and World War II were barred from coming to the United States. In the post-war era, the Justice Department launched Operation Wetback, under which 1,075,168 Mexicans were deported in 1954.

The Immigration and Nationality Act of 1965, also known as the Hart-Cellar Act, abolished the system of national-origin quotas. By equalizing immigration policies, the act resulted in new immigration from non-European nations, which changed the ethnic make-up of the United States. In 1970, 60% of immigrants were from Europe; this decreased to 15% by 2000. In 1990, George H. W. Bush signed the Immigration Act of 1990, which increased legal immigration to the United States by 40%. In 1991, Bush signed the Armed Forces Immigration Adjustment Act 1991, allowing foreign service members who had served 12 or more years in the US Armed Forces to qualify for permanent residency and, in some cases, citizenship.

In November 1994, California voters passed Proposition 187 amending the state constitution, denying state financial aid to illegal immigrants. The federal courts voided this change, ruling that it violated the federal constitution.

Appointed by Bill Clinton, the U.S. Commission on Immigration Reform recommended reducing legal immigration from about 800,000 people per year to approximately 550,000. While an influx of new residents from different cultures presents some challenges, "the United States has always been energized by its immigrant populations," said President Bill Clinton in 1998. "America has constantly drawn strength and spirit from wave after wave of immigrants ... They have proved to be the most restless, the most adventurous, the most innovative, the most industrious of people."

In 2001, President George W. Bush discussed an accord with Mexican President Vincente Fox. This possible accord was derailed by the September 11 attacks. From 2005 to 2013, the US Congress discussed various ways of controlling immigration. The Senate and House were unable to reach an agreement.

Nearly 14 million immigrants entered the United States from 2000 to 2010, and over one million persons were naturalized as U.S. citizens in 2008. The per-country limit applies the same maximum on the number of visas to all countries regardless of their population and has therefore had the effect of significantly restricting immigration of persons born in populous nations such as Mexico, China, India, and the Philippines—the leading countries of origin for legally admitted immigrants to the United States in 2013; nevertheless, China, India, and Mexico were the leading countries of origin for immigrants overall to the United States in 2013, regardless of legal status, according to a U.S. Census Bureau study.

Nearly 8 million people immigrated to the United States from 2000 to 2005; 3.7 million of them entered without papers. In 1986 president Ronald Reagan signed immigration reform that gave amnesty to 3 million undocumented immigrants in the country. Hispanic immigrants suffered job losses during the late-2000s recession, but since the recession's end in June 2009, immigrants posted a net gain of 656,000 jobs. Over 1 million immigrants were granted legal residence in 2011.

For those who enter the US illegally across the Mexico–United States border and elsewhere, migration is difficult, expensive and dangerous. Virtually all undocumented immigrants have no avenues for legal entry to the United States due to the restrictive legal limits on green cards, and lack of immigrant visas for low-skilled workers. Participants in debates on immigration in the early twenty-first century called for increasing enforcement of existing laws governing illegal immigration to the United States, building a barrier along some or all of the Mexico-U.S. border, or creating a new guest worker program. Through much of 2006 the country and Congress was immersed in a debate about these proposals. few of these proposals had become law, though a partial border fence had been approved and subsequently canceled.

According to a report released by ICE, during the fiscal year of 2016 ICE removed 240,255 immigrants. During the fiscal year of 2018, ICE removed 256,085 immigrants. There has been a significant increase in the removal of immigrants since President Trump took office. The reason for the increase in removals is due to the policies that the Trump administrations have put in place.

In January 2017, U.S. President Donald Trump signed an executive order temporarily suspending entry to the United States by nationals of seven Muslim-majority countries. It was replaced by another executive order in March 2017 and by a presidential proclamation in September 2017, with various changes to the list of countries and exemptions. The orders were temporarily suspended by federal courts but later allowed to proceed by the Supreme Court, pending a definite ruling on their legality. Another executive order called for the immediate construction of a wall across the U.S.–Mexico border, the hiring of 5,000 new border patrol agents and 10,000 new immigration officers, and federal funding penalties for sanctuary cities.

The most recent Trump policy to affect immigration to the United States was his ‘zero tolerance policy’. The ‘zero tolerance’ policy was put in place by President Trump in 2018, Attorney General Jeff Sessions made a formal statement putting in place the ‘zero tolerance’ policy, this policy legally allows children to be separated from adults unlawfully entering the United States. This is justified by labeling all adults that enter unlawfully as criminals thus subjecting them to criminal prosecution. The policy has faced a lot of criticism and backlash and was reportedly stopped in June 2018. “The United Nations condemned the USA government’s Zero Tolerance policy as ‘The Trump administration’s practice of separating children from migrant families entering the United States violates their rights and international law’”. Only after the stopping the ‘zero tolerance policy’ did the Trump administration uncover that there were no official plans in place to reunite families; resulting in further separation. Learn more about the Trump administration family separation policy.

The Trump Administration has continued their promise of a heavy hand on immigration and is now making it harder for asylum seekers. Most recent policies are attacking what it means for an asylum seeker to claim credible fear, these policies are changing the ways in which asylum officers assess an asylee’s circumstance, “A passage has been altered on individuals’ ‘demeanor, candor, and responsiveness’ as a factor in their credibility. Both the 2017 and 2014 versions note that migrants’ demeanor is often affected by cultural factors, including being detained in a foreign land and perhaps not speaking the language, as well as by trauma sustained at home or on the journey to the US. But the new version removes guidance that said these factors shouldn't be ‘significant factors’ in determining someone’s credibility — essentially allowing asylum officers to consider signs of stress as a reason to doubt someone’s credibility”. To further decrease the amount of asylum seekers into the United States, Attorney Jeff Sessions released a decision that restricts those fleeing gang violence and domestic abuse as ‘private crime’, therefore making their claims ineligible for asylum, “The 31-page decision narrows the ground for asylum for victims of ‘private crime’ and will cut off an avenue to refuge for women fleeing to the United States from Central America. ‘Generally, claims by aliens pertaining to domestic violence or gang violence perpetrated by non-governmental actors will not qualify for asylum,’ Sessions said in the opinion”. These new policies that have been put in place are putting many lives at risk, to the point that the ACLU has officially sued Jeff Sessions along with other members of the Trump Administration. The ACLU claims that the policies that are currently being put in place by this Presidential Administration is undermining the fundamental human rights of those immigrating into the United States, specifically women. They also claim that these policies violate decades of settle asylum law (.

Since the Trump Administration took office, it remained true to its hard stance on immigration. Trump and his administration almost immediately looked to remove the DACA program that was put in place by the Obama Administration. A policy was passed to stop granting citizenship requests. If you go to the DACA page on the United States Citizenship and Immigration Services a warning appears that states: “Important information about DACA requests: Due to federal court orders, USCIS has resumed accepting requests to renew a grant of deferred action under DACA. USCIS is not accepting requests from individuals who have never before been granted deferred action under DACA. Until further notice, and unless otherwise provided in this guidance, the DACA policy will be operated on the terms in place before it was rescinded on Sept. 5, 2017”. The Trump administration ordered federal courts to no longer grant citizenship to DACA requestors, making the process to citizenship for young children brought to the country illegally by their parents almost non-existent.

In April 2020, President Trump said he will sign an executive order to temporarily suspend immigration to the United States because of the COVID-19 pandemic in the United States.

Note: "Other Latin America" includes Central America, South America and the Caribbean.



According to the Department of State, in the 2016 fiscal year 84,988 refugees were accepted into the US from around the world. In the fiscal year of 2017, 53,691 refugees were accepted to the US. There was a significant decrease after Trump took office and it continues in the fiscal year of 2018 when only 22,405 refugees were accepted into the US. This displays a massive drop in acceptance of refugees since the Trump Administration has been in place.

Approximately half of immigrants living in the United States are from Mexico and other Latin American countries. Many Central Americans are fleeing because of desperate social and economic circumstances created in part by U.S. foreign policy in Central America over many decades. The large number of Central American refugees arriving in the U.S. have been explained as "blowback" to policies such as U.S. military interventions and covert operations that installed or maintained in power authoritarian leaders allied with wealthy land owners and multinational corporations who crush family farming and democratic efforts, which have caused drastically sharp social inequality, wide scale poverty and rampant crime. Economic austerity dictated by neoliberal policies imposed by the International Monetary Fund and its ally, the U.S., has also been cited as a driver of the dire social and economic conditions, as has the U.S. "War on Drugs," which has been understood as fueling murderous gang violence in the region. Another major migration driver from central America (Guatemala, Honduras, and El Salvador) are crop failures, which are (partly) caused by climate change. “The current debate … is almost totally about what to do about immigrants when they get here. But the 800-pound gorilla that’s missing from the table is what we have been doing there that brings them here, that drives them here," according to Jeff Faux, an economist who is a distinguished fellow at the Economic Policy Institute.

Until the 1930s most legal immigrants were male. By the 1990s women accounted for just over half of all legal immigrants. Contemporary immigrants tend to be younger than the native population of the United States, with people between the ages of 15 and 34 substantially overrepresented. Immigrants are also more likely to be married and less likely to be divorced than native-born Americans of the same age.

Immigrants are likely to move to and live in areas populated by people with similar backgrounds. This phenomenon has held true throughout the history of immigration to the United States. Seven out of ten immigrants surveyed by Public Agenda in 2009 said they intended to make the U.S. their permanent home, and 71% said if they could do it over again they would still come to the US. In the same study, 76% of immigrants say the government has become stricter on enforcing immigration laws since the September 11, 2001 attacks ("9/11"), and 24% report that they personally have experienced some or a great deal of discrimination.

Public attitudes about immigration in the U.S. were heavily influenced in the aftermath of the 9/11 attacks. After the attacks, 52% of Americans believed that immigration was a good thing overall for the U.S., down from 62% the year before, according to a 2009 Gallup poll. A 2008 Public Agenda survey found that half of Americans said tighter controls on immigration would do "a great deal" to enhance U.S. national security. Harvard political scientist and historian Samuel P. Huntington argued in his 2004 book "Who Are We? The Challenges to America's National Identity" that a potential future consequence of continuing massive immigration from Latin America, especially Mexico, could lead to the bifurcation of the United States.

The estimated population of illegal Mexican immigrants in the US fell from approximately 7 million in 2007 to 6.1 million in 2011 Commentators link the reversal of the immigration trend to the economic downturn that started in 2008 and which meant fewer available jobs, and to the introduction of tough immigration laws in many states. According to the Pew Hispanic Center the net immigration of Mexican born persons had stagnated in 2010, and tended toward going into negative figures.

More than 80 cities in the United States, including Washington D.C., New York City, Los Angeles, Chicago, San Francisco, San Diego, San Jose, Salt Lake City, Phoenix, Dallas, Fort Worth, Houston, Detroit, Jersey City, Minneapolis, Denver, Baltimore, Seattle, Portland, Oregon and Portland, Maine, have sanctuary policies, which vary locally.


Source: US Department of Homeland Security, Office of Immigration Statistics

Top 10 sending countries in the recent years:

The United States admitted more legal immigrants from 1991 to 2000, between ten and eleven million, than in any previous decade. In the most recent decade, the 10 million legal immigrants that settled in the U.S. represent roughly one third of the annual growth, as the U.S. population grew by 32 million (from 249 million to 281 million). By comparison, the highest previous decade was the 1900s, when 8.8 million people arrived, increasing the total U.S. population by one percent every year. Specifically, "nearly 15% of Americans were foreign-born in 1910, while in 1999, only about 10% were foreign-born."

By 1970, immigrants accounted for 4.7 percent of the US population and rising to 6.2 percent in 1980, with an estimated 12.5 percent in 2009. , 25% of US residents under age 18 were first- or second-generation immigrants. Eight percent of all babies born in the U.S. in 2008 belonged to illegal immigrant parents, according to a recent analysis of U.S. Census Bureau data by the Pew Hispanic Center.

Legal immigration to the U.S. increased from 250,000 in the 1930s, to 2.5 million in the 1950s, to 4.5 million in the 1970s, and to 7.3 million in the 1980s, before resting at about 10 million in the 1990s. Since 2000, legal immigrants to the United States number approximately 1,000,000 per year, of whom about 600,000 are "Change of Status" who already are in the U.S. Legal immigrants to the United States now are at their highest level ever, at just over 37,000,000 legal immigrants. In reports in 2005-2006, estimates of illegal immigration ranged from 700,000 to 1,500,000 per year. Immigration led to a 57.4% increase in foreign born population from 1990 to 2000.

Foreign-born immigration has caused the U.S. population to continue its rapid increase with the foreign-born population doubling from almost 20 million in 1990 to over 47 million in 2015. In 2018, there were almost 90 million immigrants and U.S.-born children of immigrants (second-generation Americans) in the United States, accounting for 28% of the overall U.S. population.

While immigration has increased drastically over the last century, the foreign born share of the population is, at 13.4, only somewhat below what it was at its peak in 1910 at 14.7%. A number of factors may be attributed to the decrease in the representation of foreign born residents in the United States. Most significant has been the change in the composition of immigrants; prior to 1890, 82% of immigrants came from North and Western Europe. From 1891 to 1920, that number dropped to 25%, with a rise in immigrants from East, Central, and South Europe, summing up to 64%. Animosity towards these different and foreign immigrants rose in the United States, resulting in much legislation to limit immigration.

Contemporary immigrants settle predominantly in seven states, California, New York, Florida, Texas, Pennsylvania, New Jersey and Illinois, comprising about 44% of the U.S. population as a whole. The combined total immigrant population of these seven states was 70% of the total foreign-born population in 2000.


The Census Bureau estimates the US population will grow from 317 million in 2014 to 417 million in 2060 with immigration, when nearly 20% will be foreign born. A 2015 report from the Pew Research Center projects that by 2065, non-Hispanic whites will account for 46% of the population, down from the 2005 figure of 67%. Non-Hispanic whites made up 85% of the population in 1960. It also foresees the Hispanic population rising from 17% in 2014 to 29% by 2060. The Asian population is expected to nearly double in 2060. Overall, the Pew Report predicts the population of the United States will rise from 296 million in 2005 to 441 million in 2065, but only to 338 million with no immigration.

In 35 of the country's 50 largest cities, non-Hispanic whites were at the last census or are predicted to be in the minority. In California, non-Hispanic whites slipped from 80% of the state's population in 1970 to 42% in 2001 and 39% in 2013.

Immigrant segregation declined in the first half of the 20th century, but has been rising over the past few decades. This has caused questioning of the correctness of describing the United States as a melting pot. One explanation is that groups with lower socioeconomic status concentrate in more densely populated area that have access to public transit while groups with higher socioeconomic status move to suburban areas. Another is that some recent immigrant groups are more culturally and linguistically different from earlier groups and prefer to live together due to factors such as communication costs. Another explanation for increased segregation is white flight.


Source: 1990, 2000 and 2010 decennial Census and 2017 American Community Survey

A survey of leading economists shows a consensus behind the view that high-skilled immigration makes the average American better off. A survey of the same economists also shows strong support behind the notion that low-skilled immigration makes the average American better off. According to David Card, Christian Dustmann, and Ian Preston, "most existing studies of the economic impacts of immigration suggest these impacts are small, and on average benefit the native population". In a survey of the existing literature, Örn B Bodvarsson and Hendrik Van den Berg write, "a comparison of the evidence from all the studies ... makes it clear that, with very few exceptions, there is no strong statistical support for the view held by many members of the public, namely that immigration has an adverse effect on native-born workers in the destination country."

Whereas the impact on the average native tends to be small and positive, studies show more mixed results for low-skilled natives, but whether the effects are positive or negative, they tend to be small either way.

Immigrants may often do types of work that natives are largely unwilling to do, contributing to greater economic prosperity for the economy as a whole: for instance, Mexican migrant workers taking up manual farm work in the United States has close to zero effect on native employment in that occupation, which means that the effect of Mexican workers on U.S. employment outside farm work was therefore most likely positive, since they raised overall economic productivity. Research indicates that immigrants are more likely to work in risky jobs than U.S.-born workers, partly due to differences in average characteristics, such as immigrants' lower English language ability and educational attainment. Further, some studies indicate that higher ethnic concentration in metropolitan areas is positively related to the probability of self-employment of immigrants.

Research also suggests that diversity has a net positive effect on productivity and economic prosperity. A study by Nathan Nunn, Nancy Qian and Sandra Sequeira found that the Age of Mass Migration (1850–1920) has had substantially beneficial long-term effects on U.S. economic prosperity: "locations with more historical immigration today have higher incomes, less poverty, less unemployment, higher rates of urbanization, and greater educational attainment. The long-run effects appear to arise from the persistence of sizeable short-run benefits, including earlier and more intensive industrialization, increased agricultural productivity, and more innovation." The authors also find that the immigration had short-term benefits: "that there is no evidence that these long-run benefits come at short-run costs. In fact, immigration immediately led to economic benefits that took the form of higher incomes, higher productivity, more innovation, and more industrialization."

Research also finds that migration leads to greater trade in goods and services. Using 130 years of data on historical migrations to the United States, one study finds "that a doubling of the number of residents with ancestry from a given foreign country relative to the mean increases by 4.2 percentage points the probability that at least one local firm invests in that country, and increases by 31% the number of employees at domestic recipients of FDI from that country. The size of these effects increases with the ethnic diversity of the local population, the geographic distance to the origin country, and the ethno-linguistic fractionalization of the origin country."

Some research suggests that immigration can offset some of the adverse effects of automation on native labor outcomes in the United States. By increasing overall demand, immigrants could push natives out of low-skilled manual labor into better paying occupations. A 2018 study in the "American Economic Review" found that the Bracero program (which allowed almost half a million Mexican workers to do seasonal farm labor in the United States) did not have any adverse impact on the labor market outcomes of American-born farm workers.

A 2011 literature review of the economic impacts of immigration found that the net fiscal impact of migrants varies across studies but that the most credible analyses typically find small and positive fiscal effects on average. According to the authors, "the net social impact of an immigrant over his or her lifetime depends substantially and in predictable ways on the immigrant's age at arrival, education, reason for migration, and similar".

A 2016 report by the National Academies of Sciences, Engineering, and Medicine concluded that over a 75-year time horizon, "the fiscal impacts of immigrants are generally positive at the federal level and generally negative at the state and local level." The reason for the costs to state and local governments is that the cost of educating the immigrants' children falls on state and local governments. According to a 2007 literature review by the Congressional Budget Office, "Over the past two decades, most efforts to estimate the fiscal impact of immigration in the United States have concluded that, in aggregate and over the long term, tax revenues of all types generated by immigrants—both legal and unauthorized—exceed the cost of the services they use."

According to James Smith, a senior economist at Santa Monica-based RAND Corporation and lead author of the United States National Research Council's study """", immigrants contribute as much as $10 billion to the U.S. economy each year. The NRC report found that although immigrants, especially those from Latin America, caused a net loss in terms of taxes paid versus social services received, immigration can provide an overall gain to the domestic economy due to an increase in pay for higher-skilled workers, lower prices for goods and services produced by immigrant labor, and more efficiency and lower wages for some owners of capital. The report also notes that although immigrant workers compete with domestic workers for low-skilled jobs, some immigrants specialize in activities that otherwise would not exist in an area, and thus can be beneficial for all domestic residents.

Immigration and foreign labor documentation fees increased over 80% in 2007, with over 90% of funding for USCIS derived from immigration application fees, creating many USCIS jobs involving immigration to US, such as immigration interview officials, finger print processor, Department of Homeland Security, etc.

Overall immigration has not had much effect on native wage inequality but low-skill immigration has been linked to greater income inequality in the native population.

Research on the economic effects of undocumented immigrants is scant but existing peer-reviewed studies suggest that the effects are positive for the native population and public coffers. A 2015 study shows that "increasing deportation rates and tightening border control weakens low-skilled labor markets, increasing unemployment of native low-skilled workers. Legalization, instead, decreases the unemployment rate of low-skilled natives and increases income per native." Studies show that legalization of undocumented immigrants would boost the U.S. economy; a 2013 study found that granting legal status to undocumented immigrants would raise their incomes by a quarter (increasing U.S. GDP by approximately $1.4 trillion over a ten-year period), and 2016 study found that "legalization would increase the economic contribution of the unauthorized population by about 20%, to 3.6% of private-sector GDP."

A 2007 literature by the Congressional Budget Office found that estimating the fiscal effects of undocumented immigrants has proven difficult: "currently available estimates have significant limitations; therefore, using them to determine an aggregate effect across all states would be difficult and prone to considerable error". The impact of undocumented immigrants differs on federal levels than state and local levels, with research suggesting modest fiscal costs at the state and local levels but with substantial fiscal gains at the federal level.

In 2009, a study by the Cato Institute, a free market think tank, found that legalization of low-skilled illegal resident workers in the US would result in a net increase in US GDP of $180 billion over ten years. The Cato Institute study did not examine the impact on per capita income for most Americans. Jason Riley notes that because of progressive income taxation, in which the top 1% of earners pay 37% of federal income taxes (even though they actually pay a lower tax percentage based on their income), 60% of Americans collect more in government services than they pay in, which also reflects on immigrants. In any event, the typical immigrant and his children will pay a net $80,000 more in their lifetime than they collect in government services according to the NAS. Legal immigration policy is set to maximize net taxation. Illegal immigrants even after an amnesty tend to be recipients of more services than they pay in taxes. In 2010, an econometrics study by a Rutgers economist found that immigration helped increase bilateral trade when the incoming people were connected via networks to their country of origin, particularly boosting trade of final goods as opposed to intermediate goods, but that the trade benefit weakened when the immigrants became assimilated into American culture.

According to NPR in 2005, about 3% of illegal immigrants were working in agriculture. The H-2A visa allows U.S. employers to bring foreign nationals to the United States to fill temporary agricultural jobs. The passing of tough immigration laws in several states from around 2009 provides a number of practical case studies. The state of Georgia passed immigration law HB 87 in 2011; this led, according to the coalition of top Kansas businesses, to 50% of its agricultural produce being left to rot in the fields, at a cost to the state of more than $400 million. Overall losses caused by the act were $1 billion; it was estimated that the figure would become over $20 billion if all the estimated 325,000 unauthorized workers left Georgia. The cost to Alabama of its crackdown in June 2011 has been estimated at almost $11 billion, with up to 80,000 unauthorized immigrant workers leaving the state.

Studies of refugees' impact on native welfare are scant but the existing literature shows a positive fiscal impact and mixed results (negative, positive and no significant effects) on native welfare. A 2017 paper by Evans and Fitzgerald found that refugees to the United States pay "$21,000 more in taxes than they receive in benefits over their first 20 years in the U.S." An internal study by the Department of Health and Human Services under the Trump administration, which was suppressed and not shown to the public, found that refugees to the United States brought in $63 billion more in government revenues than they cost the government. According to labor economist Giovanni Peri, the existing literature suggests that there are no economic reasons why the American labor market could not easily absorb 100,000 Syrian refugees in a year. Refugees integrate more slowly into host countries' labor markets than labor migrants, in part due to the loss and depreciation of human capital and credentials during the asylum procedure. 

According to one survey of the existing economic literature, "much of the existing research points towards positive net contributions by immigrant entrepreneurs." Areas where immigrant are more prevalent in the United States have substantially more innovation (as measured by patenting and citations). Immigrants to the United States create businesses at higher rates than natives. According to a 2018 paper, "first-generation immigrants create about 25% of new firms in the United States, but this share exceeds 40% in some states." Another 2018 paper links H-1B visa holders to innovation.

Immigrants have been linked to greater invention and innovation in the US. According to one report, "immigrants have started more than half (44 of 87) of America's startup companies valued at $1 billion or more and are key members of management or product development teams in over 70 percent (62 of 87) of these companies." Foreign doctoral students are a major source of innovation in the American economy. In the United States, immigrant workers hold a disproportionate share of jobs in science, technology, engineering, and math (STEM): "In 2013, foreign-born workers accounted for 19.2 percent of STEM workers with a bachelor's degree, 40.7 percent of those with a master's degree, and more than half—54.5 percent—of those with a Ph.D."

The Kauffman Foundation's index of entrepreneurial activity is nearly 40% higher for immigrants than for natives. Immigrants were involved in the founding of many prominent American high-tech companies, such as Google, Yahoo, YouTube, Sun Microsystems, and eBay.

Irish immigration was opposed in the 1850s by the nativist Know Nothing movement, originating in New York in 1843. It was engendered by popular fears that the country was being overwhelmed by Irish Catholic immigrants. On March 14, 1891, a lynch mob stormed a local jail and lynched several Italians following the acquittal of several Sicilian immigrants alleged to be involved in the murder of New Orleans police chief David Hennessy. The Congress passed the Emergency Quota Act in 1921, followed by the Immigration Act of 1924. The Immigration Act of 1924 was aimed at limiting immigration overall, and making sure that the nationalities of new arrivals matched the overall national profile.

A 2014 meta-analysis of racial discrimination in product markets found extensive evidence of minority applicants being quoted higher prices for products. A 1995 study found that car dealers "quoted significantly lower prices to white males than to black or female test buyers using identical, scripted bargaining strategies." A 2013 study found that eBay sellers of iPods received 21 percent more offers if a white hand held the iPod in the photo than a black hand.

Research suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of racial minorities among crime suspects. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for racial minorities. A 2012 study found that "(i) juries formed from all-white jury pools convict black defendants significantly (16 percentage points) more often than white defendants, and (ii) this gap in conviction rates is entirely eliminated when the jury pool includes at least one black member." Research has found evidence of in-group bias, where "black (white) juveniles who are randomly assigned to black (white) judges are more likely to get incarcerated (as opposed to being placed on probation), and they receive longer sentences." In-group bias has also been observed when it comes to traffic citations, as black and white cops are more likely to cite out-groups.

A 2015 study using correspondence tests "found that when considering requests from prospective students seeking mentoring in the future, faculty were significantly more responsive to White males than to all other categories of students, collectively, particularly in higher-paying disciplines and private institutions." Through affirmative action, there is reason to believe that elite colleges favor minority applicants.

A 2014 meta-analysis found extensive evidence of racial discrimination in the American housing market. Minority applicants for housing needed to make many more enquiries to view properties. Geographical steering of African-Americans in US housing remained significant. A 2003 study finds "evidence that agents interpret an initial housing request as an indication of a customer's preferences, but also are more likely to withhold a house from all customers when it is in an integrated suburban neighborhood (redlining). Moreover, agents' marketing efforts increase with asking price for white, but not for black, customers; blacks are more likely than whites to see houses in suburban, integrated areas (steering); and the houses agents show are more likely to deviate from the initial request when the customer is black than when the customer is white. These three findings are consistent with the possibility that agents act upon the belief that some types of transactions are relatively unlikely for black customers (statistical discrimination)."

A report by the federal Department of Housing and Urban Development where the department sent African-Americans and whites to look at apartments found that African-Americans were shown fewer apartments to rent and houses for sale.

Several meta-analyses find extensive evidence of ethnic and racial discrimination in hiring in the American labor market. A 2016 meta-analysis of 738 correspondence tests—tests where identical CVs for stereotypically black and white names were sent to employers—in 43 separate studies conducted in OECD countries between 1990 and 2015 finds that there is extensive racial discrimination in hiring decisions in Europe and North-America. These correspondence tests showed that equivalent minority candidates need to send around 50% more applications to be invited for an interview than majority candidates. A study that examine the job applications of actual people provided with identical résumés and similar interview training showed that African-American applicants with no criminal record were offered jobs at a rate as low as white applicants who had criminal records.

Racist thinking among and between minority groups does occur; examples of this are conflicts between blacks and Korean immigrants, notably in the 1992 Los Angeles Riots, and between African Americans and non-white Latino immigrants. There has been a long running racial tension between African American and Mexican prison gangs, as well as significant riots in California prisons where they have targeted each other, for ethnic reasons. There have been reports of racially motivated attacks against African Americans who have moved into neighborhoods occupied mostly by people of Mexican origin, and vice versa. There has also been an increase in violence between non-Hispanic whites and Latino immigrants, and between African immigrants and African Americans.

A 2018 study in the "American Sociological Review" found that within racial groups, most immigrants to the United States had fully assimilated within a span of 20 years. Immigrants arriving in the United States after 1994 assimilate more rapidly than immigrants who arrived in previous periods. Measuring assimilation can be difficult due to "ethnic attrition", which refers to when descendants of migrants cease to self-identify with the nationality or ethnicity of their ancestors. This means that successful cases of assimilation will be underestimated. Research shows that ethnic attrition is sizable in Hispanic and Asian immigrant groups in the United States. By taking account of ethnic attrition, the assimilation rate of Hispanics in the United States improves significantly. A 2016 paper challenges the view that cultural differences are necessarily an obstacle to long-run economic performance of migrants. It finds that "first generation migrants seem to be less likely to success the more culturally distant they are, but this effect vanishes as time spent in the USA increases." A 2020 study found that recent immigrants to the United States assimilated at a similar pace as historical immigrants.

Immigration from South Asia and elsewhere has contributed to enlarging the religious composition of the United States. Islam in the United States is growing mainly due to immigration. Hinduism in the United States, Buddhism in the United States, and Sikhism in the United States are other examples. Whereas non-Christians together constitute only 4% of the U.S. population, they made up 20% of the 2003 cohort of new immigrants.

Since 1992, an estimated 1.7 million Muslims, approximately 1 million Hindus, and approximately 1 million Buddhists have immigrated legally to the United States.

Conversely, non-religious are underrepresented in the immigrant populations. Although "other" non-Christian religions are also slightly more common among immigrants than among U.S. adults—1.9% compared with 1.0%—those professing no religion are slightly under-represented among new immigrants. Whereas 12% of immigrants said they had no religion, the figure was 15% for adult Americans. This lack of representation for non-religious could be related to stigmas around atheists and agnostics or could relate to the need for identity when entering a new country.

The American Federation of Labor (AFL), a coalition of labor unions formed in the 1880s, vigorously opposed unrestricted immigration from Europe for moral, cultural, and racial reasons. The issue unified the workers who feared that an influx of new workers would flood the labor market and lower wages. Nativism was not a factor because upwards of half the union members were themselves immigrants or the sons of immigrants from Ireland, Germany and Britain. However, nativism was a factor when the AFL even more strenuously opposed all immigration from Asia because it represented (to its Euro-American members) an alien culture that could not be assimilated into American society. The AFL intensified its opposition after 1906 and was instrumental in passing immigration restriction bills from the 1890s to the 1920s, such as the 1921 Emergency Quota Act and the Immigration Act of 1924, and seeing that they were strictly enforced.

Mink (1986) concludes that the link between the AFL and the Democratic Party rested in part on immigration issues, noting the large corporations, which supported the Republicans, wanted more immigration to augment their labor force.

United Farm Workers during Cesar Chavez tenure was committed to restricting immigration. Chavez and Dolores Huerta, cofounder and president of the UFW, fought the Bracero Program that existed from 1942 to 1964. Their opposition stemmed from their belief that the program undermined U.S. workers and exploited the migrant workers. Since the Bracero Program ensured a constant supply of cheap immigrant labor for growers, immigrants could not protest any infringement of their rights, lest they be fired and replaced. Their efforts contributed to Congress ending the Bracero Program in 1964. In 1973, the UFW was one of the first labor unions to oppose proposed employer sanctions that would have prohibited hiring illegal immigrants.

On a few occasions, concerns that illegal immigrant labor would undermine UFW strike campaigns led to a number of controversial events, which the UFW describes as anti-strikebreaking events, but which have also been interpreted as being anti-immigrant. In 1973, Chavez and members of the UFW marched through the Imperial and Coachella Valleys to the border of Mexico to protest growers' use of illegal immigrants as strikebreakers. Joining him on the march were Reverend Ralph Abernathy and U.S. Senator Walter Mondale. In its early years, the UFW and Chavez went so far as to report illegal immigrants who served as strikebreaking replacement workers (as well as those who refused to unionize) to the Immigration and Naturalization Service.

In 1973, the United Farm Workers set up a "wet line" along the United States-Mexico border to prevent Mexican immigrants from entering the United States illegally and potentially undermining the UFW's unionization efforts. During one such event, in which Chavez was not involved, some UFW members, under the guidance of Chavez's cousin Manuel, physically attacked the strikebreakers after peaceful attempts to persuade them not to cross the border failed.

In 1979, Chavez used a forum of a U.S. Senate committee hearing to denounce the federal immigration service, which he said the U.S. Immigration and Naturalization Service purportedly refused to arrest illegal Mexican immigrants who Chavez claims are being used to break the union's strike.

A "Boston Globe" article attributed Barack Obama's win in the 2008 U.S. Presidential election to a marked reduction over the preceding decades in the percentage of whites in the American electorate, attributing this demographic change to the Immigration Act of 1965. The article quoted Simon Rosenberg, president and founder of the New Democrat Network, as having said that the Act is "the most important piece of legislation that no one's ever heard of," and that it "set America on a very different demographic course than the previous 300 years."

Immigrants differ on their political views; however, the Democratic Party is considered to be in a far stronger position among immigrants overall. Research shows that religious affiliation can also significantly impact both their social values and voting patterns of immigrants, as well as the broader American population. Hispanic evangelicals, for example, are more strongly conservative than non-Hispanic evangelicals. This trend is often similar for Hispanics or others strongly identifying with the Catholic Church, a religion that strongly opposes abortion and gay marriage.

The key interests groups that lobby on immigration are religious, ethnic and business groups, together with some liberals and some conservative public policy organizations. Both the pro- and anti- groups affect policy.

Studies have suggested that some special interest group lobby for less immigration for their own group and more immigration for other groups since they see effects of immigration, such as increased labor competition, as detrimental when affecting their own group but beneficial when affecting other groups.

A 2011 paper found that both pro- and anti-immigration special interest groups play a role in migration policy. "Barriers to migration are lower in sectors in which business lobbies incur larger lobbying expenditures and higher in sectors where labor unions are more important." A 2011 study examining the voting of US representatives on migration policy suggests that "representatives from more skilled labor abundant districts are more likely to support an open immigration policy towards the unskilled, whereas the opposite is true for representatives from more unskilled labor abundant districts."

After the 2010 election, Gary Segura of Latino Decisions stated that Hispanic voters influenced the outcome and "may have saved the Senate for Democrats". Several ethnic lobbies support immigration reforms that would allow illegal immigrants that have succeeded in entering to gain citizenship. They may also lobby for special arrangements for their own group. The Chairman for the Irish Lobby for Immigration Reform has stated that "the Irish Lobby will push for any special arrangement it can get—'as will every other ethnic group in the country.'" The irredentist and ethnic separatist movements for Reconquista and Aztlán see immigration from Mexico as strengthening their cause.

The book "Ethnic Lobbies and US Foreign Policy" (2009) states that several ethnic special interest groups are involved in pro-immigration lobbying. Ethnic lobbies also influence foreign policy. The authors write that "Increasingly, ethnic tensions surface in electoral races, with House, Senate, and gubernatorial contests serving as proxy battlegrounds for antagonistic ethnoracial groups and communities. In addition, ethnic politics affect party politics as well, as groups compete for relative political power within a party". However, the authors argue that currently ethnic interest groups, in general, do not have too much power in foreign policy and can balance other special interest groups.

In a 2012 news story, "Reuters" reported, "Strong support from Hispanics, the fastest-growing demographic in the United States, helped tip President Barack Obama's fortunes as he secured a second term in the White House, according to Election Day polling."

Lately, there is talk among several Republican leaders, such as governors Bobby Jindal and Susana Martinez, of taking a new, friendlier approach to immigration. Former US Secretary of Commerce Carlos Gutierrez is promoting the creation of Republicans for Immigration Reform.

Bernie Sanders opposes guest worker programs and is also skeptical about skilled immigrant (H-1B) visas, saying, "Last year, the top 10 employers of H-1B guest workers were all offshore outsourcing companies. These firms are responsible for shipping large numbers of American information technology jobs to India and other countries." In an interview with "Vox" he stated his opposition to an open borders immigration policy, describing it as:

... a right-wing proposal, which says essentially there is no United States ... you're doing away with the concept of a nation-state. What right-wing people in this country would love is an open-border policy. Bring in all kinds of people, work for $2 or $3 an hour, that would be great for them. I don't believe in that. I think we have to raise wages in this country, I think we have to do everything we can to create millions of jobs.

April 2018, Trump calls for National Guard at the border to secure the ongoing attempts at a border wall along the United States-Mexico border. According to the LAtimes, "Defense Secretary James N. Mattis has signed an order to send up to 4,000 National Guard troops to the U.S.-Mexico border but barred them from interacting with migrants detained by the Border Patrol in most circumstances".

The caravan of migrants from Central America have reached the United States to seek asylum. The last of the caravan have arrived and are processing as of May 4, 2018. Remarks by Attorney General Sessions have expressed hesitation with asylum seekers. Sessions has stated, "The system is being gamed, there's no doubt about it". This statement implied asylum seekers were attempting to immigrate to the United States for work or various other reasons rather than seeking refuge.

A 2020 study found no evidence that immigration was associated with adverse health impacts for native-born Americans. To the contrary, the study found that "the presence of low‐skilled immigrants may improve the health of low‐skilled U.S.‐born individuals," possibly by nudging low-skilled Americans from physically dangerous and risky jobs toward occupations that require more communication and interactive ability.

On average, per capita health care spending is lower for immigrants than it is for native-born Americans. The non-emergency use of emergency rooms ostensibly indicates an incapacity to pay, yet some studies allege disproportionately lower access to unpaid health care by immigrants. For this and other reasons, there have been various disputes about how much immigration is costing the United States public health system. University of Maryland economist and Cato Institute scholar Julian Lincoln Simon concluded in 1995 that while immigrants probably pay more into the health system than they take out, this is not the case for elderly immigrants and refugees, who are more dependent on public services for survival. Immigration itself may impact women's health. A 2017 study found that Latino women suffer higher rates of intimate partner violence (IPV) than native US women. Migration may worsen IPV rates and outcomes. Migration itself may not cause IPV, but it may make it more difficult for women to get help. According to Kim et al., the IPV is usually the result of unequal family structures rather than the process of migration.

Immigration from areas of high incidences of disease is thought to have fueled the resurgence of tuberculosis (TB), chagas, and hepatitis in areas of low incidence. According to Centers for Disease Control and Prevention (CDC), TB cases among foreign-born individuals remain disproportionately high, at nearly nine times the rate of U.S.-born persons. To reduce the risk of diseases in low-incidence areas, the main countermeasure has been the screening of immigrants on arrival. HIV/AIDS entered the United States in around 1969, likely through a single infected immigrant from
Haiti. Conversely, many new HIV infections in Mexico can be traced back to the United States. People infected with HIV were banned from entering the United States in 1987 by executive order, but the 1993 statute supporting the ban was lifted in 2009. The executive branch is expected to administratively remove HIV from the list of infectious diseases barring immigration, but immigrants generally would need to show that they would not be a burden on public welfare. Researchers have also found what is known as the "healthy immigrant effect", in which immigrants in general tend to be healthier than individuals born in the U.S. Immigrants are more likely than native-born Americans to have a medical visit labeled uncompensated care.

There is no empirical evidence that either legal or illegal immigration increases crime in the United States. In fact, a majority of studies in the U.S. have found lower crime rates among immigrants than among non-immigrants, and that higher concentrations of immigrants are associated with lower crime rates. Explanations proposed to account for this relationship have included ethnic enclaves, self-selection, and the hypothesis that immigrants revitalize communities to which they emigrate.

Some research even suggests that increases in immigration may partly explain the reduction in the U.S. crime rate. A 2005 study showed that immigration to large U.S. metropolitan areas does not increase, and in some cases decreases, crime rates there. A 2009 study found that recent immigration was not associated with homicide in Austin, Texas. The low crime rates of immigrants to the United States despite having lower levels of education, lower levels of income and residing in urban areas (factors that should lead to higher crime rates) may be due to lower rates of antisocial behavior among immigrants. A 2015 study found that Mexican immigration to the United States was associated with an increase in aggravated assaults and a decrease in property crimes. A 2016 study finds no link between immigrant populations and violent crime, although there is a small but significant association between undocumented immigrants and drug-related crime.

A 2018 study found that undocumented immigration to the United States did not increase violent crime. Research finds that Secure Communities, an immigration enforcement program which led to a quarter of a million of detentions (when the study was published; November 2014), had no observable impact on the crime rate. A 2015 study found that the 1986 Immigration Reform and Control Act, which legalized almost 3 million immigrants, led to "decreases in crime of 3–5 percent, primarily due to decline in property crimes, equivalent to 120,000-180,000 fewer violent and property crimes committed each year due to legalization".

According to one study, sanctuary cities—which adopt policies designed to not prosecute people solely for being an illegal immigrant—have no statistically meaningful effect on crime.

One of the first political analyses in the U.S. of the relationship between immigration and crime was performed in the beginning of the 20th century by the Dillingham Commission, which found a relationship especially for immigrants from non-Northern European countries, resulting in the sweeping 1920s immigration reduction acts, including the Emergency Quota Act of 1921, which favored immigration from northern and western Europe. Recent research is skeptical of the conclusion drawn by the Dillingham Commission. One study finds that "major government commissions on immigration and crime in the early twentieth century relied on evidence that suffered from aggregation bias and the absence of accurate population data, which led them to present partial and sometimes misleading views of the immigrant-native criminality comparison. With improved data and methods, we find that in 1904, prison commitment rates for more serious crimes were quite similar by nativity for all ages except ages 18 and 19, for which the commitment rate for immigrants was higher than for the native-born. By 1930, immigrants were less likely than natives to be committed to prisons at all ages 20 and older, but this advantage disappears when one looks at commitments for violent offenses."

For the early twentieth century, one study found that immigrants had "quite similar" imprisonment rates for major crimes as natives in 1904 but lower for major crimes (except violent offenses; the rate was similar) in 1930. Contemporary commissions used dubious data and interpreted it in questionable ways.

Research suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of immigrants among crime suspects. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for immigrants.

Crimmigration has emerged as a field in which critical immigration scholars conceptualize the current immigration law enforcement system. Crimmigration is broadly defined as the convergence of the criminal justice system and immigration enforcement, where immigration law enforcement has adopted the "criminal" law enforcement approach. This frames undocumented immigrants as "criminal" deviants and security risks. Crime and migration control have become completely intertwined, so much so that both undocumented and documented individuals suspected of being a noncitizen may be targeted.

Using a "crimmigration" point of thought, César Cuauhtémoc García Hernández explains the criminalization of undocumented immigrants began in the aftermath of the civil rights movement. Michelle Alexander explores how the U.S. criminal justice system is made of "colorblind" policies and law enforcement practices that have shaped the mass incarceration of people of color into an era of "The New Jim Crow" As Alexander and García Hernández state, overt racism and racist laws became culturally scorned, and covert racism became the norm. This new form of racism focuses on penalizing criminal activity and promoting "neutral" rhetoric.

"Crimmigration" recognizes how laws and policies throughout different states contribute to the convergence of criminal law enforcement and immigration law. For example, states are implementing a variety of immigration related criminal offenses that are punishable by imprisonment. California, Oregon, and Wyoming criminalize the use of fraudulent immigration or citizenship documents. Arizona allows judges to confine witnesses in certain "criminal" cases if they are suspected of being in the U.S. without documentation. The most common violations of immigration law on the federal level are unauthorized entry (a federal misdemeanor) and unauthorized reentry (a federal felony). These "offenses" deemed as "crimes" under immigration law set the tone of "crimmigration" and for what García Hernández refers to as the "removal pipeline" of immigrants.

Some scholars focus on the organization of "crimmigration" as it relates to the mass removal of certain immigrants. Jennifer Chacón finds that immigration law enforcement is being decentralized. Customs and Border Patrol (CBP), Immigration and Customs Enforcement (ICE), and the Department of Homeland Security (DHS) are the central law enforcement agencies in control of enforcing immigration law. However, other federal, state and local law enforcement agencies, such as sheriff's offices, municipal police departments, the Federal Bureau of Investigation (FBI), and the Drug and Enforcement Agency (DEA), aid in immigrant removal. In 1996, Congress expanded power to state and local law enforcement agencies to enforce federal immigration law. These agencies keep people locked up in jails or prison when they receive an "immigration detainer" from ICE, and therefore aid in interior enforcement. In addition, some agencies participate in the State Criminal Alien Assistance Program ("SCAAP"), which gives these agencies financial incentives to cooperate with ICE in identifying immigrants in their custody.

Scientific laboratories and startup internet opportunities have been a powerful American magnet. By 2000, 23% of scientists with a PhD in the U.S. were immigrants, including 40% of those in engineering and computers. Roughly a third of the United States' college and universities graduate students in STEM fields are foreign nationals—in some states it is well over half of their graduate students. On Ash Wednesday, March 5, 2014, the presidents of 28 Catholic and Jesuit colleges and universities, joined the "Fast for Families" movement. The "Fast for Families" movement reignited the immigration debate in the fall of 2013 when the movement's leaders, supported by many members of Congress and the President, fasted for twenty-two days on the National Mall in Washington, D.C.

A study on public schools in California found that white enrollment declined in response to increases in the number of Spanish-speaking Limited English Proficient and Hispanic students. This white flight was greater for schools with relatively larger proportions of Spanish-speaking Limited English Proficient.

A North Carolina study found that the presence of Latin American children in schools had no significant negative effects on peers, but that students with limited English skills had slight negative effects on peers.

In the United States, a significant proportion of scientists and engineers are foreign-born, as well as students in science and engineering programs. However, this is not unique to the US since foreigners make up significant amounts of scientists and engineers in other countries. As of 2011, 28% of graduate students in science, engineering, and health are foreign. The number of science and engineering (S&E) bachelor's degrees has risen steadily over the past 15 years, reaching a new peak of about half a million in 2009. Since 2000, foreign born students in the United States have consistently earned a small share (3%-4%) of S&E degrees at the bachelor's level. Foreign students make up a much higher proportion of S&E master's degree recipients than of bachelor's or associate degree recipients. In 2009, foreign students earned 27% of S&E master's degrees and 33% in doctorate degrees. Significant numbers of foreign born students in science and engineering are not unique to America since foreign students now account for nearly 60% of graduate students in mathematics, computer sciences, and engineering globally. In Switzerland and the United Kingdom, more than 40% of doctoral students are foreign. A number of other countries, including New Zealand, Austria, Australia, Belgium, Canada, and the United States, have relatively high percentages (more than 20%) of doctoral students who are foreign. Foreign student enrollment in the United Kingdom has been increasing. In 2008, foreign students made up 47% of all graduate students studying S&E in the United Kingdom (an increase from 32% in 1998). Top destinations for international students include the United Kingdom (12%), Germany (9%), and France (9%). Together with the U.S., these countries receive more than half of all internationally mobile students worldwide. Although the United States continues to attract the largest number and fraction of foreign students worldwide, its share of foreign students has decreased in recent years.

55% of Ph.D. students in engineering in the United States are foreign born (2004). Between 1980 and 2000, the percentage of Ph.D. scientists and engineers employed in the United States who were born abroad has increased from 24% to 37%. 45% of Ph.D. physicists working in the United States are foreign born in 2004. 80% of total post-doctoral chemical and materials engineering in the United States are foreign-born (1988).

At the undergraduate level, US-born engineering students constitute upwards of 90-95% of the student population (most foreign born candidates for engineering graduate schools are trained in their home countries). However, the pool of BS engineering graduates with US citizenship is much larger than the number who apply to engineering graduate schools. The proportion of foreign-born engineers among assistant professors younger than 35 years has increased from 10% in 1972 to 50%-55% in 1983-1985, illustrating a dramatic increase on US dependence on foreign-born students in the US college system. The increase in non-citizen assistant professors of engineering is the result of the fact that, in recent years, foreign-born engineers received close to 50 percent of newly awarded engineering doctorates (naturalized citizens accounted for about 4 percent) and, furthermore, they entered academe in disproportionately large numbers. 33% of all U.S. Ph.D.s in science and engineering are now awarded to foreign born graduate students as of 2004.

In 1982, foreign-born engineers constituted about 3.6% of all engineers employed in the United States, 13.9% of which were naturalized; and foreign-born Phds in Engineering constituted 15% and 20% were naturalized. In 1985, foreign-born Phds represented almost 33% of the engineering post-doctorate researchers in US universities. Foreign-born Phd engineers often accept postdoctoral positions because other employment is unavailable until green card is obtained. A system that further incentivising replacement of US-citizens in the upper echelons of academic and private sector engineering firms due to higher educational attainment relative to native-born engineer who for the most part do train beyond undergraduate level.

In recent years, the number of applicants for faculty openings at research universities have increased dramatically. Numbers of 50 to 200 applications for a single faculty opening have become typical, yet even with such high numbers of applicants, the foreign-born component is in excess of 50%. 60% of the top science students and 65 percent of the top math students in the United States are the children of immigrants. In addition, foreign-born high school students make up 50 percent of the 2004 U.S.Math Olympiad's top scorers, 38 percent of the U.S. Physics Team, and 25 percent of the Intel Science Talent Search finalists—the United States' most prestigious awards for young scientists and mathematicians.

Among 1985 foreign-born engineering doctorate holders, about 40% expected to work in the United States after graduating. An additional 17 percent planned to stay on as post-doctorates, and most of these are likely to remain permanently in the United States. Thus, almost 60% of foreign-born engineering doctorate holders are likely to become part of the US engineering labor force within a few years after graduating. The other approximately 40% of foreign born engineering PhDs mostly likely find employment working for multinational corporations outside of the US.

In the 2004 Intel Science Talent Search, more children (18) have parents who entered the country on H-1B (professional) visas than parents born in the United States (16). To place this finding in perspective, note that new H-1B visa holders each year represent less than 0.04 percent of the U.S. population. Foreign born faculty now account for over 50% of faculty in engineering (1994).

27 out the 87 (more than 30%) American Nobel Prize winners in Medicine and Physiology between 1901 and 2005 were born outside the US.

1993 median salaries of U.S. recipients of Ph.D.s in Science and Engineering foreign-born vs. native-born were as follows:
Major American corporations spent $345 million lobbying for just three pro-immigration bills between 2006 and 2008.

The two most prominent groups lobbying for more restrictive immigration policies for the United States are NumbersUSA and the Federation for American Immigration Reform (FAIR); additionally, the Center for Immigration Studies think tank produces policy analysis supportive of a more restrictive stance.

The ambivalent feeling of Americans toward immigrants is shown by a positive attitude toward groups that have been visible for a century or more, and much more negative attitude toward recent arrivals. For example, a 1982 national poll by the Roper Center at the University of Connecticut showed respondents a card listing a number of groups and asked, "Thinking both of what they have contributed to this country and have gotten from this country, for each one tell me whether you think, on balance, they've been a good or a bad thing for this country," which produced the results shown in the table. "By high margins, Americans are telling pollsters it was a very good thing that Poles, Italians, and Jews immigrated to America. Once again, it's the newcomers who are viewed with suspicion. This time, it's the Mexicans, the Filipinos, and the people from the Caribbean who make Americans nervous."

In a 2002 study, which took place soon after the September 11 attacks, 55% of Americans favored decreasing legal immigration, 27% favored keeping it at the same level, and 15% favored increasing it.

In 2006, the immigration-reduction advocacy think tank the Center for Immigration Studies released a poll that found that 68% of Americans think U.S. immigration levels are too high, and just 2% said they are too low. They also found that 70% said they are less likely to vote for candidates that favor increasing legal immigration. In 2004, 55% of Americans believed legal immigration should remain at the current level or increased and 41% said it should be decreased. The less contact a native-born American has with immigrants, the more likely one would have a negative view of immigrants.

One of the most important factors regarding public opinion about immigration is the level of unemployment; anti-immigrant sentiment is where unemployment is highest, and vice versa.

Surveys indicate that the U.S. public consistently makes a sharp distinction between legal and illegal immigrants, and generally views those perceived as "playing by the rules" with more sympathy than immigrants that have entered the country illegally.

According to a Gallup poll in July 2015, immigration is the fourth most important problem facing the United States and seven percent of Americans said it was the most important problem facing America today. In March 2015, another Gallup poll provided insight into American public opinion on immigration; the poll revealed that 39% of people worried about immigration "a great deal." A January poll showed that only 33% of Americans were satisfied with the current state of immigration in America. As an issue that is very important to Americans, polling reveals change in sentiment over time and diverse opinions regarding how to handle immigration.

Before 2012, majority of Americans supported securing United States borders compared to dealing with illegal immigrants in the United States. In 2013, that trend has reversed and 55% of people polled by Gallup revealed that they would choose "developing a plan to deal with immigrants who are currently in the U.S. illegally." Changes regarding border control are consistent across party lines, with Republicans saying that "securing U.S. borders to halt flow of illegal immigrants" is extremely important decreasing from 68% in 2011 to 56% in 2014. Meanwhile, Democrats who chose extremely important shifted from 42% in 2011 to 31% in 2014. In July 2013, 87% of Americans said they would vote in support of a law that would "allow immigrants already in the country to become U.S. citizens if they meet certain requirements including paying taxes, having a criminal background check and learning English." However, in the same survey, 83% also said they would support the tightening of U.S. border security.

Donald Trump's campaign for Presidency focused on a rhetoric of reducing illegal immigration and toughening border security. In July 2015, 48% of Americans thought that Donald Trump would do a poor job of handling immigration problems. In November 2016, 55% of Trump's voters thought that he would do the right thing in regards to illegal immigration. In general, Trump supporters are not united upon how to handle immigration. In December 2016, Trump voters were polled and 60% said that "undocumented immigrants in the U.S. who meet certain requirements should be allowed to stay legally."

American opinion regarding how immigrants affect the country and how the government should respond to illegal immigration have changed over time. In 2006, out of all U.S. adults surveyed, 28% declared that they believed the growing number of immigrants helped American workers and 55% believed that it hurt American workers. In 2016, those views had changed, with 42% believing that they helped and 45% believing that they hurt. The PRRI 2015 American Values Atlas showed that between 46% and 53% of Americans believed that "the growing number of newcomers from other countries ... strengthens American society." In the same year, 57% and 66% of Americans chose that the U.S. should "allow [immigrants living in the U.S. illegally] a way to become citizens provided they meet certain requirements." 

In February 2017, the American Enterprise Institute released a report on recent surveys about immigration issues. In July 2016, 63% of Americans favored the temporary bans of immigrants from areas with high levels of terrorism and 53% said the U.S. should allow fewer refugees to enter the country. In November 2016, 55% of Americans were opposed to building a border wall with Mexico. Since 1994, Pew Research center has tracked a change from 63% of Americans saying that immigrants are a burden on the country to 27%.

Public response to the Trump Administration’s ‘zero tolerance’ policy: What really stood out to the public in regards to the issue were the ways in which children were treated during their detainment. For example in Southern Texas, children were held in an old warehouse. Hundreds of children waited in cages made of metal fencing. One of the cages had 20 children inside, looking throughout the facility bottles of water and bags of chips can be found scattered about. Children were also forced to use large foil sheets as blankets. This created massive amounts of outrage, the campaign ‘Close the Camps’ was created and the facilities that were detaining young children were compared to concentration camps and internment camps. Many have begun to recognize the ways in which these policies are negatively impacting the children that have been separated from their families. There has been research showing that this has had psychological impacts on these young children, many of them have been diagnosed with post-traumatic stress disorder, these children are still distraught from the stressors of living their home countries and from the journey. Due to this, when these children are separated they showed more feelings of fear, abandonment and post-traumatic stress symptoms than children who were not separated from their families.

Religious figures in the United States have put forth their views on the topic of immigration as informed by their religious traditions.


Laws concerning immigration and naturalization include:

AEDPA and IIRARA exemplify many categories of criminal activity for which immigrants, including green card holders, can be deported and have imposed mandatory detention for certain types of cases.

In contrast to economic migrants, who generally do not gain legal admission, refugees, as defined by international law, can gain legal status through a process of seeking and receiving asylum, either by being designated a refugee while abroad, or by physically entering the United States and requesting asylum status thereafter. A specified number of legally defined refugees, who either apply for asylum overseas or after arriving in the U.S., are admitted annually. Refugees compose about one-tenth of the total annual immigration to the United States, though some large refugee populations are very prominent. In the year 2014, the number of asylum seekers accepted into the U.S. was about 120,000. This compared with about 31,000 in the UK and 13,500 in Canada. Japan accepted just 41 refugees for resettlement in 2007.

Since 1975, more than 1.3 million refugees from Asia have been resettled in the United States. Since 2000 the main refugee-sending regions have been Somalia, Liberia, Sudan, and Ethiopia. The ceiling for refugee resettlement for fiscal year 2008 was 80,000 refugees. The United States expected to admit a minimum of 17,000 Iraqi refugees during fiscal year 2009. The U.S. has resettled more than 42,000 Bhutanese refugees from Nepal since 2008.

In fiscal year 2008, the Office of Refugee Resettlement (ORR) appropriated over $655 million for long-term services provided to refugees after their arrival in the US. The Obama administration has kept to about the same level.

A common problem in the current system for asylum seekers is the lack of resources. Asylum offices in the United States receive more applications for asylum than they can process every month and every year. These continuous applications pile onto the backlog.

In removal proceedings in front of an immigration judge, cancellation of removal is a form of relief that is available for certain long-time residents of the United States. It allows a person being faced with the threat of removal to obtain permanent residence if that person has been physically present in the U.S. for at least ten years, has had good moral character during that period, has not been convicted of certain crimes, and can show that removal would result in exceptional and extremely unusual hardship to his or her U.S. citizen or permanent resident spouse, children, or parent. This form of relief is only available when a person is served with a Notice to Appear to appear in the proceedings in the court.

Members of Congress may submit private bills granting residency to specific named individuals. A special committee vets the requests, which require extensive documentation. The Central Intelligence Agency has the statutory authority to admit up to one hundred people a year outside of normal immigration procedures, and to provide for their settlement and support. The program is called "PL110", named after the legislation that created the agency, Public Law 110, the Central Intelligence Agency Act.

The illegal immigrant population of the United States is estimated to be between 11 and 12 million. The population of unauthorized immigrants peaked in 2007 and has declined since that time. The majority of the U.S. unauthorized immigrants are from Mexico, but "their numbers (and share of the total) have been declining" and as of 2016 Mexicans no longer make up a clear majority of unauthorized immigrants, as they did in the past. Unauthorized immigrants made up about 5% of the total U.S. civilian labor force in 2014. By the 2010s, an increasing share of U.S. unauthorized immigrants were long-term residents; in 2015, 66% of adult unauthorized residents had lived in the country for at least ten years, while only 14% had lived in the U.S. for less than five years.

In June 2012, President Obama issued a memorandum instructing officers of the federal government to defer deporting young undocumented immigrants who were brought to the U.S. as children as part of the Deferred Action for Childhood Arrivals (DACA) program. Under the program, eligible recipients who applied and were granted DACA status were granted a two-year deferral from deportation and temporary eligibility to work legally in the country. Among other criteria, in order to be eligible a youth applicant must (1) be between age 15 and 31; (2) have come to the United States before the age of 16; (3) have lived in the U.S. continuously for at least five years; (4) be a current student, or have earned a high school diploma or equivalent, or have received an honorable discharge from the U.S. armed services; and (5) must not "have not been convicted of a felony, significant misdemeanor, or three or more misdemeanors, and do not otherwise pose a threat to public safety or national security." The Migration Policy Institution estimated that as of 2016, about 1.3 million unauthorized young adults ages 15 and older were "immediately eligible for DACA"; of this eligible population, 63% had applied as of March 2016.

Children of legal migrants will not qualify as Dreamers under DACA protection because they entered the country legally. This is highlighted as the biggest contradiction in US immigration policy by many advocates of legal immigrants.

In 2014, President Obama announced a set of executive actions, the Deferred Action for Parents of Americans and Lawful Permanent Residents. Under this program, "unauthorized immigrants who are parents of U.S. citizens or lawful permanent residents (LPRs) would qualify for deferred action for three years if they meet certain other requirements." A February 2016 Migration Policy Institute/Urban Institute report found that about 3.6 million people were potentially eligible for DAPA and "more than 10 million people live in households with at least one potentially DAPA-eligible adult, including some 4.3 million children under age 18 - an estimated 85 percent of whom are U.S. citizens." The report also found that "the potentially DAPA eligible are well settled with strong U.S. roots, with 69 percent having lived in the United States ten years or more, and 25 percent at least 20 years."

Although not without precedent under prior presidents, President Obama's authority to create DAPA and expand DACA were challenged in the federal courts by Texas and 25 other states. In November 2015, the U.S. Court of Appeals for the Fifth Circuit, in a 2-1 decision in "United States v. Texas", upheld a preliminary injunction blocking the programs from going forward. The case was heard by the U.S. Supreme Court, which in June 2016 deadlocked 4-4, thus affirming the ruling of the Fifth Circuit but setting no nationally binding precedent.

On November 15, 2013, the United States Citizenship and Immigration Services announced that they would be issuing a new policy memorandum called "parole in place." Parole in place would offer green cards to immigrant parents, spouses and children of active military duty personnel. Prior to this law relatives of military personnel – excluding husbands and wives – were forced to leave the United States and apply for green cards in their home countries. The law allows for family members to avoid the possible ten-year bar from the United States and remain in the United States while applying for lawful permanent residence. The parole status, given in one year terms, will be subject to the family member being "absent a criminal conviction or other serious adverse factors."

Military children born in foreign countries are considered American from birth assuming both parents were American citizens at the time of birth. Children born to American citizens will have to process Conciliary Reports of Birth Abroad. This report of birth abroad is the equivalent of a birth certificate and the child will use the report in place of a Birth Certificate for documentation. However, children born in foreign countries to United States servicemembers before they have gained citizenship could only gain citizenship through the naturalization process.

Most immigration proceedings are civil matters, including deportation proceedings, asylum cases, employment without authorization, and visa overstay. People who evade border enforcement (such as by crossing outside any official border checkpoint), who commit fraud to gain entry, or who commit identity theft to gain employment, may face criminal charges. People entering illegally were seldom charged with this crime until Operation Streamline in 2005. Conviction of this crime generally leads to a prison term, after which the person is deported if they are not eligible to remain in the country.

The guarantees under the Sixth Amendment to the United States Constitution, such as the right to counsel, and the right to a jury trial, have not been held to apply to civil immigration proceedings. As a result, people generally represent themselves in asylum and deportation cases unless they can afford an immigration lawyer or receive assistance from a legal charity. In contrast, the Due Process Clause of the Fifth Amendment "has" been applied to immigration proceedings. Because the right to confrontation in the Sixth Amendment does not apply, people can be ordered deported "in absentia" - without being present at the immigration proceeding.

Removal proceedings are considered administrative proceedings under the authority of the United States Attorney General, acting through the Executive Office for Immigration Review, part of the Justice Department. Immigration judges are employees of the Justice Department, and thus part of the executive branch rather than the judicial branch of government. Appeals are heard within the EOIR by the Board of Immigration Appeals, and the Attorney General may intervene in individual cases, within the bounds of due process.

After various actions by Attorney General Jeff Sessions pressuring judges to speed up deportations, the National Association of Immigration Judges and "The Boston Globe" editorial board called for moving immigration courts to the judicial branch, to prevent abuse by strengthening separation of powers.

Whether people who are awaiting a decision on their deportation are detained or released to live in the United States in the meantime (possibly paying bail) is a matter of both law and discretion of the Justice Department. The policy has varied over time and differs for those with crimes (including entry outside an official checkpoint) versus civil infractions.

The 2001 Supreme Court case "Zadvydas v. Davis" held that immigrants who cannot be deported because no country will accept them cannot be detained indefinitely.

The history of immigration to the United States is the history of the country itself, and the journey from beyond the sea is an element found in American folklore, appearing over and over again in everything from "The Godfather" to "Gangs of New York" to "The Song of Myself" to Neil Diamond's "America" to the animated feature "An American Tail".

From the 1880s to the 1910s, vaudeville dominated the popular image of immigrants, with very popular caricature portrayals of ethnic groups. The specific features of these caricatures became widely accepted as accurate portrayals.

In "The Melting Pot" (1908), playwright Israel Zangwill (1864–1926) explored issues that dominated Progressive Era debates about immigration policies. Zangwill's theme of the positive benefits of the American melting pot resonated widely in popular culture and literary and academic circles in the 20th century; his cultural symbolism – in which he situated immigration issues – likewise informed American cultural imagining of immigrants for decades, as exemplified by Hollywood films.
The popular culture's image of ethnic celebrities often includes stereotypes about immigrant groups. For example, Frank Sinatra's public image as a superstar contained important elements of the "American Dream" while simultaneously incorporating stereotypes about Italian Americans that were based in nativist and Progressive responses to immigration.

The process of assimilation has been a common theme of popular culture. For example, "lace-curtain Irish" refers to middle-class Irish Americans desiring assimilation into mainstream society in counterpoint to the older, more raffish "shanty Irish". The occasional malapropisms and left-footed social blunders of these upward mobiles were gleefully lampooned in vaudeville, popular song, and the comic strips of the day such as "Bringing Up Father", starring Maggie and Jiggs, which ran in daily newspapers for 87 years (1913 to 2000). In "The Departed" (2006), Staff Sergeant Dignam regularly points out the dichotomy between the lace curtain Irish lifestyle Billy Costigan enjoyed with his mother, and the shanty Irish lifestyle of Costigan's father. In recent years the popular culture has paid special attention to Mexican immigration and the film "Spanglish" (2004) tells of a friendship of a Mexican housemaid (Paz Vega) and her boss played by Adam Sandler.

Novelists and writers have captured much of the color and challenge in their immigrant lives through their writings.

Regarding Irish women in the 19th century, there were numerous novels and short stories by Harvey O'Higgins, Peter McCorry, Bernard O'Reilly and Sarah Orne Jewett that emphasize emancipation from Old World controls, new opportunities and expansiveness of the immigrant experience.

On the other hand, Hladnik studies three popular novels of the late 19th century that warned Slovenes not to immigrate to the dangerous new world of the United States.

Jewish American writer Anzia Yezierska wrote her novel "Bread Givers" (1925) to explore such themes as Russian-Jewish immigration in the early 20th century, the tension between Old and New World Yiddish culture, and women's experience of immigration. A well established author Yezierska focused on the Jewish struggle to escape the ghetto and enter middle- and upper-class America. In the novel, the heroine, Sara Smolinsky, escape from New York City's "down-town ghetto" by breaking tradition. She quits her job at the family store and soon becomes engaged to a rich real-estate magnate. She graduates college and takes a high-prestige job teaching public school. Finally Sara restores her broken links to family and religion.
The Swedish author Vilhelm Moberg in the mid-20th century wrote a series of four novels describing one Swedish family's migration from Småland to Minnesota in the late 19th century, a destiny shared by almost one million people. The author emphasizes the authenticity of the experiences as depicted (although he did change names). These novels have been translated into English ("The Emigrants", 1951, "Unto a Good Land", 1954, "The Settlers", 1961, "The Last Letter Home", 1961). The musical Kristina från Duvemåla by ex-ABBA members Björn Ulvaeus and Benny Andersson is based on this story.

"The Immigrant" is a musical by Steven Alper, Sarah Knapp, and Mark Harelik. The show is based on the story of Harelik's grandparents, Matleh and Haskell Harelik, who traveled to Galveston, Texas in 1909.

In their documentary "", filmmakers Shari Robertson and Michael Camerini examine the American political system through the lens of immigration reform from 2001 to 2007. Since the debut of the first five films, the series has become an important resource for advocates, policy-makers and educators.

That film series premiered nearly a decade after the filmmakers' landmark documentary film "Well-Founded Fear" which provided a behind-the-scenes look at the process for seeking asylum in the United States. That film still marks the only time that a film-crew was privy to the private proceedings at the U.S. Immigration and Naturalization Service (INS), where individual asylum officers ponder the often life-or-death fate of immigrants seeking asylum.

University of North Carolina law professor Hiroshi Motomura has identified three approaches the United States has taken to the legal status of immigrants in his book "Americans in Waiting: The Lost Story of Immigration and Citizenship in the United States". The first, dominant in the 19th century, treated immigrants as in transition; in other words, as prospective citizens. As soon as people declared their intention to become citizens, they received multiple low-cost benefits, including the eligibility for free homesteads in the Homestead Act of 1869, and in many states, the right to vote. The goal was to make the country more attractive, so large numbers of farmers and skilled craftsmen would settle new lands. By the 1880s, a second approach took over, treating newcomers as "immigrants by contract". An implicit deal existed where immigrants who were literate and could earn their own living were permitted in restricted numbers. Once in the United States, they would have limited legal rights, but were not allowed to vote until they became citizens, and would not be eligible for the New Deal government benefits available in the 1930s. The third and more recent policy is "immigration by affiliation", which Motomura argues is the treatment which depends on how deeply rooted people have become in the country. An immigrant who applies for citizenship as soon as permitted, has a long history of working in the United States, and has significant family ties, is more deeply affiliated and can expect better treatment.
The American Dream is the belief that through hard work and determination, any United States immigrant can achieve a better life, usually in terms of financial prosperity and enhanced personal freedom of choice. According to historians, the rapid economic and industrial expansion of the U.S. is not simply a function of being a resource rich, hard working, and inventive country, but the belief that anybody could get a share of the country's wealth if he or she was willing to work hard. This dream has been a major factor in attracting immigrants to the United States.









</doc>
<doc id="15052" url="https://en.wikipedia.org/wiki?curid=15052" title="Image and Scanner Interface Specification">
Image and Scanner Interface Specification

Image and Scanner Interface Specification (ISIS) is an industry standard interface for image scanning technologies, developed by Pixel Translations in 1990 (which became EMC Corporation's Captiva Software and later acquired by OpenText).

ISIS is an open standard for scanner control and a complete image-processing framework. It is currently supported by a number of application and scanner vendors.

The modular design allows the scanner to be accessed both directly or with built-in routines to handle most situations automatically.

A message-based interface with tags is used so that features, operations, and formats not yet supported by ISIS can be added as desired without waiting for a new version of the specification.

The standard addresses all of the issues that an application using a scanner needs to be concerned with. Functions include but are not limited to selecting, installing, and configuring a new scanner; setting scanner-specific parameters; scanning, reading and writing files, and fast image scaling, rotating, displaying, and printing. Drivers have been written to dynamically process data for operations such as converting grayscale to binary image data.

An ISIS interface can run scanners at or above their rated speed by linking drivers together in a pipe so that data flows from a scanner driver to compression driver, to packaging driver, to a file, viewer, or printer in a continuous stream, usually without the need to buffer more than a small portion of the full image. As a result of using the piping method, each driver can be optimised to perform one function well. Drivers are typically small and modular in order to make it simple to add new functionality to an existing application.




</doc>
<doc id="15053" url="https://en.wikipedia.org/wiki?curid=15053" title="Ivo Caprino">
Ivo Caprino

Ivo Caprino (17 February 1920 – 8 February 2001) was a Norwegian film director and writer, best known for his puppet films. His most famous film is "Flåklypa Grand Prix" ("Pinchcliffe Grand Prix"), made in 1975.

In the mid-1940s, Caprino helped his mother design puppets for a puppet theatre, which inspired him to try making a film using his mother's designs. The result of their collaboration was "Tim og Tøffe", an 8-minute film released in 1949. 

Several films followed in the next couple of years, including two 15-minute shorts that are still shown regularly in Norway today, "Veslefrikk med Fela" (Little Freddy and his Fiddle), based on a Norwegian folk tale, and "Karius og Baktus", a story by Thorbjørn Egner of two little trolls, representing Caries and Bacterium, living in a boy's teeth. Ingeborg Gude made the puppets for these films as well, as she would continue to do up until her death in the mid sixties.

When making "Tim og Tøffe", Caprino invented an ingenious method for controlling the puppet's movements in real time. The technique can be described as a primitive, mechanical version of animatronics.

Caprino's films received rave reviews, and he quickly became a celebrity in Norway. In particular, the public were fascinated with the secret technology used to make his films. When he switched to traditional stop motion, Caprino tried to maintain the impression that he was still using some kind of "magic" technology to make the puppets move, even though all his later films were made with traditional stop motion techniques.

In addition to the short films, Caprino produced dozens of advertising films with puppets. In 1959, he directed a live action feature film, "Ugler i Mosen", which also contained stop motion sequences. He then embarked on his most ambitious project, a feature film about Peter Christen Asbjørnsen, who travelled around Norway in the 19th century collecting traditional folk tales. 

The plan was to use live action for the sequences showing Asbjørnsen, and then to realise the folk tales using stop motion. Unfortunately, Caprino was unable to secure funding for the project, so he ended up making the planned folk tale sequences as separate 16-minute puppet films, bookended by live action sequences showing Asbjørnsen.

In 1970, Caprino and his small team of collaborators, started work on a 25 minutes TV special, which would eventually become "The Pinchcliffe Grand Prix". Based on a series of books by Norwegian cartoonist and author Kjell Aukrust, it featured a group of eccentric characters all living in the small village of Pinchcliffe. The TV special was a collection of sketches based on Aukrust's books, with no real story line. After 1.5 years of work, it was decided that it didn't really work as a whole, so production on the TV special was stopped (with the exception of some very short clips, no material from it has ever been seen by the public), and Caprino and Aukrust instead wrote a screenplay for a feature film using the characters and environments that had already been built.

The result was "The Pinchcliffe Grand Prix", which stars Theodore Rimspoke (No. Reodor Felgen) and his two assistants, Sonny Duckworth (No. Solan Gundersen), a cheerful and optimistic bird, and Lambert (No. Ludvig), a nervous, pessimistic and melancholic hedgehog. Theodore works as a bicycle repairman, though he spends most of his time inventing weird Rube Goldberg-like contraptions. One day, the trio discover that one of Theodore's former assistants, Rudolph Gore-Slimey (), has stolen his design for a race car engine, and has become a world champion Formula One driver.

Sonny secures funding from an Arab oil sheik who happens to be vacationing in Pinchcliffe, and the trio then build a gigantic racing car, "Il Tempo Gigante" – a fabulous construction with two engines, radar and its own blood bank. Theodore then enters a race, and ends up winning, beating Gore-Slimey despite his attempts at sabotage.

The film was made in 3.5 years by a team of approximately 5 people. Caprino directed and animated, Bjarne Sandemose (Caprino's principal collaborator throughout his career) built the sets and the cars, and was in charge of the technical side, Ingeborg Riiser modeled the puppets and Gerd Alfsen made the costumes and props.

When it came out in 1975, The Pinchcliffe Grand Prix was an enormous success in Norway, selling 1 million tickets in its first year of release. It remains the biggest box office hit of all time in Norway (Caprino Studios claim it has sold 5.5 million tickets to date) and was also released in many other countries.

To help promote the film abroad, Caprino and Sandemose built a full-scale replica of Il Tempo Gigante that is legal for public roads, but is usually exposited at Hunderfossen Familiepark.

Except for some TV work in the late 1970s, Caprino made no more puppet films, focusing instead on creating attractions for the "Hunderfossen" theme park outside Lillehammer based on his folk tale movies, and making tourist films using a custom built multi camera setup of his own design that shoots 280 degrees panorama movies.

Caprino was the son of Italian furniture designer Mario Caprino and the artist Ingeborg Gude, who was a granddaughter of the painter Hans Gude. He was born and died in Oslo, but lived all of his life at Snarøya in Bærum. He died in 2001 after having lived several years with a cancer diagnosis. Since Caprino's death, his son Remo has had great success developing a computer game based on "Flåklypa Grand Prix".





</doc>
<doc id="15054" url="https://en.wikipedia.org/wiki?curid=15054" title="Intel 80286">
Intel 80286

The Intel 80286 (also marketed as the iAPX 286 and often called Intel 286) is a 16-bit microprocessor that was introduced on February 1, 1982. It was the first 8086-based CPU with separate, non-multiplexed address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186, it could correctly execute most software written for the earlier Intel 8086 and 8088 processors.

The 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s.

Intel's first 80286 chips were specified for a maximum clockrate of 4, 6 or 8 MHz and later releases for 12.5 MHz. AMD and Harris later produced 16 MHz, 20 MHz and 25 MHz parts, respectively. Intersil and Fujitsu also designed fully static CMOS versions of Intel's original depletion-load nMOS implementation, largely aimed at battery-powered devices.

On average, the 80286 was reportedly measured to have a speed of about 0.21 instructions per clock on "typical" programs, although it could be significantly faster on optimized code and in tight loops, as many instructions could execute in 2 clock cycles each. The 6 MHz, 10 MHz and 12 MHz models were reportedly measured to operate at 0.9 MIPS, 1.5 MIPS and 2.66 MIPS respectively.

The later E-stepping level of the 80286 was free of the several significant errata that caused problems for programmers and operating-system writers in the earlier B-step and C-step CPUs (common in the AT and AT clones).

Intel did not expect personal computers to use the 286. 
The CPU was designed for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: the address unit, bus unit, instruction unit and execution unit, organized into a loosely coupled (buffered) pipeline, just as in the 8086. The significantly increased performance over the 8086 was primarily due to the non-multiplexed address and data buses, more address-calculation hardware (most importantly, a dedicated adder) and a faster (more hardware-based) multiplier. It was produced in a 68-pin package, including PLCC (plastic leaded chip carrier), LCC (leadless chip carrier) and PGA (pin grid array) packages.

The performance increase of the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e., a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was partly due to the non-multiplexed address and data buses, but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286, while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.

The 80286 included, in addition to all of the 8086 instructions, all of the new instructions of the 80186: ENTER, LEAVE, BOUND, INS, OUTS, PUSHA, POPA, PUSH immediate, IMUL immediate, and immediate shifts and rotates. The 80286 also added new instructions for protected mode: ARPL, CLTS, LAR, LGDT, LIDT, LLDT, LMSW, LSL, LTR, SGDT, SIDT, SLDT, SMSW, STR, VERR, and VERW. Some of the instructions for protected mode can (or must) be used in real mode to set up and switch to protected mode, and a few (such as SMSW and LMSW) are useful for real mode itself.

The Intel 80286 had a 24-bit address bus and was able to address up to 16 MB of RAM, compared to the 1 MB addressability of its predecessor. However, memory cost and the initial rarity of software using the memory above 1 MB meant that 80286 computers were rarely shipped with more than one megabyte of RAM. Additionally, there was a performance penalty involved in accessing extended memory from real mode (in which DOS, the dominant PC operating system until the mid-1990s, ran), as noted below.

The 286 was the first of the x86 CPU family to support "protected virtual-address mode", commonly called "protected mode". In addition, it was the first commercially available microprocessor with on-chip MMU capabilities (systems using the contemporaneous Motorola 68010 and NS320xx could be equipped with an optional MMU controller). This would allow IBM compatibles to have advanced multitasking OSes for the first time and compete in the Unix-dominated server/workstation market.

Several additional instructions were introduced in protected mode of 80286, which are helpful for multitasking operating systems.

Another important feature of 80286 is prevention of unauthorized access. This is achieved by:

In 80286 (and in its co-processor Intel 80287), arithmetic operations can be performed on the following different types of numbers:

By design, the 286 could not revert from protected mode to the basic 8086-compatible "real address mode" ("real mode") without a hardware-initiated reset. In the PC/AT introduced in 1984, IBM added external circuitry, as well as specialized code in the ROM BIOS and the 8042 peripheral microcontroller to enable software to cause the reset, allowing real-mode reentry while retaining active memory and returning control to the program that initiated the reset. (The BIOS is necessarily involved because it obtains control directly whenever the CPU resets.) Though it worked correctly, the method imposed a huge performance penalty.

In theory, real-mode applications could be directly executed in 16-bit protected mode if certain rules (newly proposed with the introduction of the 80286) were followed; however, as many DOS programs did not conform to those rules, protected mode was not widely used until the appearance of its successor, the 32-bit Intel 80386, which was designed to go back and forth between modes easily and to provide an emulation of real mode within protected mode. When Intel designed the 286, it was not designed to be able to multitask real-mode applications; real mode was intended to be a simple way for a bootstrap loader to prepare the system and then switch to protected mode; essentially, in protected mode the 80286 was designed to be a new processor with many similarities to its predecessors, while real mode on the 80286 was offered for smaller-scale systems that could benefit from a more advanced version of the 80186 CPU core, with advantages such as higher clock rates, faster instruction execution (measured in clock cycles), and unmultiplexed buses, but not the 24-bit (16 MB) memory space.

To support protected mode, new instructions have been added: ARPL, VERR, VERW, LAR, LSL, SMSW, SGDT, SIDT, SLDT, STR, LMSW, LGDT, LIDT, LLDT, LTR, CLTS. There are also new exceptions (internal interrupts): invalid opcode, coprocessor not available, double fault, coprocessor segment overrun, stack fault, segment overrun/general protection fault, and others only for protected mode.

The protected mode of the 80286 was not utilized until many years after its release, in part because of the high cost of adding extended memory to a PC, but also because of the need for software to support the large user base of 8086 PCs. For example, in 1986 the only program that made use of it was VDISK, a RAM disk driver included with PC DOS 3.0 and 3.1. A DOS could utilize the additional RAM available in protected mode (extended memory) either via a BIOS call (INT 15h, AH=87h), as a RAM disk, or as emulation of expanded memory. The difficulty lay in the incompatibility of older real-mode DOS programs with protected mode. They simply could not natively run in this new mode without significant modification. In protected mode, memory management and interrupt handling were done differently than in real mode. In addition, DOS programs typically would directly access data and code segments that did not belong to them, as real mode allowed them to do without restriction; in contrast, the design intent of protected mode was to prevent programs from accessing any segments other than their own unless special access was explicitly allowed. While it was possible to set up a protected-mode environment that allowed all programs access to all segments (by putting all segment descriptors into the GDT and assigning them all the same privilege level), this undermined nearly all of the advantages of protected mode except the extended (24-bit) address space. The choice that OS developers faced was either to start from scratch and create an OS that would not run the vast majority of the old programs, or to come up with a version of DOS that was slow and ugly (i.e., ugly from an internal technical viewpoint) but would still run a majority of the old programs. Protected mode also did not provide a significant enough performance advantage over the 8086-compatible real mode to justify supporting its capabilities; actually, except for task switches when multitasking, it actually yielded only a performance disadvantage, by slowing down many instructions through a litany of added privilege checks. In protected mode, registers were still 16-bit, and the programmer was still forced to use a memory map composed of 64 kB segments, just like in real mode.

In January 1985, Digital Research previewed the Concurrent DOS 286 1.0 operating system developed in cooperation with Intel. The product would function strictly as an 80286 native-mode (i.e. protected-mode) operating system, allowing users to take full advantage of the protected mode to perform multi-user, multitasking operations while running 8086 emulation. This worked on the B-1 prototype step of the chip, but Digital Research discovered problems with the emulation on the production level C-1 step in May, which would not allow Concurrent DOS 286 to run 8086 software in protected mode. The release of Concurrent DOS 286 was delayed until Intel would develop a new version of the chip. In August, after extensive testing on E-1 step samples of the 80286, Digital Research acknowledged that Intel corrected all documented 286 errata, but said that there were still undocumented chip performance problems with the prerelease version of Concurrent DOS 286 running on the E-1 step. Intel said that the approach Digital Research wished to take in emulating 8086 software in protected mode differed from the original specifications. Nevertheless, in the E-2 step, they implemented minor changes in the microcode that would allow Digital Research to run emulation mode much faster. Named IBM 4680 OS, IBM originally chose DR Concurrent DOS 286 as the basis of their IBM 4680 computer for IBM Plant System products and point-of-sale terminals in 1986. Digital Research's FlexOS 286 version 1.3, a derivation of Concurrent DOS 286, was developed in 1986, introduced in January 1987, and later adopted by IBM for their IBM 4690 OS, but the same limitations affected it.

The problems led to Bill Gates famously referring to the 80286 as a "brain-dead chip", since it was clear that the new Microsoft Windows environment would not be able to run multiple MS-DOS applications with the 286. It was arguably responsible for the split between Microsoft and IBM, since IBM insisted that OS/2, originally a joint venture between IBM and Microsoft, would run on a 286 (and in text mode).

Other operating systems that used the protected mode of the 286 were Microsoft Xenix (around 1984), Coherent, and Minix. These were less hindered by the limitations of the 80286 protected mode because they did not aim to run MS-DOS applications or other real-mode programs. In its successor 80386 chip, Intel enhanced the protected mode to address more memory and also added the separate virtual 8086 mode, a mode within protected mode with much better MS-DOS compatibility, in order to satisfy the diverging needs of the market.




</doc>
<doc id="15055" url="https://en.wikipedia.org/wiki?curid=15055" title="Ivanhoe">
Ivanhoe

Ivanhoe: A Romance () by Walter Scott is an historical novel published in three volumes, in 1819, as one of the Waverley novels. At the time it was written, the novel represented Scott's shift away from writing fairly realistic novels set in the comparatively recent past of Scotland, to a fanciful depiction of England in the Middle Ages, thus "Ivanhoe" proved to be one of the best-known and most influential of Scott's novels.

Set in 12th-century England, with colourful descriptions of a tournament, outlaws, a witch trial, and divisions between Jews and Christians, "Ivanhoe" is credited for increased interest in chivalric romance and medievalism. John Henry Newman claimed that Scott "had first turned men's minds in the direction of the Middle Ages", while Thomas Carlyle and John Ruskin likewise asserted Scott's great influence upon the revival of interest in the mediaeval period, primarily based upon the publication of the novel "Ivanhoe". Moreover, "Ivanhoe" much influenced popular perceptions of Richard the Lionheart, King John, and Robin Hood.

In June 1819, Walter Scott was still suffering the severe stomach pains that had forced him to dictate the last part of "The Bride of Lammermoor", and also most of "A Legend of the Wars of Montrose", which he finished at the end of May. By the beginning of July, at the latest, Scott had started dictating his new novel "Ivanhoe", again with John Ballantyne and William Laidlaw as amanuenses. For the second half of the manuscript, Scott was able to take up the pen, and completed "Ivanhoe: A Romance" in early November 1819.

For detailed information about the middle ages Scott drew on three works by the antiquarian Joseph Strutt: "Horda Angel-cynnan or a Compleat View of the Manners, Customs, Arms, Habits etc. of the Inhabitants of England" (1775–76), "Dress and Habits of the People of England" (1796–99), and "Sports and Pastimes of the People of England" (1801). Two historians gave him a solid grounding in the period: Robert Henry with "The History of Great Britain" (1771–93), and Sharon Turner with "The History of the Anglo-Saxons from the Earliest Period to the Norman Conquest" (1799–1805). His clearest debt to an original medieval source involved the Templar Rule, reproduced in "The Theatre of Honour and Knight-Hood" (1623) translated from the French of André Favine. Scott was happy to introduce details from the later middle ages, and Chaucer was particularly helpful, as (in a different way) was the fourteenth-century romance "Richard Coeur de Lion".

"Ivanhoe" was published by Archibald Constable in Edinburgh. All first editions carry the date of 1820, but it was released on 20 December 1819 and issued in London on the 29th by Hurst, Robinson and Co.. As with all of the Waverley novels before 1827, publication was anonymous. The print run was 10,000 copies, and the cost was £1 10"s" (£1.50). It is possible that Scott was involved in minor changes to the text during the early 1820s but his main revision was carried out in 1829 for the 'Magnum' edition where the novel appeared in Volumes 16 and 17 in September and October 1830. 

The standard modern edition, by Graham Tulloch, appeared as Volume 8 of the Edinburgh Edition of the Waverley Novels in 1998: this is based on the first edition with emendations principally from Scott's manuscript in the second half of the work; the new Magnum material is included in Volume 25b.

"Ivanhoe" is the story of one of the remaining Anglo-Saxon noble families at a time when the nobility in England was overwhelmingly Norman. It follows the Saxon protagonist, Sir Wilfred of Ivanhoe, who is out of favour with his father for his allegiance to the Norman king Richard the Lionheart. The story is set in 1194, after the failure of the Third Crusade, when many of the Crusaders were still returning to their homes in Europe. King Richard, who had been captured by Leopold of Austria on his return journey to England, was believed to still be in captivity.

Protagonist Wilfred of Ivanhoe is disinherited by his father Cedric of Rotherwood for supporting the Norman King Richard and for falling in love with the Lady Rowena, a ward of Cedric and descendant of the Saxon Kings of England. Cedric planned to have Rowena marry the powerful Lord Athelstane, a pretender to the Crown of England by his descent from the last Saxon King, Harold Godwinson. Ivanhoe accompanies King Richard on the Crusades, where he is said to have played a notable role in the Siege of Acre; and tends to Louis of Thuringia, who suffers from malaria.

The book opens with a scene of Norman knights and prelates seeking the hospitality of Cedric. They are guided there by a pilgrim, known at that time as a palmer. Also returning from the Holy Land that same night, Isaac of York, a Jewish moneylender, seeks refuge at Rotherwood. Following the night's meal, the palmer observes one of the Normans, the Templar Brian de Bois-Guilbert, issue orders to his Saracen soldiers to capture Isaac.

The palmer then assists in Isaac's escape from Rotherwood, with the additional aid of the swineherd Gurth.

Isaac of York offers to repay his debt to the palmer with a suit of armour and a war horse to participate in the tournament at Ashby-de-la-Zouch Castle, on his inference that the palmer was secretly a knight. The palmer is taken by surprise, but accepts the offer.

The tournament is presided over by Prince John. Also in attendance are Cedric, Athelstane, Lady Rowena, Isaac of York, his daughter Rebecca, Robin of Locksley and his men, Prince John's advisor Waldemar Fitzurse, and numerous Norman knights.

On the first day of the tournament, in a bout of individual jousting, a mysterious knight, identifying himself only as "Desdichado" (described in the book as Spanish, taken by the Saxons to mean Disinherited), defeats Bois-Guilbert. The masked knight declines to reveal himself despite Prince John's request, but is nevertheless declared the champion of the day and is permitted to choose the Queen of the Tournament. He bestows this honour upon Lady Rowena.

On the second day, at a melee, Desdichado is the leader of one party, opposed by his former adversaries. Desdichado's side is soon hard pressed and he himself beset by multiple foes until rescued by a knight nicknamed 'Le Noir Faineant' ("the Black Sluggard"), who thereafter departs in secret. When forced to unmask himself to receive his coronet (the sign of championship), Desdichado is identified as Wilfred of Ivanhoe, returned from the Crusades. This causes much consternation to Prince John and his court who now fear the imminent return of King Richard.

Ivanhoe is severely wounded in the competition yet his father does not move quickly to tend to him. Instead, Rebecca, a skilled healer, tends to him while they are lodged near the tournament and then convinces her father to take Ivanhoe with them to their home in York, when he is fit for that trip. The conclusion of the tournament includes feats of archery by Locksley, such as splitting a willow reed with his arrow. Prince John’s dinner for the local Saxons ends in insults.

In the forests between Ashby and York, Isaac, Rebecca and the wounded Ivanhoe are abandoned by their guards, who fear bandits and take all of Isaac’s horses. Cedric, Athelstane and the Lady Rowena meet them and agree to travel together. The party is captured by de Bracy and his companions and taken to Torquilstone, the castle of Front-de-Boeuf. The swineherd Gurth and Wamba the jester manage to escape, and then encounter Locksley, who plans a rescue.

The Black Knight, having taken refuge for the night in the hut of local friar, the Holy Clerk of Copmanhurst, volunteers his assistance on learning about the captives from Robin of Locksley. They then besiege the Castle of Torquilstone with Robin's own men, including the friar and assorted Saxon yeomen. Inside Torquilstone, de Bracy expresses his love for the Lady Rowena but is refused. Brian de Bois-Guilbert tries to seduce Rebecca and is rebuffed. Front-de-Boeuf tries to wring a hefty ransom from Isaac of York, but Isaac refuses to pay unless his daughter is freed.

When the besiegers deliver a note to yield up the captives, their Norman captors demand a priest to administer the Final Sacrament to Cedric; whereupon Cedric's jester Wamba slips in disguised as a priest, and takes the place of Cedric, who escapes and brings important information to the besiegers on the strength of the garrison and its layout. The besiegers storm the castle. The castle is set aflame during the assault by Ulrica, the daughter of the original lord of the castle, Lord Torquilstone, as revenge for her father's death. Front-de-Boeuf is killed in the fire while de Bracy surrenders to the Black Knight, who identifies himself as King Richard and releases de Bracy. Bois-Guilbert escapes with Rebecca while Isaac is rescued by the Clerk of Copmanhurst. The Lady Rowena is saved by Cedric, while the still-wounded Ivanhoe is rescued from the burning castle by King Richard. In the fighting, Athelstane is wounded and presumed dead while attempting to rescue Rebecca, whom he mistakes for Rowena.

Following the battle, Locksley plays host to King Richard. Word is conveyed by de Bracy to Prince John of the King's return and the fall of Torquilstone. In the meantime, Bois-Guilbert rushes with his captive to the nearest Templar Preceptory, where Lucas de Beaumanoir, the Grand Master of the Templars, takes umbrage at Bois-Guilbert's infatuation and subjects Rebecca to a trial for witchcraft. At Bois-Guilbert's secret request, she claims the right to trial by combat; and Bois-Guilbert, who had hoped for the position, is devastated when the Grand-Master orders him to fight against Rebecca's champion. Rebecca then writes to her father to procure a champion for her. Cedric organises Athelstane's funeral at Coningsburgh, in the midst of which the Black Knight arrives with a companion. Cedric, who had not been present at Locksley's carousal, is ill-disposed towards the knight upon learning his true identity; but Richard calms Cedric and reconciles him with his son. During this conversation, Athelstane emerges – not dead, but laid in his coffin alive by monks desirous of the funeral money. Over Cedric's renewed protests, Athelstane pledges his homage to the Norman King Richard and urges Cedric to marry Rowena to Ivanhoe; to which Cedric finally agrees.

Soon after this reconciliation, Ivanhoe receives word from Isaac beseeching him to fight on Rebecca's behalf. Ivanhoe, riding day and night, arrives in time for the trial by combat, but horse and man are exhausted, with little chance of victory. The two knights make one charge at each other with lances, Bois-Guilbert appearing to have the advantage. However, Bois-Guilbert, a man trying to have it all without offering to marry Rebecca, dies of natural causes in the saddle before the combat can continue.

Ivanhoe and Rowena marry and live a long and happy life together. Fearing further persecution, Rebecca and her father plan to quit England for Granada. Before leaving, Rebecca comes to Rowena shortly after the wedding to bid her a solemn farewell. Ivanhoe's military service ends with the death of King Richard five years later.

"(principal characters in bold)"


Dedicatory Epistle: An imaginary letter from the Rev. Dr Dryasdust from Laurence Templeton who has found the materials for the following tale mostly in the Anglo-Norman Wardour Manuscript. He wishes to provide an English counterpart to the preceding Waverley novels, in spite of various difficulties arising from the chronologically remote setting made necessary by the earlier progress of civilisation south of the Border.

Ch. 1: Historical sketch. Gurth the swineherd and Wamba the jester discuss life under Norman rule.

Ch. 2: Wamba and Gurth wilfully misdirect a group of horsemen headed by Prior Aymer and Brian de Bois-Guilbert seeking shelter at Cedric's Rotherwood. Aymer and Bois-Guilbert discuss the beauty of Cedric's ward Rowena and are redirected, this time correctly, by a palmer [Ivanhoe in disguise].

Ch. 3: Cedric anxiously awaits the return of Gurth and the pigs. Aymer and Bois-Guilbert arrive.

Ch. 4: Bois-Guilbert admires Rowena as she enters for the evening feast.

Ch. 5: During the feast: Isaac enters and is befriended by the palmer; Cedric laments the decay of the Saxon language; the palmer refutes Bois-Guilbert's assertion of Templar supremacy with an account of a tournament in Palestine, where Ivanhoe defeated him; the palmer and Rowena give a pledge for a return match; and Isaac is thunderstruck by Bois-Guilbert's denial of his assertion of poverty.

Ch. 6: Next day the palmer tells Rowena that Ivanhoe will soon be home. He offers to protect Isaac from Bois-Guilbert, whom he has overheard giving instructions for his capture. On the road to Sheffield Isaac mentions a source of horse and armour of which he guesses the palmer has need.

Ch. 7: As the audience for a tournament at Ashby assembles, Prince John amuses himself by making fun of Athelstane and Isaac.

Ch. 8: After a series of Saxon defeats in the tournament the 'Disinherited Knight' [Ivanhoe] triumphs over Bois-Guilbert and the other Norman challengers.

Ch. 9: The Disinherited Knight nominates Rowena as Queen of the Tournament.

Ch. 10: The Disinherited Knight refuses to ransom Bois-Guilbert's armour, declaring that their business is not concluded. He instructs his attendant, Gurth in disguise, to convey money to Isaac to repay him for arranging the provision of his horse and armour. Gurth does so, but Rebecca secretly refunds the money.

Ch. 11: Gurth is assailed by a band of outlaws, but they spare him on hearing his story and after he has defeated one of their number, a miller, at quarter-staves.

Ch. 12: The Disinherited Knight's party triumph at the tournament, with the aid of a knight in black [Richard in disguise]; he is revealed as Ivanhoe and faints as a result of the wounds he has incurred.

Ch. 13: John encourages De Bracy to court Rowena and receives a warning from France that Richard has escaped. Locksley [Robin Hood] triumphs in an archery contest.

Ch. 14: At the tournament banquet Cedric continues to disown his son (who has been associating with the Normans) but drinks to the health of Richard, rather than John, as the noblest of that race.

Ch. 1 (15): De Bracy (disguised as a forester) tells Fitzurse of his plan to capture Rowena and then 'rescue' her in his own person.

Ch. 2 (16): The Black Knight is entertained by a hermit [Friar Tuck] at Copmanhurst.

Ch. 3 (17): The Black Knight and the hermit exchange songs.

Ch. 4 (18): (Retrospect: Before going to the banquet Cedric learned that Ivanhoe had been removed by unknown carers; Gurth was recognised and captured by Cedric's cupbearer Oswald.) Cedric finds Athelstane unresponsive to his attempts to interest him in Rowena, who is herself only attracted by Ivanhoe.

Ch. 5 (19): Rowena persuades Cedric to escort Isaac and Rebecca, who have been abandoned (along with a sick man [Ivanhoe] in their care) by their hired protectors. Wamba helps Gurth to escape again. De Bracy mounts his attack, during which Wamba escapes. He meets up with Gurth and they encounter Locksley who, after investigation, advises against a counter-attack, the captives not being in immediate danger.

Ch. 6 (20): Locksley sends two of his men to watch De Bracy. At Copmanhurst he meets the Black Knight who agrees to join in the rescue.

Ch. 7 (21): De Bracy tells Bois-Guilbert he has decided to abandon his 'rescue' plan, mistrusting his companion though the Templar says it is Rebecca he is interested in. On arrival at Torquilstone castle Cedric laments its decline.

Ch. 8 (22): Under threat of torture Isaac agrees to pay Front-de-Bœuf a thousand pounds, but only if Rebecca is released.

Ch. 9 (23): De Bracy uses Ivanhoe's danger from Front-de-Bœuf to put pressure on Rowena, but he is moved by her resulting distress. The narrator refers the reader to historical instances of baronial oppression in medieval England.

Ch. 10 (24): A hag Urfried [Ulrica] warns Rebecca of her forthcoming fate. Rebecca impresses Bois-Guilbert by her spirited resistance to his advances.

Ch. 11 (25): Front-de-Bœuf rejects a written challenge from Gurth and Wamba. Wamba offers to spy out the castle posing as a confessor. 

Ch. 12 (26): Entering the castle, Wamba exchanges clothes with Cedric who encounters Rebecca and Urfried.

Ch. 13 (27): Urfried recognises Cedric as a Saxon and, revealing herself as Ulrica, tells her story which involves Front-de-Bœuf murdering his father, who had killed her father and seven brothers when taking the castle, and had become her detested lover. She says she will give a signal when the time is ripe for storming the castle. Front-de-Bœuf sends the presumed friar with a message to summon reinforcements. Athelstane defies him, claiming that Rowena is his fiancée. The monk Ambrose arrives seeking help for Aymer who has been captured by Locksley's men.

Ch. 14 (28): (Retrospective chapter detailing Rebecca's care for Ivanhoe from the tournament to the assault on Torquilstone.)

Ch. 15 (29): Rebecca describes the assault on Torquilstone to the wounded Ivanhoe, disagreeing with his exalted view of chivalry.

Ch. 16 (30): Front-de-Bœuf being mortally wounded, Bois-Guilbert and De Bracy discuss how best to repel the besiegers. Ulrica sets fire to the castle and exults over Front-de-Bœuf who perishes in the flames.

Ch. 1 (31): (The chapter opens with a retrospective account of the attackers' plans and the taking of the barbican.) The Black Knight defeats De Bracy, making himself known to him as Richard, and rescues Ivanhoe. Bois-Guilbert rescues Rebecca, striking down Athelstane who thinks she is Rowena. Ulrica perishes in the flames after singing a wild pagan hymn.

Ch. 2 (32): Locksley supervises the orderly division of the spoil. Friar Tuck brings Isaac whom he has made captive, and engages in good-natured buffeting with the Black Knight.

Ch. 3 (33): Locksley arranges ransom terms for Isaac and Aymer.

Ch. 4 (34): De Bracy informs John that Richard is in England. Together with Fitzurse he threatens to desert John, but the prince responds cunningly.

Ch. 5 (35): At York, Nathan is horrified by Isaac's determination to seek Rebecca at Templestowe. At the priory the Grand-Master Beaumanoir tells Conrade Mountfitchet that he intends to take a hard line with Templar irregularities. Arriving, Isaac shows him a letter from Aymer to Bois-Guilbert referring to Rebecca whom Beaumanoir regards as a witch.

Ch. 6 (36): Beaumanoir tells Preceptor Albert Malvoisin of his outrage at Rebecca's presence in the preceptory. Albert insists to Bois-Guilbert that her trial for sorcery must proceed. Mountfichet says he will seek evidence against her.

Ch. 7 (37): Rebecca is tried and found guilty. At Bois-Guilbert's secret prompting she demands that a champion defend her in trial by combat.

Ch. 8 (38): Rebecca's demand is accepted, Bois-Guilbert being appointed champion for the prosecution. Bearing a message to her father, the peasant Higg meets him and Nathan on their way to the preceptory, and Isaac goes in search of Ivanhoe.

Ch. 9 (39): Rebecca rejects Bois-Guilbert's offer to fail to appear for the combat in return for her love. Albert persuades him that it is in his interest to appear.

Ch. 10 (40): The Black Knight leaves Ivanhoe to travel to Coningsburgh castle for Athelstane's funeral, and Ivanhoe follows him the next day. The Black Knight is rescued by Locksley from an attack carried out by Fitzurse on John's orders, and reveals his identity as Richard to his companions, prompting Locksley to identify himself as Robin Hood.

Ch. 11 (41): Richard talks to Ivanhoe and dines with the outlaws before Robin arranges a false alarm to put an end to the delay. The party arrive at Coningsburgh.

Ch. 12 (42): Richard procures Ivanhoe's pardon from his father. Athelstane appears, not dead, giving his allegiance to Richard and surrendering Rowena to Ivanhoe.

Ch. 13 (43): Ivanhoe appears as Rebecca's champion, and as they charge Bois-Guilbert dies the victim of his contending passions.

Ch. 14 (44): Beaumanoir and his Templars leave Richard defiantly. Cedric agrees to the marriage of Ivanhoe and Rowena. Rebecca takes her leave of Rowena before her father and she quit England to make a new life under the tolerant King of Grenada.

Critics of the novel have treated it as a romance intended mainly to entertain boys. "Ivanhoe" maintains many of the elements of the Romance genre, including the quest, a chivalric setting, and the overthrowing of a corrupt social order to bring on a time of happiness. Other critics assert that the novel creates a realistic and vibrant story, idealising neither the past nor its main character.

Scott treats themes similar to those of some of his earlier novels, like "Rob Roy" and "The Heart of Midlothian", examining the conflict between heroic ideals and modern society. In the latter novels, industrial society becomes the centre of this conflict as the backward Scottish nationalists and the "advanced" English have to arise from chaos to create unity. Similarly, the Normans in "Ivanhoe", who represent a more sophisticated culture, and the Saxons, who are poor, disenfranchised, and resentful of Norman rule, band together and begin to mould themselves into one people. The conflict between the Saxons and Normans focuses on the losses both groups must experience before they can be reconciled and thus forge a united England. The particular loss is in the extremes of their own cultural values, which must be disavowed in order for the society to function. For the Saxons, this value is the final admission of the hopelessness of the Saxon cause. The Normans must learn to overcome the materialism and violence in their own codes of chivalry. Ivanhoe and Richard represent the hope of reconciliation for a unified future.

Ivanhoe, though of a more noble lineage than some of the other characters, represents a middling individual in the medieval class system who is not exceptionally outstanding in his abilities, as is expected of other quasi-historical fictional characters, such as the Greek heroes. Critic György Lukács points to middling main characters like Ivanhoe in Walter Scott's other novels as one of the primary reasons Scott's historical novels depart from previous historical works, and better explore social and cultural history.

The location of the novel is centred upon southern Yorkshire, north-west Leicestershire and northern Nottinghamshire in England. Castles mentioned within the story include Ashby de la Zouch Castle (now a ruin in the care of English Heritage), York (though the mention of Clifford's Tower, likewise an extant English Heritage property, is anachronistic, it not having been called that until later after various rebuilds) and 'Coningsburgh', which is based upon Conisbrough Castle, in the ancient town of Conisbrough near Doncaster (the castle also being a popular English Heritage site). Reference is made within the story to York Minster, where the climactic wedding takes place, and to the Bishop of Sheffield, although the Diocese of Sheffield did not exist at either the time of the novel or the time Scott wrote the novel and was not founded until 1914. Such references suggest that Robin Hood lived or travelled in the region.

Conisbrough is so dedicated to the story of "Ivanhoe" that many of its streets, schools, and public buildings are named after characters from the book.

The modern conception of Robin Hood as a cheerful, decent, patriotic rebel owes much to "Ivanhoe".

"Locksley" becomes Robin Hood's title in the Scott novel, and it has been used ever since to refer to the legendary outlaw. Scott appears to have taken the name from an anonymous manuscript – written in 1600 – that employs "Locksley" as an epithet for Robin Hood. Owing to Scott's decision to make use of the manuscript, Robin Hood from Locksley has been transformed for all time into "Robin of Locksley", alias Robin Hood. (There is, incidentally, a village called Loxley in Yorkshire.)

Scott makes the 12th-century's Saxon-Norman conflict a major theme in his novel. The original medieval stories about Robin Hood did not mention any conflict between Saxons and Normans; it was Scott who introduced this theme into the legend. The characters in "Ivanhoe" refer to Prince John and King Richard I as "Normans"; contemporary medieval documents from this period do not refer to either of these two rulers as Normans. Recent re-tellings of the story retain Scott's emphasis on the Norman-Saxon conflict.

Scott also shunned the late 16th-century depiction of Robin as a dispossessed nobleman (the Earl of Huntingdon).

This, however, has not prevented Scott from making an important contribution to the noble-hero strand of the legend, too, because some subsequent motion picture treatments of Robin Hood's adventures give Robin traits that are characteristic of Ivanhoe as well. The most notable Robin Hood films are the lavish Douglas Fairbanks 1922 silent film, the 1938 triple Academy Award-winning "Adventures of Robin Hood" with Errol Flynn as Robin (which contemporary reviewer Frank Nugent links specifically with "Ivanhoe"), and the 1991 box-office success "" with Kevin Costner). There is also the Mel Brooks spoof "".

In most versions of Robin Hood, both Ivanhoe and Robin, for instance, are returning Crusaders. They have quarrelled with their respective fathers, they are proud to be Saxons, they display a highly evolved sense of justice, they support the rightful king even though he is of Norman-French ancestry, they are adept with weapons, and they each fall in love with a "fair maid" (Rowena and Marian, respectively).

This particular time-frame was popularised by Scott. He borrowed it from the writings of the 16th-century chronicler John Mair or a 17th-century ballad presumably to make the plot of his novel more gripping. Medieval balladeers had generally placed Robin about two centuries later in the reign of Edward I, II or III.

Robin's familiar feat of splitting his competitor's arrow in an archery contest appears for the first time in "Ivanhoe".

The general political events depicted in the novel are relatively accurate; the novel tells of the period just after King Richard's imprisonment in Austria following the Crusade and of his return to England after a ransom is paid. Yet the story is also heavily fictionalised. Scott himself acknowledged that he had taken liberties with history in his "Dedicatory Epistle" to "Ivanhoe". Modern readers are cautioned to understand that Scott's aim was to create a compelling novel set in a historical period, not to provide a book of history.

There has been criticism of Scott's portrayal of the bitter extent of the "enmity of Saxon and Norman, represented as persisting in the days of Richard" as "unsupported by the evidence of contemporary records that forms the basis of the story." Historian E. A. Freeman criticised Scott's novel, stating its depiction of a Saxon–Norman conflict in late twelfth-century England was unhistorical. Freeman cited medieval writer Walter Map, who claimed that tension between the Saxons and Normans had declined by the reign of Henry I. Freeman also cited the late twelfth-century book "Dialogus de Scaccario" by Richard FitzNeal. This book claimed that the Saxons and Normans had so merged together through intermarriage and cultural assimilation that (outside the aristocracy) it was impossible to tell "one from the other." Finally, Freeman ended his critique of Scott by saying that by the end of the twelfth century, the descendants of both Saxons and Normans in England referred to themselves as "English", not "Saxon" or "Norman". 

However, Scott may have intended to suggest parallels between the Norman conquest of England, about 130 years previously, and the prevailing situation in Scott's native Scotland (Scotland's union with England in 1707 – about the same length of time had elapsed before Scott's writing and the resurgence in his time of Scottish nationalism evidenced by the cult of Robert Burns, the famous poet who deliberately chose to work in Scots vernacular though he was an educated man and spoke modern English eloquently). Indeed, some experts suggest that Scott deliberately used "Ivanhoe" to illustrate his own combination of Scottish patriotism and pro-British Unionism.

The novel generated a new name in English – Cedric. The original Saxon name had been "Cerdic" but Scott misspelled it – an example of metathesis. "It is not a name but a misspelling" said satirist H. H. Munro.

In England in 1194, it would have been unlikely for Rebecca to face the threat of being burned at the stake on charges of witchcraft. It is thought that it was shortly afterwards, from the 1250s, that the Church began to undertake the finding and punishment of witches and death did not become the usual penalty until the 15th century. Even then, the form of execution used for witches in England was hanging, burning being reserved for those also convicted of treason. There are various minor errors, e.g. the description of the tournament at Ashby owes more to the 14th century, most of the coins mentioned by Scott are exotic, William Rufus is said to have been John Lackland's grandfather, but he was actually his great-great-uncle, and Wamba (disguised as a monk) says "I am a poor brother of the Order of St Francis", but St. Francis of Assisi only began his preaching ten years after the death of Richard I.

"For a writer whose early novels were prized for their historical accuracy, Scott was remarkably loose with the facts when he wrote "Ivanhoe"... But it is crucial to remember that "Ivanhoe", unlike the Waverly books, is entirely a romance. It is meant to please, not to instruct, and is more an act of imagination than one of research. Despite this fancifulness, however, "Ivanhoe" does make some prescient historical points. The novel is occasionally quite critical of King Richard, who seems to love adventure more than he loves the well-being of his subjects. This criticism did not match the typical idealised, romantic view of Richard the Lion-Hearted that was popular when Scott wrote the book, and yet it accurately echoes the way King Richard is often judged by historians today."

Rebecca may be based on Rebecca Gratz, a Philadelphia teacher and philanthropist and the first Jewish female college student in America. Scott's attention had been drawn to Gratz's character by novelist Washington Irving, who was a close friend of the Gratz family. The assertion has been disputed, but it has been supported by "The Original of Rebecca in Ivanhoe", in "The Century Magazine" in 1882. The two Jewish characters, the moneylender Isaac of York and his beautiful daughter Rebecca, feature as main characters; the book was written and published during a period of increasing advancement and awareness for the emancipation of the Jews in England, and their position in society is well documented.

Most of the original reviewers gave "Ivanhoe" an enthusiastic or broadly favourable reception.
As usual, Scott's descriptive powers and his ability to present the matters of the past were generally praised. More than one reviewer found the work notably poetic. Several of them found themselves transported imaginatively to the remote period of the novel, although some problems were recognised: the combining of features from the high and late middle ages; an awkwardly created language for the dialogue; and antiquarian overload. The author's excursion into England was generally judged a success, the forest outlaws and the creation of 'merry England' attracting particular praise. Rebecca was almost unanimously admired, especially in her farewell scene. The plot was either criticised for its weakness, or just regarded as of less importance than the scenes and characters. The scenes at Torquilstone were judged horrible by several critics, with special focus on Ulrica. Athelstane's resurrection found no favour, the kindest response being that of Francis Jeffrey in "The Edinburgh Review" who suggested (writing anonymously, like all the reviewers) that it was 'introduced out of the very wantonness of merriment'.

The Eglinton Tournament of 1839 held by the 13th Earl of Eglinton at Eglinton Castle in Ayrshire was inspired by and modelled on "Ivanhoe".

On November 5, 2019, the "BBC News" listed "Ivanhoe" on its list of the 100 most influential novels.


The novel has been the basis for several motion pictures:


There have also been many television adaptations of the novel, including:

Victor Sieg's dramatic cantata "Ivanhoé" won the Prix de Rome in 1864 and premiered in Paris the same year. An operatic adaptation of the novel by Sir Arthur Sullivan (entitled "Ivanhoe") ran for over 150 consecutive performances in 1891. Other operas based on the novel have been composed by Gioachino Rossini ("Ivanhoé"), Thomas Sari ("Ivanhoé"), Bartolomeo Pisani ("Rebecca"), A. Castagnier ("Rébecca"), Otto Nicolai ("Il Templario"), and Heinrich Marschner ("Der Templer und die Jüdin"). Rossini's opera is a "pasticcio" (an opera in which the music for a new text is chosen from pre-existent music by one or more composers). Scott attended a performance of it and recorded in his journal, "It was an opera, and, of course, the story sadly mangled and the dialogue, in part nonsense."

The railway running through Ashby-de-la-Zouch was known as the Ivanhoe line between 1993 and 2005, in reference to the book's setting in the locality.




</doc>
<doc id="15056" url="https://en.wikipedia.org/wiki?curid=15056" title="Isoelectric point">
Isoelectric point

The isoelectric point (pI, pH(I), IEP), is the pH at which a molecule carries no net electrical charge or is electrically neutral in the statistical mean. The standard nomenclature to represent the isoelectric point is pH(I). However, pI is also used. For brevity, this article uses pI. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H).

Surfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H/OH, the net surface charge is affected by the pH of the liquid in which the solid is submerged.

The pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated by net charge in a polyacrylamide gel using either preparative gel electrophoresis, which uses a constant pH to separate proteins or isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is also the first step in 2-D gel polyacrylamide gel electrophoresis.

In biomolecules, proteins can be separated by ion exchange chromatography. Biological proteins are made up of zwitterionic amino acid compounds; the net charge of these proteins can be positive or negative depending on the pH of the environment. The specific pI of the target protein can be used to model the process around and the compound can then be purified from the rest of the mixture. Buffers of various pH can be used for this purification process to change the pH of the environment. When a mixture containing a target protein is loaded into an ion exchanger, the stationary matrix can be either positively-charged (for mobile anions) or negatively-charged (for mobile cations). At low pH values, the net charge of most proteins in the mixture is positive - in cation exchangers, these positively-charged proteins bind to the negatively-charged matrix. At high pH values, the net charge of most proteins is negative, where they bind to the positively-charged matrix in anion exchangers. When the environment is at a pH value equal to the protein's pI, the net charge is zero, and the protein is not bound to any exchanger, and therefore, can be eluted out.

For an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.

The pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.

In the two examples (on the right) the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units so the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution
is not known.

The other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of (AMP)H is negligible at the isoelectric point in this case.
If the pI is greater than the pH, the molecule will have a positive charge.

A number of algorithms for estimating isoelectric points of peptides and proteins have been developed. Most of them use Henderson–Hasselbalch equation with different pK values. For instance, within the model proposed by Bjellqvist and co-workers the pK's were determined between closely related immobilines, by focusing the same sample in overlapping pH gradients. Some improvements in the methodology (especially in the determination of the pK values for modified amino acids) have been also proposed. More advanced methods take into account the effect of adjacent amino acids ±3 residues away from a charged aspartic or glutamic acid, the effects on free C terminus, as well as they apply a correction term to the corresponding pK values using genetic algorithm. Other recent approaches are based on a support vector machine algorithm and pKa optimization against experimentally known protein/peptide isoelectric points.

Moreover, experimentally measured isoelectric point of proteins were aggregated into the databases. Recently, a database of isoelectric points for all proteins predicted using most of the available methods had been also developed.

The isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominate surface species is M-O, while at pH values below the IEP, M-OH species predominate. Some approximate values of common ceramics are listed below:

"Note: The following list gives the isoelectric point at 25 °C for selected materials in water. The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. Moreover, the precise measurement of isoelectric points can be difficult, thus many sources often cite differing values for isoelectric points of these materials."

Mixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, a synthetically prepared amorphous aluminosilicate (AlO-SiO) was initially measured as having IEP of 4.5 (the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value). Significantly higher IEP values (pH 6 to 8) have been reported for 3AlO-2SiO by others. Similarly, also IEP of barium titanate, BaTiO was reported in the range 5-6 while others got a value of 3. Mixtures of titania (TiO) and zirconia (ZrO) were studied and found to have an isoelectric point between 5.3-6.9, varying non-linearly with %(ZrO). The surface charge of the mixed oxides was correlated with acidity. Greater titania content led to increased Lewis acidity, whereas zirconia-rich oxides displayed Br::onsted acidity. The different types of acidities produced differences in ion adsorption rates and capacities.

The terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.

In systems in which H/OH are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different than the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.

According to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p"K" and p"K" to define the two conditions in terms of the relative number of charged sites:

For large Δp"K" (>4 according to Jolivet), the predominant species is MOH while there are relatively few charged species - so the PZC is relevant. For small values of Δp"K", there are many charged species in approximately equal numbers, so one speaks of the IEP.





</doc>
<doc id="15058" url="https://en.wikipedia.org/wiki?curid=15058" title="International reply coupon">
International reply coupon

An international reply coupon (IRC) is a coupon that can be exchanged for one or more postage stamps representing the minimum postage for an unregistered priority airmail letter of up to twenty grams sent to another Universal Postal Union (UPU) member country. IRCs are accepted by all UPU member countries.

UPU member postal services are obliged to exchange an IRC for postage, but are not obliged to sell them.

The purpose of the IRC is to allow a person to send someone in another country a letter, along with the cost of postage for a reply. If the addressee is within the same country, there is no need for an IRC because a self-addressed stamped envelope (SASE) or return postcard will suffice; but if the addressee is in another country an IRC removes the necessity of acquiring foreign postage or sending appropriate currency.

International reply coupons (in French, "Coupons-Reponse Internationaux") are printed in blue ink on paper that has the letters “UPU” in large characters in the watermark. The front of each coupon is printed in French. The reverse side of the coupon, which has text relating to its use, is printed in German, English, Arabic, Chinese, Spanish, and Russian. Under Universal Postal Union's regulations, participating member countries are not required to place a control stamp or postmark on the international reply coupons that they sell. Therefore, some foreign issue reply coupons that are tendered for redemption may bear the name of the issuing country (generally in French) rather than the optional control stamp or postmark.

The Nairobi Model was an international reply coupon printed by the Universal Postal Union which is approximately 3.75 inches by 6 inches and had an expiration date of December 31, 2013. This model was designed by Rob Van Goor, a graphic artist from the Luxembourg Post. It was selected from among 10 designs presented by Universal Postal Union member countries. Van Goor interpreted the theme of the contest – "The Postage Stamp: A Vehicle for Exchange" – by depicting the world being cradled by a hand and the perforated outline of a postage stamp.

The Doha Model is named for the 25th UPU congress held in Doha, Qatar, in 2012. The Doha model, designed by Czech artist and graphic designer Michal Sindelar, shows cupped hands catching a stream of water, to celebrate the theme of Water for Life. It expires after December 31, 2017.

The Istanbul Model was designed by graphic artist Nguyen Du's and features a pair of hands and a dove against an Arctic backdrop to represent sustainable development in the postal sector. Ten countries participated in the competition which was held Oct. 7, 2016, during the UPU congress in Istanbul, Turkey. It expires after December 31, 2021.

The IRC was introduced in 1906 at a Universal Postal Union congress in Rome. At the time an IRC could be exchanged for a single-rate, ordinary postage stamp for surface delivery to a foreign country, as this was before the introduction of airmail services. An IRC is exchangeable in a UPU member country for the minimum postage of a priority or unregistered airmail letter to a foreign country.

The current IRC, which features the theme "the Post and sustainable development", was designed by Vietnamese artist Nguyen Du for 2017-2021 and was adopted in Istanbul in 2016, it is known also as the "Istanbul model" for this reason. The previous design, "Water for Life" by Czech artist and graphic designer Michal Sindelar, was issued in 2013 and was valid until 31 December 2017.

IRCs are ordered from the UPU headquarters in Bern, Switzerland by postal authorities. They are generally available at large post offices; in the U.S., they were requisitioned along with regular domestic stamps by any post office that had sufficient demand for them.

Prices for IRCs vary by country. In the United States in November 2012, the purchase price was $2.20 USD; however, the US Postal Service discontinued sales of IRCs on 27 January 2013 due to declining demand. Britain's Royal Mail also stopped selling IRCs on 18 February 2012, citing minimal sales and claiming that the average post office sold less than one IRC per year. IRCs purchased in foreign countries may be used in the United States toward the purchase of postage stamps and embossed stamped envelopes at the current one-ounce First Class International rate (US$1.05 as of April 2012) per coupon.

IRCs are often used by amateur radio operators sending QSL cards to each other; it has traditionally been considered good practice and common courtesy to include an IRC when writing to a foreign operator and expecting a reply by mail. If the operator's home country does not sell IRCs, then a foreign IRC may be used.

Previous editions of the IRC, the "Beijing" model and all subsequent versions, bear an expiration date. Consequently, a new IRC will be issued every three years.

International reply coupons are sold by the HongKong Post for 19 HKD as of 2018-10-19.

International reply coupons are sold by the Swiss Post in packs of 10 for 25 CHF.

The Royal Mail stopped selling IRCs on 31 December 2011 due to a lack of demand.

The United States Postal Service stopped selling international reply coupons on January 27, 2013.

Thailand Post currently sells IRC for 53 THB as of 2020.

In 1920, Charles Ponzi made use of the idea that profit could be made by taking advantage of the differing postal rates in different countries to buy IRCs cheaply in one country and exchange them for stamps of a higher value in another country. This subsequently became the fraudulent Ponzi scheme. In practice, the overhead on buying and selling large numbers of the very low-value IRCs precluded any profitability.

The selling price and exchange value in stamps in each country have been adjusted to some extent to remove some of the potential for profit, but ongoing fluctuations in currency value and exchange rates make it impossible to achieve this completely, as long as stamps represent a specific currency value, instead of acting as vouchers granting specific postal services, devoid of currency nomination.




</doc>
<doc id="15059" url="https://en.wikipedia.org/wiki?curid=15059" title="Isaac Bonewits">
Isaac Bonewits

Phillip Emmons Isaac Bonewits (October 1, 1949 – August 12, 2010) was an American Neo-Druid who published a number of books on the subject of Neopaganism and magic. He was a public speaker, liturgist, singer and songwriter, and founder of the Neopagan organizations Ár nDraíocht Féin and the Aquarian Anti-Defamation League. Born in Royal Oak, Michigan, Bonewits had been heavily involved in occultism since the 1960s.

Bonewits was born on October 1, 1949 in Royal Oak, Michigan, as the fourth of five children. His mother and father were Roman Catholics. Spending much of his childhood in Ferndale, he was moved at age 12 to San Clemente, California, where he spent a short time in a Catholic high school before he went back to public school to graduate from high school a year early. He enrolled at UC Berkeley in 1966; he graduated from the university in 1970 with a Bachelor of Arts in Magic, perhaps becoming the first and only person known to have ever received any kind of academic degree in Magic from an accredited university.

In 1966, while enrolled at UC Berkeley, Bonewits joined the Reformed Druids of North America (RDNA). Bonewits was ordained as a Neo-druid priest in 1969. During this period, the 18-year-old Bonewits was also recruited by the Church of Satan, but left due to political and philosophical conflicts with Anton LaVey. During his stint in the Church of Satan, Bonewits appeared in some scenes of the 1970 documentary "Satanis: The Devil's Mass". Bonewits, in his article "My Satanic Adventure", asserts that the rituals in "Satanis" were staged for the movie at the behest of the filmmakers and were not authentic ceremonies.

His first book, "Real Magic", was published in 1972. Between 1973 and 1975 Bonewits was employed as the editor of "Gnostica" magazine in Minnesota (published by Llewellyn Publications). He established an offshoot group of the Reformed Druids of North America (RDNA) called the Schismatic Druids of North America, and helped create a group called the Hasidic Druids of North America (despite, in his words, his "lifelong status as a gentile"). He also founded the short-lived Aquarian Anti-Defamation League (AADL), an early Pagan civil rights group.

In 1976, Bonewits moved back to Berkeley and rejoined his original grove there, now part of the New Reformed Druids of North America (NRDNA). He was later elected Archdruid of the Berkeley Grove.

Throughout his life Bonewits had varying degrees of involvement with occult groups including Gardnerian Wicca and the New Reformed Orthodox Order of the Golden Dawn (a Wiccan organization not to be confused with the Hermetic Order of the Golden Dawn). Bonewits was a regular presenter at Neopagan conferences and festivals all over the US, as well as attending gaming conventions in the Bay Area. He promoted his book "Authentic Thaumaturgy" to gamers as a way of organizing Dungeons and Dragons games and to give a background to games of .

In 1983, Bonewits founded Ár nDraíocht Féin (also known as "A Druid Fellowship" or ADF), which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization. Although illness curtailed many of his activities and travels for a time, he remained Archdruid of ADF until 1996. In that year, he resigned from the position of Archdruid but retained the lifelong title of ADF Archdruid Emeritus.

A songwriter, singer, and recording artist, he produced two CDs of pagan music and numerous recorded lectures and panel discussions, produced and distributed by the Association for Consciousness Exploration. He lived in Rockland County, New York, and was a member of the Covenant of Unitarian Universalist Pagans (CUUPS).

Bonewits encouraged charity programs to help Neopagan seniors, and in January 2006 was the keynote speaker at the Conference On Current Pagan Studies at the Claremont Graduate University in Claremont, CA.

Bonewits was married five times. He was married to Rusty Elliot from 1973 to 1976. His second wife was Selene Kumin Vega, followed by marriage to Sally Eaton (1980 to 1985). His fourth wife was author Deborah Lipp, from 1988 to 1998. On July 23, 2004, he was married in a handfasting ceremony to a former vice-president of the Covenant of Unitarian Universalist Pagans, Phaedra Heyman Bonewits. At the time of the handfasting, the marriage was not yet legal because he had not yet been legally divorced from Lipp, although they had been separated for several years. Paperwork and legalities caught up on December 31, 2007, making them legally married.

Bonewits' only child, Arthur Shaffrey Lipp-Bonewits, was born to Deborah Lipp in 1990.

In 1990, Bonewits was diagnosed with eosinophilia-myalgia syndrome. The illness was a factor in his eventual resignation from the position of Archdruid of the ADF.

On October 25, 2009, Bonewits was diagnosed with a rare form of colon cancer, for which he underwent treatment. He died at home, on August 12, 2010, surrounded by his family.

In his book "Real Magic" (1971), Bonewits proposed his "Laws of Magic." These "laws" are synthesized from a multitude of belief systems from around the world to explain and categorize magical beliefs within a cohesive framework. Many interrelationships exist, and some belief systems are subsets of others. This work was chosen by Dennis Wheatley in the 1970s to be part of his publishing project 'Library of the Occult'.

Bonewits also coined much of the modern terminology used to articulate the themes and issues that affect the North American Neopagan community.








</doc>
<doc id="15062" url="https://en.wikipedia.org/wiki?curid=15062" title="Intel 8080">
Intel 8080

The Intel 8080 (""eighty-eighty"") is the second 8-bit microprocessor designed and manufactured by Intel. It first appeared in April 1974 and is an extended and enhanced variant of the earlier 8008 design, although without binary compatibility. The initial specified clock rate or frequency limit was 2 MHz, and with common instructions using 4, 5, 7, 10, or 11 cycles this meant that it operated at a typical speed of a few hundred thousand instructions per second. A faster variant 8080A-1 (Sometimes called the 8080B) became available later with clock frequency limit up to 3.125 MHz.

The 8080 needs two support chips to function in most applications, the i8224 clock generator/driver and the i8228 bus controller, and it is implemented in N-type metal-oxide-semiconductor logic (NMOS) using non-saturated enhancement mode transistors as loads thus demanding a +12 V and a −5 V voltage in addition to the main transistor–transistor logic (TTL) compatible +5 V.

Although earlier microprocessors were used for calculators, cash registers, computer terminals, industrial robots, and other applications, the 8080 became one of the first widespread microprocessors. Several factors contributed to its popularity: its 40-pin package made it easier to interface than the 18-pin 8008, and also made its data bus more efficient; its NMOS implementation gave it faster transistors than those of the P-type metal-oxide-semiconductor logic (PMOS) 8008, while also simplifying interfacing by making it TTL-compatible; a wider variety of support chips was available; its instruction set was enhanced over the 8008; and its full 16-bit address bus (versus the 14-bit one of the 8008) enabled it to access 64 KB of memory, four times more than the 8008's range of 16 KB. It became the engine of the Altair 8800, and subsequent S-100 bus personal computers, until it was replaced by the Z80 in this role, and was the original target CPU for CP/M operating systems developed by Gary Kildall.

The 8080 was successful enough that translation compatibility at the assembly language level became a design requirement for the Intel 8086 when its design began in 1976, and led to the 8080 directly influencing all later variants of the ubiquitous 32-bit and 64-bit x86 architectures.

The Intel 8080 is the successor to the 8008. It uses the same basic instruction set and register model as the 8008 (developed by Computer Terminal Corporation), even though it is not source code compatible nor binary code compatible with its predecessor. Every instruction in the 8008 has an equivalent instruction in the 8080 (even though the opcodes differ between the two CPUs). The 8080 also adds a few 16-bit operations in its instruction set. Whereas the 8008 required the use of the HL register pair to indirectly access its 14-bit memory space, the 8080 added addressing modes to allow direct access to its full 16-bit memory space. In addition, the internal 7-level push-down call stack of the 8008 was replaced by a dedicated 16-bit stack-pointer (SP) register. The 8080's large 40-pin DIP packaging permits it to provide a 16-bit address bus and an 8-bit data bus, allowing easy access to 64 KiB of memory.

The processor has seven 8-bit registers (A, B, C, D, E, H, and L), where A is the primary 8-bit accumulator, and the other six registers can be used as either individual 8-bit registers or as three 16-bit register pairs (BC, DE, and HL, referred to as B, D and H in Intel documents) depending on the particular instruction. Some instructions also enable the HL register pair to be used as a (limited) 16-bit accumulator, and a pseudo-register M can be used almost anywhere that any other register can be used, referring to the memory address pointed to by the HL pair. It also has a 16-bit stack pointer to memory (replacing the 8008's internal stack), and a 16-bit program counter.

The processor maintains internal flag bits (a status register), which indicate the results of arithmetic and logical instructions. Only certain instructions affect the flags. The flags are:

The carry bit can be set or complemented by specific instructions. Conditional-branch instructions test the various flag status bits. The flags can be copied as a group to the accumulator. The A accumulator and the flags together are called the PSW register, or program status word.

As with many other 8-bit processors, all instructions are encoded in one byte (including register numbers, but excluding immediate data), for simplicity. Some of them are followed by one or two bytes of data, which can be an immediate operand, a memory address, or a port number. Like larger processors, it has automatic CALL and RET instructions for multi-level procedure calls and returns (which can even be conditionally executed, like jumps) and instructions to save and restore any 16-bit register pair on the machine stack. There are also eight one-byte call instructions () for subroutines located at the fixed addresses 00h, 08h, 10h, ..., 38h. These are intended to be supplied by external hardware in order to invoke a corresponding interrupt service routine, but are also often employed as fast system calls. The most sophisticated command is , which is used for exchanging the register pair HL with the value stored at the address indicated by the stack pointer.

Most 8-bit operations can only be performed on the 8-bit accumulator (the A register). For 8-bit operations with two operands, the other operand can be either an immediate value, another 8-bit register, or a memory byte addressed by the 16-bit register pair HL. Direct copying is supported between any two 8-bit registers and between any 8-bit register and an HL-addressed memory byte. Due to the regular encoding of the instruction (using a quarter of available opcode space), there are redundant codes to copy a register into itself (, for instance), which are of little use, except for delays. However, what would have been a copy from the HL-addressed cell into itself (i.e., ) is instead used to encode the halt () instruction, halting execution until an external reset or interrupt occurs.

Although the 8080 is generally an 8-bit processor, it also has limited abilities to perform 16-bit operations: Any of the three 16-bit register pairs (BC, DE, or HL, referred to as B, D, H in Intel documents) or SP can be loaded with an immediate 16-bit value (using ), incremented or decremented (using and ), or added to HL (using ). The instruction exchanges the values of the HL and DE register pairs. By adding HL to itself, it is possible to achieve the same result as a 16-bit arithmetical left shift with one instruction. The only 16-bit instructions that affect any flag are , which set the CY (carry) flag in order to allow for programmed 24-bit or 32-bit arithmetic (or larger), needed to implement floating-point arithmetic, for instance.

The 8080 supports up to 256 input/output (I/O) ports, accessed via dedicated I/O instructions taking port addresses as operands. This I/O mapping scheme is regarded as an advantage, as it frees up the processor's limited address space. Many CPU architectures instead use so-called memory-mapped I/O (MMIO), in which a common address space is used for both RAM and peripheral chips. This removes the need for dedicated I/O instructions, although a drawback in such designs may be that special hardware must be used to insert wait states, as peripherals are often slower than memory. However, in some simple 8080 computers, I/O is indeed addressed as if they were memory cells, "memory-mapped", leaving the I/O commands unused. I/O addressing can also sometimes employ the fact that the processor outputs the same 8-bit port address to both the lower and the higher address byte (i.e., would put the address 0505h on the 16-bit address bus). Similar I/O-port schemes are used in the backward-compatible Zilog Z80 and Intel 8085, and the closely related x86 microprocessor families.

One of the bits in the processor state word (see below) indicates that the processor is accessing data from the stack. Using this signal, it is possible to implement a separate stack memory space. However, this feature is seldom used.

For more advanced systems, during one phase of its working loop, the processor set its "internal state byte" on the data bus. This byte contains flags that determine whether the memory or I/O port is accessed and whether it is necessary to handle an interrupt.

The interrupt system state (enabled or disabled) is also output on a separate pin. For simple systems, where the interrupts are not used, it is possible to find cases where this pin is used as an additional single-bit output port (the popular Radio-86RK computer made in the Soviet Union, for instance).

The following 8080/8085 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.

The address bus has its own 16 pins, and the data bus has 8 pins that are usable without any multiplexing. Using the two additional pins (read and write signals), it is possible to assemble simple microprocessor devices very easily. Only the separate IO space, interrupts, and DMA need added chips to decode the processor pin signals. However, the processor load capacity is limited, and even simple computers often contain bus amplifiers.

The processor needs three power sources (−5, +5, and +12 V) and two non-overlapping high-amplitude synchronizing signals. However, at least the late Soviet version КР580ВМ80А was able to work with a single +5 V power source, the +12 V pin being connected to +5 V and the −5 V pin to ground. The processor consumes about 1.3 W of power.

The pin-out table, from the chip's accompanying documentation, describes the pins as follows:
A key factor in the success of the 8080 was the broad range of support chips available, providing serial communications, counter/timing, input/output, direct memory access, and programmable interrupt control amongst other functions:

The 8080 integrated circuit uses non-saturated enhancement-load nMOS gates, demanding extra voltages (for the load-gate bias). It was manufactured in a silicon gate process using a minimal feature size of 6 µm. A single layer of metal is used to interconnect the approximately 6,000 transistors in the design, but the higher resistance polysilicon layer, which required higher voltage for some interconnects, is implemented with transistor gates. The die size is approximately 20 mm.

The 8080 is used in many early microcomputers, such as the MITS Altair 8800 Computer, Processor Technology SOL-20 Terminal Computer and IMSAI 8080 Microcomputer, forming the basis for machines running the CP/M operating system (the later, almost fully compatible and more able, Zilog Z80 processor would capitalize on this, with Z80 & CP/M becoming the dominant CPU and OS combination of the period circa 1976 to 1983 much as did the x86 & DOS for the PC a decade later).

Even in 1979 after introduction of the Z80 and 8085 processors, five manufacturers of the 8080 were selling an estimated 500,000 units per month at a price around $3 to $4 each.

The first single-board microcomputers, such as MYCRO-1 and the "dyna-micro" / MMD-1 (see: Single-board computer) were based on the Intel 8080. One of the early uses of the 8080 was made in the late 1970s by Cubic-Western Data of San Diego, CA in its Automated Fare Collection Systems custom designed for mass transit systems around the world. An early industrial use of the 8080 is as the "brain" of the DatagraphiX Auto-COM (Computer Output Microfiche) line of products which takes large amounts of user data from reel-to-reel tape and images it onto microfiche. The Auto-COM instruments also include an entire automated film cutting, processing, washing, and drying sub-system – quite a feat, both then and in the 21st century, to all be accomplished successfully with only an 8-bit microprocessor running at a clock speed of less than 1 MHz with a 64 KB memory limit. Also, several early video arcade games were built around the 8080 microprocessor, including "Space Invaders", one of the most popular arcade games ever made.

Shortly after the launch of the 8080, the Motorola 6800 competing design was introduced, and after that, the MOS Technology 6502 derivative of the 6800.

Zilog introduced the Z80, which has a compatible machine language instruction set and initially used the same assembly language as the 8080, but for legal reasons, Zilog developed a syntactically-different (but code compatible) alternative assembly language for the Z80. At Intel, the 8080 was followed by the compatible and electrically more elegant 8085.

Later Intel issued the assembly-language compatible (but not binary-compatible) 16-bit 8086 and then the 8/16-bit 8088, which was selected by IBM for its new PC to be launched in 1981. Later NEC made the NEC V20 (an 8088 clone with Intel 80186 instruction set compatibility) which also supports an 8080 emulation mode. This is also supported by NEC's V30 (a similarly enhanced 8086 clone). Thus, the 8080, via its instruction set architecture (ISA), made a lasting impact on computer history.

A number of processors compatible with the Intel 8080A were manufactured in the Eastern Bloc: the KR580VM80A (initially marked as KP580ИK80) in the Soviet Union, the MCY7880 made by Unitra CEMI in Poland, the MHB8080A made by TESLA in Czechoslovakia, the 8080APC made by Tungsram / MEV in Hungary, and the MMN8080 made by Microelectronica Bucharest in Romania.

, the 8080 is still in production at Lansdale Semiconductors.

The 8080 also changed how computers were created. When the 8080 was introduced, computer systems were usually created by computer manufacturers such as Digital Equipment Corporation, Hewlett Packard, or IBM. A manufacturer would produce the whole computer, including processor, terminals, and system software such as compilers and operating system. The 8080 was designed for almost any application "except" a complete computer system. Hewlett Packard developed the HP 2640 series of smart terminals around the 8080. The HP 2647 is a terminal which runs the programming language BASIC on the 8080. Microsoft would market as its founding product the first popular language for the 8080, and would later acquire DOS for the IBM PC.

The 8080 and 8085 gave rise to the 8086, which was designed as a source code compatible (although not binary compatible) extension of the 8085. This design, in turn, later spawned the x86 family of chips, the basis for most CPUs in use today. Many of the 8080's core machine instructions and concepts, for example, registers named "A", "B", "C", and "D", and many of the flags used to control conditional jumps, are still in use in the widespread x86 platform. 8080 assembly code can still be directly translated into x86 instructions; all of its core elements are still present.

Federico Faggin, the originator of the 8080 architecture in early 1972, proposed it to Intel's management and pushed for its implementation. He finally got the permission to develop it six months later. Faggin hired Masatoshi Shima from Japan in November 1972, who did the detailed design under his direction, using the design methodology for random logic with silicon gate that Faggin had created for the 4000 family. Stanley Mazor contributed a couple of instructions to the instruction set.

Shima finished the layout in August 1973. After the regulation of NMOS fabrication, a prototype of the 8080 was completed in January 1974. It had a flaw, in that driving with standard TTL devices increased the ground voltage because high current flowed into the narrow line. However, Intel had already produced 40,000 units of the 8080 at the direction of the sales section before Shima characterized the prototype. It was released as requiring Low-power Schottky TTL (LS TTL) devices. The 8080A fixed this flaw.

Intel offered an instruction set simulator for the 8080 named INTERP/80. It was written by Gary Kildall while he worked as a consultant for Intel.






</doc>
<doc id="15063" url="https://en.wikipedia.org/wiki?curid=15063" title="Intel 8086">
Intel 8086

The 8086 (also called iAPX 86) is a 16-bit microprocessor chip designed by Intel between early 1976 and June 8, 1978, when it was released. The Intel 8088, released July 1, 1979, is a slightly modified chip with an external 8-bit data bus (allowing the use of cheaper and fewer supporting ICs), and is notable as the processor used in the original IBM PC design.

The 8086 gave rise to the x86 architecture, which eventually became Intel's most successful line of processors. On June 5, 2018, Intel released a limited-edition CPU celebrating the 40th anniversary of the Intel 8086, called the Intel Core i7-8086K.

In 1972, Intel launched the 8008, the first 8-bit microprocessor. It implemented an instruction set designed by Datapoint corporation with programmable CRT terminals in mind, which also proved to be fairly general-purpose. The device needed several additional ICs to produce a functional computer, in part due to it being packaged in a small 18-pin "memory package", which ruled out the use of a separate address bus (Intel was primarily a DRAM manufacturer at the time).

Two years later, Intel launched the 8080, employing the new 40-pin DIL packages originally developed for calculator ICs to enable a separate address bus. It has an extended instruction set that is source-compatible (not binary compatible) with the 8008 and also includes some 16-bit instructions to make programming easier. The 8080 device, was eventually replaced by the depletion-load-based 8085 (1977), which sufficed with a single +5 V power supply instead of the three different operating voltages of earlier chips. Other well known 8-bit microprocessors that emerged during these years are Motorola 6800 (1974), General Instrument PIC16X (1975), MOS Technology 6502 (1975), Zilog Z80 (1976), and Motorola 6809 (1978).

The 8086 project started in May 1976 and was originally intended as a temporary substitute for the ambitious and delayed iAPX 432 project. It was an attempt to draw attention from the less-delayed 16- and 32-bit processors of other manufacturers (such as Motorola, Zilog, and National Semiconductor) and at the same time to counter the threat from the Zilog Z80 (designed by former Intel employees), which became very successful. Both the architecture and the physical chip were therefore developed rather quickly by a small group of people, and using the same basic microarchitecture elements and physical implementation techniques as employed for the slightly older 8085 (and for which the 8086 also would function as a continuation).

Marketed as source compatible, the 8086 was designed to allow assembly language for the 8008, 8080, or 8085 to be automatically converted into equivalent (suboptimal) 8086 source code, with little or no hand-editing. The programming model and instruction set is (loosely) based on the 8080 in order to make this possible. However, the 8086 design was expanded to support full 16-bit processing, instead of the fairly limited 16-bit capabilities of the 8080 and 8085.

New kinds of instructions were added as well; full support for signed integers, base+offset addressing, and self-repeating operations were akin to the Z80 design but were all made slightly more general in the 8086. Instructions directly supporting nested ALGOL-family languages such as Pascal and PL/M were also added. According to principal architect Stephen P. Morse, this was a result of a more software-centric approach than in the design of earlier Intel processors (the designers had experience working with compiler implementations). Other enhancements included microcoded multiply and divide instructions and a bus structure better adapted to future coprocessors (such as 8087 and 8089) and multiprocessor systems.

The first revision of the instruction set and high level architecture was ready after about three months, and as almost no CAD tools were used, four engineers and 12 layout people were simultaneously working on the chip. The 8086 took a little more than two years from idea to working product, which was considered rather fast for a complex design in 1976–1978.

The 8086 was sequenced using a mixture of random logic and microcode and was implemented using depletion-load nMOS circuitry with approximately 20,000 active transistors (29,000 counting all ROM and PLA sites). It was soon moved to a new refined nMOS manufacturing process called HMOS (for High performance MOS) that Intel originally developed for manufacturing of fast static RAM products. This was followed by HMOS-II, HMOS-III versions, and, eventually, a fully static CMOS version for battery powered devices, manufactured using Intel's CHMOS processes. The original chip measured 33 mm² and minimum feature size was 3.2 μm.

The architecture was defined by Stephen P. Morse with some help and assistance by Bruce Ravenel (the architect of the 8087) in refining the final revisions. Logic designer Jim McKevitt and John Bayliss were the lead engineers of the hardware-level development team and Bill Pohlman the manager for the project. The legacy of the 8086 is enduring in the basic instruction set of today's personal computers and servers; the 8086 also lent its last two digits to later extended versions of the design, such as the Intel 286 and the Intel 386, all of which eventually became known as the x86 family. (Another reference is that the PCI Vendor ID for Intel devices is 8086.)

All internal registers, as well as internal and external data buses, are 16 bits wide, which firmly established the "16-bit microprocessor" identity of the 8086. A 20-bit external address bus provides a 1 MB physical address space (2 = 1,048,576). This address space is addressed by means of internal memory "segmentation". The data bus is multiplexed with the address bus in order to fit all of the control lines into a standard 40-pin dual in-line package. It provides a 16-bit I/O address bus, supporting 64 KB of separate I/O space. The maximum linear address space is limited to 64 KB, simply because internal address/index registers are only 16 bits wide. Programming over 64 KB memory boundaries involves adjusting the segment registers (see below); this difficulty existed until the 80386 architecture introduced wider (32-bit) registers (the memory management hardware in the 80286 did not help in this regard, as its registers are still only 16 bits wide).

Some of the control pins, which carry essential signals for all external operations, have more than one function depending upon whether the device is operated in "min" or "max" mode. The former mode is intended for small single-processor systems, while the latter is for medium or large systems using more than one processor (a kind of multiprocessor mode). Maximum mode is required when using an 8087 or 8089 coprocessor. The voltage on pin 33 (MN/) determine the mode. Changing the state of pin 33 changes the function of certain other pins, most of which have to do with how the CPU handles the (local) bus. The mode is usually hardwired into the circuit and therefore cannot be changed by software. The workings of these modes are described in terms of timing diagrams in Intel datasheets and manuals. In minimum mode, all control signals are generated by the 8086 itself.

The 8086 has eight more or less general 16-bit registers (including the stack pointer but excluding the instruction pointer, flag register and segment registers). Four of them, AX, BX, CX, DX, can also be accessed as twice as many 8-bit registers (see figure) while the other four, SI, DI, BP, SP, are 16-bit only.

Due to a compact encoding inspired by 8-bit processors, most instructions are one-address or two-address operations, which means that the result is stored in one of the operands. At most one of the operands can be in memory, but this memory operand can also be the "destination", while the other operand, the "source", can be either "register" or "immediate". A single memory location can also often be used as both "source" and "destination" which, among other factors, further contributes to a code density comparable to (and often better than) most eight-bit machines at the time.

The degree of generality of most registers are much greater than in the 8080 or 8085. However, 8086 registers were more specialized than in most contemporary minicomputers and are also used implicitly by some instructions. While perfectly sensible for the assembly programmer, this makes register allocation for compilers more complicated compared to more orthogonal 16-bit and 32-bit processors of the time such as the PDP-11, VAX, 68000, 32016 etc. On the other hand, being more regular than the rather minimalistic but ubiquitous 8-bit microprocessors such as the 6502, 6800, 6809, 8085, MCS-48, 8051, and other contemporary accumulator-based machines, it is significantly easier to construct an efficient code generator for the 8086 architecture.

Another factor for this is that the 8086 also introduced some new instructions (not present in the 8080 and 8085) to better support stack-based high-level programming languages such as Pascal and PL/M; some of the more useful instructions are push "mem-op", and ret "size", supporting the "Pascal calling convention" directly. (Several others, such as push "immed" and enter, were added in the subsequent 80186, 80286, and 80386 processors.)

A 64 KB (one segment) stack growing towards lower addresses is supported in hardware; 16-bit words are pushed onto the stack, and the top of the stack is pointed to by SS:SP. There are 256 interrupts, which can be invoked by both hardware and software. The interrupts can cascade, using the stack to store the return addresses.

The 8086 has 64 K of 8-bit (or alternatively 32 K of 16-bit word) I/O port space.

The 8086 has a 16-bit flags register. Nine of these condition code flags are active, and indicate the current state of the processor: Carry flag (CF), Parity flag (PF), Auxiliary carry flag (AF), Zero flag (ZF), Sign flag (SF), Trap flag (TF), Interrupt flag (IF), Direction flag (DF), and Overflow flag (OF).
Also referred to as the status word, the layout of the flags register is as follows:

There are also three 16-bit segment registers (see figure) that allow the 8086 CPU to access one megabyte of memory in an unusual way. Rather than concatenating the segment register with the address register, as in most processors whose address space exceeds their register size, the 8086 shifts the 16-bit segment only four bits left before adding it to the 16-bit offset (16×segment + offset), therefore producing a 20-bit external (or effective or physical) address from the 32-bit segment:offset pair. As a result, each external address can be referred to by 2 = 4096 different segment:offset pairs.

Although considered complicated and cumbersome by many programmers, this scheme also has advantages; a small program (less than 64 KB) can be loaded starting at a fixed offset (such as 0000) in its own segment, avoiding the need for relocation, with at most 15 bytes of alignment waste.

Compilers for the 8086 family commonly support two types of pointer, "near" and "far". Near pointers are 16-bit offsets implicitly associated with the program's code or data segment and so can be used only within parts of a program small enough to fit in one segment. Far pointers are 32-bit segment:offset pairs resolving to 20-bit external addresses. Some compilers also support "huge" pointers, which are like far pointers except that pointer arithmetic on a huge pointer treats it as a linear 20-bit pointer, while pointer arithmetic on a far pointer wraps around within its 16-bit offset without touching the segment part of the address.

To avoid the need to specify "near" and "far" on numerous pointers, data structures, and functions, compilers also support "memory models" which specify default pointer sizes. The "tiny" (max 64K), "small" (max 128K), "compact" (data > 64K), "medium" (code > 64K), "large" (code,data > 64K), and "huge" (individual arrays > 64K) models cover practical combinations of near, far, and huge pointers for code and data. The "tiny" model means that code and data are shared in a single segment, just as in most 8-bit based processors, and can be used to build ".com" files for instance. Precompiled libraries often come in several versions compiled for different memory models.

According to Morse et al.. the designers actually contemplated using an 8-bit shift (instead of 4-bit), in order to create a 16 MB physical address space. However, as this would have forced segments to begin on 256-byte boundaries, and 1 MB was considered very large for a microprocessor around 1976, the idea was dismissed. Also, there were not enough pins available on a low cost 40-pin package for the additional four address bus pins.

In principle, the address space of the x86 series "could" have been extended in later processors by increasing the shift value, as long as applications obtained their segments from the operating system and did not make assumptions about the equivalence of different segment:offset pairs. In practice the use of "huge" pointers and similar mechanisms was widespread and the flat 32-bit addressing made possible with the 32-bit offset registers in the 80386 eventually extended the limited addressing range in a more general way (see below).

Intel could have decided to implement memory in 16 bit words (which would have eliminated the signal along with much of the address bus complexities already described). This would mean that all instruction object codes and data would have to be accessed in 16-bit units. Users of the 8080 long ago realized, in hindsight, that the processor makes very efficient use of its memory. By having a large number of 8-bit object codes, the 8080 produces object code as compact as some of the most powerful minicomputers on the market at the time.

If the 8086 is to retain 8-bit object codes and hence the efficient memory use of the 8080, then it cannot guarantee that (16-bit) opcodes and data will lie on an even-odd byte address boundary. The first 8-bit opcode will shift the next 8-bit instruction to an odd byte or a 16-bit instruction to an odd-even byte boundary. By implementing the signal and the extra logic needed, the 8086 allows instructions to exist as 1-byte, 3-byte or any other odd byte object codes.

Simply put: this is a trade off. If memory addressing is simplified so that memory is only accessed in 16-bit units, memory will be used less efficiently. Intel decided to make the logic more complicated, but memory use more efficient. This was at a time when memory size was considerably smaller, and at a premium, than that which users are used to today.

Small programs could ignore the segmentation and just use plain 16-bit addressing. This allows 8-bit software to be quite easily ported to the 8086. The authors of most DOS implementations took advantage of this by providing an Application Programming Interface very similar to CP/M as well as including the simple ".com" executable file format, identical to CP/M. This was important when the 8086 and MS-DOS were new, because it allowed many existing CP/M (and other) applications to be quickly made available, greatly easing acceptance of the new platform.

The following 8086/8088 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.

The code above uses the BP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by most ALGOL-like languages since the late 1950s.

The above routine is a rather cumbersome way to copy blocks of data. The 8086 provides dedicated instructions for copying strings of bytes. These instructions assume that the source data is stored at DS:SI, the destination data is stored at ES:DI, and that the number of elements to copy is stored in CX. The above routine requires the source and the destination block to be in the same segment, therefore DS is copied to ES. The loop section of the above can be replaced by:

This copies the block of data one byte at a time. The codice_2 instruction causes the following codice_3 to repeat until CX is zero, automatically incrementing SI and DI and decrementing CX as it repeats. Alternatively the codice_4 instruction can be used to copy 16-bit words (double bytes) at a time (in which case CX counts the number of words copied instead of the number of bytes). Most assemblers will properly recognize the codice_2 instruction if used as an in-line prefix to the codice_3 instruction, as in codice_7.

This routine will operate correctly if interrupted, because the program counter will continue to point to the codice_8 instruction until the block copy is completed. The copy will therefore continue from where it left off when the interrupt service routine returns control.

Although partly shadowed by other design choices in this particular chip, the multiplexed address and data buses limit performance slightly; transfers of 16-bit or 8-bit quantities are done in a four-clock memory access cycle, which is faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions vary from one to six bytes, fetch and execution are made concurrent and decoupled into separate units (as it remains in today's x86 processors): The "bus interface unit" feeds the instruction stream to the "execution unit" through a 6-byte prefetch queue (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, as well as other enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).

As can be seen from these tables, operations on registers and immediates were fast (between 2 and 4 cycles), while memory-operand instructions and jumps were quite slow; jumps took more cycles than on the simple 8080 and 8085, and the 8088 (used in the IBM PC) was additionally hampered by its narrower bus. The reasons why most memory related instructions were slow were threefold:

However, memory access performance was drastically enhanced with Intel's next generation of 8086 family CPUs. The 80186 and 80286 both had dedicated address calculation hardware, saving many cycles, and the 80286 also had separate (non-multiplexed) address and data buses.

The 8086/8088 could be connected to a mathematical coprocessor to add hardware/microcode-based floating-point performance. The Intel 8087 was the standard math coprocessor for the 8086 and 8088, operating on 80-bit numbers. Manufacturers like Cyrix (8087-compatible) and Weitek ("not" 8087-compatible) eventually came up with high-performance floating-point coprocessors that competed with the 8087.

The clock frequency was originally limited to 5 MHz, but the last versions in HMOS were specified for 10 MHz. HMOS-III and CMOS versions were manufactured for a long time (at least a while into the 1990s) for embedded systems, although its successor, the 80186/80188 (which includes some on-chip peripherals), has been more popular for embedded use.

The 80C86, the CMOS version of the 8086, was used in the GRiDPad, Toshiba T1200, HP 110, and finally the 1998–1999 Lunar Prospector.

For the packaging, the Intel 8086 was available both in ceramic and plastic DIP packages.

Compatible—and, in many cases, enhanced—versions were manufactured by Fujitsu, Harris/Intersil, OKI, Siemens AG, Texas Instruments, NEC, Mitsubishi, and AMD. For example, the NEC V20 and NEC V30 pair were hardware-compatible with the 8088 and 8086 even though NEC made original Intel clones μPD8088D and μPD8086D respectively, but incorporated the instruction set of the 80186 along with some (but not all) of the 80186 speed enhancements, providing a drop-in capability to upgrade both instruction set and processing speed without manufacturers having to modify their designs. Such relatively simple and low-power 8086-compatible processors in CMOS are still used in embedded systems.

The electronics industry of the Soviet Union was able to replicate the 8086 through . The resulting chip, K1810VM86, was binary and pin-compatible with the 8086.

i8086 and i8088 were respectively the cores of the Soviet-made PC-compatible EC1831 and EC1832 desktops. (EC1831 is the EC identification of IZOT 1036C and EC1832 is the EC identification of IZOT 1037C, developed and manufactured in Bulgaria. EC stands for Единая Система.) However, the EC1831 computer (IZOT 1036C) had significant hardware differences from the IBM PC prototype. The EC1831 was the first PC-compatible computer with dynamic bus sizing (US Pat. No 4,831,514). Later some of the EC1831 principles were adopted in PS/2 (US Pat. No 5,548,786) and some other machines (UK Patent Application, Publication No. GB-A-2211325, Published June 28, 1989).






</doc>
<doc id="15064" url="https://en.wikipedia.org/wiki?curid=15064" title="Intel 8088">
Intel 8088

The Intel 8088 (""eighty-eighty-eight"", also called iAPX 88) microprocessor is a variant of the Intel 8086. Introduced on June 1, 1979, the 8088 had an eight-bit external data bus instead of the 16-bit bus of the 8086. The 16-bit registers and the one megabyte address range were unchanged, however. In fact, according to the Intel documentation, the 8086 and 8088 have the same execution unit (EU)—only the bus interface unit (BIU) is different. The original IBM PC was based on the 8088, as were its clones.

The 8088 was designed at Intel's laboratory in Haifa, Israel, as were a large number of Intel's processors. The 8088 was targeted at economical systems by allowing the use of an eight-bit data path and eight-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The prefetch queue of the 8088 was shortened to four bytes, from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's then-new design office and laboratory in Haifa.

Variants of the 8088 with more than 5 MHz maximal clock frequency include the 8088–2, which was fabricated using Intel's new enhanced nMOS process called HMOS and specified for a maximal frequency of 8 MHz. Later followed the 80C88, a fully static CHMOS design, which could operate with clock speeds from 0 to 8 MHz. There were also several other, more or less similar, variants from other manufacturers. For instance, the NEC V20 was a pin-compatible and slightly faster (at the same clock frequency) variant of the 8088, designed and manufactured by NEC. Successive NEC 8088 compatible processors would run at up to 16 MHz. In 1984, Commodore International signed a deal to manufacture the 8088 for use in a licensed Dynalogic Hyperion clone, in a move that was regarded as signaling a major new direction for the company.

When announced, the list price of the 8088 was US$124.80.

The 8088 is architecturally very similar to the 8086. The main difference is that there are only eight data lines instead of the 8086's 16 lines. All of the other pins of the device perform the same function as they do with the 8086 with two exceptions. First, pin 34 is no longer (this is the high-order byte select on the 8086—the 8088 does not have a high-order byte on its eight-bit data bus). Instead it outputs a maximum mode status, . Combined with the IO/ and DT/ signals, the bus cycles can be decoded (it generally indicates when a write operation or an interrupt is in progress). The second change is the pin that signals whether a memory access or input/output access is being made has had it sense reversed. The pin on the 8088 is IO/. On the 8086 part it is /M. The reason for the reversal is that it makes the 8088 compatible with the 8085.

Depending on the clock frequency, the number of memory wait states, as well as on the characteristics of the particular application program, the "average" performance for the Intel 8088 ranged approximately from 0.33 to 1 million instructions per second. Meanwhile, the codice_1 and codice_2 instructions, taking two and three cycles respectively, yielded an "absolute peak" performance of between and  MIPS per MHz, that is, somewhere in the range 3–5 MIPS at 10 MHz.

The speed of the execution unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the prefetch queue a good bit of the time. Cutting down the bus to eight bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the four-byte prefetch queue. When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take four clock cycles to complete a bus cycle; whereas for the 8086 this means four clocks to transfer two bytes, on the 8088 it is four clocks per byte. Therefore, for example, a two-byte shift or rotate instruction, which takes the EU only two clock cycles to execute, actually takes eight clock cycles to complete if it is not in the prefetch queue. A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and 
in general, because so many basic instructions execute in fewer than four clocks per instruction byte—including almost all the ALU and data-movement instructions on register operands and some of these on memory operands—it is practically impossible to avoid idling the EU in the 8088 at least ¼ of the time while executing useful real-world programs, and it is not hard to idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).

A side effect of the 8088 design, with the slow bus and the small prefetch queue, is that the speed of code execution can be very dependent on instruction order. When programming the 8088, for CPU efficiency, it is vital to interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte prefetch queue to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.

The 8088 is also (like the 8086) slow at accessing memory. The same ALU that is used to execute arithmetic and logic instructions is also used to calculate effective addresses. There is a separate adder for adding a shifted segment register to the offset address, but the offset EA itself is always calculated entirely in the main ALU. Furthermore, the loose coupling of the EU and BIU (bus unit) inserts communication overhead between the units, and the four-clock period bus transfer cycle is not particularly streamlined. Contrast this with the two-clock period bus cycle of the 6502 CPU and the 80286's three-clock period bus cycle with pipelining down to two cycles for most transfers. Most 8088 instructions that can operate on either registers or memory, including common ALU and data-movement operations, are at least four times slower for memory operands than for only register operands. Therefore, efficient 8088 (and 8086) programs avoid repeated access of memory operands when possible, loading operands from memory into registers to work with them there and storing back only the finished results. The relatively large general register set of the 8088 compared to its contemporaries assists this strategy. When there are not enough registers for all variables that are needed at once, saving registers by pushing them onto the stack and popping them back to restore them is the fastest way to use memory to augment the registers, as the stack PUSH and POP instructions are the fastest memory operations. The same is probably not true on the 80286 and later; they have dedicated address ALUs and perform memory accesses much faster than the 8088 and 8086.

Finally, because calls, jumps, and interrupts reset the prefetch queue, and because loading the IP register requires communication between the EU and the BIU (since the IP register is in the BIU, not in the EU, where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires four clock cycles if not taken, but if taken, it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions to achieve the same result.

Intel datasheets for the 8086 and 8088 advertised the dedicated multiply and divide instructions (MUL, IMUL, DIV, and IDIV), but they are very slow, on the order of 100–200 clock cycles each. Many simple multiplications by small constants (besides powers of 2, for which shifts can be used) can be done much faster using dedicated short subroutines. The 80286 and 80386 each greatly increased the execution speed of these multiply and divide instructions. 

The original IBM PC was the most influential microcomputer to use the 8088. It used a clock frequency of 4.77 MHz (4/3 the NTSC colorburst frequency). Some of IBM's engineers and other employees wanted to use the IBM 801 processor, some would have preferred the new Motorola 68000, while others argued for a small and simple microprocessor, such as the MOS Technology 6502 or Zilog Z80, which had been used in earlier personal computers. However, IBM already had a history of using Intel chips in its products and had also acquired the rights to manufacture the 8086 family.

IBM chose the 8088 over the 8086 because Intel offered a better price for the former and could supply more units. Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses, i.e. existing and mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's), which were already well known by many engineers, further reducing cost.

The descendants of the 8088 include the 80188, 80186, 80286, 80386, 80486, and later software-compatible processors, which are in use today.





</doc>
<doc id="15066" url="https://en.wikipedia.org/wiki?curid=15066" title="Insulator (electricity)">
Insulator (electricity)

An electrical insulator is a material in which the electron does not flow freely or the atom of the insulator have tightly bound electrons whose internal electric charges do not flow freely; very little electric current will flow through it under the influence of an electric field. This contrasts with other materials, semiconductors and conductors, which conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors. The most common examples are non-metals.

A perfect insulator does not exist because even insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass, paper and Teflon, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics which can be thermoset or thermoplastic in nature.

Insulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called "insulation". The term "insulator" is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.

Electrical insulation is the absence of electrical conduction. Electronic band theory (a branch of physics) dictates that a charge flows if states are available into which electrons can be excited. This allows electrons to gain energy and thereby move through a conductor such as a metal. If no such states are available, the material is an insulator.

Most (though not all, see Mott insulator) insulators have a large band gap. This occurs because the "valence" band containing the highest energy electrons is full, and a large energy gap separates this band from the next band above it. There is always some voltage (called the breakdown voltage) that gives electrons enough energy to be excited into this band. Once this voltage is exceeded the material ceases being an insulator, and charge begins to pass through it. However, it is usually accompanied by physical or chemical changes that permanently degrade the material's insulating properties.

Materials that lack electron conduction are insulators if they lack other mobile charges as well. For example, if a liquid or gas contains ions, then the ions can be made to flow as an electric current, and the material is a conductor. Electrolytes and plasmas contain ions and act as conductors whether or not electron flow is involved.

When subjected to a high enough voltage, insulators suffer from the phenomenon of electrical breakdown. When the electric field applied across an insulating substance exceeds in any location the threshold breakdown field for that substance, the insulator suddenly becomes a conductor, causing a large increase in current, an electric arc through the substance. Electrical breakdown occurs when the electric field in the material is strong enough to accelerate free charge carriers (electrons and ions, which are always present at low concentrations) to a high enough velocity to knock electrons from atoms when they strike them, ionizing the atoms. These freed electrons and ions are in turn accelerated and strike other atoms, creating more charge carriers, in a chain reaction. Rapidly the insulator becomes filled with mobile charge carriers, and its resistance drops to a low level. In a solid, the breakdown voltage is proportional to the band gap energy. When corona discharge occurs, the air in a region around a high-voltage conductor can break down and ionise without a catastrophic increase in current. However, if the region of air breakdown extends to another conductor at a different voltage it creates a conductive path between them, and a large current flows through the air, creating an "electric arc". Even a vacuum can suffer a sort of breakdown, but in this case the breakdown or vacuum arc involves charges ejected from the surface of metal electrodes rather than produced by the vacuum itself.

In addition, all insulators become conductors at very high temperatures as the thermal energy of the valence electrons is sufficient to put them in the conduction band.

In certain capacitors, shorts between electrodes formed due to dielectric breakdown can disappear when the applied electric field is reduced.

A very flexible coating of an insulator is often applied to electric wire and cable, this is called "insulated wire". Wires sometimes don't use an insulating coating, just air, since a solid (e.g. plastic) coating may be impractical. However, wires that touch each other produce cross connections, short circuits, and fire hazards. In coaxial cable the center conductor must be supported exactly in the middle of the hollow shield to prevent EM wave reflections. Finally, wires that expose voltages higher than 60 V can cause human shock and electrocution hazards. Insulating coatings help to prevent all of these problems.

Some wires have a mechanical covering with no voltage rating—e.g.: service-drop, welding, doorbell, thermostat wire. An insulated wire or cable has a voltage rating and a maximum conductor temperature rating. It may not have an ampacity (current-carrying capacity) rating, since this is dependent upon the surrounding environment (e.g. ambient temperature).

In electronic systems, printed circuit boards are made from epoxy plastic and fibreglass. The nonconductive boards support layers of copper foil conductors. In electronic devices, the tiny and delicate active components are embedded within nonconductive epoxy or phenolic plastics, or within baked glass or ceramic coatings.

In microelectronic components such as transistors and ICs, the silicon material is normally a conductor because of doping, but it can easily be selectively transformed into a good insulator by the application of heat and oxygen. Oxidised silicon is quartz, i.e. silicon dioxide, the primary component of glass.

In high voltage systems containing transformers and capacitors, liquid insulator oil is the typical method used for preventing arcs. The oil replaces air in spaces that must support significant voltage without electrical breakdown. Other high voltage system insulation materials include ceramic or glass wire holders, gas, vacuum, and simply placing wires far enough apart to use air as insulation.

Overhead conductors for high-voltage electric power transmission are bare, and are insulated by the surrounding air. Conductors for lower voltages in distribution may have some insulation but are often bare as well. Insulating supports called "insulators" are required at the points where they are supported by utility poles or transmission towers. Insulators are also required where the wire enters buildings or electrical devices, such as transformers or circuit breakers, to insulate the wire from the case. These hollow insulators with a conductor inside them are called bushings.

Insulators used for high-voltage power transmission are made from glass, porcelain or composite polymer materials. Porcelain insulators are made from clay, quartz or alumina and feldspar, and are covered with a smooth glaze to shed water. Insulators made from porcelain rich in alumina are used where high mechanical strength is a criterion. Porcelain has a dielectric strength of about 4–10 kV/mm. Glass has a higher dielectric strength, but it attracts condensation and the thick irregular shapes needed for insulators are difficult to cast without internal strains. Some insulator manufacturers stopped making glass insulators in the late 1960s, switching to ceramic materials.

Recently, some electric utilities have begun converting to polymer composite materials for some types of insulators. These are typically composed of a central rod made of fibre reinforced plastic and an outer weathershed made of silicone rubber or ethylene propylene diene monomer rubber (EPDM). Composite insulators are less costly, lighter in weight, and have excellent hydrophobic capability. This combination makes them ideal for service in polluted areas. However, these materials do not yet have the long-term proven service life of glass and porcelain.

The electrical breakdown of an insulator due to excessive voltage can occur in one of two ways:
Most high voltage insulators are designed with a lower flashover voltage than puncture voltage, so they flash over before they puncture, to avoid damage.

Dirt, pollution, salt, and particularly water on the surface of a high voltage insulator can create a conductive path across it, causing leakage currents and flashovers. The flashover voltage can be reduced by more than 50% when the insulator is wet. High voltage insulators for outdoor use are shaped to maximise the length of the leakage path along the surface from one end to the other, called the creepage length, to minimise these leakage currents. To accomplish this the surface is moulded into a series of corrugations or concentric disc shapes. These usually include one or more "sheds"; downward facing cup-shaped surfaces that act as umbrellas to ensure that the part of the surface leakage path under the 'cup' stays dry in wet weather. Minimum creepage distances are 20–25 mm/kV, but must be increased in high pollution or airborne sea-salt areas.

These are the common classes of insulators:

Pin-type insulators are unsuitable for voltages greater than about 69 kV line-to-line. Higher transmission voltages use suspension insulator strings, which can be made for any practical transmission voltage by adding insulator elements to the string.

Higher voltage transmission lines usually use modular suspension insulator designs. The wires are suspended from a 'string' of identical disc-shaped insulators that attach to each other with metal clevis pin or ball and socket links. The advantage of this design is that insulator strings with different breakdown voltages, for use with different line voltages, can be constructed by using different numbers of the basic units. Also, if one of the insulator units in the string breaks, it can be replaced without discarding the entire string.

Each unit is constructed of a ceramic or glass disc with a metal cap and pin cemented to opposite sides. To make defective units obvious, glass units are designed so that an overvoltage causes a puncture arc through the glass instead of a flashover. The glass is heat-treated so it shatters, making the damaged unit visible. However the mechanical strength of the unit is unchanged, so the insulator string stays together.

Standard suspension disc insulator units are in diameter and long, can support a load of 80-120 kN (18-27 klbf), have a dry flashover voltage of about 72 kV, and are rated at an operating voltage of 10-12 kV. However, the flashover voltage of a string is less than the sum of its component discs, because the electric field is not distributed evenly across the string but is strongest at the disc nearest to the conductor, which flashes over first. Metal "grading rings" are sometimes added around the disc at the high voltage end, to reduce the electric field across that disc and improve flashover voltage.

In very high voltage lines the insulator may be surrounded by corona rings. These typically consist of toruses of aluminium (most commonly) or copper tubing attached to the line. They are designed to reduce the electric field at the point where the insulator is attached to the line, to prevent corona discharge, which results in power losses.

The first electrical systems to make use of insulators were telegraph lines; direct attachment of wires to wooden poles was found to give very poor results, especially during damp weather.

The first glass insulators used in large quantities had an unthreaded pinhole. These pieces of glass were positioned on a tapered wooden pin, vertically extending upwards from the pole's crossarm (commonly only two insulators to a pole and maybe one on top of the pole itself). Natural contraction and expansion of the wires tied to these "threadless insulators" resulted in insulators unseating from their pins, requiring manual reseating.

Amongst the first to produce ceramic insulators were companies in the United Kingdom, with Stiff and Doulton using stoneware from the mid-1840s, Joseph Bourne (later renamed Denby) producing them from around 1860 and Bullers from 1868. Utility patent number 48,906 was granted to Louis A. Cauvet on 25 July 1865 for a process to produce insulators with a threaded pinhole: pin-type insulators still have threaded pinholes.

The invention of suspension-type insulators made high-voltage power transmission possible. As transmission line voltages reached and passed 60,000 volts, the insulators required become very large and heavy, with insulators made for a safety margin of 88,000 volts being about the practical limit for manufacturing and installation. Suspension insulators, on the other hand, can be connected into strings as long as required for the line's voltage.

A large variety of telephone, telegraph and power insulators have been made; some people collect them, both for their historic interest and for the aesthetic quality of many insulator designs and finishes. One collectors organisation is the US National Insulator Association, which has over 9,000 members.

Often a broadcasting radio antenna is built as a mast radiator, which means that the entire mast structure is energised with high voltage and must be insulated from the ground. Steatite mountings are used. They have to withstand not only the voltage of the mast radiator to ground, which can reach values up to 400 kV at some antennas, but also the weight of the mast construction and dynamic forces. Arcing horns and lightning arresters are necessary because lightning strikes to the mast are common.

Guy wires supporting antenna masts usually have strain insulators inserted in the cable run, to keep the high voltages on the antenna from short circuiting to ground or creating a shock hazard. Often guy cables have several insulators, placed to break up the cable into lengths that prevent unwanted electrical resonances in the guy. These insulators are usually ceramic and cylindrical or egg-shaped (see picture). This construction has the advantage that the ceramic is under compression rather than tension, so it can withstand greater load, and that if the insulator breaks, the cable ends are still linked.

These insulators also have to be equipped with overvoltage protection equipment. For the dimensions of the guy insulation, static charges on guys have to be considered. For high masts, these can be much higher than the voltage caused by the transmitter, requiring guys divided by insulators in multiple sections on the highest masts. In this case, guys which are grounded at the anchor basements via a coil - or if possible, directly - are the better choice.

Feedlines attaching antennas to radio equipment, particularly twin lead type, often must be kept at a distance from metal structures. The insulated supports used for this purpose are called "standoff insulators".

The most important insulation material is air. A variety of solid, liquid, and gaseous insulators are also used in electrical apparatus. In smaller transformers, generators, and electric motors, insulation on the wire coils consists of up to four thin layers of polymer varnish film. Film insulated magnet wire permits a manufacturer to obtain the maximum number of turns within the available space. Windings that use thicker conductors are often wrapped with supplemental fiberglass insulating tape. Windings may also be impregnated with insulating varnishes to prevent electrical corona and reduce magnetically induced wire vibration. Large power transformer windings are still mostly insulated with paper, wood, varnish, and mineral oil; although these materials have been used for more than 100 years, they still provide a good balance of economy and adequate performance. Busbars and circuit breakers in switchgear may be insulated with glass-reinforced plastic insulation, treated to have low flame spread and to prevent tracking of current across the material.

In older apparatus made up to the early 1970s, boards made of compressed asbestos may be found; while this is an adequate insulator at power frequencies, handling or repairs to asbestos material can release dangerous fibers into the air and must be carried cautiously. Wire insulated with felted asbestos was used in high-temperature and rugged applications from the 1920s. Wire of this type was sold by General Electric under the trade name "Deltabeston."

Live-front switchboards up to the early part of the 20th century were made of slate or marble. Some high voltage equipment is designed to operate within a high pressure insulating gas such as sulfur hexafluoride. Insulation materials that perform well at power and low frequencies may be unsatisfactory at radio frequency, due to heating from excessive dielectric dissipation.

Electrical wires may be insulated with polyethylene, crosslinked polyethylene (either through electron beam processing or chemical crosslinking), PVC, Kapton, rubber-like polymers, oil impregnated paper, Teflon, silicone, or modified ethylene tetrafluoroethylene (ETFE). Larger power cables may use compressed inorganic powder, depending on the application.

Flexible insulating materials such as PVC (polyvinyl chloride) are used to insulate the circuit and prevent human contact with a 'live' wire – one having voltage of 600 volts or less. Alternative materials are likely to become increasingly used due to EU safety and environmental legislation making PVC less economic.

All portable or hand-held electrical devices are insulated to protect their user from harmful shock.

Class I insulation requires that the metal body and other exposed metal parts of the device be connected to earth via a "grounding wire" that is earthed at the main service panel—but only needs basic insulation on the conductors. This equipment needs an extra pin on the power plug for the grounding connection.

Class II insulation means that the device is "double insulated". This is used on some appliances such as electric shavers, hair dryers and portable power tools. Double insulation requires that the devices have both basic and supplementary insulation, each of which is sufficient to prevent electric shock. All internal electrically energized components are totally enclosed within an insulated body that prevents any contact with "live" parts. In the EU, double insulated appliances all are marked with a symbol of two squares, one inside the other.



</doc>
<doc id="15067" url="https://en.wikipedia.org/wiki?curid=15067" title="Internetworking">
Internetworking

Internetworking is the practice of interconnecting multiple computer networks, such that any pair of hosts in the connected networks can exchange messages irrespective of their hardware-level networking technology. The resulting system of interconnected networks are called an "internetwork", or simply an "internet".

The most notable example of internetworking is the Internet, a network of networks based on many underlying hardware technologies. The Internet is defined by a unified global addressing system, packet format, and routing methods provided by the Internet Protocol.

The term "internetworking" is a combination of the components "inter" ("between") and "networking". An earlier term for an internetwork is "catenet", a short-form of "(con)catenating networks".

Internetworking started as a way to connect disparate types of networking technology, but it became widespread through the developing need to connect two or more local area networks via some sort of wide area network.

The first two interconnected networks were the ARPANET and the NPL network via Peter Kirstein's group at University College London. The network elements used to connect individual networks in the ARPANET, the predecessor of the Internet, were originally called gateways, but the term has been deprecated in this context, because of possible confusion with functionally different devices. Research at NPL confirmed establishing a common host protocol would be more reliable and efficient. By 1973-4, researchers in the United States, the United Kingdom and France had worked out an approach to internetworking where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible, as demonstrated in the CYCLADES network. 

Today the interconnecting gateways are called routers. The definition of an internetwork today includes the connection of other types of computer networks such as personal area networks. 

To build an internetwork, the following are needed: A standardized scheme to address packets to any host on any participating network; a standardized protocol defining format and handling of transmitted packets; components interconnecting the participating networks by routing packets to their destinations based on standardized addresses. 

Another type of interconnection of networks often occurs within enterprises at the Link Layer of the networking model, i.e. at the hardware-centric layer below the level of the TCP/IP logical interfaces. Such interconnection is accomplished with network bridges and network switches. This is sometimes incorrectly termed internetworking, but the resulting system is simply a larger, single subnetwork, and no internetworking protocol, such as Internet Protocol, is required to traverse these devices. However, a single computer network may be converted into an internetwork by dividing the network into segments and logically dividing the segment traffic with routers and having an internetworking software layer that applications employ.

The Internet Protocol is designed to provide an unreliable (not guaranteed) packet service across the network. The architecture avoids intermediate network elements maintaining any state of the network. Instead, this function is assigned to the endpoints of each communication session. To transfer data reliably, applications must utilize an appropriate Transport Layer protocol, such as Transmission Control Protocol (TCP), which provides a reliable stream. Some applications use a simpler, connection-less transport protocol, User Datagram Protocol (UDP), for tasks which do not require reliable delivery of data or that require real-time service, such as video streaming or voice chat.

Catenet is an obsolete term for a system of packet-switched communication networks interconnected via gateways.

The term was coined by Louis Pouzin in October 1973 in a note circulated to the International Networking Working Group, later published in a 1974 paper ""A Proposal for Interconnecting Packet Switching Networks"". Pouzin was a pioneer in packet-switching technology and founder of the CYCLADES network, at a time when "network" meant what is now called a local area network. Catenet was the concept of linking these networks into a "network of networks" with specifications for compatibility of addressing and routing. The term catenet was gradually displaced by the short-form of the term internetwork, "internet" (lower-case "i"), when the Internet Protocol replaced earlier protocols on the ARPANET.

Two architectural models are commonly used to describe the protocols and methods used in internetworking. The Open System Interconnection (OSI) reference model was developed under the auspices of the International Organization for Standardization (ISO) and provides a rigorous description for layering protocol functions from the underlying hardware to the software interface concepts in user applications. Internetworking is implemented in the Network Layer (Layer 3) of the model.

The Internet Protocol Suite, also known as the TCP/IP model, was not designed to conform to the OSI model and does not refer to it in any of the normative specifications in Requests for Comment and Internet standards. Despite similar appearance as a layered model, it has a much less rigorous, loosely defined architecture that concerns itself only with the aspects of the style of networking in its own historical provenance. It assumes the availability of any suitable hardware infrastructure, without discussing hardware-specific low-level interfaces, and that a host has access to this local network to which it is connected via a Link Layer interface.

For a period in the late 1980s and early 1990s, the network engineering community was polarized over the implementation of competing protocol suites, commonly known as the Protocol Wars. It was unclear which of the OSI model and the Internet protocol suite would result in the best and most robust computer networks.



</doc>
<doc id="15068" url="https://en.wikipedia.org/wiki?curid=15068" title="Infantry">
Infantry

Infantry is a military specialization that engages in military combat on foot, distinguished from cavalry, artillery, and armored forces. Also known as foot soldiers or infantrymen, infantry traditionally relies on moving by foot between combats as well, but may also use mounts, military vehicles, or other transport. Infantry make up a large portion of all armed forces in most nations, and typically bear the largest brunt in warfare, as measured by casualties, deprivation, or physical and psychological stress.

The first military forces in history were infantry. In antiquity, infantry were armed with an early melee weapon such as a spear, axe or sword, or an early ranged weapon like a javelin, sling, or bow, with a few infantrymen having both a melee and a ranged weapon. With the development of gunpowder, infantry began converting to primarily firearms. By the time of Napoleonic warfare, infantry, cavalry, and artillery formed a basic triad of ground forces, though infantry usually remained the most numerous. With armoured warfare, armoured fighting vehicles have replaced the horses of cavalry, and airpower has added a new dimension to ground combat, but infantry remains pivotal to all modern combined arms operations.

Infantry have much greater local situational awareness than other military forces, due to their inherent intimate contact with the battlefield ("boots on the ground"); this is vital for engaging and infiltrating enemy positions, holding and defending ground (any military objectives), securing battlefield victories, maintaining military area control and security both at and behind the front lines, for capturing ordnance or materiel, taking prisoners, and military occupation. Infantry can more easily recognise, adapt and respond to local conditions, weather, and changing enemy weapons or tactics. They can operate in a wide range of terrain inaccessible to military vehicles, and can operate with a lower logistical burden. Infantry are the most easily delivered forces to ground combat areas, by simple and reliable marching, or by trucks, sea or air transport; they can also be inserted directly into combat by amphibious landing, or for air assault by parachute (airborne infantry) or helicopter (airmobile infantry). They can be augmented with a variety of crew-served weapons, armoured personnel carriers, and infantry fighting vehicles.

In English, use of the term "infantry" began about the 1570s, describing soldiers who march and fight on foot. The word derives from Middle French "infanterie", from older Italian (also Spanish) "infanteria" (foot soldiers too inexperienced for cavalry), from Latin "īnfāns" (without speech, newborn, foolish), from which English also gets "infant". The individual-soldier term "infantryman" was not coined until 1837. In modern usage, foot soldiers of any era are now considered infantry and infantrymen.

From the mid-18th century until 1881 the British Army named its infantry as numbered regiments "of Foot" to distinguish them from cavalry and dragoon regiments (see List of Regiments of Foot).

Infantry equipped with special weapons were often named after that weapon, such as grenadiers for their grenades, or fusiliers for their "fusils". These names can persist long after the weapon speciality; examples of infantry units that retained such names are the Royal Irish Fusiliers and the Grenadier Guards.

More commonly in modern times, infantry with special tactics are named for their roles, such as commandos, rangers, snipers, marines, (who all have additional training) and militia (who have limited training); they are still infantry due to their expectation to fight as infantry when they enter combat.

Dragoons were created as mounted infantry, with horses for travel between battles; they were still considered infantry since they dismounted before combat. However, if light cavalry was lacking in an army, any available dragoons might be assigned their duties; this practise increased over time, and dragoons eventually received all the weapons and training as both infantry and cavalry, and could be classified as both. Conversely, starting about the mid-19th century, regular cavalry have been forced to spend more of their time dismounted in combat due to the ever-increasing effectiveness of enemy infantry firearms. Thus most cavalry transitioned to mounted infantry. As with grenadiers, the "dragoon" and "cavalry" designations can be retained long after their horses, such as in the Royal Dragoon Guards, Royal Lancers, and King's Royal Hussars.

Similarly, motorised infantry have trucks and other unarmed vehicles for non-combat movement, but are still infantry since they leave their vehicles for any combat. Most modern infantry have vehicle transport, to the point where infantry being motorised is generally assumed, and the few exceptions might be identified as modern "light infantry", or "leg infantry" colloquially. Mechanised infantry go beyond motorised, having transport vehicles with combat abilities, armoured personnel carriers (APCs), providing at least some options for combat without leaving their vehicles. In modern infantry, some APCs have evolved to be infantry fighting vehicles (IFVs), which are transport vehicles with more substantial combat abilities, approaching those of light tanks. Some well-equipped mechanised infantry can be designated as "armoured infantry". Given that infantry forces typically also have some tanks, and given that most armoured forces have more mechanised infantry units than tank units in their organisation, the distinction between mechanised infantry and armour forces has blurred.

The terms "infantry", "armour", and "cavalry" used in the official names for military units like divisions, brigades, or regiments might be better understood as a description of their expected balance of defensive, offensive, and mobility roles, rather than just use of vehicles. Some modern mechanised infantry units are termed "cavalry" or "armoured cavalry", even though they never had horses, to emphasise their combat mobility.

In the modern US Army, about 15% of soldiers are officially "Infantry". The basic training for all new US Army soldiers includes use of infantry weapons and tactics, even for tank crews, artillery crews, and base and logistical personnel.

The first warriors, adopting hunting weapons or improvised melee weapons, before the existence of any organised military, likely started essentially as loose groups without any organisation or formation. But this changed sometime before recorded history; the first ancient empires (2500–1500 BC) are shown to have some soldiers with standardised military equipment, and the training and discipline required for battlefield formations and manoeuvres: regular infantry. Though the main force of the army, these forces were usually kept small due to their cost of training and upkeep, and might be supplemented by local short-term mass-conscript forces using the older irregular infantry weapons and tactics; this remained a common practice almost up to modern times.

Before the adoption of the chariot to create the first mobile fighting forces , all armies were pure infantry. Even after, with a few exceptions like the Mongol Empire, infantry has been the largest component of most armies in history.

In the Western world, from Classical Antiquity through the Middle Ages ( 8th century BC to 15th century AD), infantry are categorised as either heavy infantry or light infantry. Heavy infantry, such as Greek hoplites, Macedonian phalangites, and Roman legionaries, specialised in dense, solid formations driving into the main enemy lines, using weight of numbers to achieve a decisive victory, and were usually equipped with heavier weapons and armour to fit their role. Light infantry, such as Greek peltasts, Balearic slingers, and Roman velites, using open formations and greater manoeuvrability, took on most other combat roles: scouting, screening the army on the march, skirmishing to delay, disrupt, or weaken the enemy to prepare for the main forces' battlefield attack, protecting them from flanking manoeuvers, and then afterwards either pursuing the fleeing enemy or covering their army's retreat.

After the fall of Rome, the quality of heavy infantry declined, and warfare was dominated by heavy cavalry, such as knights, forming small elite units for decisive shock combat, supported by peasant infantry militias and assorted light infantry from the lower classes. Towards the end of Middle Ages, this began to change, where more professional and better trained light infantry could be effective against knights, such as the English longbowmen in the Hundred Years' War. By the start of the Renaissance, the infantry began to return to dominance, with Swiss pikemen and German Landsknechts filling the role of heavy infantry again, using dense formations of pikes to drive off any cavalry.

Dense formations are vulnerable to ranged weapons. Technological developments allowed the raising of large numbers of light infantry units armed with ranged weapons, without the years of training expected for traditional high-skilled archers and slingers. This started slowly, first with crossbowmen, then hand cannoneers and arquebusiers, each with increasing effectiveness, marking the beginning of early modern warfare, when firearms rendered the use of heavy infantry obsolete. The introduction of musketeers using bayonets in the mid 17th century began replacement of the pike with the infantry square replacing the pike square.

To maximise their firepower, musketeer infantry were trained to fight in wide lines facing the enemy, creating line infantry. These fulfilled the central battlefield role of earlier heavy infantry, using ranged weapons instead of melee weapons. To support these lines, smaller infantry formations using dispersed skirmish lines were created, called light infantry, fulfilling the same multiple roles as earlier light infantry. Their arms were no lighter than line infantry; they were distinguished by their skirmish formation and flexible tactics.

The modern rifleman infantry became the primary force for taking and holding ground on battlefields worldwide, a vital element of combined arms combat. As firepower continued to increase, use of infantry lines diminished, until all infantry became light infantry in practice.

Modern classifications of infantry have expanded to reflect modern equipment and tactics, such as motorised infantry, mechanised or armoured infantry, mountain infantry, marine infantry, and airborne infantry.

An infantryman's equipment is of vital concern both for the man and the military. The needs of the infantryman to maintain fitness and effectiveness must be constantly balanced against being overburdened. While soldiers in other military branches can use their mount or vehicle for carrying equipment, and tend to operate together as crews serving their vehicle or ordnance, infantrymen must operate more independently; each infantryman usually having much more personal equipment to use and carry. This encourages searching for ingenious combinations of effective, rugged, serviceable and adaptable, yet light, compact, and handy infantry equipment.

Beyond their main arms and armour, each infantryman's "military kit" includes combat boots, battledress or combat uniform, camping gear, heavy weather gear, survival gear, secondary weapons and ammunition, weapon service and repair kits, health and hygiene items, mess kit, rations, filled water canteen, and all other consumables each infantryman needs for the expected duration of time operating away from their unit's base, plus any special mission-specific equipment. One of the most valuable pieces of gear is the entrenching tool—basically a folding spade—which can be employed not only to dig important defences, but also in a variety of other daily tasks, and even sometimes as a weapon. Infantry typically have shared equipment on top of this, like tents or heavy weapons, where the carrying burden is spread across several infantrymen. In all, this can reach for each soldier on the march. Such heavy infantry burdens have changed little over centuries of warfare; in the late Roman Republic, legionaries were nicknamed "Marius' mules" as their main activity seemed to be carrying the weight of their legion around on their backs.

When combat is expected, infantry typically switch to "packing light", meaning reducing their equipment to weapons, ammo, and bare essentials, and leaving the rest with their transport or baggage train, at camp or rally point, in temporary hidden caches, or even (in emergencies) discarding whatever may slow them down. Additional specialised equipment may be required, depending on the mission or to the particular terrain or environment, including satchel charges, demolition tools, mines, barbed wire, carried by the infantry or attached specialists.

Historically, infantry have suffered high casualty rates from disease, exposure, exhaustion and privation — often in excess of the casualties suffered from enemy attacks. Better infantry equipment to support their health, energy, and protect from environmental factors greatly reduces these rates of loss, and increase their level of effective action. Health, energy, and morale are greatly influenced by how the soldier is fed, so militaries often standardised field rations, starting from hardtack, to US K-rations, to modern MREs.

Communications gear has become a necessity, as it allows effective command of infantry units over greater distances, and communication with artillery and other support units. Modern infantry can have GPS, encrypted individual communications equipment, surveillance and night vision equipment, advanced intelligence and other high-tech mission-unique aids.

Armies have sought to improve and standardise infantry gear to reduce fatigue for extended carrying, increase freedom of movement, accessibility, and compatibility with other carried gear, such as the US All-purpose Lightweight Individual Carrying Equipment (ALICE).

Infantrymen are defined by their primary arms – the personal weapons and body armour for their own individual use. The available technology, resources, history, and society can produce quite different weapons for each military and era, but common infantry weapons can be distinguished in a few basic categories.


Infantrymen often carry secondary or back-up weapons, sometimes called a sidearm or ancillary weapons in modern terminology, either issued officially as an addition to the soldier's standard arms, or acquired unofficially by any other means as an individual preference. Such weapons are used when the primary weapon is no longer effective, such it becoming damaged, running out of ammunition, malfunction, or in a change of tactical situation where another weapon is preferred, such as going from ranged to close combat. Infantry with ranged or pole weapons often carried a sword or dagger for possible hand-to-hand combat. The "pilum" was a javelin of the Roman legionaries threw just before drawing their primary weapon, the "gladius" (short sword), and closing with the enemy line.

Modern infantrymen now treat the bayonet as a backup weapon, but may also have handguns or pistols. They may also deploy anti-personnel mines, booby traps, incendiary or explosive devices defensively before combat.

Some non-weapon equipment are designed for close combat shock effects, to get and psychological edge before melee, such as battle flags, war drums, brilliant uniforms, fierce body paint or tattoos, and even battle cries. These have become mostly only ceremonial since the decline of close combat military tactics.

Infantry have employed many different methods of protection from enemy attacks, including various kinds of armour and other gear, and tactical procedures.

The most basic is personal armour. This includes shields, helmets and many types of armour – padded linen, leather, lamellar, mail, plate, and kevlar. Initially, armour was used to defend both from ranged and close combat; even a fairly light shield could help defend against most slings and javelins, though high-strength bows and crossbows might penetrate common armour at very close range. Infantry armour had to compromise between protection and coverage, as a full suit of attack-proof armour would be too heavy to wear in combat.

As firearms improved, armour for ranged defence had to be thicker and stronger. With the introduction of the heavy arquebus designed to pierce standard steel armour, it was proven easier to make heavier firearms than heavier armour; armour transitioned to be only for close combat purposes. Pikemen armour tended to be just steel helmets and breastplates, and gunners little or no armour. By the time of the musket, the dominance of firepower shifted militaries away from any close combat, and use of armour decreased, until infantry typically went without any armour.

Helmets were added back during World War I as artillery began to dominate the battlefield, to protect against their fragmentation and other blast effects beyond a direct hit. Modern developments in bullet-proof composite materials like kevlar have started a return to body armour for infantry, though the extra weight is a notable burden.

In modern times, infantrymen must also often carry protective measures against chemical and biological attack, including military gas masks, counter-agents, and protective suits. All of these protective measures add to the weight an infantryman must carry, and may decrease combat efficiency. Modern militaries are struggling to balance the value of personal body protection versus the weight burden and ability to function under such weight.

Early crew-served weapons were siege weapons, like the ballista, trebuchet, and battering ram. Modern versions include machine guns, anti-tank missiles, and infantry mortars.

Beginning with the development the first regular military forces, close-combat regular infantry fought less as unorganised groups of individuals and more in coordinated units, maintaining a defined tactical formation during combat, for increased battlefield effectiveness; such infantry formations and the arms they used developed together, starting with the spear and the shield.

A spear has decent attack abilities with the additional advantage keeping opponents at distance; this advantage can be increased by using longer spears, but this could allow the opponent to side-step the point of the spear and close for hand-to-hand combat where the longer spear is near useless. This can be avoided when each spearman stays side by side with the others in close formation, each covering the ones next to him, presenting a solid wall of spears to the enemy that they cannot get around.

Similarly, a shield has decent defence abilities, but is literally hit-or-miss; an attack from an unexpected angle can bypass it completely. Larger shields can cover more, but are also heavier and less manoeuvrable, making unexpected attacks even more of a problem. This can be avoided by having shield-armed soldiers stand close together, side-by-side, each protecting both themselves and their immediate comrades, presenting a solid shield wall to the enemy.

The opponents for these first formations, the close-combat infantry of more tribal societies, or any military without regular infantry (so called "barbarians") used arms that focused on the individual – weapons using personal strength and force, such as larger swinging swords, axes, and clubs. These take more room and individual freedom to swing and wield, necessitating a more loose organisation. While this may allow for a fierce running attack (an initial shock advantage) the tighter formation of the heavy spear and shield infantry gave them a local manpower advantage where several might be able to fight each opponent.

Thus tight formations heightened advantages of heavy arms, and gave greater local numbers in melee. To also increase their staying power, multiple rows of heavy infantrymen were added. This also increased their shock combat effect; individual opponents saw themselves literally lined-up against several heavy infantryman each, with seemingly no chance of defeating all of them. "Heavy infantry" developed into huge solid block formations, up to a hundred meters wide and a dozen rows deep.

Maintaining the advantages of heavy infantry meant maintaining formation; this became even more important when two forces with heavy infantry met in battle; the solidity of the formation became the deciding factor. Intense discipline and training became paramount. Empires formed around their military.

The organization of military forces into regular military units is first noted in Egyptian records of the Battle of Kadesh (). Soldiers were grouped into units of 50, which were in turn grouped into larger units of 250, then 1,000, and finally into units of up to 5,000 – the largest independent command. Several of these Egyptian "divisions" made up an army, but operated independently, both on the march and tactically, demonstrating sufficient military command and control organisation for basic battlefield manoeuvres. Similar hierarchical organizations have been noted in other ancient armies, typically with approximately 10 to 100 to 1,000 ratios (even where base 10 was not common), similar to modern sections (squads), companies, and regiments.

The training of the infantry has differed drastically over time and from place to place. The cost of maintaining an army in fighting order and the seasonal nature of warfare precluded large permanent armies.

The antiquity saw everything from the well-trained and motivated citizen armies of Greek and Rome, the tribal host assembled from farmers and hunters with only passing acquaintance with warfare and masses of lightly armed and ill-trained militia put up as a last ditch effort.

In medieval times the foot soldiers varied from peasant levies to semi-permanent companies of mercenaries, foremost among them the Swiss, English, Aragonese and German, to men-at-arms who went into battle as well-armoured as knights, the latter of which at times also fought on foot.

The creation of standing armies—permanently assembled for war or defence—saw increase in training and experience. The increased use of firearms and the need for drill to handle them efficiently.

The introduction of national and mass armies saw an establishment of minimum requirements and the introduction of special troops (first of them the engineers going back to medieval times, but also different kinds of infantry adopted to specific terrain, bicycle, motorcycle, motorised and mechanised troops) culminating with the introduction of highly trained special forces during the first and second World War.

Attack operations are the most basic role of the infantry, and along with defence, form the main stances of the infantry on the battlefield. Traditionally, in an open battle, or meeting engagement, two armies would manoeuvre to contact, at which point they would form up their infantry and other units opposite each other. Then one or both would advance and attempt to defeat the enemy force. The goal of an attack remains the same: to advance into an enemy-held "objective," most frequently a hill, river crossing, city or other dominant terrain feature, and dislodge the enemy, thereby establishing control of the objective.

Attacks are often feared by the infantry conducting them because of the high number of casualties suffered while advancing to close with and destroy the enemy while under enemy fire. In mechanised infantry the armoured personnel carrier (APC) is considered the assaulting position. These APCs can deliver infantrymen through the front lines to the battle and—in the case of infantry fighting vehicles—contribute supporting firepower to engage the enemy. Successful attacks rely on sufficient force, preparative reconnaissance and battlefield preparation with bomb assets. Retention of discipline and cohesion throughout the attack is paramount to success. A subcategory of attacks is the ambush, where infantrymen lie in wait for enemy forces before attacking at a vulnerable moment. This gives the ambushing infantrymen the combat advantage of surprise, concealment and superior firing positions, and causes confusion. The ambushed unit does not know what it is up against, or where they are attacking from.

Patrolling is the most common infantry mission. Full-scale attacks and defensive efforts are occasional, but patrols are constant. Patrols consist of small groups of infantry moving about in areas of possible enemy activity to locate the enemy and destroy them when found. Patrols are used not only on the front-lines, but in rear areas where enemy infiltration or insurgencies are possible.

Pursuit is a role that the infantry often assumes. The objective of pursuit operations is the destruction of withdrawing enemy forces which are not capable of effectively engaging friendly units, before they can build their strength to the point where they are effective. Infantry traditionally have been the main force to overrun these units in the past, and in modern combat are used to pursue enemy forces in constricted terrain (urban areas in particular), where faster forces, such as armoured vehicles are incapable of going or would be exposed to ambush.

Defence operations are the natural counter to attacks, in which the mission is to hold an objective and defeat enemy forces attempting to dislodge the defender. Defensive posture offers many advantages to the infantry, including the ability to use terrain and constructed fortifications to advantage; these reduce exposure to enemy fire compared with advancing forces. Effective defence relies on minimising losses to enemy fire, breaking the enemy's cohesion before their advance is completed, and preventing enemy penetration of defensive positions.

Escorting consists of protecting support units from ambush, particularly from hostile infantry forces. Combat support units (a majority of the military) are not as well armed or trained as infantry units and have a different mission. Therefore, they need the protection of the infantry, particularly when on the move. This is one of the most important roles for the modern infantry, particularly when operating alongside armoured vehicles. In this capacity, infantry essentially conducts patrol on the move, scouring terrain which may hide enemy infantry waiting to ambush friendly vehicles, and identifying enemy strong points for attack by the heavier units.

Infantry units are tasked to protect certain areas like command posts or airbases. Units assigned to this job usually have a large number of military police attached to them for control of checkpoints and prisons.

Maneouvering consumes much of an infantry unit's time. Infantry, like all combat arms units, are often manoeuvred to meet battlefield needs, and often must do so under enemy attack. The infantry must maintain their cohesion and readiness during the move to ensure their usefulness when they reach their objective. Traditionally, infantry have relied on their own legs for mobility, but mechanised or armoured infantry often uses trucks and armoured vehicles for transport. These units can quickly disembark and transition to light infantry, without vehicles, to access terrain which armoured vehicles can't effectively access.

Surveillance operations are often carried out with the employment of small recon units or sniper teams which gather information about the enemy, reporting on characteristics such as size, activity, location, unit and equipment. These infantry units typically are known for their stealth and ability to operate for periods of time within close proximity of the enemy without being detected. They may engage high-profile targets, or be employed to hunt down terrorist cells and insurgents within a given area. These units may also entice the enemy to engage a located recon unit, thus disclosing their location to be destroyed by more powerful friendly forces.

Some assignments for infantry units involve deployment behind the front, although patrol and security operations are usually maintained in case of enemy infiltration. This is usually the best time for infantry units to integrate replacements into units and to maintain equipment. Additionally, soldiers can be rested and general readiness should improve. However, the unit must be ready for deployment at any point.

This can be undertaken either in reserve or on the front, but consists of using infantry troops as labor for construction of field positions, roads, bridges, airfields, and all other manner of structures. The infantry is often given this assignment because of the physical quantity of strong men within the unit, although it can lessen a unit's morale and limit the unit's ability to maintain readiness and perform other missions. More often, such jobs are given to specialist engineering corps.

Infantry units are trained to quickly mobilise, infiltrate, enter and neutralise threat forces when appropriate combat intelligence indicates to secure a location, rescue or capture high-profile targets.

Urban combat poses unique challenges to the combat forces. It is one of the most complicated type of operations an infantry unit will undertake. With many places for the enemy to hide and ambush from, infantry units must be trained in how to enter a city, and systematically clear the buildings, which most likely will be booby trapped, in order to kill or capture enemy personnel within the city. Care must be taken to differentiate innocent civilians who often hide and support the enemy from the non-uniformed armed enemy forces. Civilian and military casualties both are usually very high.

Because of an infantryman's duties with firearms, explosives, physical and emotional stress, and physical violence, casualties and deaths are not uncommon in both war and in peacetime training or operations. It is a highly dangerous and demanding combat service; in World War II, military doctors concluded that even physically unwounded soldiers were psychologically worn out after about 200 days of combat.

The physical, mental, and environmental operating demands of the infantryman are high. All of the combat necessities such as ammunition, weapon systems, food, water, clothing, and shelter are carried on the backs of the infantrymen, at least in light role as opposed to mounted/mechanised. Combat loads of over 36 kg (80 lbs) are standard, and greater loads in excess of 45 kg (100 lbs) are very common. These heavy loads, combined with long foot patrols of over a day, in any climate from in temperature, require the infantryman to be in good physical and mental condition. Infantrymen live, fight and die outdoors in all types of brutal climates, often with no physical shelter. Poor climate conditions adds misery to this already demanding existence. Disease epidemics, frostbite, heat stroke, trench foot, insect and wild animal bites are common along with stress disorders and these have sometimes caused more casualties than enemy action.

Infantrymen are expected to continue with their combat missions despite the death and injury of friends, fear, despair, fatigue, and bodily injury.

Some infantry units are considered Special Forces. The earliest Special Forces commando units were more highly trained infantrymen, with special weapons, equipment, and missions. Special Forces units recruit heavily from regular infantry units to fill their ranks.

Foreign and domestic militaries typically have a slang term for their infantrymen. In the U.S. military, the slang term among both Marine and Army infantrymen for themselves is "grunt." In the British Army, they are the "squaddies." The infantry is a small close-knit community, and the slang names are terms of endearment that convey mutual respect and shared experiences.

Naval infantry, commonly known as marines, are primarily a category of infantry that form part of the naval forces of states and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.

Air force infantry and base defence forces, such as the Royal Air Force Regiment, United States Air Force Security Forces, Royal Australian Air Force Airfield Defence Guards, and Indonesian Air Force Paskhas Corps, are used primarily for ground-based defence of air bases and other air force facilities. They also have a number of other, specialist roles. These include, among others, Chemical, Biological, Radiological and Nuclear (CBRN) defence and training other airmen in basic ground defence tactics.



</doc>
<doc id="15069" url="https://en.wikipedia.org/wiki?curid=15069" title="Identity function">
Identity function

In mathematics, an identity function, also called an identity relation or identity map or identity transformation, is a function that always returns the same value that was used as its argument. That is, for being identity, the equality holds for all .

Formally, if is a set, the identity function on is defined to be that function with domain and codomain which satisfies

In other words, the function value in (that is, the codomain) is always the same input element of (now considered as the domain). The identity function on is clearly an injective function as well as a surjective function, so it is also bijective.

The identity function on is often denoted by .

In set theory, where a function is defined as a particular kind of binary relation, the identity function is given by the identity relation, or "diagonal" of .

If is any function, then we have (where "∘" denotes function composition). In particular, is the identity element of the monoid of all functions from to .

Since the identity element of a monoid is unique, one can alternately define the identity function on to be this identity element. Such a definition generalizes to the concept of an identity morphism in category theory, where the endomorphisms of need not be functions.




</doc>
<doc id="15070" url="https://en.wikipedia.org/wiki?curid=15070" title="Intel 80386">
Intel 80386

The Intel 80386, also known as i386 or just 386, is a 32-bit microprocessor introduced in 1985. The first versions had 275,000 transistors and were the CPU of many workstations and high-end personal computers of the time. As the original implementation of the 32-bit extension of the 80286 architecture, the 80386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the "i386-architecture", "x86", or "IA-32", depending on context.

The 32-bit 80386 can correctly execute most code intended for the earlier 16-bit processors such as 8086 and 80286 that were ubiquitous in early PCs. (Following the same tradition, modern 64-bit x86 processors are able to run most programs written for older x86 CPUs, all the way back to the original 16-bit 8086 of 1978.) Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086). A 33 MHz 80386 was reportedly measured to operate at about 11.4 MIPS.

The 80386 was introduced in October 1985, while manufacturing of the chips in significant quantities commenced in June 1986. Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was rationalized upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq and marked the first time a fundamental component in the IBM PC compatible de facto standard was updated by a company other than IBM.

In May 2006, Intel announced that 80386 production would stop at the end of September 2007. Although it had long been obsolete as a personal computer CPU, Intel and others had continued making the chip for embedded systems. Such systems using an 80386 or one of many derivatives are common in aerospace technology and electronic musical instruments, among others. Some mobile phones also used (later fully static CMOS variants of) the 80386 processor, such as BlackBerry 950 and Nokia 9000 Communicator. Linux continued to support 80386 processors until December 11, 2012; when the kernel cut 386-specific instructions in version 3.8.

The processor was a significant evolution in the x86 architecture, and extended a long line of processors that stretched back to the Intel 8008. The predecessor of the 80386 was the Intel 80286, a 16-bit processor with a segment-based memory management and protection system. The 80386 added a three-stage instruction pipeline, extended the architecture from 16-bits to 32-bits, and added an on-chip memory management unit. This paging translation unit made it much easier to implement operating systems that used virtual memory. It also offered support for register debugging.

The 80386 featured three operating modes: real mode, protected mode and virtual mode. The protected mode, which debuted in the 286, was extended to allow the 386 to address up to 4 GB of memory. The all new virtual 8086 mode (or "VM86") made it possible to run one or more real mode programs in a protected environment, although some programs were not compatible.

The ability for a 386 to be set up to act like it had a flat memory model in protected mode despite the fact that it uses a segmented memory model in all modes was arguably the most important feature change for the x86 processor family until AMD released x86-64 in 2003.

Several new instructions have been added to 386: BSF, BSR, BT, BTS, BTR, BTC, CDQ, CWDE, LFS, LGS, LSS, MOVSX, MOVZX, SETcc, SHLD, SHRD.

Two new segment registers have been added (FS and GS) for general-purpose programs, single Machine Status Word of 286 grew into eight control registers CR0–CR7. Debug registers DR0–DR7 were added for hardware breakpoints. New forms of MOV instruction are used to access them.

Chief architect in the development of the 80386 was John H. Crawford. He was responsible for extending the 80286 architecture and instruction set to 32-bit, and then led the microprogram development for the 80386 chip.

The 80486 and P5 Pentium line of processors were descendants of the 80386 design.

The following data types are directly supported and thus implemented by one or more 80386 machine instructions; these data types are briefly described here.:

The following 80386 assembly source code is for a subroutine named codice_1 that copies a null-terminated ASCIIZ character string from one location to another, converting all alphabetic characters to lower case. The string is copied one byte (8-bit character) at a time.

The example code uses the EBP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code and has been used by Algol-like languages since the late 1950s. A flat memory model is assumed, specifically, that the DS and ES segments address the same region of memory.

In 1988, Intel introduced the 80386SX, most often referred to as the 386SX, a cut-down version of the 80386 with a 16-bit data bus mainly intended for lower-cost PCs aimed at the home, educational, and small-business markets, while the 386DX remained the high-end variant used in workstations, servers, and other demanding tasks. The CPU remained fully 32-bit internally, but the 16-bit bus was intended to simplify circuit-board layout and reduce total cost. The 16-bit bus simplified designs but hampered performance. Only 24 pins were connected to the address bus, therefore limiting addressing to 16 MB, but this was not a critical constraint at the time. Performance differences were due not only to differing data-bus widths, but also due to performance-enhancing cache memories often employed on boards using the original chip.

The original 80386 was subsequently renamed 80386DX to avoid confusion. However, Intel subsequently used the "DX" suffix to refer to the floating-point capability of the 80486DX. The 80387SX was an 80387 part that was compatible with the 386SX (i.e. with a 16-bit databus). The 386SX was packaged in a surface-mount QFP and sometimes offered in a socket to allow for an upgrade.

The i386SL was introduced as a power-efficient version for laptop computers. The processor offered several power-management options (e.g. SMM), as well as different "sleep" modes to conserve battery power. It also contained support for an external cache of 16 to 64 kB. The extra functions and circuit implementation techniques caused this variant to have over 3 times as many transistors as the i386DX. The i386SL was first available at 20 MHz clock speed, with the 25 MHz model later added.

The first company to design and manufacture a PC based on the Intel 80386 was Compaq. By extending the 16/24-bit IBM PC/AT standard into a natively 32-bit computing environment, Compaq became the first third party to implement a major technical hardware advance on the PC platform. IBM was offered use of the 80386, but had manufacturing rights for the earlier 80286. IBM therefore chose to rely on that processor for a couple more years. The early success of the Compaq 386 PC played an important role in legitimizing the PC "clone" industry and in de-emphasizing IBM's role within it.

Prior to the 386, the difficulty of manufacturing microchips and the uncertainty of reliable supply made it desirable that any mass-market semiconductor be multi-sourced, that is, made by two or more manufacturers, the second and subsequent companies manufacturing under license from the originating company. The 386 was for "a time" (4.7 years) only available from Intel, since Andy Grove, Intel's CEO at the time, made the decision not to encourage other manufacturers to produce the processor as second sources. This decision was ultimately crucial to Intel's success in the market. The 386 was the first significant microprocessor to be single-sourced. Single-sourcing the 386 allowed Intel greater control over its development and substantially greater profits in later years.

AMD introduced its compatible Am386 processor in March 1991 after overcoming legal obstacles, thus ending Intel's 4.7-year monopoly on 386-compatible processors. From 1991 IBM also manufactured 386 chips under license for use only in IBM PCs and boards.


Intel originally intended for the 80386 to debut at 16 MHz. However, due to poor yields, it was instead introduced at 12.5 MHz.

Early in production, Intel discovered a marginal circuit that could cause a system to return incorrect results from 32-bit multiply operations. Not all of the processors already manufactured were affected, so Intel tested its inventory. Processors that were found to be bug-free were marked with a double sigma (ΣΣ), and affected processors were marked "16 BIT S/W ONLY". These latter processors were sold as good parts, since at the time 32-bit capability was not relevant for most users. Such chips are now extremely rare and became collectible.

The i387 math coprocessor was not ready in time for the introduction of the 80386, and so many of the early 80386 motherboards instead provided a socket and hardware logic to make use of an 80287. In this configuration the FPU operated asynchronously to the CPU, usually with a clock rate of 10 MHz. The original Compaq Deskpro 386 is an example of such design. However, this was an annoyance to those who depended on floating-point performance, as the performance advantages of the 80387 over the 80287 were significant.

Intel later offered a modified version of its 80486DX in 80386 packaging, branded as the Intel RapidCAD. This provided an upgrade path for users with 80386-compatible hardware. The upgrade was a pair of chips that replaced both the 80386 and 80387. Since the 80486DX design contained an FPU, the chip that replaced the 80386 contained the floating-point functionality, and the chip that replaced the 80387 served very little purpose. However, the latter chip was necessary in order to provide the FERR signal to the mainboard and appear to function as a normal floating-point unit.

Third parties offered a wide range of upgrades, for both SX and DX systems. The most popular ones were based on the Cyrix 486DLC/SLC core, which typically offered a substantial speed improvement due to its more efficient instruction pipeline and internal L1 SRAM cache. The cache was usually 1 kB, or sometimes 8 kB in the TI variant. Some of these upgrade chips (such as the 486DRx2/SRx2) were marketed by Cyrix themselves, but they were more commonly found in kits offered by upgrade specialists such as Kingston, Evergreen and Improve-It Technologies. Some of the fastest CPU upgrade modules featured the IBM SLC/DLC family (notable for its 16 kB L1 cache), or even the Intel 486 itself. Many 386 upgrade kits were advertised as being simple drop-in replacements, but often required complicated software to control the cache or clock doubling. Part of the problem was that on most 386 motherboards, the A20 line was controlled entirely by the motherboard with the CPU being unaware, which caused problems on CPUs with internal caches.

Overall, it was very difficult to configure upgrades to produce the results advertised on the packaging, and upgrades were often not very stable or not fully compatible.

Original version, released in October 1985.

A specially packaged Intel 486DX and a dummy floating point unit (FPU) designed as pin-compatible replacements for an Intel 80386 processor and 80387 FPU.

This was an embedded version of the 80386SX which did not support real mode and paging in the MMU.

System and power management and built in peripheral and support functions: Two 82C59A interrupt controllers; Timer, Counter (3 channels); Asynchronous SIO (2 channels); Synchronous SIO (1 channel); Watchdog timer (Hardware/Software); PIO. Usable with 80387SX or i387SL FPUs.

Transparent power management mode, integrated MMU and TTL compatible inputs (only 386SXSA). Usable with i387SX or i387SL FPUs.

Transparent power management mode and integrated MMU. Usable with i387SX or i387SL FPUs.




</doc>
<doc id="15072" url="https://en.wikipedia.org/wiki?curid=15072" title="Instruction register">
Instruction register

In computing, the instruction register (IR) or current instruction register (CIR) is the part of a CPU's control unit that holds the instruction currently being executed or decoded. In simple processors, each instruction to be executed is loaded into the instruction register, which holds it while it is decoded, prepared and ultimately executed, which can take several steps.

Some of the complicated processors use a pipeline of instruction registers where each stage of the pipeline does part of the decoding, preparation or execution and then passes it to the next stage for its step. Modern processors can even do some of the steps out of order as decoding on several instructions is done in parallel.

Decoding the op-code in the instruction register includes determining the instruction, determining where its operands are in memory, retrieving the operands from memory, allocating processor resources to execute the command (in super scalar processors), etc.

The output of the IR is available to control circuits, which generate the timing signals that control the various processing elements involved in executing the instruction.

In the instruction cycle, the instruction is loaded into the instruction register after the processor fetches it from the memory location pointed to by the program counter.



</doc>
<doc id="15073" url="https://en.wikipedia.org/wiki?curid=15073" title="Lists of islands">
Lists of islands

This is a list of the lists of islands in the world grouped by country, by continent, by body of water, and by other classifications. For rank-order lists, see the other lists of islands below.

By ocean:

By other bodies of water:


</doc>
<doc id="15075" url="https://en.wikipedia.org/wiki?curid=15075" title="INTERCAL">
INTERCAL

The Compiler Language With No Pronounceable Acronym (INTERCAL) is an esoteric programming language that was created as a parody by Don Woods and James M. Lyon, two Princeton University students, in 1972. It satirizes aspects of the various programming languages at the time, as well as the proliferation of proposed language constructs and notations in the 1960s.

There are two maintained implementations of INTERCAL dialects: C-INTERCAL, maintained by Eric S. Raymond, and CLC-INTERCAL, maintained by Claudio Calvelli. , both implementations were available in the Debian Software Archive.

According to the original manual by the authors,

The original Princeton implementation used punched cards and the EBCDIC character set. To allow INTERCAL to run on computers using ASCII, substitutions for two characters had to be made: $ substituted for ¢ as the "mingle" operator, "represent[ing] the increasing cost of software in relation to hardware", and ? was substituted for ⊻ as the unary exclusive-or operator to "correctly express the average person's reaction on first encountering exclusive-or". In recent versions of C-INTERCAL, the older operators are supported as alternatives; INTERCAL programs may now be encoded in ASCII, Latin-1, or UTF-8. .

C-INTERCAL swaps the major and minor version numbers, compared to tradition, the HISTORY file showing releases starting at version 0.3 and having progressed to 0.31, but containing 1.26 between 0.26 and 0.27.

CLC-INTERCAL version numbering scheme was traditional until version 0.06, when it changed to the scheme documented in the README file, which says:

INTERCAL was intended to be completely different from all other computer languages. Common operations in other languages have cryptic and redundant syntax in INTERCAL. From the INTERCAL Reference Manual:

INTERCAL has many other features designed to make it even more aesthetically unpleasing to the programmer: it uses statements such as "READ OUT", "IGNORE", "FORGET", and modifiers such as "PLEASE". This last keyword provides two reasons for the program's rejection by the compiler: if "PLEASE" does not appear often enough, the program is considered insufficiently polite, and the error message says this; if too often, the program could be rejected as excessively polite. Although this feature existed in the original INTERCAL compiler, it was undocumented.

Despite the language's intentionally obtuse and wordy syntax, INTERCAL is nevertheless Turing-complete: given enough memory, INTERCAL can solve any problem that a Universal Turing machine can solve. Most implementations of INTERCAL do this very slowly, however. A Sieve of Eratosthenes benchmark, computing all prime numbers less than 65536, was tested on a Sun SPARCstation 1. In C, it took less than half a second; the same program in INTERCAL took over seventeen hours.

The INTERCAL Reference Manual contains many paradoxical, nonsensical, or otherwise humorous instructions:

The manual also contains a "tonsil", as explained in this footnote: "4) Since all other reference manuals have appendices, it was decided that the INTERCAL manual should contain some other type of removable organ."

The INTERCAL manual gives unusual names to all non-alphanumeric ASCII characters: single and double quotes are "sparks" and "rabbit ears" respectively. (The exception is the ampersand: as the Jargon File states, "what could be sillier?") The assignment operator, represented as an equals sign (INTERCAL's "half mesh") in many other programming languages, is in INTERCAL a left-arrow, codice_1, made up of an "angle" and a "worm", obviously read as "gets".

Input (using the codice_2 instruction) and output (using the codice_3 instruction) do not use the usual formats; in INTERCAL-72, WRITE IN inputs a number written out as digits in English (such as SIX FIVE FIVE THREE FIVE), and READ OUT outputs it in "butchered" Roman numerals. More recent versions have their own I/O systems.

Comments can be achieved by using the inverted statement identifiers involving NOT or N'T; these cause lines to be initially ABSTAINed so that they have no effect. (A line can be ABSTAINed from even if it doesn't have valid syntax; syntax errors happen at runtime, and only then when the line is un-ABSTAINed.)

INTERCAL-72 (the original version of INTERCAL) had only four data types: the 16-bit integer (represented with a codice_4, called a "spot"), the 32-bit integer (codice_5, a "twospot"), the array of 16-bit integers (codice_6, a "tail"), and the array of 32-bit integers (codice_7, a "hybrid"). There are 65535 available variables of each type, numbered from codice_8 to codice_9 for 16-bit integers, for instance. However, each of these variables has its own stack on which it can be pushed and popped (STASHed and RETRIEVEd, in INTERCAL terminology), increasing the possible complexity of data structures. More modern versions of INTERCAL have by and large kept the same data structures, with appropriate modifications; TriINTERCAL, which modifies the radix with which numbers are represented, can use a 10-trit type rather than a 16-bit type, and CLC-INTERCAL implements many of its own data structures, such as "classes and lectures", by making the basic data types store more information rather than adding new types. Arrays are dimensioned by assigning to them as if they were a scalar variable. Constants can also be used, and are represented by a codice_10 ("mesh") followed by the constant itself, written as a decimal number; only integer constants from 0 to 65535 are supported.

There are only five operators in INTERCAL-72. Implementations vary in which characters represent which operation, and many accept more than one character, so more than one possibility is given for many of the operators.

Contrary to most other languages, AND, OR, and XOR are unary operators, which work on consecutive bits of their argument; the most significant bit of the result is the operator applied to the least significant and most significant bits of the input, the second-most-significant bit of the result is the operator applied to the most and second-most significant bits, the third-most-significant bit of the result is the operator applied to the second-most and third-most bits, and so on. The operator is placed between the punctuation mark specifying a variable name or constant and the number that specifies which variable it is, or just inside grouping marks (i.e. one character later than it would be in programming languages like C.) SELECT and INTERLEAVE (which is also known as MINGLE) are infix binary operators; SELECT takes the bits of its first operand that correspond to "1" bits of its second operand and removes the bits that correspond to "0" bits, shifting towards the least significant bit and padding with zeroes (so 51 (110011 in binary) SELECT 21 (10101 in binary) is 5 (101 in binary)); MINGLE alternates bits from its first and second operands (in such a way that the least significant bit of its second operand is the least significant bit of the result). There is no operator precedence; grouping marks must be used to disambiguate the precedence where it would otherwise be ambiguous (the grouping marks available are codice_11 ("spark"), which matches another spark, and codice_12 ("rabbit ears"), which matches another rabbit ears; the programmer is responsible for using these in such a way that they make the expression unambiguous).

INTERCAL statements all start with a "statement identifier"; in INTERCAL-72, this can be codice_13, codice_14, or codice_15, all of which mean the same to the program (but using one of these too heavily causes the program to be rejected, an undocumented feature in INTERCAL-72 that was mentioned in the C-INTERCAL manual), or an inverted form (with codice_16 or codice_17 appended to the identifier). Backtracking INTERCAL, a modern variant, also allows variants using codice_18 (possibly combined with PLEASE or DO) as a statement identifier, which introduces a choice-point. Before the identifier, an optional line number (an integer enclosed in parentheses) can be given; after the identifier, a percent chance of the line executing can be given in the format codice_19, which defaults to 100%.

In INTERCAL-72, the main control structures are NEXT, RESUME, and FORGET. codice_20 branches to the line specified, remembering the next line that would be executed if it weren't for the NEXT on a call stack (other identifiers than DO can be used on any statement, DO is given as an example); codice_21 removes "expression" entries from the top of the call stack (this is useful to avoid the error that otherwise happens when there are more than 80 entries), and codice_22 removes "expression" entries from the call stack and jumps to the last line remembered.

C-INTERCAL also provides the COME FROM instruction, written codice_23; CLC-INTERCAL and the most recent C-INTERCAL versions also provide computed COME FROM (codice_24 and NEXT FROM, which is like COME FROM but also saves a return address on the NEXT STACK.

Alternative ways to affect program flow, originally available in INTERCAL-72, are to use the IGNORE and REMEMBER instructions on variables (which cause writes to the variable to be silently ignored and to take effect again, so that instructions can be disabled by causing them to have no effect), and the ABSTAIN and REINSTATE instructions on lines or on types of statement, causing the lines to have no effect or to have an effect again respectively.

The traditional "Hello, world!" program demonstrates how different INTERCAL is from standard programming languages. In C, it could read as follows:

int main(void) {

The equivalent program in C-INTERCAL is longer and harder to read:
DO ,1 <- #13
PLEASE DO ,1 SUB #1 <- #238
DO ,1 SUB #2 <- #108
DO ,1 SUB #3 <- #112
DO ,1 SUB #4 <- #0
DO ,1 SUB #5 <- #64
DO ,1 SUB #6 <- #194
DO ,1 SUB #7 <- #48
PLEASE DO ,1 SUB #8 <- #22
DO ,1 SUB #9 <- #248
DO ,1 SUB #10 <- #168
DO ,1 SUB #11 <- #24
DO ,1 SUB #12 <- #16
DO ,1 SUB #13 <- #162
PLEASE READ OUT ,1
PLEASE GIVE UP
The original Woods–Lyon INTERCAL was very limited in its input/output capabilities: the only acceptable input were numbers with the digits spelled out, and the only output was an extended version of Roman numerals.

The C-INTERCAL reimplementation, being available on the Internet, has made the language more popular with devotees of esoteric programming languages. The C-INTERCAL dialect has a few differences from original INTERCAL and introduced a few new features, such as a COME FROM statement and a means of doing text I/O based on the Turing Text Model.

The authors of C-INTERCAL also created the TriINTERCAL variant, based on the Ternary numeral system and generalizing INTERCAL's set of operators.

A more recent variant is Threaded Intercal, which extends the functionality of COME FROM to support multithreading.

CLC-INTERCAL has a library called INTERNET for networking functionality including being an INTERCAL server, and also includes features such as Quantum Intercal, which enables multi-value calculations in a way purportedly ready for the first quantum computers.
In early 2017 a .NET Implementation targeting the .NET Framework appeared on GitHub. This implementation supports the creation of standalone binary libraries and interop with other programming languages. 

In the article "A Box, Darkly: Obfuscation, Weird Languages, and Code Aesthetics", INTERCAL is described under the heading "Abandon all sanity, ye who enter here: INTERCAL". The compiler and commenting strategy are among the "weird" features described:

In "Technomasochism", Lev Bratishenko characterizes the INTERCAL compiler as a dominatrix:
The Nitrome Enjoyment System, a fictional video game console created by British indie game developer Nitrome, has games which are programmed in INTERCAL.



</doc>
<doc id="15076" url="https://en.wikipedia.org/wiki?curid=15076" title="International Data Encryption Algorithm">
International Data Encryption Algorithm

In cryptography, the International Data Encryption Algorithm (IDEA), originally called Improved Proposed Encryption Standard (IPES), is a symmetric-key block cipher designed by James Massey of ETH Zurich and Xuejia Lai and was first described in 1991. The algorithm was intended as a replacement for the Data Encryption Standard (DES). IDEA is a minor revision of an earlier cipher Proposed Encryption Standard (PES).

The cipher was designed under a research contract with the Hasler Foundation, which became part of Ascom-Tech AG. The cipher was patented in a number of countries but was freely available for non-commercial use. The name "IDEA" is also a trademark. The last patents expired in 2012, and IDEA is now patent-free and thus completely free for all uses.

IDEA was used in Pretty Good Privacy (PGP) v2.0 and was incorporated after the original cipher used in v1.0, BassOmatic, was found to be insecure. IDEA is an optional algorithm in the OpenPGP standard.

IDEA operates on 64-bit blocks using a 128-bit key and consists of a series of 8 identical transformations (a "round", see the illustration) and an output transformation (the "half-round"). The processes for encryption and decryption are similar. IDEA derives much of its security by interleaving operations from different groups — modular addition and multiplication, and bitwise eXclusive OR (XOR) — which are algebraically "incompatible" in some sense. In more detail, these operators, which all deal with 16-bit quantities, are:

After the 8 rounds comes a final “half-round”, the output transformation illustrated below (the swap of the middle two values cancels out the swap at the end of the last round, so that there is no net swap):

The overall structure of IDEA follows the Lai–Massey scheme. XOR is used for both subtraction and addition. IDEA uses a key-dependent half-round function. To work with 16-bit words (meaning 4 inputs instead of 2 for the 64-bit block size), IDEA uses the Lai–Massey scheme twice in parallel, with the two parallel round functions being interwoven with each other. To ensure sufficient diffusion, two of the sub-blocks are swapped after each round.

Each round uses 6 16-bit sub-keys, while the half-round uses 4, a total of 52 for 8.5 rounds. The first 8 sub-keys are extracted directly from the key, with K1 from the first round being the lower 16 bits; further groups of 8 keys are created by rotating the main key left 25 bits between each group of 8. This means that it is rotated less than once per round, on average, for a total of 6 rotations.

Decryption works like encryption, but the order of the round keys is inverted, and the subkeys for the odd rounds are inversed. For instance, the values of subkeys K1–K4 are replaced by the inverse of K49–K52 for the respective group operation, K5 and K6 of each group should be replaced by K47 and K48 for decryption.

The designers analysed IDEA to measure its strength against differential cryptanalysis and concluded that it is immune under certain assumptions. No successful linear or algebraic weaknesses have been reported. , the best attack applied to all keys could break IDEA reduced to 6 rounds (the full IDEA cipher uses 8.5 rounds). Note that a "break" is any attack that requires less than 2 operations; the 6-round attack requires 2 known plaintexts and 2 operations.

Bruce Schneier thought highly of IDEA in 1996, writing: "In my opinion, it is the best and most secure block algorithm available to the public at this time." ("Applied Cryptography", 2nd ed.) However, by 1999 he was no longer recommending IDEA due to the availability of faster algorithms, some progress in its cryptanalysis, and the issue of patents.

In 2011 full 8.5-round IDEA was broken using a meet-in-the-middle attack. Independently in 2012, full 8.5-round IDEA was broken using a narrow-bicliques attack, with a reduction of cryptographic strength of about 2 bits, similar to the effect of the previous bicliques attack on AES; however, this attack does not threaten the security of IDEA in practice.

The very simple key schedule makes IDEA subject to a class of weak keys; some keys containing a large number of 0 bits produce weak encryption. These are of little concern in practice, being sufficiently rare that they are unnecessary to avoid explicitly when generating keys randomly. A simple fix was proposed: XORing each subkey with a 16-bit constant, such as 0x0DAE.

Larger classes of weak keys were found in 2002.

This is still of negligible probability to be a concern to a randomly chosen key, and some of the problems are fixed by the constant XOR proposed earlier, but the paper is not certain if all of them are. A more comprehensive redesign of the IDEA key schedule may be desirable.

A patent application for IDEA was first filed in Switzerland (CH A 1690/90) on May 18, 1990, then an international patent application was filed under the Patent Cooperation Treaty on May 16, 1991. Patents were eventually granted in Austria, France, Germany, Italy, the Netherlands, Spain, Sweden, Switzerland, the United Kingdom, (, filed May 16, 1991, issued June 22, 1994 and expired May 16, 2011), the United States (, issued May 25, 1993 and expired January 7, 2012) and Japan (JP 3225440) (expired May 16, 2011).

MediaCrypt AG is now offering a successor to IDEA and focuses on its new cipher (official release on May 2005) IDEA NXT, which was previously called FOX.




</doc>
<doc id="15077" url="https://en.wikipedia.org/wiki?curid=15077" title="Indoor rower">
Indoor rower

An indoor rower, or rowing machine, is a machine used to simulate the action of watercraft rowing for the purpose of exercise or training for rowing. Indoor rowing has become established as a sport in its own right. The term "indoor rower" also refers to a participant in this sport.

Modern indoor rowers are often known as ergometers (colloquially erg or ergo), which is technically incorrect, as an ergometer is a device which measures the amount of work performed. The indoor rower is calibrated to measure the amount of energy the rower is using through their use of the equipment. Typically the display of the ergometer will show the time it takes to row 500m at each strokes power, also called split rate, or split. 

Chabrias, an Athenian admiral of the 4th century BC, introduced the first rowing machines as supplemental military training devices. "To train inexperienced oarsmen, Chabrias built wooden rowing frames on shore where beginners could learn technique and timing before they went on board ship."

Early rowing machines are known to have existed from the mid-1800s, a US patent being issued to W.B. Curtis in 1872 for a particular hydraulic based damper design. Machines using linear pneumatic resistance were common around 1900—one of the most popular was the Narragansett hydraulic rower, manufactured in Rhode Island from around 1900–1960. However they did not simulate actual rowing very accurately nor measure power output.

In the 1950s and 1960s, coaches in many countries began using specially made rowing machines for training and improved power measurement. One original design incorporated a large, heavy, solid iron flywheel with a mechanical friction brake, developed by John Harrison of Leichhardt Rowing Club in Sydney, later to become a professor of mechanical engineering at the University of New South Wales. Harrison, a dual Australian champion beach sprinter who went on to row in the coxless four at the 1956 Melbourne Olympics, had been introduced to rowing after a chance meeting with one of the fathers of modern athletic physiological training and testing, and the coach of the Leichhardt "Guinea Pigs", Professor Frank Cotton. Cotton had produced a rudimentary friction-based machine for evaluating potential rowers by exhausting them, without any pretence of accurately measuring power output. Harrison realised the importance of using a small braking area with a non-absorbent braking material, combined with a large flywheel. The advantage of this design (produced by Ted Curtain Engineering, Curtain being a fellow Guinea Pig) was the virtual elimination of factors able to interfere with accurate results—for instance ambient humidity or temperature. The Harrison-Cotton machine represents the very first piece of equipment able to accurately quantify human power output; power calculation within an accuracy range as achieved by his machine of less than 1% remains an impressive result today. The friction brake was adjusted according to a rower's weight to give an accurate appraisal of boat-moving ability (drag on a boat is proportional to weight). Inferior copies of Harrison's machine were produced in several countries utilising a smaller flywheel and leather straps—unfortunately the leather straps were sensitive to humidity, and the relatively large braking area made results far less accurate than Harrison's machine. The weight correction factor tended to make them unpopular among rowers of the time. Harrison, arguably the father of modern athletic power evaluation, died in February 2012.
In the 1970s, the Gjessing-Nilson ergometer from Norway used a friction brake mechanism with industrial strapping applied over the broad rim of the flywheel. Weights hanging from the strap ensured that an adjustable and predictable friction could be calculated. The cord from the handle mechanism ran over a helical pulley with varying radius, thereby adjusting the gearing and speed of the handle in a similar way to the changing mechanical gearing of the oar through the stroke, derived from changes in oar angle and other factors. This machine was for many years the internationally accepted standard for measurement.

The first air resistance ergometers were introduced around 1980 by Repco.

In 1981, Peter and Richard Dreissigacker, and Jonathan Williams, filed for U.S. patent protection, as joint inventors of a "Stationary Rowing Unit". The patent was granted in 1983 (US 4396188A). The first commercial embodiment of the Concept2 "rowing ergometer" (as it came to be known) was the Model A, a fixed-frame sliding-seat design using a bicycle wheel with fins attached for air resistance. The Model B, introduced in 1986, introduced a solid cast flywheel (now enclosed by a cage) and the first digital performance monitor, which proved revolutionary. This machine's capability of accurate calibration combined with easy transportability spawned the sport of competitive indoor rowing, and revolutionised training and selection procedures for watercraft rowing. Later models were the C (1993) and D (2003).

In 1995, Casper Rekers, a Dutch engineer, was granted a U.S. patent for a (US 5382210A) "Dynamically Balanced Rowing Simulator". This device differed from the prior art in that the flywheel and footrests are fixed to a carriage, the carriage being free to slide fore and aft on a rail or rails integral to the frame. The seat is also free to slide fore and aft on a rail or rails integral to the frame. From the patent Abstract: "During exercise, the independent seat and energy dissipating unit move apart and then together in a coordinated manner as a function of the stroke cycle of the oarsman."

All rowing-machine designs consist of an energy damper or braking mechanism connected to a chain, strap, belt and/or handle. Footrests are attached to the same mounting as the energy damper. Most include a rail which either the seat or the mechanism slide upon. Different machines have a variety of layouts and damping mechanisms, each of which have certain advantages and disadvantages.

Currently available ergometer (flywheel-type) rowing machines use a spring or elastic cord to take up the pull chain/strap and return the handle. Advances in elastic cord and spring technology have contributed to the longevity and reliability of this strategy, but it still has disadvantages. With time and usage, an elastic element loses its strength and elasticity. Occasionally it will require adjustment, and eventually it will no longer take up the chain with sufficient vigour, and will need to be replaced. The resilience of an elastic cord is also directly proportional to temperature. In an unheated space in a cold climate, an elastic cord equipped rowing ergometer is unusable because the chain take-up is too sluggish. Thus, as the result of several factors, the force required to stretch the elastic cord is a variable, not a constant. This is of little consequence if the exercise device is used for general fitness, but it is an unacknowledged problem, the "dirty little secret", of indoor rowing competitions. The electronic monitor only measures the user input to the flywheel. It does not measure the energy expenditure to stretch the elastic cord. A claim of a "level playing field" cannot be made when a resistance variable exists (that of the elastic cord) which is not measured or monitored in any way (see more on this in "Competitions" section).

In the patent record, means are disclosed whereby the chain/cable take-up and handle return are accomplished without the use of a spring or elastic cord, thereby avoiding the stated disadvantages and defects of this broadly used method. One example is the Gjessing-Nilson device described above. Partially discernable in the thumbnail photo, it utilizes a cable wrapped around a helical pulley on the flywheel shaft, the ends of this cable being connected to opposite ends of a long pole to which a handle is fixed. The obvious disadvantage of this system is the forward space requirement to accommodate the extension of the handle pole at the "catch" portion of the stroke. The advantage is that, except for small transmission losses, all of the user's energy output is imparted to the flywheel, where it can be accurately measured, not split between the flywheel and an elastic cord of variable, unmeasured resistance. If a similar system were installed on all rowing ergometers used in indoor rowing competitions, consistency between machines would be guaranteed because the variability factor of elastic cord resistance would be eliminated, and this would therefore ensure that the monitor displayed actual user energy input.

In a 1988 US patent (US 4772013A), Elliot Tarlow discloses another non-elastic chain/cable take-up and handle return strategy. Described and depicted is a continuous chain/cable loop that passes around the flywheel sprocket and around and between fixed pulleys and sprockets positioned fore and aft on the device. The handle is secured in the middle of the exposed upper horizontal section of the chain/cable loop. Although somewhat lacking in aesthetics, the Tarlow device does eliminate the stated disadvantages and defects of the ubiquitous elastic cord handle return. Tarlow further argues that the disclosed method provides an improved replication of rowing because in actual rowing the rower is not assisted by the contraction of a spring or elastic cord during the "recovery" portion of the stroke. The rower must push the oar handle forward against wind and oarlock resistance in preparation for the next stroke. Tarlow asserts that the invention replicates that resistance.

A third non-elastic handle return strategy is disclosed in US patent, "Gravity Return Rowing Exercise Device" (US9878200 B2, 2018) granted to Robert Edmondson. As stated in the patent document, the utilization of gravity (i.e.: a weight) to take up the chain and return the handle eliminates the inevitable variability of handle return force associated with an elastic cord system and thereby ensures consistency between machines.

Machines with a digital display calculate the user's power by measuring the speed of the flywheel during the stroke and then recording the rate at which it decelerates during the recovery. Using this and the known moment of inertia of the flywheel, the computer is able to calculate speed, power, distance and energy usage. Some ergometers can be connected to a personal computer using software, and data on individual exercise sessions can be collected and analysed. In addition, some software packages allows users to connect multiple ergometers either directly or over the internet for virtual races and workouts.

At the current state of the art, indoor rowers which utilize flywheel resistance can be categorized into two motion types. In both types, the rowing movement of the user causes the footrests and the seat to move further and closer apart in co-ordination with the user's stroke. The difference between the two types is in the movement, or absence of movement, of the footrests relative to ground.

The first type is characterized by the Dreissigacker/Williams device (referenced above). With this type the flywheel and footrests are fixed to a stationary frame, and the seat is free to slide fore and aft on a rail or rails integral to the stationary frame. Therefore, during use, the seat moves relative to the footrests and also relative to ground, while the flywheel and footrests remain stationary relative to ground.

The second type is characterized by the Rekers device (referenced above). With this type, both the seat and the footrests are free to slide fore and aft on a rail or rails integral to a stationary frame. Therefore, during use, the seat and the footrests move relative to each other, and both also move relative to ground.

Piston resistance comes from hydraulic cylinders that are attached to the handles of the rowing machine. The length of the rower handles on this class of rower is typically adjustable, however, during the row the handle length is fixed which in turn fixes the trajectory that the hands must take on the stroke and return, thus making the stroke less accurate than is possible on the other types of resistance models where it is possible to emulate the difference in hand height on the stroke and return. Furthermore, many models in this class have a fixed seat position that eliminates the leg drive which is the foundation of competitive on water rowing technique. Because of the compact size of the pistons and mechanical simplicity of design, these models are typically not as large or as expensive as the others types.

Braked flywheel resistance models comprise magnetic, air and water resistance rowers. These machines are mechanically similar since all three types use a handle connected to a flywheel by rope, chain, or strap to provide resistance to the user – the types differ only in braking mechanism. Because the handle is attached to the resistance source by rope or similarly flexible media, the trajectory of the hands in the vertical plane is free making it possible for the rower to emulate the hand height difference between the stroke and the return. Most of these models have the characteristic sliding seat typical of competitive on-the-water boats.

Magnetic resistance models control resistance by means of permanent magnets or electromagnets. A rotary plate, made of non-magnetic, electrical conducting material such as aluminum or copper, and either integral with, or independent of the flywheel, cuts through the magnetic field of the permanent magnet or the electromagnet, resulting in induced eddy currents which generate a retarding force that opposes the motion of the rotary plate. Resistance is adjusted with the permanent magnet system by changing the position of the permanent magnet relative to the rotary plate. Resistance is adjusted with the electromagnetic system by varying the strength of the electromagnetic field through which the rotary plate moves. The magnetic braking system is quieter than the other braked flywheel types and energy can be accurately measured on this type of rower. The drawback of this type of resistance mechanism is that the resistance is constant for any given setting. Rowers using air or water resistance more accurately simulate actual rowing, where the resistance increases the harder the handle is pulled. Some rowing machines incorporate both air and magnetic resistance.

Air resistance models use vanes on the flywheel to provide the flywheel braking needed to generate resistance. As the flywheel is spun faster, the air resistance increases. An adjustable vent can be used to control the volume of air moved by the vanes of the rotating flywheel, therefore a larger vent opening results in a higher resistance, and a smaller vent opening results in a lower resistance. The energy dissipated can be accurately calculated given the known moment of inertia of the flywheel and a tachometer to measure the deceleration of the flywheel. Air resistance rowing machines are most often used by sport rowers (particularly during the off season and inclement weather) and competitive indoor rowers. 

Water resistance models consist of a paddle revolving in an enclosed tank of water. The mass and drag of the moving water creates the resistance. Proponents claim that this approach results in a more realistic action than possible with air or magnetic type machines. "WaterRower" was the first company to manufacturer this type of rowing machine. The company was formed in the 1980s by John Duke, a US National Team rower, and inventor of the device (1989 US Patent ). At that time, in the patent record, there were a few prior art fluid resistance rowing machines, but they lacked the simplicity and elegance of the Duke design. From the 1989 patent Abstract: "... rowing machine features a hollow container that holds a supply of water. Pulling on a drive cord during a pulling segment of a stroke rotates a paddle or like mechanism within the container to provide a momentum effect."

An extremely efficient method of exercise, rowing uses 86% of muscles when done with correct form. Its health benefits are often contrasted with those of spinning, since both are dually categorized as static and dynamic exercises. Indoor rowing primarily works the cardiovascular systems with typical workouts consisting of steady pieces of 20–40 minutes, although the standard trial distance for record attempts is 2000 m, which can take from five and a half minutes (best elite rowers) to nine minutes or more. Like other forms of cardio focused exercise, interval training is also commonly used in indoor rowing. While cardio-focused, rowing also stresses many muscle groups throughout the body anaerobically, thus rowing is often referred to as a strength-endurance sport.

The standard measurement of speed on an ergometer is generally known as the "split", or the amount of time in minutes and seconds required to travel at the current pace — a split of 2:00 represents a speed of two minutes per 500 metres, or about .

Although ergometer tests are used by rowing coaches to evaluate rowers and are part of athlete selection for many senior and junior national rowing teams, "the data suggest that physiological and performance tests performed on a rowing ergometer are not good indicators of on water performance".

Rowing technique on the erg broadly follows the same pattern as that of a normal rowing stroke on water, but with minor modifications: it is not necessary to "tap down" at the finish, since there are no blades to extract from water; but many who also row on water do this anyway. Also, the rigid, single-piece handle enables neither a sweep nor a sculling stroke. The oar handle during a sweep stroke follows a long arc, while the oar handles during a sculling stroke follow two arcs. The standard handle does neither. But regardless of this, to reduce the chance of injury, an exercise machine should enable a bio-mechanically correct movement of the user. The handle is the interface between the human and the machine, and should adapt to the natural movement of the user, not the user to the machine, as is now the case. During competitions an exaggerated finish is often used, whereby the hands are pulled further up the chest than would be possible on the water, resulting in a steep angulation of the wrists - but even with a normal stroke, stop-action images show wrist angulation at the finish, evidence that the standard rigid, single-piece handle does not allow the user to maintain a bio-mechanically correct alignment of hands, wrists, and forearms in the direction of applied force. On the Concept 2 website "Forum", many regular users of the indoor rower have complained of chronic wrist pain. Some have rigged handgrips with flexible straps to enable their hands, wrists, and forearms to maintain proper alignment, and thereby reduce the possibility of repetitive strain injury. Rowing machine manufacturers have ignored this problem.

Rowing on an ergometer requires four basic phases to complete one stroke; the catch, the drive, the finish and the recovery. The catch is the initial part of the stroke. The drive is where the power from the rower is generated while the finish is the final part of the stroke. Then, the recovery is the initial phase to begin taking a new stroke. The phases repeat until a time duration or a distance is completed.

Knees are bent with the shins in a vertical position. The back should be roughly parallel to the thigh without hyperflexion (leaning forward too far). The arms and shoulders should be extended forward and relaxed. The arms should be level.

The drive is initiated by the extension of the legs; the body remains in the catch posture at this point of the drive. As the legs continue to full extension, the rower engages the core to begin the motion of the body levering backward, adding to the work of the legs. When the legs are flat, the rower begins to pull the handle toward the chest with their arms while keeping their arms straight and parallel to the floor.

The legs are at full extension and flat. The shoulders are slightly behind the pelvis, and the arms are in full contraction with the elbows bent and hands against the chest below the nipples. The back of the rower is still maintained in an upright posture and wrists should be flat.

The recovery is a slow slide back to the initial part of the stroke, it gives the rower time to recover from the previous stroke. During the recovery the actions are in reverse order of the drive. The arms are fully extended so that they are straight. The torso is engaged to move forward back over the pelvis. Weight transfers from the back of the seat to the front of the seat at this time. When the hands come over the knees, the legs contract back towards the foot stretcher. Slowly the back becomes more parallel to the thighs until the recovery becomes the catch.

The first indoor rowing competition was held in Cambridge, MA in February 1982 with participation of 96 on-water rowers who called themselves the "Charles River Association of Sculling Has-Beens". Thus the acronym, "CRASH-B". A large number of indoor rowing competitions are now held worldwide, including the indoor rowing world championships (still known as CRASH-B Sprints) held in Boston, Massachusetts, United States in February and the British Indoor Rowing Championships held in Birmingham, England in November, or in more recent years the Lee Valley VeloPark London in December; both are rowed on Concept2s. The core event for most competitions is the individual 2000-m; less common are the mile (e.g., Evesham), the 2500 meter (e.g., Basingstoke—also the original distance of the CRASH-B Sprints). Many competitions also include a sprint event (100–500m) and sometimes team relay events.

Most competitions are organized into categories based on sex, age, and weight class. While the fastest times are generally achieved by rowers between 20 and 40 years old, teenagers and rowers over 90 are common at competitions. There is a nexus between performance on-water and performance on the ergometer, with open events at the World Championships often being dominated by elite on-water rowers. Former men's Olympic single scull champions Pertti Karppinen and Rob Waddell and five-time gold medalist Sir Steven Redgrave have all won world championships or set world records in indoor rowing. The Briton Graham Benton and the Italian Emanuele Romoli are two of the main "non-rower" that won several indoor rowing competitions.

In addition to live venue competitions, many erg racers compete by internet, either offline by posting scores to challenges, or live online races facilitated by computer connection. Online Challenges sponsored by Concept2 include the annual ultra-rowing challenge, the Virtual Team Challenge.



</doc>
<doc id="15078" url="https://en.wikipedia.org/wiki?curid=15078" title="Internetwork Packet Exchange">
Internetwork Packet Exchange

Internetwork Packet Exchange (IPX) is the network layer protocol in the IPX/SPX protocol suite. IPX is derived from Xerox Network Systems' IDP. It may act as a transport layer protocol as well.

The IPX/SPX protocol suite was very popular through the late 1980s into the mid-1990s because it was used by the Novell NetWare network operating system. Because of Novell NetWare popularity, the IPX became a prominent internetworking protocol.

A big advantage of IPX was a small memory footprint of the IPX driver, which was vital for DOS and Windows up to the version Windows 95 because of limited size of the conventional memory. Another IPX advantage is an easy configuration of the client computers. However, IPX does not scale well for large networks such as the Internet, and as such, IPX usage decreased as the boom of the Internet made TCP/IP nearly universal. Computers and networks can run multiple network protocols, so almost all IPX sites will be running TCP/IP as well to allow Internet connectivity. It is also possible to run later Novell products without IPX, with the beginning of full support for both IPX and TCP/IP by NetWare version 5 in late 1998.

A big advantage of IPX protocol is its little or no need for configuration. In the time when protocols for dynamic host configuration did not exist and the BOOTP protocol for centralized assigning of addresses was not common, the IPX network could be configured almost automatically. A client computer uses the MAC address of its network card as the node address and learns what it needs to know about the network topology from the servers or routers – routes are propagated by Routing Information Protocol, services by Service Advertising Protocol.

A small IPX network administrator had to care only

Each IPX packet begins with a header with the following structure:

The Packet Type values are:

An IPX address has the following structure:

The network number allows to address (and communicate with) the IPX nodes which do not belong to the same network or "cabling system". The cabling system is a network in which a data link layer protocol can be used for communication. To allow communication between different networks, they must be connected with IPX routers. A set of interconnected networks is called an internetwork. Any Novell NetWare server may serve as an IPX router. Novell also supplied stand-alone routers. Multiprotocol routers of other vendors often support IPX routing. Using different frame formats in one cabling system is possible, but it works similarly as if separate cabling systems were used (i.e. different network numbers must be used for different frame formats even in the same cabling system and a router must be used to allow communication between nodes using different frame formats in the same cabling system).


The node number is used to address an individual computer (or more exactly, a network interface) in the network. Client stations use its network interface card MAC address as the node number.

The value FF:FF:FF:FF:FF:FF may be used as a node number in a destination address to broadcast a packet to "all nodes in the current network".

The socket number serves to select a process or application in the destination node.
The presence of a socket number in the IPX address allows the IPX to act as a transport layer protocol, comparable with the User Datagram Protocol (UDP) in the Internet protocol suite.

The IPX network number is conceptually identical to the network part of the IP address (the parts with netmask bits set to 1); the node number has the same meaning as the bits of IP address with netmask bits set to 0. The difference is that the boundary between network and node part of address in IP is variable, while in IPX it is fixed. As the node address is usually identical to the MAC address of the network adapter, the Address Resolution Protocol is not needed in IPX.

For routing, the entries in the IPX routing table are similar to IP routing tables; routing is done by network address, and for each network address a network:node of the next router is specified in a similar fashion an IP address/netmask is specified in IP routing tables.

There are three routing protocols available for IPX networks. In early IPX networks, a version of Routing Information Protocol (RIP) was the only available protocol to exchange routing information. Unlike RIP for IP, it uses delay time as the main metric, retaining the hop count as a secondary metric. Since NetWare 3, the NetWare Link Services Protocol (NLSP) based on IS-IS is available, which is more suitable for larger networks. Cisco routers implement an IPX version of EIGRP protocol as well.

IPX can be transmitted over Ethernet using one of the following 4 frame formats or encapsulation types:


In non-Ethernet networks only 802.2 and SNAP frame types are available.



</doc>
<doc id="15079" url="https://en.wikipedia.org/wiki?curid=15079" title="International human rights instruments">
International human rights instruments

International human rights instruments are the treaties and other international texts that serve as legal sources for international human rights law and the protection of human rights in general. There are many varying types, but most can be classified into two broad categories: "declarations", adopted by bodies such as the United Nations General Assembly, which are by nature declaratory, so not legally-binding although they may be politically authoritative and very well-respected soft law;, and often express guiding principles; and "conventions" that are multi-party treaties that are designed to become legally binding, usually include prescriptive and very specific language, and usually are concluded by a long procedure that frequently requires ratification by each states' legislature. Lesser known are some "recommendations" which are similar to conventions in being multilaterally agreed, yet cannot be ratified, and serve to set common standards. There may also be administrative guidelines that are agreed multilaterally by states, as well as the statutes of tribunals or other institutions. A specific prescription or principle from any of these various international instruments can, over time, attain the status of customary international law whether it is specifically accepted by a state or not, just because it is well-recognized and followed over a sufficiently long time.

International human rights instruments can be divided further into "global instruments", to which any state in the world can be a party, and "regional instruments", which are restricted to states in a particular region of the world.

Most conventions and recommendations (but few declarations) establish mechanisms for monitoring and establish bodies to oversee their implementation. In some cases these bodies that may have relatively little political authority or legal means, and may be ignored by member states; in other cases these mechanisms have bodies with great political authority and their decisions are almost always implemented. A good example of the latter is the European Court of Human Rights.

Monitoring mechanisms also vary as to the degree of individual access to expose cases of abuse and plea for remedies. Under some conventions or recommendations – e.g. the European Convention on Human Rights – individuals or states are permitted, subject to certain conditions, to take individual cases to a full-fledged tribunal at international level. Sometimes, this can be done in national courts because of universal jurisdiction.

The Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights, and the International Covenant on Economic, Social and Cultural Rights together with other international human rights instruments are sometimes referred to as the "international bill of rights". International human rights instruments are identified by the OHCHR and most are referenced on the OHCHR website.





According to OHCHR, there are 9 "core" international human rights instruments and several optional protocols. The core instruments are:


Several more human rights instruments exist. A few examples:









</doc>
<doc id="15080" url="https://en.wikipedia.org/wiki?curid=15080" title="Indian removal">
Indian removal

Indian removal was a forced migration in the 19th century whereby Native Americans were forced by the United States government to leave their ancestral homelands in the eastern United States to lands west of the Mississippi River, specifically to a designated Indian Territory (roughly, modern Oklahoma). The Indian Removal Act, the key law that forced the removal of the Indians, was signed by Andrew Jackson in 1830. Jackson took a hard line on Indian removal, but the law was put into effect primarily under the Martin van Buren administration.

Indian removal was a consequence of actions first by European settlers to North America in the colonial period, then by the United States government and its citizens until the mid-20th century. The policy traced its direct origins to the administration of James Monroe, though it addressed conflicts between European Americans and Native Americans that had been occurring since the 17th century, and were escalating into the early 19th century as white settlers were continually pushing westward.

American leaders in the Revolutionary and Early National era debated whether the American Indians should be treated officially as individuals or as nations in their own right. Some of these views are summarized below.

In a draft, "Proposed Articles of Confederation", presented to the Continental Congress on May 10, 1775, Benjamin Franklin called for a "perpetual Alliance" with the Indians for the nation about to take birth, especially with the Six Nations of the Iroquois Confederacy:
In his Notes on the State of Virginia (1785), Thomas Jefferson defended American Indian culture and marveled at how the tribes of Virginia "never submitted themselves to any laws, any coercive power, any shadow of government" due to their "moral sense of right and wrong". He would later write to the Marquis de Chastellux in 1785, "I believe the Indian then to be in body and mind equal to the whiteman". His desire, as interpreted by Francis Paul Prucha, was for the Native Americans to intermix with European Americans and to become one people. To achieve that end, Jefferson would, as president, offer U.S. citizenship to some Indian nations, and propose offering credit to them to facilitate their trade.

President George Washington, in his address to the Seneca nation in 1790, describing the pre-Constitutional Indian land sale difficulties as "evils", asserted that the case was now entirely altered, and publicly pledged to uphold their "just rights". In March and April 1792, Washington met with 50 tribal chiefs in Philadelphia—including the Iroquois—to discuss closer friendship between them and the United States. Later that same year, in his Fourth Annual Message to Congress, Washington stressed the need for building peace, trust, and commerce with America's Indian neighbors:

In 1795, in his Seventh Annual Message to Congress, Washington intimated that if the U.S. government wanted peace with the Indians, then it must give peace to them, and that if the U.S. wanted raids by Indians to stop, then raids by American "frontier inhabitants" must also stop.

The Confederation Congress passed the Northwest Ordinance of 1787, which would serve broadly as a precedent for the manner in which the United States' territorial expansion would occur for years to come, calling for the protection of Indians' "property, rights, and liberty": The U.S. Constitution of 1787 (Article I, Section 8) makes Congress responsible for regulating commerce with the Indian tribes. In 1790, the new U.S. Congress passed the Indian Nonintercourse Act (renewed and amended in 1793, 1796, 1799, 1802, and 1834) to protect and codify the land rights of recognized tribes.

As president, Thomas Jefferson developed a far-reaching Indian policy that had two primary goals. First, the security of the new United States was paramount, so Jefferson wanted to assure that the Native nations were tightly bound to the United States, and not other foreign nations. Second, he wanted "to civilize" them into adopting an agricultural, rather than a hunter-gatherer lifestyle. These goals would be achieved through the development of trade and the signing of treaties.

Jefferson initially promoted an American policy that encouraged Native Americans to become assimilated, or "civilized". As president, Jefferson made sustained efforts to win the friendship and cooperation of many Native American tribes, repeatedly articulating his desire for a united nation of both whites and Indians, as in a letter to the Seneca spiritual leader, Handsome Lake, dated November 3, 1802:

When a delegation from the Upper Towns of the Cherokee Nation lobbied Jefferson for the full and equal citizenship George Washington had promised to Indians living in American territory, his response indicated that he was willing to accommodate citizenship for those Indian nations that sought it. In his Eighth Annual Message to Congress on November 8, 1808, he presented to the nation a vision of white and Indian unity:
As some of Jefferson's other writings illustrate, however, he was ambivalent about Indian assimilation, even going so far as to use the words "exterminate" and "extirpate" regarding tribes that resisted American expansion and were willing to fight to defend their lands. Jefferson's intention was to change Indian lifestyles from hunting and gathering to farming, largely through "the decrease of game rendering their subsistence by hunting insufficient". He expected that the switch to agriculture would make them dependent on white Americans for trade goods and therefore more likely to give up their land in exchange, or else be removed to lands west of the Mississippi. In a private 1803 letter to William Henry Harrison, Jefferson wrote:
Elsewhere in the same letter, Jefferson spoke of protecting the Indians from injustices perpetrated by whites:
By the terms of the treaty of February 27, 1819, the U.S. government would again offer citizenship to the Cherokees who lived east of the Mississippi River, along with 640 acres of land per family. Native American land was sometimes purchased, either via a treaty or under duress. The idea of land exchange, that is, that Native Americans would give up their land east of the Mississippi in exchange for a similar amount of territory west of the river, was first proposed by Jefferson in 1803 and had first been incorporated in treaties in 1817, years after the Jefferson presidency. The Indian Removal Act of 1830 incorporated this concept.

Under President James Monroe, Secretary of War John C. Calhoun devised the first plans for Indian removal. By late 1824, Monroe approved Calhoun's plans and in a special message to the Senate on January 27, 1825, requested the creation of the Arkansaw Territory and Indian Territory. The Indians east of the Mississippi were to voluntarily exchange their lands for lands west of the river. The Senate accepted Monroe's request and asked Calhoun to draft a bill, which was killed in the House of Representatives by the Georgia delegation. President John Quincy Adams assumed the Calhoun–Monroe policy and was determined to remove the Indians by non-forceful means, but Georgia refused to submit to Adams' request, forcing Adams to make a treaty with the Cherokees granting Georgia the Cherokee lands. On July 26, 1827, the Cherokee Nation adopted a written constitution modeled after that of the United States which declared they were an independent nation with jurisdiction over their own lands. Georgia contended that it would not countenance a sovereign state within its own territory, and proceeded to assert its authority over Cherokee territory. When Andrew Jackson became president as the candidate of the newly organized Democratic Party, he agreed that the Indians should be forced to exchange their eastern lands for western lands and relocate to them, and enforced Indian removal policy vigorously.

When Andrew Jackson assumed office as president of the United States in 1829, his government took a hard line on Indian Removal policy. Jackson abandoned the policy of his predecessors of treating different Indian groups as separate nations. Instead, he aggressively pursued plans against all Indian tribes which claimed constitutional sovereignty and independence from state laws, and which were based east of the Mississippi River. They were to be removed to reservations in Indian Territory west of the Mississippi (now Oklahoma), where their laws could be sovereign without any state interference. At Jackson's request, the United States Congress opened a debate on an Indian Removal Bill. After fierce disagreements, the Senate passed the measure 28–19, the House 102–97. Jackson signed the legislation into law May 30, 1830.

In 1830, the majority of the "Five Civilized Tribes"—the Chickasaw, Choctaw, Creek, Seminole, and Cherokee—were living east of the Mississippi. The Indian Removal Act of 1830 implemented the federal government's policy towards the Indian populations, which called for moving Native American tribes living east of the Mississippi River to lands west of the river. While it did not authorize the forced removal of the indigenous tribes, it authorized the president to negotiate land exchange treaties with tribes located in lands of the United States.

On September 27, 1830, the Choctaw signed the Treaty of Dancing Rabbit Creek and by concession, became the first Native American tribe to be removed. The agreement represented one of the largest transfers of land that was signed between the U.S. Government and Native Americans without being instigated by warfare. By the treaty, the Choctaw signed away their remaining traditional homelands, opening them up for European-American settlement in Mississippi Territory. When the Choctaw reached Little Rock, a Choctaw chief referred to the trek as a "trail of tears and death".

In 1831, Alexis de Tocqueville, the French historian and political thinker, witnessed an exhausted group of Choctaw men, women and children emerging from the forest during an exceptionally cold winter near Memphis, Tennessee, on their way to the Mississippi to be loaded onto a steamboat, and wrote:

While the Indian Removal Act made the move of the tribes voluntary, it was often abused by government officials. The best-known example is the Treaty of New Echota, which was negotiated and signed by a small faction of only twenty Cherokee tribal members, not the tribal leadership, on December 29, 1835. Most of the Cherokees later blamed them and the treaty for the forced relocation of the tribe in 1838. An estimated 4,000 Cherokees died in the march, now known as the Trail of Tears. Missionary organizer Jeremiah Evarts urged the Cherokee Nation to take their case to the U.S. Supreme Court.

The Marshall court heard the case in "Cherokee Nation v. Georgia" (1831), but declined to rule on its merits, instead declaring that the Native American tribes were not sovereign nations, and had no status to "maintain an action" in the courts of the United States. In "Worcester v. Georgia" (1832), the court held, in an opinion written by Chief Justice Marshall, that individual states had no authority in American Indian affairs.

Yet the state of Georgia defied the Supreme Court ruling, and the desire of white settlers and land speculators for Indian lands continued unabated. Some whites claimed that the Indian presence was a threat to peace and security; the Georgia legislature passed a law that after March 31, 1831, forbade whites from living on Indian territory without a license from the state, in order to exclude white missionaries who opposed Indian removal.

In 1835, the Seminole people refused to leave their lands in Florida, leading to the Second Seminole War. Osceola was a war leader of the Seminole in their fight against removal. Based in the Everglades of Florida, Osceola and his band used surprise attacks to defeat the U.S. Army in many battles. In 1837, Osceola was seized by deceit upon the orders of U.S. General Thomas Jesup when Osceola came under a flag of truce to negotiate a peace near Fort Peyton. Osceola died in prison of illness. The war would result in over 1,500 U.S. deaths and cost the government $20 million. Some Seminole traveled deeper into the Everglades, while others moved west. Removal continued out west and numerous wars ensued over land.

In the aftermath of the Treaty of Fort Jackson and the Treaty of Washington, the Muscogee were confined to a small strip of land in present-day east central Alabama. Following the Indian Removal Act, in 1832 the Creek National Council signed the Treaty of Cusseta, ceding their remaining lands east of the Mississippi to the U.S., and accepting relocation to the Indian Territory. Most Muscogee were removed to Indian Territory during the Trail of Tears in 1834, although some remained behind.

Unlike other tribes who exchanged land grants, the Chickasaw were to receive mostly financial compensation of $3 million from the United States for their lands east of the Mississippi River. In 1836, the Chickasaw reached an agreement that purchased land from the previously removed Choctaw after a bitter five-year debate, paying them $530,000 for the westernmost part of Choctaw land. Most of the Chickasaw moved in 1837–1838. The $3,000,000 that the U.S. owed the Chickasaw went unpaid for nearly 30 years.

As a result, the Five Civilized Tribes were resettled in the new Indian Territory in modern-day Oklahoma. The Cherokee occupied the northeast corner of the Territory, as well as a strip of land seventy miles wide in Kansas on the border between the two. Some indigenous nations resisted forced migration more strongly. Those few that stayed behind eventually formed tribal groups, including the Eastern Band of Cherokee based in North Carolina, the Mississippi Band of Choctaw Indians, the Seminole Tribe of Florida, and the Creeks in Alabama, including the Poarch Band.

Tribes in the Old Northwest were far smaller and more fragmented than the Five Civilized Tribes, so the treaty and emigration process was more piecemeal. Bands of Shawnee, Ottawa, Potawatomi, Sauk, and Meskwaki (Fox) signed treaties and relocated to the Indian Territory. In 1832, a Sauk leader named Black Hawk led a band of Sauk and Fox back to their lands in Illinois; in the ensuing Black Hawk War, the U.S. Army and Illinois militia defeated Black Hawk and his warriors, resulting in the Sauk and Fox being relocated into what would become present day Iowa.

Tribes further to the east, such as the already displaced Lenape (or Delaware tribe), as well as the Kickapoo and Shawnee, were removed from Indiana, Michigan, and Ohio in the 1820s. The Potawatomi were forced out in late 1838 and resettled in Kansas Territory. Many Miami were resettled to Indian Territory in the 1840s. Communities in present-day Ohio were forced to move to Louisiana, which was then controlled by Spain.

By the terms of the Second Treaty of Buffalo Creek (1838), the Senecas transferred all their land in New York, excepting one small reservation, in exchange for 200,000 acres of land in Indian Territory. The U.S. federal government would be responsible for the removal of those Senecas who opted to go west, while the Ogden Land company would acquire their lands in New York. The lands were sold by government officials, however, and the money deposited in the U.S. Treasury. The Senecas asserted that they had been defrauded, and sued for redress in the U.S. Court of Claims. The case was not resolved until 1898, when the United States awarded $1,998,714.46 in compensation to "the New York Indians". In 1842 and 1857, the U.S. signed treaties with the Senecas and the Tonawanda Senecas, respectively. Under the treaty of 1857, the Tonawandas renounced all claim to lands west of the Mississippi in exchange for the right to buy back the lands of the Tonawanda reservation from the Ogden Land Company. Over a century later, the Senecas purchased a nine-acre plot (part of their original reservation) in downtown Buffalo to build the "Seneca Buffalo Creek Casino".

The following is a compilation of the statistics, many containing rounded figures, regarding the Southern removals.
Historical views regarding the Indian Removal have been re-evaluated since that time. Widespread acceptance at the time of the policy, due in part to an embracing of the concept of Manifest destiny by the general populace, have since given way to somewhat harsher views. Descriptions such as "paternalism", ethnic cleansing, and even genocide have been ascribed by historians past and present to the motivation behind the Removals.

Andrew Jackson's reputation took a blow for his treatment of the Indians. Historians who admire Jackson's strong presidential leadership, such as Arthur Schlesinger, Jr., would skip over the Indian question with a footnote. Writing in 1969, Francis Paul Prucha argued that Jackson's removal of the Five Civilized Tribes from the very hostile white environment in the Old South to Oklahoma probably saved their very existence. In the 1970s, however, Jackson came under sharp attack from writers, such as Michael Paul Rogin and Howard Zinn, chiefly on this issue. Zinn called him "exterminator of Indians"; Paul R. Bartrop and Steven Leonard Jacobs argue that Jackson's policies did not meet the criterion for genocide or cultural genocide.





</doc>
<doc id="15081" url="https://en.wikipedia.org/wiki?curid=15081" title="Green Party (Ireland)">
Green Party (Ireland)

The Green Party - An Comhaontas Glas (, literally "Green Alliance") is a green political party that operates in the Republic of Ireland and Northern Ireland. It was founded as the Ecology Party of Ireland in 1981 by Dublin teacher Christopher Fettes. The party became the Green Alliance in 1983 and adopted its current English language name in 1987 while the Irish name was kept unchanged. , its leader is Eamon Ryan, its deputy leader is Catherine Martin and its chairperson is Hazel Chu. Green Party candidates have been elected to most levels of representation: local (in the Republic), Dáil Éireann, Northern Ireland Assembly and European Parliament.

The Green Party first entered the Dáil in 1989. It has participated in the Irish government twice, from 2007 to 2011 as junior partner in a coalition with Fianna Fáil, and since June 2020 in a coalition with Fianna Fáil and Fine Gael. Following the first period in government, the party suffered a wipeout in the February 2011 election, losing all six of its TDs. In the February 2016 election, it returned to the Dáil with two seats. Following this, Grace O'Sullivan was elected to the Seanad on 26 April that year of 2016 and Joe O'Brien was elected to Dáil Éireann in the 2019 Dublin Fingal by-election. In the 2020 general election the party had its best result ever, securing 12 TDs and becoming the fourth largest party in Ireland before entering into government a second time.

The Green Party began life as the "Ecology Party" in 1981, with Christopher Fettes serving as the party's first chairperson. The party's first public appearance was modest: the event announced that they would be contesting the November 1982 general election, and was attended by their 7 election candidates, 20 party supporters, and one singular journalist. Fettes had opened the meeting by noting the party didn't expect to win any seats. Willy Clingan, the journalist present, recalled that "The Ecology Party introduced its seven election candidates at the nicest and most endearingly honest press conference of the whole campaign". The Ecology party took 0.2% of the vote that year.

Following a name change to the "Green Alliance", it contested the 1984 European elections, with party founder Roger Garland winning 1.9% in the Dublin constituency. The following year, it won its first election when Marcus Counihan was elected to Killarney Urban District Council at the 1985 local elections, buoyed by winning 5,200 first preference votes as a European candidate in Dublin the previous year. The party nationally ran 34 candidates and won 0.6% of the vote.

The party continued to struggle until the 1989 general election when the Green Party (as it was now named) won its first seat in Dáil Éireann, when Roger Garland was elected in Dublin South. Garland lost his seat at the 1992 general election, while Trevor Sargent gained a seat in Dublin North. In the 1994 European election, Patricia McKenna topped the poll in the Dublin constituency and Nuala Ahern won a seat in Leinster. They retained their European Parliament seats in the 1999 European election, although the party lost five councillors in local elections held that year despite an increase in its vote. At the 1997 general election, the party gained a seat when John Gormley won a Dáil seat in Dublin South-East.

At the 2002 general election the party made a breakthrough, getting six Teachtaí Dála (TDs) elected to the Dáil with 4% of the national vote. However, in the 2004 European election, the party lost both of its European Parliament seats. In the 2004 local elections, it increased its number of councillors at county level from 8 to 18 (out of 883) and at town council level from 5 to 14 (out of 744).

The party gained its first representation in the Northern Ireland Assembly in 2007, the Green Party in Northern Ireland having become a regional branch of the party the previous year. 

The Green Party entered government for the first time after the 2007 general election, held on 24 May. Although its share of first-preference votes increased at the election, the party failed to increase the number of TDs returned. Mary White won a seat for the first time in Carlow–Kilkenny; however, Dan Boyle lost his seat in Cork South-Central.

The party had approached the 2007 general election on an independent platform, ruling out no coalition partners while expressing its preference for an alternative to the outgoing coalition of Fianna Fáil and the Progressive Democrats. Neither the outgoing government nor an alternative of Fine Gael, Labour and the Green Party had sufficient seats to form a majority. Fine Gael ruled out a coalition arrangement with Sinn Féin, opening the way for Green Party negotiations with Fianna Fáil.

Before the negotiations began, Ciarán Cuffe TD wrote on his blog that "a deal with Fianna Fáil would be a deal with the devil… and [the Green Party would be] decimated as a Party". The negotiations were undertaken by Donall Geoghegan (the party's general secretary), Dan Boyle and the then party Chair John Gormley. The Green Party walked out after six days; this, Geoghegan later said, was owing to there not being "enough in [the deal] to allow [the Green Party] to continue". The negotiations restarted on 11 June; a draft programme for government was agreed the next day, which under party rules needed 66% of members to endorse it at a special convention. On 13 June 2007, Green members at the Mansion House in Dublin voted 86% in favour (441 to 67; with 2 spoilt votes) of entering coalition with Fianna Fáil. The following day, the six Green Party TDs voted for the re-election of Bertie Ahern as Taoiseach.

New party leader John Gormley was appointed as Minister for the Environment, Heritage and Local Government and Eamon Ryan was appointed as Minister for Communications, Energy and Natural Resources. Trevor Sargent was named Minister of State for Food and Horticulture.

Before its entry into government, the Green Party had been a vocal supporter of the Shell to Sea movement, the campaign to reroute the M3 motorway away from Tara and (to a lesser extent) the campaign to end United States military use of Shannon Airport. After the party entered government there were no substantive changes in government policy on these issues, which meant that Eamon Ryan oversaw the Corrib gas project while he was in office. The Green Party had, at its last annual conference, made an inquiry into the irregularities surrounding the project (see Corrib gas controversy) a precondition of entering government but changed its stance during post-election negotiations with Fianna Fáil.

The 2008 budget, announced on 6 December 2007, did not include a carbon levy on fuels such as petrol, diesel and home heating oil, which the Green Party had sought before the election. A carbon levy was, however, introduced in the 2010 Budget. The 2008 budget did include a separate carbon budget announced by Gormley, which introduced new energy efficiency tax credit, a ban on incandescent bulbs from January 2009, a tax scheme incentivising commuters' purchases of bicycles and a new scale of vehicle registration tax based on carbon emissions.

At a special convention on whether to support the Treaty of Lisbon on 19 January 2008, the party voted 63.5% in favour of supporting the Treaty; this fell short of the party's two-thirds majority requirement for policy issues. As a result, the Green Party did not have an official campaign in the first Lisbon Treaty referendum, although individual members were involved on different sides. The referendum did not pass in 2008, and following the Irish government's negotiation with EU member states of additional legal guarantees and assurances, the Green Party held another special convention meeting in Dublin on 18 July 2009 to decide its position on the second Lisbon referendum. Precisely two-thirds of party members present voted to campaign for a 'Yes' in the referendum. This was the first time in the party's history that it had campaigned in favour of a European treaty.

The government's response to the post-2008 banking crisis significantly affected the party's support, and it suffered at the 2009 local elections, returning with only three County Council seats in total and losing its entire traditional Dublin base, with the exception of a Town Council seat in Balbriggan.

Déirdre de Búrca, one of two Green Senators nominated by Taoiseach Bertie Ahern in 2007, resigned from the party and her seat in 2010, in part owing to the party's inability to secure her a job in the European Commission. On 23 February 2010, Trevor Sargent resigned as Minister of State for Food and Horticulture owing to allegations over contacting Gardaí about a criminal case involving a constituent. On 23 March 2010, Ciarán Cuffe was appointed as Minister for Horticulture, Sustainable Travel, Planning and Heritage while the party gained a junior ministerial position with Mary White appointed as Minister for Equality, Human Rights and Integration.

The Green Party supported the passage legislation for EC–ECB–IMF financial support for Ireland's bank bailout. On 19 January, the party derailed Taoiseach Brian Cowen's plans to reshuffle his cabinet when it refused to endorse Cowen's intended replacement ministers, forcing Cowen to redistribute the vacant portfolios among incumbent ministers. The Greens were angered at not having been consulted about this effort, and went as far as to threaten to pull out of the coalition unless Cowen set a firm date for an election due that spring. He ultimately set the date for 11 March.

On 23 January 2011, the Green Party met with Cowen following his resignation as leader of senior coalition partner Fianna Fáil the previous afternoon. The Green Party then announced it was breaking off the coalition and going into opposition with immediate effect. Green Party leader John Gormley said at a press conference announcing the withdrawal: The government ministerial posts of Gormley and Ryan were reassigned to Fianna Fáil ministers Éamon Ó Cuív and Pat Carey respectively. Green Ministers of State Ciarán Cuffe and Mary White also resigned from their roles.

In almost four years in Government, from 2007 to 2011, the Green Party contributed to the passage of civil partnership for same-sex couples, the introduction of major planning reform, a major increase in renewable energy output, progressive budgets, and a nationwide scheme of home insulation retrofitting.

The party suffered a wipeout at the 2011 general election, with all of its six TDs losing their seats, including those of former Ministers John Gormley and Eamon Ryan. Three of their six incumbent TDs lost their deposits. The party's share of the vote fell below 2%, meaning that they could not reclaim election expenses, and their lack of parliamentary representation led to the ending of state funding for the party.

The party candidates in the 2011 election to the Seanad were Dan Boyle and Niall Ó Brolcháin; neither was elected, and as a result, for the first time since 1989 the Green Party had no representatives in the Oireachtas.

Eamon Ryan was elected as party leader on 27 May 2011, succeeding John Gormley. Catherine Martin, was later appointed deputy leader, while Ciarán Cuffe and Mark Dearey were also placed on the party's front bench.

In the 2014 European election the party received 4.9% of the vote nationally (an increase of 3% on the 2009 result), failing to return a candidate to the European Parliament. In the 2014 local elections the party received 1.6% of the vote nationally. 12 candidates were elected to County Councils, an increase of nine.

At the 2016 general election the Green Party gained two seats, becoming the first Irish political party to lose all seats at an election and win seats at the subsequent election. In the subsequent election to Seanad Éireann, Grace O'Sullivan became the first elected Green Party Senator, winning a seat of the Agricultural Panel. She established the Civil Engagement group with five Independent Senators. On 30 May 2016, the Green Party joined with the Social Democrats to form a technical group in the Dáil.

In the 2019 local elections the Green Party saw significant gains, increasing their number of councillors from 12 to 49 and becoming the second largest party on Dublin City Council. At the concurrent 2019 European Parliament election the party received 11.4% of the vote nationally (an increase of 6.5% on the 2014 result), the highest share they have won at any election to date. As a result, the Greens are represented in the European Parliament for the first time since 2004 by two MEPs - former TD Ciarán Cuffe in Dublin and Senator Grace O'Sullivan in South.

On 1 November 2019, Pippa Hackett was elected to Seanad Éireann. She filled the seat left vacant by Grace O'Sullivan after the 2019 European Parliament election.

Joe O'Brien was elected to Dáil Éireann on 29 November 2019 in the 2019 Dublin Fingal by-election. He became the party's first TD to win a by-election and the party's third TD in the 32nd Dáil.

In the 2020 general election, the party had its best result ever, earning 7.1% of the first-preference votes and returning 12 TDs, up from three. It became the fourth-largest party in the Dáil and entered government in coalition with Fianna Fáil and Fine Gael. In the 2020 Seanad election the party returned two senators. A further two senators were nominated by Taoiseach, Micheál Martin bringing the total party representation in the Oireachtas to 16. In July 2020 Eamon Ryan retained his leadership of the party over Catherine Martin in the 2020 Green Party leadership election.

The Green Party has seven "founding principles". Broadly, these founding principles reflect the "Four Pillars" of Green Politics observed by the majority of Green Parties internationally: Ecological wisdom, Social justice, Grassroots democracy and Nonviolence. They also reflect the Six guiding principles of the Global Greens, which also includes Respect for diversity as a principle. 

While strongly associated with environmentalist policies, the party also has policies covering all other key areas. These include protection of the Irish language, lowering the voting age in Ireland to 16, a directly elected Seanad, support for universal healthcare, and a constitutional amendment which guarantees that the water of Ireland will never be privatised.

The party also advocates that terminally ill people should have the right to legally choose assisted dying, on which subject it believes "provisions should apply only to those with a terminal illness which is likely to result in death within six months". It also states that "such a right would only apply where the person has a clear and settled intention to end their own life which is proved by making, and signing, a written declaration to that effect. Such a declaration must be countersigned by two qualified doctors".

The National Executive Committee is the organising committee of the party. It comprises the party leader Eamon Ryan, the deputy leader Catherine Martin, the chair Hazel Chu, the National Coordinator, the General Secretary (in a non-voting role), a Young Greens representative, the Treasurer and ten members elected annually at the party convention.

The party did not have a national leader until 2001. At a special "Leadership Convention" in Kilkenny on 6 October 2001, Trevor Sargent was elected the first official leader of the Green Party. He was re-elected to this position in 2003 and again in 2005. The party's constitution requires that a leadership election be held within six months of a general election.

Sargent resigned the leadership in the wake of the 2007 general election to the 30th Dáil. During the campaign, Sargent had promised that he would not lead the party into Government with Fianna Fáil. At the election the party retained six Dáil seats, making it the most likely partner for Fianna Fáil. Sargent and the party negotiated a coalition government; at the 12 June 2007 membership meeting to approve the agreement, he announced his resignation as leader.

In the subsequent leadership election, John Gormley became the new leader on 17 July 2007, defeating Patricia McKenna by 478 votes to 263. Mary White was subsequently elected as the deputy Leader. Gormley served as Minister for the Environment, Heritage and Local Government from July 2007 until the Green Party's decision to exit government in December 2010.

Following the election defeats of 2011, Gormley announced his intention not to seek another term as Green Party leader. Eamon Ryan was elected as the new party leader, over party colleagues Phil Kearney and Cllr Malcolm Noonan in a postal ballot election of party members in May 2011. Monaghan-based former councillor Catherine Martin defeated Down-based Dr John Barry and former Senator Mark Dearey to the post of deputy leader on 11 June 2011 during the party's annual convention. Roderic O'Gorman was elected party chairperson.

The Green Party lost all its Dáil seats in the 2011 general election. Party Chairman Dan Boyle and Déirdre de Búrca were nominated by the Taoiseach to Seanad Éireann after the formation of the Fianna Fáil–Progressive Democrats–Green Party government in 2007, and Niall Ó Brolcháin was elected in December 2009. De Búrca resigned in February 2010, and was replaced by Mark Dearey. Neither Boyle nor O'Brolchain was re-elected to Seanad Éireann in the Seanad election of 2011, leaving the Green Party without Oireachtas representation until the 2016 general election, in which it regained two Dáil seats.

Ryan's leadership was challenged by deputy leader Catherine Martin in 2020 after the 2020 government formation; he narrowly won a poll of party members, 994 votes (51.2%) to 946.

The Green Party is organised throughout the island of Ireland, with regional structures in both the Republic of Ireland and Northern Ireland. The Green Party in Northern Ireland voted to become a regional partner of the Green Party in Ireland in 2005 at its annual convention, and again in a postal ballot in March 2006. Brian Wilson, formerly a councillor for the Alliance Party, won the Green Party's first seat in the Northern Ireland Assembly in the 2007 election. Steven Agnew held that seat in the 2011 election.




</doc>
<doc id="15085" url="https://en.wikipedia.org/wiki?curid=15085" title="Iconoclasm">
Iconoclasm

Iconoclasm (from Greek: + ) is the social belief in the importance of the destruction of icons and other images or monuments, most frequently for religious or political reasons. People who engage in or support iconoclasm are called iconoclasts, a term that has come to be figuratively applied to any individual who challenges "cherished beliefs or venerated institutions on the grounds that they are erroneous or pernicious."

Conversely, one who reveres or venerates religious images is called (by iconoclasts) an "iconolater"; in a Byzantine context, such a person is called an "iconodule" or "iconophile." Iconoclasm does not generally encompass the destruction of the images of a specific ruler after his or her death or overthrow, a practice better known as "damnatio memoriae".

While iconoclasm may be carried out by adherents of a different religion, it is more commonly the result of sectarian disputes between factions of the same religion. The term originates from the Byzantine Iconoclasm, the struggles between proponents and opponents of religious icons in the Byzantine Empire from 726 to 842 AD. Degrees of iconoclasm vary greatly among religions and their branches, but are strongest in religions which oppose idolatry, including the Abrahamic religions. Outside of the religious context, iconoclasm can refer to movements for widespread destruction in symbols of an ideology or cause, such as the destruction of monarchist symbols during the French Revolution.

In the Bronze Age, the most significant episode of iconoclasm occurred in Egypt during the Amarna Period, when Akhenaten, based in his new capital of Akhetaten, instituted a significant shift in Egyptian artistic styles alongside a campaign of intolerance towards the traditional gods and a new emphasis on a state monolatristic tradition focused on the god Aten, the Sun disk—many temples and monuments were destroyed as a result:

In rebellion against the old religion and the powerful priests of Amun, Akhenaten ordered the eradication of all of Egypt's traditional gods. He sent royal officials to chisel out and destroy every reference to Amun and the names of other deities on tombs, temple walls, and cartouches to instill in the people that the Aten was the one true god.

Public references to Akhenaten were destroyed soon after his death. Comparing the ancient Egyptians with the Israelites, Jan Assmann writes:

For Egypt, the greatest horror was the destruction or abduction of the cult images. In the eyes of the Israelites, the erection of images meant the destruction of divine presence; in the eyes of the Egyptians, this same effect was attained by the destruction of images. In Egypt, iconoclasm was the most terrible religious crime; in Israel, the most terrible religious crime was idolatry. In this respect Osarseph alias Akhenaten, the iconoclast, and the Golden Calf, the paragon of idolatry, correspond to each other inversely, and it is strange that Aaron could so easily avoid the role of the religious criminal. It is more than probable that these traditions evolved under mutual influence. In this respect, Moses and Akhenaten became, after all, closely related.

According to the Hebrew Bible, God instructed the Israelites to "destroy all [the] engraved stones, destroy all [the] molded images, and demolish all [the] high places" of the indigenous Canaanite population as soon as they entered the Promised Land.

In Judaism, King Hezekiah purged Solomon's Temple in Jerusalem and all figures were also destroyed in the Land of Israel, including the Nehushtan, as recorded in the Second Book of Kings. His reforms were reversed in the reign of his son Manasseh.

Scattered expressions of opposition to the use of images have been reported: in 305–306 AD, the Synod of Elvira appeared to endorse iconoclasm; Canon 36 states, "Pictures are not to be placed in churches, so that they do not become objects of worship and adoration." Proscription ceased after the destruction of pagan temples. However, widespread use of Christian iconography only began as Christianity increasingly spread among gentiles after the legalization of Christianity by Roman Emperor Constantine (c. 312 AD). During the process of Christianisation under Constantine, Christian groups destroyed the images and sculptures expressive of the Roman Empire's polytheist state religion.

The period after the reign of Byzantine Emperor Justinian (527–565) evidently saw a huge increase in the use of images, both in volume and quality, and a gathering aniconic reaction.

One notable change within the Byzantine Empire came in 695, when Justinian II's government added a full-face image of Christ on the obverse of imperial gold coins. The change caused the Caliph Abd al-Malik to stop his earlier adoption of Byzantine coin types. He started a purely Islamic coinage with lettering only. A letter by the Patriarch Germanus, written before 726 to two iconoclast bishops, says that "now whole towns and multitudes of people are in considerable agitation over this matter," but there is little written evidence of the debate.

Government-led iconoclasm began with Byzantine Emperor Leo III, who issued a series of edicts between 726 and 730 against the veneration of images. The religious conflict created political and economic divisions in Byzantine society; iconoclasm was generally supported by the Eastern, poorer, non-Greek peoples of the Empire who had to frequently deal with raids from the new Muslim Empire. On the other hand, the wealthier Greeks of Constantinople and the peoples of the Balkan and Italian provinces strongly opposed iconoclasm.

The first iconoclastic wave happened in Wittenberg in the early 1520s under reformers Thomas Müntzer and Andreas Karlstadt. It prompted Martin Luther, then concealing as 'Junker Jörg', to intervene. Luther argued that the mental picturing of Christ when reading the Scriptures was similar in character to artistic renderings of Christ.

In contrast to the Lutherans who favoured sacred art in their churches and homes, the Reformed (Calvinist) leaders, in particular Andreas Karlstadt, Huldrych Zwingli and John Calvin, encouraged the removal of religious images by invoking the Decalogue's prohibition of idolatry and the manufacture of graven (sculpted) images of God. As a result, individuals attacked statues and images. However, in most cases, civil authorities removed images in an orderly manner in the newly Reformed Protestant cities and territories of Europe.

The belief of iconoclasm caused havoc throughout Europe. In 1523, specifically due to the Swiss reformer Huldrych Zwingli, a vast number of his followers viewed themselves as being involved in a spiritual community that in matters of faith should obey neither the visible Church nor lay authorities. According to Peter George Wallace "Zwingli's attack on images, at the first debate, triggered iconoclastic incidents in Zurich and the villages under civic jurisdiction that the reformer was unwilling to condone." Due to this action of protest against authority, "Zwingli responded with a carefully reasoned treatise that men could not live in society without laws and constraint."

Significant iconoclastic riots took place in Basel (in 1529), Zurich (1523), Copenhagen (1530), Münster (1534), Geneva (1535), Augsburg (1537), Scotland (1559), Rouen (1560), and Saintes and La Rochelle (1562). Calvinist iconoclasm in Europe "provoked reactive riots by Lutheran mobs" in Germany and "antagonized the neighbouring Eastern Orthodox" in the Baltic region.

The Seventeen Provinces (now the Netherlands, Belgium, and parts of Northern France) were disrupted by widespread Calvinist iconoclasm in the summer of 1566. This period, known as the "Beeldenstorm", began with the destruction of the statuary of the Monastery of Saint Lawrence in Steenvoorde after a ""Hagenpreek"," or field sermon, by Sebastiaan Matte. Hundreds of other attacks included the sacking of the Monastery of Saint Anthony after a sermon by Jacob de Buysere. The Beeldenstorm marked the start of the revolution against the Spanish forces and the Catholic Church.

During the Reformation in England, which started during the reign of Anglican monarch Henry VIII, and was urged on by reformers such as Hugh Latimer and Thomas Cranmer, limited official action was taken against religious images in churches in the late 1530s. Henry's young son, Edward VI, came to the throne in 1547 and, under Cranmer's guidance, issued injunctions for Religious Reforms in the same year and in 1550, an Act of Parliament "for the abolition and putting away of divers books and images." During the English Civil War, Bishop Joseph Hall of Norwich described the events of 1643 when troops and citizens, encouraged by a Parliamentary ordinance against superstition and idolatry, behaved thus:

Lord what work was here! What clattering of glasses! What beating down of walls! What tearing up of monuments! What pulling down of seats! What wresting out of irons and brass from the windows! What defacing of arms! What demolishing of curious stonework! What tooting and piping upon organ pipes! And what a hideous triumph in the market-place before all the country, when all the mangled organ pipes, vestments, both copes and surplices, together with the leaden cross which had newly been sawn down from the Green-yard pulpit and the service-books and singing books that could be carried to the fire in the public market-place were heaped together.

Protestant Christianity was not uniformly hostile to the use of religious images. Martin Luther taught the "importance of images as tools for instruction and aids to devotion," stating: "If it is not a sin but good to have the image of Christ in my heart, why should it be a sin to have it in my eyes?" Lutheran churches retained ornate church interiors with a prominent crucifix, reflecting their high view of the real presence of Christ in Eucharist. As such, "Lutheran worship became a complex ritual choreography set in a richly furnished church interior." For Lutherans, "the Reformation renewed rather than removed the religious image."

Lutheran scholar Jeremiah Ohl writes:Zwingli and others for the sake of saving the Word rejected all plastic art; Luther, with an equal concern for the Word, but far more conservative, would have all the arts to be the servants of the Gospel. "I am not of the opinion" said [Luther], "that through the Gospel all the arts should be banished and driven away, as some zealots want to make us believe; but I wish to see them all, especially music, in the service of Him Who gave and created them." Again he says: "I have myself heard those who oppose pictures, read from my German Bible.… But this contains many pictures of God, of the angels, of men, and of animals, especially in the Revelation of St. John, in the books of Moses, and in the book of Joshua. We therefore kindly beg these fanatics to permit us also to paint these pictures on the wall that they may be remembered and better understood, inasmuch as they can harm as little on the walls as in books. Would to God that I could persuade those who can afford it to paint the whole Bible on their houses, inside and outside, so that all might see; this would indeed be a Christian work. For I am convinced that it is God's will that we should hear and learn what He has done, especially what Christ suffered. But when I hear these things and meditate upon them, I find it impossible not to picture them in my heart. Whether I want to or not, when I hear, of Christ, a human form hanging upon a cross rises up in my heart: just as I see my natural face reflected when I look into water. Now if it is not sinful for me to have Christ's picture in my heart, why should it be sinful to have it before my eyes?The Ottoman Sultan Suleiman the Magnificent, who had pragmatic reasons to support the Dutch Revolt (the rebels, like himself, were fighting against Spain) also completely approved of their act of "destroying idols," which accorded well with Muslim teachings.

A bit later in Dutch history, in 1627 the artist Johannes van der Beeck was arrested and tortured, charged with being a religious non-conformist and a blasphemer, heretic, atheist, and Satanist. The 25 January 1628 judgment from five noted advocates of The Hague pronounced him guilty of "blasphemy against God and avowed atheism, at the same time as leading a frightful and pernicious lifestyle. At the court's order his paintings were burned, and only a few of them survive."

From the 16th through the 19th centuries, many of the polytheistic religious deities and texts of pre-colonial Americas, Oceania, and Africa were destroyed by Christian missionaries and their converts, such as during the Spanish conquest of the Aztec Empire and the Spanish conquest of the Inca Empire.

Many of the moai of Easter Island were toppled during the 18th century in the iconoclasm of civil wars before any European encounter. Other instances of iconoclasm may have occurred throughout Eastern Polynesia during its conversion to Christianity in the 19th century.

After the Second Vatican Council in the late 20th century, some Roman Catholic parish churches discarded much of their traditional imagery, art, and architecture.

Islam has a much stronger tradition of opposing the depictions of figures, especially religious figures, with Sunni Islam being more opposed than Shia Islam.
In the history of Islam, the act of removing idols from the Ka'ba in Mecca has great symbolic and historic importance for all believers.

In general, Muslim societies have avoided the depiction of living beings (animals and humans) within such sacred spaces as mosques and madrasahs. This opposition to figural representation is based not on the Qur'an, but on traditions contained within the Hadith. The prohibition of figuration has not always been extended to the secular sphere, and a robust tradition of figural representation exists within Muslim art.
However, Western authors have tended to perceive "a long, culturally determined, and unchanging tradition of violent iconoclastic acts" within Islamic society.

The first act of Muslim iconoclasm dates to the beginning of Islam, in 630, when the various statues of Arabian deities housed in the Kaaba in Mecca were destroyed. There is a tradition that Muhammad spared a fresco of Mary and Jesus. This act was intended to bring an end to the idolatry which, in the Muslim view, characterized Jahiliyyah.

The destruction of the idols of Mecca did not, however, determine the treatment of other religious communities living under Muslim rule after the expansion of the caliphate. Most Christians under Muslim rule, for example, continued to produce icons and to decorate their churches as they wished. A major exception to this pattern of tolerance in early Islamic history was the "Edict of Yazīd", issued by the Umayyad caliph Yazīd II in 722–723.
This edict ordered the destruction of crosses and Christian images within the territory of the caliphate. Researchers have discovered evidence that the order was followed, particularly in present-day Jordan, where archaeological evidence shows the removal of images from the mosaic floors of some, although not all, of the churches that stood at this time. But, Yazīd's iconoclastic policies were not continued by his successors, and Christian communities of the Levant continued to make icons without significant interruption from the sixth century to the ninth.

Al-Maqrīzī, writing in the 15th century, attributes the missing nose on the Great Sphinx of Giza to iconoclasm by Muhammad Sa'im al-Dahr, a Sufi Muslim in the mid-1300s. He was reportedly outraged by local Muslims making offerings to the Great Sphinx in the hope of controlling the flood cycle, and he was later executed for vandalism. However, whether this was actually the cause of the missing nose has been debated by historians. Mark Lehner, having performed an archaeological study, concluded that it was broken with instruments at an earlier unknown time between the 3rd and 10th centuries.

Certain conquering Muslim armies have used local temples or houses of worship as mosques. An example is Hagia Sophia in Istanbul (formerly Constantinople), which was converted into a mosque in 1453. Most icons were desecrated and the rest were covered with plaster. In the 1934, It'd decided to use of Hagia Sophia Mosque as a museum,(it was registered as a mosque), and the restoration of the mosaics was undertaken by the American Byzantine Institute beginning in 1932. In July 2020, Hagia Sophia was again converted to a mosque.

Certain Muslim denominations continue to pursue iconoclastic agendas. There has been much controversy within Islam over the recent and apparently on-going destruction of historic sites by Saudi Arabian authorities, prompted by the fear they could become the subject of "idolatry."

A recent act of iconoclasm was the 2001 destruction of the giant Buddhas of Bamyan by the then-Taliban government of Afghanistan. The act generated worldwide protests and was not supported by other Muslim governments and organizations. It was widely perceived in the Western media as a result of the Muslim prohibition against figural decoration. Such an account overlooks "the coexistence between the Buddhas and the Muslim population that marveled at them for over a millennium" before their destruction. The Buddhas had twice in the past been attacked by Nadir Shah and Aurengzeb. According to art historian F. B. Flood, analysis of the Taliban's statements regarding the Buddhas suggest that their destruction was motivated more by political than by theological concerns. Taliban spokespeople have given many different explanations of the motives for the destruction.

During the Tuareg rebellion of 2012, the radical Islamist militia Ansar Dine destroyed various Sufi shrines from the 15th and 16th centuries in the city of Timbuktu, Mali. In 2016, the International Criminal Court (ICC) sentenced Ahmad al-Faqi al-Mahdi, a former member of Ansar Dine, to nine years in prison for this destruction of cultural world heritage. This was the first time that the ICC convicted a person for such a crime.

The short-lived Islamic State of Iraq and the Levant carried out iconoclastic attacks such as the destruction of Shia mosques and shrines. Notable incidents include blowing up the Mosque of the Prophet Yunus (Jonah) and destroying the Shrine to Seth in Mosul.

In early Medieval India, there were numerous recorded instances of temple desecration by Indian kings against rival Indian kingdoms, which involved conflicts between devotees of different Hindu deities, as well as conflicts between Hindus, Buddhists, and Jains. 

In 642, the Pallava king Narasimhavarman I looted a Ganesha temple in the Chalukyan capital of Vatapi. In c. 692, Chalukya armies invaded northern India where they looted temples of Ganga and Yamuna.

In the 8th century, Bengali troops from the Buddhist Pala Empire desecrated temples of Vishnu Vaikuṇṭha, the state deity of Lalitaditya's kingdom in Kashmir. In the early 9th century, Indian Hindu kings from Kanchipuram and the Pandyan king Srimara Srivallabha looted Buddhist temples in Sri Lanka. In the early 10th century, the Pratihara king Herambapala looted an image from a temple in the Sahi kingdom of Kangra, which was later looted by the Pratihara king Yasovarman.

Records from the campaign recorded in the "Chach Nama" record the destruction of temples during the early 8th century when the Umayyad governor of Damascus, al-Hajjaj ibn Yusuf, mobilized an expedition of 6000 cavalry under Muhammad bin Qasim in 712.

Historian Upendra Thakur records the persecution of Hindus and Buddhists:

In the early 11th century, the Chola king Rajendra I looted temples in a number of neighbouring kingdoms, including:


In the mid-11th century, the Chola king Rajadhiraja plundered a temple in Kalyani. In the late 11th century, the Hindu king Harsha of Kashmir plundered temples as an institutionalised activity. In the late 12th to early 13th centuries, the Paramara dynasty attacked and plundered Jain temples in Gujarat. 

Perhaps the most notorious episode of iconoclasm in India was Mahmud of Ghazni's attack on the Somnath temple from across the Thar Desert. The temple was first raided in 725, when Junayad, the governor of Sind, sent his armies to destroy it. In 1024, during the reign of Bhima I, the prominent Turkic-Muslim ruler Mahmud of Ghazni raided Gujarat, plundering the Somnath temple and breaking its jyotirlinga despite pleas by Brahmins not to break it. He took away a booty of 20 million dinars. The attack may have been inspired by the belief that an idol of the goddess Manat had been secretly transferred to the temple. According to the Ghaznavid court-poet Farrukhi Sistani, who claimed to have accompanied Mahmud on his raid, "Somnat" (as rendered in Persian) was a garbled version of "su-manat" referring to the goddess Manat. According to him, as well as a later Ghaznavid historian Abu Sa'id Gardezi, the images of the other goddesses were destroyed in Arabia but the one of Manat was secretly sent away to Kathiawar (in modern Gujarat) for safekeeping. Since the idol of Manat was an aniconic image of black stone, it could have been easily confused with a lingam at Somnath. Mahmud is said to have broken the idol and taken away parts of it as loot and placed so that people would walk on it. In his letters to the Caliphate, Mahmud exaggerated the size, wealth and religious significance of the Somnath temple, receiving grandiose titles from the Caliph in return.

The wooden structure was replaced by Kumarapala (r. 1143–72), who rebuilt the temple out of stone.

Historical records compiled by Muslim historian Maulana Hakim Saiyid Abdul Hai attest to the religious violence during the Mamluk dynasty under Qutb-ud-din Aybak. The first mosque built in Delhi, the "Quwwat al-Islam" was built with demolished parts of 20 Hindu and Jain temples. This pattern of iconoclasm was common during his reign.

During the Delhi Sultanate, a muslim army led by Malik Kafur, a general of Alauddin Khalji, pursued two violent campaigns into south India, between 1309 and 1311, against three Hindu kingdoms of Deogiri (Maharashtra), Warangal (Telangana) and Madurai (Tamil Nadu). Many Temples were plundered; Hoysaleswara Temple was destroyed.

In Kashmir, Sikandar Shah Miri (1389–1413) began expanding, and unleashed religious violence that earned him the name "but-shikan", or 'idol-breaker'. He earned this sobriquet because of the sheer scale of desecration and destruction of Hindu and Buddhist temples, shrines, ashrams, hermitages, and other holy places in what is now known as Kashmir and its neighboring territories. Firishta states, "After the emigration of the Bramins, Sikundur ordered all the temples in Kashmeer to be thrown down." He destroyed vast majority of Hindu and Buddhist temples in his reach in Kashmir region (north and northwest India).

In the 1460s, Kapilendra, founder of the Suryavamsi Gajapati dynasty, sacked the Saiva and Vaishnava temples in the Cauvery delta in the course of wars of conquest in the Tamil country. Vijayanagara king Krishnadevaraya looted a Bala Krishna temple in Udayagiri in 1514, and looted a Vittala temple in Pandharpur in 1520.

A regional tradition, along with the Hindu text "Madala Panji", states that Kalapahad attacked and damaged the Konark Sun Temple
in 1568.

Some of the most dramatic cases of iconoclasm by Muslims are found in parts of India where Hindu and Buddhist temples were razed and mosques erected in their place. Aurangzeb, the 6th Mughal Emperor, destroyed the famous Hindu temples at Varanasi and Mathura.

In modern India, the most high-profile case of iconoclasm was from 1992. Hindus, led by the Vishva Hindu Parishad and Bajrang Dal, destroyed the 430-year-old Islamic Janmasthan Mosque in Ayodhya that was built in 1528 by demolishing a Rama temple.

There have been a number of anti-Buddhist campaigns in Chinese history that led to the destruction of Buddhist temples and images. One of the most notable of these campaigns was the Great Anti-Buddhist Persecution of the Tang dynasty.

During and after the 1911 Xinhai Revolution, there was widespread destruction of religious and secular images in China.

During the Northern Expedition in Guangxi in 1926, Kuomintang General Bai Chongxi led his troops in destroying Buddhist temples and smashing Buddhist images, turning the temples into schools and Kuomintang party headquarters. It was reported that almost all of the viharas in Guangxi were destroyed and the monks were removed. Bai also led a wave of anti-foreignism in Guangxi, attacking Americans, Europeans, and other foreigners, and generally making the province unsafe for foreigners and missionaries. Westerners fled from the province and some Chinese Christians were also attacked as imperialist agents. The three goals of the movement were anti-foreignism, anti-imperialism and anti-religion. Bai led the anti-religious movement against superstition. Huang Shaohong, also a Kuomintang member of the New Guangxi clique, supported Bai's campaign. The anti-religious campaign was agreed upon by all Guangxi Kuomintang members.

There was extensive destruction of religious and secular imagery in Tibet after it was invaded and occupied by China.

Many religious and secular images were destroyed during the Cultural Revolution of 1966-1976, ostensibly because they were a holdover from China's traditional past (which the Communist regime led by Mao Zedong reviled). The Cultural Revolution included widespread destruction of historic artworks in public places and private collections, whether religious or secular. Objects in state museums were mostly left intact.

According to an article in "Buddhist-Christian Studies":Over the course of the last decade [1990s] a fairly large number of Buddhist temples in South Korea have been destroyed or damaged by fire by Christian fundamentalists. More recently, Buddhist statues have been identified as idols, and attacked and decapitated in the name of Jesus. Arrests are hard to effect, as the arsonists and vandals work by stealth of night.

Revolutions and changes of regime, whether through uprising of the local population, foreign invasion, or a combination of both, are often accompanied by the public destruction of statues and monuments identified with the previous regime. This may also be known as "damnatio memoriae", the ancient Roman practice of official obliteration of the memory of a specific individual. Stricter definitions of "iconoclasm" exclude both types of action, reserving the term for religious or more widely cultural destruction. In many cases, such as Revolutionary Russia or Ancient Egypt, this distinction can be hard to make.

Among Roman emperors and other political figures subject to decrees of "damnatio memoriae" were Sejanus, Publius Septimius Geta, and Domitian. Several Emperors, such as Domitian and Commodus had during their reigns erected numerous statues of themselves, which were pulled down and destroyed when they were overthrown.

The perception of "damnatio memoriae" in the Classical world was an act of erasing memory has been challenged by scholars who have argued that it "did not negate historical traces, but created gestures which served to "dishonor" the record of the person and so, in an oblique way, to confirm memory," and was in effect a spectacular display of "pantomime forgetfulness." Examining cases of political monument destruction in modern Irish history, Guy Beiner has demonstrated that iconoclastic vandalism often entails subtle expressions of ambiguous remembrance and that, rather than effacing memory, such acts of de-commemorating effectively preserve memory in obscure forms.

Throughout the radical phase of the French Revolution, iconoclasm was supported by members of the government as well as the citizenry. Numerous monuments, religious works, and other historically significant pieces were destroyed in an attempt to eradicate any memory of the Old Regime. A statue of King Louis XV in the Paris square which until then bore his name, was pulled down and destroyed. This was a prelude to the guillotining of his successor Louis XVI in the same site, renamed "Place de la Révolution" (at present Place de la Concorde). Later that year, the bodies of many French kings were exhumed from the Basilica of Saint-Denis and dumped in a mass grave.

Some episodes of iconoclasm were carried out spontaneously by crowds of citizens, including the destruction of statues of kings during the insurrection of 10 August 1792 in Paris. Some were directly sanctioned by the Republican government, including the Saint-Denis exhumations. Nonetheless, the Republican government also took steps to preserve historic artworks, notably by founding the Louvre museum to house and display the former royal art collection. This allowed the physical objects and national heritage to be preserved while stripping them of their association with the monarchy. Alexandre Lenoir saved many royal monuments by diverting them to preservation in a museum.

The statue of Napoleon on the column at Place Vendôme, Paris was also the target of iconoclasm several times: destroyed after the Bourbon Restoration, restored by Louis-Philippe, destroyed during the Paris Commune and restored by Adolphe Thiers.

Other examples of political destruction of images include:


During and after the October Revolution, widespread destruction of religious and secular imagery in Russia took place, as well as the destruction of imagery related to the Imperial family. The Revolution was accompanied by destruction of monuments of tsars, as well as the destruction of imperial eagles at various locations throughout Russia. According to Christopher Wharton:In front of a Moscow cathedral, crowds cheered as the enormous statue of Tsar Alexander III was bound with ropes and gradually beaten to the ground. After a considerable amount of time, the statue was decapitated and its remaining parts were broken into rubble.The Soviet Union actively destroyed religious sites, including Russian Orthodox churches and Jewish cemeteries, in order to discourage religious practice and curb the activities of religious groups.
During the Hungarian Revolution of 1956 and during the Revolutions of 1989, protesters often attacked and took down sculptures and images of Joseph Stalin, such as the Stalin Monument in Budapest. 

The fall of Communism in 1989-1991 was also followed by the destruction or removal of statues of Vladimir Lenin and other Communist leaders in the former Soviet Union and in other Eastern Bloc countries. Particularly well-known was the destruction of "Iron Felix", the statue of Felix Dzerzhinsky outside the KGB's headquarters. Another statue of Dzerzhinsky was destroyed in a Warsaw square that was named after him during communist rule, but which is now called Bank Square.

During the American Revolution, the Sons of Liberty pulled down and destroyed the gilded lead statue of George III of the United Kingdom on Bowling Green (New York City), melting it down to be recast as ammunition. Similar acts have accompanied the independence of most ex-colonial territories. Sometimes relatively intact monuments are moved to a collected display in a less prominent place, as in India and also post-Communist countries.

In August 2017, a statue of a Confederate soldier dedicated to "the boys who wore the gray" was pulled down from its pedestal in front of Durham County Courthouse in North Carolina by protesters. This followed the events at the 2017 Unite the Right rally in response to growing calls to remove Confederate monuments and memorials across the U.S.

During the George Floyd protests of 2020, demonstrators pulled down dozens of statues which they considered symbols of the Confederacy, slavery, segregation, or racism, including the statue of Williams Carter Wickham in Richmond, Virginia, and the statue of Edward Colston in Bristol in England. 

Further demonstrations in the wake of the George Floyd protests have resulted in the removal of:


Multiple statues have been vandalized, including that of George Washington and Thomas Jefferson, both founding fathers of the United States, as well as statues of abolitionist Frederick Douglass, a slave-turned-freeman, which was vandalized and found in a nearby river; Christopher Columbus in Boston, MA, which was beheaded; Winston Churchill in London, England; and of Pim Fortuyn and of Piet Hein in Rotterdam, the Netherlands. Not only statues were affected, but also museums, such as the Tropenmuseum in Amsterdam, the Netherlands that was smeared with paint and street names in Tilburg were crossed out as well.





</doc>
<doc id="15086" url="https://en.wikipedia.org/wiki?curid=15086" title="IWW (disambiguation)">
IWW (disambiguation)

IWW, or Industrial Workers of the World (known as the Wobblies), are an international union founded in 1905.

IWW may also refer to:



</doc>
<doc id="15087" url="https://en.wikipedia.org/wiki?curid=15087" title="Imbolc">
Imbolc

Imbolc or Imbolg (), also called (Saint) Brigid's Day (, , ), is a Gaelic traditional festival marking the beginning of spring. It is held on 1 February in the northern hemisphere or 1 August in the Southern Hemisphere. It lands about halfway between the winter solstice and the spring equinox. Historically, it was widely observed throughout Ireland, Scotland and the Isle of Man. It is one of the four Gaelic seasonal festivals—along with Beltane, Lughnasadh and Samhain. For Christians, especially in Ireland, it is the feast day of Saint Brigid.

Imbolc is mentioned in early Irish literature, and there is evidence suggesting it was also an important date in ancient times. It is believed that Imbolc was originally a pagan festival associated with the goddess Brigid, and that it was Christianized as a festival of Saint Brigid, who is thought to be a Christianization of the goddess. On Imbolc/St Brigid's Day, Brigid's crosses were made and a doll-like figure of Brigid (a "") would be paraded from house-to-house by girls, sometimes accompanied by 'strawboys'. Brigid was said to visit one's home at Imbolc. To receive her blessings, people would make a bed for Brigid and leave her food and drink, and items of clothing would be left outside for her to bless. Brigid was also invoked to protect homes and livestock. Special feasts were had, holy wells were visited, and it was a time for divination.

Although many of its customs died out in the 20th century, it is still observed and in some places it has been revived as a cultural event. Since the latter 20th century, Celtic neopagans and Wiccans have observed Imbolc as a religious holiday.

The etymology of Imbolc/Imbolg is unclear. The most common explanation is that is comes from the Old Irish "i mbolc" (Modern Irish "i mbolg"), meaning "in the belly", and refers to the pregnancy of ewes. Another possible origin is the Old Irish "imb-fholc", "to wash/cleanse oneself", referring to a ritual cleansing. Eric P. Hamp derives it from a Proto-Indo-European root meaning both "milk" and "cleansing". Professor Alan Ward derives it from the Proto-Celtic "*embibolgon", "budding". The 10th century Cormac's Glossary derives it from "oimelc", "ewe milk", but many scholars see this as a folk etymology. Nevertheless, some Neopagans have adopted "Oimelc" as a name for the festival.

Since Imbolc is immediately followed (on 2 February) by Candlemas (Irish "Lá Fhéile Muire na gCoinneal" "feast day of Mary of the Candles", Welsh "Gŵyl Fair y Canhwyllau"), Irish "Imbolc" is sometimes translated into English as "Candlemas"; e.g. "iar n-imbulc, ba garb a ngeilt" translated as "after Candlemas, rough was their herding".

The date of Imbolc is thought to have been significant in Ireland since the Neolithic period. Some passage tombs in Ireland are aligned with the sunrise around the times of Imbolc and Samhain. This includes the Mound of the Hostages on the Hill of Tara, and Cairn L at Slieve na Calliagh.

In Gaelic Ireland, Imbolc was the "feis" or festival marking the beginning of spring, during which great feasts were held. It is attested in some of the earliest Old Irish literature, from the 10th century onward. It was one of four Gaelic seasonal festivals: Samhain (~1 November), Imbolc (~1 February), Beltane (~1 May) and Lughnasadh (~1 August).

From the 18th century to the mid 20th century, many accounts of Imbolc or St Brigid's Day were recorded by folklorists and other writers. They tell us how it was celebrated then, and shed light on how it may have been celebrated in the past.
Imbolc has traditionally been celebrated on 1 February. However, because the day was deemed to begin and end at sunset, the celebrations would start on what is now 31 January. It has also been argued that the timing of the festival was originally more fluid and based on seasonal changes. It has been associated with the onset of the lambing season (which could vary by as much as two weeks before or after 1 February), the beginning of the spring sowing, and the blooming of blackthorn.

The holiday was a festival of the hearth and home, and a celebration of the lengthening days and the early signs of spring. Celebrations often involved hearthfires, special foods, divination or watching for omens, candles or a bonfire if the weather permitted. Fire and purification were an important part of the festival. The lighting of candles and fires represented the return of warmth and the increasing power of the Sun over the coming months. A spring cleaning was also customary.

Holy wells were visited at Imbolc, and at the other Gaelic festivals of Beltane and Lughnasa. Visitors to holy wells would pray for health while walking 'sunwise' around the well. They would then leave offerings, typically coins or clooties (see clootie well). Water from the well was used to bless the home, family members, livestock and fields.

Donald Alexander Mackenzie also recorded that offerings were made "to earth and sea". The offering could be milk poured into the ground or porridge poured into the water, as a libation.

Imbolc is strongly associated with Saint Brigid (, modern Irish: ', modern Scottish Gaelic: ' or "", anglicised "Bridget"). Saint Brigid is thought to have been based on Brigid, a Gaelic goddess. The festival, which celebrates the onset of spring, is thought to be linked with Brigid in her role as a fertility goddess.

On Imbolc Eve, Brigid was said to visit virtuous households and bless the inhabitants. As Brigid represented the light half of the year, and the power that will bring people from the dark season of winter into spring, her presence was very important at this time of year.

Families would have a special meal or supper on Imbolc Eve. This typically included food such as colcannon, sowans, dumplings, barmbrack or bannocks. Often, some of the food and drink would be set aside for Brigid.

Brigid would be symbolically invited into the house and a bed would often be made for her. In the north of Ireland a family member, representing Brigid, would circle the home three times carrying rushes. They would then knock the door three times, asking to be let in. On the third attempt they are welcomed in, the meal is had, and the rushes are then made into a bed or crosses. In 18th century Mann, the custom was to stand at the door with a bundle of rushes and say "Brede, Brede, come to my house tonight. Open the door for Brede and let Brede come in". The rushes were then strewn on the floor as a carpet or bed for Brigid. In the 19th century, some old Manx women would make a bed for Brigid in the barn with food, ale, and a candle on a table. In the Hebrides in the late 18th century, a bed of hay would be made for Brigid and someone would then call out three times: "'" (", come in; thy bed is ready"). A white wand, usually made of birch, would be set by the bed. It represented the wand that Brigid was said to use to make the vegetation start growing again. In the 19th century, women in the Hebrides would dance while holding a large cloth and calling out "'" (", come over and make your bed"). However, by this time the bed itself was rarely made.

Before going to bed, people would leave items of clothing or strips of cloth outside for Brigid to bless. Ashes from the fire would be raked smooth and, in the morning, they would look for some kind of mark on the ashes as a sign that Brigid had visited. The clothes or strips of cloth would be brought inside, and believed to now have powers of healing and protection.

In Ireland and Scotland, a representation of Brigid would be paraded around the community by girls and young women. Usually it was a doll-like figure known as a ' (also called a 'Breedhoge' or 'Biddy'). It would be made from rushes or reeds and clad in bits of cloth, flowers or shells. In the Hebrides of Scotland, a bright shell or crystal called the ' (guiding star of Brigid) was set on its chest. The girls would carry it in procession while singing a hymn to Brigid. All wore white with their hair unbound as a symbol of purity and youth. They visited every house in the area, where they received either food or more decoration for the . Afterwards, they feasted in a house with the set in a place of honour, and put it to bed with lullabies. In the late 17th century, Catholic families in the Hebrides would make a bed for the out of a basket. When the meal was done, the local young men humbly asked for admission, made obeisance to the , and joined the girls in dancing and merrymaking. In many places, only unwed girls could carry the , but in some both boys and girls carried it. Sometimes, rather than carrying a , a girl impersonated Brigid. Escorted by other girls, she went house-to-house wearing 'Brigid's crown' and carrying 'Brigid's shield' and 'Brigid's cross', all of which were made from rushes. The procession in some places included 'strawboys', who wore conical straw hats, masks and played folk music; much like the wrenboys. Up until the mid-20th century, children in Ireland still went house-to-house asking for pennies for "poor Biddy", or money for the poor. In County Kerry, men in white robes went from house to house singing.

In Ireland, Brigid's crosses ("pictured on the right") were made at Imbolc. A Brigid's cross usually consists of rushes woven into a four-armed equilateral cross, although three-armed crosses have also been recorded. They were often hung over doors, windows and stables to welcome Brigid and for protection against fire, lightning, illness and evil spirits. The crosses were generally left there until the next Imbolc. In western , people would make a "" ('s girdle); a great ring of rushes with a cross woven in the middle. Young boys would carry it around the village, inviting people to step through it and so be blessed.

Today, some people still make Brigid's crosses and s or visit holy wells dedicated to St Brigid on 1 February. Brigid's Day parades have been revived in the town of Killorglin, County Kerry, which holds a yearly "Biddy's Day Festival". Men and women wearing elaborate straw hats and masks visit public houses carrying a to bring good luck for the coming year. They play folk music, dance and sing. The highlight of this festival is a torchlight parade through the town followed by a song and dance contest.

Imbolc was traditionally a time of weather divination, and the old tradition of watching to see if serpents or badgers came from their winter dens may be a forerunner of the North American Groundhog Day. A Scottish Gaelic proverb about the day is:
Imbolc was believed to be when the —the divine hag of Gaelic tradition—gathers her firewood for the rest of the winter. Legend has it that if she wishes to make the winter last a good while longer, she will make sure the weather on Imbolc is bright and sunny, so she can gather plenty of firewood. Therefore, people would be relieved if Imbolc is a day of foul weather, as it means the is asleep and winter is almost over. At Imbolc on the Isle of Man, where she is known as "", the is said to take the form of a gigantic bird carrying sticks in her beak.

Imbolc or Imbolc-based festivals are held by some Neopagans. As there are many kinds of Neopaganism, their Imbolc celebrations can be very different despite the shared name. Some try to emulate the historic festival as much as possible. Other Neopagans base their celebrations on many sources, with historic accounts of Imbolc being only one of them.

Neopagans usually celebrate Imbolc on 1 February in the Northern Hemisphere and 1 August in the Southern Hemisphere. Some Neopagans celebrate it at the astronomical midpoint between the winter solstice and spring equinox (or the full moon nearest this point). In the Northern Hemisphere, this is usually on 3 or 4 February. Other Neopagans celebrate Imbolc when the primroses, dandelions, and other spring flowers emerge.

Celtic Reconstructionists strive to reconstruct the pre-Christian religions of the Celts. Their religious practices are based on research and historical accounts, but may be modified slightly to suit modern life. They avoid syncretism (i.e. combining practises from different cultures). They usually celebrate the festival when the first stirrings of spring are felt, or on the full moon nearest this. Many use traditional songs and rites from sources such as "The Silver Bough" and "The Carmina Gadelica". It is a time of honouring the Goddess Brigid, and many of her dedicants choose this time of year for rituals to her.

Wiccans and Neo-Druids celebrate Imbolc as one of the eight Sabbats in their Wheel of the Year, following Midwinter and preceding Ostara. In Wicca, Imbolc is commonly associated with the goddess Brigid and as such it is sometimes seen as a "women's holiday" with specific rites only for female members of a coven. Among Dianic Wiccans, Imbolc is the traditional time for initiations.






</doc>
<doc id="15088" url="https://en.wikipedia.org/wiki?curid=15088" title="Isaiah">
Isaiah

Isaiah was the 8th-century BC Israelite prophet after whom the Book of Isaiah is named.

Within the text of the Book of Isaiah, Isaiah himself is referred to as "the prophet", but the exact relationship between the Book of Isaiah and any such historical Isaiah is complicated. The traditional view is that all 66 chapters of the book of Isaiah were written by one man, Isaiah, possibly in two periods between 740 BC and c. 686 BC, separated by approximately 15 years, and that the book includes dramatic prophetic declarations of Cyrus the Great in the Bible, acting to restore the nation of Israel from Babylonian captivity. Another widely held view is that parts of the first half of the book (chapters 1–39) originated with the historical prophet, interspersed with prose commentaries written in the time of King Josiah a hundred years later, and that the remainder of the book dates from immediately before and immediately after the end of the exile in Babylon, almost two centuries after the time of the historical prophet.

The first verse of the Book of Isaiah states that Isaiah prophesied during the reigns of Uzziah (or Azariah), Jotham, Ahaz, and Hezekiah, the kings of Judah (). Uzziah's reign was 52 years in the middle of the 8th century BC, and Isaiah must have begun his ministry a few years before Uzziah's death, probably in the 740s BC. Isaiah lived until the fourteenth year of the reign of Hezekiah (who died 698 BC). He may have been contemporary for some years with Manasseh. Thus Isaiah may have prophesied for as long as 64 years.

According to some modern interpretations, Isaiah's wife was called "the prophetess" (), either because she was endowed with the prophetic gift, like Deborah () and Huldah (), or simply because she was the "wife of the prophet". They had three sons, naming the eldest Shear-jashub, meaning "A remnant shall return" (), the next Immanuel, meaning "God with us" (), and the youngest, Maher-Shalal-Hash-Baz, meaning, "Spoil quickly, plunder speedily" ().
Soon after this, Shalmaneser V determined to subdue the kingdom of Israel, taking over and destroying Samaria (722 BC). So long as Ahaz reigned, the kingdom of Judah was untouched by the Assyrian power. But when Hezekiah gained the throne, he was encouraged to rebel "against the king of Assyria" (), and entered into an alliance with the king of Egypt (). The king of Assyria threatened the king of Judah, and at length invaded the land. Sennacherib (701 BC) led a powerful army into Judah. Hezekiah was reduced to despair, and submitted to the Assyrians (). But after a brief interval, war broke out again. Again Sennacherib led an army into Judah, one detachment of which threatened Jerusalem (; ). Isaiah on that occasion encouraged Hezekiah to resist the Assyrians (), whereupon Sennacherib sent a threatening letter to Hezekiah, which he "spread before the LORD" ().

According to the account in 2 Kings 19 (and its derivative account in 2 Chronicles 32) an angel of God fell on the Assyrian army and 185,000 of its men were killed in one night. "Like Xerxes in Greece, Sennacherib never recovered from the shock of the disaster in Judah. He made no more expeditions against either Southern Palestine or Egypt."

The remaining years of Hezekiah's reign were peaceful (). Isaiah probably lived to its close, and possibly into the reign of Manasseh. The time and manner of his death are not specified in either the Bible or other primary sources. The Talmud [Yevamot 49b] says that he suffered martyrdom by being sawn in two under the orders of Manasseh. According to rabbinic literature, Isaiah was the maternal grandfather of Manasseh.

The book of Isaiah, along with the book of Jeremiah, is distinctive in the Hebrew bible for its direct portrayal of the "wrath of the Lord" as presented, for example, in Isaiah 9:19 stating, "Through the wrath of the Lord of hosts is the land darkened, and the people shall be as the fuel of the fire."

The Ascension of Isaiah, a pseudepigraphical Christian text dated to sometime between the end of the 1st century to the beginning of the 3rd, gives a detailed story of Isaiah confronting an evil false prophet and ending with Isaiah being martyred – none of which is attested in the original Biblical account.

Gregory of Nyssa (c. 335–395) believed that the Prophet Isaiah "knew more perfectly than all others the mystery of the religion of the Gospel". Jerome (c. 342–420) also lauds the Prophet Isaiah, saying, "He was more of an Evangelist than a Prophet, because he described all of the Mysteries of the Church of Christ so vividly that you would assume he was not prophesying about the future, but rather was composing a history of past events." Of specific note are the songs of the Suffering Servant, which Christians say are a direct prophetic revelation of the nature, purpose, and detail of the death of Jesus Christ.

The Book of Isaiah is quoted many times by New Testament writers. Ten of those references are about the Suffering Servant, how he will suffer and die to save many from their sins, be buried in a rich man's tomb, and be a light to the Gentiles. The Gospel of John says that Isaiah "saw Jesus’ glory and spoke about him."

The Eastern Orthodox Church celebrates Saint Isaiah the Prophet on May 9.

The Book of Mormon quotes Jesus Christ as stating that "great are the words of Isaiah", and that all things prophesied by Isaiah have been and will be fulfilled. The Book of Mormon and Doctrine and Covenants also quote Isaiah more than any other prophet from the Old Testament. Additionally, members of The Church of Jesus Christ of Latter-day Saints consider the founding of the church by Joseph Smith in the 19th century to be a fulfillment of Isaiah 11, the translation of the Book of Mormon to be a fulfillment of Isaiah 29, and the building of Latter-day Saint temples as a fulfillment of Isaiah 2:2.

Isaiah, or his Arabic name أشعياء (transliterated: "Ishaʻyā), is not mentioned by name in the Quran or the Hadith, but appears frequently as a prophet in Islamic sources, such as Qisas Al-Anbiya and Tafsir. Tabari (310/923) provides the typical accounts for Islamic traditions regarding Isaiah. He is further mentioned and accepted as a prophet by other Islamic scholars such as Ibn Kathir, Al-Tha`labi and Kisa'i and also modern scholars such as Muhammad Asad and Abdullah Yusuf Ali. According to Muslim scholars, Isaiah predicted the coming of Jesus and Muhammad, although the reference to Muhammad is disputed by other religious scholars. Isaiah's narrative in Islamic literature can be divided into three sections. The first establishes Isaiah as a prophet of Israel during the reign of Hezekiah; the second relates Isaiah's actions during the siege of Jerusalem by Sennacherib; and the third warns the nation of coming doom. 
Paralleling the Hebrew Bible, Islamic tradition states that Hezekiah was king in Jerusalem during Isaiah's time. Hezekiah heard and obeyed Isaiah's advice, but could not quell the turbulence in Israel. This tradition maintains that Hezekiah was a righteous man and that the turbulence worsened after him. After the death of the king, Isaiah told the people not to forsake God, and warned Israel to cease from its persistent sin and disobedience. Muslim tradition maintains that the unrighteous of Israel in their anger sought to kill Isaiah. In a death that resembles that attributed to Isaiah in "Lives of the Prophets", Muslim exegesis recounts that Isaiah was martyred by Israelites by being sawn in two.

In the courts of Al-Ma'mun, the seventh Abbasid caliph, Ali al-Ridha, the great grandson of Muhammad and prominent scholar (Imam) of his era, was questioned by the High Jewish Rabbi to prove through the Torah that both Jesus and Muhammad were prophets. Among his several proofs, the Imam references the Book of Isaiah, stating "Sha‘ya (Isaiah), the Prophet, said in the Torah concerning what you and your companions say: ‘I have seen two riders to whom (He) illuminated earth. One of them was on a donkey and the other was on a camel. Who is the rider of the donkey, and who is the rider of the camel?'" The Rabbi was unable to answer with certainty. Al-Ridha goes on to state that "As for the rider of the donkey, he is ‘Isa (Jesus); and as for the rider of the camel, he is Muhammad, may Allah bless him and his family. Do you deny that this (statement) is in the Torah?" The Rabbi responds "No, I do not deny it." 

According to the rabbinic literature, Isaiah was a descendant of the royal house of Judah and Tamar (Sotah 10b). He was the son of Amoz (not to be confused with Prophet Amos), who was the brother of King Amaziah of Judah. (Talmud tractate Megillah 15a).

In February 2018 archaeologist Eilat Mazar announced that she and her team had discovered a small seal impression which reads "[belonging] to Isaiah nvy" (could be reconstructed and read as "[belonging] to Isaiah the prophet") during the Ophel excavations, just south of the Temple Mount in Jerusalem. The tiny bulla was found "only 10 feet away" from where an intact bulla bearing the inscription "[belonging] to King Hezekiah of Judah" was discovered in 2015 by the same team. Although the name "Isaiah" in Paleo-Hebrew alphabet is unmistakable, the damage on the bottom left part of the seal causes difficulties in confirming the word "prophet" or a common Hebrew name "Navi", casting some doubts whether this seal really belongs to the prophet Isaiah.




</doc>
<doc id="15089" url="https://en.wikipedia.org/wiki?curid=15089" title="Interpreted language">
Interpreted language

An interpreted language is a type of programming language for which most of its implementations execute instructions directly and freely, without previously compiling a program into machine-language instructions. The interpreter executes the program directly, translating each statement into a sequence of one or more subroutines, and then into another language (often machine code).

The terms "interpreted language" and "compiled language" are not well defined because, in theory, any programming language can be either interpreted or compiled. In modern programming language implementation, it is increasingly popular for a platform to provide both options.

Interpreted languages can also be contrasted with machine languages. Functionally, both execution and interpretation mean the same thing — fetching the next instruction/statement from the program and executing it. Although interpreted byte code is additionally identical to machine code in form and has an assembler representation, the term "interpreted" is sometimes reserved for "software processed" languages (by virtual machine or emulator) on top of the native (i.e. hardware) processor.

In principle, programs in many languages may be compiled or interpreted, emulated or executed natively, so this designation is applied solely based on common implementation practice, rather than representing an essential property of a language.

Many languages have been implemented using both compilers and interpreters, including BASIC, C, Lisp, and Pascal. Java and C# are compiled into bytecode, the virtual-machine-friendly interpreted language. Lisp implementations can freely mix interpreted and compiled code.

The distinction between a compiler and an interpreter is not always well defined, and many language processors do a combination of both.

In the early days of computing, language design was heavily influenced by the decision to use compiling or interpreting as a mode of execution. For example, Smalltalk (1980), which was designed to be interpreted at run-time, allows generic objects to dynamically interact with each other.

Initially, interpreted languages were compiled line-by-line; that is, each line was compiled as it was about to be executed, and if a loop or subroutine caused certain lines to be executed multiple times, they would be recompiled every time. This has become much less common. Most so-called interpreted languages use an intermediate representation, which combines compiling and interpreting.

Examples include: 

The intermediate representation can be compiled once and for all (as in Java), each time before execution (as in Ruby), or each time a change in the source is detected before execution (as in Python).

Interpreting a language gives implementations some additional flexibility over compiled implementations. Features that are often easier to implement in interpreters than in compilers include:

Furthermore, source code can be read and copied, giving users more freedom.

Disadvantages of interpreted languages are:

Several criteria can be used to determine whether a particular language is likely to be called compiled or interpreted by its users:


These are not definitive. Compiled languages can have interpreter-like properties and vice versa.


Many languages are first compiled to bytecode. Sometimes, bytecode can also be compiled to a native binary using an AOT compiler or executed natively, by hardware processor.



</doc>
<doc id="15095" url="https://en.wikipedia.org/wiki?curid=15095" title="Intifada">
Intifada

An intifada ( "") is a rebellion or uprising, or a resistance movement. It is a key concept in contemporary Arabic usage referring to a legitimate uprising against oppression.

"Intifada" is an Arabic word literally meaning, as a noun, "tremor", "shivering", "shuddering". It is derived from an Arabic term "nafada" meaning "to shake", "shake off", "get rid of", as a dog might shrug off water, or as one might shake off sleep, or dirt from one's sandals.

The concept intifada was first utilized in modern times in 1952 within the Kingdom of Iraq, when socialist and communist parties took to the streets to protest the Hashemite monarchy, with inspiration of the 1952 Egyptian Revolution. 

The concept was adopted in Western Sahara, with the gradual withdrawal of Spanish forces in the 1970s as the Zemla Intifada, but was essentially rooted into the Western Sahara conflict with the First Sahrawi Intifada - protests by Sahrawi activists in the Western Sahara, south of Morocco (1999–2004), Independence Intifada (Western Sahara) or Second Sahrawi Intifada and finally the Gdeim Izik protests in 2011.

In the Palestinian context, the word refers to attempts to "shake off" the Israeli occupation of the West Bank and Gaza Strip in the First and Second Intifadas, where it was originally chosen to connote "aggressive nonviolent resistance", a meaning it bore among Palestinian students in struggles in the 1980s and which they adopted as less confrontational than terms in earlier militant rhetoric since it bore no nuance of violence.

Intifada may refer to these events:




</doc>
<doc id="15097" url="https://en.wikipedia.org/wiki?curid=15097" title="Ionosphere">
Ionosphere

The ionosphere () is the ionized part of Earth's upper atmosphere, from about to altitude, a region that includes the thermosphere and parts of the mesosphere and exosphere. The ionosphere is ionized by solar radiation. It plays an important role in atmospheric electricity and forms the inner edge of the magnetosphere. It has practical importance because, among other functions, it influences radio propagation to distant places on the Earth. 

As early as 1839, the German mathematician and physicist Carl Friedrich Gauss postulated that an electrically conducting region of the atmosphere could account for observed variations of Earth's magnetic field. Sixty years later, Guglielmo Marconi received the first trans-Atlantic radio signal on December 12, 1901, in St. John's, Newfoundland (now in Canada) using a kite-supported antenna for reception. The transmitting station in Poldhu, Cornwall, used a spark-gap transmitter to produce a signal with a frequency of approximately 500 kHz and a power of 100 times more than any radio signal previously produced. The message received was three dits, the Morse code for the letter S. To reach Newfoundland the signal would have to bounce off the ionosphere twice. Dr. Jack Belrose has contested this, however, based on theoretical and experimental work. However, Marconi did achieve transatlantic wireless communications in Glace Bay, Nova Scotia, one year later.

In 1902, Oliver Heaviside proposed the existence of the Kennelly–Heaviside layer of the ionosphere which bears his name. Heaviside's proposal included means by which radio signals are transmitted around the Earth's curvature. . Also in 1902, Arthur Edwin Kennelly discovered some of the ionosphere's radio-electrical properties.

In 1912, the U.S. Congress imposed the Radio Act of 1912 on amateur radio operators, limiting their operations to frequencies above 1.5 MHz (wavelength 200 meters or smaller). . This led to the discovery of HF radio propagation via the ionosphere in 1923.

In 1926, Scottish physicist Robert Watson-Watt introduced the term "ionosphere" in a letter published only in 1969 in "Nature":

In the early 1930s, test transmissions of Radio Luxembourg inadvertently provided evidence of the first radio modification of the ionosphere; HAARP ran a series of experiments in 2017 using the eponymous Luxembourg Effect.

Edward V. Appleton was awarded a Nobel Prize in 1947 for his confirmation in 1927 of the existence of the ionosphere. Lloyd Berkner first measured the height and density of the ionosphere. This permitted the first complete theory of short-wave radio propagation. Maurice V. Wilkes and J. A. Ratcliffe researched the topic of radio propagation of very long radio waves in the ionosphere. Vitaly Ginzburg has developed a theory of electromagnetic wave propagation in plasmas such as the ionosphere.

In 1962, the Canadian satellite Alouette 1 was launched to study the ionosphere. Following its success were Alouette 2 in 1965 and the two ISIS satellites in 1969 and 1971, further AEROS-A and -B in 1972 and 1975, all for measuring the ionosphere.

On July 26, 1963 the first operational geosynchronous satellite Syncom 2 was launched. The board radio beacons on this satellite (and its successors) enabled – for the first time – the measurement of total electron content (TEC) variation along a radio beam from geostationary orbit to an earth receiver. (The rotation of the plane of polarization directly measures TEC along the path.) Australian geophysicist Elizabeth Essex-Cohen from 1969 onwards was using this technique to monitor the atmosphere above Australia and Antarctica.

The ionosphere is a shell of electrons and electrically charged atoms and molecules that surrounds the Earth, stretching from a height of about to more than . It exists primarily due to ultraviolet radiation from the Sun.

The lowest part of the Earth's atmosphere, the troposphere extends from the surface to about . Above that is the stratosphere, followed by the mesosphere. In the stratosphere incoming solar radiation creates the ozone layer. At heights of above , in the thermosphere, the atmosphere is so thin that free electrons can exist for short periods of time before they are captured by a nearby positive ion. The number of these free electrons is sufficient to affect radio propagation. This portion of the atmosphere is partially "ionized" and contains a plasma which is referred to as the ionosphere.

Ultraviolet (UV), X-ray and shorter wavelengths of solar radiation are "ionizing," since photons at these frequencies contain sufficient energy to dislodge an electron from a neutral gas atom or molecule upon absorption. In this process the light electron obtains a high velocity so that the temperature of the created electronic gas is much higher (of the order of thousand K) than the one of ions and neutrals. The reverse process to ionization is recombination, in which a free electron is "captured" by a positive ion. Recombination occurs spontaneously, and causes the emission of a photon carrying away the energy produced upon recombination. As gas density increases at lower altitudes, the recombination process prevails, since the gas molecules and ions are closer together. The balance between these two processes determines the quantity of ionization present.

Ionization depends primarily on the Sun and its activity. The amount of ionization in the ionosphere varies greatly with the amount of radiation received from the Sun. Thus there is a diurnal (time of day) effect and a seasonal effect. The local winter hemisphere is tipped away from the Sun, thus there is less received solar radiation. The activity of the Sun modulates following the solar cycle, with more radiation occurring with more sunspots, with a periodicity of around 11 years. Radiation received also varies with geographical location (polar, auroral zones, mid-latitudes, and equatorial regions). There are also mechanisms that disturb the ionosphere and decrease the ionization. There are disturbances such as solar flares and the associated release of charged particles into the solar wind which reaches the Earth and interacts with its geomagnetic field.

Sydney Chapman proposed that the region below the ionosphere be called neutrosphere
(the neutral atmosphere).
At night the F layer is the only layer of significant ionization present, while the ionization in the E and D layers is extremely low. During the day, the D and E layers become much more heavily ionized, as does the F layer, which develops an additional, weaker region of ionisation known as the F layer. The F layer persists by day and night and is the main region responsible for the refraction and reflection of radio waves.

The D layer is the innermost layer, to above the surface of the Earth. Ionization here is due to Lyman series-alpha hydrogen radiation at a wavelength of 121.6 nanometre (nm) ionizing nitric oxide (NO). In addition, high solar activity can generate hard X-rays (wavelength ) that ionize N and O. Recombination rates are high in the D layer, so there are many more neutral air molecules than ions.

Medium frequency (MF) and lower high frequency (HF) radio waves are significantly attenuated within the D layer, as the passing radio waves cause electrons to move, which then collide with the neutral molecules, giving up their energy. Lower frequencies experience greater absorption because they move the electrons farther, leading to greater chance of collisions. This is the main reason for absorption of HF radio waves, particularly at 10 MHz and below, with progressively less absorption at higher frequencies. This effect peaks around noon and is reduced at night due to a decrease in the D layer's thickness; only a small part remains due to cosmic rays. A common example of the D layer in action is the disappearance of distant AM broadcast band stations in the daytime.

During solar proton events, ionization can reach unusually high levels in the D-region over high and polar latitudes. Such very rare events are known as Polar Cap Absorption (or PCA) events, because the increased ionization significantly enhances the absorption of radio signals passing through the region. In fact, absorption levels can increase by many tens of dB during intense events, which is enough to absorb most (if not all) transpolar HF radio signal transmissions. Such events typically last less than 24 to 48 hours.

The E layer is the middle layer, to above the surface of the Earth. Ionization is due to soft X-ray (1–10 nm) and far ultraviolet (UV) solar radiation ionization of molecular oxygen (O). Normally, at oblique incidence, this layer can only reflect radio waves having frequencies lower than about 10 MHz and may contribute a bit to absorption on frequencies above. However, during intense sporadic E events, the E layer can reflect frequencies up to 50 MHz and higher. The vertical structure of the E layer is primarily determined by the competing effects of ionization and recombination. At night the E layer weakens because the primary source of ionization is no longer present. After sunset an increase in the height of the E layer maximum increases the range to which radio waves can travel by reflection from the layer.

This region is also known as the Kennelly–Heaviside layer or simply the Heaviside layer. Its existence was predicted in 1902 independently and almost simultaneously by the American electrical engineer Arthur Edwin Kennelly (1861–1939) and the British physicist Oliver Heaviside (1850–1925). In 1924 that its existence was detected by Edward V. Appleton and Miles Barnett.

The E layer (sporadic E-layer) is characterized by small, thin clouds of intense ionization, which can support reflection of radio waves, rarely up to 225 MHz. Sporadic-E events may last for just a few minutes to several hours. Sporadic E propagation makes VHF-operating radio amateurs very excited, as propagation paths that are generally unreachable can open up. There are multiple causes of sporadic-E that are still being pursued by researchers. This propagation occurs most frequently during the summer months when high signal levels may be reached. The skip distances are generally around . Distances for one hop propagation can be anywhere from to . Double-hop reception over is possible.

The F layer or region, also known as the Appleton–Barnett layer, extends from about to more than above the surface of Earth. It is the layer with the highest electron density, which implies signals penetrating this layer will escape into space. Electron production is dominated by extreme ultraviolet (UV, 10–100 nm) radiation ionizing atomic oxygen. The F layer consists of one layer (F) at night, but during the day, a secondary peak (labelled F) often forms in the electron density profile. Because the F layer remains by day and night, it is responsible for most skywave propagation of radio waves and long distance high frequency (HF, or shortwave) radio communications.

Above the F layer, the number of oxygen ions decreases and lighter ions such as hydrogen and helium become dominant. This region above the F layer peak and below the plasmasphere is called the topside ionosphere.

From 1972 to 1975 NASA launched the AEROS and AEROS B satellites to study the F region.

An ionospheric model is a mathematical description of the ionosphere as a function of location, altitude, day of year, phase of the sunspot cycle and geomagnetic activity. Geophysically, the state of the ionospheric plasma may be described by four parameters: "electron density, electron and ion temperature" and, since several species of ions are present, "ionic composition". Radio propagation depends uniquely on electron density.

Models are usually expressed as computer programs. The model may be based on basic physics of the interactions of the ions and electrons with the neutral atmosphere and sunlight, or it may be a statistical description based on a large number of observations or a combination of physics and observations. One of the most widely used models is the International Reference Ionosphere (IRI), which is based on data and specifies the four parameters just mentioned. The IRI is an international project sponsored by the Committee on Space Research (COSPAR) and the International Union of Radio Science (URSI). The major data sources are the worldwide network of ionosondes, the powerful incoherent scatter radars (Jicamarca, Arecibo, Millstone Hill, Malvern, St Santin), the ISIS and Alouette topside sounders, and in situ instruments on several satellites and rockets. IRI is updated yearly. IRI is more accurate in describing the variation of the electron density from bottom of the ionosphere to the altitude of maximum density than in describing the total electron content (TEC). Since 1999 this model is "International Standard" for the terrestrial ionosphere (standard TS16457).

Ionograms allow deducing, via computation, the true shape of the different layers. Nonhomogeneous structure of the electron/ion-plasma produces rough echo traces, seen predominantly at night and at higher latitudes, and during disturbed conditions.

At mid-latitudes, the F layer daytime ion production is higher in the summer, as expected, since the Sun shines more directly on the Earth. However, there are seasonal changes in the molecular-to-atomic ratio of the neutral atmosphere that cause the summer ion loss rate to be even higher. The result is that the increase in the summertime loss overwhelms the increase in summertime production, and total F ionization is actually lower in the local summer months. This effect is known as the winter anomaly. The anomaly is always present in the northern hemisphere, but is usually absent in the southern hemisphere during periods of low solar activity.

Within approximately ± 20 degrees of the "magnetic equator", is the "equatorial anomaly". It is the occurrence of a trough in the ionization in the F layer at the equator and crests at about 17 degrees in magnetic latitude. The Earth's magnetic field lines are horizontal at the magnetic equator. Solar heating and tidal oscillations in the lower ionosphere move plasma up and across the magnetic field lines. This sets up a sheet of electric current in the E region which, with the horizontal magnetic field, forces ionization up into the F layer, concentrating at ± 20 degrees from the magnetic equator. This phenomenon is known as the "equatorial fountain".

The worldwide solar-driven wind results in the so-called Sq (solar quiet) current system in the E region of the Earth's ionosphere (ionospheric dynamo region) ( altitude). Resulting from this current is an electrostatic field directed west–east (dawn–dusk) in the equatorial day side of the ionosphere. At the magnetic dip equator, where the geomagnetic field is horizontal, this electric field results in an enhanced eastward current flow within ± 3 degrees of the magnetic equator, known as the equatorial electrojet.

When the Sun is active, strong solar flares can occur that hit the sunlit side of Earth with hard X-rays. The X-rays penetrate to the D-region, releasing electrons that rapidly increase absorption, causing a high frequency (3–30 MHz) radio blackout. During this time very low frequency (3–30 kHz) signals will be reflected by the D layer instead of the E layer, where the increased atmospheric density will usually increase the absorption of the wave and thus dampen it. As soon as the X-rays end, the sudden ionospheric disturbance (SID) or radio black-out ends as the electrons in the D-region recombine rapidly and signal strengths return to normal.

Associated with solar flares is a release of high-energy protons. These particles can hit the Earth within 15 minutes to 2 hours of the solar flare. The protons spiral around and down the magnetic field lines of the Earth and penetrate into the atmosphere near the magnetic poles increasing the ionization of the D and E layers. PCA's typically last anywhere from about an hour to several days, with an average of around 24 to 36 hours. Coronal mass ejections can also release energetic protons that enhance D-region absorption in the polar regions.

A geomagnetic storm is a temporary intense disturbance of the Earth's magnetosphere.

Lightning can cause ionospheric perturbations in the D-region in one of two ways. The first is through VLF (very low frequency) radio waves launched into the magnetosphere. These so-called "whistler" mode waves can interact with radiation belt particles and cause them to precipitate onto the ionosphere, adding ionization to the D-region. These disturbances are called "lightning-induced electron precipitation" (LEP) events.

Additional ionization can also occur from direct heating/ionization as a result of huge motions of charge in lightning strikes. These events are called early/fast.

In 1925, C. T. R. Wilson proposed a mechanism by which electrical discharge from lightning storms could propagate upwards from clouds to the ionosphere. Around the same time, Robert Watson-Watt, working at the Radio Research Station in Slough, UK, suggested that the ionospheric sporadic E layer (E) appeared to be enhanced as a result of lightning but that more work was needed. In 2005, C. Davis and C. Johnson, working at the Rutherford Appleton Laboratory in Oxfordshire, UK, demonstrated that the E layer was indeed enhanced as a result of lightning activity. Their subsequent research has focused on the mechanism by which this process can occur.

Due to the ability of ionized atmospheric gases to refract high frequency (HF, or shortwave) radio waves, the ionosphere can reflect radio waves directed into the sky back toward the Earth. Radio waves directed at an angle into the sky can return to Earth beyond the horizon. This technique, called "skip" or "skywave" propagation, has been used since the 1920s to communicate at international or intercontinental distances. The returning radio waves can reflect off the Earth's surface into the sky again, allowing greater ranges to be achieved with multiple hops. This communication method is variable and unreliable, with reception over a given path depending on time of day or night, the seasons, weather, and the 11-year sunspot cycle. During the first half of the 20th century it was widely used for transoceanic telephone and telegraph service, and business and diplomatic communication. Due to its relative unreliability, shortwave radio communication has been mostly abandoned by the telecommunications industry, though it remains important for high-latitude communication where satellite-based radio communication is not possible. Some broadcasting stations and automated services still use shortwave radio frequencies, as do radio amateur hobbyists for private recreational contacts.

When a radio wave reaches the ionosphere, the electric field in the wave forces the electrons in the ionosphere into oscillation at the same frequency as the radio wave. Some of the radio-frequency energy is given up to this resonant oscillation. The oscillating electrons will then either be lost to recombination or will re-radiate the original wave energy. Total refraction can occur when the collision frequency of the ionosphere is less than the radio frequency, and if the electron density in the ionosphere is great enough.

A qualitative understanding of how an electromagnetic wave propagates through the ionosphere can be obtained by recalling geometric optics. Since the ionosphere is a plasma, it can be shown that the refractive index is less than unity. Hence, the electromagnetic "ray" is bent away from the normal rather than toward the normal as would be indicated when the refractive index is greater than unity. It can also be shown that the refractive index of a plasma, and hence the ionosphere, is frequency-dependent, see Dispersion (optics).

The critical frequency is the limiting frequency at or below which a radio wave is reflected by an ionospheric layer at vertical incidence. If the transmitted frequency is higher than the plasma frequency of the ionosphere, then the electrons cannot respond fast enough, and they are not able to re-radiate the signal. It is calculated as shown below:

where N = electron density per m and f is in Hz.

The Maximum Usable Frequency (MUF) is defined as the upper frequency limit that can be used for transmission between two points at a specified time.

where formula_3 = angle of attack, the angle of the wave relative to the horizon, and sin is the sine function.

The cutoff frequency is the frequency below which a radio wave fails to penetrate a layer of the ionosphere at the incidence angle required for transmission between two specified points by refraction from the layer.

There are a number of models used to understand the effects of the ionosphere global navigation satellite systems. The Klobuchar model is currently used to compensate for ionospheric effects in GPS. This model was developed at the US Air Force Geophysical Research Laboratory circa 1974 by John (Jack) Klobuchar. The Galileo navigation system uses the NeQuick model.

The open system electrodynamic tether, which uses the ionosphere, is being researched. The space tether uses plasma contactors and the ionosphere as parts of a circuit to extract energy from the Earth's magnetic field by electromagnetic induction.

Scientists explore the structure of the ionosphere by a wide variety of methods. They include:

A variety of experiments, such as HAARP (High Frequency Active Auroral Research Program), involve high power radio transmitters to modify the properties of the ionosphere. These investigations focus on studying the properties and behavior of ionospheric plasma, with particular emphasis on being able to understand and use it to enhance communications and surveillance systems for both civilian and military purposes. HAARP was started in 1993 as a proposed twenty-year experiment, and is currently active near Gakona, Alaska.

The SuperDARN radar project researches the high- and mid-latitudes using coherent backscatter of radio waves in the 8 to 20 MHz range. Coherent backscatter is similar to Bragg scattering in crystals and involves the constructive interference of scattering from ionospheric density irregularities. The project involves more than 11 different countries and multiple radars in both hemispheres.

Scientists are also examining the ionosphere by the changes to radio waves, from satellites and stars, passing through it. The Arecibo radio telescope located in Puerto Rico, was originally intended to study Earth's ionosphere.

Ionograms show the virtual heights and critical frequencies of the ionospheric layers and which are measured by an ionosonde. An ionosonde sweeps a range of frequencies, usually from 0.1 to 30 MHz, transmitting at vertical incidence to the ionosphere. As the frequency increases, each wave is refracted less by the ionization in the layer, and so each penetrates further before it is reflected. Eventually, a frequency is reached that enables the wave to penetrate the layer without being reflected. For ordinary mode waves, this occurs when the transmitted frequency just exceeds the peak plasma, or critical, frequency of the layer. Tracings of the reflected high frequency radio pulses are known as ionograms. Reduction rules are given in: "URSI Handbook of Ionogram Interpretation and Reduction", edited by William Roy Piggott and Karl Rawer, Elsevier Amsterdam, 1961 (translations into Chinese, French, Japanese and Russian are available).

Incoherent scatter radars operate above the critical frequencies. Therefore, the technique allows probing the ionosphere, unlike ionosondes, also above the electron density peaks. The thermal fluctuations of the electron density scattering the transmitted signals lack coherence, which gave the technique its name. Their power spectrum contains information not only on the density, but also on the ion and electron temperatures, ion masses and drift velocities.

Radio occultation is a remote sensing technique where a GNSS signal tangentially scrapes the Earth, passing through the atmosphere, and is received by a Low Earth Orbit (LEO) satellite. As the signal passes through the atmosphere, it is refracted, curved and delayed. An LEO satellite samples the total electron content and bending angle of many such signal paths as it watches the GNSS satellite rise or set behind the Earth. Using an Inverse Abel's transform, a radial profile of refractivity at that tangent point on earth can be reconstructed.

Major GNSS radio occultation missions include the GRACE, CHAMP, and COSMIC.

In empirical models of the ionosphere such as Nequick, the following indices are used as indirect indicators of the state of the ionosphere.

F10.7 and R12 are two indices commonly used in ionospheric modelling. Both are valuable for their long historical records covering multiple solar cycles. F10.7 is a measurement of the intensity of solar radio emissions at a frequency of 2800 MHz made using a ground radio telescope. R12 is a 12 months average of daily sunspot numbers. Both indices have been shown to be correlated to each other.

However, both indices are only indirect indicators of solar ultraviolet and X-ray emissions, which are primarily responsible for causing ionization in the Earth's upper atmosphere. We now have data from the GOES spacecraft that measures the background X-ray flux from the Sun, a parameter more closely related to the ionization levels in the ionosphere.


Objects in the Solar System that have appreciable atmospheres (i.e., all of the major planets and many of the larger natural satellites) generally produce ionospheres. Planets known to have ionospheres include Venus, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.

The atmosphere of Titan includes an ionosphere that ranges from about to in altitude and contains carbon compounds. Ionospheres have also been observed at Io, Europa, Ganymede, and Triton.





</doc>
<doc id="15100" url="https://en.wikipedia.org/wiki?curid=15100" title="Interlingua">
Interlingua

Interlingua (; ISO 639 language codes "ia", "ina") is an Italic international auxiliary language (IAL), developed between 1937 and 1951 by the International Auxiliary Language Association (IALA). It ranks among the top most widely used IALs, and is the most widely used naturalistic IAL: in other words, its vocabulary, grammar and other characteristics are derived from natural languages, rather than being centrally planned. Interlingua was developed to combine a simple, mostly regular grammar with a vocabulary common to the widest possible range of western European languages, making it unusually easy to learn, at least for those whose native languages were sources of Interlingua's vocabulary and grammar. Conversely, it is used as a rapid introduction to many natural languages.

Interlingua literature maintains that (written) Interlingua is comprehensible to the hundreds of millions of people who speak Romance languages, though it is actively spoken by only a few hundred.

The name Interlingua comes from the Latin words ', meaning "between", and ', meaning "tongue" or "language". These morphemes are identical in Interlingua. Thus, "Interlingua" would mean "between language".

The expansive movements of science, technology, trade, diplomacy, and the arts, combined with the historical dominance of the Greek and Latin languages have resulted in a large common vocabulary among European languages. With Interlingua, an objective procedure is used to extract and standardize the most widespread word or words for a concept found in a set of primary control languages: English, French, Italian, Spanish and Portuguese, with German and Russian as secondary control languages. Words from any language are eligible for inclusion, so long as their internationality is shown by their presence in these control languages. Hence, Interlingua includes such diverse word forms as Japanese "geisha" and "samurai", Arabic "califa", Guugu Yimithirr "gangurru" (Interlingua: kanguru), and Finnish "sauna".

Interlingua combines this pre-existing vocabulary with a minimal grammar based on the control languages. People with a good knowledge of a Romance language, or a smattering of a Romance language plus a good knowledge of the "international scientific vocabulary" can frequently understand it immediately on reading or hearing it. The immediate comprehension of Interlingua, in turn, makes it unusually easy to learn. Speakers of other languages can also learn to speak and write Interlingua in a short time, thanks to its simple grammar and regular word formation using a small number of roots and affixes.

Once learned, Interlingua can be used to learn other related languages quickly and easily, and in some studies, even to understand them immediately. Research with Swedish students has shown that, after learning Interlingua, they can translate elementary texts from Italian, Portuguese, and Spanish. In one 1974 study, an Interlingua class translated a Spanish text that students who had taken 150 hours of Spanish found too difficult to understand. Gopsill has suggested that Interlingua's freedom from irregularities allowed the students to grasp the mechanisms of language quickly.

The American heiress Alice Vanderbilt Morris (1874–1950) became interested in linguistics and the international auxiliary language movement in the early 1920s, and in 1924, Morris and her husband, Dave Hennen Morris, established the non-profit International Auxiliary Language Association (IALA) in New York City. Their aim was to place the study of IALs on a scientific basis. Morris developed the research program of IALA in consultation with Edward Sapir, William Edward Collinson, and Otto Jespersen.

The IALA became a major supporter of mainstream American linguistics. Numerous studies by Sapir, Collinson, and Morris Swadesh in the 1930s and 1940s, for example, were funded by IALA. Alice Morris edited several of these studies and provided much of IALA's financial support. IALA also received support from such prestigious groups as the Carnegie Corporation, the Ford Foundation, the Research Corporation, and the Rockefeller Foundation.

In its early years, IALA concerned itself with three tasks: finding other organizations around the world with similar goals; building a library of books about languages and interlinguistics; and comparing extant IALs, including Esperanto, Esperanto II, Ido, Peano's Interlingua (Latino sine flexione), Novial, and Interlingue (Occidental). In pursuit of the last goal, it conducted parallel studies of these languages, with comparative studies of national languages, under the direction of scholars at American and European universities. It also arranged conferences with proponents of these IALs, who debated features and goals of their respective languages. With a "concession rule" that required participants to make a certain number of concessions, early debates at IALA sometimes grew from heated to explosive.

At the Second International Interlanguage Congress, held in Geneva in 1931, IALA began to break new ground; 27 recognized linguists signed a testimonial of support for IALA's research program. An additional eight added their signatures at the third congress, convened in Rome in 1933. That same year, Herbert N. Shenton and Edward L. Thorndike became influential in IALA's work by authoring key studies in the interlinguistic field.

The first steps towards the finalization of Interlingua were taken in 1937, when a committee of 24 eminent linguists from 19 universities published "Some Criteria for an International Language and Commentary". However, the outbreak of World War II in 1939 cut short the intended biannual meetings of the committee.

Originally, the association had not set out to create its own language. Its goal was to identify which auxiliary language already available was best suited for international communication, and how to promote it most effectively. However, after ten years of research, more and more members of IALA concluded that none of the existing interlanguages were up to the task. By 1937, the members had made the decision to create a new language, to the surprise of the world's interlanguage community.

To that point, much of the debate had been equivocal on the decision to use naturalistic (e.g., Peano's Interlingua, Novial and Occidental) or systematic (e.g., Esperanto and Ido) words. During the war years, proponents of a naturalistic interlanguage won out. The first support was Thorndike's paper; the second was a concession by proponents of the systematic languages that thousands of words were already present in many, or even a majority, of the European languages. Their argument was that systematic derivation of words was a Procrustean bed, forcing the learner to unlearn and re-memorize a new derivation scheme when a usable vocabulary was already available. This finally convinced supporters of the systematic languages, and IALA from that point assumed the position that a naturalistic language would be best.

IALA's research activities were based in Liverpool, before relocating to New York due to the outbreak of World War II, where E. Clark Stillman established a new research staff. Stillman, with the assistance of Alexander Gode, developed a "prototyping" technique – an objective methodology for selecting and standardizing vocabulary based on a comparison of "control languages".

In 1943 Stillman left for war work and Gode became Acting Director of Research. IALA began to develop models of the proposed language, the first of which were presented in Morris's "General Report" in 1945.

From 1946 to 1948, French linguist André Martinet was Director of Research. During this period IALA continued to develop models and conducted polling to determine the optimal form of the final language. In 1946, IALA sent an extensive survey to more than 3,000 language teachers and related professionals on three continents.

Four models were canvassed:

The results of the survey were striking. The two more schematic models were rejected – K overwhelmingly. Of the two naturalistic models, M received somewhat more support than P. IALA decided on a compromise between P and M, with certain elements of C.

Martinet took up a position at Columbia University in 1948, and Gode took on the last phase of Interlingua's development. The vocabulary and grammar of Interlingua were first presented in 1951, when IALA published the finalized "" and the 27,000-word "Interlingua–English Dictionary" (IED). In 1954, IALA published an introductory manual entitled "Interlingua a Prime Vista" ("Interlingua at First Sight").

Interlingua as presented by the IALA is very close to Peano's Interlingua (Latino sine flexione), both in its grammar and especially in its vocabulary. Accordingly, the very name "Interlingua" was kept, yet a distinct abbreviation was adopted: IA instead of IL.

An early practical application of Interlingua was the scientific newsletter "Spectroscopia Molecular", published from 1952 to 1980. In 1954, Interlingua was used at the Second World Cardiological Congress in Washington, D.C. for both written summaries and oral interpretation. Within a few years, it found similar use at nine further medical congresses. Between the mid-1950s and the late 1970s, some thirty scientific and especially medical journals provided article summaries in Interlingua. Science Service, the publisher of "Science Newsletter" at the time, published a monthly column in Interlingua from the early 1950s until Gode's death in 1970. In 1967, the International Organization for Standardization, which normalizes terminology, voted almost unanimously to adopt Interlingua as the basis for its dictionaries.

The IALA closed its doors in 1953 but was not formally dissolved until 1956 or later. Its role in promoting Interlingua was largely taken on by Science Service, which hired Gode as head of its newly formed Interlingua Division. Hugh E. Blair, Gode's close friend and colleague, became his assistant. A successor organization, the Interlingua Institute, was founded in 1970 to promote Interlingua in the US and Canada. The new institute supported the work of other linguistic organizations, made considerable scholarly contributions and produced Interlingua summaries for scholarly and medical publications. One of its largest achievements was two immense volumes on phytopathology produced by the American Phytopathological Society in 1976 and 1977.

Interlingua had attracted many former adherents of other international-language projects, notably Occidental and Ido. The former Occidentalist Ric Berger founded The Union Mundial pro Interlingua (UMI) in 1955, and by the late 1950s, interest in Interlingua in Europe had already begun to overtake that in North America.

Beginning in the 1980s, UMI has held international conferences every two years (typical attendance at the earlier meetings was 50 to 100) and launched a publishing programme that eventually produced over 100 volumes. Other Interlingua-language works were published by university presses in Sweden and Italy, and in the 1990s, Brazil and Switzerland. Several Scandinavian schools undertook projects that used Interlingua as a means of teaching the international scientific and intellectual vocabulary.

In 2000, the Interlingua Institute was dissolved amid funding disputes with the UMI; the American Interlingua Society, established the following year, succeeded the institute and responded to new interest emerging in Mexico.

Interlingua was spoken and promoted in the Soviet bloc, despite attempts to suppress the language. In East Germany, government officials confiscated the letters and magazines that the UMI sent to Walter Rädler, the Interlingua representative there.

In Czechoslovakia, Július Tomin published his first article on Interlingua in the Slovak magazine "Príroda a spoločnosť" (Nature and Society) in 1971, after which he received several anonymous threatening letters. He went on to become the Czech Interlingua representative, teach Interlingua in the school system, and publish a series of articles and books.

Today, interest in Interlingua has expanded from the scientific community to the general public. Individuals, governments, and private companies use Interlingua for learning and instruction, travel, online publishing, and communication across language barriers. Interlingua is promoted internationally by the Union Mundial pro Interlingua. Periodicals and books are produced by many national organizations, such as the Societate American pro Interlingua, the Svenska Sällskapet för Interlingua, and the Union Brazilian pro Interlingua.

It is not certain how many people have an active knowledge of Interlingua. As noted above, Interlingua is claimed to be the most widely spoken naturalistic auxiliary language.

Interlingua's greatest advantage is that it is the most widely "understood" international auxiliary language besides Interlingua (IL) de A.p.I. by virtue of its naturalistic (as opposed to schematic) grammar and vocabulary, allowing those familiar with a Romance language, and educated speakers of English, to read and understand it without prior study.

Interlingua has active speakers on all continents, especially in South America and in Eastern and Northern Europe, most notably Scandinavia; also in Russia and Ukraine. There are copious Interlingua web pages, including editions of Wikipedia and Wiktionary, and a number of periodicals, including "Panorama in Interlingua" from the Union Mundial pro Interlingua (UMI) and magazines of the national societies allied with it. There are several active mailing lists, and Interlingua is also in use in certain Usenet newsgroups, particularly in the europa.* hierarchy. Interlingua is presented on CDs, radio, and television.

Interlingua is taught in many high schools and universities, sometimes as a means of teaching other languages quickly, presenting interlinguistics, or introducing the international vocabulary. The University of Granada in Spain, for example, offers an Interlingua course in collaboration with the Centro de Formación Continua.

Every two years, the UMI organizes an international conference in a different country. In the year between, the Scandinavian Interlingua societies co-organize a conference in Sweden. National organizations such as the Union Brazilian pro Interlingua also organize regular conferences.

, Google Keyboard supports Interlingua.

Interlingua has a largely phonemic orthography.

Interlingua uses the 26 letters of the ISO basic Latin alphabet with no diacritics. The alphabet, pronunciation in IPA and letter name in Interlingua are:


The book "Grammar of Interlingua" defines in §15 a "collateral orthography".

Interlingua is primarily a written language, and the pronunciation is not entirely settled. The sounds in parentheses are not used by all speakers.

For the most part, consonants are pronounced as in English, while the vowels are like Spanish. Written double consonants may be geminated as in Italian for extra clarity or pronounced as single as in English or French. Interlingua has five falling diphthongs, , and , although and are rare.

The "general rule" is that stress falls on the vowel before the last consonant (e.g., "lingua", 'language', "esser", 'to be', "requirimento", 'requirement') ignoring the final plural "-(e)s" (e.g. "linguas", the plural of "lingua", still has the same stress as the singular), and where that is not possible, on the first vowel ("via", 'way', "io crea", 'I create'). There are a few exceptions, and the following rules account for most of them:

Speakers may pronounce all words according to the general rule mentioned above. For example, "kilometro" is acceptable, although "kilometro" is more common.

Interlingua has no explicitly defined phonotactics. However, the prototyping procedure for determining Interlingua words, which strives for internationality, should in general lead naturally to words that are easy for most learners to pronounce. In the process of forming new words, an ending cannot always be added without a modification of some kind in between. A good example is the plural "-s", which is always preceded by a vowel to prevent the occurrence of a hard-to-pronounce consonant cluster at the end. If the singular does not end in a vowel, the final "-s" becomes "-es."

Unassimilated foreign loanwords, or borrowed words, are spelled as in their language of origin. Their spelling may contain diacritics, or accent marks. If the diacritics do not affect pronunciation, they are removed.

Words in Interlingua may be taken from any language, as long as their internationality is verified by their presence in seven "control" languages: Spanish, Portuguese, Italian, French, and English, with German and Russian acting as secondary controls. These are the most widely spoken Romance, Germanic, and Slavic languages, respectively. Because of their close relationship, Spanish and Portuguese are treated as one unit. The largest number of Interlingua words are of Latin origin, with the Greek and Germanic languages providing the second and third largest number. The remainder of the vocabulary originates in Slavic and non-Indo-European languages.

A word, that is a form with meaning, is eligible for the Interlingua vocabulary if it is verified by at least three of the four primary control languages. Either secondary control language can substitute for a primary language. Any word of Indo-European origin found in a control language can contribute to the eligibility of an international word. In some cases, the archaic or "potential" presence of a word can contribute to its eligibility.

A word can be potentially present in a language when a derivative is present, but the word itself is not. English "proximity", for example, gives support to Interlingua "proxime", meaning 'near, close'. This counts as long as one or more control languages actually have this basic root word, which the Romance languages all do. Potentiality also occurs when a concept is represented as a compound or derivative in a control language, the morphemes that make it up are themselves international, and the combination adequately conveys the meaning of the larger word. An example is Italian "fiammifero" (lit. flamebearer), meaning "match, lucifer", which leads to Interlingua "flammifero", or "match". This word is thus said to be potentially present in the other languages although they may represent the meaning with a single morpheme.

Words do not enter the Interlingua vocabulary solely because cognates exist in a sufficient number of languages. If their meanings have become different over time, they are considered different words for the purpose of Interlingua eligibility. If they still have one or more meanings in common, however, the word can enter Interlingua with this smaller set of meanings.

If this procedure did not produce an international word, the word for a concept was originally taken from Latin (see below). This only occurred with a few grammatical particles.

The form of an Interlingua word is considered an "international prototype" with respect to the other words. On the one hand, it should be neutral, free from characteristics peculiar to one language. On the other hand, it should maximally capture the characteristics common to all contributing languages. As a result, it can be transformed into any of the contributing variants using only these language-specific characteristics. If the word has any derivatives that occur in the source languages with appropriate parallel meanings, then their morphological connection must remain intact; for example, the Interlingua word for 'time' is spelled "tempore" and not "*tempus" or "*tempo" in order to match it with its derived adjectives, such as "temporal".

The language-specific characteristics are closely related to the sound laws of the individual languages; the resulting words are often close or even identical to the most recent form common to the contributing words. This sometimes corresponds with that of Vulgar Latin. At other times, it is much more recent or even contemporary. It is never older than the classical period.

The French "œil", Italian "occhio", Spanish "ojo", and Portuguese "olho" appear quite different, but they descend from a historical form "oculus". German "Auge", Dutch "oog" and English "eye" (cf. Czech and Polish "oko", Ukrainian "око" "(óko)") are related to this form in that all three descend from Proto-Indo-European "*okʷ". In addition, international derivatives like "ocular" and "oculista" occur in all of Interlingua's control languages. Each of these forms contributes to the eligibility of the Interlingua word. German and English base words do not influence the form of the Interlingua word, because their Indo-European connection is considered too remote. Instead, the remaining base words and especially the derivatives determine the form "oculo" found in Interlingua.

Interlingua has been developed to omit any grammatical feature that is absent from any one primary control language. Thus, Interlingua has no noun–adjective agreement by gender, case, or number (cf. Spanish and Portuguese "gatas negras" or Italian "gatte nere", 'black female cats'), because this is absent from English, and it has no progressive verb tenses (English "I am reading"), because they are absent from French. Conversely, Interlingua distinguishes singular nouns from plural nouns because all the control languages do. With respect to the secondary control languages, Interlingua has articles, unlike Russian.

The definite article "le" is invariable, as in English. Nouns have no grammatical gender. Plurals are formed by adding "-s", or "-es" after a final consonant. Personal pronouns take one form for the subject and one for the direct object and reflexive. In the third person, the reflexive is always "se". Most adverbs are derived regularly from adjectives by adding "-mente", or "-amente" after a "-c". An adverb can be formed from any adjective in this way.

Verbs take the same form for all persons ("io vive, tu vive, illa vive", 'I live', 'you live', 'she lives'). The indicative ("pare", 'appear', 'appears') is the same as the imperative ("pare!" 'appear!'), and there is no subjunctive. Three common verbs usually take short forms in the present tense: "es" for 'is', 'am', 'are;' "ha" for 'has', 'have;' and "va" for 'go', 'goes'. A few irregular verb forms are available, but rarely used.

There are four simple tenses (present, past, future, and conditional), three compound tenses (past, future, and conditional), and the passive voice. The compound structures employ an auxiliary plus the infinitive or the past participle (e.g., "Ille ha arrivate", 'He has arrived'). Simple and compound tenses can be combined in various ways to express more complex tenses (e.g., "Nos haberea morite", 'We would have died').

Word order is subject–verb–object, except that a direct object pronoun or reflexive pronoun comes before the verb ("Io les vide", 'I see them'). Adjectives may precede or follow the nouns they modify, but they most often follow it. The position of adverbs is flexible, though constrained by common sense.

The grammar of Interlingua has been described as similar to that of the Romance languages, but greatly simplified, primarily under the influence of English. More recently, Interlingua's grammar has been likened to the simple grammars of Japanese and particularly Chinese.

Critics argue that, being based on a few European languages, Interlingua is suitable for speakers of European languages. Others contend that Interlingua has spelling irregularities that, while internationally recognizable in written form, increase the time needed to fully learn the language, especially for those unfamiliar with Indo-European languages.

Proponents argue that Interlingua's source languages include not only Romance languages but English, German, and Russian as well. Moreover, the source languages are widely spoken, and large numbers of their words also appear in other languages – still more when derivative forms and loan translations are included. Tests had shown that if a larger number of source languages were used, the results would be about the same.

From an essay by Alexander Gode:

As with Esperanto, there have been proposals for a flag of Interlingua; the proposal by Czech translator Karel Podrazil is recognized by multilingual sites. It consists of a white four-pointed star extending to the edges of the flag and dividing it into an upper blue and lower red half. The star is symbolic of the four cardinal directions, and the two halves symbolize Romance and non-Romance speakers of Interlingua who understand each other.

Another symbol of Interlingua is the "Blue Marble" surrounded by twelve stars on a black or blue background, echoing the twelve stars of the Flag of Europe (because the source languages of Interlingua are purely European).





</doc>
<doc id="15102" url="https://en.wikipedia.org/wiki?curid=15102" title="Isle of Wight">
Isle of Wight

The Isle of Wight () is a county and the largest and second-most populous island in England. It is in the English Channel, between two and five miles off the coast of Hampshire, separated by the Solent. The island has resorts that have been holiday destinations since Victorian times, and is known for its mild climate, coastal scenery, and verdant landscape of fields, downland and chines. The island is designated a UNESCO Biosphere Reserve.

The island has been home to the poets Algernon Charles Swinburne and Alfred, Lord Tennyson and to Queen Victoria, who built her much-loved summer residence and final home Osborne House at East Cowes. It has a maritime and industrial tradition including boat-building, sail-making, the manufacture of flying boats, the hovercraft, and Britain's space rockets. The island hosts annual music festivals including the Isle of Wight Festival, which in 1970 was the largest rock music event ever held. It has well-conserved wildlife and some of the richest cliffs and quarries for dinosaur fossils in Europe.

The isle was owned by a Norman family until 1293 and was earlier a kingdom in its own right, Wihtwara. In common with the Crown dependencies, the British Crown was then represented on the island by the Governor of the Isle of Wight until 1995. The island has played an important part in the defence of the ports of Southampton and Portsmouth, and been near the front-line of conflicts through the ages, including the Spanish Armada and the Battle of Britain. Rural for most of its history, its Victorian fashionability and the growing affordability of holidays led to significant urban development during the late 19th and early 20th centuries. Historically part of Hampshire, the island became a separate administrative county in 1890. It continued to share the Lord Lieutenant of Hampshire until 1974, when it was made its own ceremonial county. Apart from a shared police force, and the island's Anglican churches belonging to the Diocese of Portsmouth (originally Winchester), there is now no administrative link with Hampshire; although a combined local authority with Portsmouth and Southampton was considered, this is now unlikely to proceed.

The quickest public transport link to the mainland is the hovercraft from Ryde to Southsea; three vehicle ferry and two catamaran services cross the Solent to Southampton, Lymington and Portsmouth.

During Pleistocene glacial periods, sea levels were lower and the present day Solent was part of the valley of the Solent River. The river flowed eastward from Dorset, following the course of the modern Solent strait, before travelling south and southwest towards the major Channel River system.

The earliest evidence of archaic human presence on what is now the Isle of Wight is found at Priory Bay. Here more than 300 acheulean handaxes have been recovered from the beach and cliff slopes, originating from a sequence of Pleistocene gravels dating approximately to MIS 11 (424,000-374,000 years ago).

A Mousterian flint assemblage, consisting of 50 handaxes and debitage has been recovered from Great Pan Farm near Newport. Possibly dating to MIS 7 (c.240,000 years ago), these tools are associated with Neanderthal occupation.

A submerged escarpment 11m below sea level off Bouldnor Cliff on the island's northwest coastline is home to an internationally significant mesolithic archaeological site. The site has yielded evidence of seasonal occupation by mesolithic hunter-gatherers dating to c.8000 years BP. Finds include flint tools, burnt flint, worked timbers, wooden platforms and pits. The worked wood shows evidence of the splitting of large planks from oak trunks, interpreted as being intended for use as dug-out canoes. DNA analysis of sediments at the site yielded wheat DNA, not found in Britain until the Neolithic 2000 years after the occupation at Bouldnor Cliff. It has been suggested this is evidence of wide-reaching trade in mesolithic Europe, however the contemporaneity of the wheat with the Mesolithic occupation has been contested. When hunter-gatherers used the site it was located on a river bank surrounded by wetland and woodland. As sea levels rose throughout the Holocene the river valley slowly flooded, submerging the site.

Evidence of mesolithic occupation on the island is generally found along the river valleys, particularly along the north of the Island, and in the former catchment of the western Yar. Further key sites are found at Newtown Creek, Werrar and Wootton-Quarr.

Neolithic occupation on the Isle of Wight is primarily attested to by flint tools and monuments. Unlike the previous mesolithic hunter-gatherer population Neolithic communities on the Isle of Wight were based on farming and linked to a wide-scale migration of Neolithic populations from France and northwest Europe to Britain c.6000 years ago.

The Isle of Wight's most visible Neolithic site is the Longstone at Mottistone, the remains of a long-barrow originally constructed with two standing stones at the entrance. Only one stone remains standing today. A Neolithic mortuary enclosure has been identified on Tennyson Down near Freshwater.

Bronze Age Britain had large reserves of tin in the areas of Cornwall and Devon and tin is necessary to smelt bronze. At that time the sea level was much lower and carts of tin were brought across the Solent at low tide for export, possibly on the Ferriby Boats. Anthony Snodgrass suggests that a shortage of tin, as a part of the Bronze Age Collapse and trade disruptions in the Mediterranean around 1300 BC, forced metalworkers to seek an alternative to bronze. During Iron Age Britain, the Late Iron Age, the Isle of Wight would appear to have been occupied by the Celtic tribe, the Durotriges – as attested by finds of their coins, for example, the South Wight Hoard, and the Shalfleet Hoard. South eastern Britain experienced significant immigration that is reflected in the genetic makeup of the current residents. As the Iron Age began the value of tin likely dropped sharply and this likely greatly changed the economy of the Isle of Wight. Trade however continued as evidenced by the remarkable local abundance of European Iron Age coins.

Julius Caesar reported that the Belgae took the Isle of Wight in about 85 BC, and recognised the culture of this general region as "Belgic", but made no reference to Vectis. The Roman historian Suetonius mentions that the island was captured by the commander Vespasian. The Romans built no towns on the island, but the remains of at least seven Roman villas have been found, indicating the prosperity of local agriculture. First-century exports were principally hides, slaves, hunting dogs, grain, cattle, silver, gold, and iron. 

Starting in AD 449 (according to the Anglo Saxon Chronicles) the 5th and 6th centuries saw groups of Germanic speaking peoples from Northern Europe crossing the English Channel and setting up home. Bede's (731) "Historia ecclesiastica gentis Anglorum" identifies three separate groups of invaders: of these, the Jutes from Denmark settled the Isle of Wight and Kent. From then onwards, there are indications that the island had wide trading links, with a port at Bouldnor, evidence of Bronze Age tin trading, and finds of Late Iron Age coins.

During the Dark Ages the island was settled by Jutes as the pagan kingdom of Wihtwara under King Arwald. In 685 it was invaded by Caedwalla, who tried to replace the inhabitants with his own followers. In 686 Arwald was defeated and the island became the last part of English lands to be converted to Christianity, added to Wessex and then becoming part of England under King Alfred the Great, included within the shire of Hampshire.

It suffered especially from Viking raids, and was often used as a winter base by Viking raiders when they were unable to reach Normandy. Later, both Earl Tostig and his brother Harold Godwinson (who became King Harold II) held manors on the island.

The Norman Conquest of 1066 created the position of Lord of the Isle of Wight; the island was given by William the Conqueror to his kinsman William FitzOsbern. Carisbrooke Priory and the fort of Carisbrooke Castle were then founded. Allegiance was sworn to FitzOsbern rather than the king; the Lordship was subsequently granted to the de Redvers family by Henry I, after his succession in 1100.

For nearly 200 years the island was a semi-independent feudal fiefdom, with the de Redvers family ruling from Carisbrooke. The final private owner was the Countess Isabella de Fortibus, who, on her deathbed in 1293, was persuaded to sell it to Edward I. Thereafter the island was under control of the English Crown and its Lordship a royal appointment.

The island continued to be attacked from the continent: it was raided in 1374 by the fleet of Castile, and in 1377 by French raiders who burned several towns, including Newtown, and laid siege to Carisbrooke Castle before they were defeated.

Under Henry VIII, who developed the Royal Navy and its Portsmouth base, the island was fortified at Yarmouth, Cowes, East Cowes, and Sandown.

The French invasion on 21 July 1545 (famous for the sinking of the Mary Rose on the 19th) was repulsed by local militia.

During the English Civil War, King Charles fled to the Isle of Wight, believing he would receive sympathy from the governor Robert Hammond, but Hammond imprisoned the king in Carisbrooke Castle.

During the Seven Years' War, the island was used as a staging post for British troops departing on expeditions against the French coast, such as the Raid on Rochefort. During 1759, with a planned French invasion imminent, a large force of soldiers was stationed there. The French called off their invasion following the Battle of Quiberon Bay.

In the 1860s, what remains in real terms the most expensive ever government spending project saw fortifications built on the island and in the Solent, as well as elsewhere along the south coast, including the Palmerston Forts, The Needles Batteries and Fort Victoria, because of fears about possible French invasion.

The future Queen Victoria spent childhood holidays on the island and became fond of it. When queen she made Osborne House her winter home, and so the island became a fashionable holiday resort, including for Alfred, Lord Tennyson, Julia Margaret Cameron, and Charles Dickens (who wrote much of "David Copperfield" there), as well as the French painter Berthe Morisot and members of European royalty.

Until the queen's example, the island had been rural, with most people employed in farming, fishing or boat-building. The boom in tourism, spurred by growing wealth and leisure time, and by Victoria's presence, led to significant urban development of the island's coastal resorts. As one report summarizes, "The Queen’s regular presence on the island helped put the Isle of Wight 'on the map' as a Victorian holiday and wellness destination ... and her former residence Osborne House is now one of the most visited attractions on the island While on the island, the queen used a bathing machine that could be wheeled into the water on Osborne Beach; inside the small wooden hut she could undress and then bathe, without being visible to others.

On 14 January 1878, Alexander Graham Bell demonstrated an early version of the telephone to the queen, placing calls to Cowes, Southampton and London. These were the first publicly-witnessed long distance telephone calls in the UK. The queen tried the device and considered the process to be "quite extraordinary" although the sound was "rather faint". She later asked to buy the equipment that was used, but Bell offered to make "a set of telephones" specifically for her.

The world's first radio station was set up by Marconi in 1897, during her reign, at the Needles Battery, at the western tip of the island. A 168 foot high mast was erected near the Royal Needles Hotel, as part of an experiment of communicating with ships at sea. That location is now the site of the Marconi Monument. In 1898 the first paid wireless telegram (called a "Marconigram") was sent from this station, and the island was for some time the home of the National Wireless Museum, near Ryde.

Queen Victoria died at Osborne House on 22 January 1901, aged 81.

During the Second World War the island was frequently bombed. With its proximity to German-occupied France, the island hosted observation stations and transmitters, as well as the RAF radar station at Ventnor. It was the starting-point for one of the earlier Operation Pluto pipelines to feed fuel to Europe after the Normandy landings.

The Needles Battery was used to develop and test the Black Arrow and Black Knight space rockets, which were subsequently launched from Woomera, Australia.
The Isle of Wight Festival was a very large rock festival that took place near Afton Down, West Wight in 1970, following two smaller concerts in 1968 and 1969. The 1970 show was notable both as one of the last public performances by Jimi Hendrix and for the number of attendees, reaching by some estimates 600,000. The festival was revived in 2002 in a different format, and is now an annual event.

The oldest records that give a name for the Isle of Wight are from the Roman Empire: it was then called "Vectis" or "Vecta" in Latin, "Iktis" or "Ouiktis" in Greek. From the Anglo-Saxon period Latin "Vecta", Old English "Wiht" and Old Welsh forms "Gueid" and "Guith" are recorded. In Domesday Book it is "Wit"; the modern Welsh name is "Ynys Wyth" ("ynys" = island). These are all variant forms of the same name, possibly Celtic in origin. It may mean "place of the division", because the island divides the two arms of the Solent.

The Isle of Wight is situated between the Solent and the English Channel, is roughly rhomboid in shape, and covers an area of . Slightly more than half, mainly in the west, is designated as the Isle of Wight Area of Outstanding Natural Beauty. The island has of farmland, of developed areas, and of coastline. Its landscapes are diverse, leading to its oft-quoted description as "England in miniature". In June 2019 the whole island was designated a UNESCO Biosphere Reserve, recognising the sustainable relationships between its residents and the local environment.

West Wight is predominantly rural, with dramatic coastlines dominated by the chalk downland ridge, running across the whole island and ending in the Needles stacks. The southwestern quarter is commonly referred to as the Back of the Wight, and has a unique character. The highest point on the island is St Boniface Down in the south east, which at is a marilyn. The most notable habitats on the rest of the island are probably the soft cliffs and sea ledges, which are scenic features, important for wildlife, and internationally protected.

The island has three principal rivers. The River Medina flows north into the Solent, the Eastern Yar flows roughly northeast to Bembridge Harbour, and the Western Yar flows the short distance from Freshwater Bay to a relatively large estuary at Yarmouth. Without human intervention the sea might well have split the island into three: at the west end where a bank of pebbles separates Freshwater Bay from the marshy backwaters of the Western Yar east of Freshwater, and at the east end where a thin strip of land separates Sandown Bay from the marshy Eastern Yar basin.

The Undercliff between St Catherine's Point and Bonchurch is the largest area of landslip morphology in western Europe.

The north coast is unusual in having four high tides each day, with a double high tide every twelve and a half hours. This arises because the western Solent is narrower than the eastern; the initial tide of water flowing from the west starts to ebb before the stronger flow around the south of the island returns through the eastern Solent to create a second high water.

The Isle of Wight is made up of a variety of rock types dating from early Cretaceous (around 127 million years ago) to the middle of the Palaeogene (around 30 million years ago). The geological structure is dominated by a large monocline which causes a marked change in age of strata from the northern younger Tertiary beds to the older Cretaceous beds of the south. This gives rise to a dip of almost 90 degrees in the chalk beds, seen best at the Needles.

The northern half of the island is mainly composed of clays, with the southern half formed of the chalk of the central east–west downs, as well as Upper and Lower Greensands and Wealden strata. These strata continue west from the island across the Solent into Dorset, forming the basin of Poole Harbour (Tertiary) and the Isle of Purbeck (Cretaceous) respectively. The chalky ridges of Wight and Purbeck were a single formation before they were breached by waters from the River Frome during the last ice age, forming the Solent and turning Wight into an island. The Needles, along with Old Harry Rocks on Purbeck, represent the edges of this breach.

All the rocks found on the island are sedimentary, such as limestones, mudstones and sandstones. They are rich in fossils; many can be seen exposed on beaches as the cliffs erode. Lignitic coal is present in small quantities within seams, and can be seen on the cliffs and shore at Whitecliff Bay. Fossilised molluscs have been found there, and also on the northern coast along with fossilised crocodiles, turtles and mammal bones; the youngest date back to around 30 million years ago.

The island is one of the most important areas in Europe for dinosaur fossils. The eroding cliffs often reveal previously hidden remains, particularly along the Back of the Wight. Dinosaur bones and fossilised footprints can be seen in and on the rocks exposed around the island's beaches, especially at Yaverland and Compton Bay. As a result, the island has been nicknamed "Dinosaur Island" and Dinosaur Isle was established in 2001.

The area was affected by sea level changes during the repeated Quaternary glaciations. The island probably became separated from the mainland about 125,000 years ago, during the Ipswichian interglacial.

Like the rest of the UK, the island has an oceanic climate, but is somewhat milder and sunnier, which makes it a holiday destination. It also has a longer growing season. Lower Ventnor and the neighbouring Undercliff have a particular microclimate, because of their sheltered position south of the downs. The island enjoys 1,800–2,100 hours of sunshine a year. Some years have almost no snow in winter, and only a few days of hard frost. The island is in Hardiness zone 9.
The Isle of Wight is one of the few places in England where the red squirrel is still flourishing; no grey squirrels are to be found. There are occasional sightings of wild deer, and there is a colony of wild goats on Ventnor's downs. Protected species such as the dormouse and rare bats can be found. The Glanville fritillary butterfly's distribution in the United Kingdom is largely restricted to the edges of the island's crumbling cliffs.

A competition in 2002 named the pyramidal orchid as the Isle of Wight's county flower.

The island has a single Member of Parliament. The Isle of Wight constituency covers the entire island, with 138,300 permanent residents in 2011, being one of the most populated constituencies in the United Kingdom (more than 50% above the English average). In 2011 following passage of the Parliamentary Voting System and Constituencies Act, the Sixth Periodic Review of Westminster constituencies was to have changed this, but this was deferred to no earlier than October 2018 by the Electoral Registration and Administration Act 2013. Thus the single constituency remained for the 2015 and 2017 general elections. However, two separate East and West constituencies are proposed for the island under the 2018 review now under way.

The Isle of Wight is a ceremonial and non-metropolitan county. Since the abolition of its two borough councils and restructuring of the Isle of Wight County Council into the new Isle of Wight Council in 1995, it has been administered by a single unitary authority.

Elections in the constituency have traditionally been a battle between the Conservatives and the Liberal Democrats. Andrew Turner of the Conservative Party gained the seat from Peter Brand of the Lib Dems at the 2001 general election. Since 2009, Turner was embroiled in controversy over his expenses, health, and relationships with colleagues, with local Conservatives having tried but failed to remove him in the runup to the 2015 general election. He stood down prior to the 2017 snap general election, and the new Conservative Party candidate Bob Seely was elected with a majority of 21,069 votes.

At the Isle of Wight Council election of 2013, the Conservatives lost the majority which they had held since 2005 to the Island Independents, with Island Independent councillors holding 16 of the 40 seats, and a further five councillors sitting as independents outside the group. The Conservatives regained control, winning 25 seats, at the 2017 local election.

There have been small regionalist movements: the Vectis National Party and the Isle of Wight Party; but they have attracted little support at elections.

The local accent is similar to the traditional dialect of Hampshire, featuring the dropping of some consonants and an emphasis on longer vowels. It is similar to the West Country dialects heard in South West England, but less pronounced.

The island has its own local and regional words. Some, such as "nipper/nips" (a young male person), are still commonly used and are shared with neighbouring areas of the mainland. A few are unique to the island, for example "overner" and "caulkhead" (see below). Others are more obscure and now used mainly for comic emphasis, such as "mallishag" (meaning "caterpillar"), "gurt" meaning "large", "nammit" (a mid-morning snack) and "gallybagger" ("scarecrow", and now the name of a local cheese).

There remains occasional confusion between the Isle of Wight as a county and its former position within Hampshire. The island was regarded and administered as a part of Hampshire until 1890, when its distinct identity was recognised with the formation of Isle of Wight County Council (see also "Politics of the Isle of Wight"). However, it remained a part of Hampshire until the local government reforms of 1974 when it became a full ceremonial county with its own Lord Lieutenant.

In January 2009, the first general flag for the county was accepted by the Flag Institute.

Island residents are sometimes referred to as "Vectensians", "Vectians" or, if born on the island, "caulkheads". One theory is that this last comes from the once prevalent local industry of caulking or sealing wooden boats; the term became attached to islanders either because they were so employed, or as a derisory term for perceived unintelligent labourers from elsewhere. The term "overner" is used for island residents originating from the mainland (an abbreviated form of "overlander", which is an archaic term for "outsider" still found in parts of Australia).

Residents refer to the island as "The Island", as did Jane Austen in "Mansfield Park", and sometimes to the UK mainland as "North Island".

To promote the island's identity and culture, the High Sheriff Robin Courage founded an Isle of Wight Day; the first was held on Saturday 24 September 2016.

The island is said to be the most haunted in the world, sometimes being referred to as "Ghost Island". Notable claimed hauntings include God's Providence House in Newport (now a tea room), Appuldurcombe House, and the remains of Knighton Gorges.

The island is well known for its cycling, and it was included within Lonely Planet's "Best in Travel Guide" (2010) top ten cycling locations. The island also hosts events such as the Isle of Wight Randonnée and the Isle of Wight Cycling Festival each year. A popular cycling track is the Sunshine Trail which starts in Newport and ends in Sandown.

There are rowing clubs at Newport, Ryde and Shanklin, all members of the Hants and Dorset rowing association.

There is a long tradition of rowing around the island dating back to the 1880s.

In May 1999 a group of local women made history by becoming the first ladies' crew to row around the island, in ten hours and twenty minutes. Rowers from Ryde Rowing Club have rowed around the island several times since 1880. The fours record was set 16 August 1995 at 7 hours 54 minutes.

Two rowers from Southampton ARC (Chris Bennett and Roger Slaymaker) set the two-man record in July 2003 at 8 hours 34 minutes, and in 2005 Gus McKechnie of Coalporters Rowing Club became the first adaptive rower to row around, completing a clockwise row.

The route around the island is about and usually rowed anticlockwise. Even in good conditions, it includes a number of significant obstacles such as the Needles and the overfalls at St Catherine's Point. The traditional start and finish were at Ryde Rowing Club; however, other starts have been chosen in recent years to give a tidal advantage.

Cowes is a centre for sailing, hosting several racing regattas. Cowes Week is the longest-running regular regatta in the world, with over 1,000 yachts and 8,500 competitors taking part in over 50 classes of racing.
In 1851 the first America's Cup race was around the island. Other major sailing events hosted in Cowes include the Fastnet race, the Round the Island Race, the Admiral's Cup, and the Commodore's Cup.

There are two main trampoline clubs on the island, in Freshwater and Newport, competing at regional, national and international grades.

The Isle of Wight Marathon is the United Kingdom's oldest continuously held marathon, having been run every year since 1957. Since 2013 the course has started and finished in Cowes, heading out to the west of the island and passing through Gurnard, Rew Street, Porchfield, Shalfleet, Yarmouth, Afton, Willmingham, Thorley, Wellow, Shalfleet, Porchfield, and Northwood. It is an undulating course with a total climb of .

The island is home to the Wightlink Warriors speedway team, who compete in the sport's third division, the National League.

Following an amalgamation of local hockey clubs in 2011, the Isle of Wight Hockey Club now runs two men's senior and two ladies' senior teams. These compete at a range of levels in the Hampshire open leagues.

The now-disbanded Ryde Sports F.C., founded in 1888, was one of the eight founder members of the Hampshire League in 1896. There are several non-league clubs such as Newport (IOW) F.C. There is an Isle of Wight Saturday Football League which feeds into the Hampshire League with two divisions and two reserve team leagues, and a rugby union club.

The Isle of Wight is the 39th official county in English cricket, and the Isle of Wight Cricket Board organises a league of local clubs. Ventnor Cricket Club competes in the Southern Premier League, and has won the Second Division several times. Newclose County Cricket Ground near Newport opened officially in 2009 but with its first match held on 6 September 2008. The island has produced some notable cricketers, such as Danny Briggs, who plays county cricket for Sussex.

The Isle of Wight competes in the biennial Island Games, which it hosted in 1993 and again in 2011.

The annual Isle of Wight International Scooter Rally has since 1980 met on the August Bank Holiday. This is now one of the biggest scooter rallies in the world, attracting between four and seven thousand participants.

There are eight Golf courses on the Isle of Wight.

The island is home to the Isle of Wight Festival and until 2016, Bestival before it was relocated to Lulworth Estate in Dorset. In 1970, the festival was headlined by Jimi Hendrix attracting an audience of 600,000, some six times the local population at the time. It is the home of the bands The Bees, Trixie's Big Red Motorbike and Level 42.

The table below shows the regional gross value (in millions of pounds) added by the Isle of Wight economy, at current prices, compiled by the Office for National Statistics.
According to the 2011 census, the island's population of 138,625 lives in 61,085 households, giving an average household size of 2.27 people.

41% of households own their home outright and a further 29% own with a mortgage, so in total 70% of households are owned (compared to 68% for South East England).

Compared to South East England, the island has fewer children (19% aged 0–17 against 22% for the South East) and more elderly (24% aged 65+ against 16%), giving an average age of 44 years for an island resident compared to 40 in South East England.

The largest industry is tourism, but the island also has a strong agricultural heritage, including sheep and dairy farming and arable crops. Traditional agricultural commodities are more difficult to market off the island because of transport costs, but local farmers have succeeded in exploiting some specialist markets, with the higher price of such products absorbing the transport costs. One of the most successful agricultural sectors is now the growing of crops under cover, particularly salad crops including tomatoes and cucumbers. The island has a warmer climate and a longer growing season than much of the United Kingdom. Garlic has been successfully grown in Newchurch for many years, and is even exported to France. This has led to the establishment of an annual Garlic Festival at Newchurch, which is one of the largest events of the local calendar. A favourable climate supports two vineyards, including one of the oldest in the British Isles at Adgestone. Lavender is grown for its oil. The largest agricultural sector has been dairying, but due to low milk prices and strict legislation for UK milk producers, the dairy industry has been in decline: there were nearly 150 producers in the mid-1980s, but now just 24.

Maritime industries, especially the making of sailcloth and boat building, have long been associated with the island, although this has diminished somewhat in recent years. GKN operates what began as the British Hovercraft Corporation, a subsidiary of (and known latterly as) Westland Aircraft, although they have reduced the extent of plant and workforce and sold the main site. Previously it had been the independent company Saunders-Roe, one of the island's most notable historic firms that produced many flying boats and the world's first hovercraft.

Another manufacturing activity is in composite materials, used by boat-builders and the wind turbine manufacturer Vestas, which has a wind turbine blade factory and testing facilities in West Medina Mills and East Cowes.

Bembridge Airfield is the home of Britten-Norman, manufacturers of the Islander and Trislander aircraft. This is shortly to become the site of the European assembly line for Cirrus light aircraft. The Norman Aeroplane Company is a smaller aircraft manufacturing company operating in Sandown. There have been three other firms that built planes on the island.

In 2005, Northern Petroleum began exploratory drilling for oil at its Sandhills-2 borehole at Porchfield, but ceased operations in October that year after failing to find significant reserves.

There are three breweries on the island. Goddards Brewery in Ryde opened in 1993. David Yates, who was head brewer of the Island Brewery, started brewing as Yates Brewery at the Inn at St Lawrence in 2000.

Ventnor Brewery, which closed in 2009, was the last incarnation of Burt's Brewery, brewing since the 1840s in Ventnor. Until the 1960s most pubs were owned by Mews Brewery, situated in Newport near the old railway station, but it closed and the pubs were taken over by Strong's, and then by Whitbread. By some accounts Mews beer was apt to be rather cloudy and dark. In the 19th century they pioneered the use of screw top cans for export to British India.

The island's heritage is a major asset that has for many years supported its tourist economy. Holidays focused on natural heritage, including wildlife and geology, are becoming an alternative to the traditional British seaside holiday, which went into decline in the second half of the 20th century due to the increased affordability of foreign holidays. The island is still an important destination for coach tours from other parts of the United Kingdom.
Tourism is still the largest industry, and most island towns and villages offer hotels, hostels and camping sites. In 1999, it hosted 2.7 million visitors, with 1.5 million staying overnight, and 1.2 million day visits; only 150,000 of these were from abroad. Between 1993 and 2000, visits increased at an average rate of 3% per year.

At the turn of the 19th century the island had ten pleasure piers, including two at Ryde and a "chain pier" at Seaview. The Victoria Pier in Cowes succeeded the earlier Royal Pier but was itself removed in 1960. The piers at Ryde, Seaview, Sandown, Shanklin and Ventnor originally served a coastal steamer service that operated from Southsea on the mainland. The piers at Seaview, Shanklin, Ventnor and Alum Bay were all destroyed by various storms during the 20th century; only the railway pier at Ryde and the piers at Sandown, Totland Bay (currently closed to the public) and Yarmouth survive.

Blackgang Chine is the oldest theme park in Britain, opened in 1843. The skeleton of a dead whale that its founder Alexander Dabell found in 1844 is still on display.

As well as its more traditional attractions, the island is often host to walking or cycling holidays through the attractive scenery. An annual walking festival has attracted considerable interest. The Isle of Wight Coastal Path follows the coastline as far as possible, deviating onto roads where the route along the coast is impassable.

The tourist board for the island is Visit Isle of Wight, a not for profit company. It is the Destination Management Organisation for the Isle of Wight, a public and private sector partnership led by the private sector, and consists of over 1,200 companies, including the ferry operators, the local bus company, rail operator and tourism providers working together to collectively promote the island. Its income is derived from the Wight BID, a business improvement district levy fund.

A major contributor to the local economy is sailing and marine-related tourism.

Summer Camp at Camp Beaumont is an attraction at the old Bembridge School site.

The Isle of Wight has of roadway. It does not have a motorway, although there is a short stretch of dual carriageway towards the north of Newport near the hospital and prison.

A comprehensive bus network operated by Southern Vectis links most settlements, with Newport as its central hub.

Journeys away from the island involve a ferry journey. Car ferry and passenger catamaran services are run by Wightlink and Red Funnel, and a hovercraft passenger service (the only such remaining in the world) by Hovertravel.

The island formerly had its own railway network of over , but only one line remains in regular use. The Island Line is part of the United Kingdom's National Rail network, running a little under from to , where there is a connecting ferry service to station on the mainland network. The line was opened by the Isle of Wight Railway in 1864, and from 1996 to 2007 was run by the smallest train operating company on the network, Island Line Trains. It is notable for utilising old ex-London Underground rolling stock, due to the small size of its tunnels and unmodernised signalling. Branching off the Island Line at is the heritage Isle of Wight Steam Railway, which runs for to the outskirts of on the former line to Newport.

There are two airfields for general aviation, Isle of Wight Airport at Sandown and Bembridge Airport.

The island has over of cycleways, many of which can be enjoyed off-road. The principal trails are:

The main local newspaper is the "Isle of Wight County Press", published most Fridays.

The island hosts a news website, "Island Echo", which was launched in May 2012.

The island has a local commercial radio station and a community radio station: commercial station Isle of Wight Radio has broadcast in the medium-wave band since 1990 and on 107.0 MHz (with three smaller transmitters on 102.0 MHz) FM since 1998, as well as streaming on the Internet. Community station Vectis Radio has broadcast online since 2010, and in 2017 started broadcasting on FM 104.6. The station operates from the Riverside Centre in Newport.

The island is also covered by a number of local stations on the mainland, including the BBC station BBC Radio Solent broadcast from Southampton.

The island's not-for-profit community radio station Angel Radio opened in 2007. Angel Radio began broadcasting on 91.5 MHz from studios in Cowes and a transmitter near Newport.

Other online news sources for the Isle of Wight include "On the Wight".

The island has had community television stations in the past, first TV12 and then Solent TV from 2002 until its closure on 24 May 2007. iWight.tv is a local internet video news channel.

The Isle of Wight is part of the BBC South region and the ITV Meridian region.

Important broadcasting infrastructure includes Chillerton Down transmitting station with a mast that is the tallest structure on the island, and Rowridge transmitting station, which broadcasts the main television signal both locally and for most of Hampshire and parts of Dorset and West Sussex.

The Isle of Wight is near the densely populated south of England, yet separated from the mainland. This position led to it hosting three prisons: Albany, Camp Hill and Parkhurst, all located outside Newport near the main road to Cowes. Albany and Parkhurst were among the few Category A prisons in the UK until they were downgraded in the 1990s. The downgrading of Parkhurst was precipitated by a major escape: three prisoners (two murderers and a blackmailer) escaped from the prison on 3 January 1995 for four days, before being recaptured. Parkhurst enjoyed notoriety as one of the toughest jails in the United Kingdom, and housed many notable inmates including the Yorkshire Ripper Peter Sutcliffe, New Zealand drug lord Terry Clark and the Kray twins.

Camp Hill is located adjacent but to the west of Albany and Parkhurst, on the very edge of Parkhurst Forest, having been converted first to a borstal and later to a Category C prison. It was built on the site of an army camp (both Albany and Parkhurst were barracks); there is a small estate of tree-lined roads with the former officers' quarters (now privately owned) to the south and east. Camp Hill closed as a prison in March 2013.

The management of all three prisons was merged into a single administration, under HMP Isle of Wight in April 2009.

There are 69 local education authority-maintained schools on the Isle of Wight, and two independent schools. As a rural community, many of these are small and with fewer pupils than in urban areas. The Isle of Wight College is located on the outskirts of Newport.

From September 2010, there was a transition period from the three-tier system of primary, middle and high schools to the two-tier system that is usual in England. Some schools have now closed, such as Chale C.E. Primary. Others have become "federated", such as Brading C.E. Primary and St Helen's Primary. Christ the King College started as two "middle schools," Trinity Middle School and Archbishop King Catholic Middle School, but has now been converted into a dual-faith secondary school and sixth form.

Since September 2011 five new secondary schools, with an age range of 11 to 18 years, replaced the island's high schools (as a part of the previous three-tier system).

Notable residents have included:



The Isle of Wight has given names to many parts of former colonies, most notably Isle of Wight County in Virginia founded by settlers from the island in the 17th century. Its county seat is a town named Isle of Wight.

Other notable examples include:



The Isle of Wight was:










</doc>
<doc id="15107" url="https://en.wikipedia.org/wiki?curid=15107" title="Internet Control Message Protocol">
Internet Control Message Protocol

The Internet Control Message Protocol (ICMP) is a supporting protocol in the Internet protocol suite. It is used by network devices, including routers, to send error messages and operational information indicating success or failure when communicating with another IP address, for example, an error is indicated when a requested service is not available or that a host or router could not be reached. ICMP differs from transport protocols such as TCP and UDP in that it is not typically used to exchange data between systems, nor is it regularly employed by end-user network applications (with the exception of some diagnostic tools like ping and traceroute).

ICMP for IPv4 is defined in RFC 792.

ICMP is part of the Internet protocol suite as defined in RFC 792. ICMP messages are typically used for diagnostic or control purposes or generated in response to errors in IP operations (as specified in RFC 1122). ICMP errors are directed to the source IP address of the originating packet. 

For example, every device (such as an intermediate router) forwarding an IP datagram first decrements the time to live (TTL) field in the IP header by one. If the resulting TTL is 0, the packet is discarded and an ICMP time exceeded in transit message is sent to the datagram's source address.

Many commonly used network utilities are based on ICMP messages. The traceroute command can be implemented by transmitting IP datagrams with specially set IP TTL header fields, and looking for ICMP time exceeded in transit and Destination unreachable messages generated in response. The related ping utility is implemented using the ICMP "echo request" and "echo reply" messages.

ICMP uses the basic support of IP as if it were a higher-level protocol, however, ICMP is actually an integral part of IP. Although ICMP messages are contained within standard IP packets, ICMP messages are usually processed as a special case, distinguished from normal IP processing. In many cases, it is necessary to inspect the contents of the ICMP message and deliver the appropriate error message to the application responsible for transmitting the IP packet that prompted the ICMP message to be sent.

ICMP is a network-layer protocol. There is no TCP or UDP port number associated with ICMP packets as these numbers are associated with the transport layer above.

The ICMP packet is encapsulated in an IPv4 packet. The packet consists of header and data sections.

The ICMP header starts after the IPv4 header and is identified by IP protocol number '1'. All ICMP packets have an 8-byte header and variable-sized data section. The first 4 bytes of the header have fixed format, while the last 4 bytes depend on the type/code of that ICMP packet.


ICMP error messages contain a data section that includes a copy of the entire IPv4 header, plus at least the first eight bytes of data from the IPv4 packet that caused the error message. The maximum length of ICMP error messages is 576 bytes. This data is used by the host to match the message to the appropriate process. If a higher level protocol uses port numbers, they are assumed to be in the first eight bytes of the original datagram's data.

The variable size of the ICMP packet data section has been exploited. In the "Ping of death", large or fragmented ICMP packets are used for denial-of-service attacks. ICMP data can also be used to create covert channels for communication. These channels are known as ICMP tunnels.

Control messages are identified by the value in the "type" field. The "code" field gives additional context information for the message. Some control messages have been deprecated since the protocol was first introduced.
"Source Quench" requests that the sender decrease the rate of messages sent to a router or host. This message may be generated if a router or host does not have sufficient buffer space to process the request, or may occur if the router or host buffer is approaching its limit.

Data is sent at a very high speed from a host or from several hosts at the same time to a particular router on a network. Although a router has buffering capabilities, the buffering is limited to within a specified range. The router cannot queue any more data than the capacity of the limited buffering space. Thus if the queue gets filled up, incoming data is discarded until the queue is no longer full. But as no acknowledgement mechanism is present in the network layer, the client does not know whether the data has reached the destination successfully. Hence some remedial measures should be taken by the network layer to avoid these kind of situations. These measures are referred to as source quench. In a source quench mechanism, the router sees that the incoming data rate is much faster than the outgoing data rate, and sends an ICMP message to the clients, informing them that they should slow down their data transfer speeds or wait for a certain amount of time before attempting to send more data. When a client receives this message, it will automatically slow down the outgoing data rate or wait for a sufficient amount of time, which enables the router to empty the queue. Thus the source quench ICMP message acts as flow control in the network layer.

Since research suggested that "ICMP Source Quench [was] an ineffective (and unfair) antidote for congestion", routers' creation of source quench messages was deprecated in 1995 by RFC 1812. Furthermore, forwarding of and any kind of reaction to (flow control actions) source quench messages was deprecated from 2012 by RFC 6633.

Where:

"Redirect" requests data packets be sent on an alternative route. ICMP Redirect is a mechanism for routers to convey routing information to hosts. The message informs a host to update its routing information (to send packets on an alternative route). If a host tries to send data through a router (R1) and R1 sends the data on another router (R2) and a direct path from the host to R2 is available (that is, the host and R2 are on the same Ethernet segment), then R1 will send a redirect message to inform the host that the best route for the destination is via R2. The host should then send packets for the destination directly to R2. The router will still send the original datagram to the intended destination. However, if the datagram contains routing information, this message will not be sent even if a better route is available. RFC 1122 states that redirects should only be sent by gateways and should not be sent by Internet hosts.

Where:

"Time Exceeded" is generated by a gateway to inform the source of a discarded datagram due to the time to live field reaching zero. A time exceeded message may also be sent by a host if it fails to reassemble a fragmented datagram within its time limit.

Time exceeded messages are used by the traceroute utility to identify gateways on the path between two hosts.

Where:

"Timestamp" is used for time synchronization. The originating timestamp is set to the time (in milliseconds since midnight) the sender last touched the packet. The receive and transmit timestamps are not used.

Where:

"Timestamp Reply" replies to a "Timestamp" message. It consists of the originating timestamp sent by the sender of the "Timestamp" as well as a receive timestamp indicating when the "Timestamp" was received and a transmit timestamp indicating when the "Timestamp reply" was sent.

Where:

"Address mask request" is normally sent by a host to a router in order to obtain an appropriate subnet mask.

Recipients should reply to this message with an "Address mask reply" message.

Where:

ICMP Address Mask Request may be used as a part of reconnaissance attack to gather information on the target network, therefore ICMP Address Mask Reply is disabled by default on Cisco IOS.

"Address mask reply" is used to reply to an address mask request message with an appropriate subnet mask.

Where:

"Destination unreachable" is generated by the host or its inbound gateway to inform the client that the destination is unreachable for some reason. Reasons for this message may include: the physical connection to the host does not exist (distance is infinite); the indicated protocol or port is not active; the data must be fragmented but the 'don't fragment' flag is on. Unreachable TCP ports notably respond with TCP RST rather than a "destination unreachable" type 3 as might be expected. "Destination unreachable" is never reported for IP Multicast transmissions. 

Where:




</doc>
<doc id="15108" url="https://en.wikipedia.org/wiki?curid=15108" title="ICMP">
ICMP

ICMP may refer to:




</doc>
<doc id="15109" url="https://en.wikipedia.org/wiki?curid=15109" title="Inverse limit">
Inverse limit

In mathematics, the inverse limit (also called the projective limit) is a construction that allows one to "glue together" several related objects, the precise manner of the gluing process being specified by morphisms between the objects. Inverse limits can be defined in any category, and they are a special case of the concept of a limit in category theory.

We start with the definition of an inverse system (or projective system) of groups and homomorphisms. Let ("I", ≤) be a directed poset (not all authors require "I" to be directed). Let ("A") be a family of groups and suppose we have a family of homomorphisms "f": "A" → "A" for all "i" ≤ "j" (note the order), called bonding maps, with the following properties:
Then the pair (("A"), ("f")) is called an inverse system of groups and morphisms over "I", and the morphisms "f" are called the transition morphisms of the system.

We define the inverse limit of the inverse system (("A"), ("f")) as a particular subgroup of the direct product of the "A"'s:

The inverse limit "A" comes equipped with "natural projections" π: "A" → "A" which pick out the "i"th component of the direct product for each "i" in "I". The inverse limit and the natural projections satisfy a universal property described in the next section.

This same construction may be carried out if the "A"'s are sets, semigroups, topological spaces, rings, modules (over a fixed ring), algebras (over a fixed ring), etc., and the homomorphisms are morphisms in the corresponding category. The inverse limit will also belong to that category.

The inverse limit can be defined abstractly in an arbitrary category by means of a universal property. Let ("X", "f") be an inverse system of objects and morphisms in a category "C" (same definition as above). The inverse limit of this system is an object "X" in "C" together with morphisms π: "X" → "X" (called "projections") satisfying π = "f" ∘ π for all "i" ≤ "j". The pair ("X", π) must be universal in the sense that for any other such pair ("Y", ψ) (i.e. ψ: "Y" → "X" with ψ = "f" ∘ ψ for all "i" ≤ "j") there exists a unique morphism "u": "Y" → "X" such that the diagram

commutes for all "i" ≤ "j", for which it suffices to show that ψ = π ∘ "u" for all "i". The inverse limit is often denoted
with the inverse system ("X", "f") being understood.

In some categories, the inverse limit of certain inverse systems does not exist. If it does, however, it is unique in a strong sense: given any two inverse limits "X" and "X"' of an inverse system, there exists a "unique" isomorphism "X"′ → "X" commuting with the projection maps.

We note that an inverse system in a category "C" admits an alternative description in terms of functors. Any partially ordered set "I" can be considered as a small category where the morphisms consist of arrows "i" → "j" if and only if "i" ≤ "j". An inverse system is then just a contravariant functor "I" → "C", and the inverse limit functor
formula_3 is a covariant functor.


For an abelian category "C", the inverse limit functor
is left exact. If "I" is ordered (not simply partially ordered) and countable, and "C" is the category Ab of abelian groups, the Mittag-Leffler condition is a condition on the transition morphisms "f" that ensures the exactness of formula_23. Specifically, Eilenberg constructed a functor
(pronounced "lim one") such that if ("A", "f"), ("B", "g"), and ("C", "h") are three inverse systems of abelian groups, and
is a short exact sequence of inverse systems, then
is an exact sequence in Ab.

If the ranges of the morphisms of an inverse system of abelian groups ("A", "f") are "stationary", that is, for every "k" there exists "j" ≥ "k" such that for all "i" ≥ "j" :formula_27 one says that the system satisfies the Mittag-Leffler condition.

The name "Mittag-Leffler" for this condition was given by Bourbaki in their chapter on uniform structures for a similar result about inverse limits of complete Hausdorff uniform spaces. Mittag-Leffler used a similar argument in the proof of Mittag-Leffler's theorem.

The following situations are examples where the Mittag-Leffler condition is satisfied: 

An example where formula_28 is non-zero is obtained by taking "I" to be the non-negative integers, letting "A" = "p"Z, "B" = Z, and "C" = "B" / "A" = Z/"p"Z. Then
where Z denotes the p-adic integers.

More generally, if "C" is an arbitrary abelian category that has enough injectives, then so does "C", and the right derived functors of the inverse limit functor can thus be defined. The "n"th right derived functor is denoted
In the case where "C" satisfies Grothendieck's axiom (AB4*), Jan-Erik Roos generalized the functor lim on Ab to series of functors lim such that
It was thought for almost 40 years that Roos had proved (in "Sur les foncteurs dérivés de lim. Applications. ") that lim "A" = 0 for ("A", "f") an inverse system with surjective transition morphisms and "I" the set of non-negative integers (such inverse systems are often called "Mittag-Leffler sequences"). However, in 2002, Amnon Neeman and Pierre Deligne constructed an example of such a system in a category satisfying (AB4) (in addition to (AB4*)) with lim "A" ≠ 0. Roos has since shown (in "Derived functors of inverse limits revisited") that his result is correct if "C" has a set of generators (in addition to satisfying (AB3) and (AB4*)).

Barry Mitchell has shown (in "The cohomological dimension of a directed set") that if "I" has cardinality formula_32 (the "d"th infinite cardinal), then "R"lim is zero for all "n" ≥ "d" + 2. This applies to the "I"-indexed diagrams in the category of "R"-modules, with "R" a commutative ring; it is not necessarily true in an arbitrary abelian category (see Roos' "Derived functors of inverse limits revisited" for examples of abelian categories in which lim, on diagrams indexed by a countable set, is nonzero for "n" > 1).

The categorical dual of an inverse limit is a direct limit (or inductive limit). More general concepts are the limits and colimits of category theory. The terminology is somewhat confusing: inverse limits are a class of limits, while direct limits are a class of colimits.




</doc>
<doc id="15111" url="https://en.wikipedia.org/wiki?curid=15111" title="Interplanetary spaceflight">
Interplanetary spaceflight

Interplanetary spaceflight or interplanetary travel is the crewed or uncrewed travel between stars and planets, usually within a single planetary system. In practice, spaceflights of this type are confined to travel between the planets of the Solar System.

Remotely guided space probes have flown by all of the planets of the Solar System from Mercury to Neptune, with the "New Horizons" probe having flown by the dwarf planet Pluto and the "Dawn" spacecraft currently orbiting the dwarf planet Ceres. The most distant spacecraft, "Voyager 1" and "Voyager 2" have left the Solar System as of while "Pioneer 10", "Pioneer 11", and "New Horizons" are on course to leave it.

In general, planetary orbiters and landers return much more detailed and comprehensive information than fly-by missions. Space probes have been placed into orbit around all the five planets known to the ancients: The first being Venus (Venera 7, 1970), Jupiter ("Galileo", 1995), Saturn ("Cassini/Huygens", 2004), and most recently Mercury ("MESSENGER", March 2011), and have returned data about these bodies and their natural satellites.

The NEAR Shoemaker mission in 2000 orbited the large near-Earth asteroid 433 Eros, and was even successfully landed there, though it had not been designed with this maneuver in mind. The Japanese ion-drive spacecraft "Hayabusa" in 2005 also orbited the small near-Earth asteroid 25143 Itokawa, landing on it briefly and returning grains of its surface material to Earth. Another powerful ion-drive mission, "Dawn", has orbited the large asteroid Vesta (July 2011 – September 2012) and later moved on to the dwarf planet Ceres, arriving in March 2015.

Remotely controlled landers such as Viking, Pathfinder and the two Mars Exploration Rovers have landed on the surface of Mars and several Venera and Vega spacecraft have landed on the surface of Venus. The "Huygens" probe successfully landed on Saturn's moon, Titan.

No crewed missions have been sent to any planet of the Solar System. NASA's Apollo program, however, landed twelve people on the Moon and returned them to Earth. The American Vision for Space Exploration, originally introduced by U.S. President George W. Bush and put into practice through the Constellation program, had as a long-term goal to eventually send human astronauts to Mars. However, on February 1, 2010, President Barack Obama proposed cancelling the program in Fiscal Year 2011. An earlier project which received some significant planning by NASA included a crewed fly-by of Venus in the Manned Venus Flyby mission, but was cancelled when the Apollo Applications Program was terminated due to NASA budget cuts in the late 1960s.

The costs and risk of interplanetary travel receive a lot of publicity—spectacular examples include the malfunctions or complete failures of probes without a human crew, such as Mars 96, Deep Space 2, and Beagle 2 (the article List of Solar System probes gives a full list).

Many astronomers, geologists and biologists believe that exploration of the Solar System provides knowledge that could not be gained by observations from Earth's surface or from orbit around Earth. But they disagree about whether human-crewed missions make a useful scientific contribution—some think robotic probes are cheaper and safer, while others argue that either astronauts advised by Earth-based scientists, or spacefaring scientists advised by Earth-based scientists, can respond more flexibly and intelligently to new or unexpected features of the region they are exploring.

Those who pay for such missions (primarily in the public sector) are more likely to be interested in benefits for themselves or for the human race as a whole. So far the only benefits of this type have been "spin-off" technologies which were developed for space missions and then were found to be at least as useful in other activities (NASA publicizes spin-offs from its activities).

Other practical motivations for interplanetary travel are more speculative, because our current technologies are not yet advanced enough to support test projects. But science fiction writers have a fairly good track record in predicting future technologies—for example geosynchronous communications satellites (Arthur C. Clarke) and many aspects of computer technology (Mack Reynolds).

Many science fiction stories feature detailed descriptions of how people could extract minerals from asteroids and energy from sources including orbital solar panels (unhampered by clouds) and the very strong magnetic field of Jupiter. Some point out that such techniques may be the only way to provide rising standards of living without being stopped by pollution or by depletion of Earth's resources (for example peak oil).

Finally, colonizing other parts of the Solar System would prevent the whole human species from being exterminated by any one of a number of possible events (see Human extinction). One of these possible events is an asteroid impact like the one which may have resulted in the Cretaceous–Paleogene extinction event. Although various Spaceguard projects monitor the Solar System for objects that might come dangerously close to Earth, current asteroid deflection strategies are crude and untested. To make the task more difficult, carbonaceous chondrites are rather sooty and therefore very hard to detect. Although carbonaceous chondrites are thought to be rare, some are very large and the suspected "dinosaur-killer" may have been a carbonaceous chondrite.

Some scientists, including members of the Space Studies Institute, argue that the vast majority of mankind eventually will live in space and will benefit from doing this.

One of the main challenges in interplanetary travel is producing the very large velocity changes necessary to travel from one body to another in the Solar System.

Due to the Sun's gravitational pull, a spacecraft moving farther from the Sun will slow down, while a spacecraft moving closer will speed up. Also, since any two planets are at different distances from the Sun, the planet from which the spacecraft starts is moving around the Sun at a different speed than the planet to which the spacecraft is travelling (in accordance with Kepler's Third Law). Because of these facts, a spacecraft desiring to transfer to a planet closer to the Sun must decrease its speed with respect to the Sun by a large amount in order to intercept it, while a spacecraft traveling to a planet farther out from the Sun must increase its speed substantially. Then, if additionally the spacecraft wishes to enter into orbit around the destination planet (instead of just flying by it), it must match the planet's orbital speed around the Sun, usually requiring another large velocity change.

Simply doing this by brute force – accelerating in the shortest route to the destination and then matching the planet's speed – would require an extremely large amount of fuel. And the fuel required for producing these velocity changes has to be launched along with the payload, and therefore even more fuel is needed to put both the spacecraft and the fuel required for its interplanetary journey into orbit. Thus, several techniques have been devised to reduce the fuel requirements of interplanetary travel.

As an example of the velocity changes involved, a spacecraft travelling from low Earth orbit to Mars using a simple trajectory must first undergo a change in speed (also known as a delta-v), in this case an increase, of about 3.8 km/s. Then, after intercepting Mars, it must change its speed by another 2.3 km/s in order to match Mars' orbital speed around the Sun and enter an orbit around it. For comparison, launching a spacecraft into low Earth orbit requires a change in speed of about 9.5 km/s.

For many years economical interplanetary travel meant using the Hohmann transfer orbit. Hohmann demonstrated that the lowest energy route between any two orbits is an elliptical "orbit" which forms a tangent to the starting and destination orbits. Once the spacecraft arrives, a second application of thrust will re-circularize the orbit at the new location. In the case of planetary transfers this means directing the spacecraft, originally in an orbit almost identical to Earth's, so that the aphelion of the transfer orbit is on the far side of the Sun near the orbit of the other planet. A spacecraft traveling from Earth to Mars via this method will arrive near Mars orbit in approximately 8.5 months, but because the orbital velocity is greater when closer to the center of mass (i.e. the Sun) and slower when farther from the center, the spacecraft will be traveling quite slowly and a small application of thrust is all that is needed to put it into a circular orbit around Mars. If the manoeuver is timed properly, Mars will be "arriving" under the spacecraft when this happens.

The Hohmann transfer applies to any two orbits, not just those with planets involved. For instance it is the most common way to transfer satellites into geostationary orbit, after first being "parked" in low Earth orbit. However, the Hohmann transfer takes an amount of time similar to ½ of the orbital period of the outer orbit, so in the case of the outer planets this is many years – too long to wait. It is also based on the assumption that the points at both ends are massless, as in the case when transferring between two orbits around Earth for instance. With a planet at the destination end of the transfer, calculations become considerably more difficult.

The gravitational slingshot technique uses the gravity of planets and moons to change the speed and direction of a spacecraft without using fuel. In typical example, a spacecraft is sent to a distant planet on a path that is much faster than what the Hohmann transfer would call for. This would typically mean that it would arrive at the planet's orbit and continue past it. However, if there is a planet between the departure point and the target, it can be used to bend the path toward the target, and in many cases the overall travel time is greatly reduced. A prime example of this are the two crafts of the Voyager program, which used slingshot effects to change trajectories several times in the outer Solar System. It is difficult to use this method for journeys in the inner part of the Solar System, although it is possible to use other nearby planets such as Venus or even the Moon as slingshots in journeys to the outer planets.

This maneuver can only change an object's velocity relative to a third, uninvolved object, – possibly the “centre of mass” or the Sun. There is no change in the velocities of the two objects involved in the maneuver relative to each other. The Sun cannot be used in a gravitational slingshot because it is stationary compared to rest of the Solar System, which orbits the Sun. It may be used to send a spaceship or probe into the galaxy because the Sun revolves around the center of the Milky Way.

A powered slingshot is the use of a rocket engine at or around closest approach to a body (periapsis). The use at this point multiplies up the effect of the delta-v, and gives a bigger effect than at other times.

Computers did not exist when Hohmann transfer orbits were first proposed (1925) and were slow, expensive and unreliable when gravitational slingshots were developed (1959). Recent advances in computing have made it possible to exploit many more features of the gravity fields of astronomical bodies and thus calculate even lower-cost trajectories. Paths have been calculated which link the Lagrange points of the various planets into the so-called Interplanetary Transport Network. Such "fuzzy orbits" use significantly less energy than Hohmann transfers but are much, much slower. They aren't practical for human crewed missions because they generally take years or decades, but may be useful for high-volume transport of low-value commodities if humanity develops a space-based economy.

Aerobraking uses the atmosphere of the target planet to slow down. It was first used on the Apollo program where the returning spacecraft did not enter Earth orbit but instead used a S-shaped vertical descent profile (starting with an initially steep descent, followed by a leveling out, followed by a slight climb, followed by a return to a positive rate of descent continuing to splash-down in the ocean) through Earth's atmosphere to reduce its speed until the parachute system could be deployed enabling a safe landing. Aerobraking does not require a thick atmosphere – for example most Mars landers use the technique, and Mars' atmosphere is only about 1% as thick as Earth's.

Aerobraking converts the spacecraft's kinetic energy into heat, so it requires a heatshield to prevent the craft from burning up. As a result, aerobraking is only helpful in cases where the fuel needed to transport the heatshield to the planet is less than the fuel that would be required to brake an unshielded craft by firing its engines. This can be addressed by creating heatshields from material available near the target

Several technologies have been proposed which both save fuel and provide significantly faster travel than the traditional methodology of using Hohmann transfers. Some are still just theoretical, but over time, several of the theoretical approaches have been tested on spaceflight missions. For example, the Deep Space 1 mission was a successful test of an ion drive. These improved technologies typically focus on one or more of:

Besides making travel faster or cost less, such improvements could also allow greater design "safety margins" by reducing the imperative to make spacecraft lighter.

All rocket concepts are limited by the rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, of initial ("M", including fuel) to final ("M", fuel depleted) mass. The main consequence is that mission velocities of more than a few times the velocity of the rocket motor exhaust (with respect to the vehicle) rapidly become impractical.

In a nuclear thermal rocket or solar thermal rocket a working fluid, usually hydrogen, is heated to a high temperature, and then expands through a rocket nozzle to create thrust. The energy replaces the chemical energy of the reactive chemicals in a traditional rocket engine. Due to the low molecular mass and hence high thermal velocity of hydrogen these engines are at least twice as fuel efficient as chemical engines, even after including the weight of the reactor.

The US Atomic Energy Commission and NASA tested a few designs from 1959 to 1968. The NASA designs were conceived as replacements for the upper stages of the Saturn V launch vehicle, but the tests revealed reliability problems, mainly caused by the vibration and heating involved in running the engines at such high thrust levels. Political and environmental considerations make it unlikely such an engine will be used in the foreseeable future, since nuclear thermal rockets would be most useful at or near the Earth's surface and the consequences of a malfunction could be disastrous. Fission-based thermal rocket concepts produce lower exhaust velocities than the electric and plasma concepts described below, and are therefore less attractive solutions. For applications requiring high thrust-to-weight ratio, such as planetary escape, nuclear thermal is potentially more attractive.

Electric propulsion systems use an external source such as a nuclear reactor or solar cells to generate electricity, which is then used to accelerate a chemically inert propellant to speeds far higher than achieved in a chemical rocket. Such drives produce feeble thrust, and are therefore unsuitable for quick maneuvers or for launching from the surface of a planet. But they are so economical in their use of
reaction mass that they can keep firing continuously for days or weeks, while chemical rockets use up reaction mass so quickly that they can only fire for seconds or minutes. Even a trip to the Moon is long enough for an electric propulsion system to outrun a chemical rocket – the Apollo missions took 3 days in each direction.

NASA's Deep Space One was a very successful test of a prototype ion drive, which fired for a total of 678 days and enabled the probe to run down Comet Borrelly, a feat which would have been impossible for a chemical rocket. "Dawn", the first NASA operational (i.e., non-technology demonstration) mission to use an ion drive for its primary propulsion, successfully orbited the large main-belt asteroids 1 Ceres and 4 Vesta. A more ambitious, nuclear-powered version was intended for a Jupiter mission without human crew, the Jupiter Icy Moons Orbiter (JIMO), originally planned for launch sometime in the next decade. Due to a shift in priorities at NASA that favored human crewed space missions, the project lost funding in 2005. A similar mission is currently under discussion as the US component of a joint NASA/ESA program for the exploration of Europa and Ganymede.

A NASA multi-center Technology Applications Assessment Team led from the Johnson Spaceflight Center, has as of January 2011 described "Nautilus-X", a concept study for a multi-mission space exploration vehicle useful for missions beyond low Earth orbit (LEO), of up to 24 months duration for a crew of up to six. Although Nautilus-X is adaptable to a variety of mission-specific propulsion units of various low-thrust, high specific impulse (I) designs, nuclear ion-electric drive is shown for illustrative purposes. It is intended for integration and checkout at the International Space Station (ISS), and would be suitable for deep-space missions from the ISS to and beyond the Moon, including Earth/Moon L1, Sun/Earth L2, near-Earth asteroidal, and Mars orbital destinations. It incorporates a reduced-g centrifuge providing artificial gravity for crew health to ameliorate the effects of long-term 0g exposure, and the capability to mitigate the space radiation environment.

The electric propulsion missions already flown, or currently scheduled, have used solar electric power, limiting their capability to operate far from the Sun, and also limiting their peak acceleration due to the mass of the electric power source. Nuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, can reach speeds much greater than chemically powered vehicles.

Fusion rockets, powered by nuclear fusion reactions, would "burn" such light element fuels as deuterium, tritium, or He. Because fusion yields about 1% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases only about 0.1% of the fuel's mass-energy. However, either fission or fusion technologies can in principle achieve velocities far higher than needed for Solar System exploration, and fusion energy still awaits practical demonstration on Earth.

One proposal using a fusion rocket was Project Daedalus. Another fairly detailed vehicle system, designed and optimized for crewed Solar System exploration, "Discovery II", based on the DHe reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of >300 km/s with an acceleration of ~1.7•10 "g", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%.

See the spacecraft propulsion article for a discussion of a number of other technologies that could, in the medium to longer term, be the basis of interplanetary missions. Unlike the situation with interstellar travel, the barriers to fast interplanetary travel involve engineering and economics rather than any basic physics.

Solar sails rely on the fact that light reflected from a surface exerts pressure on the surface. The radiation pressure is small and decreases by the square of the distance from the Sun, but unlike rockets, solar sails require no fuel. Although the thrust is small, it continues as long as the Sun shines and the sail is deployed.

The original concept relied only on radiation from the Sun – for example in Arthur C. Clarke's 1965 story "Sunjammer". More recent light sail designs propose to boost the thrust by aiming ground-based lasers or masers at the sail. Ground-based lasers or masers can also help a light-sail spacecraft to "decelerate": the sail splits into an outer and inner section, the outer section is pushed forward and its shape is changed mechanically to focus reflected radiation on the inner portion, and the radiation focused on the inner section acts as a brake.

Although most articles about light sails focus on interstellar travel, there have been several proposals for their use within the Solar System.

Currently, the only spacecraft to use a solar sail as the main method of propulsion is IKAROS which was launched by JAXA on May 21, 2010. It has since been successfully deployed, and shown to be producing acceleration as expected. Many ordinary spacecraft and satellites also use solar collectors, temperature-control panels and Sun shades as light sails, to make minor corrections to their attitude and orbit without using fuel. A few have even had small purpose-built solar sails for this use (for example Eurostar E3000 geostationary communications satellites built by EADS Astrium).

It is possible to put stations or spacecraft on orbits that cycle between different planets, for example a Mars cycler would synchronously cycle between Mars and Earth, with very little propellant usage to maintain the trajectory. Cyclers are conceptually a good idea, because massive radiation shields, life support and other equipment only need to be put onto the cycler trajectory once. A cycler could combine several roles: habitat (for example it could spin to produce an "artificial gravity" effect); mothership (providing life support for the crews of smaller spacecraft which hitch a ride on it). Cyclers could also possibly make excellent cargo ships for resupply of a colony.

A space elevator is a theoretical structure that would transport material from a planet's surface into orbit. The idea is that, once the expensive job of building the elevator is complete, an indefinite number of loads can be transported into orbit at minimal cost. Even the simplest designs avoid the vicious circle of rocket launches from the surface, wherein the fuel needed to travel the last 10% of the distance into orbit must be lifted all the way from the surface, requiring even more fuel, and so on. More sophisticated space elevator designs reduce the energy cost per trip by using counterweights, and the most ambitious schemes aim to balance loads going up and down and thus make the energy cost close to zero. Space elevators have also sometimes been referred to as "beanstalks", "space bridges", "space lifts", "space ladders" and "orbital towers". 

A terrestrial space elevator is beyond our current technology, although a lunar space elevator could theoretically be built using existing materials.

A skyhook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for skyhooks include designs that employ tethers spinning at hypersonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit. In addition, it has been suggested that the rotating skyhook is "not engineeringly feasible using presently available materials".

The SpaceX Starship, with maiden launch slated to be no earlier than 2020, is designed to be fully and rapidly reusable, making use of the SpaceX reusable technology that was developed during 2011–2018 for Falcon 9 and Falcon Heavy launch vehicles.

SpaceX CEO Elon Musk estimates that the reusability capability alone, on both the launch vehicle and the spacecraft associated with the Starship will reduce overall system costs per tonne delivered to Mars by at least two orders of magnitude over what NASA had previously achieved.

When launching interplanetary probes from the surface of Earth, carrying all energy needed for the long-duration mission, payload quantities are necessarily extremely limited, due to the basis mass limitations described theoretically by the rocket equation. One alternative to transport more mass on interplanetary trajectories is to use up nearly all of the upper stage propellant on launch, and then refill propellants in Earth orbit before firing the rocket to escape velocity for a heliocentric trajectory. These propellants could be stored on orbit at a propellant depot, or carried to orbit in a propellant tanker to be directly transferred to the interplanetary spacecraft. For returning mass to Earth, a related option is to mine raw materials from a solar system celestial object, refine, process, and store the reaction products (propellant) on the Solar System body until such time as a vehicle needs to be loaded for launch.

As of 2019, SpaceX is developing a system in which a reusable first stage vehicle would transport a crewed interplanetary spacecraft to Earth orbit, detach, return to its launch pad where a tanker spacecraft would be mounted atop it, then both fueled, then launched again to rendezvous with the waiting crewed spacecraft. The tanker would then transfer its fuel to the human crewed spacecraft for use on its interplanetary voyage. The SpaceX Starship is a stainless steel-structure spacecraft propelled by six Raptor engines operating on densified methane/oxygen propellants. It is -long, -diameter at its widest point, and is capable of transporting up to of cargo and passengers per trip to Mars, with on-orbit propellant refill before the interplanetary part of the journey.

As an example of a funded project currently under development, a key part of the system SpaceX has designed for Mars in order to radically decrease the cost of spaceflight to interplanetary destinations is the placement and operation of a physical plant on Mars to handle production and storage of the propellant components necessary to launch and fly the Starships back to Earth, or perhaps to increase the mass that can be transported onward to destinations in the outer Solar System.

The first Starship to Mars will carry a small propellant plant as a part of its cargo load. The plant will be expanded over multiple synods as more equipment arrives, is installed, and placed into mostly-autonomous production.

The SpaceX propellant plant will take advantage of the large supplies of carbon dioxide and water resources on Mars, mining the water (HO) from subsurface ice and collecting CO from the atmosphere. A chemical plant will process the raw materials by means of electrolysis and the Sabatier process to produce oxygen (O) and methane (CH), and then liquefy it to facilitate long-term storage and ultimate use.

Current space vehicles attempt to launch with all their fuel (propellants and energy supplies) on board that they will need for their entire journey, and current space structures are lifted from the Earth's surface. Non-terrestrial sources of energy and materials are mostly a lot further away, but most would not require lifting out of a strong gravity field and therefore should be much cheaper to use in space in the long term.

The most important non-terrestrial resource is energy, because it can be used to transform non-terrestrial materials into useful forms (some of which may also produce energy). At least two fundamental non-terrestrial energy sources have been proposed: solar-powered energy generation (unhampered by clouds), either directly by solar cells or indirectly by focusing solar radiation on boilers which produce steam to drive generators; and electrodynamic tethers which generate electricity from the powerful magnetic fields of some planets (Jupiter has a very powerful magnetic field).

Water ice would be very useful and is widespread on the moons of Jupiter and Saturn:

Oxygen is a common constituent of the moon's crust, and is probably abundant in most other bodies in the Solar System. Non-terrestrial oxygen would be valuable as a source of water ice only if an adequate source of hydrogen can be found. Possible uses include:


Unfortunately hydrogen, along with other volatiles like carbon and nitrogen, are much less abundant than oxygen in the inner Solar System.

Scientists expect to find a vast range of organic compounds in some of the planets, moons and comets of the outer Solar System, and the range of possible uses is even wider. For example, methane can be used as a fuel (burned with non-terrestrial oxygen), or as a feedstock for petrochemical processes such as making plastics. And ammonia could be a valuable feedstock for producing fertilizers to be used in the vegetable gardens of orbital and planetary bases, reducing the need to lift food to them from Earth.

Even unprocessed rock may be useful as rocket propellant if mass drivers are employed.

Life support systems must be capable of supporting human life for weeks, months or even years. A breathable atmosphere of at least must be maintained, with adequate amounts of oxygen, nitrogen, and controlled levels of carbon dioxide, trace gases and water vapor.

In October 2015, the NASA Office of Inspector General issued a health hazards report related to human spaceflight, including a human mission to Mars.
Once a vehicle leaves low Earth orbit and the protection of Earth's magnetosphere, it enters the Van Allen radiation belt, a region of high radiation. Once through there the radiation drops to lower levels, with a constant background of high energy cosmic rays which pose a health threat. These are dangerous over periods of years to decades.

Scientists of Russian Academy of Sciences are searching for methods of reducing the risk of radiation-induced cancer in preparation for the mission to Mars. They consider as one of the options a life support system generating drinking water with low content of deuterium (a stable isotope of hydrogen) to be consumed by the crew members. Preliminary investigations have shown that deuterium-depleted water features certain anti-cancer effects. Hence, deuterium-free drinking water is considered to have the potential of lowering the risk of cancer caused by extreme radiation exposure of the Martian crew.

In addition, coronal mass ejections from the Sun are highly dangerous, and are fatal within a very short timescale to humans unless they are protected by massive shielding.

Any major failure to a spacecraft en route is likely to be fatal, and even a minor one could have dangerous results if not repaired quickly, something difficult to accomplish in open space. The crew of the Apollo 13 mission survived despite an explosion caused by a faulty oxygen tank (1970).

For astrodynamics reasons, economic spacecraft travel to other planets is only practical within certain time windows. Outside these windows the planets are essentially inaccessible from Earth with current technology. This constrains flights and limits rescue options in the case of an emergency.


</doc>
<doc id="15112" url="https://en.wikipedia.org/wiki?curid=15112" title="Wave interference">
Wave interference

In physics, interference is a phenomenon in which two waves superpose to form a resultant wave of greater, lower, or the same amplitude. Constructive and destructive interference result from the interaction of waves that are correlated or coherent with each other, either because they come from the same source or because they have the same or nearly the same frequency. Interference effects can be observed with all types of waves, for example, light, radio, acoustic, surface water waves, gravity waves, or matter waves. The resulting images or graphs are called interferograms.

The principle of superposition of waves states that when two or more propagating waves of same type are incident on the same point, the resultant amplitude at that point is equal to the vector sum of the amplitudes of the individual waves. If a crest of a wave meets a crest of another wave of the same frequency at the same point, then the amplitude is the sum of the individual amplitudes—this is constructive interference. If a crest of one wave meets a trough of another wave, then the amplitude is equal to the difference in the individual amplitudes—this is known as destructive interference.

Constructive interference occurs when the phase difference between the waves is an even multiple of (180°) , whereas destructive interference occurs when the difference is an odd multiple of . If the difference between the phases is intermediate between these two extremes, then the magnitude of the displacement of the summed waves lies between the minimum and maximum values.

Consider, for example, what happens when two identical stones are dropped into a still pool of water at different locations. Each stone generates a circular wave propagating outwards from the point where the stone was dropped. When the two waves overlap, the net displacement at a particular point is the sum of the displacements of the individual waves. At some points, these will be in phase, and will produce a maximum displacement. In other places, the waves will be in anti-phase, and there will be no net displacement at these points. Thus, parts of the surface will be stationary—these are seen in the figure above and to the right as stationary blue-green lines radiating from the centre.

Interference of light is a common phenomenon that can be explained classically by the superposition of waves, however a deeper understanding of light interference requires knowledge of wave-particle duality of light which is due to quantum mechanics. Prime examples of light interference are the famous double-slit experiment, laser speckle, anti-reflective coatings and interferometers. Traditionally the classical wave model is taught as a basis for understanding optical interference, based on the Huygens–Fresnel principle.

The above can be demonstrated in one dimension by deriving the formula for the sum of two waves. The equation for the amplitude of a sinusoidal wave traveling to the right along the x-axis is
where formula_2 is the peak amplitude, formula_3 is the wavenumber and formula_4 is the angular frequency of the wave. Suppose a second wave of the same frequency and amplitude but with a different phase is also traveling to the right 
where formula_6 is the phase difference between the waves in radians. The two waves will superpose and add: the sum of the two waves is
Using the trigonometric identity for the sum of two cosines: formula_8 this can be written
This represents a wave at the original frequency, traveling to the right like the components, whose amplitude is proportional to the cosine of formula_10. 

A simple form of interference pattern is obtained if two plane waves of the same frequency intersect at an angle. 
Interference is essentially an energy redistribution process. The energy which is lost at the destructive interference is regained at the constructive interference.
One wave is travelling horizontally, and the other is travelling downwards at an angle θ to the first wave. Assuming that the two waves are in phase at the point B, then the relative phase changes along the "x"-axis. The phase difference at the point A is given by

It can be seen that the two waves are in phase when

and are half a cycle out of phase when

Constructive interference occurs when the waves are in phase, and destructive interference when they are half a cycle out of phase. Thus, an interference fringe pattern is produced, where the separation of the maxima is

and is known as the fringe spacing. The fringe spacing increases with increase in wavelength, and with decreasing angle .

The fringes are observed wherever the two waves overlap and the fringe spacing is uniform throughout.

A point source produces a spherical wave. If the light from two point sources overlaps, the interference pattern maps out the way in which the phase difference between the two waves varies in space. This depends on the wavelength and on the separation of the point sources. The figure to the right shows interference between two spherical waves. The wavelength increases from top to bottom, and the distance between the sources increases from left to right.

When the plane of observation is far enough away, the fringe pattern will be a series of almost straight lines, since the waves will then be almost planar.

Interference occurs when several waves are added together provided that the phase differences between them remain constant over the observation time.

It is sometimes desirable for several waves of the same frequency and amplitude to sum to zero (that is, interfere destructively, cancel). This is the principle behind, for example, 3-phase power and the diffraction grating. In both of these cases, the result is achieved by uniform spacing of the phases.

It is easy to see that a set of waves will cancel if they have the same amplitude and their phases are spaced equally in angle. Using phasors, each wave can be represented as formula_21 for formula_22 waves from formula_23 to formula_24, where

To show that

one merely assumes the converse, then multiplies both sides by formula_27

The Fabry–Pérot interferometer uses interference between multiple reflections.

A diffraction grating can be considered to be a multiple-beam interferometer; since the peaks which it produces are generated by interference between the light transmitted by each of the elements in the grating; see interference vs. diffraction for further discussion.

Because the frequency of light waves (~10 Hz) is too high to be detected by currently available detectors, it is possible to observe only the intensity of an optical interference pattern. The intensity of the light at a given point is proportional to the square of the average amplitude of the wave. This can be expressed mathematically as follows. The displacement of the two waves at a point is:

where represents the magnitude of the displacement, represents the phase and represents the angular frequency.

The displacement of the summed waves is

The intensity of the light at is given by

This can be expressed in terms of the intensities of the individual waves as

Thus, the interference pattern maps out the difference in phase between the two waves, with maxima occurring when the phase difference is a multiple of 2. If the two beams are of equal intensity, the maxima are four times as bright as the individual beams, and the minima have zero intensity.

The two waves must have the same polarization to give rise to interference fringes since it is not possible for waves of different polarizations to cancel one another out or add together. Instead, when waves of different polarization are added together, they give rise to a wave of a different polarization state.

The discussion above assumes that the waves which interfere with one another are monochromatic, i.e. have a single frequency—this requires that they are infinite in time. This is not, however, either practical or necessary. Two identical waves of finite duration whose frequency is fixed over that period will give rise to an interference pattern while they overlap. Two identical waves which consist of a narrow spectrum of frequency waves of finite duration, will give a series of fringe patterns of slightly differing spacings, and provided the spread of spacings is significantly less than the average fringe spacing, a fringe pattern will again be observed during the time when the two waves overlap.

Conventional light sources emit waves of differing frequencies and at different times from different points in the source. If the light is split into two waves and then re-combined, each individual light wave may generate an interference pattern with its other half, but the individual fringe patterns generated will have different phases and spacings, and normally no overall fringe pattern will be observable. However, single-element light sources, such as sodium- or mercury-vapor lamps have emission lines with quite narrow frequency spectra. When these are spatially and colour filtered, and then split into two waves, they can be superimposed to generate interference fringes. All interferometry prior to the invention of the laser was done using such sources and had a wide range of successful applications.

A laser beam generally approximates much more closely to a monochromatic source, and it is much more straightforward to generate interference fringes using a laser. The ease with which interference fringes can be observed with a laser beam can sometimes cause problems in that stray reflections may give spurious interference fringes which can result in errors.

Normally, a single laser beam is used in interferometry, though interference has been observed using two independent lasers whose frequencies were sufficiently matched to satisfy the phase requirements.
This has also been observed for widefield interference between two incoherent laser sources.
It is also possible to observe interference fringes using white light. A white light fringe pattern can be considered to be made up of a 'spectrum' of fringe patterns each of slightly different spacing. If all the fringe patterns are in phase in the centre, then the fringes will increase in size as the wavelength decreases and the summed intensity will show three to four fringes of varying colour. Young describes this very elegantly in his discussion of two slit interference. Since white light fringes are obtained only when the two waves have travelled equal distances from the light source, they can be very useful in interferometry, as they allow the zero path difference fringe to be identified.

To generate interference fringes, light from the source has to be divided into two waves which have then to be re-combined. Traditionally, interferometers have been classified as either amplitude-division or wavefront-division systems.

In an amplitude-division system, a beam splitter is used to divide the light into two beams travelling in different directions, which are then superimposed to produce the interference pattern. The Michelson interferometer and the Mach–Zehnder interferometer are examples of amplitude-division systems.

In wavefront-division systems, the wave is divided in space—examples are Young's double slit interferometer and Lloyd's mirror.

Interference can also be seen in everyday phenomena such as iridescence and structural coloration. For example, the colours seen in a soap bubble arise from interference of light reflecting off the front and back surfaces of the thin soap film. Depending on the thickness of the film, different colours interfere constructively and destructively.

Interferometry has played an important role in the advancement of physics, and also has a wide range of applications in physical and engineering measurement.

Thomas Young's double slit interferometer in 1803 demonstrated interference fringes when two small holes were illuminated by light from another small hole which was illuminated by sunlight. Young was able to estimate the wavelength of different colours in the spectrum from the spacing of the fringes. The experiment played a major role in the general acceptance of the wave theory of light. 
In quantum mechanics, this experiment is considered to demonstrate the inseparability of the wave and particle natures of light and other quantum particles (wave–particle duality). Richard Feynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment.
The results of the Michelson–Morley experiment are generally considered to be the first strong evidence against the theory of a luminiferous aether and in favor of special relativity.

Interferometry has been used in defining and calibrating length standards. When the metre was defined as the distance between two marks on a platinum-iridium bar, Michelson and Benoît used interferometry to measure the wavelength of the red cadmium line in the new standard, and also showed that it could be used as a length standard. Sixty years later, in 1960, the metre in the new SI system was defined to be equal to 1,650,763.73 wavelengths of the orange-red emission line in the electromagnetic spectrum of the krypton-86 atom in a vacuum. This definition was replaced in 1983 by defining the metre as the distance travelled by light in vacuum during a specific time interval. Interferometry is still fundamental in establishing the calibration chain in length measurement.

Interferometry is used in the calibration of slip gauges (called gauge blocks in the US) and in coordinate-measuring machines. It is also used in the testing of optical components.

In 1946, a technique called astronomical interferometry was developed. Astronomical radio interferometers usually consist either of arrays of parabolic dishes or two-dimensional arrays of omni-directional antennas. All of the telescopes in the array are widely separated and are usually connected together using coaxial cable, waveguide, optical fiber, or other type of transmission line. Interferometry increases the total signal collected, but its primary purpose is to vastly increase the resolution through a process called Aperture synthesis. This technique works by superposing (interfering) the signal waves from the different telescopes on the principle that waves that coincide with the same phase will add to each other while two waves that have opposite phases will cancel each other out. This creates a combined telescope that is equivalent in resolution (though not in sensitivity) to a single antenna whose diameter is equal to the spacing of the antennas furthest apart in the array.

An acoustic interferometer is an instrument for measuring the physical characteristics of sound waves in a gas or liquid, such velocity, wavelength, absorption, or impedance. A vibrating crystal creates ultrasonic waves that are radiated into the medium. The waves strike a reflector placed parallel to the crystal, reflected back to the source and measured.

If a system is in state formula_33, its wavefunction is described in Dirac or bra–ket notation as:

where the formula_35s specify the different quantum "alternatives" available (technically, they form an eigenvector basis) and the formula_36 are the probability amplitude coefficients, which are complex numbers.

The probability of observing the system making a transition or quantum leap from state formula_33 to a new state formula_38 is the square of the modulus of the scalar or inner product of the two states:

where formula_41 (as defined above) and similarly formula_42 are the coefficients of the final state of the system. * is the complex conjugate so that formula_43, etc.

Now consider the situation classically and imagine that the system transited from formula_44 to formula_45 via an intermediate state formula_46. Then we would "classically" expect the probability of the two-step transition to be the sum of all the possible intermediate steps. So we would have

The classical and quantum derivations for the transition probability differ by the presence, in the quantum case, of the extra terms formula_49; these extra quantum terms represent "interference" between the different formula_50 intermediate "alternatives". These are consequently known as the "quantum interference terms", or "cross terms". This is a purely quantum effect and is a consequence of the non-additivity of the probabilities of quantum alternatives.

The interference terms vanish, via the mechanism of quantum decoherence, if the intermediate state formula_35 is measured or coupled with its environment. 



</doc>
<doc id="15114" url="https://en.wikipedia.org/wiki?curid=15114" title="Indictable offence">
Indictable offence

In many common law jurisdictions (e.g., England and Wales, Ireland, Canada, Hong Kong, India, Australia, New Zealand, Malaysia, Singapore), an indictable offence is an offence which can only be tried on an indictment after a preliminary hearing to determine whether there is a "prima facie" case to answer or by a grand jury (in contrast to a summary offence). In the United States, a crime of similar severity and rules is called a felony, which also requires an indictment. In Scotland, which is a hybrid common law jurisdiction, the procurator fiscal will commence solemn proceedings for serious crimes to be prosecuted on indictment before a jury.

In Canada, an indictable offence is a crime that is more serious than a summary offence. Examples of indictable offences include theft over $5,000, breaking and entering, aggravated sexual assault, and murder. Maximum penalties for indictable offences are different depending on the crime and can include life in prison. There are minimum penalties for some indictable offences.

In relation to England and Wales, the expression "indictable offence" means an offence which, if committed by an adult, is triable on indictment, whether it is exclusively so triable or triable either way; and the term "indictable", in its application to offences, is to be construed accordingly. In this definition, references to the way or ways in which an offence is triable are to be construed without regard to the effect, if any, of section 22 of the Magistrates' Courts Act 1980 on the mode of trial in a particular case.

An either-way offence allows the defendant to elect between trial by jury on indictment in the Crown Court and summary trial in a magistrates' court. However, the election may be overruled by the magistrates' court if the facts suggest that the sentencing powers of a magistrates' court would be inadequate to reflect the seriousness of the offence. 

In relation to some indictable offences, for example criminal damage, only summary trial is available unless the damage caused exceeds £5,000. 

A youth court has jurisdiction to try all indictable offences with the exception of homicide and certain firearms offences, and will normally do so provided that the available sentencing power of two years' detention is adequate to punish the offender if found guilty.

See section 64 of the Criminal Law Act 1977. 

Grand juries were abolished in 1933.

Some offences such as murder and rape are considered so serious that they can only be tried on indictment at the Crown Court where the widest range of sentencing powers is available to the judge.

The expression "indictable-only offence" was defined by section 51 of the Crime and Disorder Act 1998, as originally enacted, as an offence triable only on indictment. Sections 51 and 52 of, and Schedule 3 to, that Act abolished committal proceedings for such offences and made other provisions in relation to them.

When the accused is charged with an indictable-only offence, he or she will be tried in the Crown Court. The rules are different in England and Wales in respect of those under 18 years of age.

See also section 14(a) of the Criminal Law Act 1977.

Similarly in New Zealand, a rape or murder charge will be tried at the High Court, while less serious offences such as theft, will be tried at the District Court. However, the District Court can hold both jury and summary trials.

In United States penal law, other than a felony, court findings of, or an act of gross negligence can be counted as an indictable offence.



</doc>
<doc id="15116" url="https://en.wikipedia.org/wiki?curid=15116" title="Inter Milan">
Inter Milan

Football Club Internazionale Milano, commonly referred to as Internazionale () or simply Inter, and known as Inter Milan outside Italy, is an Italian professional football club based in Milan, Lombardy. Inter is the only Italian club never to have been relegated from the top flight of Italian football.

Founded in 1908 following a schism within the Milan Cricket and Football Club (now A.C. Milan), Inter won its first championship in 1910. Since its formation, the club has won 30 domestic trophies, including 18 league titles, 7 Coppa Italia and 5 Supercoppa Italiana. From 2006 to 2010, the club won five successive league titles, equalling the all-time record at that time. They have won the Champions League three times: two back-to-back in 1964 and 1965 and then another in 2010. Their latest win completed an unprecedented Italian seasonal treble, with Inter winning the Coppa Italia and the "Scudetto" the same year. The club has also won three UEFA Cups, two Intercontinental Cups and one FIFA Club World Cup.

Inter's home games are played at the San Siro stadium, which they share with city rivals A.C. Milan. The stadium is the largest in Italian football with a capacity of 80,018. They have long-standing rivalries with A.C. Milan, with whom they contest the Derby della Madonnina, and Juventus, with whom they contest the Derby d'Italia; their rivalry with the former is one of the most followed derbies in football. , Inter has the highest home game attendance in Italy and the sixth highest attendance in Europe. The club is one of the most valuable in Italian and world football.

The club was founded on 9 March 1908 as "Football Club Internazionale", following the schism with the Milan Cricket and Football Club (now A.C. Milan). The name of the club derives from the wish of its founding members to accept foreign players without limits as well as Italians.

The club won its very first championship in 1910 and its second in 1920. The captain and coach of the first championship winning team was Virgilio Fossati, who was later killed in battle while serving in the Italian army during World War I. In 1922, Inter was at risk of relegation to the second division, but they remained in the top league after winning two play-offs.

Six years later, during the Fascist era, the club was forced to merge with the "Unione Sportiva Milanese" and was renamed "Società Sportiva Ambrosiana". During the 1928–29 season, the team wore white jerseys with a red cross emblazoned on it; the jersey's design was inspired by the flag and coat of arms of the city of Milan. In 1929, the new club chairman Oreste Simonotti changed the club's name to "Associazione Sportiva Ambrosiana" and restored the previous black-and-blue jerseys, however supporters continued to call the team "Inter", and in 1931 new chairman Pozzani caved in to shareholder pressure and changed the name to "Associazione Sportiva Ambrosiana-Inter".
Their first Coppa Italia (Italian Cup) was won in 1938–39, led by the iconic Giuseppe Meazza, after whom the San Siro stadium is officially named. A fifth championship followed in 1940, despite Meazza incurring an injury. After the end of World War II the club regained its original name, winning its sixth championship in 1953 and its seventh in 1954.

In 1960, manager Helenio Herrera joined Inter from Barcelona, bringing with him his midfield general Luis Suárez, who won the European Footballer of the Year in the same year for his role in Barcelona's La Liga/Fairs Cup double. He would transform Inter into one of the greatest teams in Europe. He modified a 5–3–2 tactic known as the ""Verrou"" ("door bolt") which created greater flexibility for counterattacks. The "catenaccio" system was invented by an Austrian coach, Karl Rappan. Rappan's original system was implemented with four fixed defenders, playing a strict man-to-man marking system, plus a playmaker in the middle of the field who plays the ball together with two midfield wings. Herrera would modify it by adding a fifth defender, the sweeper or libero behind the two centre backs. The sweeper or libero who acted as the free man would deal with any attackers who went through the two centre backs. Inter finished third in the Serie A in his first season, second the next year and first in his third season. Then followed a back-to-back European Cup victory in 1964 and 1965, earning him the title ""il Mago"" ("the Wizard"). The core of Herrera's team were the attacking fullbacks Tarcisio Burgnich and Giacinto Facchetti, Armando Picchi the sweeper, Suárez the playmaker, Jair the winger, Mario Corso the left midfielder, and Sandro Mazzola, who played on the inside-right.
In 1964, Inter reached the European Cup Final by beating Borussia Dortmund in the semi-final and Partizan in the quarter-final. In the final, they met Real Madrid, a team that had reached seven out of the nine finals to date. Mazzola scored two goals in a 3–1 victory, and then the team won the Intercontinental Cup against Independiente. A year later, Inter repeated the feat by beating two-time winner Benfica in the final held at home, from a Jair goal, and then again beat Independiente in the Intercontinental Cup.

In 1967, with Jair gone and Suárez injured, Inter lost the European Cup Final 2–1 to Celtic. During that year the club changed its name to "Football Club Internazionale Milano".

Following the golden era of the 1960s, Inter managed to win their eleventh league title in 1971 and their twelfth in 1980. Inter were defeated for the second time in five years in the final of the European Cup, going down 0–2 to Johan Cruyff's Ajax in 1972. During the 1970s and the 1980s, Inter also added two to its Coppa Italia tally, in 1977–78 and 1981–82.

Led by the German duo of Andreas Brehme and Lothar Matthäus, and Argentine Ramón Díaz, Inter captured the 1989 Serie A championship. Inter were unable to defend their title despite adding fellow German Jürgen Klinsmann to the squad and winning their first Supercoppa Italiana at the start of the season.

The 1990s was a period of disappointment. While their great rivals Milan and Juventus were achieving success both domestically and in Europe, Inter were left behind, with repeated mediocre results in the domestic league standings, their worst coming in 1993–94 when they finished just one point out of the relegation zone. Nevertheless, they achieved some European success with three UEFA Cup victories in 1991, 1994 and 1998.

With Massimo Moratti's takeover from Ernesto Pellegrini in 1995, Inter twice broke the world record transfer fee in this period (£19.5 million for Ronaldo from Barcelona in 1997 and £31 million for Christian Vieri from Lazio two years later). However, the 1990s remained the only decade in Inter's history in which they did not win a single Serie A championship. For Inter fans, it was difficult to find who in particular was to blame for the troubled times and this led to some icy relations between them and the chairman, the managers and even some individual players.
Moratti later became a target of the fans, especially when he sacked the much-loved coach Luigi Simoni after only a few games into the 1998–99 season, having just received the Italian manager of the year award for 1998 the day before being dismissed. That season, Inter failed to qualify for any European competition for the first time in almost ten years, finishing in eighth place.

The following season, Moratti appointed former Juventus manager Marcello Lippi, and signed players such as Angelo Peruzzi and Laurent Blanc together with other former Juventus players Vieri and Vladimir Jugović. The team came close to their first domestic success since 1989 when they reached the Coppa Italia final only to be defeated by Lazio.

Inter's misfortunes continued the following season, losing the 2000 Supercoppa Italiana match against Lazio 4–3 after initially taking the lead through new signing Robbie Keane. They were also eliminated in the preliminary round of the Champions League by Swedish club Helsingborgs IF, with Álvaro Recoba missing a crucial late penalty. Lippi was sacked after only a single game of the new season following Inter's first ever Serie A defeat to Reggina. Marco Tardelli, chosen to replace Lippi, failed to improve results, and is remembered by Inter fans as the manager that lost 6–0 in the city derby against Milan. Other members of the Inter "family" during this period that suffered were the likes of Vieri and Fabio Cannavaro, both of whom had their restaurants in Milan vandalised after defeats to the "Rossoneri".

In 2002, not only did Inter manage to make it to the UEFA Cup semi-finals, but were also only 45 minutes away from capturing the "Scudetto" when they needed to maintain their one-goal advantage away to Lazio. Inter were 2–1 up after only 24 minutes. Lazio equalised during first half injury time and then scored two more goals in the second half to clinch victory that eventually saw Juventus win the championship. The next season, Inter finished as league runners-up and also managed to make it to the 2002–03 Champions League semi-finals against Milan, losing on the away goals rule.

On 8 July 2004, Inter appointed former Lazio coach Roberto Mancini as its new head coach. In his first season, the team collected 72 points from 18 wins, 18 draws and only two losses, as well as winning the Coppa Italia and later the Supercoppa Italiana. On 11 May 2006, Inter retained their Coppa Italia title once again after defeating Roma with a 4–1 aggregate victory (a 1–1 scoreline in Rome and a 3–1 win at the San Siro).

Inter were awarded the 2005–06 Serie A championship retrospectively after points were stripped from Juventus and Milan due to the match fixing scandal that year. During the following season, Inter went on a record-breaking run of 17 consecutive victories in Serie A, starting on 25 September 2006 with a 4–1 home victory over Livorno, and ending on 28 February 2007, after a 1–1 draw at home to Udinese. On 22 April 2007, Inter won their second consecutive "Scudetto"—and first on the field since 1989—when they defeated Siena 2–1 at Stadio Artemio Franchi. Italian World Cup-winning defender Marco Materazzi scored both goals.

Inter started the 2007–08 season with the goal of winning both Serie A and Champions League. The team started well in the league, topping the table from the first round of matches, and also managed to qualify for the Champions League knockout stage. However, a late collapse, leading to a 2–0 defeat with ten men away to Liverpool on 19 February in the Champions League, threw into question manager Roberto Mancini's future at Inter while domestic form took a sharp turn of fortune with the team failing to win in the three following Serie A games. After being eliminated by Liverpool in the Champions League, Mancini announced his intention to leave his job immediately only to change his mind the following day. On the final day of the 2007–08 Serie A season, Inter played Parma away, and two goals from Zlatan Ibrahimović sealed their third consecutive championship. Mancini, however, was sacked soon after due to his previous announcement to leave the club.
On 2 June 2008, Inter appointed former Porto and Chelsea boss José Mourinho as new head coach. In his first season, the "Nerazzurri" won a Suppercoppa Italiana and a fourth consecutive title, though falling in the Champions League in the first knockout round for a third-straight year, losing to eventual finalist Manchester United. In winning the league title Inter became the first club in the last 60 years to win the title for the fourth consecutive time and joined Torino and Juventus as the only clubs to accomplish this feat, as well as being the first club based outside Turin.

Inter enjoyed more success in the 2009–10 Champions League, defeating reigning champions Barcelona in the semi-final, and then beating Bayern Munich 2–0 in the final with two goals from Diego Milito. Inter also won the 2009–10 Serie A title by two points over Roma, and the 2010 Coppa Italia by defeating the same side 1–0 in the final. This made Inter the first Italian team to win Treble. At the end of the season, Mourinho left the club to manage Real Madrid; he was replaced by Rafael Benítez.

On 21 August 2010, Inter defeated Roma 3–1 and won the 2010 Supercoppa Italiana, their fourth trophy of the year. In December 2010, they claimed the FIFA Club World Cup for the first time after a 3–0 win against TP Mazembe in the final. However, after this win, on 23 December 2010, due to their declining performance in Serie A, the team fired Benítez. He was replaced by Leonardo the following day.

Leonardo started with 30 points from 12 games, with an average of 2.5 points per game, better than his predecessors Benítez and Mourinho. On 6 March 2011, Leonardo set a new Italian Serie A record by collecting 33 points in 13 games; the previous record was 32 points in 13 games made by Fabio Capello in the 2004–05 season. Leonardo led the club to the quarter-finals of the Champions League before losing to Schalke 04, and lead them to Coppa Italia title. At the end of the season, however, he resigned and was followed by new managers Gian Piero Gasperini, Claudio Ranieri and Andrea Stramaccioni, all hired during the following season. 

On 1 August 2012, the club announced that Moratti was to sell a minority interest of the club to a Chinese consortium led by Kenneth Huang. On the same day, Inter announced an agreement was formed with China Railway Construction Corporation Limited for a new stadium project, however, the deal with the Chinese eventually collapsed. The 2012–13 season was the worst in recent club history with Inter finishing ninth in Serie A and failing to qualify for any European competitions. Walter Mazzarri was appointed to replace Stramaccioni as the manager for 2013–14 season on 24 May 2013, having ended his tenure at Napoli. He guided the club to fifth in Serie A and to 2014–15 UEFA Europa League qualification.
On 15 October 2013, an Indonesian consortium (International Sports Capital HK Ltd.) led by Erick Thohir, Handy Soetedjo and Rosan Roeslani, signed an agreement to acquire 70% of Inter shares from Internazionale Holding S.r.l. Immediately after the deal, Moratti's Internazionale Holding S.r.l. still retained 29.5% of the shares of FC Internazionale Milano S.p.A. After the deal, the shares of Inter was owned by a chain of holding companies, namely International Sports Capital S.p.A. of Italy (for 70% stake), International Sports Capital HK Limited and Asian Sports Ventures HK Limited of Hong Kong. Asian Sports Ventures HK Limited, itself another intermediate holding company, was owned by Nusantara Sports Ventures HK Limited (60% stake, a company owned by Thohir), Alke Sports Investment HK Limited (20% stake) and Aksis Sports Capital HK Limited (20% stake).

Thohir, whom also co-owned Major League Soccer (MLS) club D.C. United and Indonesia Super League (ISL) club Persib Bandung, announced on 2 December 2013 that Inter and D.C. United had formed strategic partnership. During the Thohir era the club began to modify its financial structure from one reliant on continual owner investment to a more self sustain business model although the club still breached UEFA Financial Fair Play Regulations in 2015. The club was fined and received squad reduction in UEFA competitions, with additional penalties suspended in the probation period. During this time, Roberto Mancini returned as the club manager on 14 November 2014, with Inter finishing 8th. Inter finished 2015–2016 season fourth, failing to return to Champions League. 

On 6 June 2016, Suning Holdings Group (via a Luxembourg-based subsidiary Great Horizon S.á r.l.) a company owned by Zhang Jindong, co-founder and chairman of Suning Commerce Group, acquired a majority stake of Inter from Thohir's consortium International Sports Capital S.p.A. and from Moratti family's remaining shares in Internazionale Holding S.r.l. According to various filings, the total investment from Suning was €270 million. The deal was approved by an extraordinary general meeting on 28 June 2016, from which Suning Holdings Group had acquired a 68.55% stake in the club.

The first season of new ownership, however, started with poor performance in pre-season friendlies. On 8 August 2016, Inter parted company with head coach Roberto Mancini by mutual consent over disagreements regarding the club's direction. He was replaced by Frank de Boer who was sacked on 1 November 2016 after leading Inter to a 4W–2D–5L record in 11 Serie A games as head coach. The successor, Stefano Pioli, didn't save the team from getting the worst group result in UEFA competitions in the club's history. Despite an eight-game winning streak, he and the club parted away before season's end when it became clear they would finish outside the league's top three for the sixth consecutive season. On 9 June 2017, former Roma coach Luciano Spalletti was appointed as Inter manager, signing a two-year contract, and eleven months later Inter clinched a UEFA Champions League group stage spot after going six years without Champions League participation thanks to a 3–2 victory against Lazio in the final game of 2017–18 Serie A. Due to this success, in August the club extended the contract with Spalletti to 2021.

On 26 October 2018, Steven Zhang was appointed as new president of the club. On 25 January 2019, the club officially announced that LionRock Capital from Hong Kong reached an agreement with International Sports Capital HK Limited, in order to acquire its 31.05% shares in Inter and to become the club's new minority shareholder. After 2018–19 Serie A season, despite Inter finishing 4th, Spaletti was sacked. On 31 May 2019, Inter appointed former Juventus and Italian manager Antonio Conte as their new coach, signing a three-year deal. In September 2019, Steven Zhang was elected to the board of the European Club Association. In the 2019–20 Serie A, Inter Milan finished as runner-up as they won 2–0 against Atalanta on the last matchday. They also reached the 2020 UEFA Europa League Final.

One of the founders of Inter, a painter named Giorgio Muggiani, was responsible for the design of the first Inter logo in 1908. The first design incorporated the letters "FCIM" in the centre of a series of circles that formed the badge of the club. The basic elements of the design have remained constant even as finer details have been modified over the years. Starting at the 1999–2000 season, the original club crest was reduced in size, to give place for the addition of the club's name and foundation year at the upper and lower part of the logo respectively.

In 2007, the logo was returned to the pre-1999–2000 era. It was given a more modern look with a smaller "Scudetto" star and lighter color scheme. This version was used until July 2014, when the club decided to undertake a rebranding. The most significant difference between the current and the previous logo is the omission of the star from other media except match kits.

Since its founding in 1908, Inter have almost always worn black and blue stripes, earning them the nickname "Nerazzurri". According to the tradition, the colours were adopted to represent the nocturnal sky: in fact, the club was established on the night of 9 March, at 23:30; moreover, blue was chosen by Giorgio Muggiani because he considered it to be the opposite colour to red, worn by the Milan Cricket and Football Club rivals.

During the 1928–29 season, however, Inter were forced to abandon their black and blue uniforms. In 1928, Inter's name and philosophy made the ruling Fascist Party uneasy; as a result, during the same year the 20-year-old club was merged with "Unione Sportiva Milanese": the new club was named "Società Sportiva Ambrosiana" after the patron saint of Milan. The flag of Milan (the red cross on white background) replaced the traditional black and blue. In 1929 the black-and-blue jerseys were restored, and after World War II, when the Fascists had fallen from power, the club reverted to their original name. In 2008, Inter celebrated their centenary with a red cross on their away shirt. The cross is reminiscent of the flag of their city, and they continue to use the pattern on their third kit. In 2014, the club adopted a predominantly black home kit with thin blue pinstripes before returning to a more traditional design the following season.

Animals are often used to represent football clubs in Italy – the grass snake, called "Biscione", represents Inter. The snake is an important symbol for the city of Milan, appearing often in Milanese heraldry as a coiled viper with a man in its jaws. The symbol is present on the coat of arms of the House of Sforza (which ruled over Italy from Milan during the Renaissance period), the city of Milan, the historical Duchy of Milan (a 400-year state of the Holy Roman Empire) and Insubria (a historical region the city of Milan falls within). For the 2010–11 season, Inter's away kit featured the serpent.

The team's stadium is the 80,018 seat San Siro, officially known as the "Stadio Giuseppe Meazza" after the former player who represented both Milan and Inter. The more commonly used name, "San Siro", is the name of the district where it is located. San Siro has been the home of Milan since 1926, when it was privately built by funding from Milan's chairman at the time, Piero Pirelli. Construction was performed by 120 workers, and took 13 and a half months to complete. The stadium was owned by the club until it was sold to the city in 1935, and since 1947 it has been shared with Inter, when they were accepted as joint tenant.

The first game played at the stadium was on 19 September 1926, when Inter beat Milan 6–3 in a friendly match. Milan played its first league game in San Siro on 19 September 1926, losing 1–2 to Sampierdarenese. From an initial capacity of 35,000 spectators, the stadium has undergone several major renovations, most recently in preparation for the 1990 FIFA World Cup when its capacity was set to 85,700, all covered with a polycarbonate roof. In the summer of 2008, its capacity was reduced to 80,018 to meet the new standards set by UEFA.

Based on the English model for stadiums, San Siro is specifically designed for football matches, as opposed to many multi-purpose stadiums used in Serie A. It is therefore renowned in Italy for its fantastic atmosphere during matches owing to the closeness of the stands to the pitch. The frequent use of flares by supporters contributes to the atmosphere, but the practice has occasionally also caused problems.

Inter is one of the most supported clubs in Italy, according to an August 2007 research by Italian newspaper "La Repubblica". Historically, the largest section of Inter fans from the city of Milan were the middle-class bourgeoisie Milanese, while Milan fans were typically working-class.

The traditional ultras group of Inter is "Boys San"; they hold a significant place in the history of the ultras scene in general due to the fact that they are one of the oldest, being founded in 1969. Politically, the ultras of Inter are usually considered right-wing and they have good relationships with the Lazio ultras. As well as the main group of "Boys San", there are four more significant groups: "Viking", "Irriducibili", "Ultras", and "Brianza Alcoolica".

Inter's most vocal fans are known to gather in the Curva Nord, or north curve of the San Siro. This longstanding tradition has led to the Curva Nord being synonymous with the club's most die-hard supporters, who unfurl banners and wave flags in support of their team.
Inter have several rivalries, two of which are highly significant in Italian football; firstly, they participate in the intra city "Derby della Madonnina" with Milan; the rivalry has existed ever since Inter splintered off from Milan in 1908. The name of the derby refers to the Blessed Virgin Mary, whose statue atop the Milan Cathedral is one of the city's main attractions. The match usually creates a lively atmosphere, with numerous (often humorous or offensive) banners unfolded before the match. Flares are commonly present, but they also led to the abandonment of the second leg of the 2004–05 Champions League quarter-final matchup between Milan and Inter on 12 April after a flare thrown from the crowd by an Inter supporter struck Milan keeper Dida on the shoulder.

The other most significant rivalry is with Juventus; matches between the two clubs are known as the "Derby d'Italia". Up until the 2006 Italian football scandal, which saw Juventus relegated, the two were the only Italian clubs to have never played below Serie A. In the 2000s, Inter developed a rivalry with Roma, who finished as runners-up to Inter in all but one of Inter's five "Scudetto"-winning seasons between 2005 and 2010. The two sides have also contested in five Coppa Italia finals and four Supercoppa Italiana finals since 2006. Other clubs, like Atalanta and Napoli, are also considered amongst their rivals. Their supporters collectively go by "Interisti", or "Nerazzurri."

Inter have won 30 domestic trophies, including the league 18 times, the Coppa Italia seven and the Supercoppa Italiana five. From 2006 to 2010, the club won five successive league titles, equalling the all-time record before 2017, when Juventus won the sixth successive league title. They have won the Champions League three times: two back-to-back in 1964 and 1965 and then another in 2010; the last completed an unprecedented Italian treble with the Coppa Italia and the "Scudetto". The club has also won three UEFA Cups, two Intercontinental Cups and one FIFA Club World Cup.

Inter has never been relegated from the top flight of Italian football in its entire existence. It is the sole club to have competed in Serie A and its predecessors in every season. The "Nerrazurri" currently have the longest unbroken run in the top flight leagues of any club on the Continent, 106 seasons, and among European clubs, only five British clubs have longer current spells in the top flight.

Javier Zanetti holds the records for both total appearances and Serie A appearances for Inter, with 858 official games played in total and 618 in Serie A.

Giuseppe Meazza is Inter's all-time top goalscorer, with 284 goals in 408 games. Behind him, in second place, is Alessandro Altobelli with 209 goals in 466 games, and Roberto Boninsegna in third place, with 171 goals over 281 games.

Helenio Herrera had the longest reign as Inter coach, with nine years (eight consecutive) in charge, and is the most successful coach in Inter history with three "Scudetti", two European Cups, and two Intercontinental Cup wins. José Mourinho, who was appointed on 2 June 2008, completed his first season in Italy by winning the Serie A title and the Supercoppa Italiana; in his second season he won the first "treble" in Italian history: the Serie A, Coppa Italia and the UEFA Champions League.

. Players in bold will officially leave the team (e.g. bought out), while those in "italics" will end their contract with Inter at the end of the season.

 

3 – Giacinto Facchetti, left back, 1960–1978 "(posthumous honour)". The number was retired on 8 September 2006. The last player to wear the shirt was Argentinian center back Nicolás Burdisso, who took on the number 16 shirt for the rest of the season.
<br>
4 – Javier Zanetti, defensive midfielder, played 858 games for Inter between 1995 and his retirement in the summer of 2014. Club chairman Erick Thohir confirmed that Zanetti's number 4 was to be retired out of respect.

Below is a list of Inter chairmen from 1908 until the present day.

Below is a list of Inter coaches from 1909 until the present day.

FC Internazionale Milano S.p.A. was described as one of the financial "black-holes" among the Italian clubs, which was heavily dependent on the financial contribution from the owner Massimo Moratti. In June 2006, the shirt sponsor and the minority shareholder of the club, Pirelli, sold 15.26% shares of the club to Moratti family, for €13.5 million. The tyre manufacturer retained 4.2%. However, due to several capital increases of Inter, such as a reversed merger with an intermediate holding company, Inter Capital S.r.l. in 2006, which held 89% shares of Inter and €70 million capitals at that time, or issues new shares for €70.8 million in June 2007, €99.9 million in December 2007, €86.6 million in 2008, €70 million in 2009, €40 million in 2010 and 2011, €35 million in 2012 or allowing Thoir subscribed €75 million new shares of Inter in 2013, Pirelli became the third largest shareholders of just 0.5%, . Inter had yet another recapitalization that was reserved for Suning Holdings Group in 2016. In the prospectus of Pirelli's second IPO in 2017, the company also revealed that the value of the remaining shares of Inter that was owned by Pirelli, was write-off to zero in 2016 financial year. Inter also received direct capital contribution from the shareholders to cover loss which was excluded from issuing shares in the past. ()

Right before the takeover of Thohir, the consolidated balance sheets of "Internazionale Holding S.r.l." showed the whole companies group had a bank debt of €157 million, including the bank debt of a subsidiary "Inter Brand Srl", as well as the club itself, to Istituto per il Credito Sportivo (ICS), for €15.674 million on the balance sheet at end of 2012–13 financial year. In 2006 Inter sold its brand to the new subsidiary, "Inter Brand S.r.l.", a special purpose entity with a shares capital of €40 million, for €158 million (the deal made Internazionale make a net loss of just €31 million in a separate financial statement). At the same time the subsidiary secured a €120 million loan from Banca Antonveneta, which would be repaid in installments until 30 June 2016; "La Repubblica" described the deal as "doping". In September 2011 Inter secured a loan from ICS by factoring the sponsorship of Pirelli of 2012–13 and 2013–14 season, for €24.8 million, in an interest rate of 3 months Euribor + 1.95% spread. In June 2014 new Inter Group secured €230 million loan from Goldman Sachs and UniCredit at a new interest rate of 3 months Euribor + 5.5% spread, as well as setting up a new subsidiary to be the debt carrier: "Inter Media and Communication S.r.l.". €200 million of which would be utilized in debt refinancing of the group. The €230million loan, €1 million (plus interests) would be due on 30 June 2015, €45 million (plus interests) would be repaid in 15 installments from 30 September 2015 to 31 March 2019, as well as €184 million (plus interests) would be due on 30 June 2019. In ownership side, the Hong Kong-based International Sports Capital HK Limited, had pledged the shares of Italy-based International Sports Capital S.p.A. (the direct holding company of Inter) to CPPIB Credit Investments for €170 million in 2015, at an interest rate of 8% p.a (due March 2018) to 15% p.a. (due March 2020). ISC repaid the notes on 1 July 2016 after they sold part of the shares of Inter to Suning Holdings Group. However, in the late 2016 the shares of ISC S.p.A. was pledged again by ISC HK to private equity funds of OCP Asia for US$80 million. In December 2017, the club also refinanced its debt of €300 million, by issuing corporate bond to the market, via Goldman Sachs as the bookkeeper, for an interest rate of 4.875% p.a.

Considering revenue alone, Inter surpassed city rivals in Deloitte Football Money League for the first time, in the 2008–09 season, to rank in 9th place, one place behind Juventus in 8th place, with Milan in 10th place. In the 2009–10 season, Inter remained in 9th place, surpassing Juventus (10th) but Milan re-took the leading role as the 7th. Inter became the 8th in 2010–11, but was still one place behind Milan. Since 2011, Inter fell to 11th in 2011–12, 15th in 2012–13, 17th in 2013–14, 19th in 2014–15 and 2015–16 season. In 2016–17 season, Inter was ranked 15th in the "Money League".

In 2010 "Football Money League" (2008–09 season), the normalized revenue of €196.5 million were divided up between matchday (14%, €28.2 million), broadcasting (59%, €115.7 million, +7%, +€8 million) and commercial (27%, €52.6 million, +43%). Kit sponsors Nike and Pirelli contributed €18.1 million and €9.3 million respectively to commercial revenues, while broadcasting revenues were boosted €1.6 million (6%) by Champions League distribution. Deloitte expressed the idea that issues in Italian football, particularly matchday revenue issues were holding Inter back compared to other European giants, and developing their own stadia would result in Serie A clubs being more competitive on the world stage.

In 2009–10 season the revenue of Inter was boosted by the sales of Ibrahimović, the treble and the release clause of coach José Mourinho. According to the normalized figures by Deloitte in their 2011 "Football Money League", in 2009–10 season, the revenue had increased €28.3 million (14%) to €224.8 million. The ratio of matchday, broadcasting and commercial in the adjusted figures was 17%:62%:21%.

For the 2010–11 season, Serie A clubs started negotiating club TV rights collectively rather than individually. This was predicted to result in lower broadcasting revenues for big clubs such as Juventus and Inter, with smaller clubs gaining from the loss. Eventually the result included an extraordinary income of €13 million from RAI. In 2012 "Football Money League" (2010–11 season), the normalized revenue was €211.4 million. The ratio of matchday, broadcasting and commercial in the adjusted figures was 16%:58%:26%.

However, combining revenue and cost, in the 2006–07 season they had a net loss of €206 million (€112 million extraordinary basis, due to the abolition of non-standard accounting practice of the special amortization fund), followed by a net loss of €148 million in the 2007–08 season, a net loss of €154 million in 2008–09 season, a net loss of €69 million in the 2009–10 season, a net loss of €87 million in the 2010–11 season, a net loss of €77 million in the 2011–12 season, a net loss of €80 million in 2012–13 season and a net profit of €33 million in 2013–14 season, due to special income from the establishment of subsidiary Inter Media and Communication. All aforementioned figures were in separate financial statement. Figures from consolidated financial statement were announced since 2014–15 season, which were net losses of €140.4 million (2014–15), €59.6 million (2015–16 season, before 2017 restatement) and €24.6 million (2016–17).

In 2015 Inter and Roma were the only two Italian clubs that were sanctioned by the UEFA due to their breaking of UEFA Financial Fair Play Regulations, which was followed by Milan which was once barred from returning to European competition in 2018. As a probation to avoid further sanction, Inter agreed to have a three-year aggregate break-even from 2015 to 2018, with the 2015–16 season being allowed to have a net loss of a maximum of €30 million, followed by break-even in the 2016–17 season and onwards. Inter was also fined €6 million plus an additional €14 million in probation.

Inter also made a financial trick in the transfer market in mid-2015, in which Stevan Jovetić and Miranda were signed by Inter on temporary deals plus an obligation to sign outright in 2017, making their cost less in the loan period. Moreover, despite heavily investing in new signings, namely Geoffrey Kondogbia and Ivan Perišić that potentially increased the cost in amortization, Inter also sold Mateo Kovačić for €29 million, making a windfall profit. In November 2018, documents from Football Leaks further revealed that the loan signings such as Xherdan Shaqiri in January 2015, was in fact had inevitable conditions to trigger the outright purchase.

On 21 April 2017, Inter announced that their net loss (FFP adjusted) of 2015–16 season was within the allowable limit of €30 million. However, on the same day UEFA also announced that the reduction of squad size of Inter in European competitions would not be lifted yet, due to partial fulfilment of the targets in the settlement agreement. Same announcement was made by UEFA in June 2018, based on Inter's 2016–17 season financial result.

In February 2020, Inter Milan is suing MLS for trademark infringement, claiming that the term “Inter” is synonymous with its club and no one else.




</doc>
